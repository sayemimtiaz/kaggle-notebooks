{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"8cc4199ed202e0835a532937d928a43bd6f941b1","_cell_guid":"907b4290-8ccd-4a1a-b4d8-ed432a21ea14"},"source":"# Predicting Stock Market Prices Through News Texts\n### Homework project prepared for the Machine Learning Course at Higher School of Economics\n\n# 0. Introduction\nThe Dow Jones Industrial Average (DJIA) is the arithmetic average of the stock prices of the 30 largest US companies. DJIA reports the current state of affairs on the stock exchange since prices of the stocks of these companies correlate in some way: if DJIA grows, the prices of the companies' stocks also grow, and if it falls, the stock prices fall too. \n\nDynamics of stock prices changing can be caused by a variety of different events in the world, both economic and political. Stock traders are constantly up to date on the latest news to be aware of possible changes in stock prices. Roughly, all the incoming news could be divided into the two types: the ones causing the growth of certain stocks, and the ones causing their fall. Here comes an idea that it could be possible to create a trading bot that if could automatically process incoming data about events in the world from the news and sell or buy stocks. However, the problem is that there is a too great variety of circumstances that could cause stock price changes. Moreover, not all the information about of these factors could be obviously derived from news reports, and, after all, the trading strategy, decision-making algorithms and correct treshholds for different actions are not less that important, than a clever predictive model.\n\nNevertheless, I decided to check whether it is possible to win on the stock market using only an automatic classifier trained on the news bulletins using only the linguistic analysis of news texts without any external information. This notebook is divided into two parts: data exploration which could open more inside on the dataset, and a feature engineering in which I try to get different linguistic features from the raw text data."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9c0618b2a15bd5cf4d32de5cfc4463016339742c","_cell_guid":"248b993f-9eab-43d9-98e5-c09d433848d7"},"execution_count":null,"source":"# basic text pre-processing\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, LancasterStemmer\nfrom nltk import pos_tag\n\n# core modules\nfrom pandas import DataFrame, concat, options\nimport numpy as np\nimport re\nfrom collections import defaultdict, Counter\n\n# tools and models for machine learning\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom gensim.models import Word2Vec\n\n# visualization\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport matplotlib.pyplot as plt\nfrom matplotlib import use\nimport seaborn as sns\nfrom wordcloud import WordCloud","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"0aed699c0715ea4cc0de06cfb5631c9fe5c7098e","_cell_guid":"ddd0d375-db7c-4ec1-bf40-b390beeba50b"},"execution_count":null,"source":"wordnet_lemmatizer = WordNetLemmatizer()\nstemmer =  LancasterStemmer()\nstop = stopwords.words('english')\npy.init_notebook_mode(connected=True)\noptions.mode.chained_assignment = None","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"b7111d408f1a206521d7899051b16cd93e656c9c","_cell_guid":"bbe0491a-8e15-4e74-a063-875f6ff102ba"},"execution_count":null,"source":"import warnings\nwarnings.filterwarnings('ignore')","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"81e29dcf40d545c96c7d082526a622b9a0e8a0f3","_cell_guid":"e52e16f7-a7b7-42cd-96e9-b4d4b65ebbc1"},"source":"# 1. Data Exploration\n\nFirst of all, I manually divided the dataset into the training and the test set. News from 2008 to 2015 consist the training set, all other news consist the test set. "},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"1df601e2bfdff2a0636c6a73004f49f2cfa85bae","_cell_guid":"3b5bf53c-b8ed-4060-b290-ff20eceb7e23"},"execution_count":null,"source":"data = DataFrame.from_csv('../input/Combined_News_DJIA.csv').reset_index().fillna(' ')","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f1deb969738d047d19ec18898f4b1a096e29408f","_cell_guid":"dda3f3d2-6b0c-4907-ab22-f86d55e50a51"},"execution_count":null,"source":"train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']\ny_train = train.Label.values\ny_test = test.Label.values\ncol_number = 26","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"aa71f75187484f53b7495a5ae7dc82084527a5b6","_cell_guid":"e9d54627-5fa5-4e91-8867-3377ea65da01"},"execution_count":null,"source":"print('The size of the training set is {} news, the size of the test set is {} news'.format(len(train), len(test)))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fc870123b48c18274bdcd32c2fd6e0725cdf50f2","_cell_guid":"80aed54e-4b4e-4f23-9777-f9bf9b6f0d70"},"source":"Let's look at our data:"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"b3670cc9fa67e279e7308f9064ceb8709909ad79","_cell_guid":"4f3a1301-2365-4087-89ec-e82cc5601cbe"},"execution_count":null,"source":"train.head()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e264642f8f152c64f655943a240f8e3904720c61","_cell_guid":"a8ff8e90-3931-4bda-9f21-047abb33b911"},"source":"We have an almost equal class balance in the whole dataset. That's good."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"83e95adbecbdf973bfeccdcda009a67993cd0e4a","_cell_guid":"275375e0-48ba-4ac7-b849-51ead9e41b2b"},"execution_count":null,"source":"plt.figure(figsize=(12,6))\nsns.barplot(data.Label.value_counts().index, data.Label.value_counts().values, alpha=0.8)\nplt.ylabel('Amount of objects', fontsize=16)\nplt.xlabel('Class', fontsize=16)\nplt.show();","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"107676a7f27030ec78d1504849c991e7851b9f97","_cell_guid":"8124bdb0-dfc8-431c-9b64-efbdcff7c70c"},"source":"To begin with, I preprocess the texts with the help of basic NLP techniques: tokenization, lowercasing, lemmatization and/or stemming. Lemmatization is an algorithm of a morphological normalization which reduces the word to the to its morphological root (normalized form of a word), stemming is also an algorithm of a morphological normalization, but it reduces the word to its stem."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"73e06ad7905fc80632e102cd2eb0ebb94e6afa5a","_cell_guid":"465199ed-f302-4341-9c8a-84dc3dc26376"},"execution_count":null,"source":"lmb_f = [lambda x: re.sub(\"\"\"^b(\"|')\"\"\",'', str(x)),  \n         lambda x: str(x).lower(),\n         lambda x: str(x).replace(\"'\",''),\n         lambda x: word_tokenize(str(x)),\n         lambda x: [wordnet_lemmatizer.lemmatize(str(i)) for i in x],\n         lambda x: [stemmer.stem(str(i)) for i in x],\n         lambda x: ' '.join(x)\n        ]","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7eb4740ff5f373ba580391c40e5d70a0d2a9c20e","_cell_guid":"b230c5b2-0b21-4c47-96a9-9acb7c9ab61d"},"execution_count":null,"source":"def parse_trainset(data, preproc='lem'):\n    if preproc == 'lem':\n        lambdas = lmb_f[0:5] + lmb_f[6:]\n    elif preproc == 'stem':\n        lambdas = lmb_f[0:4] + lmb_f[5:]\n    elif preproc == 'lem+stem':\n        lambdas = lmb_f\n    elif preproc == '_':\n        lambdas = lmb_f[0:5]\n    li = []\n    for col in range(1, col_number):\n        s = data.loc[:,'Top' + str(col)]\n        for a in lambdas:\n            s = s.apply(a)\n        li.append(s)\n    return li","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"80d7bdbed05ef8f96d2071e5d0835ced860dc462","_cell_guid":"3492563e-b4b8-448f-aa77-83ce064dfd92"},"source":"I will create four types of datasets using different options of preprocessing:\n\n1. Raw texts (no morphological normalization);\n2. Lemmatization;\n3. Stemming;\n4. Lemmatization + Stemming.\n\nIn the following examples I will experiment with only the raw text, but on the final stage of this work I will compare performance on different models on different techniques of morphological normalization."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"ac79a6f8cd6490fce8c6ce5065fef9ca7363a8d0","_cell_guid":"49d0baee-f950-423d-a9d1-cf2dcddf45dc"},"execution_count":null,"source":"train_lem = concat([train.drop(train.columns[2:], axis=1), DataFrame(parse_trainset(train)).transpose()], axis=1)\ntest_lem = concat([test.drop(test.columns[2:], axis=1), DataFrame(parse_trainset(test)).transpose()], axis=1)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"b2c2a8fd57c3038df5930369c8707c49212cb8fa","_cell_guid":"417b7c37-0610-4c07-8359-79a7fdf8e825"},"execution_count":null,"source":"train_stem = concat([train.drop(train.columns[2:], axis=1), DataFrame(parse_trainset(train, 'stem')).transpose()], axis=1)\ntest_stem = concat([test.drop(test.columns[2:], axis=1), DataFrame(parse_trainset(test, 'stem')).transpose()], axis=1)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"6b46a520e577f0678ab2acbf0b68b9b77b2acecf","_cell_guid":"533f14b9-f272-4d4a-a6ec-20b402c1830b"},"execution_count":null,"source":"train_lem_stem = concat([train.drop(train.columns[2:], axis=1), DataFrame(parse_trainset(train, 'lem+stem')).transpose()], axis=1)\ntest_lem_stem = concat([test.drop(test.columns[2:], axis=1), DataFrame(parse_trainset(test, 'lem+stem')).transpose()], axis=1)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"fcf15e0074e5b6f8fb62132eddc0eaca85ac98d6","_cell_guid":"7803827a-87d2-41ca-86ff-16e3193e8f21"},"execution_count":null,"source":"train_ = concat([train.drop(train.columns[2:], axis=1), DataFrame(parse_trainset(train, '_')).transpose()], axis=1)\ntest_ = concat([test.drop(test.columns[2:], axis=1), DataFrame(parse_trainset(test, '_')).transpose()], axis=1)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"95681543a06b69ba5a15f3b6c7684dde1a459912","_cell_guid":"d59e7f9f-2040-48f9-81c8-02482ccb973c"},"source":"For instance, the stemmed dataset looks like this. The difference is notable."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a9c294c05970fd90a3866b3411b2e955dfb59c31","_cell_guid":"4d8fa9f4-a4aa-4b93-bc0b-40c0332a6428"},"execution_count":null,"source":"train_stem.head()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"45b2f05984ad2b38c68241d36f516db0284c9781","_cell_guid":"6c5b6837-fa4d-4fee-8c38-f9f47c65df11"},"source":"All at all, I need to transform the text data to numerical features with which I could train the supervised classifier.  One of the most simple text features that one could obtain from the raw text is a length of it. Before we start engineering this feature, let's check if there any correlation of this feature with DJIA:"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"11c129e2356e67d8012f13f5abd85e0997d783fa","_cell_guid":"4e74bb69-f0fb-454d-b34f-45acb0b31902"},"execution_count":null,"source":"mean_list = []\nfor number in range(train_.shape[0]):\n    s = np.mean([len(f) for f in list(train_.loc[number][2:].values)])\n    mean_list.append((s, y_train[number]))\n\nfor number2 in range(number+1, number+1+test_.shape[0]):\n    s = np.mean([len(f) for f in list(test_.loc[number2][2:].values)])\n    mean_list.append((s, y_test[number2-number-1]))\nmean_list = sorted(mean_list)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"973b049aa91a96c7ca2510b297147727b8a27f1a","_cell_guid":"4e49b795-be9b-40bf-abc4-e3fc47eb3a68","scrolled":true},"execution_count":null,"source":"plt.figure(figsize=(20,6))\nplt.plot(range(len(mean_list))[:20], list(y_train)[:20])\nplt.legend()\nplt.xticks(range(20))\nplt.xlabel('Average text length', size=16)\nplt.show();","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c33e3f0ae9e678fbb8a8ac0ac65df6cff06a12e6","_cell_guid":"df5d55e8-4f80-485b-a3e0-50ac401b49be"},"source":"Well, correlation exists, so we could use this feature to build of predictive model. Great! What other features could we use? In my opinion, another example of a nice feature is an amount of mentioning of the countries' leaders. Let's check Barack Obama for instance: does his name correlates with DJIA?"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"cbb4234c7bacdac1492128bb713f913a9183180c","_cell_guid":"eeba5600-2af8-4b3c-9daa-7b3fb9b132be"},"execution_count":null,"source":"lists = ['obama', 'us', 'u.s.', 'u s', 'u.s', 'united', 'state', 'america']\n\nall_sum = []\nfor number in range(train_.shape[0]):\n    s = [i for f in list(train_.loc[number][2:].values) for i in f]\n    summ = 0\n    for i in lists:\n        summ+=Counter(s)[i]\n    all_sum.append((summ, y_train[number]))\n\nfor number2 in range(number+1, number+1+test_.shape[0]):\n    s = [i for f in list(test_.loc[number2][2:].values) for i in f]\n    summ = 0\n    for i in lists:\n        summ+=Counter(s)[i]\n    all_sum.append((summ, y_test[number2-number-1]))","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"d9fce25b4a59dd2962af3671fc1c3836260c9cf2","_cell_guid":"080b8636-5f64-44fb-b956-d5a50233d9c9"},"execution_count":null,"source":"a = sorted([(i[0], dict(Counter(all_sum))[i]) for i in dict(Counter(all_sum)) if i[1]==0])\na1= [i[1] for i in a]\nb = sorted([(i[0], dict(Counter(all_sum))[i]) for i in dict(Counter(all_sum)) if i[1]==1])\nb1= [i[1] for i in b]\nred = '#B2182B'\nblue = '#2166AC'\nwidth = 0.35\n\nplt.figure(figsize=(12,6))\nplt.bar(np.arange(12), a1, width, color=red, label='0 label')\nplt.bar(np.arange(12), b1, width, bottom=a1, color=blue, label='1 label')\nplt.legend()\nplt.xticks(range(12))\nplt.xlabel('Key word amount', size=16)\nplt.show()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9cfc40d44d6045f2b195d003c7b553f75819f9f5","_cell_guid":"aae3991d-6a2e-476b-8196-2327d15d95fa"},"source":"Mentionong Obama shows correlation with DJIA, so now we have another feature to use. But what about other countries? How many news are about them? I will compare two countries: Russia and unrecognised (yet) Islamic State of Iraq and al-Sham (ISIS).  Let's check how many news mention Russia and Putin, and how many ones mention ISIS:"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a79ef312836523e405acbc5d8be64ad184eaaf42","_cell_guid":"b0305392-43f7-46dd-8024-286ccde7d763"},"execution_count":null,"source":"news_topics_russia = ['russian', 'russia', 'putin']\nnews_topics_isis = ['isil', 'isis', 'levant', 'daesh']","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"4ead3463f13018027fe413a0c35373fa0578d4b6","_cell_guid":"f9e90d8c-4c59-4cdf-87d2-43843f662fd2"},"execution_count":null,"source":"news_russia = []\nnews_isis = []\n\nfor number in range(train_.shape[0]):\n    news_russia += [i for f in list(train_.loc[number][2:].values) for i in f if i.lower() in news_topics_russia]\n    news_isis += [i for f in list(train_.loc[number][2:].values) for i in f if i.lower() in news_topics_isis]\nfor number2 in range(number+1, number+1+test_.shape[0]):\n    news_russia += [i for f in list(test_.loc[number2][2:].values) for i in f if i.lower() in news_topics_russia]\n    news_isis += [i for f in list(train_.loc[number][2:].values) for i in f if i.lower() in news_topics_isis]\n\nnews_counter_russia = Counter(news_russia)\nnews_counter_isis = Counter(news_isis)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"e60f242d259437e71ae73ac6ca722b3e7c6ad5f6","_cell_guid":"41f8de41-f980-4bcc-8680-c3007bc1f364"},"execution_count":null,"source":"print('Russia is mentioned {} times, and ISIS is mentioned {} times'.format(sum(news_counter_russia.values()), sum(news_counter_isis.values())))\n\nrussia_bar = go.Bar(\n    x=list(news_counter_russia.keys()),\n    y=list(news_counter_russia.values()),\n    name='Russia',\n    marker=dict(\n        color='red'\n    )\n)\nisis_bar = go.Bar(\n    x=list(news_counter_isis.keys()),\n    y=list(news_counter_isis.values()),\n    name='ISIS',\n    marker=dict(\n        color='black'\n    )\n)\n\nlayout = go.Layout(\n    title='Amount of mentioning Russia and ISIS in the news',\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='black'\n        )\n    ),\n    yaxis=dict(\n        title='Amount of news containing this word',\n        titlefont=dict(\n            size=16,\n            color='black'\n        ),\n        tickfont=dict(\n            size=14,\n            color='black'\n        )\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n    ),\n    barmode='group',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig = go.Figure(data=[russia_bar, isis_bar], layout=layout)\npy.iplot(fig, filename='style-bar')","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"1905e206b6e7090124ba758e8f4218e317cc1c9f","_cell_guid":"b0a79335-027c-48c8-8906-d4268c989285"},"source":"Well, too few news mention ISIS, and a lot news mention Russia. So, probably, features based on finding mentions of ISIS would not be so helpful as the features based of finding mentions of Russia. Let's try to explore the last thing that probably could help us to understand what features should we use. I wonder, what world crisises are mentioned the most?"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"1b5b3f948a5441f70b51012dd389438dee60adec","_cell_guid":"7bb1692b-d7da-47ff-90f6-fe7867f6f264"},"execution_count":null,"source":"crisis_marker = 'crisis'","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a2ee0a2996e0cd4846fe768bdd56663f63c1a2f8","_cell_guid":"5dedb931-248f-452e-b3e0-4316baee17c5"},"execution_count":null,"source":"list_of_crisis = defaultdict(lambda: 0.0)\n\nfor ind in range(train_.shape[0]):\n    list_of_articles = list(train_.loc[ind])[2:]\n    for i in list_of_articles:\n        if crisis_marker in i:\n            try:\n                word = pos_tag([i[i.index(crisis_marker)-1]])[0]\n                if word[1]=='JJ':\n                    list_of_crisis[word[0]] += 1.0\n            except Exception as e:\n                pass\n\nfor ind2 in range(ind+1, test_.shape[0]+1):\n    list_of_articles = list(test_.loc[ind2])[2:]\n    for i in list_of_articles:\n        if crisis_marker in i:\n            try:\n                word = pos_tag([i[i.index(crisis_marker)-1]])[0]\n                if word[1]=='JJ':\n                    list_of_crisis[word[0]] += 1.0\n            except Exception as e:\n                pass","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"842c9cab33f4bdfc35a726cb066d6113eb9749f4","_cell_guid":"11200704-57b7-45ad-84cb-88b09c01b8a3"},"execution_count":null,"source":"wordcloud = WordCloud(background_color='white')\nwordcloud.generate_from_frequencies(dict(list_of_crisis))\nplt.figure()\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"0e78db9578f4949175b8dc262242cafb2001e21f","_cell_guid":"88049004-5bbb-43b4-bba6-38d036116edf"},"source":"Notably, despite the fact that common collocative words like \"global\" or \"humanitarian\" are used with \"crisis\" most often, the adjectives that characterize specific world military conflict are also observable. We could see the crisis in Georgia, the crisis in Syria and the crisis in Ukraine (as well as the slightly less pronounced crisis in Italy and the crisis in Europe, which were more the destabilization of the economy, rather than serious political crises). However, military conflicts, for example, in African countries, are not mentioned in the news.\n\nI also looked at the collocations of the \"crisis\" word with the help of distributional semantics, visualizing the nearest words to the word \"crisis\" by learning the Word2Vec model on the news texts. But, at first, I create a column with text data merged from all 25 columns."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"784c540adff7c49d76e16cdfd1da098a0f322405","_cell_guid":"203a6e1e-1d50-4106-b000-1046b88bb2ca"},"execution_count":null,"source":"def make_raw_text_col(df):\n    df['text'] = df['Top1'].str[1:] + ' '\n    for i in df.loc[:,'Top2':'Top25']:\n        df['text'] += df[i].str[1:] + ' '\n    df['text'] = df['text'].str.lower().str.replace('[^a-zA-Z ]', '')\n    return df","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9d5b35db03c62b754962a31b91dac10a7c6b7f96","_cell_guid":"df59351e-2a3d-449b-860e-37d8abd2f1da"},"execution_count":null,"source":"def make_raw_text_col_lem(df):\n    df['text'] = df['Top1']\n    for i in df.loc[:,'Top2':'Top25']:\n        df['text'] += df[i]\n    df['text'] = df['text'].str.lower().str.replace('[^a-zA-Z ]', '')\n    return df","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"5c9cb9382c00ac87ae329836c8b0c4194785f890","_cell_guid":"b4c6e123-9d05-4894-8d98-2d2efa922dfa"},"execution_count":null,"source":"train = make_raw_text_col(train)\ntest = make_raw_text_col(test)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a15b4ede0c07594d8f17f1d07ce76b29f075723a","_cell_guid":"879f0f83-3216-45f1-84cc-2433cce42910"},"execution_count":null,"source":"train_lem = make_raw_text_col_lem(train_lem)\ntest_lem = make_raw_text_col_lem(test_lem)\n\ntrain_stem = make_raw_text_col_lem(train_stem)\ntest_stem = make_raw_text_col_lem(test_stem)\n\ntrain_lem_stem = make_raw_text_col_lem(train_lem_stem)\ntest_lem_stem = make_raw_text_col_lem(test_lem_stem)","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"I will use a standard Skip-Gram model in order to finding distributional neighbords of the word \"crisis\" in a vector space."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"6d333dfd2fc0b96fa6870ad009bbbafdc7c8f3a1","_cell_guid":"aff9db6b-6ae2-48bb-9d61-6fe91afdfa94"},"execution_count":null,"source":"model = Word2Vec([word_tokenize(text) for text in np.hstack((train.text.values, test.text.values))], min_count=4, size=300, window=4, sg=1, alpha=1e-4)","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"The high-dimensional vectors would be projected in a two-dimensional space with the help of dimensionality reduction technique t-SNE:"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7c21183094eea4aa432d64b1d4fb83732399332c","_cell_guid":"cc12e505-013d-4836-a98d-ce5f82839928"},"execution_count":null,"source":"topn=10\n\nlabels = []\ntokens = []\n\nfor word in model.most_similar(crisis_marker, topn=topn):\n    tokens.append(model[word[0]])\n    labels.append(word[0])\n\n    \ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\nnew_values = tsne_model.fit_transform(tokens)\n\nx = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n\nplt.figure(figsize=(5, 5))\nsns.set_style('whitegrid')\nplt.grid(False)\nfor i in range(len(x)):\n    plt.scatter(x[i],y[i])\n    plt.annotate(labels[i],\n                fontsize=15,\n                color='black',\n                xy=(x[i], y[i]),\n                xytext=(2, 2),\n                textcoords='offset points',\n                ha='right',\n                va='bottom')","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"0903be2f8fd2ed48a3ad845e949084b8e842cd32","_cell_guid":"59b985a7-b2b8-4b79-ad49-1146fbeab35d"},"source":"Well, in word embeddings the word \"crisis\" has neighbours that have nothing to do with words mentioned in the previous diagram."},{"cell_type":"markdown","metadata":{"_uuid":"afe46d8f6ff5a131d19a9b8479e9f5f8c0990e9f","_cell_guid":"bb518b28-a5c4-4bf7-8c4f-fd39d56d2b60"},"source":"# 2. Feature Engineering\n\nIn this part I will use different ways of obtaining numerical features from the raw texts. I will train the gradient boosting classifier on these features and explore the role of different features in the final result. \n\n## 2.1 Linguistic Features\n\nThe most simple features that one could use to train the baseline classifier are derived directly from the text. Some of them I've mentioned above in the data exploration part of this notebook, some of them are listed below:\n\n* Amount of words in the news text;\n* Amount of unique words;\n* Amount of symbols;\n* Amount of stop-words (I'm using NLTK English stop-word list);\n* Average word length."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"cb5036b8a50f294caf30d13227a2a51f947ae8c2","_cell_guid":"5f871e29-7d88-4f30-a552-76c2cd3bb96b"},"execution_count":null,"source":"lambda_func_features = [\n    (lambda x: len(str(x).split()), 'NumWords'),\n    (lambda x: len(set(str(x).split())), 'NumUniqueWords'),\n    (lambda x: len(str(x)), 'NumChars'),\n    (lambda x: len([w for w in str(x).lower().split() if w in stop]), 'NumStopWords'),\n    (lambda x: np.mean([len(w) for w in str(x).split()]), 'MeanWordLen'),\n]","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"All at all, I obtain 6x25 features which I will put into a DataFrame."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"2ac133f7ed78c366a9e4ce7f9d812718f0296fb9","_cell_guid":"80c92c38-f154-4eae-a12a-eaab9bbd9184"},"execution_count":null,"source":"def generate_features(train, test, train_la, test_la, lambda_func, func_name):\n    train_features = DataFrame([train.loc[:,'Top' + str(col)].apply(lambda_func) for col in range(1, col_number)]).transpose()\n    test_features = DataFrame([test.loc[:,'Top' + str(col)].apply(lambda_func) for col in range(1, col_number)]).transpose()\n    train_features.columns = [func_name + str(i) for i in range(1, col_number)]\n    test_features.columns = [func_name + str(i) for i in range(1, col_number)]\n    return concat([train_la, train_features], axis=1), concat([test_la, test_features], axis=1)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a5c5ca1604270fff59ed7c24803c5d525783bf01","_cell_guid":"3d6179c6-a868-4717-bb09-35b3862acced"},"execution_count":null,"source":"def create_linguistic_features(train_data, test_data):\n    train_la = DataFrame()\n    test_la = DataFrame()\n    for lambda_func, func_name in lambda_func_features:\n        train_la, test_la = generate_features(train_data, test_data, train_la, test_la, lambda_func, func_name)\n    test_la = test_la.reset_index(drop=True)\n    return train_la, test_la","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"002697769e1a56d484b9bfda780b63fef4b5b5f1","_cell_guid":"49b24069-b437-4af9-82da-ea702012c360"},"execution_count":null,"source":"train_la, test_la = create_linguistic_features(train, test)","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"These feature dataframes look like this:"},{"cell_type":"code","metadata":{},"execution_count":null,"source":"train_la.head()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8d0f3ae8f8468caeea424b82a55a362021d687e8","_cell_guid":"3d92d199-08ba-4956-b03d-1395ba3c4851"},"source":"Let's set the hyperparameters of the gradient boosting classifier and train it:"},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"bcdb20340695b4eed90fbe5b70c8c7f58220ae83","_cell_guid":"8a2e10b6-3890-43b4-bf59-a0339607e502"},"execution_count":null,"source":"params = {}\nparams['objective'] = 'multi:softprob'\nparams['eta'] = 0.1\nparams['max_depth'] = 3\nparams['silent'] = 1\nparams['num_class'] = 3\nparams['eval_metric'] = 'mlogloss'\nparams['min_child_weight'] = 1\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.3\nparams['seed'] = 0","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"bb0b8c4da94231544095f453696fd19de13d4edb","_cell_guid":"64e8ef50-0896-4321-89e7-6789b1122cc0"},"execution_count":null,"source":"boost_rounds = 20\nkfolds = 5\npred_full_test = 0\n\ndef make_xgboost_predictions(train_df, test_df):\n    pred_full_test = 0\n    pred_train = np.zeros([train_df.shape[0], len(set(y_train))])\n    \n    for dev_index, val_index in KFold(n_splits=kfolds, shuffle=True, random_state=42).split(train_df):\n        dev_X, val_X = train_df.loc[dev_index], train_df.loc[val_index]\n        dev_y, val_y = y_train[dev_index], y_train[val_index]\n        xgtrain = xgb.DMatrix(dev_X, dev_y)\n        xgtest = xgb.DMatrix(test_df)\n        model = xgb.train(params=list(params.items()), dtrain=xgtrain, num_boost_round=boost_rounds)\n        predictions = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n        pred_full_test = pred_full_test + predictions\n    return pred_full_test / kfolds","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"59809a09176e6fecc0ffb6f8cbbad5c8f7effdeb","_cell_guid":"f2951ed6-2a93-4704-81ea-1fa1c9150375"},"execution_count":null,"source":"f1_score(make_xgboost_predictions(train_la, test_la).argmax(axis=1), y_test) ","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"a3bf2316ea8de3feec5796021986cce1d8ab4c05","_cell_guid":"c83bed30-f92b-43c8-8c6d-dfbb10d1146d"},"source":"Well, the F1-score is not bad. We obtained a pretty nice baseline performance just on simple linguistic features. Let's look at more complex ways of obtaining numerical data from text."},{"cell_type":"markdown","metadata":{"_uuid":"0fc88f04f37e39bc685cad255dcc5bfa8c240ec5","_cell_guid":"6458a362-99f7-442c-81e8-5194011ab752"},"source":"## 2.2 Probabilities of Predictions of Naive Bayes as Features\n\nNow I will try to use predictions of a classifier trained on co-occurrences of words and character n-grams (word-word, n-gram-n-gram, word-text and n-gram-text). Notably, I am not pushing the co-occurrence vectors themselves into the boosting model. I am pushing probabilities of class predictions of Naive Bayes classifier and train the gradient boosting on this data. Additionally, I am using a technique of Singular Value Decomposition to reduce the dimenstionality of the obtained sparse matrices."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f8c631e84a42f30f67cc67615c61ddd76ec975ec","_cell_guid":"30a1f4ef-3fc2-43f6-9768-7ef7b1acfcaf"},"execution_count":null,"source":"vectorizers = [\n    (CountVectorizer(stop_words='english', ngram_range=(1,5), analyzer='char'), 'CountVectorizerChar'),\n    (TfidfVectorizer(stop_words='english', ngram_range=(1,5), analyzer='char'), 'TfIdfVectorizerChar'),\n    (CountVectorizer(stop_words='english', ngram_range=(1,2), analyzer='word'), 'CountVectorizerWord'),\n    (TfidfVectorizer(stop_words='english', ngram_range=(1,2), analyzer='word'), 'TfIdfVectorizerWord')\n]","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"54d2b39aae9a7e193d82f927a712ef0a766b5012","_cell_guid":"ffd83bd2-89fd-4fd2-bd12-e32202a43b81"},"execution_count":null,"source":"models = [#(MultinomialNB(), {'alpha':[0, 0.1, 0.5, 0.8, 1]}, \"Naive Bayes\"),\n          (xgb.XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, \n                        colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n                        {'max_depth':range(3,10,9), 'min_child_weight':range(1,6,5)}, \"XGB\")\n         ]\n","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9d840300fd2752b6a34ff77c15330dfd50dfe1f5","_cell_guid":"8b9852d2-92b0-483e-b58d-26c27fc59fe1"},"execution_count":null,"source":"def do_grid_search(alg, array_of_vectors, array_of_tags, parameters):\n    clf = model_selection.GridSearchCV(alg, parameters, error_score=0.0)\n    clf.fit(array_of_vectors, array_of_tags)\n    print(clf.best_estimator_)\n    return clf.best_estimator_","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"As in the previous step, I will create a DataFrame that contains all necessary feature data without any source texts."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"546ff998b257eaaf6511b21227e34311dd769e44","_cell_guid":"ea0c9787-dee3-40cf-aaaa-2d2b32516b77"},"execution_count":null,"source":"def create_proba_features(train_data, test_data):\n\n    kfolds = 5\n    train_vecs = DataFrame()\n    test_vecs = DataFrame()\n\n    for vec, vec_name in vectorizers:\n        vectorizer = vec\n        full = vectorizer.fit_transform(np.hstack((train_data.text.values, test_data.text.values)))\n\n        X_train_raw = vectorizer.transform(train_data.text.values)\n        X_test_raw = vectorizer.transform(test_data.text.values)\n\n        normalized = Normalizer()\n        normalized.fit_transform(full)\n        X_train = normalized.transform(X_train_raw)\n        X_test = normalized.transform(X_test_raw)\n\n        pred_full_test = 0\n        pred_train = np.zeros([train_data.shape[0], len(train_data.Label.unique())])\n\n        for dev_index, val_index in KFold(n_splits=kfolds, shuffle=True, random_state=42).split(train_data):\n            dev_X, val_X = X_train[dev_index], X_train[val_index]\n            dev_y, val_y = y_train[dev_index], y_train[val_index]\n            model = MultinomialNB()\n            model.fit(dev_X, dev_y)\n            pred_full_test = pred_full_test + model.predict_proba(X_test)\n            pred_train[val_index,:] = model.predict_proba(val_X)\n\n        pred_full_test = pred_full_test / kfolds\n\n        train_vecs[vec_name + 'Zero'] = pred_train[:,0]\n        train_vecs[vec_name + 'One'] = pred_train[:,1]\n        test_vecs[vec_name + 'Zero'] = pred_full_test[:,0]\n        test_vecs[vec_name + 'One'] = pred_full_test[:,1]\n        \n        return train_vecs, test_vecs","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"270416d7ae4c45a147b22d828cd70657d38584f3","_cell_guid":"fdb97784-5014-4570-bfea-46c1bc5c1251"},"execution_count":null,"source":"train_vecs, test_vecs = create_proba_features(train, test)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"159fdc8cac6f9471925cc8c8c0024130e3e1987e","_cell_guid":"4f60b3b1-21d7-4020-b635-d8c6dda00b40"},"execution_count":null,"source":"f1_score(make_xgboost_predictions(train_vecs, test_vecs).argmax(axis=1), y_test) ","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"F1-score surprisingly is not so high as with more simple features. May be texts are too short, and it is not possible to construsct an adequate model of semantics through co-ocurrence matrices. Let's try some other techniques."},{"cell_type":"markdown","metadata":{"_uuid":"abc58d32e1919eb74ed2d9c06e33328177945f82","_cell_guid":"c46c9736-2e54-46fd-876d-7883420db371"},"source":"## 2.3 Decomposed TF-IDF Vectors\n\nI will give another chance to TF-IDF vectorizer by directly pushing vectors to the gradient boosting. The matrix is decomposed, so the amount of features is lower than in previous techniques."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"131d58e302953eb76e2a4ea0c883f03ba689a4fb","_cell_guid":"6af63f78-c243-47fb-92d8-cd3c8faa014a"},"execution_count":null,"source":"def create_svd_features(train_data, test_data):\n    svd_components = 20\n\n    vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer='word')\n    full = vectorizer.fit_transform(np.hstack((train_data.text.values, test_data.text.values)))\n    X_train = vectorizer.transform(train_data.text.values)\n    X_test = vectorizer.transform(test_data.text.values)\n\n    svd = TruncatedSVD(n_components=svd_components, algorithm='arpack')\n    svd.fit(full)\n    train_svd = DataFrame(svd.transform(X_train))\n    test_svd = DataFrame(svd.transform(X_test))\n\n    train_svd.columns = ['SVD' + str(i) for i in range(svd_components)]\n    test_svd.columns = ['SVD' + str(i) for i in range(svd_components)]\n    \n    return train_svd, test_svd","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"d006898e5f89824c6180afddb418b37685036188","_cell_guid":"fac745c5-3a95-45d7-95ac-5a2109f28329"},"execution_count":null,"source":"train_svd, test_svd = create_svd_features(train, test)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"cceee47c7ca1d8560362a3b285d3aa45f2a65577","_cell_guid":"bd7cdff8-66c4-4084-87cb-5a115162cb8a"},"execution_count":null,"source":"f1_score(make_xgboost_predictions(train_svd, test_svd).argmax(axis=1), y_test) ","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"However, F1-score is still not so high as I would like."},{"cell_type":"markdown","metadata":{"_uuid":"bb54e75f34b77629c96729943b05d0f973060bf4","_cell_guid":"7bf197a7-b1e6-40b8-b5bf-3d386c8761ec"},"source":"## 2.4 Decomposed Embedding Vectors\n\nPreviously, in the data exploration part, I've already used word embeddings, real-valued representations of words obtained by counting words' neighbors in a window sliding through the sentences of a corpus. Now I will try to replace TF-IDF vectorizer with a  Skip-Gram model that is a more state-of-the-art semantics model and possibly should give a better classification score."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"95820c10c4295c0d1a23fa002d1cca60c5c4ec4d","_cell_guid":"39cd44ff-d19e-4345-a527-4d79a489bda4"},"execution_count":null,"source":"def get_feature_vec(tokens, num_features, model):\n    featureVec = np.zeros(shape=(1, num_features), dtype='float32')\n    missed = 0\n    for word in tokens:\n        try:\n            featureVec = np.add(featureVec, model[word])\n        except KeyError:\n            missed += 1\n            pass\n    if len(tokens) - missed == 0:\n        return np.zeros(shape=(num_features), dtype='float32')\n    return np.divide(featureVec, len(tokens) - missed).squeeze()","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"5d6ed1a4870e75a05f40b8e8e3db5e5a47adf9f7","_cell_guid":"d2fd6f47-341e-44bf-8ad3-58a4dd4f7389"},"execution_count":null,"source":"def create_embedding_features(train_data, test_data):\n    num_features = 100\n\n    model = Word2Vec([word_tokenize(text) for text in np.hstack((train_data.text.values, test_data.text.values))], min_count=4, size=num_features, window=4, sg=0, alpha=1e-4)\n\n    train_embedding_vectors = []\n    for i in train_data.text.values:\n        train_embedding_vectors.append(get_feature_vec(word_tokenize(i), num_features, model))\n\n    test_embedding_vectors = []\n    for i in test_data.text.values:\n        test_embedding_vectors.append(get_feature_vec(word_tokenize(i), num_features, model))\n\n    train_w2v = DataFrame(train_embedding_vectors)\n    test_w2v = DataFrame(test_embedding_vectors)\n    \n    train_w2v.columns = ['W2V' + str(i) for i in range(num_features)]\n    test_w2v.columns = ['W2V' + str(i) for i in range(num_features)]\n    \n    return train_w2v, test_w2v","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"04c5b82adb049e594fb2dd7fdf04e219a5162e85","_cell_guid":"24e2b666-fc2a-4824-8256-1bd215931c5b"},"execution_count":null,"source":"train_w2v, test_w2v = create_embedding_features(train, test)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"b463318ebcb826ccf6c01616696d0de841243f86","_cell_guid":"dac6adc8-3b98-40e5-b6ac-c2c210de70be"},"execution_count":null,"source":"f1_score(make_xgboost_predictions(train_w2v, test_w2v).argmax(axis=1), y_test) ","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"The score is increased a little bit. Well, may be the vector space models are unable to predict dynamics of DJUA. "},{"cell_type":"markdown","metadata":{"_uuid":"82ccecd22774061445271e058e4fd8e5581ad0a6","_cell_guid":"cc218e96-db54-46f8-8b0c-ababb5b2c4e6"},"source":"## 2.5 Combining Features\n\nIn the end, I will combine all the feature matrices used before in order to obtain a better classification score."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"966e16f26799657afd9b37d7017b668b9e67a3d8","_cell_guid":"62e6a540-c789-4aa9-81a1-83a945c62962"},"execution_count":null,"source":"X_train = concat([train_la, train_svd, train_vecs, train_w2v], axis=1)\nX_test = concat([test_la, test_svd, test_vecs, test_w2v], axis=1)\nprint('Feature matrix consists of {} features'.format(len(X_train.columns.values)))","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a784ee52cec839ffa885f507ead9c42e0a457d43","_cell_guid":"cd970b70-515e-47dc-aba9-bede33c259e8"},"execution_count":null,"source":"print(f1_score(make_xgboost_predictions(X_train, X_test).argmax(axis=1), y_test))","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"0ad325ab53c47532764c2b7b81f63ed2b06f72fb","_cell_guid":"ff034ca4-24c6-4b37-bdf7-ccbeb4723d07"},"execution_count":null,"source":"topf=200\nfeatures = X_train.columns.values\nmodel_xgb = xgb.XGBClassifier()\nmodel_xgb.fit(X_train, y_train)\nx, y = (list(x) for x in zip(*sorted(zip(model_xgb.feature_importances_, features), reverse = False)[:topf]))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Feature importance for XGBoost',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of TOP-{} Features importances for XGBoost'.format(topf),\n    width = 1000, height = 1000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"I will also use all the kinds of morphologically normalized text data created in the start of this notebook. This little experiment will help us to unearth whether morphological preprocessing is able to predict dynamics of stock prices more correctly."},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"970b7898563a57be83849dc14244488e3bfb40f3","_cell_guid":"ec834d79-597f-4917-9b0c-0bec10328910"},"execution_count":null,"source":"preprocessed_data = [\n                    (train, test, 'raw'),\n                    (train_lem, test_lem, 'lem'),\n                    (train_stem, test_stem, 'stem'),\n                    (train_lem_stem, test_lem_stem, 'lem+stem')\n]","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f6d969d055995d08ea7d837ec489c5382155448f","_cell_guid":"492ed8b8-1104-4f42-aae3-c8f006d25f78"},"execution_count":null,"source":"for train, test, preprocess in preprocessed_data:\n    train_la, test_la = create_linguistic_features(train, test)\n    train_proba, test_proba = create_proba_features(train, test)\n    train_svd, test_svd = create_svd_features(train, test)\n    train_w2v, test_w2v = create_embedding_features(train, test)\n\n    X_train = concat([train_la, train_svd, train_proba, train_w2v], axis=1)\n    X_test = concat([test_la, test_svd, test_proba, test_w2v], axis=1)\n    \n    print('F1 with preprocessing by {} is {:0.4f}'.format(preprocess, f1_score(make_xgboost_predictions(X_train, X_test).argmax(axis=1), y_test)))\n    print('Accuracy with preprocessing by {} is {:0.4f}\\n'.format(preprocess, accuracy_score(make_xgboost_predictions(X_train, X_test).argmax(axis=1), y_test)))","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"However, combination of all of the features used by me in this notebook wasn't unable to outperform the score obtained with the simple linguistic features despite the diagram of feature importance says that there are some vector space features that were more important then linguistic features in this task. \n\nIt is also observable that morphological normalization didn't helped to increase performance of classification. It's not so surprising since lemmatization and stemming could really increase the performance only on languages with a highly fusional morphology, like Russian. May be morphological preprocessing brings only the noise to the data of English language.\n\nAll at all, in this little study we explored some interesting characteristics of our data and tried to used different kinds of features to predict stock market prices through the news texts. The predictions are weren't very successful, but the accuracy is higher that 0.5, so hypothetically to have profit trading on a stock market with a simple model trained on news texts. \n\nThanks for reading this kernel, and I am grateful to [Ekaterina Chernyak](https://github.com/echernyak) for the very nice machine learning course for which this study was done. Natural Language Processing is fun!"}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python","file_extension":".py","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1}