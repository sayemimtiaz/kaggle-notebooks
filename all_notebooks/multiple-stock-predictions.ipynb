{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stock Indicator Functions\n\nStreamlines computing with lots of data\n\nInitiallly coded and graphed [here](https://www.kaggle.com/colinflueck/apple-stock-indicators/notebook?scriptVersionId=46331290)\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# All indicators are typically done with close, but I'm doing open for everything\n\ndef SMA(df, dataColumn, window):\n    newColumn = str(window) + \"SMA\" + \"_\" + dataColumn\n    df[newColumn] = df[dataColumn].rolling(window=window).mean()\n    \ndef EMA(df, dataColumn, window):\n    newColumn = str(window) + \"EMA\" + \"_\" + dataColumn\n    # pandas ewm method for moving average (adjust set to false to equal stock website calculations)\n    df[newColumn] = df[dataColumn].ewm(span=window,min_periods=0,adjust=False,ignore_na=False).mean()\n\n# Moving Average Convergence Divergence (12, 26 are most common EMA windows)\ndef MACD(df, dataColumn, window1, window2, signal_window, isSignal):\n    EMA1 = df[dataColumn].ewm(span=window1,min_periods=0,adjust=False,ignore_na=False).mean()\n    EMA2 = df[dataColumn].ewm(span=window2,min_periods=0,adjust=False,ignore_na=False).mean()\n    MACD = EMA1 - EMA2\n    df[str(window1) + \"-\" + str(window2) + \"MACD\" + \"_\" + dataColumn] = MACD\n    if(isSignal):\n        df[\"MACD_\" + str(signal_window) + \"SMA\"] = MACD.ewm(span=signal_window,min_periods=0,adjust=False,ignore_na=False).mean()\n\n        \n# Stochastic Oscillator (14 is most common lookback window)\ndef SO(df, dataColumn, window, signal_window, isSignal):\n    # current open - lowest open / (highest open - lowest open) * 100\n    SO = ((df[dataColumn] - df[dataColumn].rolling(window=window).min()) /\n              (df[dataColumn].rolling(window=window).max() - df[dataColumn].rolling(window=window).min())) * 100\n    df[str(window) + \"SO\" + \"_\" + dataColumn] = SO\n    # Signal line (SMA of Stochstic Oscillator, 3 is most common)\n    if(isSignal):\n        df[\"SO_\" + str(signal_window) + \"SMA\"] = SO.rolling(window=signal_window).mean()\n        \n# Relative Strength Index (14 is most common lookback window)\ndef RSI(df, dataColumn, window, isEMA, isSMA):\n        \n    data = df[dataColumn]\n    delta = data.diff()\n    # Get rid of the first row, which is NaN since it did not have a previous row to calculate the differences\n    delta = delta[1:] \n\n    # Make the positive gains (up) and negative gains (down)\n    up, down = delta.copy(), delta.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n\n    if(isEMA):\n        # Calculate the EMAs\n        roll_up1 = up.ewm(span=window,min_periods=0,adjust=False,ignore_na=False).mean()\n        roll_down1 = down.abs().ewm(span=window,min_periods=0,adjust=False,ignore_na=False).mean()\n\n        # Calculate the RSI based on EMA\n        RS1 = roll_up1 / roll_down1\n        name1 = str(window) + \"RSI\" + \"_\" + \"EMA\" + \"_\" + dataColumn\n        df[name1] = 100.0 - (100.0 / (1.0 + RS1))\n\n    if(isSMA):\n        # Calculate the SMAs\n        roll_up2 = up.rolling(window=window).mean()\n        roll_down2 = down.abs().rolling(window=window).mean()\n\n        # Calculate the RSI based on SMA\n        RS2 = roll_up2 / roll_down2\n        name2 = str(window) + \"RSI\" + \"_\" + \"SMA\" + \"_\" + dataColumn\n        df[name2] = 100.0 - (100.0 / (1.0 + RS2))\n\n# Bollinger Bands\ndef BB(df, dataColumn, window):\n    SMA = df[dataColumn].rolling(window=window).mean()\n    std = df[dataColumn].rolling(window=window).std()\n    df[str(window) + \"upper_\" + dataColumn] = SMA + (2 * std)\n    df[str(window) + \"middle_\" + dataColumn] = SMA\n    df[str(window) + \"lower_\" + dataColumn] = SMA - (2 * std)\n    \n    \n# On Balance Volume (Try RSI approach to calc up/down)\n'''\ndef OBV(df, dataColumn, volColumn, window):\n    #df['OBV_' + dataColumn ]\n    df['OBV_' + dataColumn ] = 0\n    last_obv = df['OBV_' + dataColumn ].shift(1)\n    if df[dataColumn] > df[dataColumn].shift(1):\n        df['OBV_' + dataColumn ] = last_obv + df[volColumn]\n    elif df[dataColumn] < df[dataColumn].shift(1):\n        df['OBV_' + dataColumn ] = last_obv - df[volColumn]\n    else:\n        df['OBV_' + dataColumn ] = last_obv\n    df = df.append(data,ignore_index=True)\n    \n    \n    \n    for index, row in data.iterrows():\n        if index > 0:\n            last_obv = data.at[index - 1, 'obv']\n            if row[close_col] > data.at[index - 1, close_col]:\n                current_obv = last_obv + row[vol_col]\n            elif row[close_col] < data.at[index - 1, close_col]:\n                current_obv = last_obv - row[vol_col]\n            else:\n                current_obv = last_obv\n        else:\n            last_obv = 0\n            current_obv = row[vol_col]'''  \n        \n        #data.set_value(index, 'obv', current_obv)\n\n   # data['obv_ema' + str(trend_periods)] = data['obv'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean()\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Compiling Function\n\nStreamlines data compiling.  Args: list of tickers, returns a dataframe with all the data and one-hot encoded columns\n* Calls functions to calculate stock indicator\n* Could be used for any data (just change url and columns)\n\nWill update this to use sparse vectors for one-hot encodings once I figure it out.\nI will also test out additional indicators\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getStockData(tickers):\n    df = pd.DataFrame()\n    for ticker in tickers:\n        data = pd.read_csv(\"../input/nasdaq-and-nyse-stocks-histories/fh_20181104/full_history/\" + ticker + \".csv\")\n        data = data.sort_index(axis=0, ascending=False)\n        data = data.drop(['adjclose', 'high', 'low'], axis=1)\n        \n        data[\"date\"] = pd.to_datetime(data[\"date\"])\n        #data[\"date_day_of_week\"] = data[\"date\"].dt.dayofweek\n        #data[\"date_year\"] = data[\"date\"].dt.year\n        #data[\"date_day\"] = data[\"date\"].dt.day\n\n        # Cuts the first 10% of the historical data off (IPO throws model off, typically flatter, ...)\n        \n        # 200 more cells will be cut off because of SMA. (?? What is the optimal amount of data to cut off?)\n        data = data[int(len(data)*.1):]\n        \n        #Features and Label:\n        \n# --------- Stock Indicators ---------- #\n        \n        # Simple Moving Average (df, column for calculations, window)\n        SMA(data, 'open', 50)\n        SMA(data, 'open', 200)\n        \n        # Exponetial Moving Average\n        EMA(data, 'open', 12)\n        EMA(data, 'open', 26)\n        \n        # Moving Average Convergence Divergence (plus signal) (window1_EMA - window2_EMA) \n        MACD(data, 'open', 12, 26, 9, True)\n        \n        # Stochastic Oscillator (plus signal)\n        SO(data, 'open', 14, 3, True)\n        \n        # Relative Strength Index (EMA, SMA)\n        RSI(data, 'open', 14, True, True)\n        \n        # Bollinger Bands\n        BB(data, 'open', 20)\n        \n        # On Balance Volume\n        #OBV(data, 'open', 'volume', 21)\n        \n        # SMA(data, 'open', 5)\n        # SMA(data, 'open', 20)\n\n\n        # Option 1\n        data['close-1'] = data['close'].shift(1)  #Yesterday's close\n        data['log_label'] = np.where(data['close'] > (1.0 * data['close-1']), 1, 0) # is today going to close higher than yesterday?\n        \n        # Option 2 (model performs much worse)                 <| could try with close price for indicators |>\n        #close1 = data['close'].shift(-1)  #Tomorrow's close\n        #data['log_label'] = np.where(close1 > (1.0 * data['close']), 1, 0) # is tomorrow going to close higher than today?\n        \n        \n        #close_25 = data['close'].shift(-25)\n        \n        #close_25 = data['close'].shift(25)\n        #data['log_label'] = np.where(close_25 > data['close'], 1, 0)\n        \n        # multplying close-1 shifted accuracy to .6-.7, but only predicted 20 as 1s (9 correct, 11 incorrect) | AAPL, AMZN Data\n        #data['log_label'] = np.where(data['close'] > (1.0 * data['close-1']), 1, 0)\n        \n        # alternative label  (lookback=15 for APPL, AMZN, GOOG ~ 0.92 accuracy!)\n        #return1 = data[\"close\"].pct_change(1)\n        \n        # when return > .001, accuracy goes through the roof (.79) but its 26 correct 1, 25 incorrect 1\n        #data['log_label'] = np.where(return1 > (0.000000001), 1, 0)\n        #return2 = np.where(data['return'] < (-0.1), -1, 0)\n        #data['log_label'] = return1 + return2\n        \n        data[\"ticker\"] = ticker\n        df = df.append(data,ignore_index=True)\n    \n    # one-hot encoding, haven't figured out how to do sparse vectors yet\n    df2 = pd.get_dummies(df[\"ticker\"])\n    df = pd.concat([df, df2], axis=1, join='outer')\n    return df\n\n\n# Eventually could be used to auto-generate a list of tickers for the getStockData function\ndef getTickers():\n    #use dirnames from above\n    #/kaggle/input/nasdaq-and-nyse-stocks-histories/all_symbols.txt\n    return \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of tickers\n# Apple, Amazon, Google, Microsoft, Paypal (acting weird) = PYPL, Facebook, Dell = DMVT, Sony = SNE, IBM, HP = HPQ\n\n#ticker_list = [\"AAPL\"]\n#ticker_list = [\"AMZN\", \"GOOG\"]\n#ticker_list = [\"AAPL\", \"AMZN\", \"GOOG\"]\nticker_list = [\"AAPL\", \"AMZN\", \"GOOG\", \"MSFT\", \"FB\", 'SNE', 'IBM', 'HPQ']\n\n# Call function I defined above\ndf = getStockData(ticker_list)\n\n# drop NA from SMAs\ndf = df.dropna()\n\n# this ensures (relatively) even mix of companies in each data split\ndf = df.sort_values(by=['date'])\n\ndf.tail()\n#df[df['GOOG'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# ticker = 'PYPL'\n# df[ticker][df[ticker] == 1].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# instead of log label on higher close, calculate return\n# pandas pct_change(lookback_period), like close / close.shift(1), but cleaner and accurate\n\n#df[\"return\"] = df[\"close\"].pct_change(1)\n#df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Shuffles data in random order.  (this caused a drop of .1 and .2 on val and test scores respectively.  Perhaps date might be a valuable feature.)\n\n# I also realize that the data was always in order (hence the neglible benefit of including date values), but it might actually help\n\n# Also date introduces look ahead bias if its shuffled\n\n#shuffle after splitting data?\n\n# how to shuffle:\n#    df = df.sample(frac=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 11/11 Update\nI just found out my data split has been putting companies into each bin (mostly apple in train, mostly google in val...) because the data is ordered by date, with companies stacked.\n\n* Added in sort by date, to get even mix when splitting data.  Then adding other tech companies barely drops accuracy, not as much as before.\n* Added way to seperate predictions by company (should add function to show tables for each one depending on ticker list).\n* fixed accuracy score  (for some reason it worked, but had a different arg for the accuracy score than was passed in the function)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data at 70%, 20%, and 10%\nn = len(df)\n# try model with less data\n#aapl_df = aapl_df[int(n*0.25):]\nn = len(df)\n\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\n\n# shuffles the data within each category\ntrain_df = train_df.sample(frac=1)\nval_df = val_df.sample(frac=1)\ntest_df = test_df.sample(frac=1)\n\n#get labels for each data split\ntrain_y = train_df[\"log_label\"]\nval_y = val_df[\"log_label\"]\ntest_y = test_df[\"log_label\"]\n\n#get features for each data split, everything except for open and close price of the past 3 days\n\n#train_X = train_df.drop(columns=['close', 'open', 'log_label'])\n#val_X = val_df.drop(columns=['close', 'open', 'log_label'])\n#test_X = test_df.drop(columns=['close', 'open', 'log_label'], axis=1)\n\ntrain_date = train_df['date']\nval_date = val_df['date']\ntest_date = test_df['date']\n\n\ntrain_X = train_df.drop(['log_label', 'close', 'open', 'date', 'ticker', 'volume'], axis=1)\nval_X = val_df.drop(['log_label', 'close', 'open', 'date', 'ticker', 'volume'], axis=1)\ntest_X = test_df.drop(['log_label', 'close', 'open', 'date', 'ticker', 'volume'], axis=1)\n\ntrain_X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scales the data within each category\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#train_X = scaler.fit_transform(train_X)\n#val_X = scaler.fit_transform(val_X)\n#test_X = scaler.fit_transform(test_X)\n\n\n# Option 2 for scaling, performs worse but no chance of lookahead bias\n#train_X = train_X * .01\n#val_X = val_X * .01\n#test_X = test_X * .01\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\n#Intializes the model and fits it to the training data\nmodel = linear_model.LogisticRegression(random_state=0, max_iter=5000)\n#model = linear_model.LogisticRegression(random_state=0, penalty = 'l2', C=.01)\nmodel.fit(train_X, train_y)\ny_val_pred = model.predict(val_X)\ny_test_pred = model.predict(test_X)\n\nimport math\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\ndef eval_metrics(y_actual, y_predict, dataset):\n    print(\"\\nEvaluation metrics for \" + dataset + \":\\n\")\n    print(\"Accuracy score is: %.2f\" % accuracy_score(y_actual, y_predict))\n    print(\"Mean Squared Error: %.3f\" % mean_squared_error(y_actual, y_predict))\n    print(\"Root Mean Squared Error: %.3f\" % math.sqrt(mean_squared_error(y_actual, y_predict)))\n    print(\"-----------------------------------------\")\n    \neval_metrics(val_y, y_val_pred, \"Val data\")\neval_metrics(test_y, y_test_pred, \"Test data\")\n\n\n#graph = plt.scatter(val_X, y_val_pred, label=\"Validation Data\")\n#graph = plt.scatter(test_X, y_test_pred, label=\"Test Data\")\n\n#plt.legend()\n#plt.show()\n\n\npred_df = pd.DataFrame(val_y)\npred_df[\"pred\"] = y_val_pred\npred_df['date'] = val_df['date']\npred_df['close'] = val_df['close']\nfor ticker in ticker_list:\n    pred_df[ticker] = val_X[ticker]\n\npred_df = pred_df.sort_values(by=['date'])\npred_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n#from sklearn.metrics import ConfusionMatrixDisplay\n\ncm = confusion_matrix(test_y, y_test_pred)\n\n#cm_display = ConfusionMatrixDisplay(cm).plot()\ncm_display = pd.DataFrame(cm)\ncm_display\n#X axis is predicted\n#y axis is actual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prints prediction stats for company\ndef predByCompany(ticker_list):\n    from tabulate import tabulate # nicer looking tables when printed\n    for ticker in ticker_list:\n        df = pd.DataFrame(pred_df, columns=['log_label', 'pred', ticker])\n        print(tabulate(df[df[ticker] == 1].describe(), headers=['log_label', 'pred', ticker]))\n        print('\\n')\n        \n# ticker_list defined above\npredByCompany(ticker_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11/13 Plot of 4 companies close price and model predictions\n\nThis is a cool way to visually understand the results.  It shows a bit of the model's ficklness, and how as of right now, the predictions wouldn't translate nicely to buy/sell signals.  However, I've figured out how to seperate results by company and plot them, which will continue to be useful in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (25,7))\n\n# creates data frame with only APPl stocks, uses first 50 entries for graph\ntest_df = pred_df[pred_df['AAPL'] == 1]\ntest_df = test_df[0:50]\n\nplt.subplot(1, 2, 1)\n\n# plots green dots if prediction is 1, red dots if prediction is 0\nplt.scatter(test_df['date'][test_df['pred'] == 1], (test_df['close'][test_df['pred'] == 1] * test_df['pred'][test_df['pred'] == 1]), color='Green', label = '1')\nplt.scatter(test_df['date'][test_df['pred'] == 0], (test_df['close'][test_df['pred'] == 0] + test_df['pred'][test_df['pred'] == 0]), color='Red', label = '0')\n\n# plots stock price and sets title\nplt.plot(test_df['date'], test_df['close'], color='Blue', label = 'close price')\nplt.title(\"Apple Stock with model predictions\")\nplt.legend(loc='lower right')\n\n\nplt.subplot(1, 2, 2)\n\ntest_df = pred_df[pred_df['AMZN'] == 1]\ntest_df = test_df[0:50]\n\nplt.scatter(test_df['date'][test_df['pred'] == 1], (test_df['close'][test_df['pred'] == 1] * test_df['pred'][test_df['pred'] == 1]), color='Green', label = '1')\nplt.scatter(test_df['date'][test_df['pred'] == 0], (test_df['close'][test_df['pred'] == 0] + test_df['pred'][test_df['pred'] == 0]), color='Red', label = '0')\nplt.plot(test_df['date'], test_df['close'], color='blue', label = 'close price')\nplt.title(\"Amazon Stock with model predictions\")\nplt.legend(loc='lower right')\n\nplt.show()\nplt.figure(figsize = (25,7))\n\n\nplt.subplot(1, 2, 1)\n\ntest_df = pred_df[pred_df['GOOG'] == 1]\ntest_df = test_df[0:50]\n\nplt.scatter(test_df['date'][test_df['pred'] == 1], (test_df['close'][test_df['pred'] == 1] * test_df['pred'][test_df['pred'] == 1]), color='Green', label = '1')\nplt.scatter(test_df['date'][test_df['pred'] == 0], (test_df['close'][test_df['pred'] == 0] + test_df['pred'][test_df['pred'] == 0]), color='Red', label = '0')\nplt.plot(test_df['date'], test_df['close'], color='Blue', label = 'close price')\nplt.title(\"Google Stock with model predictions\")\nplt.legend(loc='lower right')\n\n\nplt.subplot(1, 2, 2)\n\ntest_df = pred_df[pred_df['MSFT'] == 1]\ntest_df = test_df[0:50]\n\nplt.scatter(test_df['date'][test_df['pred'] == 1], (test_df['close'][test_df['pred'] == 1] * test_df['pred'][test_df['pred'] == 1]), color='Green', label = '1')\nplt.scatter(test_df['date'][test_df['pred'] == 0], (test_df['close'][test_df['pred'] == 0] + test_df['pred'][test_df['pred'] == 0]), color='Red', label = '0')\nplt.plot(test_df['date'], test_df['close'], color='blue', label = 'close price')\nplt.title(\"Microsoft Stock with model predictions\")\nplt.legend(loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# this might help instead of pd.get_dummies()\n\n'''\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\narray = df[\"ticker\"].to_numpy()\narray = array.reshape(-1, 1)\n\nd = enc.fit_transform(array)\n\nimport scipy.sparse\ndf2 = pd.DataFrame.sparse.from_spmatrix(d)\ndf['ticker_1hot'] = df2.values.tolist()\ndf\n\n# [1,0] = APPL | [0,1] = AMZN\n\n# ---________________---\n\nfrom pandas.arrays import SparseArray\n\n#array = df[\"ticker_1hot\"].SparseArray\n#df[\"ticker_1hot\"] = SparseArray(df[\"ticker_1hot\"])\ndf\n\nd = scipy.sparse.csr_matrix(df.values)\nd'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}