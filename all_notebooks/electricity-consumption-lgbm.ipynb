{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.tabular import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/electrical-consumption/train_6BJx641.csv')\ntest =  pd.read_csv('/kaggle/input/electrical-consumption/test_pavJagI.csv')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb = pd.concat([train,test])\nprint(comb.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb['datetime'] = pd.to_datetime(comb['datetime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PRE-PROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pykalman import KalmanFilter\ndef Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state\n\n# Kalman Filter\nobservation_covariance = .0015\ncomb['temperature'] = Kalman1D(comb.temperature.values,observation_covariance)\ncomb['var1'] = Kalman1D(comb.var1.values,observation_covariance)\ncomb['pressure'] = Kalman1D(comb.pressure.values,observation_covariance)\ncomb['windspeed'] = Kalman1D(comb.windspeed.values,observation_covariance)\n#test['signal'] = Kalman1D(test.signal.values,observation_covariance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# samp = pd.DataFrame()\n# import random\n# samp['A'] = [random.randint(1,100) for i in range(0,25)]\n# samp['C'] = ['a' for i in range(0,5)] + ['b' for i in range(0,5)] + ['c' for i in range(0,15)]\n# samp['emw'] = samp.groupby(['C'])['A'].apply(lambda x : x.ewm(span=24,min_periods=1).mean())\n# samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def booleancon(x):\n    if x == True:\n        return 1\n    else:\n        return 0\n    \n\n\ndef daypart(x):\n    if x >= 0 and x<=4:\n        return 101\n    elif x>=5 and x<=8:\n        return 102\n    elif x>=9 and x<=12:\n        return 103\n    elif x>=13 and x<=16:\n        return 104\n    elif x>=17 and x<=19:\n        return 105\n    elif x>=20 and x<=23:\n        return 106\n    else:\n        return 0\n    \ndef seasons(x):\n    if x>=3 and x<5:\n        return 0\n    elif x>=6 and x<=8:\n        return 1\n    elif x>=9 and x<=11:\n        return 2\n    elif x>=12 and x<=2:\n        return 3\n    else:\n        return 0\n    \ndef var2(x):\n    if x=='A':\n        return 1\n    elif x=='B':\n        return 2\n    else:\n        return 3\n\ndef time_pr(train):\n    train = add_datepart(train,'datetime',drop=False,time=True)\n    #train.drop(['datetimeIs_month_end', 'datetimeIs_quarter_end','datetimeIs_year_start','datetimeIs_year_end'], axis=1, inplace=True)\n    train['datetimeIs_month_end'] = train['datetimeIs_month_end'].apply(booleancon)\n    train['datetimeIs_month_start']   = train['datetimeIs_month_start'].apply(booleancon)\n    train['datetimeIs_quarter_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_quarter_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['datetimeIs_year_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_year_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['var2'] = train['var2'].apply(var2)\n    train['daypart'] = train['datetimeHour'].apply(daypart)\n    train['season'] = train['datetimeMonth'].apply(seasons)\n    train['year_month'] = train['datetimeYear'].astype(str)+'_'+train['datetimeMonth'].astype(str)\n    train['MonthCat'] = 'M'+train['datetimeMonth'].astype(str)\n    train['HourCat'] = 'H'+train['datetimeHour'].astype(str)\n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        d = {}\n        d['mean'+c] = train.groupby(['year_month'])[c].mean()\n        d['median'+c] = train.groupby(['year_month'])[c].median()\n        d['max'+c] = train.groupby(['year_month'])[c].max()\n        d['min'+c] = train.groupby(['year_month'])[c].min()\n        d['std'+c] = train.groupby(['year_month'])[c].std()\n        d['mean_abs_chg'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            train[v] = train['year_month'].map(d[v].to_dict())\n        train['range'+c] = train['max'+c] - train['min'+c]\n        train['maxtomin'+c] = train['max'+c] / train['min'+c]\n        train['abs_avg'+c] = (train['abs_min'+c] + train['abs_max'+c]) / 2\n        \n    \n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        train['signal_shift_+1'+c] = train.groupby(['year_month'])[c].shift(1)\n        train['signal_shift_-1'+c] = train.groupby(['year_month'])[c].shift(-1)\n        train['signal_shift_+2'+c] = train.groupby(['year_month'])[c].shift(2)\n        train['signal_shift_-2'+c] = train.groupby(['year_month'])[c].shift(-2)\n        train['signal_shift_+3'+c] = train.groupby(['year_month'])[c].shift(3)\n        train['signal_shift_-3'+c] = train.groupby(['year_month'])[c].shift(-3)\n        train['signal_shift_+4'+c] = train.groupby(['year_month'])[c].shift(4)\n        train['signal_shift_-4'+c] = train.groupby(['year_month'])[c].shift(-4)\n        train['signal_shift_+5'+c] = train.groupby(['year_month'])[c].shift(5)\n        train['signal_shift_-5'+c] = train.groupby(['year_month'])[c].shift(-5)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 1000)  # or 1000\npd.set_option('display.max_rows', 1000)  # or 1000\npd.set_option('display.max_colwidth', 199)  # or 199\ncomb = time_pr(comb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_train = comb[comb['datetimeDay']<=16].fillna(method='bfill').fillna(method='ffill')\ndummy_test = comb[(comb['datetimeDay']>16) & (comb['datetimeDay']<=23)].fillna(method='bfill').fillna(method='ffill')\n\ncol = []\nfor i in comb.columns:\n    if i!= 'electricity_consumption' and i!='ID' and i!='datetime' and i!='year_month' and i!='MonthCat' and i!='HourCat':\n        col.append(i)\n\nactual_data = comb[comb['datetimeDay']<=23].fillna(method='bfill').fillna('ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef variance_inflation_factors(X,col):\n    vifs = {}\n    \n    for i in col:\n        cols = [z for z in col]\n        cols.remove(i)\n        sub_X = X[cols].values\n        sub_y = X[[i]].values\n        clf = LinearRegression()\n        sub_clf = clf.fit(sub_X, sub_y)\n        sub_y_pred = clf.predict(sub_X)\n        \n        sub_r2 = r2_score(sub_y, sub_y_pred)\n        \n        vif = 1 / (1 - sub_r2)\n        vifs[i] =vif\n        \n    return vifs\n\ncol_dict = variance_inflation_factors(actual_data,col)\n\nfor k,v in col_dict.items():\n    if v <=100:\n        col.append(k)\n    else:\n        print(k,' removed becouse vif values is ',v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = dummy_train[col].values\ny_train = dummy_train['electricity_consumption'].values\nx_test = dummy_test[col].values\ny_test = dummy_test['electricity_consumption'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nd_train = lgb.Dataset(x_train, label=y_train)\nd_test = lgb.Dataset(x_test, label=y_test)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 3000\nparams['learning_rate'] = 0.017\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = 8\nparams['num_threads'] = 15\nparams['max_bin'] = 400\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=1729\nparams['extra_trees'] = True\nparams['top_k'] = 23\nparams['path_smooth'] = 0.10\nclf = lgb.train(params, d_train, 2000,d_test,verbose_eval=200, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\np=XGBRegressor(n_estimators=30000,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# max_depth=5,0.018\np.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='rmse',early_stopping_rounds=500,verbose=200)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\ncb_model = CatBoostRegressor(n_estimators = 1000,\n    loss_function = 'RMSE',\n    eval_metric = 'RMSE',random_state=1729)\ncb_model.fit(x_train, y_train, use_best_model=True, eval_set=(x_test, y_test), early_stopping_rounds=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = actual_data[col].values\nY = actual_data['electricity_consumption'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nd_train = lgb.Dataset(X, label=Y)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 3000\nparams['learning_rate'] = 0.017\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = 8\nparams['num_threads'] = 15\nparams['max_bin'] = 400\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=1729\nparams['extra_trees'] = True\nparams['top_k'] = 23\nparams['path_smooth'] = 0.10\n\nclf = lgb.train(params, d_train, 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p=XGBRegressor(n_estimators=1216,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# # max_depth=5,0.018\n# p.fit(X,Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = comb[comb['datetimeDay']>23]\nx_test = test[col].values\npred = clf.predict(x_test)\ntest['electricity_consumption'] =[round(i) for i in pred]\ntest[['ID','electricity_consumption']].to_csv('result.csv',header=True,index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(clf,importance_type='split', max_num_features=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}