{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n\n\nimport csv\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torchvision.transforms as transforms\nimport time\n\nimport torch_xla\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T12:28:08.507136Z","iopub.execute_input":"2021-06-30T12:28:08.50777Z","iopub.status.idle":"2021-06-30T12:29:21.186002Z","shell.execute_reply.started":"2021-06-30T12:28:08.507639Z","shell.execute_reply":"2021-06-30T12:29:21.185159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello, I made this notebook to learn how to use TPUs.\nI am just a beginner, so if you notice mistakes or things that I can do better and simpler, please tell me.","metadata":{}},{"cell_type":"markdown","source":"Preparing data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv/train.csv\", dtype = np.float32)\nprint(train.shape)\n\n# splitting data into features and labels\nlabels_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n\n# splitting data in train data and validation data 80% - 20%\nfeatures_train, features_valid, labels_train, labels_valid = train_test_split(features_numpy, labels_numpy, test_size = 0.2, random_state = 1)\n\n# converting to tensor\nfeatures_train_tensor = torch.from_numpy(features_train)\nlabels_train_tensor = torch.from_numpy(labels_train).type(torch.LongTensor)\n\nfeatures_valid_tensor = torch.from_numpy(features_valid)\nlabels_valid_tensor = torch.from_numpy(labels_valid).type(torch.LongTensor)\n\n# # data loader\n# train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = False)\n# test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:29:26.123539Z","iopub.execute_input":"2021-06-30T12:29:26.123911Z","iopub.status.idle":"2021-06-30T12:29:30.784027Z","shell.execute_reply.started":"2021-06-30T12:29:26.123877Z","shell.execute_reply":"2021-06-30T12:29:30.783137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the model\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        return out\n    \ninput_size = 28*28 # the size of an image\nhidden_size = 100 # number of neurons in the hidden layer\noutput_size = 10 # number of label classes, from 0 to 9\n\nmodel = NeuralNet(input_size, hidden_size, output_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:29:38.45809Z","iopub.execute_input":"2021-06-30T12:29:38.458463Z","iopub.status.idle":"2021-06-30T12:29:38.467018Z","shell.execute_reply.started":"2021-06-30T12:29:38.458432Z","shell.execute_reply":"2021-06-30T12:29:38.465813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _run(model):\n    \n    def train_model(train_dataloader, device, optimizer, criterion):\n\n        model.train()\n\n        for i, (images, labels) in enumerate(train_dataloader): # enumerate gives us our actual index\n            # 100, 1, 28, 28 the input we have to resize\n            # 100, 784\n            images = images.reshape(-1, 28*28)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n#             xm.master_print(f'loss = {loss.item():4f}')\n\n            # backward pass\n            optimizer.zero_grad() # clear the gradients from the previous iteration\n            loss.backward() # calculates gradients\n            xm.optimizer_step(optimizer) # updates parameters\n\n\n    def valid_model(valid_dataloader, device):\n        \n        # calculate accuracy\n        correct = 0\n        total = 0\n        # Predict test dataset\n        \n        model.eval()\n        \n        for images, labels in valid_dataloader:\n            images = images.reshape(-1, 28 * 28)\n            labels = labels\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n\n            # total number of labels\n            total += len(labels)\n            # total number of correct predictions\n            correct += (predicted == labels).sum()\n\n        accuracy = 100 * correct / float(total)\n        xm.master_print(f'Accuracy: {accuracy}')\n        \n    # batch size and epoch\n    batch_size = 64\n    num_epochs = 20\n    \n    train_dataset = torch.utils.data.TensorDataset(features_train_tensor, labels_train_tensor)\n    valid_dataset = torch.utils.data.TensorDataset(features_valid_tensor, labels_valid_tensor)\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=32, sampler=valid_sampler, num_workers=1)\n    \n    device = xm.xla_device()\n    model = model.to(device)\n    \n    # loss and optimizer\n    learning_rate = 1e-3 * xm.xrt_world_size()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    \n    train_begin = time.time()\n    for epoch in range(num_epochs):\n        \n        para_loader = pl.ParallelLoader(train_dataloader, [device])\n        \n        start = time.time()\n        xm.master_print('*'*15)\n        xm.master_print(f'EPOCH: {epoch+1}')\n        xm.master_print('*'*15)\n\n        xm.master_print('Training.....')\n        \n        train_model(train_dataloader=para_loader.per_device_loader(device),\n                   device=device,\n                   optimizer=optimizer,\n                   criterion=criterion)\n        xm.master_print(f'Epoch completed in {(time.time() - start)/60} minutes')\n        \n    with torch.no_grad():\n        para_loader = pl.ParallelLoader(valid_dataloader, [device])\n            \n        xm.master_print('Validating...')\n        valid_model(valid_dataloader=para_loader.per_device_loader(device), device=device)\n            \n    xm.master_print(f'Training completed in {(time.time() - train_begin)/60} minutes') ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:41:56.609089Z","iopub.execute_input":"2021-06-30T12:41:56.609693Z","iopub.status.idle":"2021-06-30T12:41:56.6275Z","shell.execute_reply.started":"2021-06-30T12:41:56.609627Z","shell.execute_reply":"2021-06-30T12:41:56.626572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run(model)\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:41:59.772132Z","iopub.execute_input":"2021-06-30T12:41:59.772711Z","iopub.status.idle":"2021-06-30T12:42:42.882114Z","shell.execute_reply.started":"2021-06-30T12:41:59.772659Z","shell.execute_reply":"2021-06-30T12:42:42.88055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n1. https://www.kaggle.com/abhiswain/pytorch-tpu-efficientnet-b5-tutorial-reference","metadata":{}}]}