{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduciton\n\nThis notebook seeks to be a landing page for people looking to start working with the information made available in the UNCOVER dataset. It begins with an analysis of the basic data located in each of the folders examining the columns that exist.\n\n**Please up vote to let me know this is useful or comment on other ways to make this a valuable resource**"},{"metadata":{},"cell_type":"markdown","source":"## Data Meta-Analysis: Exploring the data, columns, and content\n### General Stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Libraries Needed\nimport pandas as pd\nimport os\nfrom glob import glob\nfrom tqdm.notebook import tqdm\n\n#Settings to display pandas\npd.set_option('display.max_columns', None)\n\n#Some basic set up\nbase_path = \"/kaggle/input/uncover\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Traverse paths and pull out all .csv files\n#Explaination at \n#https://perials.com/getting-csv-files-directory-subdirectories-using-python/\nall_csv_files = []\nfor path, subdir, files in os.walk(base_path):\n    for file in glob(os.path.join(path, \"*.csv\")):\n        all_csv_files.append(file)\n\"There are a total of {} csv files in this dataset.\".format(len(all_csv_files))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uncover_df_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Try to make a pandas dataframe from each file\n#The dataframes will be accessible by their file name using the uncover_df_dictionary\nread_files = []\nskipped_files = []\nuncover_df_dictionary = {}\nfor file_path in tqdm(all_csv_files):\n    df_name = file_path.split(\"/\")[-1].replace('.csv','')\n    try:\n        uncover_df_dictionary[df_name] = pd.read_csv(file_path, low_memory=False)\n        read_files.append(file_path)\n    except:\n        skipped_files.append(file_path)\n        pass\n\"Read a total of {} files into Pandas dataframes and skipped {}.\".format(len(read_files), len(skipped_files))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common Columns\nThese could be used for merging dataframes. Columns are also called features. These can be used to build models or describe the data. This secton epxlores the columns that exist and how they relate to other dataframes. It also does some examination of how the categorical data maybe turned into numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iterate though dict and group similar\ncolumn_dict = {}\nfor name, df in uncover_df_dictionary.items():\n    all_cols = list(df.columns)\n    for col in all_cols:\n        if col in column_dict.keys():\n            column_dict[col].append(name)\n        else:\n            column_dict[col] = list([name])\n\n#Drop any columns not found in other DataFrames\nlen_before_drop = len(column_dict)\nto_pop = []\nfor col, df_list in column_dict.items():\n    if len(df_list) < 2:\n        to_pop.append(col)\n        \n#Run in seperate loop as can not change size in iterator\nfor col in to_pop:\n    column_dict.pop(col)\n    \nprint(\"A total of {} columns are unique to one dataframe.\".format((len_before_drop-len(column_dict))))\nprint(\"A total of {} columns are shared by more than one dataframe.\".format(len(column_dict)))\n\n#Make DF with index of cols and a column of dfs with that feature\ncol_df = pd.DataFrame(pd.Series(column_dict)).reset_index()\ncol_df.columns = [\"Feature\", \"DataFrames\"]\ncol_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use pivot table to turn into binary matrix with columns as files and rows as available features. A 1 will represent it is present in the data and a 0 that it is not."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explode-Make a new row for each of the values found in the DataFrames lists\ncol_df_explode = col_df.explode(\"DataFrames\")\n#Add present columns to keep track of which are where\ncol_df_explode[\"present\"] = 1\n#Pivot to binary matrix\ncol_binary_matrix = col_df_explode.pivot_table(index='Feature',\n                    columns='DataFrames',\n                    values='present',\n                    aggfunc='sum',\n                    fill_value=0)\ncol_binary_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns most commonly found"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_binary_matrix.sum(axis=1).sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DataFrames that share the most features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get all possible file combinaitons to compare\nfrom itertools import permutations\nall_pairs = permutations(col_binary_matrix.columns,2)\npairs_df_list = []\nfor df1, df2 in all_pairs:\n    boolean_check = (col_binary_matrix[df1]==1) & (col_binary_matrix[df2]==1)\n    shared_feats = list(col_binary_matrix.index[boolean_check])\n    num_shared_feats = len(shared_feats)\n    features_dict = {\"df1\":df1, \"df2\": df2,\"sim_col_count\": num_shared_feats,\"sim_col_list\": shared_feats}\n    if num_shared_feats > 1:\n        pairs_df_list.append(features_dict)\nshared_cols_dfs = pd.DataFrame(pairs_df_list).sort_values(\"sim_col_count\", ascending=False)\nshared_cols_dfs.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary of Datasets Available\nThis section will provide a description of each of the datasets in the UNCOVER data set as well as their features."},{"metadata":{},"cell_type":"markdown","source":"## Coders Against Covid\nCrowdsourced map of testing locations across the US."},{"metadata":{},"cell_type":"markdown","source":"## County Health Rankings\n\nThe annual Rankings provide a revealing snapshot of how health is influenced by where we live, learn, work, and play. They provide a starting point for change in communities."},{"metadata":{},"cell_type":"markdown","source":"## COVID 19 Canada Open Data Working Group\nA working group collecting publicly available information on confirmed and presumptive positive cases during the ongoing COVID-19 outbreak in Canada. Data are entered in a spreadsheet with each line representing a unique case, including age, sex, health region location, and history of travel where available. Sources are included as a reference for each entry. All data are exclusively collected from publicly available sources including government reports and news media."},{"metadata":{},"cell_type":"markdown","source":"# COVID Tracker Canada\nAn independent project that compiles daily reports of covid-19 cases as reported by Canadian news outlets.\n\nCopyright © COVID19Tracker.ca 2020 // COVID19Tracker.ca reports both presumptive and confirmed cases in near real-time // contact@covid19tracker.ca"},{"metadata":{},"cell_type":"markdown","source":"# COVID Tracking Project\nThe COVID Tracking Project collects and publishes the most complete testing data available for US states and territories"},{"metadata":{},"cell_type":"markdown","source":"# ECDC\nThe European Centre for Disease Prevention and Control collects the number of COVID-19 cases and deaths, based on reports from health authorities worldwide. This comprehensive and systematic process is carried out on a daily basis."},{"metadata":{},"cell_type":"markdown","source":"# Geotab\nGeotab is a telematics company specializing in GPS fleet management and vehicle tracking. This data set provides hourly average duration of border crossings between Canada and the US"},{"metadata":{},"cell_type":"markdown","source":"# Github\nThese data sets provide case counts for several countries, and are updated and maintained by citizens from government sources."},{"metadata":{},"cell_type":"markdown","source":"# Harvard Global Health Institute\nThe Harvard Global Health institute created a model of hospital capacity and readiness across the US. This model builds on bed capacity data for each of 306 U.S. hospital markets (Hospital Referral Regions, HRR) with localized estimates of available beds, and beds needed to accommodate COVID-19 patients over the coming months. It highlights where hospitals might find additional bed and ICU bed capacity as well as other shortages that need to be addressed—from workforce to ventilators.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# HDE\nThe Humanitarian Data Exchange (HDX) is an open platform for sharing data across crises and organisations. Provided are data sets with global information on testing, government responses, and school closures."},{"metadata":{},"cell_type":"markdown","source":"# HIFLD\nHomeland Infrastructure Foundation-Level Data (HIFLD) provides National foundation-level geospatial data within the open public domain that can be useful to support community preparedness, resiliency, research, and more."},{"metadata":{},"cell_type":"markdown","source":"# IHME\nData describes the forecasting carried out by the IHME on the COVID-19 impact on hospital bed-days, ICU-days, ventilator days and deaths by US state in the next 4 months"},{"metadata":{},"cell_type":"markdown","source":"# Johns Hopkins\nThese are the data powering the 2019 Novel Coronavirus Visual Dashboard operated by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). The data sources include the World Health Organization, the U.S. Centers for Disease Control and Prevention, the European Center for Disease Prevention and Control, the National Health Commission of the People’s Republic of China, local media reports, local health departments, and the DXY, one of the world’s largest online communities for physicians, health care professionals, pharmacies and facilities."},{"metadata":{},"cell_type":"markdown","source":"# New York Times\nThe New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak."},{"metadata":{},"cell_type":"markdown","source":"# Next Strain\nThis data set describes the phylogeny of the hCoV-19 / SARS-CoV-2 genomes being sequenced globally. Site numbering and genome structure uses Wuhan-Hu-1/2019 as reference. The phylogeny is rooted relative to early samples from Wuhan. Temporal resolution assumes a nucleotide substitution rate of 8 × 10^-4 subs per site per year."},{"metadata":{},"cell_type":"markdown","source":"# Open Table\nOpenTable is publishing changes in restaurant reservations across several regions in 2020 as a year-over-year percentage change compared to 2019"},{"metadata":{},"cell_type":"markdown","source":"# Our World in Data\nOur World In Data aims to aggregate existing research, bring together the relevant data and allow their readers to make sense of the published data and early research on the coronavirus outbreak. They have provided data on coronavirus cases and testing."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}