{"cells":[{"metadata":{},"cell_type":"markdown","source":"## USA cars\nThis is an entry-level analysis for practice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import the necessary libraries and the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom bs4 import BeautifulSoup as soup\nfrom urllib.request import urlopen as uReq\nimport plotly.graph_objects as go\nimport wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The dataset before cleaning\ncars = pd.read_csv('/kaggle/input/usa-cers-dataset/USA_cars_datasets.csv')\nprint(cars.shape)\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars['country'] = cars['country'].str.strip()\n\n# For title_status there is only 2 option, and salvage insurance takes only 6,5% of the database\n# so I won't work with it, because it could distort the results.\ncars = cars[cars['title_status'] == 'clean vehicle']\n\n# There is a total of 7 data from the 2499 where the country is canada. I will work only with usa.\ncars = cars[cars['country'] == 'usa']\n\n# The Unnamed: 0 column is only another index column, and I won't use the lot, because\n# the vin code will be my identifier, so I drop them too.\ncars.drop(columns = ['Unnamed: 0', 'lot','condition','title_status', 'country'], inplace = True)\nprint(cars.shape)\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation in our numeric data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\n\ncar_corr = cars.corr()\nsns.heatmap(car_corr, annot = True, annot_kws={'size':50}, center = 0, cmap = 'magma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a brief insight with heatmap, we should do some more computation for our data. On the next plot, every subplot gets a touple in the form of (correlation coefficient, p-value). The correlation coefficient shows the strength and the direction of the correlation, and the p-value shows whether it is significant. Let the null hypothesis be there is no linear correlation and the significance level 0.05 ."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize = (24, 6))\n\nplt.suptitle('Correlation in our numeric data',fontsize = 18, y = 1.05)\n\nsns.regplot(x = 'price', y = 'year', data = cars, marker = '.', ci= False,\n            line_kws={'color': '#0A333A'}, scatter_kws={'color':'#7FBCC6'}, ax = axes[0])\ncr1 = scipy.stats.pearsonr(cars['price'], cars['year'])\naxes[0].set_title(cr1, pad = 20)\n\nsns.regplot(x = 'price', y = 'mileage', data = cars, marker = '.', ci= False,\n            line_kws={'color': '#0A333A'}, scatter_kws={'color':'#7FBCC6'}, ax = axes[1])\ncr2 = scipy.stats.pearsonr(cars['price'], cars['mileage'])\naxes[1].set_title(cr2, pad = 20)\n\nsns.regplot(x = 'year', y = 'mileage', data = cars, marker = '.', ci= False,\n            line_kws={'color': '#0A333A'}, scatter_kws={'color':'#7FBCC6'}, ax = axes[2])\ncr3 = scipy.stats.pearsonr(cars['year'], cars['mileage'])\naxes[2].set_title(cr3, pad = 20)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Year vs Price: The greater the price, the newer a car. The correlation is relatively weak.\n\nMileage vs Price: The greater the price, the lower the mileage. The correlation is relatively weak.\n\nMileage vs Year: The newer the car, the lower the mileage. The correlation is moderately strong."},{"metadata":{},"cell_type":"markdown","source":"### Subdivision by brand"},{"metadata":{},"cell_type":"markdown","source":"#### Most expensive brand\nFirstly, let's see how expensive a brand, if we use the average prices. Before the task, we should clean our dataset froum outliers. I will keep a brand only, if it has more than 10 items. In this case I can work with more than 98% of the data, and I have to work only with 48% of the brands. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame(cars.groupby(['brand']).count()['vin'])\ntemp.sort_values('vin', ascending = False, inplace = True)\n# temp[temp['vin'] > 10].sum().values / temp.sum().values == 0.98239588\n# temp[temp['vin'] > 10].count().values / temp.count().values == 0.48148148\nbrand_list = temp[temp['vin'] > 10].index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"av_prices = []\nfor i in brand_list:\n    x = cars[cars['brand']==i]\n    av_price = sum(x.price)/len(x)\n    av_prices.append(av_price)\ndata = pd.DataFrame({'brand_list': brand_list,'av_prices':av_prices})\nnew_index = (data['av_prices'].sort_values(ascending=False)).index.values\nsorted_data = data.reindex(new_index)\n\nsns.barplot(y=sorted_data['brand_list'], x=sorted_data['av_prices'], palette = 'GnBu_d')\nplt.xlabel('Average Price ($)', fontsize = 14)\nplt.ylabel('Brand', fontsize = 14)\nplt.title('Average price per brand', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most frequent brand\nLet's work some more with our filtered data."},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = []\nfor i in brand_list:\n    x = cars[cars['brand']==i]\n    count = len(x.vin)\n    counts.append(count)\ndata2 = pd.DataFrame({'brand_list': brand_list,'counts':counts})\nnew_index2 = (data2['counts'].sort_values(ascending=False)).index.values\nsorted_data2 = data2.reindex(new_index2)\n\nsns.barplot(y=sorted_data2['brand_list'], x=sorted_data2['counts'], palette = 'GnBu_d')\nplt.xlabel('# of brands', fontsize = 14)\nplt.ylabel('Brand', fontsize = 14)\nplt.title('Number of brands', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we van see, 90% of the cars belongs to the top 4 brand. We could examine that, is there any connection between the price, and the frequency?"},{"metadata":{},"cell_type":"markdown","source":"#### Price and frequency\nLet the null hypothesis be there is no linear correlation and the significance level 0.05"},{"metadata":{"trusted":true},"cell_type":"code","source":"pf = sorted_data.merge(sorted_data2, on = 'brand_list')\nsns.lmplot(x = 'av_prices', y = 'counts', data = pf,\n          line_kws={'color': '#0A333A'}, scatter_kws={'color':'#7FBCC6'})\nplt.show()\nscipy.stats.pearsonr(pf['av_prices'], pf['counts'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation coefficient is 0.34, which could mean a weak positive correlation, but the p-value, which is 0.255 is greater than our significance level, so we can't refuse the null hypothesis."},{"metadata":{},"cell_type":"markdown","source":"#### Model in brand\nThe next figure shows the brand and price further broken down by the model attribute. After an eyeball test, it seems the model can be a significant variable. It could be interesting to repeat the tests so far with the model instead of brand. Also it could be interesting to test with both attributes applied, but there would be a lot of rare data, so I won't do it now."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cars['count'] = 1\nbrand_list = temp[temp['vin'] > 20].index.values\ncars_b = cars[np.in1d(cars['brand'],brand_list)]\nc_sun = px.sunburst(cars_b, path = ['brand','model'], values = 'count', color = 'price', \n            width = 750, height = 750, color_continuous_scale = 'Teal')\ncars.drop(columns = 'count', inplace = True)\nc_sun.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cars by state\nFirstly, we should add a new column to our cars table, which contains the state codes."},{"metadata":{"trusted":true},"cell_type":"code","source":"page_url = 'https://www.infoplease.com/us/postal-information/state-abbreviations-and-state-postal-codes'\nuClient = uReq(page_url)\npage_soup = soup(uClient.read(), \"html.parser\")\nuClient.close()\n\ncontainers = page_soup.tbody.findAll('tr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_filename = 'state_code.csv'\nheaders = 'state,code\\n'\n\nf = open(out_filename, \"w\")\nf.write(headers)\n\nfor container in containers:\n    cont = container.findAll('td')\n    \n    state = cont[0].text\n    code = cont[2].text\n\n    f.write(state.strip().lower() + ',' + code + '\\n')\n    \nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = pd.read_csv('state_code.csv')\ncars = cars.merge(sc, on = 'state')\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carsc = pd.DataFrame(cars.groupby(['code']).count()['vin'])\n\nfig = go.Figure(data=go.Choropleth(locations=carsc.index, z = carsc['vin'],\n                                   locationmode = 'USA-states', colorscale = 'Teal',\n                                   colorbar_title = '# of cars'))\n\nfig.update_layout(title_text = 'Cars advertised by state', geo_scope='usa')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most cars are in Pennsylvania, Florida and Texas. We worked a lot with prices so far, so we could make a map about the average prices by states. On that map I will keep only those states, where there are at least 10 cars to filter out the outlier values."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"states = carsc[carsc['vin'] >= 10].index.values\n\ncarsp = cars[np.in1d(cars['code'],states)]\ncarsp = pd.DataFrame(carsp.groupby(['code']).mean()['price'])\n\n\nfig = go.Figure(data=go.Choropleth(locations=carsp.index, z = carsp['price'],\n                                   locationmode = 'USA-states', colorscale = 'Teal',\n                                   colorbar_title = 'USD'))\n\nfig.update_layout(title_text = 'Average orice of the cars by states', geo_scope='usa')\n\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}