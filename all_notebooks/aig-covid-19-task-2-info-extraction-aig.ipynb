{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Installing and testing sentence embedding extraction modules. Testing For GPU and setting up device. \n### Using Pytorch based Bio-Bert download via biobert-embedding\n\n### **Testing below package to generate embedding** \n[Biobert Reference](https://github.com/Overfitter/biobert_embedding)\n**This package main code is modified to run it on GPU**\n\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n\n#### Reserch paper for the search is selected based on Clustering Via Citation and other networks \n\n[Links to be added](http://)\n\n#### *This will select best sentence via Network X graph. Currently using Pagerank method* Other methods can also me used like degree_centrality betweenness_centrality eigenvector_centrality"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport subprocess\nimport pickle\nimport numpy as np\nimport io\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pprint\nimport matplotlib.pyplot as plt\nimport pickle as pkl\n!pip install biobert-embedding\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\ndef preprocess_sentence(text):\n    text = text.replace('/', ' / ')\n    text = text.replace('.-', ' .- ')\n    text = text.replace('.', ' . ')\n    text = text.replace('\\'', ' \\' ')\n    text = text.lower()\n\n    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n\n    return ' '.join(tokens)\nfrom nltk import tokenize\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import package and change code to run on GPU\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from biobert_embedding.embedding import BiobertEmbedding\nimport os\nimport torch\nimport logging\nimport tensorflow as tf\nfrom pathlib import Path\nfrom biobert_embedding import downloader\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\nlogging.basicConfig(filename='app.log', filemode='w',format='%(asctime)s %(message)s', level=logging.INFO)\n\nlogger = logging.getLogger(__name__)\nclass BiobertEmbedding(object):\n    \"\"\"\n    Encoding from BioBERT model (BERT finetuned on PubMed articles).\n    Parameters\n    ----------\n    model : str, default Biobert.\n            pre-trained BERT model\n    \"\"\"\n\n    def __init__(self, model_path=None):\n\n        if model_path is not None:\n            self.model_path = model_path\n        else:\n            self.model_path = downloader.get_BioBert(\"google drive\")\n\n        self.tokens = \"\"\n        self.sentence_tokens = \"\"\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n        # Load pre-trained model (weights)\n        self.model = BertModel.from_pretrained(self.model_path)\n        self.model.to(device)\n        logger.info(\"Initialization Done !!\")\n\n    def process_text(self, text):\n\n        marked_text = \"[CLS] \" + text + \" [SEP]\"\n        # Tokenize our sentence with the BERT tokenizer.\n        tokenized_text = self.tokenizer.tokenize(marked_text)\n        return tokenized_text\n\n\n    def handle_oov(self, tokenized_text, word_embeddings):\n        embeddings = []\n        tokens = []\n        oov_len = 1\n        for token,word_embedding in zip(tokenized_text, word_embeddings):\n            if token.startswith('##'):\n                token = token[2:]\n                tokens[-1] += token\n                oov_len += 1\n                embeddings[-1] += word_embedding\n            else:\n                if oov_len > 1:\n                    embeddings[-1] /= oov_len\n                tokens.append(token)\n                embeddings.append(word_embedding)\n        return tokens,embeddings\n\n\n    def eval_fwdprop_biobert(self, tokenized_text):\n\n        # Mark each of the tokens as belonging to sentence \"1\".\n        segments_ids = [1] * len(tokenized_text)\n        # Map the token strings to their vocabulary indeces.\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n\n        # Convert inputs to PyTorch tensors\n        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n        segments_tensors = torch.tensor([segments_ids]).to(device)\n\n        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n        self.model.eval()\n        # Predict hidden states features for each layer\n        with torch.no_grad():\n            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n\n        return encoded_layers\n\n\n    def word_vector(self, text, handle_oov=True, filter_extra_tokens=True):\n\n        tokenized_text = self.process_text(text)\n\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # Concatenate the tensors for all layers. We use `stack` here to\n        # create a new dimension in the tensor.\n        token_embeddings = torch.stack(encoded_layers, dim=0)\n        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n        # Swap dimensions 0 and 1.\n        token_embeddings = token_embeddings.permute(1,0,2)\n\n        # Stores the token vectors, with shape [22 x 768]\n        word_embeddings = []\n        logger.info(\"Summing last 4 layers for each token\")\n        # For each token in the sentence...\n        for token in token_embeddings:\n\n            # `token` is a [12 x 768] tensor\n            # Sum the vectors from the last four layers.\n            sum_vec = torch.sum(token[-4:], dim=0)\n\n            # Use `sum_vec` to represent `token`.\n            word_embeddings.append(sum_vec)\n\n        self.tokens = tokenized_text\n        if filter_extra_tokens:\n            # filter_spec_tokens: filter [CLS], [SEP] tokens.\n            word_embeddings = word_embeddings[1:-1]\n            self.tokens = tokenized_text[1:-1]\n\n        if handle_oov:\n            self.tokens, word_embeddings = self.handle_oov(self.tokens,word_embeddings)\n        logger.info(self.tokens)\n        logger.info(\"Shape of Word Embeddings = %s\",str(len(word_embeddings)))\n        return word_embeddings\n\n\n\n    def sentence_vector(self,text):\n\n        logger.info(\"Taking last layer embedding of each word.\")\n        logger.info(\"Mean of all words for sentence embedding.\")\n        tokenized_text = self.process_text(text)\n        self.sentence_tokens = tokenized_text\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # `encoded_layers` has shape [12 x 1 x 22 x 768]\n        # `token_vecs` is a tensor with shape [22 x 768]\n        token_vecs = encoded_layers[11][0]\n\n        # Calculate the average of all 22 token vectors.\n        sentence_embedding = torch.mean(token_vecs, dim=0)\n        logger.info(\"Shape of Sentence Embeddings = %s\",str(len(sentence_embedding)))\n        return sentence_embedding\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"device.type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install -U sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download BioBert"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = downloader.get_BioBert(\"google drive\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing BiobertEmbedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.\"\\\n\n# Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)\nbiobert = BiobertEmbedding(model_path)\n\nword_embeddings = biobert.word_vector(text)\nsentence_embedding = biobert.sentence_vector(text)\n\nprint(\"Text Tokens: \", biobert.tokens)\n# Text Tokens:  ['breast', 'cancers', 'with', 'her2', 'amplification', 'have', 'a', 'higher', 'risk', 'of', 'cns', 'metastasis', 'and', 'poorer', 'prognosis', '.']\n\nprint ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n# Shape of Word Embeddings: 16 x 768\n\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n# Shape of Sentence Embedding =  768","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing SentenceTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use BERT for mapping tokens to embeddings\nfrom sentence_transformers import models\nfrom sentence_transformers import SentenceTransformer\nword_embedding_model = models.BERT('/kaggle/working/'+model_path.name)\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=True,\n                               pooling_mode_max_tokens=True)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nsentence_embeddings = model.encode([text])\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read papers Selected for Task2"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_excel(\"/kaggle/input/task3covid/task2_results_summary.xlsx\",index=False).dropna()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df_title_abstract=data\nlen1=meta_df_title_abstract.shape[0]\nlist1=list(range(len1))\nmeta_df_title_abstract['pid']=list1\nmeta_df_title_abstract.head()\nmeta_df_title_abstract['summary_preprocessed']=meta_df_title_abstract['Text'].apply(lambda x:tokenize.sent_tokenize(x))\nnew_data_sent=meta_df_title_abstract['summary_preprocessed'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\nnew_data_sent=new_data_sent.merge(meta_df_title_abstract[['cord_uid', 'lsid', 'gsid', 'Name', 'Text', 'Subtype', 'summary', 'pid']],right_index=True,left_index=True,how='left')\nnew_data_sent['wrd_cnt']=new_data_sent['value'].str.split().str.len()\nnew_data_sent_strip=new_data_sent[new_data_sent['wrd_cnt']>6]\nprint(\"wrd cnt > 6 \" + str(new_data_sent.shape))\nnew_data_sent_strip=new_data_sent_strip[new_data_sent_strip['wrd_cnt']<100]\nprint(\"wrd cnt < 200 \" + str(new_data_sent_strip.shape))\nnew_data_sent_strip['value_edit']=new_data_sent_strip['value'].apply(lambda x:preprocess_sentence(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# embedding function for Biobert on CPU and GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_embedding(wr):\n    #try :\n    return biobert.sentence_vector(wr).cpu()\n\ndef get_sent_bert_embedding(wr):\n    #try :\n    return model.encode([wr],show_progress_bar=False)\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\ndef process_on_set(files, num_workers,function):\n    def process_file(i):\n        filename_2 = files[i]\n\n        y_pred = function(filename_2)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(files)))\n\n    return list(predictions)\n\n#xt=new_data_sent_strip['value_edit'].head().apply(lambda x:chk_len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Speed"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working using sentence transformer for speed and polling selection"},{"metadata":{},"cell_type":"markdown","source":"## Generate embedding for each line\n#### Automatic use parallel processing if GPU is unavailable"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].values,4,get_bert_embedding)\nnew_data_sent_strip['Embedding']=xt\nimport pickle\nwith open('/kaggle/working/embeddings37912.pickle', 'wb') as handle:\n    pickle.dump(new_data_sent_strip, handle)\nnew_data_sent_strip.to_csv(\"new_data_sent_strip.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Code for Search and page rank"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataset(df,query,query_id=0,val=0.9):\n    #print(\"Query is \")\n    #print(query_text[query_id])\n    query_embedding_sent_df['match']=df['Embedding'].apply(lambda x:cosine_similarity(x[0].reshape(1,x[0].shape[0]),query[query_id].reshape(1,query[query_id].shape[0]))[0][0])\n    shape_val=0\n    while shape_val<100:\n        query_embedding_sent_df_subset=query_embedding_sent_df[query_embedding_sent_df['match']>val]\n        val=val-0.02\n        shape_val=query_embedding_sent_df_subset.shape[0]\n    original_sentences=query_embedding_sent_df_subset.value.values\n    sentence_vectors=query_embedding_sent_df_subset.Embedding.values\n    cord_uid=query_embedding_sent_df_subset.cord_uid.values\n    #print('Total Sentence := ' + str (sentence_vectors.shape[0]))\n    sentence_vectors_all=[sentence_vectors[k][0] for k in range(sentence_vectors.shape[0])]\n    sim_matrix = cosine_similarity(np.array(sentence_vectors_all))\n    #print(np.mean(sim_matrix))\n    sim_matrix_thresh = np.where(sim_matrix > np.mean(sim_matrix), sim_matrix, 0)\n    return sim_matrix_thresh,original_sentences,cord_uid\n\n\ndef print_diffmethod(function,name,original_sentences,cord_uid):\n#     print(str(name))\n    page_rank_result = pd.DataFrame({'sentence_index':list(function.keys()), 'score':list(function.values()), \n                                    'original_sentence':original_sentences,'cord_uid':cord_uid})\n    page_rank_result['key']=page_rank_result.original_sentence.str.replace(\" \",\"\")\n    page_rank_result=page_rank_result.drop_duplicates(['key'])\n#     page_rank_result.nlargest(10, 'score')\n#     for s in page_rank_result.nlargest(7, 'score')['original_sentence']:\n#         pprint.pprint(s)\n#         pprint.pprint('-------------------------------------------------------------------------------------')\n#     pprint.pprint(\"-\"*40)\n    return page_rank_result[['score','original_sentence','cord_uid']]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create embedding for sub task"},{"metadata":{"trusted":true},"cell_type":"code","source":"query_text=['What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?',\n'Smoking, pre-existing pulmonary disease',\n'Neonates and pregnant women',\n'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n'Public health mitigation measures that could be effective for control',\n'Susceptibility of populations',\n'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n'Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities']\nimport pickle\nquery_embedding=model.encode(query_text,show_progress_bar=False)\nwith open('/kaggle/working/query_embedding_sent.pickle', 'wb') as handle:\n    pickle.dump(query_embedding, handle)\nquery_embedding[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# query=pd.read_pickle('/kaggle/input/covid19-task3/query_embedding_sent.pickle')\n# query_embedding_sent_df=pd.read_pickle('/kaggle/input/covid19-task3/embeddings37912.pickle')\n# Comp_reserch_data=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nquery=query_embedding\nquery_embedding_sent_df=new_data_sent_strip.copy()\nComp_reserch_data=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Search\n** Find best sentence for query**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, HTML\n# print_diffmethod(nx.degree_centrality(G),'degree_centrality')\n# print_diffmethod(nx.betweenness_centrality(G),'betweenness_centrality')\n# print_diffmethod(nx.eigenvector_centrality(G),'eigenvector_centrality')\n\nfor query_id in range(len(query_text)):\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query,query_id=query_id,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    display(HTML('<font size=\"5\" color=\"blue\"> <b> Query Searched : </b> </font><p> <font size=\"4\">'+query_text[query_id]+'</font><p>'))\n\n    #display(HTML(all_search.to_html()))\n    display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'skyblue'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n    display(HTML(\"-------End-----\"*15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subtast queries result"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_val={'What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?':[\t\t'What is known about virus risk factors of the virus',\n\t\t'What is known about Covid-19 risk factors of the virus',\n\t\t'What is known about epidemiological studies of the virus'],\n          'Smoking, pre-existing pulmonary disease':[\t\t'What is known about smoking risk factor  of the virus',\n\t\t'What is known about potential smoking risk  of the virus',\n\t\t'What is known about pulmonary disease  risk of the virus'],\n          'Neonates and pregnant women':[\t\t'What is known about Pregnancy risk  of the virus',\n\t\t'What is known about neonates risk  of the virus'],\n          'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups':\n          [\t\t'What is known about high risk patients  of the virus'],\n          'Public health mitigation measures that could be effective for control':[\t\t'What is known about helath migration for control  of the virus']\n          ,'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups':[\t\t'What is known about risk of fatality  of the virus',\n\t\t'What is known about symptomatic hospitalized patients risk  of the virus',\n\t\t'What is known about hospitalized patients risk  of the virus',\n\t\t'What is known about severity of disease  of the virus']\n          ,'Susceptibility of populations':[\t\t'What is known about Suceptibilty by population of the virus',\n\t\t'What is known about risk for population of the virus']\n          ,'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors':[\t\t'What is known about transmission dynamics  of the virus',\n\t\t'What is known about reproductive number virus of the virus',\n\t\t'What is known about incubation period virus  of the virus',\n\t\t'What is known about serial interval virus  of the virus'],\n          'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.':[\t\t'What is known about economic impact of virus of the virus',\n\t\t'What is known about difference in economy of the virus',\n\t\t'What is known about scoio economic risk virus of the virus',\n\t\t'What is known about behavioural factor economy virus of the virus']\n          ,'Public health mitigation measures that could be effective for control':[\t\t'What is known about public health risk  of the virus',\n\t\t'What is known about public health mitigation of the virus',\n\t\t'What is known about public health mitigation measures of the virus'],\n          'Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities':[\t\t'What is known about Co-infection risk  of the virus',\n\t\t'What is known about pre infection risk  of the virus',\n\t\t'What is known about other respiratory disease  of the virus'],\n          'Public health mitigation measures that could be effective for control':[\t\t'What is known about health mitigation measures of the virus'],\n          'Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities':[\t\t'What is known about co - morbidities of the virus',\n\t\t'What is known about disease make more transmissible of the virus',\n\t\t'What is known about other infection makes virus more virulent of the virus',\n\t\t'What is known about other infection makes virus more transmissible of the virus',]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_val.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, HTML\n# print_diffmethod(nx.degree_centrality(G),'degree_centrality')\n# print_diffmethod(nx.betweenness_centrality(G),'betweenness_centrality')\n# print_diffmethod(nx.eigenvector_centrality(G),'eigenvector_centrality')\n\nfor query_id_list in dict_val.keys():\n    display(HTML('<font size=\"6\" color=\"black\"> <b> Subtask Searched : </b> </font><p> <font size=\"4\">'+query_id_list+'</font><p>'))\n    query_i_embed=model.encode([query_id_list],show_progress_bar=False)\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    \n    display(HTML('<font size=\"3\" color=\"green\"> <b> Subtask search Results</b> </font><p> <font size=\"4\">'+'</font><p>'))\n\n    #display(HTML(all_search.to_html()))\n    display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'skyblue'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n    display(HTML(\"-------End-----\"*15))\n    for query_i_text in dict_val[query_id_list]:\n        \n        query_i_embed=model.encode([query_i_text],show_progress_bar=False)\n        \n        sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n        G = nx.Graph(sim_matrix_thresh)\n        nx.pagerank(G)\n        #nx.draw_networkx(G)\n        t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n        all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n        display(HTML('<font size=\"6\" color=\"black\"> <b> Subtask Searched : </b> </font><p> <font size=\"3\">'+query_id_list+'</font><p>'))\n        display(HTML('<font size=\"4\" color=\"blue\"> <b> Specific question subtask Query Searched : </b> </font><p> <font size=\"4\">'+query_i_text+'</font><p>'))\n\n        #display(HTML(all_search.to_html()))\n        display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'pink'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n        display(HTML(\"-------End-----\"*15))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}