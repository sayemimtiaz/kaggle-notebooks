{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using Regression to predict housing prices in King County (R2 adj. 84%)"},{"metadata":{},"cell_type":"markdown","source":"# Content"},{"metadata":{"trusted":true},"cell_type":"code","source":"## to do","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Hi all, welcome to this kernel. Until now, it includes everything I wanted to include, except interactions between variables. <br>\nComments are very welcome, I would love to hear your ideas about improvents !<br>\n<br>\nThe goal of this kernel is to find a regression model that is able to predict housing prices for houses in King County, USA. Of course there are other types of models which can do that, but since I am not an expert, I choose regresion. <br>\nRegression can help us to forecast a response using a set of predictors. We can forecast the house price for a house given the squared feet, location, and other variables. <br>\nIn this kernel we will use all the provided variables, transform them if neccesary, create a linear regression model, check the performance of this model, and check if the regression assuptions hold."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{},"cell_type":"markdown","source":"First import all libraries we need, and the dataset with house data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import datasets, linear_model, metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# import data\n\npath = \"/kaggle/input/housesalesprediction/kc_house_data.csv\"\n\ndf_house = pd.read_csv(path)\ndf_house","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore data Part 1"},{"metadata":{},"cell_type":"markdown","source":"Now we have all data we need, let's check what variables we have, what they look like, and what data types they are."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display(df_house.describe())\ndisplay(df_house.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Missing vlaues</b><br>\nIn the .info() results we can see that there are no NaN values. <br>\nIn the .describe() results we can see if there are other values than NaN used as missing values, e.g. 0, -1, -99, blanks, etc.<br>\nIt looks like there are no missing values, so no need to transform missing values.<br>\n<br>\n<b>Qualitative variables</b><br>\nSince regression models only take quantitative variables, we need to check which qualitative variables we have. In this dataset, this is only the date column. Since we won't use the date variable in the regression model anyway, there is no need to transform this variable."},{"metadata":{},"cell_type":"markdown","source":"# Create New Variables"},{"metadata":{},"cell_type":"markdown","source":"<b>Change variables that make no sense</b><br>\nThe sqft_basement variable indicates how big the basement is. I believe that the actual size of the basement does not add much more to the price of a house, than simply having a basement or not. <br>\nSo instead, create a new variable 'has_basement' that indicates wether a house has a basement (1) or not (0)."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_house['has_basement'] = np.where(df_house['sqft_basement'] > 0, 1, 0)\n\ndf_house.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Distance to city center</b><br>\nIn many cities, housing prices are higher in the city center, and lower outside the city. <br>\nSo let's calculate the distance of each house to the city center of Seattle, based on longitude and latitude, and put the results in a new variable 'distance _to_city'."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from math import radians, sin, cos, asin, sqrt\n\ndef haversine(lon1, lat1, lon2, lat2):\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    return 2 * 6371 * asin(sqrt(a))\n\nlatitude_city = 47.610515\nlongitude_city = -122.33465413\n\ndf_house['distance_to_city'] = df_house.apply(lambda row: haversine(latitude_city, longitude_city, row['lat'], row['long']), axis=1)\n\ndf_house","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Good/bad neighborhoods</b><br>\nIt often happens that a similar house will cost more in a 'good' neighborhood and less in a 'bad' neighborhood. <br>\nIt is difficult to define a good or bad neighborhood with the available data. Instead, we assume that the median house price is higher in good neighborhoods and lower in bad neighborhoods. <br>\nFirst, calculate the top 30 zipcodes by median house price, and same for the bottom 30 zipcodes. <br>\nThen create a new variable 'is_top_x_zipcode' which contains 1 for houses which zipcode is in the top 30, and if not, 0. <br>\nAlso create a new variable 'is_bottomx_zipcode' which contains 1 for houses which zipcode is in the bottom 30, and if not, 0.<br>\nThe number 30 is choosen by trial and error."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if prices are higher or lower er certain zipcodes\n\nf, axe = plt.subplots(1, 1,figsize=(25,5))\nsns.boxplot(x=df_house['zipcode'],y=df_house['price'], ax=axe)\nsns.despine(left=True, bottom=True)\naxe.yaxis.tick_left()\naxe.set(xlabel='Zipcode', ylabel='Price')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# create new variables for top x and bottom x zipcodes by price\n# trial and error to get a better x\n\nmed_price_zip = df_house.groupby(['zipcode']).agg({'price': 'median', 'id': \"count\"}).sort_values('price', ascending = False)\n\nzipcode_topx = np.array([med_price_zip[c].nlargest(30).index.values for c in med_price_zip])[0]\nzipcode_bottomx = np.array([med_price_zip[c].nsmallest(30).index.values for c in med_price_zip])[0]\n\nprint(zipcode_topx)\nprint(zipcode_bottomx)\n\ndf_house[\"is_topx_zipcode\"] = [1 if x in list(zipcode_topx) else 0 for x in df_house[\"zipcode\"]]\ndf_house[\"is_bottomx_zipcode\"] = [1 if x in list(zipcode_bottomx) else 0 for x in df_house[\"zipcode\"]]\n\ndf_house","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore data Part 2"},{"metadata":{},"cell_type":"markdown","source":"Now we have our new variables, let's do some more data exploration:\n- Univariate: histograms for all variables\n- Univariate: distribution and normal proability plot for all variables\n- Bivariate: joinplots for all x variables in relation to price\n- Bivariate: correlation heatmap to vizualize all correlations between all variables"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# histograms for all variables\n# copied from: https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices\n\nh = df_house.drop(['id', 'date'], axis = 1).hist(bins = 25, figsize = (16,16), xlabelsize = '10', ylabelsize = '10', xrot = -15)\nsns.despine(left = True, bottom = True)\n[x.title.set_size(12) for x in h.ravel()];\n[x.yaxis.tick_left() for x in h.ravel()];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# graph distribution\n\nfor col in list(df_house.drop(['id', 'date'], axis = 1).columns):    \n    plt.figure(figsize=(12,4))\n    plt.subplot(1,2,1)\n    sns.distplot(df_house[col].dropna(), fit=stats.norm);\n    plt.subplot(1,2,2)\n    _=stats.probplot(df_house[col].dropna(), plot=plt)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# joinplots for all x variables in relation to x, to visualize the bivariate distribution\n\nfor col in list(df_house.drop(['id', 'date','price'], axis = 1).columns):\n    sns.jointplot(x = col, y = \"price\", data = df_house, kind = 'reg', size = 5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# create correlation matrix\ncorr_matrix = df_house.corr()\n\n\n# set up mask to hide upper triangle\n\nmask = np.zeros_like(corr_matrix, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# create seaborn heatmap\n\nf, ax = plt.subplots(figsize = (16, 10)) \n\nheatmap = sns.heatmap(corr_matrix, \n                      mask = mask,\n                      #square = True, # Makes each cell square-shaped\n                      linewidths = .5, # set width of the lines that will divide each cell to .5\n                      cmap = \"coolwarm\", # map data values to the coolwarm color space\n                      cbar_kws = {'shrink': .4, # shrink the legend size and label tick marks at [-1, -.5, 0, 0.5, 1]\n                                \"ticks\" : [-1, -.5, 0, 0.5, 1]},\n                      vmin = -1, # Set min value for color bar\n                      vmax = 1, # Set max value for color bar\n                      annot = True, # Turn on annotations for the correlation values\n                      fmt='.2f', # String formatting code to use when adding annotations\n                      annot_kws = {\"size\": 12}) # Set annotations to size 12\n\n# add title\nplt.title('House Sales King County - Correlation Heatmap', \n              fontsize=14, \n              fontweight='bold')\n\n# add the column names as labels\nax.set_xticklabels(corr_matrix.columns, rotation = 90) # Add column names to the x labels and rotate text to 90 degrees\nax.set_yticklabels(corr_matrix.columns, rotation = 0) # Add column names to the y labels and rotate text to 0 degrees\n\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True}) # Show tickmarks on bottom and left of heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform Variables"},{"metadata":{},"cell_type":"markdown","source":"<b>Handle nonlinear relationships</b><br>\nWhile many machine learning models inherently have the flexibility to detect and model non-linear relationships between a predictor variable and the target variable, linear regression, by default, will model the relationship as a linear relationship\nYou can, however, set up your regression dataset to give regression the flexibility to model non-linear relationships if they exist. <br>\n\nThe joinplot graphs that were made for each variable already show some imformation about what the relationship looks like. <br>\nAdditional, we can also calulate the skewness to see if there are variables skewed to the right (skew > 1) or left (skew < -1)."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# check for skewed data\n\nskew = df_house.skew(axis = 0, skipna = True) \n\n# show all variables which are skewed to the right (skew > 1)\nprint('Variables which are skewed to the right:')\nprint(skew[skew > 1].sort_values(ascending = False))\nprint()\n\n# show all variables which are skewed to the left (skew < -1)\nprint('Variables which are skewed to the left:')\nprint(skew[skew < -1].sort_values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Reduce skewness</b><br>\nThe results show a number of variables which are skewed to the right, and none that are skewed to the left. <br>\nTo handle variables skewed to the right, take the log of the variable. This only works if there are no zero or negative values. Otherwise, take the cube root or square root. <br>\nFor the variables price, sqft_lot, and distane_to_city, a new variable is created with the log of the variable."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_house['price_log'] = np.log(df_house['price'])\ndf_house['sqft_lot_log'] = np.log(df_house['sqft_lot'])\ndf_house['distance_to_city_log'] = np.log(df_house['distance_to_city'])\n\ndf_house","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Binning:</b><br>\nData binning is a preprocessing technique used to reduce the effects of minor observation errors.< Often we see this with a variable like age. <br>\nWe will do the same for the 'age' of the houses. <br>\nStep 1: partition the age into bins. <br>\nStep 2: create dummy variables for every bin."},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices\n\n# add the age of the buildings when the houses were sold as a new column\nage = df_house['date'].astype(str).str[:4].astype(int) - df_house['yr_built']\n\n# partition the age into bins\nbins = [-2, 0, 5, 10, 25, 50, 75, 100, 100000]\nlabels = ['<1', '1-5', '6-10', '11-25', '26-50', '51-75', '76-100', '>100']\ndf_house['age_binned'] = pd.cut(age, bins = bins, labels = labels)\n\n# histograms for the binned columns\nplot = sns.countplot(df_house['age_binned'])\nfor p in plot.patches:\n    height = p.get_height()\n    plot.text(p.get_x() + p.get_width() / 2, height + 50, height, ha = \"center\")   \n\nax.set(xlabel='Age')\nax.yaxis.tick_left()\n\n# transform the factor values to be able to use in the model\ndf_house = pd.get_dummies(df_house, columns=['age_binned'])\n\ndf_house","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now some important variables are transformed, including the y variable, let's do the joinplots and the correlation heatmap again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# joinplots for all x variables in relation to x, to visualize the bivariate distribution\n\nfor col in list(df_house.drop(['id', 'date','price'], axis = 1).columns):\n    sns.jointplot(x = col, y = \"price_log\", data = df_house, kind = 'reg', size = 5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create correlation matrix\ncorr_matrix = df_house.corr()\n#display(corr_matrix)\n\n\n# set up mask to hide upper triangle\n\nmask = np.zeros_like(corr_matrix, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# create seaborn heatmap\n\nf, ax = plt.subplots(figsize = (16, 10)) \n\nheatmap = sns.heatmap(corr_matrix, \n                      mask = mask,\n                      #square = True, # Makes each cell square-shaped\n                      linewidths = .5, # set width of the lines that will divide each cell to .5\n                      cmap = \"coolwarm\", # map data values to the coolwarm color space\n                      cbar_kws = {'shrink': .4, # shrink the legend size and label tick marks at [-1, -.5, 0, 0.5, 1]\n                                \"ticks\" : [-1, -.5, 0, 0.5, 1]},\n                      vmin = -1, # Set min value for color bar\n                      vmax = 1, # Set max value for color bar\n                      annot = True, # Turn on annotations for the correlation values\n                      fmt='.2f', # String formatting code to use when adding annotations\n                      annot_kws = {\"size\": 12}) # Set annotations to size 12\n\n# add title\nplt.title('House Sales King County - Correlation Heatmap', \n              fontsize=14, \n              fontweight='bold')\n\n# add the column names as labels\nax.set_xticklabels(corr_matrix.columns, rotation = 90) # Add column names to the x labels and rotate text to 90 degrees\nax.set_yticklabels(corr_matrix.columns, rotation = 0) # Add column names to the y labels and rotate text to 0 degrees\n\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True}) # Show tickmarks on bottom and left of heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create model and fit with test dataset"},{"metadata":{},"cell_type":"markdown","source":"<b>Split dataset in train and test datasets</b>\n<br>\nThe dataset will be split with random 80% in a training dataset to train the model, and the other random 20% in a test dataset. This split allows us to validate the model’s predictive power on a different set of data than the model was trained on.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split in train and test dataset, and x and y\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_house.drop(['id', 'date', # variables not used from start\n                   'price', 'price_log', # exclude dependent variables\n                   'sqft_lot', 'distance_to_city', # exclude original variables after creating log transformatian\n                   'sqft_basement','yr_built', 'zipcode', # exclude original variables after transformation\n                   'sqft_above'], # remove to avoid multicoliniarity\n                  axis = 1)\ny = df_house['price_log']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Create model</b><br>\nNow we can finally create a model ! Sklearn is used to create the model and for further steps. Statsmodels is also used to show the p-values."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# create linear model\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlrmodel1 = lr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print model coefficients\n\ncoef_lr = pd.Series(lrmodel1.coef_, index = X_train.columns)\nprint('Intercept:', lrmodel1.intercept_)\nprint()\nprint('Coefficients:')\nprint(coef_lr.round(4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get p values for coefficients with statsmodel\n\nimport statsmodels.api as sm\n\n#Fitting sm.OLS model\nX_1 = sm.add_constant(X_train)\nmodel = sm.OLS(y_train,X_1).fit()\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Interpreting model coefficients</b><br>\n\n\n<i>Coef</i>: this is the weight assigned to that input variable. Positive indicates a positive relationship between the input variable and the target variable (controlling for all other input variables in the model). A higher absolute value indicates a larger impact on the target variable (although be sure to consider the potentially different scale of each of the input variables when comparing across input variables). <br>\n<br>\n<i>P>|t|</i>: this is the likelihood that there is no relationship between the input variable and the target variable; usually, variables are excluded if they have a p-value more than a certain threshold (such as 0.05)."},{"metadata":{},"cell_type":"markdown","source":"<b>Interpreting coefficients with log-transformed Variables</b><br>\n<br>\n<i>Only the dependent/response variable is log-transformed.</i> <br>\nExponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the response for every one-unit increase in the independent variable. <br>\nExample: the coefficient is 0.198. (exp(0.198) – 1) x 100 = 21.9. For every one-unit increase in the independent variable, our dependent variable increases by about 22%.<br>\nIn python: (np.exp(0.198) - 1) x 100<br>\n<br>\n<i>Only independent/predictor variable(s) is log-transformed.</i> <br>\nDivide the coefficient by 100. This tells us that a 1% increase in the independent variable increases (or decreases) the dependent variable by (coefficient/100) units. <br>\nExample: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase in the independent variable, our dependent variable increases by about 0.002. For x percent increase, multiply the coefficient by log(1.x). <br>\nExample: For every 10% increase in the independent variable, our dependent variable increases by about 0.198 x log(1.10) = 0.02.<br>\n<br>\n<i>Both dependent/response variable and independent/predictor variable(s) are log-transformed.</i> <br>\nInterpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. <br>\nExample: the coefficient is 0.198. For every 1% increase in the independent variable, our dependent variable increases by about 0.20%. For x percent increase, calculate 1.x to the power of the coefficient, subtract from 1, and multiply by 100. <br>\nExample: For every 20% increase in the independent variable, our dependent variable increases by about (1.20 0.198 – 1) x 100 = 3.7 percent.<br>"},{"metadata":{},"cell_type":"markdown","source":"# Predict values on test dataset"},{"metadata":{},"cell_type":"markdown","source":"Assessing a model: assesses predictive power of trained model on an independent dataset that the model was not trained on.<br>\nThis is the best indicator of how the model will actually perform when applied to new data points."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# predict y values based on model coefficients\n\npred_train = lrmodel1.predict(X_train)\npred_test = lr.predict(X_test)\n\nplt.scatter(y_test,pred_test)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r\")\nplt.xlabel('y actual')\nplt.ylabel('y predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return predictions obtained for each element when it was in the test set\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_cross = cross_val_predict(lrmodel1, X, y, cv = 5)\n\nplt.scatter(y, y_cross)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r\")\nplt.xlabel('y actual')\nplt.ylabel('y predicted (cross)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check regression assumptions"},{"metadata":{},"cell_type":"markdown","source":"<b>While aspects of the regression model rely on these assumptions to be truly robust/accurate, a regression model can often be predictive/useful even when certain assumptions are not met!</b><br>\nThis model is used for prediction, and not determining significance of particular variables. For this reason, I regard the results of the assumptions check not as 'the model is wrong and unuseful', but as nothing more than suggestions for model improvement."},{"metadata":{},"cell_type":"markdown","source":"<b>Residuals/errors (model’s predicted value – actual value for each observation) are approximately normally distributed.</b> <br>\nProblem:\n    - If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares.\nCheck:\n    - After model is created, you could plot distribution of residuals to see whether it looks normal\n    - QQ (quantile quantile) / probability plot. If the data comes from a normal distribution, the plot would show fairly straight line.\n    - You can also perform statistical tests of normality such as Kolmogorov-Smirnov test, Shapiro-Wilk test.\nFix:\n    - If the errors are not normally distributed, non – linear transformation of the variables (response or predictors) can bring improvement in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create probability plot\n\nresiduals = y_train - pred_train.reshape(-1)\n\nplt.figure(figsize=(7,7))\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title(\"Normal Q-Q Plot\")\nprint(\"If the residuals (blue dots) fall on the red line, the residuals are approximately normally distributed\")\nplt.show()\nprint()\n\n\n# Kolmogorov-Smirnov test\n# if the test is significant, this indicates that the model’s residuals are not normally distributed\n\nkstest = stats.kstest(residuals, 'norm')\n\nprint(\"Kolmogorov-Smirnov:\")\nprint(kstest)\nif kstest[1] < 0.05:\n    print(\"Evidence that the residuals are not normally distributed\")\n    print('Assumption not satisfied')\nelse:\n    print(\"No evidence that the residuals are not normally distributed\")\n    print('Assumption satisfied')\nprint()\n\n\n# Shapiro Wilk test\n# if the test is significant, this indicates that the model’s residuals are not normally distributed\n\nshapiro = stats.shapiro(residuals)\n\nprint(\"Shapiro Wilk:\")\nprint(shapiro)\nif shapiro[1] < 0.05:\n    print(\"Evidence that the residuals are not normally distributed\")\n    print('Assumption not satisfied')\nelse:\n    print(\"No evidence that the residuals are not normally distributed\")\n    print('Assumption satisfied')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residuals are NOT normally distributed. <br>\nThe model predicts too low for low house prices, and too high for high house prices. <br>\nSince we already did variable transformation to capture non linear relationships, and in the mid section the results look good, no further action is taken. <br>\nIn a future version I might take a look at what is happening in the tails."},{"metadata":{},"cell_type":"markdown","source":"<b>No autocorrelation: residuals are independent across observations</b><br>\nProblem:\n\t- The presence of correlation in error terms drastically reduces model’s accuracy. \n\tIf the error terms are correlated, the estimated standard errors tend to underestimate the true standard error. \n\tIf this happens, it causes confidence intervals and prediction intervals to be narrower. Narrower confidence interval means that a 95% confidence interval would have lesser probability than 0.95 that it would contain the actual value of coefficients. \n\tAlso, lower standard errors would cause the associated p-values to be lower than actual. This will make us incorrectly conclude a parameter to be statistically significant. \nCheck:\n\t- Look for Durbin – Watson (DW) statistic. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. \n\t- Also, you can see residual vs time plot and look for the seasonal or correlated pattern in residual values.\nFix:\n    - The result of Durbin-Watson shows that either you fitted a linear function to your data which have nonlinear relationship or you didn't considered an important variable in your model. Try to transform your data in a way that you can guarantee a linear relationship and think if you omitted an important variable or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.stats.api as sms\n\ndw = sms.durbin_watson(residuals)\n\nprint('Durbin-Watson: {:.3f}'.format(dw))\nif dw < 1.5:\n    print('Signs of positive autocorrelation')\n    print('Assumption not satisfied')\nelif dw > 2.5:\n    print('Signs of negative autocorrelation')\n    print('Assumption not satisfied')\nelse:\n    print('Little to no autocorrelation')\n    print('Assumption satisfied')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There appears to be no autocorrelation, this is good, no need for action"},{"metadata":{},"cell_type":"markdown","source":"<b>Homoscedasticity: error terms must have constant variance</b><br>\nThe variance of residuals is not correlated with model’s predicted values.<br>\nProblem:\n\t- The standard errors (which are used to conduct significance tests, and calculate the confidence intervals) will be biased\nCheck:\n\t- Plot residuals versus predicted values and check whether residuals are generally evenly distributed regardless of whether the predicted value is high or low.\n\tThere should be no pattern. \n\tIf there exist any pattern (may be, a parabolic shape) in this plot, consider it as signs of non-linearity in the data. It means that the model doesn’t capture non-linear effects.\n\tIf a funnel shape is evident in the plot, consider it as the signs of non constant variance i.e. heteroskedasticity.\n\t- This can be tested using a few different statistical tests, these include the Brown-Forsythe test, Levene’s test, Bruesch-Pagan test, or Cook-Weisberg test.\nFix:\n\t- To overcome the issue of non-linearity, you can do a non linear transformation of predictors such as log (X), √X or X² transform the dependent variable. \n\t- To overcome heteroskedasticity, a possible way is to transform the response variable such as log(Y) or √Y. \n\t- Also, you can use weighted least square method to tackle heteroskedasticity.\n\t- Fix outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual plot\n\nsns.residplot(pred_train, y_train, lowess = True,\n                                  line_kws = {'color': 'red', 'lw': 1, 'alpha': 1})\nplt.xlabel(\"Predicted Y\")\nplt.ylabel(\"Residual\")\nplt.title('Residual plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the red line on the graph deviates much from the green line, this is a sign of no constant variance between error terms. <br>\nIt is not an excact match, but the deviation is not extreme, so no forther action is taken."},{"metadata":{},"cell_type":"markdown","source":"<b>No multicolinearity: input variables in the final model are not highly correlated with one another</b><br>\nProblem:\n\t- It makes it hard to determine which input variable is really driving the target variable\n\t- With presence of correlated predictors, the standard errors tend to increase. And, with large standard errors, the confidence interval becomes wider leading to less precise estimates of slope parameters.\n\t- Also, when predictors are correlated, the estimated regression coefficient of a correlated variable depends on which other predictors are available in the model. If this happens, you’ll end up with an incorrect conclusion that a variable strongly / weakly affects target variable. Since, even if you drop one correlated variable from the model, its estimated regression coefficients would change. That’s not good!\nCheck:\n\t- Plot correlation coefficient matrix for all of the input variables and look for cases where variables are highly correlated. Should be < 0.8\n\t- Check variance inflation factor (VIF) of each input variable and ensure that it is less than 5 or 10. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. \nFix:\n    - If there is high multicolinearity between 2 variables, you could recode the variables to have more unique information or exclude the input variable that is less correlated with the target variable from the model\n    - Sometimes it is unavoidable, e.g. with dummy variables"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# show all correlations above 0.8\n\ncor = X_train.corr().round(2).abs() # create correlation df with rounded absolute values\ncor = cor.unstack() # unstack to one row per correlation\ncor = cor.reset_index(drop=False) # create new index instead of variable names as index\ncor = cor[cor['level_0'] != cor['level_1']] # remove correlation for each variable with itsself\ncor = cor[cor[0] >= 0.80] # show only correlations above this level\ncor = cor.iloc[0:-1:2] # there are double rows for each correlation. take only one row. this is not fool proof !\ncor.sort_values([0], ascending = False, inplace = True) # sort from high correlation to low\nprint(cor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate vif\n# situations in which a high VIF is not a problem and can be safely ignored:\n# 1. The variables with high VIFs are control variables, and the variables of interest do not have high VIFs.\n# 2. The high VIFs are caused by the inclusion of powers or products of other variables. \n# 3. The variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three+ categories.\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif[\"features\"] = X_train.columns\nvif[\"VIF Factor\"] = vif[\"VIF Factor\"].round(1)\nvif.sort_values(['VIF Factor'], ascending = False, inplace = True)\nvif[\"VIF Factor\"] = vif[\"VIF Factor\"].astype(str)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables show multicoliniarity. <br>\nWith dummy and binned variables this is unavoidable. <br>\nWith trial and error variables that relate too much to another variable are removed from the model. <br>\nAt the end, only removing the variable sqft_above resulted in an improvent of the model. "},{"metadata":{},"cell_type":"markdown","source":"# Access model performance"},{"metadata":{},"cell_type":"markdown","source":"Because the goal of the regression is the use the model for predictions, we need to know how capable the model is in prediction. To determine the model performance, RMSE, R2, and R2 adjusted are used. <br>\nAll metrics are calculated for the train model, the test model, and cross validation.<br>\n<br>\n<b>RMSE</b><br>\nThe RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit.<br>\n<br>\n<b>R Squared (R²) and Adjusted R Squared</b><br>\nR Squared & Adjusted R Squared describe what percent of the variability in your dependent variable(s) is explained by your selected independent variable(s). <br>\nAn adjusted R² will consider the marginal improvement added by an additional term in your model. So it will increase if you add the useful terms and it will decrease if you add less useful predictors. However, R² increases with increasing terms even though the model is not actually improving."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate MSQE, R2, and R2 adjusted, for train and test model\n\nfrom sklearn.metrics import mean_squared_error, r2_score \nfrom sklearn.model_selection import cross_val_score\n\nprint(\"Train model scores\")\n\nmse = np.sqrt(mean_squared_error(y_train, pred_train)) \nprint('Root mean square error:', mse) \n\nr2 = r2_score(y_train, pred_train)\nprint('R2: {:.2%}'.format(r2))\n\nn = X_train.shape[0]\np = X_train.shape[1]\nr2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)\nprint('R2 Adjusted: {:.2%}'.format(r2_adj))\nprint()\n\n\nprint(\"Test model scores\")\n\nmse = np.sqrt(mean_squared_error(y_test,pred_test)) \nprint('Root mean square error:', mse) \n\nr2 = r2_score(y_test, pred_test)\nprint('R2: {:.2%}'.format(r2))\n\nn = X_test.shape[0]\np = X_test.shape[1]\nr2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)\nprint('R2 Adjusted: {:.2%}'.format(r2_adj))\nprint()\n\n\nprint(\"Cross validation scores\")\n\nscores = cross_val_score(lrmodel1, X, y, cv = 5)\nprint(\"All scores:\",(scores * 100).round(2))\nprint('Average cross validation score: {:.2%}'.format(np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Adding another input variable to the regression will generally always increase the model’s predictive power (increase the r-squared value) on the training dataset by at least a nominal amount<br>\nHowever, adding a variable could actually result in lower predictive power on a holdout dataset due to the bias-variance tradeoff (this is called overfitting)<br>\nHence, it is important to be sure that the additional variable is statistically significant in the training dataset and check to make sure that the adjusted r-squared also increases<br>\nThere are several methods for feature selection, below RFE is used. Recursive Feature Elimination (RFE) recursively removes features, builds a model using the remaining attributes, and calculates model accuracy."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# feature selection with RFE\n\nfrom sklearn.feature_selection import RFE\n\n# choose optimal # of features\n\n#no of features\ncolumns_count = len(X_train.columns)\nnof_list = np.arange(1, columns_count + 1)\nhigh_score = 0\n\n#Variable to store the optimum features\nnof = 0\nscore_list = []\n\nfor n in range(len(nof_list)):\n    #X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model, nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train, y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe, y_train)\n    score = model.score(X_test_rfe, y_test)\n    score_list.append(score)\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))\nprint()\n\n\n# run RFE \n\ncols = list(X.columns)\nmodel = LinearRegression()\n\n#Initializing RFE model\nrfe = RFE(model, nof)\n\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)\n\n#Fitting the data to model\nmodel.fit(X_rfe,y)\ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(\"Selected features:\", list(selected_features_rfe))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are similar to the results with all features. Since the p-values of all features are significant, this is no suprise."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}