{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nfrom pprint import pprint\nimport collections\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n\nimport gensim\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(37)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data\nWe shuffle the data frame in case the classes would be sorted. This can be done with the reindex method applied on the permutation of the original indices. In this notebook we will only focus on the text variable and the class variable."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/grammar-and-online-product-reviews/GrammarandProductReviews.csv', header=0)\ndf = df.reindex(np.random.permutation(df.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"reviews.doRecommend\", data=df, kind=\"count\", size=6, aspect=1.5, palette=\"PuBuGn_d\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"reviews.rating\", data=df, kind=\"count\", size=6, aspect=1.5, palette=\"PuBuGn_d\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Despues de analisis"},{"metadata":{"trusted":true},"cell_type":"code","source":"the_df = df[['name' , 'reviews.rating']]\nthe_df = the_df.rename(columns={ \"reviews.rating\": \"rating\"})\n\ngrouped = the_df.groupby('name').sum().reset_index()\nbest = grouped.sort_values('rating', ascending=False)\nbest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = the_df.groupby('name').sum().reset_index()\nworst = grouped.sort_values('rating', ascending=True)\nworst.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"the_df = df[['reviews.username' , 'reviews.rating']]\nthe_df = the_df.rename(columns={\"reviews.username\":\"username\" ,\"reviews.rating\": \"rating\"})\n\n#grouped = the_df.groupby('username').sum().reset_index()\ngrouped = the_df.groupby('username').filter(lambda group: group.size < 3)\nbest = grouped.sort_values('rating', ascending=False)\nbest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"the_df = df[['manufacturer' , 'reviews.rating']]\nthe_df = the_df.rename(columns={\"reviews.rating\": \"rating\"})\n\ngrouped = the_df.groupby('manufacturer').sum().reset_index()\nworst = grouped.sort_values('rating', ascending=True)\nworst.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"the_df = df[['manufacturer' , 'reviews.rating']]\nthe_df = the_df.rename(columns={\"reviews.rating\": \"rating\"})\n\ngrouped = the_df.groupby('manufacturer').sum().reset_index()\nbest = grouped.sort_values('rating', ascending=False)\nbest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextCounts(BaseEstimator, TransformerMixin):\n    \n    def count_regex(self, pattern, tweet):\n        return len(re.findall(pattern, tweet))\n    \n    def fit(self, X, y=None, **fit_params):\n        # fit method is used when specific operations need to be done on the train data, but not on the test data\n        return self\n    \n    def transform(self, X, **transform_params):\n        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', x))\n        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n        # Moreover, it will result in having more words in the tweet\n        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n        \n        df = pd.DataFrame({'count_words': count_words\n                           , 'count_mentions': count_mentions\n                           , 'count_hashtags': count_hashtags\n                           , 'count_capital_words': count_capital_words\n                           , 'count_excl_quest_marks': count_excl_quest_marks\n                           , 'count_urls': count_urls\n                           , 'count_emojis': count_emojis\n                          })\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the only things we are going to use in the dataset is reviews.rating and reviews.text\nnew_df = df[['reviews.text', 'reviews.rating']]\nnew_df = new_df.rename(columns={\"reviews.text\": \"text\", \"reviews.rating\": \"rating\"})\n#new_df\n# remove all non letters from column text\nnew_df.text = new_df.text.str.replace('[^a-zA-Z]', ' ')\n#try and parse them as string\nnew_df['text'] = new_df['text'].astype(str)\n#new_df.keys()\n\ntc = TextCounts()\ndf_eda = tc.fit_transform(new_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_eda['rating'] = new_df.rating\ndf_eda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_dist(df, col):\n    print('Descriptive stats for {}'.format(col))\n    print('-'*(len(col)+22))\n    print(df.groupby('rating')[col].describe())\n    bins = np.arange(df[col].min(), df[col].max() + 1)\n    g = sns.FacetGrid(df, col='rating', size=5, hue='rating', palette=\"PuBuGn_d\")\n    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_dist(df_eda, 'count_words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CleanText(BaseEstimator, TransformerMixin):\n    def remove_mentions(self, input_text):\n        return re.sub(r'@\\w+', '', input_text)\n    \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n    \n    def emoji_oneword(self, input_text):\n        # By compressing the underscore, the emoji is kept as one word\n        return input_text.replace('_','')\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n    \n    def stemming(self, input_text):\n        porter = PorterStemmer()\n        words = input_text.split() \n        stemmed_words = [porter.stem(word) for word in words]\n        return \" \".join(stemmed_words)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n        return clean_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = CleanText()\nsr_clean = ct.fit_transform(new_df.text)\nsr_clean.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model_clean = pd.DataFrame(sr_clean)\ndf_model_clean['rating'] = new_df.rating\ndf_model_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model_clean.columns.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(df_model_clean.drop('rating', axis=1), df_model_clean.rating, test_size=0.3, random_state=50)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"class ColumnExtractor(TransformerMixin, BaseEstimator):\n    def __init__(self, cols):\n        self.cols = cols\n\n    def transform(self, X, **transform_params):\n        return X[self.cols]\n\n    def fit(self, X, y=None, **fit_params):\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#create a new df model\ndf_model = df_eda\ndf_model['clean_text'] = pd.DataFrame(sr_clean)\ndf_model.columns.tolist()\n\ntextcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags','count_mentions','count_urls','count_words']\n\n#textcountscols = ['text', 'rating']\n    \nfeatures = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n                         , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))\n                                              , ('vect', CountVectorizer(max_df=0.5, min_df=1, ngram_range=(1,2)))]))]\n                       , n_jobs=-1)\n\npipeline = Pipeline([\n    ('features', features)\n    , ('clf', LogisticRegression(C=1.0, penalty='l2'))\n])\n\nbest_model = pipeline.fit(df_model.drop('rating', axis=1), df_model.rating)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_reviews = pd.Series([\"These sneakers feel great i had a great time using them i would buy them again in a heartbeat üëç\"\n                      ,\"This soap sucks i dont like it at all it felt like fat i would not buy again\"\n                      ,\"This detergent felt like it cleansed everything the best thing i've brought i like it a lot üòÉ\"])\n\ndf_counts_pos = tc.transform(new_reviews)\ndf_clean_pos = ct.transform(new_reviews)\ndf_model_pos = df_counts_pos\ndf_model_pos['clean_text'] = df_clean_pos\n\nbest_model.predict(df_model_pos).tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see avobe it worked as we used hand-made reviews to test the predictor and got 4, 1, 5 (in stars):\n* These sneakers feel great i had a great time using them i would buy them again in a heartbeat üëç **its a 4**\n* This soap sucks i dont like it at all it felt like fat i would not buy again **its a 1**\n* This detergent felt like it cleansed everything the best thing i've brought i like it a lot üòÉ **its a 5**\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}