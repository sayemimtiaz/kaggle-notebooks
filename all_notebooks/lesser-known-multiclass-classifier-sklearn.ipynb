{"cells":[{"metadata":{},"cell_type":"markdown","source":"### UPDATED - 18/08  [Dataset Change]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some Lesser Known MultiClass Classifier from SKLearn\n\nIn this notebook, we're going to have a look at some of the lesser known **Inherently MultiClass Classifier Algorithm** from SK-Learn library. \n\n* Extra Tree Classifier [sklearn.tree module]\n* Extra Tree Classifier [sklearn.ensemble module]\n* MLP Classifier\n* Nearest Centroid\n* Quadratic Discriminant Analysis\n* Radius Neighbors Classifier\n* Ridge Classifier\n\nWe'll be using the [Abalone Dataset](https://archive.ics.uci.edu/ml/datasets/abalone) Dataset for the multi-class classification.\n\nAs always, I'll keep the notebook organized & well commented for easy reading. Please do consider to UPVOTE if you find it helpful.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, warnings, gc\nwarnings.filterwarnings(\"ignore\")\n\n# Sklearn Classifier Algorithm\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import NearestCentroid, RadiusNeighborsClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier\n\n# Sklearn (other)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom tabulate import tabulate\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"url = '../input/all-datasets-for-practicing-ml/Class/Class_Abalone.csv'\ndata = pd.read_csv(url, header='infer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Records\nprint(\"Total Records: \", data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for empty/null/missing records\nprint(\"Is Dataset Empty: \", data.empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Records per Classes\ndata.Sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep\n\nWe'll use Label Encoder to convert the 'Sex' column to numerical format for easy ingestion by the algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating Label Encoder\nencoder = LabelEncoder()\n\n# Columns List\ncolumns = data.columns\n\n# Encode the column \ndata['Sex']= encoder.fit_transform(data['Sex']) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering, Data Split & Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature & Target Selection\ntarget = ['Sex']   \nfeatures = columns [1:]\n\nX = data[features]\ny = data[target]\n\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0, shuffle=True) \n\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.transform(X_val)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Extra Tree Classifier (Tree Module)\n\nAn extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n\n**Note**: Extra-trees should only be used within ensemble methods.\n\nIn this notebook, will be using another unknown ensemble classifier i.e. BaggingClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Extra Tree Classifier\net = ExtraTreeClassifier(random_state=1)\n\n# Bagging Classifier\nbgc = BaggingClassifier(et, random_state=1, max_features=8, verbose=0)\n\n# Train \nbgc.fit(X_train, y_train)\n\n# Prediction\ny_pred = bgc.predict(X_val)\n\n# Accuracy\nprint(\"Extra Tree Classifier(Tree Module) Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\n\ntab_data = []\ntab_data.append(['Extra Tree(Tree)', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Tree Classifier (Ensemble Module)\n\nAn extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\netc = ExtraTreesClassifier(n_estimators=100, max_depth= 5,\n                           verbose=0, random_state=1)\n\n# Train\netc.fit(X_train, y_train)\n\n# Prediction\ny_pred = etc.predict(X_val)\n\n# Accuracy\nprint(\"Extra Tree Classifier(Ensemble Module) Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Extra Tree(Ensemble)', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLP Classifier\n\nMulti-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic gradient descent.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\nmlp = MLPClassifier(random_state=1, max_iter=300,solver='sgd',\n                    batch_size=200, learning_rate='adaptive', learning_rate_init=0.001,\n                    shuffle=True, verbose=0)\n\n# Train\nmlp.fit(X_train, y_train)\n\n# Prediction\ny_pred = mlp.predict(X_val)\n\n# Accuracy\nprint(\"MLP Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['MLP', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Nearest Centroid\n\nNearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to the class with the nearest centroid.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\nnc = NearestCentroid()\n\n# Train\nnc.fit(X_train, y_train)\n\n# Prediction\ny_pred = nc.predict(X_val)\n\n# Accuracy\nprint(\"Nearest Centroid Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Nearest Centroid', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quadratic Discriminant Analysis\n\nQuadratic Discriminant Analysis\n\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule.\n\nThe model fits a Gaussian density to each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\nqda = QuadraticDiscriminantAnalysis()\n\n# Train\nqda.fit(X_train, y_train)\n\n# Prediction\ny_pred = qda.predict(X_val)\n\n# Accuracy\nprint(\"Quadratic Discriminant Analysis Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Quadratic Discriminant Analysis', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Radius Neighbours Classifier\n\nClassifier implementing a vote among neighbors within a given radius","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\nrnc = RadiusNeighborsClassifier(radius=2.0, )\n\n# Train\nrnc.fit(X_train, y_train)\n\n# Prediction\ny_pred = rnc.predict(X_val)\n\n# Accuracy\nprint(\"Radius Neighbours Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Radius Neighbours', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Classifier\n\nClassifier using Ridge regression.\n\nThis classifier first converts the target values into {-1, 1} and then treats the problem as a regression task (multi-output regression in the multiclass case).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Classifier\nrc = RidgeClassifier(class_weight='balanced', random_state=1)\n\n# Train\nrc.fit(X_train, y_train)\n\n# Prediction\ny_pred = rc.predict(X_val)\n\n# Accuracy\nprint(\"Ridge Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Ridge Classifier', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tabulate(tab_data, headers=['Classifiers','Accuracy'], tablefmt='pretty'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe, the accuracy of these classifiers are fairely close to each other. The next logical step would be to fine-tune the parameters to increase the accuracy. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}