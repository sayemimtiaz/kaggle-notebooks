{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle \nimport pprint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and visualise data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/trajectory-inference-single-cell-rna-seq/GSE90047_Mouse_Liver_Hepatoblast_invivo.csv', index_col = 0)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.index[0][1:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract Timing label from data index\n\ny = [float(df.index[i][1:5]) for i in range(len(df.index)) ]\ny = np.array(y)\n#np.unique(y)\npd.Series(y).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nfig = plt.figure(figsize = (15,5))\ndata2 = PCA().fit_transform(df)\n#plt.scatter(data2[:,0],data2[:,1], c= y , alpha = 0.9)# ,cmap=plt.cm.Paired) # ,c=np.array(irx)  , linewidth = data_linewidth\nsns.scatterplot(x = data2[:,0],y = data2[:,1], hue=y)\nplt.grid()\nplt.xlabel('PCA1')\nplt.ylabel('PCA2')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions for MST trajectories construction and visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.cluster import DBSCAN, KMeans, OPTICS, SpectralClustering\nfrom sklearn.cluster import  KMeans,  SpectralClustering # DBSCAN, OPTICS,\nimport umap\nimport numpy as np\nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy.sparse.csgraph import minimum_spanning_tree\nfrom sklearn.decomposition import PCA\n\ndef create_tree_by_cluster_knn_mst(X, n_clusters='sqrt', n_neighbors= 10,   verbose =  0, clustering_method = 'Kmeans' ):\n  '''\n  # Calcuates a tree approximation for given dataset X, by kmeans+knn+mst\n  # \n  #' @param n_clusters number of clusters for clustering or 'sqrt' - square root of dataset size\n  #' @param n_neighbors used by knn-graph step\n  #\n  #' @return\n  # dict_result - dictionary with results:\n  #   dict_result['csr_mst'] # adjancy matrix of MST graph in csr format (column sparse matrix - scipy )\n  #   dict_result['edges_mst'] # edges matrix of MST graph, shape = n_nodes X 2 , each row contains ends of the correspoding edges\n  #   dict_result['nodes_positions']  graph nodes positions\n  #   dict_result['predicted_clusters'] vector with cluster number for each point of  input X\n  #   dict_result['csr_knn'] same as 'csr_mst', but for intermediate knn-graph  \n  #   dict_result['edges_knn'] same as 'edges_mst', but for intermediate knn-graph \n  # \n  #' @examples\n  # X = np.random.rand(1000,10)\n  # dict_result =  create_tree_by_cluster_knn_mst(X)# - Calcuates a tree for given dataset, by kmeans+knn+mst\n  # edges =  dict_result['edges_mst']\n  # nodes_positions = dict_result['nodes_positions']\n  # plot_graph(edges, nodes_positions, data = X)\n  # plt.show()  \n  '''\n  if n_clusters == 'sqrt':\n    n_clusters = int( np.sqrt(X.shape[0] ) ) \n\n  #print(\"clustering_method.lower() == 'Spectral'.lower()\", clustering_method.lower() == 'Spectral'.lower() )\n  if isinstance(clustering_method ,str) and ( clustering_method.lower() == 'Spectral'.lower() ):\n    clustering = SpectralClustering(n_clusters=n_clusters, random_state=0).fit(X)\n    predicted_clusters = clustering.labels_ # kmeans.predict(X)\n    # Get cluster centers by averaging:\n    l = len(np.unique(predicted_clusters))\n    cluster_centers_ = np.zeros( (l, X.shape[1]))\n    for i,v in  enumerate(np.unique(predicted_clusters)):\n      m = predicted_clusters==v \n      cluster_centers_[i,:] = np.mean(X[m,:],axis = 0 )\n  else: # Kmeans clustering by defualt:\n    clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n    cluster_centers_ = clustering.cluster_centers_\n  predicted_clusters = clustering.labels_ # kmeans.predict(X)\n\n  if verbose >= 100:\n    print('cluster_centers_[:2,:]', cluster_centers_[:2,:])\n    print('predicted_clusters.shape', predicted_clusters.shape)\n    print('predicted_clusters[:2]', predicted_clusters[:2])\n\n  # sklearn.neighbors.kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)[source]Â¶\n  if n_neighbors > len(cluster_centers_):\n    n_neighbors = len(cluster_centers_) # To avoid exception for small number of clusters \n  csr_knn = kneighbors_graph(cluster_centers_, n_neighbors= n_neighbors, mode= 'distance', include_self=True) # mode=  'connectivity'\n  if verbose >= 100:\n    print('csr_knn', type(csr_knn), csr_knn.shape)\n\n  csr_mst = minimum_spanning_tree(csr_knn)\n  if verbose >= 100:\n    print('csr_mst', type(csr_mst),csr_mst.shape)\n\n  dict_result = {}\n  dict_result['csr_mst'] = csr_mst\n  dict_result['csr_knn'] = csr_knn\n  dict_result['nodes_positions'] = cluster_centers_\n  dict_result['predicted_clusters'] = predicted_clusters\n  dict_result['edges_mst'] = get_edges_from_adj_matrix( csr_mst )\n  dict_result['edges_knn'] = get_edges_from_adj_matrix( csr_knn )\n\n  return dict_result\n\ndef get_edges_from_adj_matrix( adj_matrix ):\n  '''\n  #' From adjacency matrix construct an edge list\n  #' either [k1,k2]!=0 or [k2,k1]!=0, causes edge (k1,k2)\n  #'\n  #' @param  adj_matrix - adjacency matrix of an unoriented graph\n  #'\n  #' @return numpy.ndarray of shape Nx2, containing vertices for each edge \n  #'    \n  #' @examples\n  #' adj_matrix = np.array([[0,1,1],[0,0,0],[0,2,0]])\n  #' edges = get_edges_from_adj_matrix( adj_matrix )\n  '''\n  list_edges = []\n  n_vertex = adj_matrix.shape[0]\n  for k1 in range(  n_vertex  ) :\n    for k2 in range(k1, n_vertex ) :\n      if ( adj_matrix[k1,k2] != 0) or (adj_matrix[k2,k1] != 0):\n          list_edges.append( (k1,k2) )\n  return np.array(list_edges)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ntry :\n    import umap\nexcept:\n    print('cannot import umap')\n\ndef plot_graph(edges, nodes_positions, data = None, dim_reduction = 'PCA', graph_color = 'black', graph_linewidth=2, \n               plot_data = True, data_linewidth = 1,  data_color = 'tab:red', data_transparency_alpha = 0.9,\n               umap_n_neighbors = 50, umap_min_dist = 0.99):\n  '''\n  #' Plots graphs defined by edges and nodes_positions, optionally - scatter plot the \"data\" on the same plot,\n  #' Optionally performs PCA/etc (depending on dim_reduction)\n  #'\n  #' @param edges Nx2-shape matrix with edges ends, i.e. edges[k,0], edges[k,1] - ends of k-th edge  \n  #' @param nodes_positions  matrix of nodes positions \n  #' @param data  \"original dataset\", basically arbitrary dataset for scatter plot, it should have same shape[1] as nodes_positions\n  #' @param plot_data  True/False - to scatterplot or not data\n  #' @param dim_reduction  'PCA', 'plot_first2axis', 'umap'\n  #' @param data_color can be a vector or predefined color - argument for c = data_color in scatter\n\n  #' @examples\n  # edges = np.array([ [0,1],[1,2],[2,0] ] )\n  # nodes_positions = np.random.rand(3,10) # 3 points in 10d space\n  # plot_graph(edges, nodes_positions)\n  #\n  # t = elpigraph_output\n  # edges = t[0]['Edges'][0]\n  # nodes_positions = t[0]['NodePositions']\n  # plot_graph(edges, nodes_positions)\n  '''\n  str_dim_reduction = dim_reduction\n  if dim_reduction in ['PCA', 'umap' ]: #  not 'plot_first2axis':\n    if dim_reduction.upper() == 'PCA':\n      reducer = PCA()\n    elif dim_reduction.lower() == 'umap':\n      n_neighbors = umap_n_neighbors#  50\n      min_dist= umap_min_dist # 0.99\n      #n_components=n_components\n      reducer = umap.UMAP( n_neighbors=n_neighbors,        min_dist=min_dist, n_components = 2)\n\n    if data is not None:\n      data2 = reducer.fit_transform(data)\n      if plot_data == True:\n        if data_color is None:\n          plt.scatter(data2[:,0],data2[:,1], linewidth = data_linewidth , alpha = data_transparency_alpha)# ,cmap=plt.cm.Paired) # ,c=np.array(irx) \n          plt.xlabel(str_dim_reduction+'1')\n          plt.ylabel(str_dim_reduction+'2')\n        else:\n          plt.scatter(data2[:,0],data2[:,1] ,cmap=plt.cm.Paired,c= data_color, linewidth = data_linewidth, alpha = data_transparency_alpha ) \n          plt.xlabel(str_dim_reduction+'1')\n          plt.ylabel(str_dim_reduction+'2')\n    else:\n      reducer.fit(nodes_positions)\n\n    nodes_positions2 = reducer.transform( nodes_positions )\n  else:\n    if plot_data == True:\n      if data is not None:\n        if data_color is None:\n          plt.scatter(data[:,0],data[:,1] , linewidth = linewidth, alpha = data_transparency_alpha )# ,cmap=plt.cm.Paired) # ,c=np.array(irx) \n        else:\n          plt.scatter(data[:,0],data[:,1] ,cmap=plt.cm.Paired,c= data_color , linewidth = data_linewidth, alpha = data_transparency_alpha ) \n    nodes_positions2 = nodes_positions\n\n  plt.scatter(nodes_positions2[:,0],nodes_positions2[:,1],c = graph_color, linewidth = graph_linewidth)#, cmap=plt.cm.Paired)\n\n  edgeCount = edges.shape[0]\n  for k in range(edgeCount):\n    n0 = edges[k,0]\n    n1 = edges[k,1]\n    x_line = [ nodes_positions2[n0,0],  nodes_positions2[n1,0] ]\n    y_line = [ nodes_positions2[n0,1],  nodes_positions2[n1,1] ]\n    plt.plot(x_line, y_line, graph_color, linewidth = graph_linewidth) # 'black')\n\n    \nedges = np.array([ [0,1],[1,2],[2,0] ] )\nnodes_positions = np.random.rand(3,10) # 3 points in 10d space\nplot_graph(edges, nodes_positions)\nplt.title('Example graph plot with  plot_graph function')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trajectories by MST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nX = df.values\nX = PCA().fit_transform(X)\nX = X[:,:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nc = 0\nfig = plt.figure(figsize = (25,6) )\nfor n_clusters in [9,10,20,23]: #  [2,25]: # range(0):\n  c+=1\n  n_neighbors = n_clusters \n  dict_result =  create_tree_by_cluster_knn_mst(X, n_clusters = n_clusters, n_neighbors=n_neighbors)# - Calcuates a tree for given dataset, by kmeans+knn+mst\n  edges =  dict_result['edges_mst']\n  nodes_positions = dict_result['nodes_positions']\n\n  fig.add_subplot(1,4,c)\n  plot_graph(edges, nodes_positions, data = X , data_color = 'tab:blue', data_transparency_alpha = 0.3 )\n  plt.title('Trajectories MST  '+ str(n_clusters) + ' nodes' )\n  #plt.show()  \n\n  if c%4 == 0:\n    plt.show()\n    fig = plt.figure(figsize = (25,6) )\n    c = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trajectories by ElPiGraph"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install  --no-dependencies  git+https://github.com/j-bac/elpigraph-python.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import elpigraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt0 = time.time()\nc = 0\nfig = plt.figure(figsize = (25,6) )\nfor n_nodes in [9,10,20,23]: #  [2,25]: # range(0):\n    c+=1\n    \n    tree_elpi = elpigraph.computeElasticPrincipalTree(X , NumNodes=n_nodes, \n      alpha=0.01,FinalEnergy='Penalized', StoreGraphEvolution = True )#,\n\n    nodes_positions = tree_elpi[0]['NodePositions'] # ['AllNodePositions'][k]\n    matrix_edges_weights = tree_elpi[0]['ElasticMatrix'] # ['AllElasticMatrices'][k]\n    matrix_edges_weights = np.triu(matrix_edges_weights,1 )\n    edges = np.array( np.nonzero(matrix_edges_weights), dtype = int ).transpose()\n\n    fig.add_subplot(1,4,c)\n    plot_graph(edges, nodes_positions, data = X , data_color = 'tab:blue', data_transparency_alpha = 0.3 )\n    plt.title('Trajectories ElPiGraph ' + str(n_nodes)+' nodes');\n    #plt.show()  \n\n    if c%4 == 0:\n        plt.show()\n        fig = plt.figure(figsize = (25,6) )\n        c = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions for splitting graph into segments and clustering dataset accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"import igraph\ndef partition_data(Xcp, NodePositions, MaxBlockSize = 10**6,SquaredXcp= None,\n                  TrimmingRadius=float('inf')):\n    '''\n    # Partition the data by proximity to graph nodes\n    # (same step as in K-means EM procedure)\n    #\n    # Inputs:\n    #   X is n-by-m matrix of datapoints with one data point per row. n is\n    #       number of data points and m is dimension of data space.\n    #   NodePositions is k-by-m matrix of embedded coordinates of graph nodes,\n    #       where k is number of nodes and m is dimension of data space.\n    #   MaxBlockSize integer number which defines maximal number of\n    #       simultaneously calculated distances. Maximal size of created matrix\n    #       is MaxBlockSize-by-k, where k is number of nodes.\n    #   SquaredX is n-by-1 vector of data vectors length: SquaredX = sum(X.^2,2);\n    #   TrimmingRadius (optional) is squared trimming radius.\n    #\n    # Outputs\n    #   partition is n-by-1 vector. partition[i] is number of the node which is\n    #       associated with data point X[i, ].\n    #   dists is n-by-1 vector. dists[i] is squared distance between the node with\n    #       number partition[i] and data point X[i, ].\n    '''\n    if SquaredXcp is None:\n      SquaredXcp = np.sum(Xcp**2,1)[:,np.newaxis]\n    NodePositionscp = np.asarray(NodePositions)\n    n = Xcp.shape[0]\n    partition = np.zeros((n, 1), dtype=int)\n    dists = np.zeros((n, 1))\n    all_dists = np.zeros((n, NodePositions.shape[0] ))\n    # Calculate squared length of centroids\n    cent = NodePositionscp.T\n    centrLength = (cent**2).sum(axis=0)\n    # Process partitioning without trimming\n    for i in range(0, n, MaxBlockSize):\n        # Define last element for calculation\n        last = i+MaxBlockSize\n        if last > n:\n            last = n\n        # Calculate distances\n        d = SquaredXcp[i:last] + centrLength-2*np.dot(Xcp[i:last, ], cent)\n        tmp = d.argmin(axis=1)\n        partition[i:last] = tmp[:, np.newaxis]\n        dists[i:last] = d[np.arange(d.shape[0]), tmp][:, np.newaxis]\n        all_dists[i:last,:] = d\n    # Apply trimming\n    if not np.isinf(TrimmingRadius):\n        ind = dists > (TrimmingRadius**2)\n        partition[ind] = -1\n        dists[ind] = TrimmingRadius**2\n    \n    \n    return np.asarray(partition), np.asarray(dists), np.asarray(all_dists)\n\n\n\ndef find_branches( graph, verbose = 0 ):\n  '''\n  #' Computes \"branches\" of the graph, i.e. paths from branch vertex (or terminal vertex)  to branch vertex (or terminal vertex)\n  #' Can process disconnected graphs. Stand-alone point - is \"branch\".\n  #' Circle is exceptional case - each circle (can be several connected components) is \"branch\"\n  #'\n  #' @param g - graph (igraph) \n  #' @param verbose - details output\n  #' \n  #' @examples\n  #' import igraph\n  #' g = igraph.Graph.Lattice([3,3], circular = False ) \n  #' dict_output = find_branches(g, verbose = 1000)\n  #' print( dict_output['branches'] )\n  '''\n  #verbose = np.inf\n  #\n  g = graph\n  n_vertices_input_graph =   g.vcount()  \n  set_vertices_input_graph = set( range( n_vertices_input_graph  ) ) \n\n  dict_output = {}\n  #dict_output['branches'] = found_branches.copy()\n\n  # Main variables for process: \n  found_branches = []\n  processed_edges = []\n  processed_vertices = set()\n\n  ############################################################################################################################################\n  # Connected components loop:\n  count_connected_components = 0 \n  while True: # Need loop if graph has several connected components, each iteration - new component\n    count_connected_components += 1\n\n    def find_start_vertex(g, processed_vertices ): \n      '''\n      #' Find starting vertex for branches-search algorithm. \n      #' It should be either branching vertex (i.e. degree >2) or terminal vertex (i.e. degree 0 or 1), in special case when unprocessed part of graph is union of circles - processed outside function\n      '''\n      n_vertices = n_vertices_input_graph #  = g.count()# \n      if n_vertices == len( processed_vertices ):\n        return -1,-1 # All vertices proccessed\n      flag_found_start_vertex = 0 \n      for v in set_vertices_input_graph: \n        if v in processed_vertices: continue\n        if g.degree(v) != 2:\n          flag_found_start_vertex = 1\n          return v, flag_found_start_vertex\n      return -1, 0 # All unprocessed vertices are of degree 2, that means graph is circle of collection or collection of circles\n\n    ############################################################################################################################################\n    # Starting point initialization. End process condtion.\n    #\n    # Set correctly the starting vertex for the algorithm\n    # That should be branch vertex or terminal vertex, only in case graph is set of circles(disconnected) we take arbitrary vertex as initial, each circle will be a branch\n    initial_vertex, flag_found_start_vertex = find_start_vertex(g, processed_vertices )\n    if   flag_found_start_vertex > 0:\n      current_vertex  = initial_vertex\n    elif flag_found_start_vertex == 0: # All unprocessed vertices are of degree 2, that means graph is circle of collection or collection of circles\n      # Take any unprocessed element \n      tmp_set = set_vertices_input_graph  - processed_vertices\n      current_vertex = tmp_set.pop()\n    else:\n      # No vertices to process \n      if verbose >= 10:\n        print('Process finished')\n      dict_output['branches'] = found_branches.copy()\n      return dict_output\n      #break\n\n    ############################################################################################################################################\n    # Core function implementing \"Breath First Search\" like algorithm\n    # with some updates in storage, since we need to arrange edges into \"branches\"\n    def find_branches_core( current_vertex , previous_vertex, current_branch  ):\n      core_call_count[0] = core_call_count[0] + 1\n      if verbose >= 1000:\n        print(core_call_count[0], 'core call.', 'current_vertex', current_vertex , 'previous_vertex', previous_vertex,'found_branches',found_branches, 'current_branch',current_branch )\n\n      processed_vertices.add(current_vertex)\n      neis = g.neighbors(current_vertex) \n      if len(neis) == 0: # current_vertex is standalone vertex\n        found_branches.append( [current_vertex] )\n        return \n      if len(neis) == 1: # current_vertex is terminal vertex\n        if neis[0] == previous_vertex:\n          current_branch.append( current_vertex  )\n          found_branches.append( current_branch.copy() )\n          # processed_edges.append(  set([current_vertex , previous_vertex])  )  \n          return \n        else:\n          # That case may happen if we just started from that vertex\n          # Because it has one neigbour, but it is not previous_vertex, so it is None, which is only at start \n          current_branch = [current_vertex] # , neis[0] ] # .append( current_vertex  )\n          processed_edges.append(  set([current_vertex , neis[0] ])  )\n          find_branches_core( current_vertex = neis[0] , previous_vertex = current_vertex, current_branch = current_branch )  \n          return\n      if len(neis) == 2: # \n        # continue the current branch:\n        current_branch.append( current_vertex  )\n        next_vertex = neis[0]\n        if next_vertex == previous_vertex: next_vertex = neis[1]\n        if next_vertex in processed_vertices: # Cannot happen for trees, but may happen if graph has a loop\n          if set([current_vertex , next_vertex]) not in processed_edges:\n            current_branch.append( next_vertex  )\n            found_branches.append( current_branch.copy() )\n            processed_edges.append(  set([current_vertex , next_vertex])  )\n            return \n          else:\n            return\n        processed_edges.append(  set([current_vertex , next_vertex])  )          \n        find_branches_core( current_vertex=next_vertex , previous_vertex = current_vertex, current_branch = current_branch )\n        return\n      if len(neis)  > 2 : #Branch point\n        if  previous_vertex is not None:\n          # Stop current branch\n          current_branch.append( current_vertex  )\n          found_branches.append(current_branch.copy())\n        for next_vertex in neis:\n            if next_vertex ==  previous_vertex:    continue\n            if next_vertex in  processed_vertices: # Cannot happen for trees, but may happen if graph has a loop\n              if set([current_vertex , next_vertex]) not in processed_edges:\n                processed_edges.append(  set([current_vertex , next_vertex])  )\n                found_branches.append( [current_vertex, next_vertex ] )\n              continue\n            current_branch = [current_vertex]\n            processed_edges.append(  set([current_vertex , next_vertex])  )\n            find_branches_core( current_vertex = next_vertex , previous_vertex = current_vertex , current_branch = current_branch)\n      return\n\n    ############################################################################################################################################\n    # Core function call. It should process the whole connected component\n    if verbose >= 10:\n      print('Start process count_connected_components', count_connected_components, 'initial_vertex', current_vertex)\n    processed_vertices.add(current_vertex)\n    core_call_count = [0]\n    find_branches_core( current_vertex = current_vertex , previous_vertex = None , current_branch = [])\n\n    ############################################################################################################################################\n    # Output of results for connected component\n    if verbose >=10:\n      print('Connected component ', count_connected_components, ' processed ')\n      print('Final found_branches',found_branches)\n      print('N Final found_branches', len( found_branches) )\n\n\ndef branch_labler( X , graph, nodes_positions, verbose = 0 ):\n  '''\n  #' Labels points of the dataset X by \"nearest\"-\"branches\" of graph.\n  #' \n  #'\n  #' @examples\n  # X = np.array( [[0.1,0.1], [0.1,0.2], [1,2],[3,4],[5,0]] )\n  # nodes_positions = np.array( [ [0,0], [1,0], [0,1], [1,1] ]  ) \n  # import igraph\n  # g = igraph.Graph(); g.add_vertices(  4  )\n  # g.add_edges([[0,1],[0,2],[0,3]])\n  # vec_labels_by_branches = branch_labler( X , g, nodes_positions )\n  '''\n  #####################################################################################\n  # Calculate branches and clustering by vertices of graph \n  dict_output = find_branches(graph, verbose = verbose )\n  if verbose >=100:\n    print('Function find_branches results branches:',  dict_output['branches'] )\n  vec_labels_by_vertices, dists, all_dists = partition_data(X, nodes_positions) # np.array([[1,2,3,4], [1,2,3,4], [1,2,3,4], [10,20,30,40]]), [[1,2,3,4], [10,20,30,40]], 10**6)#,SquaredX)\n  vec_labels_by_vertices = vec_labels_by_vertices.ravel()\n  if verbose >=100:\n    print('Function partition_data returns: vec_labels_by_vertices.shape, dists.shape, all_dists.shape', vec_labels_by_vertices.shape, dists.shape, all_dists.shape )\n  #####################################################################################\n\n  n_vertices = len( nodes_positions)\n  branches = dict_output['branches']\n\n  #####################################################################################\n  # Create dictionary vertex to list of branches it belongs to  \n  dict_vertex2branches = {}\n  for i,b in enumerate( branches):\n    for v in b:\n      if v in dict_vertex2branches.keys():\n        dict_vertex2branches[v].append(i)\n      else:\n        dict_vertex2branches[v] = [i]\n  if verbose >=100:\n    print( 'dict_vertex2branches', dict_vertex2branches )\n\n\n  #####################################################################################\n  # create list of branch and non-branch vertices\n  list_branch_vertices = []\n  list_non_branch_vertices = []\n  for v in dict_vertex2branches:\n    list_branches = dict_vertex2branches[v]\n    if len(list_branches) == 1:\n      list_non_branch_vertices.append(v)\n    else:\n      list_branch_vertices.append(v)\n  if verbose >=100:  \n    print('list_branch_vertices, list_non_branch_vertices', list_branch_vertices, list_non_branch_vertices)\n\n  #####################################################################################\n  # First stage of creation of final output - create labels by branches vector \n  # After that step it will be only correct for non-branch points \n  vec_vertex2branch = np.zeros(  n_vertices  ) \n  for i in range( n_vertices  ):\n    vec_vertex2branch[i] = dict_vertex2branches[i][0]\n  vec_labels_by_branches = vec_vertex2branch[ vec_labels_by_vertices ] \n  if verbose >= 100:\n    print('branches', branches)\n    print('vec_labels_by_branches', vec_labels_by_branches)\n\n  #####################################################################################\n  # Second stage of creation of final output - \n  # make correct calculation for branch-vertices create labels by correct branches \n  for branch_vertex in list_branch_vertices:\n    if verbose >= 100:\n      print('all_dists.shape', all_dists.shape)\n    def labels_for_one_branch_vertex( branch_vertex , vec_labels_by_vertices,  all_dists ):\n      '''\n      #' For the branch_vertex re-labels points of dataset which were labeled by it to label by \"correct branch\".\n      #' \"Correct branch\" label is a branch 'censored'-nearest to given point. \n      #' Where 'censored'-nearest means the minimal distance between the point  and all points of the branch except the given branch_vertex\n      #'\n      #' Function changes vec_labels_by_branches defined above\n      #' Uses vec_labels_by_vertices defined above - vector of same length as dataset, which contains labels by vertices \n      '''\n\n      mask = vec_labels_by_vertices.ravel() == branch_vertex # Select part of the dataset which is closest to branch_vertex\n\n      # Allocate memory for array: first coordinate - point of dataset[mask],  second coordinate - branch number , for all branches contianing given vertex (i.e. branch_vertex) \n      # For each point of dataset[mask] it contains 'censored'-distances to \"branches\" adjoint to \"branch_vertex\", \n      # 'censored' means minimal over vertices belonging to  distance to branches (excluding branch_vertex)\n      dist2branches = np.zeros( [ mask.sum(), len(dict_vertex2branches[branch_vertex] )  ] )\n      list_branch_ids = [] # that will be necessary to renumerate local number to branch_ids \n      for i,branch_id in enumerate( dict_vertex2branches[branch_vertex] ):\n        list_branch_ids.append(branch_id)\n        # Create list of vertices of current branch, with EXCLUSION of branch_vertex\n        branch_vertices_wo_given_branch_vertex = [v for v in branches[branch_id] if v != branch_vertex ]\n        # For all points of dataset[mask] calculate minimal distances to given branch (with exclusion of branch_point), i.e. mininal difference for  \n        if verbose >= 1000:\n          print('mask.shape, all_dists.shape', mask.shape, all_dists.shape)\n        dist2branches[ : ,i ] = np.min( all_dists[mask,:][:,branch_vertices_wo_given_branch_vertex], 1 ).ravel()\n\n      vec_labels_by_branches[mask] = np.array(list_branch_ids)[ np.argmin( dist2branches, 1) ]\n    labels_for_one_branch_vertex( branch_vertex, vec_labels_by_vertices,  all_dists  )\n\n    if verbose >= 10:    \n      print('Output: vec_labels_by_branches', vec_labels_by_branches)\n\n\n  return vec_labels_by_branches\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nimport pandas as pd \n\ndef compare_clusterings(y, y_pred, X = None,  df_report = None, df_report_colomn_name = None, list_scores = 'All'):\n  if df_report is None:\n    df_report = pd.DataFrame()\n  if df_report_colomn_name  is None:\n    df_report_colomn_name  = df_report.shape[1]+1\n  \n  list_scores_ids_require_ground_truth = ['adjusted_rand_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score',\n                'homogeneity_score', 'completeness_score',  'v_measure_score', 'v_measure06_score', 'fowlkes_mallows_score',\n                   ]\n  list_scores_ids_not_require_ground_truth = ['inertia_score', 'silhouette_score', 'calinski_harabasz_score', 'davies_bouldin_score'  ]\n  if list_scores == 'All':\n    list_full = list_scores_ids_require_ground_truth + list_scores_ids_not_require_ground_truth\n  else:\n    list_full = list_scores\n\n    \n  def inertia_score(X, y):\n    b = X.copy()\n    for v in np.unique(y):\n      #print(v)\n      b[y==v,:] = np.mean(X[y==v,:],axis = 0) #[10,10])\n    #print(b)\n    return np.sum( np.sum( (X-b)**2 , 1) )  \n\n  s = ''\n  for score_id in list_full:\n    #print(score_id)\n    if (y is None) and (score_id in list_scores_ids_require_ground_truth):\n        continue\n    if score_id in list_scores_ids_require_ground_truth:\n      if score_id not in ['v_measure06_score']:\n        func_score  = getattr(metrics, score_id )\n        m = func_score(y, y_pred)\n      else:\n        m = metrics.v_measure_score(y, y_pred, beta = 0.6)\n    elif (score_id in list_scores_ids_not_require_ground_truth) and (X is not None):\n      if score_id not in ['inertia_score']:\n        if len(np.unique( y_pred )) > 1: # silu\n          func_score  = getattr(metrics, score_id )\n          m = func_score(X, y_pred)\n        else:\n          if score_id in ['silhouette_score',  'davies_bouldin_score']:\n            m = 0\n          elif score_id in ['calinski_harabasz_score']:\n            m = np.inf\n          else:\n            # raise ValueError('unexpected score id')\n            func_score  = getattr(metrics, score_id )\n            m = func_score(X, y_pred)\n      elif X is not None:\n        m = inertia_score(X, y_pred)\n    else:\n      continue\n    str_score_inf = score_id.replace('_score','') \n    #print(str_score_inf)\n    s += str_score_inf + ' ' + ('%0.3f')%(m) + '\\n'\n    df_report.loc[str_score_inf, df_report_colomn_name ] = m\n\n  return s,df_report \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculation and analysis of unsupervised trajectory scores"},{"metadata":{},"cell_type":"markdown","source":"## MST"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt0 = time.time()\ndf_stat = pd.DataFrame()\nc = 0\nfig = plt.figure(figsize = (25,10) )\nfor n_nodes in range(3,30): #  [2,25]: # range(0):\n  c+=1\n  dict_result =  create_tree_by_cluster_knn_mst(X, n_clusters = n_nodes, n_neighbors=n_nodes)# - Calcuates a tree for given dataset, by kmeans+knn+mst\n  edges =  dict_result['edges_mst']\n  nodes_positions = dict_result['nodes_positions']\n\n  g = igraph.Graph()\n  g.add_vertices( len (nodes_positions ))\n  g.add_edges(edges )\n  vec_labels_by_branches = branch_labler( X , g, nodes_positions )\n  case_id = n_nodes # 'n_nodes '+str(n_nodes) # Combined'+' Noise sigma' +str( sigma_noise ) + ' n_nodes_mst'+str(n_mst_nodes) + ' n_nodes' + str(k) + ' trial'+str(cc)\n  s,df_stat = compare_clusterings(None,vec_labels_by_branches, X,  df_stat, case_id )\n  df_stat.loc['n_branch pnts', case_id ] = np.sum(np.array(g.degree()) >= 3)\n  df_stat.loc['n_clusters', case_id ] = len(np.unique(vec_labels_by_branches))\n  df_stat.loc['n_components', case_id ] = len( list( g.components() ) )\n  print('n_nodes=',n_nodes, time.time() - t0, 'seconds passed')\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (25,5) ) \nplt.suptitle('MST Trajectories. Unsupervised metrics.')\n\nc = 0 \nlist_data_names = ['silhouette', 'calinski_harabasz', 'davies_bouldin'] #, 'inertia'  ]\nsubplots_number = len(list_data_names )\nfor data_name in list_data_names :\n  #print()\n  #print(   data_name ) \n  c+=1\n  fig.add_subplot(1, subplots_number , c) \n  v = df_stat.loc[data_name,:]\n  #v.plot()\n  plt.plot(v,'-*')\n  plt.title(data_name)\n  plt.xlabel('nodes number in graph')\n \n  #print(df_stat.columns[v.argmax()] )\n  #print( v.sort_values().tail(3) )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ElPiGraph"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nt0 = time.time()\ndf_stat = pd.DataFrame()\nc = 0\nfig = plt.figure(figsize = (25,10) )\nfor n_nodes in range(3,30): #  [2,25]: # range(0):\n  c+=1\n  tree_elpi = elpigraph.computeElasticPrincipalTree(X , NumNodes=n_nodes, \n      alpha=0.01,FinalEnergy='Penalized', StoreGraphEvolution = True )#,\n\n  nodes_positions = tree_elpi[0]['NodePositions'] # ['AllNodePositions'][k]\n  matrix_edges_weights = tree_elpi[0]['ElasticMatrix'] # ['AllElasticMatrices'][k]\n  matrix_edges_weights = np.triu(matrix_edges_weights,1 )\n  edges = np.array( np.nonzero(matrix_edges_weights), dtype = int ).transpose()\n\n  g = igraph.Graph()\n  g.add_vertices( len (nodes_positions ))\n  g.add_edges(edges )\n  vec_labels_by_branches = branch_labler( X , g, nodes_positions )\n  case_id = n_nodes # 'n_nodes '+str(n_nodes) # Combined'+' Noise sigma' +str( sigma_noise ) + ' n_nodes_mst'+str(n_mst_nodes) + ' n_nodes' + str(k) + ' trial'+str(cc)\n  s,df_stat = compare_clusterings(None,vec_labels_by_branches, X,  df_stat, case_id )\n  df_stat.loc['n_branch pnts', case_id ] = np.sum(np.array(g.degree()) >= 3)\n  df_stat.loc['n_clusters', case_id ] = len(np.unique(vec_labels_by_branches))\n  df_stat.loc['n_components', case_id ] = len( list( g.components() ) )\n  print('n_nodes=',n_nodes, time.time() - t0, 'seconds passed')\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (25,5) ) \nplt.suptitle('ElPiGraph Trajectories. Unsupervised metrics.')\nc = 0 \nlist_data_names = ['silhouette', 'calinski_harabasz', 'davies_bouldin'] #, 'inertia'  ]\nsubplots_number = len(list_data_names )\nfor data_name in list_data_names :\n  #print()\n  #print(   data_name ) \n  c+=1\n  fig.add_subplot(1, subplots_number , c) \n  v = df_stat.loc[data_name,:]\n  #v.plot()\n  plt.plot(v,'-*')\n  plt.title(data_name)\n  plt.xlabel('nodes number in graph')\n \n  #print(df_stat.columns[v.argmax()] )\n  #print( v.sort_values().tail(3) )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MST plots trajectories for 3-20 nodes\n\ncolored by graph segments\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt0 = time.time()\nc = 0\nfig = plt.figure(figsize = (25,6) )\nfor n_nodes in range(3,20): #  [2,25]: # range(0):\n  c+=1\n  dict_result =  create_tree_by_cluster_knn_mst(X, n_clusters = n_nodes, n_neighbors=n_nodes)# - Calcuates a tree for given dataset, by kmeans+knn+mst\n  edges =  dict_result['edges_mst']\n  nodes_positions = dict_result['nodes_positions']\n\n  g = igraph.Graph()\n  g.add_vertices( len (nodes_positions ))\n  g.add_edges(edges )\n  vec_labels_by_branches = branch_labler( X , g, nodes_positions )\n  #print('n_nodes',n_nodes,time.time() - t0, 'seconds passed')\n\n  fig.add_subplot(1,4,c)\n  plot_graph(edges, nodes_positions, data = X, data_color = vec_labels_by_branches )\n  #plt.title(n_clusters)\n  plt.title('Trajectories MST ' + str(n_nodes)+' nodes');\n  #plt.show()  \n\n  if c%4 == 0:\n    plt.show()\n    fig = plt.figure(figsize = (25,6) )\n    c = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ElPiGraph Plots trajectories for 3-20 nodes\n\ncolored by graph segments"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nc = 0\nfig = plt.figure(figsize = (25,6) )\nfor n_nodes in range(3,20): #  [2,25]: # range(0):\n  c+=1\n  tree_elpi = elpigraph.computeElasticPrincipalTree(X , NumNodes=n_nodes, \n      alpha=0.01,FinalEnergy='Penalized', StoreGraphEvolution = True )#,\n\n  nodes_positions = tree_elpi[0]['NodePositions'] # ['AllNodePositions'][k]\n  matrix_edges_weights = tree_elpi[0]['ElasticMatrix'] # ['AllElasticMatrices'][k]\n  matrix_edges_weights = np.triu(matrix_edges_weights,1 )\n  edges = np.array( np.nonzero(matrix_edges_weights), dtype = int ).transpose()\n\n  g = igraph.Graph()\n  g.add_vertices( len (nodes_positions ))\n  g.add_edges(edges )\n  vec_labels_by_branches = branch_labler( X , g, nodes_positions )\n  #print('n_nodes',n_nodes,time.time() - t0, 'seconds passed')\n\n  fig.add_subplot(1,4,c)\n  plot_graph(edges, nodes_positions, data = X, data_color = vec_labels_by_branches )\n  #plt.title(n_clusters)\n  plt.title('Trajectories ElPiGraph ' + str(n_nodes)+' nodes');\n  #plt.show()  \n\n  if c%4 == 0:\n    plt.show()\n    fig = plt.figure(figsize = (25,6) )\n    c = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}