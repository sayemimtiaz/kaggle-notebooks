{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Edit, 05-Jan-2021, Deal with imbalancing data by oversampling -> accuracy, precision, recall, auc socre, and roc curve are improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport sys\nfrom imblearn.over_sampling import SMOTE\n\nwarnings.filterwarnings(\"ignore\")\nsns.set()\nnp.set_printoptions(threshold=sys.maxsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndata.drop('enrollee_id', axis=1, inplace=True)\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- enrollee_id : Unique ID for candidate\n- city: City code\n- city_ development _index : Developement index of the city (scaled)\n- gender: Gender of candidate\n- relevent_experience: Relevant experience of candidate\n- enrolled_university: Type of University course enrolled if any\n- education_level: Education level of candidate\n- major_discipline :Education major discipline of candidate\n- experience: Candidate total experience in years\n- company_size: No of employees in current employer's company\n- company_type : Type of current employer\n- lastnewjob: Difference in years between previous job and current job\n- training_hours: training hours completed\n\ntarget: 0 – Not looking for job change, 1 – Looking for a job change"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(data=data, x='target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imbalanced dataset\nA problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary.\n\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model. \n\nInstead, new samples can be synthesized from the existing samples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and creating a new sample at a point along that line."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_classification\nX_ex, y_ex = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=111)\n\ndata_ex = pd.DataFrame(np.concatenate([X_ex, y_ex.reshape(-1,1)],axis=1))\n\nzero = data_ex[data_ex[2]==0.0]\none = data_ex[data_ex[2]==1.0]\n\nX_ex_oversampled, y_ex_oversampled = SMOTE().fit_resample(X_ex, y_ex)\ndata_ex_oversampled = pd.DataFrame(np.concatenate([X_ex_oversampled, y_ex_oversampled.reshape(-1,1)],axis=1))\n\nzero_oversampled = data_ex_oversampled[data_ex_oversampled[2]==0.0]\none_oversampled = data_ex_oversampled[data_ex_oversampled[2]==1.0]\n\nfig, ax = plt.subplots(1,2, sharey=True)\nfig.set_size_inches(13,5)\nax[0].scatter(zero[0], zero[1], label='class 0')\nax[0].scatter(one[0], one[1], label='class 1')\nax[0].set_title('Original Data')\nax[0].set_ylabel('feature_1')\nax[0].set_xlabel('feature_0')\nax[1].scatter(zero_oversampled[0], zero_oversampled[1], label='class 0')\nax[1].scatter(one_oversampled[0], one_oversampled[1], label='class 1')\nax[1].set_title('Oversampled Data')\nax[1].set_xlabel('feature_0')\nplt.legend(bbox_to_anchor=(1.3,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\"\n\n[reference](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)"},{"metadata":{},"cell_type":"markdown","source":"## Be aware of imbalancing, we'll deal with it later."},{"metadata":{},"cell_type":"markdown","source":"**First, I'll separate categorical columns and numerical columns into two DataFrame named \"category\", \"numeric\" respectively.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = data.select_dtypes(exclude='object')\ncategory = data.select_dtypes(include='object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Missing values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"missing = pd.DataFrame(data.isnull().sum()/len(data), columns=['Missing'])\n\ncm = sns.light_palette(\"green\", as_cmap=True)\nmissing.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deal with NaN in categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"category_notNull = category.fillna('No')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean the data a little bit."},{"metadata":{"trusted":true},"cell_type":"code","source":"category_notNull['company_size'] = category_notNull['company_size'].replace('10/49', '10-49')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Encode categorical features"},{"metadata":{},"cell_type":"markdown","source":"## 2.1) Ordinal encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nOrdinal_encoder = OrdinalEncoder([\n    ['No', 'Primary School',  'High School', 'Graduate', 'Masters', 'Phd'],\n    'No,<1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,>20'.split(','),\n    ['No','<10', '10-49', '50-99', '100-500', '500-999' ,'1000-4999', '5000-9999', '10000+'],\n    ['No','1', '2', '3', '4', '>4', 'never']\n])\n\ncategory_notNull_ordinal = category_notNull[['education_level', 'experience', 'company_size', 'last_new_job']]\n\ncategory_notNull_ordinalEncoded = Ordinal_encoder.fit_transform(category_notNull_ordinal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2) One hot encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\none_how_columns = [ col for col in category_notNull.columns if col not in ['education_level', 'experience', 'company_size', 'last_new_job']]\n\nohe = OneHotEncoder(sparse=False).fit(category_notNull.loc[:, one_how_columns])\n\ncategory_notNull_onehotEncoded = ohe.transform(category_notNull.loc[:, one_how_columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Splitting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ncategory_preprocessed = np.concatenate([category_notNull_onehotEncoded, category_notNull_ordinalEncoded], axis=1)\n\nX = np.concatenate([numeric.drop('target', axis=1).values, category_preprocessed], axis=1)\ny = numeric['target'].values\n\n\nX, y = SMOTE().fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[:5,:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First two columns are numerical data, others is encoded data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train[:,:2])\n\nX_train_scaled = X_train.copy()\nX_train_scaled[:,:2] = scaler.transform(X_train[:,:2])\n\nX_test_scaled = X_test.copy()\nX_test_scaled[:,:2] = scaler.transform(X_test[:,:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Building some models. <br>\nI'll use default parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, roc_auc_score, roc_curve \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nestimators = {\n    'Logistic Regression': [LogisticRegression()],\n    'Decision Tree' :[DecisionTreeClassifier()],\n    'Random Forest' :[RandomForestClassifier()],\n    'Gradient Boost' :[GradientBoostingClassifier()],\n    'XG Boost': [XGBClassifier()],\n}\n\n\ndef mfit(estimators, X_train, y_train):\n    for m in estimators:\n        estimators[m][0].fit(X_train, y_train)\n        print(m+' fitted')\n\nmfit(estimators, X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) Let's predict!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef mpredict(estimators, X_test, y_test):\n    outcome = dict()\n    r_a_score = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test)\n        r_a_score[m] = roc_auc_score(y_test, y_pred)\n        outcome[m] = [y_pred, confusion_matrix(y_pred,y_test), classification_report(y_pred,y_test)]\n    return outcome, r_a_score\n\noutcome, r_a_score = mpredict(estimators, X_test_scaled, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for m in outcome:\n    print('------------------------'+m+'------------------------')\n    print(outcome[m][1])\n    print(outcome[m][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc_auc_score')\nfor m in r_a_score:\n    print('------------------------'+m+'------------------------')\n    print(r_a_score[m])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking to ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(13,6)\n\nfor m in estimators:\n    y_pred = estimators[m][0].predict_proba(X_test_scaled)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=m)\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}