{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Bank Churn Prediction Modeling\n## What is Churn?\nDefined loosely, churn is the process by which customers cease doing business with a company. In this particular project, bank churn is how many customers left our bank.\n\nThis can be measured based on actual usage or failure to renew (when the product is sold using a subscription model). Often evaluated for a specific period of time, there can be a monthly, quarterly, or annual churn rate.\n\nWhen new customers begin buying and/or using a product, each new user contributes to a product’s growth rate. Inevitably some of those customers will eventually discontinue their usage or cancel their subscription; either because they switched to a competitor or alternative solution, no longer need to product’s functions, they’re unhappy with their user experience, or they can no longer afford or justify the cost. The customers that stop using/paying are the “churn” for a given period of time.\n\n## Why is churn prediction important? \nPreventing a loss in profits is one clear motivation for reducing churn, but other subtleties may underlie a company’s quest to quell it. Most strikingly, the cost of customer acquisition usually starkly outweighs that of customer retention, so stamping out churn also compels from a more subtle financial perspective. \n\nWhile churn presents an obvious difficulty to businesses, its remedy is not always immediately clear. In many cases, and without descriptive data, companies are at a loss as to what drives it. Luckily, machine learning provides effective methods for identifying churn’s underlying factors and proscriptive tools for addressing it.\n\n## 1.1 Problem Statement\n>Just like any other business fratenity, a local bank has been seeing some customers leaving them at a rather increased rate and have tasked us with investigating which factors contribute to this customer churn, predict and offer some recommendations against the churn. So, **the objective** of this project ecompasses the following, in no particular order:\n>* To identify factors that fuel customer churn based on available features;\n>* To visualize those factors that lead to churn;\n>* Later we want to build a churn predictive model which will perform the following tasks:\n     >> - Identify customers who pose a risk of leaving the bank\n     >> - Use the model on each current customer to predict whether they are at risk of leaving\n> ### Possible Questions to ask\n>- What factors influence customer churn\n>- Does Data Visualization offer better insights into the data?\n>- What hidden patterns can we reveal from the dataset?\n>- Data segnmentation Analysis: Which group seems to be churning more? $e.t.c.$\n\n## 1.2 About The Dataset\n>The dataset we will use in this project can be found [here](https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers). We will dive deeper into the features involved in our data.\n\n ## 1.3 Methodology\n>- **Step 1- Data collection:** This will involve scrapping of structured datafrom the source.\n>- **Step 2- Data Preprocessing:** In this phase, the data is prepared for the analysis purpose which contains relevant information. **Pre-processing** and **data cleaning** are some of the most important tasks that must be done before dataset can be used for machine learning. The real-world data is _noisy_, _incomplete_ and _inconsistent_. So, it is of paramount importance to clean it for optimized and realiable machine learning models.\n>- **Step 3- Feature Extraction Set/Training Data:** Feature set or training data can be prepared from the cleaned data by using any of the available\ntechniques. The feature sets and training set that has obtained by using any method will be used for the implementation of machine learning algorithms.\n>- **Step 4- Implementation of Machine Learning Algorithm on Feature Set/Training Data:** \n>- **Step 5: Testing of Data:** Testing of data is done based on training model which is classified using supervised learning algorithm.\n\n ## 1.4 Experimental Design\n>- We will use static data- dataset already available rather than real-time data from an IoT system.\nLinks to the Datasets\n\n## 1.5 Software tools & Hardware Requirements: \n>- Jupyter Notebook, \n>- Python3 Liraries such as [NumPy](https://numpy.org/doc/stable/user/quickstart.html), [Pandas](https://pandas.pydata.org/docs/), [Matplotlib](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) and [Seaborn](https://seaborn.pydata.org/) \n>- Supervised Learning libraries such [Scikit-Learn](https://scikit-learn.org/stable/)\n  ... will be exploited for the development and experimentation of the project. \n  ","metadata":{}},{"cell_type":"markdown","source":"---\n---\n# 2. Data Collection & Loading\n>The first step in the Machine Learning process is getting data.\n > ## 2.1 Importing Neccessary Libraries","metadata":{}},{"cell_type":"code","source":"# For data cleaning/wrangling\nimport numpy as np      # vectors and matrices || Linear Algebra\nimport pandas as pd     # tables and data manipulations \n\n# Visualization Libraries\nimport matplotlib.pyplot as plt # plots\nimport seaborn as sns           # attractive plots\n#plt.style.use('fivethirtyeight')\n# Above is a special style template for matplotlib, highly useful for visualizing time series data\n#sns.set(palette=\"Set2\")\n\n# To have graphs embedded in the notebook\n%matplotlib inline\n\n# Big Data Libraries\nimport tensorflow as tf\n#import keras\n\n\n# For better tables outputs\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\n# Models Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, f1_score,average_precision_score, confusion_matrix,\n                             average_precision_score, precision_score, recall_score, roc_auc_score, )\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\n#import xgboost as xgb\nfrom xgboost import XGBClassifier, plot_importance\nfrom imblearn.over_sampling import SMOTE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 2.2 Data Ingestion","metadata":{}},{"cell_type":"code","source":"# Read the csv file\ndf = pd.read_csv(\"../input/bank-customers/Churn Modeling.csv\", delimiter=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Preprocessing\n>#### What is it?\n>- According to [Techopedia](https://www.techopedia.com/definition/14650/data-preprocessing), **Data Preprocessing** is a Data Mining technique that involves transforming raw data into an understandable format.\n>#### Why Do We Need Data Preprocessing?\n>- Since real-world data tends to be incomplete, noisy or inconsistent, we need this process in order to eliminate such to avoid poor quality models we'll build later. \n>- Data Preprocessing provides operations which can organise the data into a proper form for better understanding in data mining process.\n>#### Stages of Data Preprocessing\nData preprocessing is divided into four stages: \n    1. Data cleaning, \n    2. Data integration, \n    3. Data reduction, and \n    4. Data transformation.\n    \n> ## 3.1 Data Cleaning & Inspection\n>#### What is Data Cleaning?**\n>In Data Science, data cleaning can be described in many ways, one of them being: \n<font color=purple>***the process of fixing and/or removing incorrect, corrupted, wrongly-formatted, incomplete or duplicate data within a dataset***.</font>\n\n>Quite mouthful and yet comprehensive. We want to ensure that our dataset has no duplicate data or does not contain any corrupted entries that will otherwise lead to wrong/less useful models. \n\n>As we make use of multiple data sources in data analysis, the chances of duplicating data are very high, and so are those of mislabeling the data. It goes without saying that incorrect data leads to unreliable algorithms and predictions or outcomes. While there are no sure steps of going about data cleaning due to the nature of different datasets, it is however vital to build some sort of a framework or template for data cleaning process for future references so as to at least be close to doing it right each time.\n\n>> ### 3.1.1 Exploring the Dataset: *Understanding the data*\n With the data fed into our notebook, it's time we take a look at it: we need to spend some time exploring it in order to understand what the features represent in each column. We want to avoid or at least minimize mistakes in the data analysis and the modeling process. \n\n>>Without any further ado, let's dive right into our loaded dataset.\n","metadata":{}},{"cell_type":"code","source":"# Check the first 5 entries\ndf.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">> ### 3.1.2 Featues of the Dataset:\n>> #### Columns in the dataset","metadata":{}},{"cell_type":"code","source":"# Print out columns in the dataset\npd.DataFrame(df.columns, columns=[\"Column Names\"]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">> #### <CENTER>Column Descriptions</CENTER>\n>>|$RowNumber$ | corresponds to the record (row) number and has no effect on the output. This column will be removed. |\n| :-| :-|\n| $CustomerId$ | contains random values and has no effect on customer leaving the bank. This column will be removed\n|$Surname$|the surname of a customer has no impact on their decision to leave the bank. This column will be removed.\n|$CreditScore$|can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n|$Geography$|a customer's location can affect their decision to leave the bank. We'll keep this column.\n|$Gender$|it's interesting to explore whether gender plays a role in a customer leaving the bank. We'll include this column, too.\n|$Age$|this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n|$Tenure$|refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n|$Balance$|also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n|$NumOfProducts$|refers to the number of products that a customer has purchased through the bank.\n|$HasCrCard$|denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n|$IsActiveMember$|active customers are less likely to leave the bank, so we'll keep this.\n|$EstimatedSalary$|as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n|$Exited$|whether or not the customer left the bank. This is what we have to predict.\n| :-| :-|\n\n>> ### 3.1.3 Size of the dataset: How many entries are in our dataset?","metadata":{}},{"cell_type":"code","source":"# How many rows and columns in the dataset?\n# Check the number of rows and columns\nprint(\"There are \", df.shape[0], \"rows and\", df.shape[1], \"columns/features.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $Observation$\n- We have $10000$ entries (subjects/users) and $14$ features (columns).\n\n>> ### 3.1.4 Missing Data: \n- **Do we have missing data under each feature?**\n","metadata":{}},{"cell_type":"code","source":"# Check if there's missing data\n# Turn it into a dataframe\npd.DataFrame(df.isnull().sum(), columns=[\"Number of Missing Values\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $Observation$\n- It looks like our dataset has no missing values.\n\n>> ### 3.1.5 Statistical Description of the dataset\n- **Looking at only Numerical Columns, what are some basic statistical observations?**\n","metadata":{}},{"cell_type":"code","source":"# Describe the statistical elements of the dataset\ndf.describe().T#.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $Observation$\n>The above returns important descriptive statistical summaries for only numerical columns, excluding NaN values (if available).\n> #### Parameters:\n> * **percentiles:**The percentiles to include in the output. All should fall between 0 and 1. The default is <font color=darkpink>[.25, .5, .75]</font>, which returns the 25th, 50th, and 75th percentiles.\n\n>> ### 3.1.6 Statistical Description of the dataset\n- **Looking at only Categorical Columns, what are some basic statistical observations?**\n","metadata":{}},{"cell_type":"code","source":"# Just categorical columns\ndf.describe(include=[object])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Observation$\n>The above returns important descriptive statistical summaries for only columns that hold object datatype (categorical in nature), excluding NaN values (if available).\n---\n---\n# 4. Exploratory Data Analysis\n## <center>Multivariate Data Analysis</center>\n>According to a very insightful article by Prasad Patil on [Towards Data Science](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15), Mr Patil stated that\n> <font color = blue>\n    <i>Exploratory data analysis is simply the process of performing initial investigations on data to discover pattern, anomalies with the aid of graphical representations and summary statistics</i>\n</font>\n\n>It is worth noting that EDA is not really a formal process with rigid set of rules or path to follow, it is waht one makes it. The aim is to uncover whatever may be hidden in the data, so one should feel free to investigate whatever idea that comes to mind. Of course some ideas will yield some positive outcomes, others not so much. To successfully perform data cleaning, we'll need to deploy EDA tools such as visualisation, transformation and modelling. \n\n> ## 4.1 Dropping Unnecessary Columns\n> - We can see that `RowNumber` and `CustomerID` columns are unnecessary and can be dropped.\n> - The same applied to the `Surname` column as we are not interested in who left the bank but other factors that led them to leaving.","metadata":{}},{"cell_type":"code","source":"# Drop and reassign dataframe\ndf = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1 )\n\n# View first few 5 rows\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 4.1 Credit Score\n> Let us try to see the relationship of credit score and exit status:\n> - #### Do customers with lower credit scores exit more?\n> - #### Do customers with higher credit scores tend to stay with our bank?\n\n>> ### 4.1.1 Creating Credit Score Based On FICO ranges\n>>The most commonly used scoring models have a credit score range of 300 to 850. Creditors set their own standards for what scores they'll accept, but these are general guidelines:\n>> - A score of 720 or higher is generally considered excellent credit.\n>> - A score between 690 and 719 is considered good credit.\n>> - Scores between 630 and 689 are fair credit.\n>> - And scores below 629 are poor credit.","metadata":{}},{"cell_type":"code","source":"df['CreditScoreGroup'] = pd.cut(df.CreditScore, \n                           bins = [300, 579, 669, 739, 799, 850],\n                           labels=[0, 1, 2, 3, 4]) # 0=Very Poor, 1 = Fair, 2=Good, 3=Very Good, 4=Exceptional\ndf['CreditScoreGroup'] = df.CreditScoreGroup.astype(int)\ndf.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"very_poor = df.query('CreditScore >= 300 and CreditScore <= 579')#.count()\n\nfair = df.query('CreditScore >= 580 and CreditScore <= 669')#[\"CreditScore\"]#.max()#.count()\n\ngood = df.query('CreditScore >= 670 and CreditScore <= 739')#.count()\n\nvery_good = df.query('CreditScore >= 740 and CreditScore <= 799')#.count()\n\nexceptional = df.query('CreditScore >= 800 and CreditScore <= 850')#.count()\n\n# Print Number of Customers in different FICO Score Ranges \nprint(\"There are\", very_poor[\"CreditScore\"].count(), \"customers with very poor credit score.\")\nprint(\"There are\", fair[\"CreditScore\"].count(), \"customers with fair credit score.\")\nprint(\"There are\", good[\"CreditScore\"].count(), \"customers with good credit score.\")\nprint(\"There are\", very_good[\"CreditScore\"].count(), \"customers with very good credit score.\")\nprint(\"There are\", exceptional[\"CreditScore\"].count(), \"customers with exceptional credit score.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn')\nplt.figure(figsize=(15,10))\nplt.suptitle(\"Customers' Credit Score vs. Churn Status\", fontname ='Times New Roman', \n          size = 30, color ='m')\n\nplt.subplot(2,2,1)\nplt.hist(df[df[\"Exited\"]==1][\"CreditScore\"], bins=25, alpha=0.9, color=\"red\", label=\"Exited\")\nplt.hist(df[df[\"Exited\"]==0][\"CreditScore\"], bins=25, alpha=0.5, color=\"blue\", label=\"Did Not Exit\")\n\nplt.xlabel(\"Credit Score\", size = 15, color = \"purple\")\nplt.ylabel(\"Count of customers\", size = 15, color = \"purple\")\nplt.title(\"Credit Score vs. Churn Status\", color=\"red\", fontname = \"Times New Roman\", size = 20)\nplt.legend();\nplt.subplot(2,2,2)\n# Bar plot of Different Credit Score Group Vs Exited feature\nsns.countplot(x = 'CreditScoreGroup', data = df[df[\"Exited\"] == 1])\nplt.xlabel(\"Credit Score Group\", size = 15, color = \"purple\")\nplt.ylabel(\"Count of customers\", size = 15, color = \"purple\")\nplt.title(\"Credit Score Groups vs. Churn Status\", color='red', fontname = \"Times New Roman\", size = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Observation$\n- We see that most customers who exited the bank had very poor and fair credit score.","metadata":{}},{"cell_type":"markdown","source":"> ## 4.2 Retained $vs.$ Churned Customers Percentage","metadata":{}},{"cell_type":"code","source":"plt.style.use('seaborn')\n\nchurned = pd.DataFrame(df['Exited'].value_counts())\n\n#plt.subtitle(\"Retained vs Lost Customers Percentage Distribution \", fontname ='Times New Roman', \n          #size = 22, color ='purple')\n\n# CountPlot\nsns.countplot(x='Exited', data=df, palette=\"Set1\")\nplt.style.use('seaborn')\n# Pie Chart \nexplode = (0.0, 0.1)\ncolors = ['yellow', 'magenta']\nlabels = 'Retained', 'Lost'\nchurned.plot.pie(y='Exited', figsize=(8,8), colors = colors, \n                 autopct='%1.2f%%', textprops={'color':\"b\"}, explode=explode, \n                 startangle=90, shadow=True, labels=labels, wedgeprops={'edgecolor': 'red'})\nplt.show()\nchurned","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n- <font color='blue'>$\\implies$</font>We see that we lost about 20% of the customers (2037 out of 10000) and retained about 80%(7963 customers out of 10000).\n- Our goal is to predict churn rate, so it is important that whatever model we choose at the end predicts this 20%.","metadata":{}},{"cell_type":"markdown","source":"> ## 4.3  Demographic Segmentation Analysis & Churn\n> ### 4.3.1 Geographical Segmentation Analysis\n> - #### Which countries do the bank clients come from?","metadata":{}},{"cell_type":"code","source":"plt.style.use('seaborn')\nplt.figure(figsize=(15,9))\n\n# Customers in Each Country\nclients = pd.DataFrame(df.Geography.value_counts())\n\n# Set Title for both subplots\nplt.suptitle(\"Customers Geographical Distribution \", size = 30, \n             fontname = \"Times New Roman\", color = \"m\")\n\n# Count Plot\n#plt.subplot(2, 2, 1)\nsns.countplot(x=\"Geography\", data = df, palette=\"gnuplot\")\nplt.xlabel(\"Countries\", size = 18, fontname = \"Times New Roman\", color = \"k\")\nplt.ylabel(\"Count of Customers in each country\", size = 14, fontname = \"Times New Roman\", color = \"k\")\n\n# Pie Chart\n#plt.subplot(2, 2, 2)\n#explode = (0.01, 0.02, 0.1)\n#labels = 'France', 'Spain', 'Germany'\n#pieplot = plt.pie(clients, autopct = '%1.0f%%', explode = explode, startangle = 90,\n#                  textprops={'color':\"w\"}, colors = ['indigo', 'maroon', 'brown'],\n #                 labels=labels)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> - #### How many Customers from each Country?","metadata":{}},{"cell_type":"code","source":"# Call the dataframe\nclients ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### $Analysis$\n> - The customers belong to three different geographical locations: France, Spain, and Germany. While almost 50% of the total customers are French, the number of Spanish and German customers is around 25% each.\n\n> ### Geographical Segmentation & Churn\n> - #### Which country had the highest churn rate?\n$\\implies$ To find the impact of geography on customer churn, we can plot the count of values from the **`Exited`** column against a customer’s geography:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.suptitle(\"Geography & Churn Analysis\", size = 25, fontname = \"Times New Roman\", color = \"m\")\n\n# First Plot\nplt.subplot(2, 2, 1)\nsns.countplot(x='Exited', hue='Geography', data=df, palette=\"gnuplot\")\n\n# 2nd Plot\nplt.subplot(2, 2, 2)\nsns.countplot(x='Geography', hue='Exited', data=df, palette=\"rainbow_r\")\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n- We see that the bank lost more customers in Germany than elsewhere among the 3 involved nations, despite 50% of the customers hailing from France, and Germany housing the least number of customers.\n>- That is to say, Germany has the least amount of customers for this bank, but more customers left the bank.\n>- Perhaps the bank is not performing so well in Germany that it continues to lose customers.\n>- We can also assume this is a French bank, and that customers are sticking with the local banks and are also defecting to local banks in Germany.\n>- The output shows that among the customers who left the bank, French and German customers have an equal number despite the fact that the total German customers are almost half of the total French customers.\n> - It shows that German customers are more likely to leave the bank than French and Spanish customers. \n","metadata":{}},{"cell_type":"code","source":"# Get Customers From Spain\nspain_bank = df.query('Geography == \"Spain\"')\nfrance_bank = df.query('Geography == \"France\"')\ngermany_bank = df.query('Geography == \"Germany\"')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go \nfrom plotly.offline import init_notebook_mode,iplot,plot\ninit_notebook_mode(connected=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dict(\n        type = 'choropleth',\n        colorscale = 'rainbow',\n        locations = df['Geography'],\n        locationmode = \"country names\",\n        z = df['Exited'],\n        text = df['Geography'],\n        colorbar = {'title' : 'Customers'},\n      )\n\nlayout = dict(title = 'Customer Locations',\n              geo = dict(projection = {'type':'mercator'})\n             )\n\nchoromap = go.Figure(data = [data],layout = layout)\niplot(choromap,validate=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Geographical Churn Rate Analysis\n- Let us take a lot at churn rates for both genders, and overall churn rate.","metadata":{}},{"cell_type":"code","source":"# Get total Churn\ntotal_churn = df.query('Exited == 1').count()[\"Exited\"]\n##################################################################################################################\n# Get Geography And Churn Status\n# France Churn \nfrance_churn_number = df.query('Geography == \"France\" & Exited == 1').count()[\"Exited\"]\n# France Churn Rate\nfrance_churn_rate = round((france_churn_number/total_churn)*100, 2)\n\n# Germany Churn \ngermany_churn_number = df.query('Geography == \"Germany\" & Exited == 1').count()[\"Exited\"]\n# Germany Churn Rate\ngermany_churn_rate = round((germany_churn_number/total_churn)*100, 2)\n\n# Germany Churn \ngermany_churn_number = df.query('Geography == \"Germany\" & Exited == 1').count()[\"Exited\"]\n# Germany Churn Rate\ngermany_churn_rate = round((germany_churn_number/total_churn)*100, 2)\n\n# Spain Churn \nspain_churn_number = df.query('Geography == \"Spain\" & Exited == 1').count()[\"Exited\"]\n# Spain Churn Rate\nspain_churn_rate = round((spain_churn_number/total_churn)*100, 2)\n\n##################################################################################################################\n# Create Churn Rate DataFrame\n# Generate List of Lists\ngeo_churn_data = [['France', france_churn_rate], ['Germany', germany_churn_rate], ['Spain', spain_churn_rate]]\n\n# Create the Pandas DataFrame\ngeo_churn_df = pd.DataFrame(geo_churn_data, columns = [\"Country\", \"Churn Rate (%)\"])\n# Call the dataframe\ngeo_churn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n>- Spain saw the least churn rate, while (as we said) Germany had the highest churn rate. \n>- In fact, we see something new- Germany and France had almost the same churn rates  but we could argue that for France that's because there are more bank users based there than elsewhere. ","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.2  Gender Segmentation Analysis\n> We want to understand the Gender groups of the customers of the bank and see which gender seems likely to leave the bank (if it can be uncovered from the data).\n> ####  Gender, Geography and Number of Churned customers\n> - Let's dive into how many females or males left the bank and from which country did we see the highest rate of churn.\n> - We also want to see if males or females are likely to leave the bank or gender has nothing to do with bank churn.\n> - We will put the dataframes of both genders side-by-side for easier comparison","metadata":{}},{"cell_type":"code","source":"# Group Data By Gender, Geography And Churn Status\ngrouped_data = df.groupby([\"Gender\",\"Exited\", \"Geography\"]).count()\n\n# Turn the grouped Data into a dataframe\nseg = pd.DataFrame(grouped_data[\"Age\"])\n\n# Rename the column \nseg.columns = [\"Count\"]\n\n# get Only Data about Churned Females\nfemale = seg.query('Gender == \"Female\" and Exited == 1')\n\n# get Only Data about Churned Males\nmale = seg.query('Gender == \"Male\" and Exited == 1')\n\n# Call the new dataframes side-by-side\n\n# Set Stylers\nfemale_styler = female.style.set_table_attributes(\"style='display:inline'\").set_caption(\"Female Table\")\nmale_styler = male.style.set_table_attributes(\"style='display:inline'\").set_caption(\"Male Table\")\n\n# First Import display_html\nfrom IPython.display import display_html \n\n# Output the tables\ndisplay_html(female_styler._repr_html_()+male_styler._repr_html_(), raw=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n\n| **France** | More female bank users left than male users|\n| :- | :- |\n| **Germany** | Once again, more female bank users left than male users|\n| **Spain** | AND once more, more female bank users left than male users|\n\n>#### $Conclusion$\n- We could say that females are more likely to leave a bank than male users. \n\n>**The biggest question is: among these three countries, which gender is more predominant in terms of bank use?**","metadata":{}},{"cell_type":"code","source":"print(\"There are\", df.query('Gender == \"Female\"')[\"Gender\"].count(), \"females and \\n\", df.query('Gender == \"Male\"')[\"Gender\"].count(),\"male bank users in this dataset.\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $ Analysis$\n>- Despite having fewer female bank users, upon going back to the churn rate of the genders, we see that females seem slightly highly likely to leave the bank. ","metadata":{}},{"cell_type":"code","source":"plt.style.use('seaborn')\n\ngender = pd.DataFrame(df['Gender'].value_counts())\n\n# Pie Chart \nexplode = (0.0, 0.1)\ncolors = ['blue', 'deeppink']\nlabels = 'Male Customers', 'Female Customers'\ngender.plot.pie(y='Gender', figsize=(8,8), colors = colors, \n                 autopct='%1.1f%%', textprops={'color':\"w\"}, explode=explode, \n                 startangle=90, shadow=True, labels=labels, wedgeprops={'edgecolor': 'red'})\nplt.show()\n# Rename the columns\ngender.columns = [\"Count\"]\ngender","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $ Analysis$\n- The above pie chart just summarizes for us the gender segmentation of users in these 3 different countires.\n- As we have already seen, 55% of users are male and the other 45% are females. \n\n> ### 4.3.3  Churn Rate Analysis\n> ### Gender Churn Rate Analysis\n- Let us take a lot at churn rates for both genders, and overall churn rate.","metadata":{}},{"cell_type":"code","source":"# Get total Churn\ntotal_churn = df.query('Exited == 1').count()[\"Exited\"]\n\n# Get Gender And Churn Status\n# Female Churn \nfemale_churn_number = df.query('Gender == \"Female\" & Exited == 1').count()[\"Exited\"]\n# Female Churn Rate\nfemale_churn_rate = round((female_churn_number/total_churn)*100, 2)\n\n# Male Churn \nmale_churn_number = df.query('Gender == \"Male\" & Exited == 1').count()[\"Exited\"]\n# Male Churn Rate\nmale_churn_rate = round((male_churn_number/total_churn)*100, 2)\n\n# Create Churn Rate DataFrame\n# Generate List of Lists\nchurn_data = [['Female', female_churn_rate], ['Male', male_churn_rate]]\n\n# Create the Pandas DataFrame\nchurn_df = pd.DataFrame(churn_data, columns = [\"Gender\", \"Churn Rate (%)\"])\nchurn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $ Analysis$\n- Just as we have hinted, we do see that churn rate for female bank users (55.9%) is higher than that of males (44.1%).\n\n\n>- So more females leave the bank than males.\n\n> ### Overall Churn Rate","metadata":{}},{"cell_type":"code","source":"# Calculate Overall Churn Rate\noverall_churn_rate = round(((total_churn/df.Exited.count())*100), 2)\n\n# Print the rate\nprint(\"Overall Churn Rate is\", overall_churn_rate,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n- All in all, for both genders, overall churn rate is 20.37%. \n- This is the number we want to bring down, that is we aim to improve services in the bank so that people can stay with the bank. For example, it would be a huge improvement to have a churn rate of 10% after improving the bank and service delivery.","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.4 Age Segmentation Analysis\n> #### How old is the oldest bank client in this dataset?","metadata":{}},{"cell_type":"code","source":"print(\"The oldest bank customer in this dataset is\",df.Age.max(),\"years old.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Create Age Groups Based off of available ages\n> We will use the following age groups:\n\n|Youth(0)| Adults(1) | Seniors(2)|\n| :- | :- | :- |\n| 18 - 24|25 - 64|65 & Above|","metadata":{}},{"cell_type":"code","source":"# Create Groups\ndf['AgeGroup'] = pd.cut(df.Age, \n                           bins = [17, 24, 64, 120],\n                           labels=[\"youth\", \"adult\", \"senior\"])\n# Turn the column data type into a string\ndf['AgeGroup'] = df.AgeGroup.astype(str)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Age Groups DataFrames\nyouth = df.query('Age >= 18 and Age <= 24')\nadult = df.query('Age >= 25 and Age <= 64')\nsenior = df.query('Age >= 65')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"facet = sns.FacetGrid(df, hue=\"Exited\", aspect=3)\nfacet.map(sns.kdeplot,\"Age\", shade = True)\nfacet.set(xlim = (0, df[\"Age\"].max()))\nfacet.add_legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> #### How youths, adults and seniors exited the bank? We also want to find out which group had the highest churn rate.","metadata":{}},{"cell_type":"code","source":"# Get total Churn\ntotal_churn = df.query('Exited == 1').count()[\"Exited\"]\n\n# Get Age Group And Churn Status\n# Youth Churn \nyouth_churn_number = df.query('AgeGroup == \"youth\" & Exited == 1').count()[\"Exited\"]\n# Youth Churn Rate\nyouth_churn_rate = round((youth_churn_number/total_churn)*100, 2)\n\n# Adult Churn \nadult_churn_number = df.query('AgeGroup == \"adult\" & Exited == 1').count()[\"Exited\"]\n# Adult Churn Rate\nadult_churn_rate = round((adult_churn_number/total_churn)*100, 2)\n\n# Senior Churn \nsenior_churn_number = df.query('AgeGroup == \"senior\" & Exited == 1').count()[\"Exited\"]\n# Adult Churn Rate\nsenior_churn_rate = round((senior_churn_number/total_churn)*100, 2)\n\n# Create Churn Rate DataFrame\n# Generate List of Lists\nage_churn_data = [['Youth', youth_churn_number, youth_churn_rate], ['Adult', adult_churn_number, adult_churn_rate], ['Senior', senior_churn_number, senior_churn_rate]]\n\n# Create the Pandas DataFrame\nage_churn_df = pd.DataFrame(age_churn_data, columns = [\"Age Group\", \"Churn per Group\", \"Churn Rate(%)\"])\nage_churn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n- Young people and seniors seem less likely to leave the bank\n- Adults (between 25 and 64) have a higher probability to leave the bank, for various reasons.\n","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.5 Bank Balance and Churn\n- Let's try see if we have bank users who exited yet they still had money with the bank.\n> #### How many bank users with no balance in the bank and exited the bank?","metadata":{}},{"cell_type":"code","source":"# Get Balance And Churn Status\n# No Balance & Churn\nno_bal = df.query('Balance == 0 & Exited == 1')#.count()[\"Exited\"]\n\n# With Balance & Exited\nbal = df.query('Balance > 0 & Exited == 1')#.count()[\"Exited\"]\n\n# Churn Rates\nno_bal_churn_rate = round((no_bal.Exited.count()/total_churn)*100, 2)\nbal_churn_rate = round((bal.Exited.count()/total_churn)*100, 2)\n\n# Create Balance vs. Churn DataFrame\n# Generate List of Lists\nbal_churn_data = [['Zero Balance', no_bal.Exited.count(), no_bal_churn_rate], ['With Balance', bal.Exited.count(), bal_churn_rate]]\n\n# Create the Pandas DataFrame\nbal_churn_df = pd.DataFrame(bal_churn_data, columns = [\"Balance Status\", \"Churn Numbers\",\"Churn Rate(%)\"])\nbal_churn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### $Analysis$\n- There are more users with a bank balance exiting than those without a bank balance.","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.6 Credit Card and Churn\n- Let's see how many customers with and without credit cards exited","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"HasCrCard\", hue = \"Exited\", data = df).set_title(\"Count Plot of Customers with Credit Cards vs Churn\", fontname = \"Times New Roman\", size = 20, color=\"m\")\nplt.xlabel(\"Credit Card Possession Status\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get total Churn\ntotal_churn = df.query('Exited == 1').count()[\"Exited\"]\n\n# Get Credit Card Status And Churn Status\n# With Credit Card Churn \ncrcard_churn_number = df.query('HasCrCard == 1 & Exited == 1').count()[\"Exited\"]\n# With Credit Card Churn Rate\ncrcard_churn_rate = round((crcard_churn_number/total_churn)*100, 2)\n\n# Without Credit Card Churn \ncr_churn_number = df.query('HasCrCard == 0 & Exited == 1').count()[\"Exited\"]\n# Without Credit Card Churn Rate\ncr_churn_rate = round((cr_churn_number/total_churn)*100, 2)\n\n# Create Churn Rate DataFrame\n# Generate List of Lists\ncr_churn_data = [['No CrCard', crcard_churn_rate], ['Has CrCard', cr_churn_rate]]\n\n# Create the Pandas DataFrame\ncr_churn_df = pd.DataFrame(cr_churn_data, columns = [\"CrCard Status\", \"Churn Rate (%)\"])\ncr_churn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### $Analysis$\n- 70% of customers with no credit cards leave the bank\n- 30% of customers with credit cards still leave the bank.","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.7 Number of Products and Churn\n- Let's see if having more bank products keep clients","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"NumOfProducts\", hue = \"Exited\", data = df).set_title(\"Products Number Count Plot\", fontname = \"Times New Roman\", size = 20, color=\"m\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### $Analysis$\n- We see that it is while most customers with one product were second safest bet, they still posed a threat of churn.\n- That is, Even though customers with 1 product were relatively riskier than those with 2, the majority would still be a safe bet in terms of customer retention.","metadata":{}},{"cell_type":"markdown","source":"> ### 4.3.8 Tenure $vs.$ Churn: \n> #### Is there a correlation between the number of years using the bank and churn?\n- Does staying longer with the bank help keep customers/or prevents churn?","metadata":{}},{"cell_type":"code","source":"# Countplot\nplt.figure(figsize=(15,8))\nsns.countplot(data = df, \n              y = \"Tenure\", \n              hue = \"Exited\", \n              palette = \"Set1\",\n              saturation = 0.9).set_title(\"Tenure vs. Churn Count Plot\", \n                                          fontname = \"Times New Roman\", \n                                          size = 25, \n                                          color=\"m\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### $Analysis$\n- We see that tenure doesn't have much impact on whether customers stay or not.\n- But we do see that those who have been with the bank for less than a year and for ten years seem to stay.","metadata":{}},{"cell_type":"code","source":"# Countplot\nplt.figure(figsize=(15,8))\nsns.countplot(data = df, \n              x = \"IsActiveMember\", \n              hue = \"Exited\", \n              palette = \"Set1\",\n              saturation = 0.9).set_title(\"Activity Status vs. Churn Count Plot\", \n                                          fontname = \"Times New Roman\", \n                                          size = 25, \n                                          color=\"m\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Activity And Churn Status\n# Not Active & Churn\nno_act = df.query('IsActiveMember == 0 & Exited == 1')#.count()[\"Exited\"]\n\n# Active & Exited\nact = df.query('IsActiveMember == 1 & Exited == 1')#.count()[\"Exited\"]\n\n# Churn Rates\nno_act_churn_rate = round((no_act.Exited.count()/total_churn)*100, 2)\nact_churn_rate = round((act.Exited.count()/total_churn)*100, 2)\n\n# Create Activity Status vs. Churn DataFrame\n# Generate List of Lists\nact_churn_data = [['Not Active', no_act.Exited.count(), no_act_churn_rate], ['Active', act.Exited.count(), act_churn_rate]]\n\n# Create the Pandas DataFrame\nact_churn_df = pd.DataFrame(bal_churn_data, columns = [\"Activity Status\", \"Churn Numbers\",\"Churn Rate(%)\"])\nact_churn_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group Data By Gender, Geography And Churn Status\nselect_data = df.groupby([\"AgeGroup\", \"HasCrCard\",\"Exited\"]).count()\n\n# Turn the grouped Data into a dataframe\nselect_data = pd.DataFrame(select_data[\"Age\"])\n\n# Rename the column \nselect_data.columns = [\"Count\"]\n\nselect_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n# 5. Feature Engineering\nThis is the process of selecting features which are most relevant in predicting the output variable.\n- <font color='red'>$\\implies$</font> It helps reduce data dimensionality and \n- <font color='red'>$\\implies$</font> Ensures that models' accuracy can be trusted when those features are out. Learn more [here](https://en.wikipedia.org/wiki/Feature_selection).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,10))\ndf.corr()[\"Exited\"].sort_values(ascending = False).plot(kind='bar', figsize=(20,5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ###  Encoding Categorical Columns\n- Before we can work on our models, we need to make sure all categorical columns are of numerical data type. \n    - We saw that `Geography` and `Gender` columns housed categorical values, so we need to change that as Machine Learning Models take only numerical values\n    - We will also drop the new columns we added in the exploratory data analysis stage:\n        - Those are `CreditScoreGroup` and `AgeGroup`","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf[\"Gender\"] = le.fit_transform(df[\"Gender\"])\ndf[\"Geography\"] = le.fit_transform(df[\"Geography\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop CreditScoreGroup and AgeGroup Columns\ndf = df.drop([\"AgeGroup\", \"CreditScoreGroup\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Train/Test Split ","metadata":{}},{"cell_type":"code","source":"X = df.drop(\"Exited\", axis = 1)\ny = df[\"Exited\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split as tts\n\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('-'*48)\nprint('X_train has', X_train.shape[0], 'rows, y_train also has', y_train.shape[0], 'rows')\nprint('-'*49)\nprint('X_test has', X_test.shape[0], 'rows, y_test also has', y_test.shape[0], 'rows')\nprint('-'*49)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Machine Learning Model Selection, Training, Prediction and Assessment\n> We will test out several models to see which one performs better.\n> ## 7.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Call Logistic Regression \nlr = LogisticRegression()\n\n# Fit the model\nlr.fit(X_train, y_train)\n\n# Predition on test data\nlr_y_pred = lr.predict(X_test)\n\n# Model Evaluation - Accuracy Score\nlr_score = accuracy_score(lr_y_pred, y_test)\nlr_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.2 Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Call GaussianNB() Model \ngnb = GaussianNB()\n\n# Fit the model\ngnb.fit(X_train, y_train)\n\n# Predition on test data\ngnb_y_pred = gnb.predict(X_test)\n\n# Model Evaluation - Accuracy Score\ngnb_score = accuracy_score(gnb_y_pred, y_test)\ngnb_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.3 Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# Call Model \nrf = RandomForestClassifier(n_estimators = 100, random_state = 200)\n\n# Fit the model\nrf.fit(X_train, y_train)\n\n# Predition on test data\nrf_y_pred = rf.predict(X_test)\n\n# Model Evaluation - Accuracy Score\nrf_score = accuracy_score(rf_y_pred, y_test)\nrf_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.4 Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"# Call Model \ndt = tree.DecisionTreeClassifier()\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Predition on test data\ndt_y_pred = dt.predict(X_test)\n\n# Model Evaluation - Accuracy Score\ndt_score = accuracy_score(dt_y_pred, y_test)\ndt_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.5 XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xg  = XGBClassifier(max_depth = 10,random_state = 10, n_estimators=220, eval_metric = 'auc', min_child_weight = 3,\n                    colsample_bytree = 0.75, subsample= 0.9)\nxg.fit(X_train, y_train)\nxg_y_pred = xg.predict(X_test)\nxg_score = accuracy_score(xg_y_pred, y_test)\nxg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.6 K-Nearest Neighbors Classifier (kNN)","metadata":{}},{"cell_type":"code","source":"# Instantiate the classifier\nknn = KNeighborsClassifier()\n\n# Fit the Model\nknn.fit(X_train, y_train)\n\n# Make Predicitions with the trained classifier\nknn_y_pred = knn.predict(X_test)\n\n# Model Evaluation/Score\nknn_score = accuracy_score(knn_y_pred, y_test)\nknn_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_y_pred_prob = knn.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## 7.7 Support Vector Machine Classifier (SVM)","metadata":{}},{"cell_type":"code","source":"# Instantiate the SVM Model\nsvm = SVC(kernel = 'rbf', probability = True)\n\n# Fit the model\nsvm.fit(X_train, y_train)\n\n# Make predictions\nsvm_y_pred = svm.predict(X_test)\n\n# Model Evaluation/Score\nsvm_score = accuracy_score(svm_y_pred, y_test)\nsvm_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Bonus Model:\n> ## 7.8 Neural Networks\n> While this  data set is relatively small, and neural networks generally require lots of training data to develop meaningful prediction capabilities, a simple neural network is employed for a quick comparison to the other approaches.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, load_model\nfrom keras import layers\nfrom keras.layers.core import Dropout\nfrom keras.callbacks import ModelCheckpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate Neural networks Model\nnn = Sequential()\n\n# Build up layer structure and compile model\nInput_Shape = X_train.shape[1]\nnn.add(layers.Dense(1024, input_shape=(Input_Shape,), activation = 'relu'))\nnn.add(Dropout(0.2))\nnn.add(layers.Dense(1024, activation='relu'))\nnn.add(Dropout(0.2))\nnn.add(layers.Dense(1, activation='sigmoid'))\nnn.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\nnn.summary\n\n# Set Callback to Store Model with best validation Accuracy During Training\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', model='max', verbose=0, save_best_only=True)\n\n# Train Model and Store Best Performing Version\nfit_nn = nn.fit(X_train, y_train, epochs=100, verbose=False, validation_data=(X_test, y_test),\n               batch_size=30, callbacks=[mc])\nbest_nn = load_model('best_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = nn.evaluate(X_test, y_test, verbose = False)\naccuracy[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusion\n>- We see that the best model is the Random Forest Classifier.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}