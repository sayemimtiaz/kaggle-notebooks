{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%pylab inline\nimport seaborn as sns\nimport xgboost\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Heart disease dataset - exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_dataset = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Description of columns from the text file","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* age - age\n* sex - biological sex (1 = male, 0 = female)\n* cp - chest pain type\n  * Value 1: typical angina\n  * Value 2: atypical angina\n  * Value 3: non-anginal pain\n  * Value 4: asymptomatic\n* trestbps - resting blood pressure on admission to hospital\n* chol - serum cholesterol (mg/dl)\n* fbs - fasting blood sugar (indicator variable, 1 if > 120 mg/dl, 0 o.w)\n* restecg - resting electrocardiographic results\n  * Value 0: normal\n  * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n  * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n                    \n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n  * Value 1: upsloping\n  * Value 2: flat\n  * Value 3: downsloping\n* ca - number of major vessels colored by fluroscopy\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n*  target - diagnosis of heart disease (angiographic disease status)\n   * Value 0: < 50% diameter narrowing\n   * Value 1: > 50% diameter narrowing\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = \"target\", data = heart_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to exist a slight class imbalance. We might need to accommodate this if required","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploring specific columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"splitBySex = pd.crosstab(heart_dataset[\"target\"], heart_dataset[\"sex\"])\nsplitBySex[1] /= sum(splitBySex[1])\nsplitBySex[0] /= sum(splitBySex[0])\nsplitBySex.plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you have a heart failure, there seems to be a greater chance that you're female, i.e., P(female | diseased person) > P(male | diseased person)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The other probability also (for completion)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sexSplitByDisease = pd.crosstab(heart_dataset[\"sex\"], heart_dataset[\"target\"])\nsexSplitByDisease[0] /= sum(sexSplitByDisease[0])\nsexSplitByDisease[1] /= sum(sexSplitByDisease[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sexSplitByDisease.plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you're female there's a greater chance that you're diseased i.e., P(diseased | female) > P(diseased | male)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positives = heart_dataset.loc[heart_dataset[\"target\"] == 1]\nnegatives = heart_dataset.loc[heart_dataset[\"target\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age group","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(positives[\"age\"],kde = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(negatives[\"age\"],kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bi-modality in the positives is quite striking, as is the peak in the 60s for the negatives. Looks like old people are healthy after all :) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Angina - cp","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Angina - A type of chest pain caused by reduced blood flow to the heart. Let's check the correlation of cp with heart failure rates, given that heart failure criteria is given as narrowing of artery diameter (which could cause angina, hence angina being the predictor)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(heart_dataset[\"target\"], heart_dataset[\"cp\"]).plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall\n* Value 0: typical angina\n* Value 1: atypical angina\n* Value 2: non-anginal pain\n* Value 3: asymptomatic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looks like non-anginal pain seems to be a marker for heart failure, while anginal pain seems to be a marker for no heart failure","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### trestbps - Resting blood pressure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(positives[\"trestbps\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(negatives[\"trestbps\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x = positives[\"trestbps\"], y = positives[\"age\"],kind = \"hex\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like most people having heart failures seem to be in their 50s and having moderately high (130 mm Hg) blood pressure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x = negatives[\"trestbps\"], y = negatives[\"age\"], kind = \"hex\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest distribution here too seems to be on the lower side (120s) and in the same age group as above (50s)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### ca - Number of major vessels coloured by fluoroscopy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What I understand from [here](https://www.hopkinsmedicine.org/health/treatment-tests-and-therapies/fluoroscopy-procedure) and [here](https://www.hopkinsmedicine.org/health/treatment-tests-and-therapies/cardiac-catheterization) is that a tube is inserted and the arteries coloured to check for any blockages. So I'm assuming the more vessels that were checked, the stronger our conclusions are, so the `ca` variable could be a very strong predictor of the presence or absence of the disease","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(positives[\"ca\"].value_counts()).plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negatives[\"ca\"].value_counts().plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this means if a fluoroscopy is done, there is a greater chance of not having a heart failure. For the time being, we will include `ca` as a bool - ca > 0 : 1, else 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(heart_dataset[\"exang\"],heart_dataset[\"target\"]).plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exercise induced angina seems to be a negative indicator of heart failure","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Oldpeak and thal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since I'm no doctor, I don't have an intuitive idea of what these are supposed to mean and how the slope affects diagnosis, so I plot these without any intuition","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(y = heart_dataset[\"oldpeak\"], x = heart_dataset[\"slope\"], hue = heart_dataset[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"slope 2 and low oldpeak seems to be clear cut indicators of heart failure. High values of olepeak seems to be correlated with no heart failure. So we expect similar results in the model. We will one-hot encode slope","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### chol - Serum cholesterol","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(positives[\"chol\"], kde = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(negatives[\"chol\"], kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This doesn't look like a very important variable, given that the core regions of positives and negatives overlap","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To summarize, we shall be doing the following for modelling\n\n* Age - as is, no normalization\n* Sex - as is, no modifications\n* cp - one hot encode\n* fbs - as is, no modification\n* restecg - one hot encode\n* trestbps, chol, thalach, oldpeak - Normalize using StandardScaler so that the numbers that enter the model have the same order of magnitude\n* exang - include (expect negative correlation)\n* slope - on ehot encode\n* ca - make it bool ( 1 if at least one vesse coloured by fluoroscopy)\n* thal - drop (because I don't know what this variable is)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ML Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\n\nLet's start with good ol' logistic regression, because we'll be able to understand the individual coefficients in a linear model[](http://)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from  sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create X and Y arrays for ML","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check how the data is distributed in the dataset - we don't want all positive and all negative targets together","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(heart_dataset[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yep, we need to shuffle the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = heart_dataset.sample(frac=1) #shuffle shuffle shuffle\nX = X.drop([\"thal\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Insert the ca boolean variable in place of the ca variable\nX[\"cabool\"] = X[\"ca\"].apply(lambda x : 1 if x > 0 else 0)\nX = X.drop([\"ca\"], axis = 1)\nX[\"ca\"] = X[\"cabool\"]\nX = X.drop([\"cabool\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copy the targets to y\ny = X[\"target\"].copy()\nX = X.drop([\"target\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using pipelines to do everything together - makes life easy\nonehot_encoder = OneHotEncoder(handle_unknown = \"ignore\", sparse = False)\nnumerical_scaler = StandardScaler()\npreprocessor = ColumnTransformer(transformers = [(\"numerical_scaler\",numerical_scaler,[\"trestbps\",\"chol\",\"thalach\",\"oldpeak\"]),(\"onehot_encoder\",onehot_encoder,[\"cp\",\"restecg\",\"slope\"])], remainder = \"passthrough\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size = 0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfit_model = LogisticRegression(max_iter = 1000, C = 0.5, class_weight = \"balanced\")\nlr_pipeline = Pipeline(steps = [(\"preprocessor\",preprocessor),(\"model\",fit_model)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will do a grid search to find the best logistic regression regularization parameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\"model__C\" : np.logspace(-5,0,10)}\nsearch = GridSearchCV(lr_pipeline, param_grid, n_jobs = -1)\nsearch.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Redefine the pipeline with the best C\nfit_model = LogisticRegression(max_iter = 1000, C = search.best_params_[\"model__C\"], class_weight = \"balanced\")\nlr_pipeline = Pipeline(steps = [(\"preprocessor\",preprocessor),(\"model\",fit_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pipeline.fit(X_train,y_train)\nlr_preds = lr_pipeline.predict(X_valid)\nprint(\"Accuracy = \",sum(lr_preds == y_valid)/len(y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time for some metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,f1_score\nlr_CM = confusion_matrix(y_valid, lr_preds)\nlr_TP = lr_CM[1,1]\nlr_TN = lr_CM[0,0]\nlr_FP = lr_CM[0,1]\nlr_FN = lr_CM[1,0]\nprint(lr_CM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix order :- \n\nTN | FP  \nFN | TP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_recall = lr_TP/(lr_TP + lr_FN)\nlr_specificity = lr_TN/(lr_TN + lr_FP)\nlr_precision = lr_TP/(lr_TP + lr_FP)\n\nprint(lr_recall,lr_specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model seems to have decent recall and specificity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_f1_score = f1_score(y_valid, lr_preds)\nlr_f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prep for ROC score curves\nfrom sklearn.metrics import roc_curve, roc_auc_score\nlr_y_proba = lr_pipeline.predict_proba(X_valid)[:,1]\nlr_fpr,lr_tpr,lr_threshold = roc_curve(y_valid,lr_y_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A look at the coefficients","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"column names in X_train : trestbps, chol, thalach, oldpeak, cp = 0, cp = 1, cp = 2, cp = 3, restecg = 0, restecg = 1, restecg = 2, slope = 0, slope = 1, slope = 2, age, sex, fbs, exang, ca. The fit gives us weights in the same order","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model.coef_, fit_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of our conclusions from the visualization part seem to have come true\n\n* `ca` is a very good predictor of not having a heart disease (in fact the best predictor)\n* `exang` being moderately negatively correlated with heart disease\n* `sex` being negatively correlated (women being more prone than men)\n* `slope` = 2 being a good indicator (moderately high positive weight)\n* `cp` = 2 (non-anginal pain) having the highest positive weight\n\nOverall, this has confirmed some of the trends we saw in the visualization phase","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random forests\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(n_estimators = 1000)\nrf_pipeline = Pipeline([(\"preprocessor\",preprocessor),(\"model\",rf_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_search = GridSearchCV(rf_pipeline,param_grid = {\"model__n_estimators\":np.arange(100,1100,100)},n_jobs = -1)\nrf_search.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(n_estimators = rf_search.best_params_[\"model__n_estimators\"])\nrf_pipeline = Pipeline([(\"preprocessor\",preprocessor),(\"model\",rf_model)])\nrf_pipeline.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_preds = rf_pipeline.predict(X_valid)\nprint(sum(rf_preds == y_valid)/len(rf_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all(rf_preds == lr_preds) #Confirmation in case the accuracies turn out to be equal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_CM = confusion_matrix(y_valid, rf_preds)\nrf_TP = rf_CM[1,1]\nrf_TN = rf_CM[0,0]\nrf_FP = rf_CM[0,1]\nrf_FN = rf_CM[1,0]\nprint(rf_CM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_recall = rf_TP/(rf_TP + rf_FN)\nrf_specificity = rf_TN/(rf_TN + rf_FP)\nlr_precision = rf_TP/(rf_TP + rf_FP)\n\nprint(rf_recall,rf_specificity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr_recall, lr_specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the random forest model seems to give us a better recall sacrificing specifity. We need a very good recall rate so that we have low false negatives in our diagnosis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_y_proba = rf_pipeline.predict_proba(X_valid)[:,1]\nrf_fpr,rf_tpr,rf_threshold = roc_curve(y_valid,rf_y_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lr_fpr,lr_tpr,\"b\", label = \"Logistic Regression\")\nplt.plot(rf_fpr, rf_tpr, \"r\", label = \"Random Forest\")\nplt.plot([0,1],ls = \"--\")\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"logistic regression AUC = \",roc_auc_score(y_valid,lr_preds))\nprint(\"Random forests AUC = \",roc_auc_score(y_valid,rf_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression seems to have a slight edge in the AUC score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}