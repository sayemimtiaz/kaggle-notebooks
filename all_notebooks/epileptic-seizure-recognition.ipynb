{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# additional libraries \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# for better color representation \nplt.style.use('ggplot')\nfrom inspect import signature\nfrom sklearn import tree\nfrom scipy import stats \nfrom scipy import signal\nfrom sklearn import preprocessing\nfrom scipy.fft import fft, fftfreq\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, validation_curve\nfrom sklearn import metrics \nfrom pandas.plotting import lag_plot, autocorrelation_plot\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom imblearn.over_sampling import RandomOverSampler\nimport seaborn as sns\nnp.random.seed(42) # stable execution","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read data \ndf = pd.read_csv(\"/kaggle/input/epileptic-seizure-recognition/Epileptic Seizure Recognition.csv\")\ndf.name = \"seizure_data\"\nprint(\"DataFrame name: {data_name}\".format(data_name = df.name))\nprint(f\"DataFrame size: {df.shape}\")\nprint(f\"# of datapoints: {df.shape[0]*df.shape[1]}\")\ndf.head(3)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preprocessing","metadata":{}},{"cell_type":"code","source":"# drop subject identifier\ndf.drop([\"Unnamed\"], inplace = True, axis = 1)\nassert(df.columns[0] != \"Unnamed\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values \ndf.isna().any().sum()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of all features\ndf.drop([\"y\"], axis = 1).describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation among all features \ndf.drop([\"y\"], axis = 1).corr()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a measure of correlation firt two consecutive epochs \n# positive, very high correlation\nprint(pd.DataFrame(data = {\"X1\": df[\"X1\"], \"X2\": df[\"X2\"]}).corr())\nprint()\n\n# get a measure of correlation between first and last epoch\n# no correlation \nprint(pd.DataFrame(data = {\"X1\": df[\"X1\"], \"X178\": df[\"X178\"]}).corr())\n\n# plot first and last signal \nfig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True, figsize = (15, 5))\nax1.plot(df[\"X1\"], color = \"c\")\nax1.set_title(\"Signal Curve of X1\")\nax2.plot(df[\"X178\"], color = \"g\")\nax2.set_title(\"Signal Curve of X178\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate discrete linear convolution for moving average\n\n# plot first epoch with convolution \nfig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True, figsize = (15, 5))\ninterval = 1000 \nwindow = np.ones(interval) / interval\nmoving_avg = np.convolve(df[\"X1\"], window, \"same\")\nax1.plot(moving_avg, c = \"c\")\nax1.set_title(\"Smoothed Signal Curve of X1\")\n\n# plot last epoch with alternative method \nax2 = df[\"X178\"].rolling(window = 1000).mean()\nax2.plot(c = \"g\")\nplt.title(\"Smoothed Signal Curve of X178\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define features, labels \nfeatures = df.drop([\"y\"], axis = 1)\nlabels = df[\"y\"]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# invert the time domain \nfeatures = features.T","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# switch from time domain to frequency domain with fft \n\n# define sampling rate \n\"\"\"\nnum_samples = 4094 \nduration = 23\nsampling_rate = num_samples / duration\n\"\"\"\nsampling_rate = df.shape[1]\n\n# remove DC component\nfeatures = features - np.mean(features)\n\n# fast fourier transformation \n# fourier_space = [features.iloc[:,i].ravel() for i in range(features.shape[1])]\n# fourier_space = [k for j in fourier_space for k in j]\nfourier_range = fft(features.T.values.ravel())\nfourier_domain = fftfreq(features.T.values.ravel().size, 1/sampling_rate)\n\n# use abs to deal with complex numbers \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15,5))\nax1.plot(fourier_domain, np.abs(fourier_range), c = \"c\")\nax1.plot(fourier_domain[:fourier_domain.size // 2], np.abs(fourier_range[:fourier_range.size // 2]), c = \"g\")\nax1.set_xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nax1.set_ylabel(\"Amplitude [m]\", fontweight = \"bold\")\nax2.plot(fourier_domain[:fourier_domain.size // 2], np.abs(fourier_range[:fourier_range.size // 2]), c = \"g\")\nax2.set_xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nax2.set_ylabel(\"Amplitude [m]\", fontweight = \"bold\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Welch's Method \n# make a plot with log scaling on the y-axis \nfreqs, power_spectrum = signal.welch(features.values.ravel(), sampling_rate, \"flattop\", 1024, scaling = \"spectrum\")\n\n# filter frequency and power spectrum \nfreqs, power_spectrum = freqs[(freqs > 1) & (freqs < 89)], power_spectrum[(freqs > 1) & (freqs < 89)]\n\nplt.figure(figsize = (15, 5))\nplt.semilogy(freqs, np.sqrt(power_spectrum), c = \"c\")\nplt.xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nplt.ylabel(\"Linear spectrum [V RMS]\", fontweight = \"bold\")\nplt.show()\n\n# RMS estimate \nprint(f\"RMS estimate: {round(np.sqrt(power_spectrum.max()), 2)}\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Significant Frequency Bands \n\n1. **Delta:** has a frequency of 3 Hz or below.\n1. **Theta:** has a frequency of 3.5 to 7.5 Hz and is classified as \"slow\" activity.\n1. **Alpha:** has a frequency between 7.5 and 13 Hz.\n1. **Beta:**  has a frequency bigger than 13 Hz.\n\n> [See source](https://www.medicine.mcgill.ca/physio/vlab/biomed_signals/eeg_n.htm)\n","metadata":{}},{"cell_type":"code","source":"# there is a spike in the beta waves, this could be important \n# majority of spikes in power spectrum is composed of beta waves \nwelch_df = pd.DataFrame({\"frequency\": freqs, \"power\": power_spectrum})\nwelch_df.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract individual wave amplitudes \nfor wave in range(5):\n    delta = welch_df[welch_df.frequency <= 3].power\n    theta = welch_df[(welch_df.frequency >= 3.5) & (welch_df.frequency <= 7.5)].power\n    alpha = welch_df[(welch_df.frequency >= 7.5) & (welch_df.frequency <= 13)].power\n    beta = welch_df[welch_df.frequency >= 13].power\n\n\n# of datapoints in each wave \nprint(f\"Delta size: {delta.size}\")\nprint(f\"Theta size: {theta.size}\")\nprint(f\"Alpha size: {alpha.size}\")\nprint(f\"Beta size: {beta.size}\")\nprint(f\"# of significant frequencies: {delta.size + theta.size + alpha.size + beta.size}\")\n\n# plot all waves\nfig, axs = plt.subplots(2, 2, figsize = (15,5))\naxs[0, 0].plot(delta, c = \"c\")\naxs[0, 1].plot(theta, c = \"g\")\naxs[1, 0].plot(alpha, c = \"g\")\naxs[1, 1].plot(beta, c = \"c\")\n\nfor ax in axs.flat:\n    ax.set(xlabel=\"#\", ylabel=\"[m]\")\n\nfor ax in axs.flat:\n    ax.label_outer()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- add feature ---\n\nwaves = [delta, theta, alpha, beta]\nfeatures = features.append(pd.concat(waves))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run cell after running the above cell only \n\n# recall individual wave counts \nprint(f\"Delta size: {delta.size}\")\nprint(f\"Theta size: {theta.size}\")\nprint(f\"Alpha size: {alpha.size}\")\nprint(f\"Beta size: {beta.size}\")\nprint(f\"# of significant frequencies: {delta.size + theta.size + alpha.size + beta.size}\")\n\n# extend each wave, construct amplitude feature vector\nfeat_vector = []\ntotal = delta.size + theta.size + alpha.size + beta.size\nfactor = features.iloc[-1].size // total\n\nfor wave in waves:\n    wave = wave.to_list() * factor\n    feat_vector.append(wave)\n    \n# add remainder as a combination of scarce wave types \nfeat_vector = [k for y in feat_vector for k in y]\n\navg = (delta.mean() + theta.mean() + alpha.mean() + beta.mean()) / 4 \nremainder = features.iloc[-1].size - len(feat_vector)\nprint(f\"# of beta waves for extension: {remainder}\")\n\nfor k in range(remainder):\n    feat_vector.append(avg)\n\n# check computation\nassert(len(feat_vector) == features.iloc[-1].size)\n\n# --- add feature ---\nfeatures = features.iloc[:-1]\nfeatures = features.append(pd.DataFrame(feat_vector, columns = [\"amplitudes\"]).T)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run cell after running the above cell only \n\n# add fourier domain, i.e frequencies to the feature space\nfor wave in range(5):\n    delta = welch_df[welch_df.frequency <= 3].frequency\n    theta = welch_df[(welch_df.frequency >= 3.5) & (welch_df.frequency <= 7.5)].frequency\n    alpha = welch_df[(welch_df.frequency >= 7.5) & (welch_df.frequency <= 13)].frequency\n    beta = welch_df[welch_df.frequency >= 13].frequency\n\n# --- add feature ---\nwaves = [delta, theta, alpha, beta]\nfeatures = features.append(pd.DataFrame(pd.concat(waves), columns = [\"frequency\"]).T)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run cell after running the above cell only \n\n# replace amplitude with frequency\n# perform same arithmetic operations \nfeat_vector = []\ntotal = delta.size + theta.size + alpha.size + beta.size\nfactor = features.iloc[-1].size // total\n\nfor wave in waves:\n    wave = wave.to_list() * factor\n    feat_vector.append(wave)\n    \n# add remainder as a combination of scarce wave types \nfeat_vector = [k for y in feat_vector for k in y]\n\navg = (delta.mean() + theta.mean() + alpha.mean() + beta.mean()) / 4\nremainder = features.iloc[-1].size - len(feat_vector)\nprint(f\"# of beta waves for extension: {remainder}\")\n\nfor k in range(remainder):\n    feat_vector.append(avg)\n\n# check computation\nassert(len(feat_vector) == features.iloc[-1].size)\n\n# --- add feature --- \nfeatures = features.iloc[:-1]\nfeatures = features.append(pd.DataFrame(feat_vector, columns = [\"frequency\"]).T)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample eeg-values across features \nsample = np.random.randint(low = 0, high = 178, size = 3)\nfig, ax = plt.subplots(3, figsize = (15, 5))\nfor x in range(3):\n    if x % 2 == 0:\n        color = \"c\"\n    else:\n        color = \"g\"\n    ax[x].plot(features.iloc[sample[x]], c = color)\n    ax[x].set_xticks([])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Binary Classification","metadata":{}},{"cell_type":"code","source":"# make labels binary\nbinary = lambda label: 0 if label != 1 else label\nlabels = labels.apply(binary)\n\n# features, labels as numpy arrays \nfeatures = features.values \nlabels = labels.values","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# below is principal component analysis\n# it is not necessary unless you have trouble with high dimensionality \n\n# regular train, test split here with stratification \nX_train, X_test, y_train, y_test =  train_test_split(features, labels, test_size = 0.4, \n                                                     random_state = 42, stratify = labels)\nassert(X_train.shape[0] == y_train.shape[0])\nassert(X_test.shape[0] == y_test.shape[0])\n\nprint(f\"Size of training sample: {X_train.shape}\")\nprint(f\"Size of test sample: {X_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TODO: Multi-class Classification","metadata":{}},{"cell_type":"code","source":"# principal component analysis \n\n# scale \nfeatures = preprocessing.StandardScaler().fit_transform(features)\n\npca = PCA(n_components = 50)\nreduced = pca.fit_transform(features)\npca_data = pd.DataFrame(reduced, columns = [\"component_\" + str(idx) for idx in range(1,51)])\n\n# display first five principal components \nprint(pca_data[[\"component_\" + str(idx) for idx in range(1,6)]])\n\n# total information stored \nprint(f\"\\nCumulative variance explained: {np.sum(pca.explained_variance_ratio_)}\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# linear support vector machine \n# train, test split \nX_train, X_test, y_train, y_test = train_test_split(pca_data.values, labels, \n                                                    test_size = 0.33,random_state = 42)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ratio of labels in data \nprint(\"Class distribution before over-sampling\")\nprint(f\"Ratio of label = 1: {np.sum(labels) / labels.size}\")\nprint(f\"Ratio of label = 0: {1 - np.sum(labels) / labels.size}\")\n\n# data is heavily unbalanced","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oversampling\n* Do not run for complexity reasons ","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# oversample training data\n# apply to training set only \nX_train, y_train = RandomOverSampler(sampling_strategy = \"minority\").fit_resample(X_train, y_train)\n\n# observe new ratio \nprint(\"\\nClass distribution after over-sampling\")\nprint(f\"Ratio of label = 1: {np.sum(y_train) / y_train.size}\")\nprint(f\"Ratio of label = 0: {1 - np.sum(y_train) / y_train.size}\")\n\"\"\"","metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification \nclassifier = svm.SVC(kernel = \"linear\", C = 1, random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy \nprint(f\"Accuracy of linear SVM: {metrics.accuracy_score(y_test, y_pred)}\\n\")\n\n# classification report \nprint(metrics.classification_report(y_test, y_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random forest classifier \nclassifier = RandomForestClassifier(random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy \nprint(f\"Accuracy of random forest classifier: {metrics.accuracy_score(y_test, y_pred)}\\n\")\n\n# classification report \nprint(metrics.classification_report(y_test, y_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create range of values for parameter\nparam_range = np.arange(1, 250, 50)\n\n# Calculate accuracy on training and test set using range of parameter values\ntrain_scores, test_scores = validation_curve(RandomForestClassifier(), \n                                             features, \n                                             labels, \n                                             param_name=\"n_estimators\", \n                                             param_range=param_range,\n                                             cv=3, \n                                             scoring=\"accuracy\", \n                                             n_jobs=-1)\n\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k-fold cross validation \naccs = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10).mean()*100\nprint(f\"10-fold accuracy: {round(accs, 2)}%\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no parameter optimization necessary ","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Can we predict seizures beforehand ? ","metadata":{}},{"cell_type":"code","source":"features = df.drop([\"y\"], axis = 1).T","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reindex \nidx = {}\nfor i in range(1,179):\n    idx[\"X\" + str(i)] = float(i)\nfeatures.rename(index = idx, inplace = True)\nfeatures.head(5)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# average the data before further analysis\navg_df = pd.DataFrame(features.mean(axis = 1), columns = [\"t\"])\navg_df.head(5)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot for lag = 1\nlag_plot(avg_df)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check correlation between (t+1) and (t-1)\ncorr_df = pd.concat([avg_df.shift(1), avg_df], axis=1)\ncorr_df.columns = [\"t-1\", \"t+1\"]\ncorr_df.corr()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for further analysis \ncorr_df.fillna(method = \"bfill\", inplace = True)\ncorr_df.head(1)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# autocorrelation \nautocorrelation_plot(avg_df)\nplot_acf(avg_df, lags = 31)\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline persistence model \n\n# create lagged data \nX = corr_df.values \n\n# perform the split \ntrain, test = X[:len(X)-7], X[len(X)-7:]\nX_train, y_train = train[:,0], train[:,1]\nX_test, y_test = test[:,0], test[:,1]\n\n\n# every item in X_test is a forecast \npred = [x for x in X_test]\n\n# scoring\nmse = metrics.mean_squared_error(y_test, pred)\nprint(f\"Mean squared error for persistence model: {round(mse,2)}\")\n\n# plot persistence model\nfig, ax = plt.subplots(figsize = (15,5))\nax.plot(y_test)\nax.plot(pred, c = \"b\")\nax.legend([\"true\", \"prediction\"])\nax.set_title(\"Baseline model for autoregression\", fontweight = \"bold\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make arrays one dimensional \ntrain = train.ravel()\ntest = test.ravel()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# autoregression model\n\nmodel = AutoReg(train, lags = 31)\nmodel_fit = model.fit()\nprint(f\"Coefficients are {model_fit.params}\")\n\n# make predictions \nprint()\npred = model_fit.predict(len(train), (len(train) + len(test) - 1), dynamic = False)\nfor p in range(len(pred)):\n    print(\"predicted = {predicted}\\tactual = {actual}\".format(predicted = pred[p], actual = test[p]))\n    \n# root mean squared error \nrmse = np.sqrt(metrics.mean_squared_error(test, pred))\nprint(f\"\\nRoot mean squared error: {rmse}\")\n\n# plot true, pred values with cutoff\nfig, ax = plt.subplots(figsize = (15,5))\nplt.axvline(x = 6, c = \"g\")\nax.plot(test)\nax.plot(pred, c = \"b\")\nax.legend([\"cutoff\",\"true\", \"prediction\"])\nax.set_title(\"Autoregression model\", fontweight = \"bold\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discrepancy starts at lag = 6","metadata":{}},{"cell_type":"code","source":"# learn the coefficients \nwindow = 31\nmodel = AutoReg(train, lags = window)\nmodel_fit = model.fit()\ncoef = model_fit.params\n\n# get prior 31 observations \n# make predictions \nhistory = train[len(train) - window:]\nhistory = [history[i] for i in range(len(history))]\nyhat = coef[0]\npred = []\nfor p in range(len(test)):\n    hist_len = len(history)\n    lag = [history[i] for i in range(hist_len - window, hist_len)]\n    yhat = coef[0]\n    for d in range(window):\n        yhat += coef[d+1] * lag[window-d-1]\n    obs = test[p]\n    pred.append(yhat)\n    history.append(obs)\n    print(\"predicted = {predicted}\\tactual = {actual}\".format(predicted = yhat, actual = obs))\n\n# root mean squared error \nrmse = np.sqrt(metrics.mean_squared_error(test, pred))\nprint(f\"\\nRoot mean squared error: {rmse}\")\n\n# plot true, pred values with cutoff\nfig, ax = plt.subplots(figsize = (15,5))\nplt.axvline(x = 6, c = \"g\")\nplt.axvline(x = 8, c = \"g\")\nax.plot(test)\nax.plot(pred, c = \"b\")\nax.legend([\"cutoff\",\"true\", \"prediction\"])\nax.set_title(\"Autoregression model (learned coefficients)\", fontweight = \"bold\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Major improvement in RMSE","metadata":{}},{"cell_type":"markdown","source":"### Multivariate linear regression, robustness, polynomial, lasso, etc. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}