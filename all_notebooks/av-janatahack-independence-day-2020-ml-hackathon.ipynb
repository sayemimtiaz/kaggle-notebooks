{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src='https://datahack-prod.s3.ap-south-1.amazonaws.com/__sized__/contest_cover/jantahack_i-day-thumbnail-1200x1200-90.jpg'>"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"Janatahack: Independence Day 2020 ML Hackathon\n\nTopic Modeling for Research Articles\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set.\n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics:\n\n    Computer Science\n    Physics\n    Mathematics\n    Statistics\n    Quantitative Biology\n    Quantitative Finance\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.metrics import classification_report,f1_score\nstop_words = stopwords.words('english')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport unidecode\nimport re\nfrom skmultilearn.problem_transform import LabelPowerset# initialize label powerset multi-label classifier\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading The Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/janatahack-independence-day-2020-ml-hackathon/train.csv')\ntest=pd.read_csv('../input/janatahack-independence-day-2020-ml-hackathon/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Shape: ',train.shape)\nprint('Test Shape: ',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Label Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('As count:\\n')\nprint('Computer Science: ',train['Computer Science'].sum())\nprint('Physics: ',train['Physics'].sum())\nprint('Mathematics: ',train['Mathematics'].sum())\nprint('Statistics: ',train['Statistics'].sum())\nprint('Quantitative Biology: ',train['Quantitative Biology'].sum())\nprint('Quantiative Finance: ',train['Quantitative Finance'].sum())\n\nprint('\\nAs a percentage:\\n')\nprint('Computer Science: ',round(train['Computer Science'].sum()/train.shape[0]*100))\nprint('Physics: ',round(train['Physics'].sum()/train.shape[0]*100))\nprint('Mathematics: ',round(train['Mathematics'].sum()/train.shape[0]*100))\nprint('Statistics: ',round(train['Statistics'].sum()/train.shape[0]*100))\nprint('Quantitative Biology: ',round(train['Quantitative Biology'].sum()/train.shape[0]*100))\nprint('Quantiative Finance: ',round(train['Quantitative Finance'].sum()/train.shape[0]*100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Looks fairly balanced except for quantitative biology and finance"},{"metadata":{},"cell_type":"markdown","source":"# Text Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"### Getting Text Length & Creating Title+Abstract"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TITLE_len']=train['TITLE'].apply(len) \ntest['TITLE_len']=test['TITLE'].apply(len) \n\ntrain['ABSTRACT_len']=train['ABSTRACT'].apply(len) \ntest['ABSTRACT_len']=test['ABSTRACT'].apply(len) \n\ntrain['cons']=train['TITLE']+train['ABSTRACT'] \ntest['cons']=test['TITLE']+test['ABSTRACT'] \n\ntrain['cons_len']=train['cons'].apply(len) \ntest['cons_len']=test['cons'].apply(len) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['TITLE_len'])\nsns.distplot(test['TITLE_len'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['ABSTRACT_len'])\nsns.distplot(test['ABSTRACT_len'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['cons_len'])\nsns.distplot(test['cons_len'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Title and abstract in both train and test have very similar distribution"},{"metadata":{},"cell_type":"markdown","source":"### Functions for Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. café\"\"\"\n    text = unidecode.unidecode(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_(text):\n    return text.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \ndef stop_words_removal(sentence):\n  \n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(sentence)\n  \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return (' '.join(filtered_sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming(sentence):\n    \n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply Text Pre-Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Ascents\ntrain['TITLE']=train['TITLE'].apply(remove_accented_chars)\ntest['TITLE']=test['TITLE'].apply(remove_accented_chars)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(remove_accented_chars)\ntest['ABSTRACT']=test['ABSTRACT'].apply(remove_accented_chars)\n\ntrain['cons']=train['cons'].apply(remove_accented_chars)\ntest['cons']=test['cons'].apply(remove_accented_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lower Casing the text\ntrain['TITLE']=train['TITLE'].apply(lower_)\ntest['TITLE']=test['TITLE'].apply(lower_)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(lower_)\ntest['ABSTRACT']=test['ABSTRACT'].apply(lower_)\n\ntrain['cons']=train['cons'].apply(lower_)\ntest['cons']=test['cons'].apply(lower_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Special Characters\ntrain['TITLE']=train['TITLE'].apply(remove_special_characters)\ntest['TITLE']=test['TITLE'].apply(remove_special_characters)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(remove_special_characters)\ntest['ABSTRACT']=test['ABSTRACT'].apply(remove_special_characters)\n\ntrain['cons']=train['cons'].apply(remove_special_characters)\ntest['cons']=test['cons'].apply(remove_special_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stopwords removal\ntrain['TITLE']=train['TITLE'].apply(stop_words_removal)\ntest['TITLE']=test['TITLE'].apply(stop_words_removal)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(stop_words_removal)\ntest['ABSTRACT']=test['ABSTRACT'].apply(stop_words_removal)\n\ntrain['cons']=train['cons'].apply(stop_words_removal)\ntest['cons']=test['cons'].apply(stop_words_removal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming\ntrain['TITLE']=train['TITLE'].apply(stemming)\ntest['TITLE']=test['TITLE'].apply(stemming)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(stemming)\ntest['ABSTRACT']=test['ABSTRACT'].apply(stemming)\n\ntrain['cons']=train['cons'].apply(stemming)\ntest['cons']=test['cons'].apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#Writing the pre-processed text data\ntrain.to_csv('new_train.csv')\ntest.to_csv('new_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Pre-processed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/preprocessed-av-topic-modelling/new_train.csv')\ntest=pd.read_csv('../input/preprocessed-av-topic-modelling/new_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['title_orig_len']=train['TITLE_len']\ntest['title_orig_len']=test['TITLE_len']\n\ntrain['abs_orig_len']=train['ABSTRACT_len']\ntest['abs_orig_len']=test['ABSTRACT_len']\n\ntrain['cons_orig_len']=train['cons_len']\ntest['cons_orig_len']=test['cons_len']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TITLE_len']=train['TITLE'].apply(len) \ntest['TITLE_len']=test['TITLE'].apply(len) \n\ntrain['ABSTRACT_len']=train['ABSTRACT'].apply(len) \ntest['ABSTRACT_len']=test['ABSTRACT'].apply(len) \n\ntrain['cons_len']=train['cons'].apply(len) \ntest['cons_len']=test['cons'].apply(len) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the change in length after pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.drop(['ID','title_orig_len','abs_orig_len','cons_orig_len','TITLE_len','ABSTRACT_len'],axis=1).corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train & Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr,ev = train_test_split(train,random_state=101,test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{},"cell_type":"markdown","source":"#### What is TFIDF, Count Vectorisation\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\n\nVariations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n\nOne of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. \n\nWiki Link : https://en.wikipedia.org/wiki/Tf%E2%80%93idf"},{"metadata":{},"cell_type":"markdown","source":"### TFIDF & Count Vectorization  "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(train['cons'].values) + list(test['cons'].values))\n\n#Train\nxtrain_tfv =  tfv.transform(tr['cons']) \nxvalid_tfv = tfv.transform(ev['cons'])\n\n#Test\nxtest_tfv= tfv.transform(test['cons'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(train['cons'].values) + list(test['cons'].values))\n\n#Train\nxtrain_ctv =  ctv.transform(tr['cons']) \nxvalid_ctv = ctv.transform(ev['cons'])\n\n#Test\nxtest_ctv= ctv.transform(test['cons'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\nxtest_svd = svd.transform(xtest_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\nxtest_svd_scl = scl.transform(xtest_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#targets that need to be predicted\ntargets=['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']\n\n#Empty data frame for predictions\nev_pred=pd.DataFrame()\ntest_pred=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Models using TFIDF & Vectors "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using LogisticRegression one at a time on tf_idf\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using LogisticRegression\n    classifier = LogisticRegression()\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_logit.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.7993270848353762"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using LogisticRegression one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using LogisticRegression\n    classifier = LogisticRegression(max_iter=10000)\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_logit.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.8049348230912476"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using MultinomialNB one at a time on tf_idf\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using MultinomialNB\n    classifier = MultinomialNB()\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_mnb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.676923076923077"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using MultinomialNB one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using MultinomialNB\n    classifier = MultinomialNB()\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_mnb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.8092842442259318"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using SVC One at a time\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = SVC() \n    classifier.fit(xtrain_svd_scl, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd_scl)\n    test_pred[t] = classifier.predict(xtest_svd_scl)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svc.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.821267230394996"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using XGBoost one at a time\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.7930549038010324"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using XGBClassifier one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGBClassifier\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.7930022308324527"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using XGBoost on SVD components\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB789746001881468\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_svd, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd)\n    test_pred[t] = classifier.predict(xtest_svd)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.8173573642520952"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using XGBoost on SVD components\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = xgb.XGBClassifier(nthread=10)\n    classifier.fit(xtrain_svd, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd)\n    test_pred[t] = classifier.predict(xtest_svd)\n\nfor t in targets:\n    print(t)\n    print(classification_report(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_xgb2.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.8100712807541964"},{"metadata":{},"cell_type":"markdown","source":"# Using Grid Serach CV"},{"metadata":{},"cell_type":"markdown","source":"#### What is grid search?\n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n\n#### Why should I use it?\n\nIf you work with ML, you know what a nightmare it is to stipulate values for hyper parameters. There are libraries that have been implemented, such as GridSearchCV of the sklearn library, in order to automate this process and make life a little bit easier for ML enthusiasts."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next we need a grid of parameters:\n\nparam_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in targets:\n\n    y_train=tr[t]\n    \n    print('\\n For',t)\n    # Initialize Grid Search Model\n    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro',\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n    # Fit Grid Search Model\n    model.fit(xtrain_tfv, y_train)  # we can use the full data here but im only using xtrain\n    print(\"Best score: %0.3f\" % model.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n    \n'''\n    Results from GridSerach CV\n    \n    For Computer Science:\n    Best score: 0.857\n    Best parameters set:\n        lr__C: 0.1\n        lr__penalty: 'l2'\n        svd__n_components: 180\n\n    For Physics:\n    Best score: 0.932\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 120\n    \n    For Mathematics:\n    Best score: 0.902\n    Best parameters set:\n        lr__C: 0.1\n        lr__penalty: 'l2'\n        svd__n_components: 120\n    \n    For Statistics:\n    Best score: 0.881\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 180\n    \n    For Quantitative Biology:\n    Best score: 0.974\n    Best parameters set:\n        lr__C: 10\n        lr__penalty: 'l2'\n        svd__n_components: 180\n\n    For Quantitative Finance:\n    Best score: 0.990\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 120\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Grid Search Results:\n\nsvd_comp={'Computer Science': 180, 'Physics': 120, 'Mathematics': 120, 'Statistics': 180, 'Quantitative Biology': 180, 'Quantitative Finance':120}\nlr_c={'Computer Science': 0.1, 'Physics': 1.0, 'Mathematics': 0.1, 'Statistics': 1, 'Quantitative Biology': 10, 'Quantitative Finance':1}\nlr_pen={'Computer Science': 'l2', 'Physics': 'l2', 'Mathematics': 'l2', 'Statistics': 'l2', 'Quantitative Biology': 'l2', 'Quantitative Finance':'l2'}\n\nfor t in targets:\n    \n    y_train=tr[t]\n    \n    # Initialize SVD\n    svd = TruncatedSVD(n_components=svd_comp[t])\n    \n    # Initialize the standard scaler \n    scl = preprocessing.StandardScaler()\n\n    # We will use logistic regression here..\n    lr_model = LogisticRegression(C=lr_c[t],penalty=lr_pen[t])\n    \n    svd.fit(xtrain_tfv)\n    xtrain_svd = svd.transform(xtrain_tfv)\n    xvalid_svd = svd.transform(xvalid_tfv)\n    xtest_svd = svd.transform(xtest_tfv)\n\n    # Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n    scl.fit(xtrain_svd)\n    xtrain_svd_scl = scl.transform(xtrain_svd)\n    xvalid_svd_scl = scl.transform(xvalid_svd)\n    xtest_svd_scl = scl.transform(xtest_svd)\n    \n    # Model Fit\n    lr_model.fit(xtrain_svd_scl, y_train)  \n    \n    ev_pred[t] = lr_model.predict(xvalid_svd_scl)\n    test_pred[t] = lr_model.predict(xtest_svd_scl)\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_logit_gsv.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.807033888436008"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lightgbm\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n    \n    clf = lgb.LGBMClassifier(n_estimators=450,learning_rate=0.03,random_state=42,colsample_bytree=0.5,reg_alpha=2,reg_lambda=2)\n    \n    clf.fit(xtrain_svd_scl, y_train, early_stopping_rounds=100, eval_set=[(xtrain_svd_scl, y_train), (xvalid_svd_scl, y_test)], eval_metric='f1_micro', verbose=True)\n\n    eval_score = f1_score(y_test, clf.predict(xvalid_svd_scl))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(xvalid_svd_scl)\n    test_pred[t] = clf.predict(xtest_svd_scl)\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_lgbm.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.812694032424974"},{"metadata":{},"cell_type":"markdown","source":"# Word Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(tr['cons'])]\nxvalid_glove = [sent2vec(x) for x in tqdm(ev['cons'])]\nxtest_glove = [sent2vec(x) for x in tqdm(test['cons'])]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nfor t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    clf = xgb.XGBClassifier(nthread=10, silent=False)\n    clf.fit(np.array(xtrain_glove), y_train)\n    eval_score = f1_score(y_test, clf.predict(np.array(xvalid_glove)))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(np.array(xvalid_glove))\n    test_pred[t] = clf.predict(np.array(xtest_glove))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.762793995981563"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nfor t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n    clf.fit(np.array(xtrain_glove), y_train)\n    eval_score = f1_score(y_test, clf.predict(np.array(xvalid_glove)))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(np.array(xvalid_glove))\n    test_pred[t] = clf.predict(np.array(xtest_glove))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_xgb2.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.768516313407954"},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)\nxtest_glove_scl = scl.transform(xtest_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units=1,activation='softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(x=xtrain_glove_scl,\n              y=y_train, \n              batch_size=256, \n              epochs=500, \n              verbose=1, \n              validation_data=(xvalid_glove_scl, y_test),\n              callbacks=[early_stop]\n             )\n        \n    eval_score = f1_score(y_test, model.predict(xvalid_glove_scl).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_glove_scl))\n    test_pred[t] = np.array(model.predict(xtest_glove_scl))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.343620451404883"},{"metadata":{},"cell_type":"markdown","source":"# Proceeding to LSTMs"},{"metadata":{},"cell_type":"markdown","source":"#### What is LSTMs?\n\nLong short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(train['cons']) + list(test['cons']))\nxtrain_seq = token.texts_to_sequences(tr['cons'])\nxvalid_seq = token.texts_to_sequences(ev['cons'])\nxtest_seq = token.texts_to_sequences(test['cons'])\n\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\nxtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n\n        \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('lstm_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.34362045140488257"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_pad.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bi-directional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A simple bidirectional LSTM with glove embeddings and two dense layers\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                         300,\n                         weights=[embedding_matrix],\n                         input_length=max_len,\n                         trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100,verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n    \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('bilstm_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy: 0.34362045140488257"},{"metadata":{},"cell_type":"markdown","source":"# GRU with gloves"},{"metadata":{},"cell_type":"markdown","source":"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.\n\nWiki Link - https://en.wikipedia.org/wiki/GloVe_(machine_learning)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n    \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('gru_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reference:\n\n1) https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html#:~:text=Removing%20Special%20Characters,be%20used%20to%20remove%20them.\n\n2) https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n\n3) https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"},{"metadata":{},"cell_type":"markdown","source":"## Feel free to share your feedback, do Upvote if you like/found the notebook useful!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}