{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Objective of the Notebook\n\nThe Notebook is designed for the people who want to solve problems related to Sales Prediction. It will equip you with skills and techniques required to solve regression problems. You will be provided with sufficient theory and practice material to hone your predictive modeling skills.\n\n### Expectations from notebook\n\nThe course is divided into below modules:\n\n<ul> 1)Exploratory Data Analysis</ul>\n<ul>2) Data Preparation</ul>\n<ul>3) Predictive Modeling using different techniques.</ul>\n\n### Expectation from the Student\nA student is required to follow the below steps for extracting the maximum benefit from this problem.\n\n<ul>Study the concepts.</ul>\n<ul>Go through the practical content</ul>\n<ul>download the relevant dataset(s)</ul>\n<ul>implement the solution on your own.</ul>\nIn case you need advice on something or you get stuck feel free to ask any of the mentor present at that time."},{"metadata":{},"cell_type":"markdown","source":"### Let’s look at the steps that we will follow in this notebook.\n\nWe will handle this problem in a structured way. We will be following the table of content given below.\n\n<ul> 1).Problem Statement</ul>\n<ul> 2).Hypothesis Generation</ul>\n<ul> 3).Loading Packages and Data</ul>\n<ul> 4).Data Structure and Content</ul>\n<ul> 5).Exploratory Data Analysis</ul>\n<ul> 6).Univariate Analysis</ul>\n<ul> 7).Bivariate Analysis</ul>\n<ul> 8).Missing Value Treatment</ul>\n<ul> 9).Feature Engineering</ul>\n<ul> 10).Encoding Categorical Variables</ul>\n<ul> 11).Label Encoding</ul>\n<ul> 12).One Hot Encoding</ul>\n<ul> 13).PreProcessing Data</ul>\n<ul> 14).Modeling</ul>\n<ul> 15).Linear Regression</ul>\n<ul> 16).Regularized Linear Regression</ul>\n<ul> 17).RandomForest</ul>\n<ul> 18).XGBoost</ul>\n<ul> 19).Summary</ul>"},{"metadata":{},"cell_type":"markdown","source":"Understanding the problem statement is the first and foremost step. This would help you give an intuition of what you will face ahead of time. Let us see the problem statement -\n\n### The data scientists at BigMart have collected sales data for 1559 products across 10 stores in different cities for the year 2013. Now each product has certain attributes that sets it apart from other products. Same is the case with each store.\n\n### The aim is to build a predictive model to find out the sales of each product at a particular store so that it would help the decision makers at BigMart to find out the properties of any product or store, which play a key role in increasing the overall sales.\n\nLooks intriguing right? You will get to dive into the solution in the next sections."},{"metadata":{},"cell_type":"markdown","source":"### What is hypothesis generation?\nThis is a very important stage in any machine learning process. It involves understanding the problem in detail by brainstorming as many factors as possible which can impact the outcome. It is done by understanding the problem statement thoroughly and before looking at the data.\n\n### How to do hypothesis generation?\nOne very effective technique to generate hypotheses is by creating mindmaps. You can draw it even using a pen and paper. The general methodology is as follows: Write the main idea in the center. Draw branches from the center such they are connected with one another with final outputs shown towards the end.\n\nBelow is a simple mind map. Let’s understand it."},{"metadata":{},"cell_type":"markdown","source":"![](https://www.analyticsvidhya.com/wp-content/uploads/2016/06/coggle.png)"},{"metadata":{},"cell_type":"markdown","source":"### We can start the process by working on four levels: Store Level, Product Level, Customer Level and Macro Level."},{"metadata":{},"cell_type":"markdown","source":"### Store Level Hypotheses\n\n<ul><li><b>City type</b>: Stores located in urban or Tier 1 cities should have higher sales because of the higher income levels of people there.</li>\n\n<li><b>Population Density</b>: Stores located in densely populated areas should have higher sales because of more demand.</li>\n\n<li><b>Store Capacity</b>: Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place.</li>\n\n<li><b>Competitors</b>: Stores having similar establishments nearby should have less sales because of more competition.</li>\n\n<li><b>Marketing</b>: Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.</li>\n\n<li><b>Location</b>: Stores located within popular marketplaces should have higher sales because of better access to customers</li>\n\n<li><b>Ambiance</b>: Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"### Product Level Hypotheses\n\n\n<ul><li><b>Brand</b>: Branded products should have higher sales because of higher trust in the customer.</li>\n\n<li><b>Packaging</b>: Products with good packaging can attract customers and sell more.</li>\n\n<li><b>Utility</b>: Daily use products should have a higher tendency to sell as compared to the specific use products.</li>\n\n<li><b>Display Area</b>: Products which are given bigger shelves in the store are likely to catch attention first and sell more. Visibility in Store: The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.</li>\n\n<li><b>Advertising</b>: Better advertising of products in the store will should higher sales in most cases. Promotional Offers: Products accompanied with attractive offers and discounts will sell more.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"### Customer Level Hypotheses\n\n<ul><li><b>Customer Behavior</b>: Stores keeping the right set of products to meet the local needs of customers will have higher sales.</li>\n\n<li><b>Job Profile</b>: Customer working at executive levels would have higher chances of purchasing high amount products as compared to customers working at entry or mid senior level.</li>\n\n<li><b>Family Size</b>: More the number of family members, more amount will be spent by a customer to buy products.</li>\n\n<li><b>Annual Income</b>: Higher the annual income of a customer, customer is more likely to buy high cost products. Past Purchase History: Availablity of this information can help us to determine the frequency of a product being purchased by a user.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"### Macro Level Hypotheses\n\n<ul><li><b>Environment</b>: If the environment is declared safe by government, customer would be more likely to purchase products without worrying if it’s environment friendly or not.</li>\n\n<li><b>Economic Growth</b>: If the current economy shows a consistent growth, per capita income will rise, therefore buying power of customers will increase.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"Please note that this is not an exhaustive list. You can come up with more hypotheses of your own, the more the better. Let’s begin exploring the dataset and try to find interesting patterns."},{"metadata":{},"cell_type":"markdown","source":"### Loading Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np                     # For mathematical calculations \nimport seaborn as sns                  # For data visualization \nimport matplotlib.pyplot as plt        # For plotting graphs \n%matplotlib inline \nimport warnings   # To ignore any warnings \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data\nwe are provided with 2 CSV files — Train, Test.\n<ul><li>The Train file contains 11 independent variables and 1 target variable, i.e., Item_Outlet_Sales.</li>\n<li>The Test file also contains the same set of independent variables, but there is no target variable because that is what we have to predict.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"### Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bigmart-sales-data/Train.csv')\ntest = pd.read_csv('../input/bigmart-sales-data/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially we should understand our raw data thoroughly, i.e., we should explore the no. of features/columns and rows, datatype of the features, feature names and so on. It helps in working with the data in the next stages.\n\n### Dimensions of Data\nLet’s quicky check the dimensions of our data, i.e., columns and rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s make a copy of train and test data so that even if we have to make any changes in these datasets we would not lose the original datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_original=train.copy() \ntest_original=test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features of Data\nWe will take a quick glance over the feature names of train and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info(),test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are 4 numeric and 7 categorical variables."},{"metadata":{},"cell_type":"markdown","source":"### Why do we need Exploratory Data Analysis (EDA)?\n\nAfter understanding the dimensions and properties of data, we have to deep dive and explore the data visually. It helps us in understanding the nature of data in terms of distribution of the individual variables/features, finding missing values, relationship with other variables and many other things.\n\nWe need to predict Item_Outlet_Sales for given test data\n\nlets first merge the train and test data for Exploratory Data Analysis\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['source'] = 'train'\n# test['source'] = 'test'\ntest['Item_Outlet_Sales'] = 0\ndata = pd.concat([train, test], sort = False)\nprint(train.shape, test.shape, data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Item_Outlet_Sales'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Item_Outlet_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skewness: %f' % data['Item_Outlet_Sales'].skew())\nprint('Kurtsis: %f' %data['Item_Outlet_Sales'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ul>Deviate from the normal distribution.</ul>\n<ul>Have appreciable positive skewness.</ul>\n<ul>Show peakedness.</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorial_features = data.select_dtypes(include=[np.object])\ncategorial_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = data.select_dtypes(include=[np.number])\nnumerical_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Outlet_Establishment_Year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s start with univariate EDA. It involves exploring variables individually. We will try to visualize the continuous variables using histograms and categorical variables using bar plots.\n\n### Target Variable\nSince our target variable is continuous, we can visualise it by plotting its histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Outlet_Sales'].hist(bins = 100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, it is a right skewd variable and would need some data transformation to treat its skewness."},{"metadata":{},"cell_type":"markdown","source":"### Independent Variables (numeric variables)\nNow let’s check the numeric independent variables. We’ll again use the histograms for visualizations because that will help us in visualizing the distribution of the variables."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train['Item_Weight'].hist(bins = 100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Visibility'].hist(bins = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_MRP'].hist(bins = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\nThere seems to be no clear-cut pattern in Item_Weight.\nItem_Visibility is right-skewed and should be transformed to curb its skewness.\nWe can clearly see 4 different distributions for Item_MRP. It is an interesting insight."},{"metadata":{},"cell_type":"markdown","source":"### Independent Variables (categorical variables)\n\nNow we’ll try to explore and gain some insights from the categorical variables. A categorical variable or feature can have only a finite set of values. Let’s first plot Item_Fat_Content."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.catplot(x=\"Item_Fat_Content\", kind=\"count\", data=train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the figure above, ‘LF’, ‘low fat’, and ‘Low Fat’ are the same category and can be combined into one. Similarly we can be done for ‘reg’ and ‘Regular’ into one. After making these corrections we’ll plot the same figure again."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Fat_Content'].replace({'reg':'Regular','low fat':'Low Fat','LF':'Low Fat'},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Item_Fat_Content',kind = 'count',data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now lets check other categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Item_Type',kind = 'count',data = train,aspect =4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Outlet_Identifier',kind = 'count',data = train,aspect = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Outlet_Size',kind = 'count',data = train,aspect = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Outlet_Establishment_Year',kind = 'count',data = train,aspect =4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Outlet_Type',kind = 'count',data = train,aspect =4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\nLesser number of observations in the data for the outlets established in the year 1998 as compared to the other years.\nSupermarket Type 1 seems to be the most popular category of Outlet_Type.\n"},{"metadata":{},"cell_type":"markdown","source":"### Target Variable vs Independent Numerical Variables"},{"metadata":{},"cell_type":"markdown","source":"##### After looking at every feature individually, let’s now do some bivariate analysis. Here we’ll explore the independent variables with respect to the target variable. The objective is to discover hidden relationships between the independent variable and the target variable and use those findings in missing data imputation and feature engineering in the next module.\n\n##### We will make use of scatter plots for the continuous or numeric variables and violin plots for the categorical variables.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'Item_Weight',y = 'Item_Outlet_Sales',data = train,alpha = 0.3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'Item_Outlet_Sales',y = 'Item_Visibility',data = train,alpha = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'Item_MRP',y = 'Item_Outlet_Sales',data = train,alpha = 0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\nItem_Outlet_Sales is spread well across the entire range of the Item_Weight without any obvious pattern.\nIn Item_Visibility vs Item_Outlet_Sales, there is a string of points at Item_Visibility = 0.0 which seems strange as item visibility cannot be completely zero. We will take note of this issue and deal with it in the later stages.\nIn the third plot of Item_MRP vs Item_Outlet_Sales, we can clearly see 4 segments of prices that can be used in feature engineering to create a new variable.\n"},{"metadata":{},"cell_type":"markdown","source":"### Target Variable vs Independent Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"Now we’ll visualise the categorical variables with respect to Item_Outlet_Sales. We will try to check the distribution of the target variable across all the categories of each of the categorical variable.\n\nWe could have used boxplots here, but instead we’ll use the violin plots as they show the full distribution of the data. The width of a violin plot at a particular level indicates the concentration or density of data at that level. The height of a violin tells us about the range of the target variable values."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'Item_Type',y = 'Item_Outlet_Sales',kind = 'violin',data = train,aspect=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x = 'Item_Fat_Content',y = 'Item_Outlet_Sales',data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Outlet_Identifier','Item_Outlet_Sales',kind = 'violin',data = train,aspect = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\n<ul><li>Distribution of Item_Outlet_Sales across the categories of Item_Type is not very distinct and same is the case with Item_Fat_Content.</li>\n<li>The distribution for OUT010 and OUT019 categories of Outlet_Identifier are quite similar and very much different from the rest of the categories of Outlet_Identifier.</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"In the univariate analysis let’s check the distribution of the target variable across Outlet_Size."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot('Outlet_Size','Item_Outlet_Sales',data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s examine the remaining variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot('Outlet_Location_Type','Item_Outlet_Sales',data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot('Outlet_Type','Item_Outlet_Sales',data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\n<ul><li>Tier 1 and Tier 3 locations of Outlet_Location_Type look similar.</li>\n<li>In the Outlet_Type plot, Grocery Store has most of its data points around the lower sales values as compared to the other categories.</li></ul>\n\n\nThese are the kind of insights that we can extract by visualizing our data. Hence, data visualization should be an important part of any kind data analysis.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Missing Data Treatment\n\nMissing data can have a severe impact on building predictive models because the missing values might be contain some vital information which could help in making better predictions. So, it becomes imperative to carry out missing data imputation. There are different methods to treat missing values based on the problem and the data. Some of the common techniques are as follows:"},{"metadata":{},"cell_type":"markdown","source":"<ul><b> 1) Deletion of rows</b>: In train dataset, observations having missing values in any variable are deleted. The downside of this method is the loss of information and drop in prediction power of model.\n\n<b> 2) Mean/Median/Mode Imputation</b>: In case of continuous variable, missing values can be replaced with mean or median of all known values of that variable. For categorical variables, we can use mode of the given values to replace the missing values.\n\n<b>3)Building Prediction Model</b>: We can even make a predictive model to impute missing data in a variable. Here we will treat the variable having missing data as the target variable and the other variables as predictors. We will divide our data into 2 datasets—one without any missing value for that variable and the other with missing values for that variable. The former set would be used as training set to build the predictive model and it would then be applied to the latter set to predict the missing values.</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing Missing Value\nAs you can see above, we have missing values in Item_Weight and Outlet_Size. We’ll now impute Item_Weight with mean weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nl_enc  = LabelEncoder()\na = l_enc.fit_transform(train['Item_Identifier'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Weight'].fillna(a.mean(),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Item_Weight.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Outlet_Size'].fillna('Small',inplace  = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Outlet_Size'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0 missing values! It means we have successfully imputed the missing data in the feature."},{"metadata":{},"cell_type":"markdown","source":"### Replacing 0’s in Item_Visibility variable\n\n\nSimilarly, zeroes in Item_Visibility variable can be replaced with Item_Visibility wise mean values. It can be visualized in the plot below."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Visibility'].plot(kind = 'hist',bins = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s replace the zeroes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= train[train['Item_Visibility']!=0]['Item_Visibility'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Visibility'] = train['Item_Visibility'].replace(0.00,a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Visibility'].plot(kind = 'hist',bins = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Most of the times, the given features in a dataset are not sufficient to give satisfactory predictions. In such cases, we have to create new features which might help in improving the model’s performance. Let’s try to create some new features for our dataset.\n\nIn this section we will create the following new features:\n\n<ul><li>Item_Type_new: Broader categories for the variable Item_Type.</li>\n<li>Item_category: Categorical variable derived from Item_Identifier.</li>\n<li>Outlet_Years: Years of operation for outlets.</li>\n<li>price_per_unit_wt: Item_MRP/Item_Weight</li>\n<li>Item_MRP_clusters: Binned feature for Item_MRP.</li></ul>\n\n\nWe can have a look at the Item_Type variable and classify the c\nategories into perishable and non_perishable as per our understanding and make it into a new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"perishable = [\"Breads\", \"Breakfast\", \"Dairy\", \"Fruits and Vegetables\", \"Meat\", \"Seafood\"]\nnon_perishable = [\"Baking Goods\", \"Canned\", \"Frozen Foods\", \"Hard Drinks\", \"Health and Hygiene\", \"Household\", \"Soft Drinks\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create a new feature 'Item_Type_new' "},{"metadata":{"trusted":true},"cell_type":"code","source":"item_list =[] \nfor i in train['Item_Type']:\n    if i in perishable:\n        item_list.append('perishable')\n    elif (i in non_perishable):\n        item_list.append('non_perishable')\n    else:\n        item_list.append('not_sure')\n        \ntrain['Item_Type_new'] = item_list","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train['Item_Category'] =train['Item_Identifier'].replace({'^DR[A-Z]*[0-9]*':'DR','^FD[A-Z]*[0-9]*':'FD','^NC[A-Z]*[0-9]*':'NC'},regex = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Food=pd.crosstab(train['Item_Type'],train['Item_Category'])\nFood","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also change the values of Item_Fat_Content wherever Item_category is ‘NC’ because non-consumable items cannot have any fat content. We will also create a couple of more features — Outlet_Years (years of operation) and price_per_unit_wt (price per unit weight)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Fat_Content'][(train['Item_Category']=='NC')]='Non Edible'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Fat_Content'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Outlet_Years'] = 2019-train['Outlet_Establishment_Year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Price_Per_Unit_Weight'] = train['Item_MRP']/train['Item_Weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Earlier in the Item_MRP vs Item_Outlet_Sales plot, we saw Item_MRP was spread across in 4 chunks. Now let’s assign a label to each of these chunks and use this label as a new variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clusters(x):\n    if x<69:\n        return '1st'\n    elif x in range(69,136):\n        return '2nd'\n    elif x in range(136,203):\n        return '3rd'\n    else:\n        return '4th'\ntrain['Item_MRP_Clusters'] = train['Item_MRP'].astype('int').apply(clusters)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_MRP_Clusters'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"Most of the machine learning algorithms produce better result with numerical variables only. So, it is essential to treat the categorical variables present in the data. One thing that can be done is to completely remove the categorical variables, but that would lead to enormous loss of information. Fortunately we have smarter techniques to deal with the categorical variables.\n\nIn this stage, we will convert our categorical variables into numerical ones. We will use 2 techniques — Label Encoding and One Hot Encoding.\n\n<ul><b>1). Label encoding </b>simply means converting each category in a variable to a number. It is more suitable for ordinal variables — categorical variables with some order.</ul>\n\n<ul><b>2). In One hot encoding</b>, each category of a categorical variable is converted into a new binary column (1/0).</ul>\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Label encoding for the categorical variables\n\nWe will label encode Outlet_Size and Outlet_Location_Type as these are ordinal variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# a = ['Outlet_Size','Outlet_Location_Type']\nle = LabelEncoder()\ntrain['Outlet_Size']= le.fit_transform(train['Outlet_Size'])\ntrain['Outlet_Location_Type'] = le.fit_transform(train['Outlet_Location_Type'])\ntrain['Item_Fat_Content'] = le.fit_transform(train['Item_Fat_Content'])\ntrain['Item_MRP_Clusters'] = le.fit_transform(train['Item_MRP_Clusters'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding for the categorical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['Outlet_Identifier'].unique(),train['Item_Identifier'].unique(),train['Item_Type'].unique()\na = pd.get_dummies(train[['Item_Identifier','Item_Type','Outlet_Identifier','Outlet_Type','Item_Type_new','Item_Category']])\ntrain = train.drop(['Item_Identifier','Item_Type','Outlet_Identifier','Outlet_Type','Item_Type_new','Item_Category','source'],axis = 1 )\ntrain = pd.concat([train,a],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PreProcessing Data"},{"metadata":{},"cell_type":"markdown","source":"#### What is Data PreProcessing?\n"},{"metadata":{},"cell_type":"markdown","source":"In simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm. It invloves further cleaning of data, data transformation, data scaling and many more things."},{"metadata":{},"cell_type":"markdown","source":"For our data, we will deal with the skewness and scale the numerical variables"},{"metadata":{},"cell_type":"markdown","source":"#### Removing Skewness"},{"metadata":{},"cell_type":"markdown","source":"Skewness in variables is undesirable for predictive modeling. Some machine learning methods assume normally distributed data and a skewed variable can be transformed by taking its log, square root, or cube root so as to make its distribution as close to normal distribution as possible. In our data, variables Item_Visibility and price_per_unit_wt are highly skewed. So, we will treat their skewness with the help of log transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Price_Per_Unit_Weight'] = np.log(train['Price_Per_Unit_Weight'])\ntrain['Item_Visibility'] = np.log(train['Item_Visibility'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlated Variables\nLet’s examine the correlated features of train dataset. Correlation varies from -1 to 1.\n\n<ul> 1). negative correlation: < 0 and >= -1 </ul>\n<ul> 2). positive correlation: > 0 and <= 1</ul>\n<ul> 3). no correlation: 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation plot above shows correlation between all the possible pairs of variables in out data. The correlation between any two variables is represented with a number. a positive number represents positive correlation and negative number indicates negative correlation.\n\nVariables price_per_unit_wt and Item_Weight are highly correlated as the former one was created from the latter. Similarly price_per_unit_wt and Item_MRP are highly correlated for the same reason."},{"metadata":{},"cell_type":"markdown","source":"### Model Building"},{"metadata":{},"cell_type":"markdown","source":"Finally we have arrived at most interesting stage of the whole process — predictive modeling. We will start off with the simpler models and gradually move on to more sophisticated models. We will start with the simpler linear models and then move over to more complex models like RandomForest and XGBoost.\n\nWe will build the following models in the next sections.\n\n<ul><li>Linear Regression</li></ul>\n<ul><li>Lasso Regression</li></ul>\n<ul><li>Ridge Regression</li></ul>\n<ul><li>RandomForest</li></ul>\n<ul><li>XGBoost</li></ul>"},{"metadata":{},"cell_type":"markdown","source":"####  Evaluation Metrics for Regression"},{"metadata":{},"cell_type":"markdown","source":"The process of model building is not complete without evaluation of model’s performance. That’s why we need an evaluation metric to evaluate our model. Since this is a regression problem, we can evaluate our models using any one of the following evaluation metrics:\n\n"},{"metadata":{},"cell_type":"markdown","source":"<b>Mean Absolute Error (MAE)</b> is the mean of the absolute value of the errors:"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/390/1*8DXbECB9pnKxTpIvuVD-vg.png)"},{"metadata":{},"cell_type":"markdown","source":"<b>Mean Squared Error (MSE)</b> is the mean of the squared errors:\n![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAawAAAB2CAMAAACjxFdjAAAAilBMVEX///8AAAD8/Pzt7e35+fnw8PD29vbp6enz8/Pf39/l5eXCwsLh4eE7OzvX19fr6+uurq4cHBylpaXKysqLi4tpaWnQ0NCTk5MzMzOBgYGenp5eXl7GxsYhISGysrISEhJOTk4vLy+5ublzc3NDQ0MVFRVXV1eXl5d6enpISEhubm4nJycxMTFbW1tPfSOWAAAP4ElEQVR4nO1dCWOquhJuWGQTZBdBQXGX4///ey8TAoIGpK8u9TbfPbc9pyINme2bySR+fXFwcHBwcHBwcNyHpOvwTVHePZC/C0kaeGG+ytBeyHeZYz91QBxdEOyTKwy7NEcIbXz8BZnPHRMHEyN3g1AwUFiSA3IyDggVzx0VBwteAnZyGiisCb524X25WLzPHRbHLdTQ34Gwzuqw67EbzHC0miHkPndgHLeYjCV5A3FoMux6HK7O+FITzIvjDZiBsIZR8fGGmJSGveDoyaPiYMLGwjoOsyy4VPv6MhCKnzwoDjby4TELe8E9/hYjlGvctN4BTO3QUh50qYkiKGBgx5kkA6Mcx0MBvm2jD7lSOiKSPe+xsKwnj4qDCXCDkTboUs0j+Zjo+wOpPseDsYeixCDL4ng7Yi6szwEQjGj87lFwDII9PCnmeDdyLqzPAQhrKb57FByD4HNhfQ5mnA1+DlIsrN2wchPHu2FgYQVDO2Y43gsurA8C5Fl8LfFDwGPWB8G+Q90tIzU8I3VjN3VtG381ctff23ma2q77mWUqLY3zzywD+HeoO34922bQA0W+VN/Kr0N72H4RBG+2gE4G+/OG/mX9g0mfdQctZYW6cfw8/5kiJznC2D+tm07Q/Kic9YPXKS5jQS3qYBu5EVpWGBpG6JLu0OLjVoyF9ckayXMTodWHjT1vGEneeZVNr5i2HIeQQi91+vQxPhjWjEQrWBjaM14WZC0MNcULWbor6/pYF1W1ek3Q0/00zi2c+Eghue1IFXVVxhfJsjiZ6GPNCi1RfEwxb2LHcWznuW3b8+6VeiGgpnWliiDEez1p0lsDuXw7TeOQfBOxaa0Z78hBA9EWu5rbkOYWW/zq1skOZK4EI3HKiVnO4jM64B/pZrFAq5XjFMXKyZwyrhdO1m0Ijwc0dwLWV7nzGltb/zuN3ex5w2pikqfGLUtyF36Hr5M2bF8yiakEspueY/FYzkLgWkSQMb4wWrtp2YGOdvhHcpxsEZWQY5pFUZQCS14Z2g3qCK92ZVkF8vveJuy30UsCg+At8aQsDfi71Egn5CnasU1b33bmK/Nyvm8aX/PSUCr19MCkSv0wQFxZ+XvnBaEAoq5IgqgoupWfsExfGh5dKq32LxVmvZYlH9DiNaOcUw4Ui3hMrQ0uMQqYHY7TniC9JPdy5u2fVsHAKP+pYoJlhvQ18YBfCOs7t5mmuOnSmCdBWlPv3O5ZC/ssC4vSMZ48LooEbY5E73ep3+YNasCUlhYhv5P+0mddt6MW9S6V+lmLZlzDz1oJ37hxQVM0e21ONzGptNqPeO62LHXWxzAfCs/Bv8ijgTVpa7F8ZESM0QkdujNL6kailpus1HVF1RWI9OESw73amiznmlrPt69O6byiHGybq2vd3aFGGXNfAZdw0nBFJvO6AobH4V8rtouOPXQ6pT6/NcXhAq0hNFXFHqDC58ZN1pWTCbc3wd17+UI8zbai8P6lgNHydXtNqJHoM3Nn30QHKbjJBnMU9C2N52hLrNRsaqKPIr1ZmQPzKxoGZFV7fa3fkH7WYWvY2pd7E+CeD2nCosjYUJKmzxPCVdCr6gaWwqZJ/L4gOqPpCGiESbUhBE7zz7pErdRuCKthlNZb9h1OSpaEZkMa3bXVDdF/F/QFamWlxhJIIx6i3SGyHBUhOZIgupipjxYaaVeptgVMSuNrPCOVm5e1hCUsXxUM2vBYvrwDWJ2za8NSxVrMwuN2NihWPaUSk3SNMDcKLtNurMy9bcf23lx0CCtFC8MiNdOa62oF1DuAlZs0Jgo+5YzzK6ZiQcy6MFIPRT21oZ7n+imqyHs/bAk4C4ma/lLQw+mq2FKyOzpED9k0NPLcAzbhQ/mvcRLtWPO/bg65egbUvb7jYssq06V66chGW68UVr2VTaaOxpm1n4QQjMqBivkSFezyhZLv/ZmPMdvHsWvH05kfT/3Z7FGZKXHaaEj1RMmuSm+qm5DEtdRLvWBX5r4LxSfOKCvtBiz/hvh9XRVt3ZUTmY6TFUu/i2TswdeJxLTi8nbyDp0kKqxaHeQZFfoibrIa4ga3AZ756Swhw2MHrRNi49YQ5YkyVkRFl0VFlEVd163xRNTHhtG/9KvSXObQe9UXicc3XFAN6knDSf2DVlcELaiZ8ji6YnAU4JiO9YNhZyyJot6jbzYZG3FzlOGlJb+7arGsPCGWYMPXhNvr6WcLK65fJ9LN6ntdEzgrWG7MzW5j7pbH5Q7+HplH/G2xNfvzWI/Wme9RUxjJTfVCM2svkl+c/09h4Mk5lXEDZxcBQwiT6Huq4ZKrYbS0nIvZUgEyullUt4IVne9pHblIzFqtsYub+odg0x01Jl5ohfh/fSJqY1G3NGwximXcPECtEre4w8ypy3fuhK0DS1jgRpxSzYQAdZfjBc3Sxooiq7KsKroSGtrYCi3LYhfYlOVlOAcm+VF3A9SrgX1ZpiCTtAYhYCVIwMSIZbV0bKTRhOZSvdJAn2NJALYjjLTlLdH6JlKGlCjWdxgKDav9+a508U1NzC9vnDFkSaH6ZmSam80uOZ+TjXlcbSNzVaxWq468DbZuUhmtEctcpcPdAbcQowUIn5gIkH4cscqKINkkej0Ij85I1c53Rd095PywiitouW2Ec2MeapYXaqHn4T+GEYZGeC/l1UjY2vR7FZKEMJQc/zghfxmZTmfQ0BZdisSecIiPZRCVUcS6AKqs36ml5qXTJBoHa3he9W72jl6FEg2qfSEIy6+nUdoVb9yATcqc8/5rdPD3jOiHH2tLHtbqKdfLfhAsj0myS9annbk5rYPzcpMk56XPVlHpjJWHTM6cnQIKYFmMld8u5NSthqUMMDHKSkFAJP7HMO+YqFdSCoVUMBpqpXfMlaTPc2wjBo5bnmVp2NXPsbVo40f22wox22haIFGCUcCYV++dZjTKWMGQOofQO9Mu1Z5RsKRK7PnzxjtGwfeEZdPISp4CHQ8ZOpdvhigWMQi/BCZXNeDMwbKm93+bbWbIcbaoWGyhC+BfVGwzZ7H6d2ft/VuA2rZ/z1dKCZNgEPcICiibVA0xJXB+TOHDolytSCslUovWOu/oNEC/GnArwjKv+DRlJ5BnlUskYtLSMWlz8YMkZl37DYa2VVnaNY6P23cwigadNQj+jiEEWF49YmfmZ5V+xszM6HtQShVQi6pmIiTZrPHI0omtOl1wS4JRrhwQ7kDjK1hWuaPXKooWlSHMsZQfcYNX7kLd306GZsd7jNj1p7HvulOcQ+M/M/dxdV/JrxsPegGDZ/3WKVHN+aWereYDl1z6BpWQ/G16sR45bJadoDC0vRNlm3DrrKxcHa5cNrGs8mCD8aadeoAfpFkJSYqv7NhHr16A+CqXScwhZgqLX6xEygAyrJl9S3//B2ZwV7fzBKpx8b2NuHuU0ckm0Q7tqphPkmIiLMgGmsYyq8VY9mK2hTVH2Rv2AWNyEQ1aqtcQ+6g1UjM81c+iGPYjik5kD3tU00/N9VtOz0MDCmQNNDrsSEdTdV8JvCI1Opypmg1jhRi9vwymnSiE/5D5evqeDi8SYZV0WJeSSLyjxgnFvOgBx6tZxFtV8gDKuWm+7LLziC4Ixwt3hF7Q2g1MSK2tlAk4SMeuTBkcidkIbE1hjePoavXzJcDZajE0/LlVRnUFUp2pZm4EweYBOieC/meVokMIOzVfxtPnDN7hrmrg0rKDUSZ1cU0FVY/2os2gcKAQouj4oShrFhCYsrwy0m3Sq+LMUiOP97bvn3vS+SdCYzexhkxHBmkxK4MiwayOeutbkvt/AWZrWt/Vb8cM4NWD+4K1hDYG0W4YC63o+suuJtdbnLoIG1TAIsriuDThHQdy1TiImHT8UUXrwbCOzJkVTbbWuO3mnwrYfWwuPCB40MabfV0/ACTtdMDafqPmrhySJFjulstkXza0uzT8zYPzYb1O1ofTbp1ivXDPsmS5R3OBw9hyRg13vE6Wy9M62e2CZRKcgyDB/52S5BtU9CGwsKnsGXl53NFqgVWPZfxe1vCkYlRlND9EjBoni2Kn2KxGCz57v0gHJFUdibIo3gQZSRUkSZWEkUxuLhFNnOhWOLcml7aZkTiRBFWWR7KKb4ShjiT55ewiYX+uwnjVFbuBjdyGVRvtL9Qf6/y9Iv8wrJtrRnnbAWMWw1zs/+9CmLJ3iYsJyjpit7Bm+Lh5q7QQP8gL2k0bhgXpprmemLXX/zLijFmRgKYMhvmUwCptXjk5fdesBELK+QhvPl80u+NkzAQu7XwQJL/hBP8DALvaXj+yIIYxZE3nTkc2Dq52kYgbZDSuxiHLSY0f+0GjXawc4xTUrS3N2JaLvX8HBs4GC9/N0zxPbZecqWDHs11Jcnu4t3pASyotDSu7eGrvEFLwDcwBiwmdGBmaIBhRu26vFWhR915aZhb/rYBVrxSw0FshFeLsH0lAgLEn0RWdxG5wG/9AVuoJOcub5E8OsPeld3Wz3qb2/yDCTY+s7n3mTJiCYktlk9w1ndTT8CdOkCygoyi9usc4rQmFO/u8ox9+BOumD66FYb2aULhY7B8cPLANoe3uj1G9fnStaFIMo3OC7T+kvt6G4rqv3/r0q6GEVmjpmq7oiobzdU3XLEsb62NFH+O//rGQ8D6IXloWvlTDzbmG/l4I4ZpQ7xCaKqFU/PNWCI5nQRjvSTXf/hrTBuC/led/Gsgm4dAyz3PlwIX1ywGr3TsxOMhlq+kjmws5HgySqPo+qaWf0K/ZAczBAmzRdoIN1MVHlGpw/FaQlqmC5LR61rPSwfELQCoT5eICyC35wCNj/wwksguTFKzJ3lgesn4xSGNkedqjer6/75TjnSA7VcuqxdzBIevDTvf9UyBbN8sto2RzBOv8CI5fAnFZRawvAfrt33IaFMcwhE7dWwbEPRt/TbpWO/ptbqRxm3w2gADSVhfYdWGK8rpjb6CWrLs7SsX4vHjlOdh/E+D66CyTT45xd127UvyeAz8U6NbnH771bBTY9VHplCeYdO7DNpxupjjJI+xN33jOw5/AGF22wcAuRZR0b5Ae952Q4jd2cXI8ByMjvXyClDHdX59wOBjuY86i43gAJCudppUgWQLdP2g7HMfPoe3oEb7WbHYIfNd1p/HeTaduJbjp1ZZQjjcCx6QNND7ZZrRdRCZ8GIq5WazWlQ/FwjrzmPVLsKdHSEgynGAoivJEEWV9Mmq8PuDAGY5XAEqIh748yudu8NcAztwjZUM59FL4NFU/NvI8rY8lBGGydq9yvAG6iTbE5V2O3yU40kIiCIsnxb8EeUbTKBk+l9PDfyyMy/50OHfxwQcxcfy/8O8cjgL9bLyC8UuADcdPeywH9nDf2U7H8SIIZ5QtZt1scETOG51+5kef/+cAZK9nC1a+2ywcJ3r3p0VxEIy8/kYa4UtknJzDwcHBwcHBwcHBwcHBwcHxMfgfqjHp7n8DExYAAAAASUVORK5CYII=)"},{"metadata":{},"cell_type":"markdown","source":"<b>Root Mean Squared Error (RMSE)</b> is the square root of the mean of the squared errors:\n![](data:image/png;base64,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)"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"Linear regression is the simplest and most widely used statistical technique for predictive modeling. Given below is the linear regression equation:\n![](data:image/png;base64,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)"},{"metadata":{},"cell_type":"markdown","source":"where X1, X2,…,Xn are the independent variables, Y is the target variable and all thetas are the coefficients. Magnitude of a coefficient wrt to the other coefficients determines the importance of the corresponding independent variable.\n\nFor a good linear regression model, the data should satisfy a few assumptions.\n\nOne of these assumptions is that of absence of multicollinearity, i.e, the independent variables should be correlated. However, as per the correlation plot above, we have a few highly correlated independent variables in our data. This issue of multicollinearity can be dealt with regularization.\n\nFor the time being, let’s build our linear regression model with all the variables. We will use 5-fold cross validationin all the models we are going to build. Basically cross vaidation gives an idea as to how well a model generalizes to unseen data."},{"metadata":{},"cell_type":"markdown","source":"#### Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('Item_Outlet_Sales',axis = 1)\ny = train['Item_Outlet_Sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train - Validation Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_valid,y_train,y_valid = train_test_split(X,y,random_state = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,X_valid.shape,y_train.shape,y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression(normalize=True,fit_intercept= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train,y_train),model.score(X_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_valid, y_pred),mean_absolute_error(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBOOST REGRESSOR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train,y_train)\npredictions = my_model.predict(X_valid)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(y_valid,predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LASSO REGRSSOR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = Lasso(alpha = 0.01)\nls.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = ls.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(y_valid,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}