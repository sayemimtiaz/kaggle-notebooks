{"cells":[{"metadata":{"_uuid":"a722f16a6ee48aaed5d38fa7d737662e55c71053"},"cell_type":"markdown","source":"# Optimize Your Algorithms\nDo you ever wonder if you can squeeze some more performance out of your models? I certainly do. Using the default parameters of a model can be useful if you need a quick indication of its effectiveness but it won't always give you the accuracy you need. Here is where the parameters come in. By tuning your parameters, you can increase (or decrease) the accuracy of your model. In this kernel I'll show you some predictive models and I'll also show you how to tune them.\n\n## Index\n1. Importing Libraries and Data Cleaning\n2. Data Analysis\n3. Testing different models with default parameters\n4. Support Vector Machines (SVM)\n5. Ridge Classifier\n6. Decision Tree Classifier\n7. K Neighbors Classifier\n8. Extra Trees Classifier\n9. Random Forest Classifier\n10. Gradient Boosting Classifier\n11.  Ada Boost Classifier \n12. Final Score\n13. Conclusion\n14. Sources\n\n\n\n"},{"metadata":{"_uuid":"41fb853d342130392dadade91adae8a03473c6b6"},"cell_type":"markdown","source":"## 1. Importing Libraries and Data Cleaning\nBefore we can start trying different algorithms, we will have to clean the data we're going to use. This will ensure more optimal results later on.\n\nWe are going to start by importing the libraries we are going to use and turning our csv into a pandas DataFrame."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n\nwarnings.filterwarnings(\"ignore\")\ndata = pd.read_csv('../input/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0182f9cd475fd22548da3e51d82c2291c1b4374"},"cell_type":"markdown","source":"Let's take a look at the data we've just imported."},{"metadata":{"trusted":true,"_uuid":"71b3713e67e57c7f25777ea6ad1b01f82551b0eb"},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fc26f25c304253a1345c7a4640a675fc6dcfb46"},"cell_type":"markdown","source":"We get a lot of data about the cell nucleus: radius, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension and diagnosis. \n\nFor each point of data (except diagnosis) we get 3 numbers: the mean, the standard deviation and the worst.\nThe worst of a data point is the highest/largest number of the data for this cell nucleus.\n"},{"metadata":{"trusted":true,"_uuid":"01743005e741956a3d84e5efc60bc9e443bf3c68"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6359fa24f6f33f4219cf2acc1bc8ddfc9640bd"},"cell_type":"markdown","source":"There seems to be a column named **Unnamed: 32** that is full of NaN values. We can drop it without affecting our data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.drop('Unnamed: 32', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0dfa9dbe7b2ba11cdcbc3ba87ae92595a90b309"},"cell_type":"code","source":"print('Data has {} missing values.'.format(data.isnull().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46fde9ea00b54aec7f3caed361e14e01262a7903"},"cell_type":"markdown","source":"# 2. Data Analysis\nNow that our data has been cleaned, we can take a look at what it contains. Let's start by looking at the distribution of benign and malignant cell nuclei.\n\nThe data contains 357 benign and 212 malignant cell nuclei. This means that approximately 1 out of 3 cell nuclei in the dataset is malignant. This should be more than enough to train our models."},{"metadata":{"trusted":true,"_uuid":"94c0474a340c193f294f0d975342767f6e48be89","_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(data['diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b574022c15f8c7aecdba632a0248815311e024f4"},"cell_type":"markdown","source":"Now let's take a look at some of the differences in the data between benign and malignant cell nuclei, starting with radius and texture.\n\nThe mean radius and texture of malignant cell nuclei are significantly higher than those of the benign cell nuclei. So, these are clear indications of cell nuclei being malignant."},{"metadata":{"trusted":true,"_uuid":"f75ee6029e6fbfca12380440074cc8bf464df49d","_kg_hide-input":true},"cell_type":"code","source":"data.groupby(['diagnosis']).mean()[['radius_mean', 'texture_mean', ]].plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682e1267fbf703cea7cdd9455580c18fafc4280b"},"cell_type":"markdown","source":"The mean of the perimeter also shows a significant increase when a cell nucleus is malignant. Another clear indication."},{"metadata":{"trusted":true,"_uuid":"8e928319fbad705a71054aeb2c54e117bd5130bb","_kg_hide-input":true},"cell_type":"code","source":"data.groupby(['diagnosis']).mean()['perimeter_mean'].plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08d84950dfe34b4d2d87bdde0bb88ba678f83a09"},"cell_type":"markdown","source":"Looking at the remaining data we can see this trend continuing for every data point except fractal dimension. The mean of the concavity and concave points in a malignant cell nucleus are more than triple the mean of a benign cell nucleus!"},{"metadata":{"trusted":true,"_uuid":"c3ce9e2d4009855fbc645bde39125c0b939f0e00","_kg_hide-input":true},"cell_type":"code","source":"data.groupby(['diagnosis']).mean()[['smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'fractal_dimension_mean' ]].plot.barh(figsize = (8,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97b54449fd4d68945fd7b3838e8833b627119ad9"},"cell_type":"markdown","source":"When we take a closer look at the fractal dimension of a cell nucleus, we can see that while the mean doesn't differ much between benign and malignant cell nuclei the worst values of malignant cell nuclei are higher than those of benign cell nuclei."},{"metadata":{"trusted":true,"_uuid":"63962306143cf68e50a32fa9fa19b698b11315ce","_kg_hide-input":true},"cell_type":"code","source":"data.groupby(['diagnosis']).mean()[['fractal_dimension_mean', 'fractal_dimension_worst',  ]].plot.barh(figsize = (8,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e7cc332f62b87735e97a08bb90c4427c881128b"},"cell_type":"markdown","source":"The standard deviation of the fractal dimensions of a malignant cell nucleus are also higher, though not as significantly as some other values."},{"metadata":{"trusted":true,"_uuid":"9139436184af8451fe1cbac2d73c9605d596e351","_kg_hide-input":true},"cell_type":"code","source":"data.groupby(['diagnosis']).mean()['fractal_dimension_se'].plot.barh(figsize = (8,4))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc7a1cb5e1ded43c9c94ebe78336c50f77165e8"},"cell_type":"markdown","source":"Before we take a deeper look at these correlations, we have to edit our data so the diagnosis becomes a boolean instead of a character."},{"metadata":{"trusted":true,"_uuid":"6e7d84fbc59491899e25faafe4f038007f83c799"},"cell_type":"code","source":"data['diagnosis'].replace(to_replace='M', value = 1, inplace=True)\ndata['diagnosis'].replace(to_replace='B', value = 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"469abcf72ec58c60fbbb85bfd3282338a732ff08"},"cell_type":"markdown","source":"Just as we concluded earlier, a high value of the radius, texture, perimeter and smoothness are all indicators of malignant cell nucleus"},{"metadata":{"trusted":true,"_uuid":"73a1134c1a2d2e7fc2f8f708c6b267536fd72986","_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(data[['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean', 'smoothness_mean']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9b5f4b7e5dd575e711758574ce43d365193866d"},"cell_type":"markdown","source":"Again, these correlations match up with our earlier conclusions. The fractal dimension mean is the only mean that isn't effective when trying to identify a malignant cell nucleus."},{"metadata":{"trusted":true,"_uuid":"a6cdcd9f6f9e86897f4b29b670b79acd21b11b27","_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(data[['diagnosis','compactness_mean', 'concavity_mean', 'concave points_mean', 'fractal_dimension_mean']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"602348968db647ba391d300bebd6e3a8510861c4"},"cell_type":"markdown","source":"And finally, the fractional dimension worst and standard deviation are also both useful in predicting malignant cell nuclei."},{"metadata":{"trusted":true,"_uuid":"989c6cb1e17731a9cf0be8cb980e86400596fb38","_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(data[['diagnosis', 'fractal_dimension_mean', 'fractal_dimension_worst', 'fractal_dimension_se']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e4a06cc8cbed52740f7dffcc76734888388ba4b"},"cell_type":"markdown","source":"# 3. Testing different models with default parameters\nBefore we start optimising our models, we'll first run them with the default parameters. These values can later be used to compare the optimised models against them. \n\nThe models are going to predict whether the cell nucleus is malignant or not. \n"},{"metadata":{"trusted":true,"_uuid":"9cf851a384baf8dd77dba328bacff69c4afe5629"},"cell_type":"code","source":"y = data['diagnosis']\nX = data.drop(['diagnosis', 'id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"328395bb63244fc8b01c7a531d0eff6d1a98cd5a"},"cell_type":"markdown","source":"To view the modelâ€™s performance, we'll make a new DataFrame in which we'll store the Algorithm, it's scores and the standard deviation. When possible we use **random_state=0** to ensure fair results."},{"metadata":{"trusted":true,"_uuid":"1ea19ce094d6f0cdafd4a86824910ad335d273d9"},"cell_type":"code","source":"from sklearn import ensemble, linear_model, svm, neighbors, gaussian_process, naive_bayes, tree \n\nscoreFrame = pd.DataFrame(columns = ['Algorithm Name', 'Average', 'Standard Deviation'])\n\nalgList=[\n    #linear\n    linear_model.Ridge(random_state=0),\n    linear_model.SGDClassifier(random_state=0),\n    #Neighbors\n    neighbors.KNeighborsClassifier(),\n    #SVM\n    svm.SVC(),\n    #Gaussian Process\n    gaussian_process.GaussianProcessClassifier(random_state=0),\n    #Naive Bayes\n    naive_bayes.GaussianNB(),\n    #Tree\n    tree.DecisionTreeClassifier(random_state=0),\n    #Ensemble\n    ensemble.GradientBoostingClassifier(random_state=0),\n    ensemble.RandomForestClassifier(random_state=0),\n    ensemble.ExtraTreesClassifier(random_state=0),\n    ensemble.AdaBoostClassifier(random_state=0)\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a184bfc16c8c565c73c0bea45c681d85871af2bf"},"cell_type":"markdown","source":"We are going to test the models using cross validation score. This function fits and tests the model multiple times and then returns a list of scores. Using the mean of this list we get an average score of the model."},{"metadata":{"trusted":true,"_uuid":"660bb8290dc03b6a12560a8f893293ca202b4ef9"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfor alg in algList:\n    scores = cross_val_score(alg, X, y, cv = 10)\n    algName = alg.__class__.__name__\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    scoreFrame.loc[len(scoreFrame)] = [algName, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2e671e7b9ef357d06fe93974b3d35b1d89b9d8e"},"cell_type":"markdown","source":"Using the sort_values function on the DataFrame gives us a nice overview of the performance of all the tested models."},{"metadata":{"trusted":true,"_uuid":"704bed4acfbbe70e8ba57c6fde40977f450c4827"},"cell_type":"code","source":"scoreFrame.sort_values('Average', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8542f9c5c61f56f892d9781d93a682c33c5f0c5"},"cell_type":"markdown","source":"# 4. Support Vector Machines (SVM)\n"},{"metadata":{"_uuid":"e2cf58aaafbed271cba7558400a5c3c647b792ea"},"cell_type":"markdown","source":"### How does it work?\nAn SVM uses hyperplanes to classify the data. Hyperplanes are subspaces that consist of one less dimension than the original space. So a 3D space becomes 2D, a 2D space becomes 1D, etcetera.\n### What parameters can we tune?\nSVM's are able to use different kernels to classify the given data. These kernels can all be tuned using different parameters.\n\nKernels: Parameters\n* Linear: C\n* RBF: C, gamma\n* Poly: C, gamma,  degree\n\n### Default parameters\nBy default SVC uses these default values:\n* kernel = 'rbf'\n* C = 1.0\n* gamma = 'auto' ( = 1 / n_features )\n* degree = 3"},{"metadata":{"_uuid":"91dea15e1ba848c87549098386b947ce0ec47804"},"cell_type":"markdown","source":"## C - Penalty Parameter\nC is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n\nLet's start by creating a List with some basic values to check the average score they give our model."},{"metadata":{"trusted":true,"_uuid":"952ecc26c91d0f7e6a2b24afd649937430c383b2"},"cell_type":"code","source":"svmPenaltyFrame = pd.DataFrame(columns = ['C', 'Average', 'Standard Deviation'])\n\nfor c in [0.00001, 0.0001, 0.001, 0.01, 0.1,1,10,100,1000]:\n    alg = svm.SVC(C=c)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmPenaltyFrame.loc[len(svmPenaltyFrame)] = [c, scoreAverage, scoreSTD]\n  \nsvmPenaltyFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"65c620b0d9be41668bdb78fe993eb44a5a5f6351"},"cell_type":"markdown","source":"These values don't seem to change our predictions, so we'll wait untill we have some more parameters figured out and then we'll come back to the C parameter."},{"metadata":{"_uuid":"64819b166914f886eb7141a160066e985c627536"},"cell_type":"markdown","source":"## Gamma - How exact should I fit the data?\nThe higher the gamma value the harder it tries to exactly fit the training data set\n\nLet's start by using a range of 0.001 to 1000"},{"metadata":{"trusted":true,"_uuid":"25d11e8529e16e1d56ce4b9654953d94ec082180"},"cell_type":"code","source":"svmGammaFrame = pd.DataFrame(columns = ['Gamma', 'Average', 'Standard Deviation'])\n\nfor g in [0.001, 0.01, 0.1,1,10,100,1000]:\n    alg = svm.SVC(gamma=g, C=0.1)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmGammaFrame.loc[len(svmGammaFrame)] = [g, scoreAverage, scoreSTD]\n  \nsvmGammaFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d845401f7bfab053494cc91b77d4d93539e5cb86"},"cell_type":"markdown","source":"This range doesn't give us any different scores, so we'll try a more extreme range.\nThis time we'll use all numbers between 0.000001 and 0.001"},{"metadata":{"trusted":true,"_uuid":"d03e4af0e19d7224b8832b036e10172839e94465"},"cell_type":"code","source":"for g in range(1,1000):\n    g = g/1000000\n    alg = svm.SVC(gamma=g, C=0.1)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmGammaFrame.loc[len(svmGammaFrame)] = [g, scoreAverage, scoreSTD]\n   \nsvmGammaFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac2bd73e978eda95938fd5712cf3404e1843ddbe"},"cell_type":"markdown","source":"Now we get some good results out of the SVM. The accuracy became around 0.3 points higher!"},{"metadata":{"trusted":true,"_uuid":"05d300cd17538b0a49eb6c525815a4691d3b09fc"},"cell_type":"code","source":"optimalSVMGamma = svmGammaFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb2878f26d0bcab93f203f14e102832c21c85146"},"cell_type":"markdown","source":"## Kernel - How do I fit the data?\nThe kernel decides how the data gets seperated on the hyperplane."},{"metadata":{"trusted":true,"_uuid":"2f2c96e98d2a19b7e4dcfde60f17368dabc2f561","scrolled":true},"cell_type":"code","source":"svmKernelFrame = pd.DataFrame(columns = ['Kernel', 'Average', 'Standard Deviation'])\nkernelList = ['linear', 'poly', 'rbf']\nfor k in kernelList:\n    alg = svm.SVC(gamma=optimalSVMGamma, kernel=k, C=0.1)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmKernelFrame.loc[len(svmKernelFrame)] = [k, scoreAverage, scoreSTD]\n    \nsvmKernelFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca31242231b593a76b5c95b0b53f403662914263"},"cell_type":"markdown","source":"It turns out that the Poly Kernel is the best type of kernel for our data"},{"metadata":{"trusted":true,"_uuid":"9d87c2c2bcce8f0d52aaaf087b8bc9fd2b4939a9"},"cell_type":"code","source":"optimalSVMKernel = 'poly'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b91cb39acd63b717e0bfe6c90b6cdfd05bf4ec8a"},"cell_type":"markdown","source":"## Degree - How sharp are the Poly Kernel's lines?\nDegree determines the degrees of the polynomial used to split the data on the hyperplane.\n\nWe'll try values between 1 and 3. Any values above this will only lead to really slow models."},{"metadata":{"trusted":true,"_uuid":"7a55080c5ee7be3dbae5cab0107d05d712876845"},"cell_type":"code","source":"svmDegreeFrame = pd.DataFrame(columns = ['Degrees', 'Average', 'Standard Deviation'])\n\nfor d in range(1,4):\n    alg = svm.SVC(gamma=optimalSVMGamma, kernel='poly', degree=d, C=0.1)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmDegreeFrame.loc[len(svmDegreeFrame)] = [d, scoreAverage, scoreSTD]\n   \nsvmDegreeFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a41614187670c81f44990b35df247e9f82d93737"},"cell_type":"code","source":"optimalSVMDegree = svmDegreeFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3195c1476b4dc1b5d536cb8c17c34c08898a6d"},"cell_type":"markdown","source":"## C - Again\nNow that we have some more parameters tuned, let's try our C parameter again. "},{"metadata":{"trusted":true,"_uuid":"6a7b449b3204009d48c360877c892c1248378123"},"cell_type":"code","source":"svmPenaltyFrame = pd.DataFrame(columns = ['C', 'Average', 'Standard Deviation'])\n\nfor c in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]:\n    alg = svm.SVC(gamma=optimalSVMGamma, kernel=optimalSVMKernel, degree=optimalSVMDegree, C=c)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    svmPenaltyFrame.loc[len(svmPenaltyFrame)] = [c, scoreAverage, scoreSTD]\n  \nsvmPenaltyFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8af94a04a5cd6ee3cb1cd98620b26d4148e9cb7e"},"cell_type":"code","source":"optimalSVMPenalty = svmPenaltyFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c804cb45bff194e11333ae7cc9261752321537de"},"cell_type":"markdown","source":"# 5. Ridge Classifier\n### How does it work?\nThe Ridge Classifier creates a formula that outputs the prediction. It chooses coefficients in this formula based on the data given. The closer these coefficients are to 0 the better the predictions.\n\n### What parameters can we tune?\nThe only parameter in this model that we can tune is alpha\n\n### Default parameters\nBy default Ridge Classifiers uses these default values:\n* alpha= 1.0\n\n## Alpha - Forcing coefficients toward zero\nRidge Classifiers use alpha to force coefficients more/less toward zero. If alpha is higher than it will force the coefficients more towards zero.\n\nStarting with a simple list of values from 0.001 to 1000 we'll try to find the optimal alpha."},{"metadata":{"trusted":true,"_uuid":"fd64316f179056930e95ec4b224b056c5ccc62a5"},"cell_type":"code","source":"ridgeFrame = pd.DataFrame(columns = ['Alpha', 'Average', 'Standard Deviation'])\n\nfor a in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n    alg = linear_model.Ridge(alpha = a)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    ridgeFrame.loc[len(ridgeFrame)] = [a, scoreAverage, scoreSTD]\n\nridgeFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22525abd474343c17db9a2af4a1bce2b1ce9fc07"},"cell_type":"markdown","source":"It's clear that lower alphas are better in this case, so we'll create a longer list with some lower values"},{"metadata":{"trusted":true,"_uuid":"be3ca7271af7f1a3474468964b077f80739dcbef"},"cell_type":"code","source":"ridgeFrame = pd.DataFrame(columns = ['Alpha', 'Average', 'Standard Deviation'])\n\nfor a in range(1,1000):\n    a = a/100000\n    alg = linear_model.Ridge(alpha = a)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    ridgeFrame.loc[len(ridgeFrame)] = [a, scoreAverage, scoreSTD]\n\nridgeFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91841dfa056a4daa737919c105f50edee3ac2ff4"},"cell_type":"code","source":"optimalRidgeAlpha = ridgeFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"23ae67df7a5dc1aab7bc9a587bf2e4f35f04662c"},"cell_type":"code","source":"sns.relplot(x = 'Alpha', y = 'Average', data=ridgeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02c326111184a83d29dde00a9e27d0e31bc714cd"},"cell_type":"markdown","source":"It's clear in this graph that we won't get a much better prediction than around 0.7, but to be sure we'll graph a large range of alphas."},{"metadata":{"trusted":true,"_uuid":"b64ca60e15115a349f3f1f05b0483a80b183a081"},"cell_type":"code","source":"for a in [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000]:\n    alg = linear_model.Ridge(alpha = a)\n    scores = cross_val_score(alg, X, y, cv = 3)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    ridgeFrame.loc[len(ridgeFrame)] = [a, scoreAverage, scoreSTD]\n\nsns.relplot(x = 'Alpha', y = 'Average', data=ridgeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"5cece15208b1bfd1dcbc3fc53b963a6e680a2854"},"cell_type":"markdown","source":"As expected, we don't see any spikes in accuracy above 0.7"},{"metadata":{"_uuid":"cc2cb0cd5aba9ffb4d83c26ecb5883495f4def47"},"cell_type":"markdown","source":"#  \t6. Decision Tree Classifier\n### How does it work?\nDecision Trees create a network of leaves (or nodes) and branches between these leaves. Every leaf has a condition that is either True or False. This then sends the user over to another leaf, etcetera. \n\n### What parameters can we tune?\nThe parameters we'll tune are min_samples_leaf and max_depth\n\n### Default parameters\nBy default Decision Tree Classifiers use these default values:\n* min_samples_leaf = 2\n* max_depth = None\n\n## min_samples_leaf - Minimum number of samples\nThe min_samples_leaf parameter decides if a Decision Tree is allowed to split a node. It will only be allowed if there are at least X training samples in both the left and right branch of the node, where X is the integer you gave the parameter."},{"metadata":{"trusted":true,"_uuid":"4b03d75cd13cda1a0aec57ac3669363d92afeb6f"},"cell_type":"code","source":"treeSampleFrame = pd.DataFrame(columns = ['Samples', 'Average', 'Standard Deviation'])\n\nfor n in range(1,20):\n    alg = tree.DecisionTreeClassifier(min_samples_leaf = n, random_state=0)\n    scores = cross_val_score(alg, X, y, cv = 5)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    treeSampleFrame.loc[len(treeSampleFrame)] = [n, scoreAverage, scoreSTD]\n\ntreeSampleFrame.sort_values('Average', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5209d1f473e441c95913f5c44b4abc639deb3998"},"cell_type":"code","source":"optimalTreeSamples = int(treeSampleFrame.sort_values('Average', ascending=False).iloc[0].values[0])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"cf7ed4c79bffc515798c1eb67fb0cb6bd212c017"},"cell_type":"code","source":"sns.relplot(x = 'Samples', y = 'Average', data=treeSampleFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5661f5a665d7467fa14f4341f70871a32e5568e"},"cell_type":"markdown","source":"## max_depth\nThe integer you set here determines the maximum number of layers your Decision Tree makes."},{"metadata":{"trusted":true,"_uuid":"cc0bf897598921e5e81b7b49dffab74a350dcef0"},"cell_type":"code","source":"treeDepthFrame = pd.DataFrame(columns = ['Depth', 'Average', 'Standard Deviation'])\n\nfor d in range(1,20):\n    alg = tree.DecisionTreeClassifier(min_samples_leaf = optimalTreeSamples, max_depth = d, random_state=0)\n    scores = cross_val_score(alg, X, y, cv = 5)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    treeDepthFrame.loc[len(treeDepthFrame)] = [d, scoreAverage, scoreSTD]\n\ntreeDepthFrame.sort_values('Average', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43f40d0f51ed2e69e1bd43767e2a42787e289b4b"},"cell_type":"code","source":"optimalTreeDepth = int(treeDepthFrame.sort_values('Average', ascending=False).iloc[0].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d761aa6cae81c097053d0cbb6c3d372ee4456752","_kg_hide-input":true},"cell_type":"code","source":"sns.relplot(x = 'Depth', y = 'Average', data=treeDepthFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dfcd2d84dd071e932a3fc419b7187dfabbfd682"},"cell_type":"markdown","source":"Max_depth doesn't matter much after 4 layers.\n\n## Depth & Samples\nLet's try both max_depth and min_samples_leaf at once to make sure we have the optimal values."},{"metadata":{"trusted":true,"_uuid":"6862fd01c036ae56bfbea7f0d4298b458b139030"},"cell_type":"code","source":"treeFrame = pd.DataFrame(columns = ['Depth', 'Samples', 'Average', 'Standard Deviation'])\n\nfor n in range(1,20):\n    for d in range(1, 10):\n        alg = tree.DecisionTreeClassifier(min_samples_leaf = n, max_depth=d, random_state=0)\n        scores = cross_val_score(alg, X, y, cv = 5)\n        scoreAverage = scores.mean()\n        scoreSTD = scores.std() * 2\n        treeFrame.loc[len(treeFrame)] = [d, n, scoreAverage, scoreSTD]\n\ntreeFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e60fd8bb36d91b50e33935ac00265e62a7c9b089"},"cell_type":"code","source":"optimalTreeDepth = int(treeFrame.sort_values('Average', ascending=False).iloc[0].values[0])\noptimalTreeSamples = int(treeFrame.sort_values('Average', ascending=False).iloc[0].values[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0c75fbf9a14aff02a0fff83f12d6af5a3ae3087f"},"cell_type":"code","source":"sns.heatmap(treeFrame[['Average', 'Depth', 'Samples']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c116fd602d000d917c5e4cccb28f6faf6768b152"},"cell_type":"markdown","source":"Using the graphviz library we'll visualize the end result of the Decision Tree. \nThe leaves show multiple pieces of information:\n* Condition of the leaf\n* Gini (or chance of incorrect measurement of a random training sample at that point)\n* The number of samples that passed during fitting\n* Class (or prediction) of the sample at that point\n\nAs we can see in the image, not all bottom leaves have a gini equal to 0.0. These leaves are the ones that could give use incorrect predictions. These could be improved by gathering more data that involves these leaves, though this is not always possible."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fef7e4df298ae3d917ed216751336a07b6e23b1c"},"cell_type":"code","source":"DTC = tree.DecisionTreeClassifier(min_samples_leaf = optimalTreeSamples, max_depth=optimalTreeDepth, random_state=0)\nDTC.fit(X,y)\nimport graphviz\ndot_data = tree.export_graphviz(DTC, feature_names=X.columns.values, class_names=['B', 'M'], filled=True )\ngraphviz.Source(dot_data) \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7905afcd9d09e8d968879f7da19c725c94ca243"},"cell_type":"markdown","source":"# 7. K Neighbors Classifier\n### How does it work?\nThe K Neighbors Classifier searches the correct answer based on the data it already has. It tries to predict the classification by looking at which data points are near it and what they are classified as.\n\n### What parameters can we tune?\nThis algorithm uses 3 parameters that we can tune: n_neighbors, weight and algorithm.\n\n### Default parameters\n* n_neighbors = 5\n* algorithm = 'auto'\n* weight = 'uniform'\n\n## n_neighbors - How many points are around me?\nThe value of this parameter determines how much points around the target data point are measured to determine the outcome."},{"metadata":{"trusted":true,"_uuid":"5f428bd035a0d37db8e6a809c4dfec5c20a2e804"},"cell_type":"code","source":"kNeighborFrame = pd.DataFrame(columns = ['Neighbors', 'Average', 'Standard Deviation'])\nfor n in range(1,50):\n    alg = neighbors.KNeighborsClassifier(n_neighbors=n)\n    scores = cross_val_score(alg, X, y, cv = 5)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    kNeighborFrame.loc[len(kNeighborFrame)] = [n, scoreAverage, scoreSTD]\nkNeighborFrame.sort_values('Average', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7c942095bda94994b492e8f6dfcf2808dc38741"},"cell_type":"code","source":"optimalKNNNeighbors = int(kNeighborFrame.sort_values('Average', ascending=False).iloc[0].values[0])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5e20af389e7340c2f07b6d5bd51e2492a61eaebe"},"cell_type":"code","source":"sns.relplot(x=\"Neighbors\", y='Average', data = kNeighborFrame, kind = \"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a62d3abf845f7fa121d8a313872ee039933de687"},"cell_type":"markdown","source":"As shown in the table and plot, the optimal number of neighbors is 14.\n\n## Algorithm - How do I find my neighbors?\nThere are 3 types of algorithms to use for finding neighbors:\n* ball_tree\n* kd_tree\n* brute (force)\nIf you give the parameter auto the model will try to find the best model himself."},{"metadata":{"trusted":true,"_uuid":"8a78dab2ed7976cf373949e29acd35828110ba28"},"cell_type":"code","source":"kAlgorithmFrame = pd.DataFrame(columns = ['Algorithm', 'Average', 'Standard Deviation'])\nfor a in ['ball_tree', 'kd_tree', 'brute','auto']:\n    alg = neighbors.KNeighborsClassifier(n_neighbors=optimalKNNNeighbors ,algorithm = a)\n    scores = cross_val_score(alg, X, y, cv = 5)\n    scoreAverage = scores.mean()\n    scoreSTD = scores.std() * 2\n    kAlgorithmFrame.loc[len(kAlgorithmFrame)] = [a, scoreAverage, scoreSTD]\n\nkAlgorithmFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13babcd30f6ecf8e63ace63338fe4e3aa70183bd"},"cell_type":"code","source":"optimalKNNAlg = kAlgorithmFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"ebb55f125f368600510b6ab2358c223309eb949e"},"cell_type":"markdown","source":"It seems that the algorithm doesn't matter much this time, but it never hurts to have checked.\n\n## Weight - Are all my neighbors equal?\nWe have all these different neighbors, but they aren't all the same. Not every neighbor is equally far away from our data point. This impacts our predictions.\n\nWe have 2 types of weight to give to data points:\n* uniform - All data points are equal\n* distance - The closer a data point is, the more influential it is to the prediction"},{"metadata":{"trusted":true,"_uuid":"94227c06c990df35961ce127d0726801f81aa10f"},"cell_type":"code","source":"kWeightFrame = pd.DataFrame(columns = ['Weight', 'Average', 'Standard Deviation'])\nfor w in ['uniform', 'distance']:\n        alg = neighbors.KNeighborsClassifier(weights=w, algorithm = optimalKNNAlg, n_neighbors=optimalKNNNeighbors)\n        scores = cross_val_score(alg, X, y, cv = 5)\n        scoreAverage = scores.mean()\n        scoreSTD = scores.std() * 2\n        kWeightFrame.loc[len(kWeightFrame)] = [w, scoreAverage, scoreSTD]\n\nkWeightFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69128db6bf3c17112ea7de74abf7aead06793069"},"cell_type":"code","source":"optimalKNNWeight = kWeightFrame.sort_values('Average', ascending=False).iloc[0].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b0abc41b85876b015c8ccb4b9bc4994b43f603e"},"cell_type":"markdown","source":"The uniform weight apparently suits our data better, though it doesn't differ much from the distance weight."},{"metadata":{"_uuid":"6c642edea467e8f2198b2c838c4f40569f020c52"},"cell_type":"markdown","source":"# 8. Extra Trees Classifier\n### How does it work?\nThe Extra Trees Classifier fits a number of randomized Decision Trees on sub-samples of the dataset and then uses averaging to get a more accurate prediction.\n\n### What parameters can we tune?\nThe Extra Tree Classifier parameters that we'll tune are: n_estimators, max_depth and min_samples_leaf.\n\nWe have already discussed max_depth and min_samples_leaf in Decision Trees so we will skip their explanation in this part.\n\n### Default parameters\n* n_estimators = 10\n* max_depth = None\n* min_samples_leaf = 1\n\n## n_estimators - How many?\nThe value for this parameter determines how many Decision Trees are made by the model.\n\nLet's start by trying N: 1 -> 100, Depth: 1 -> 30 and Samples: 0.1 -> 1"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e203954090edf2648f126c196bdfeef33fb89284"},"cell_type":"code","source":"extraTreeFrame = pd.DataFrame(columns = ['n_estimators', 'Depth', 'Samples', 'Average', 'Standard Deviation'])\n\nfor n in range(1, 100, 10):\n    for d in range(1, 30, 5):\n        for s in range(1, 10):\n            alg = ensemble.ExtraTreesClassifier(n_estimators=n, max_depth=d, min_samples_leaf=s ,random_state=0)\n            scores = cross_val_score(alg, X, y, cv = 5)\n            scoreAverage = scores.mean()\n            scoreSTD = scores.std() * 2\n            extraTreeFrame.loc[len(extraTreeFrame)] = [n, d, s, scoreAverage, scoreSTD]\n\nextraTreeFrame.sort_values('Average', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b8a39d3e7f0e3569de807e47b3ba152c0575519","_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(extraTreeFrame[['Average','n_estimators', 'Depth', 'Samples']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2b3b7a6d71876a00e9821c280550d0b8f486c19f"},"cell_type":"code","source":"sns.relplot(x = 'n_estimators', y = 'Average', data=extraTreeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1ca50769faab1fbbd1baa3b75c41c659be0cbd2f"},"cell_type":"code","source":"sns.relplot(x = 'Depth', y = 'Average', data=extraTreeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff157c5e8f8b0ad9476fb2b66a1a439db556389e"},"cell_type":"markdown","source":"Looking at the results and at the heatmap, we see that samples isn't really affecting our average score. The other plots show that the optimal number of trees is somewhere between 10 and 40. Knowing this we can try the other parameters a bit more in depth. The best depths are above 10 according to the plot."},{"metadata":{"trusted":true,"_uuid":"465cc42080f081ff6d28e2e4cf90be7b0421d3bb"},"cell_type":"code","source":"extraTreeFrame = pd.DataFrame(columns = ['n_estimators', 'Depth', 'Average', 'Standard Deviation'])\n\nfor n in range(10, 50):\n    for d in range(10, 30):\n        alg = ensemble.ExtraTreesClassifier(n_estimators=n, max_depth=d, random_state=0)\n        scores = cross_val_score(alg, X, y, cv = 5)\n        scoreAverage = scores.mean()\n        scoreSTD = scores.std() * 2\n        extraTreeFrame.loc[len(extraTreeFrame)] = [n, d, scoreAverage, scoreSTD]\n\nextraTreeFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abae5f1097e02e5ebbf84107a0cdd24b7825455c","_kg_hide-input":true},"cell_type":"code","source":"sns.relplot(x = 'n_estimators', y = 'Average', data=extraTreeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce739773ee71c8180050d33bcdf8cb668ca3de33","_kg_hide-input":true},"cell_type":"code","source":"sns.relplot(x = 'Depth', y = 'Average', data=extraTreeFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f89fda35f79143c3f67fc7c1a5c898e3abaa86b"},"cell_type":"code","source":"optimalExtraTreeN = int(extraTreeFrame.sort_values('Average', ascending=False).iloc[0].values[0])\noptimalExtraTreeDepth = int(extraTreeFrame.sort_values('Average', ascending=False).iloc[0].values[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20bbe4b0b1c3bcb22d25120dbdd97c77230449e1"},"cell_type":"markdown","source":"The plots show that the optimal n_estimators are mostly around 35 and the optimal maximum depth is mostly above 17."},{"metadata":{"_uuid":"8bc0e5f4c5d399ddc356de7fbe60f9bcc2e00391"},"cell_type":"markdown","source":"# 9. Random Forest Classifier\n### How does it work?\nThe Random Forest Classifier fits a number of randomized Decision Trees on sub-samples of the dataset and then uses averaging to get a more accurate prediction. This sounds really similar to the Extra Trees Classifier model, because it is. The big difference is that the Random Forest Classifier chooses the features it will use by the most discriminative thresholds instead of randomly. \n\n### What parameters can we tune?\nThe Random Forest parameters that we'll tune are: n_estimators, max_depth and min_samples_leaf.\n\nWe have already discussed all of these in previous models, max_depth & min_samples_leaf in Decision Tree and n_estimators in Extra Trees.\n\n### Default parameters\n* n_estimators = 10\n* max_depth = None\n* min_samples_leaf = 1\n\nLet's start by trying N: 1 -> 100, Depth: 1 -> 30 and Samples: 1 -> 5"},{"metadata":{"trusted":true,"_uuid":"ff7575ae5f5d6b87a549fcf65a75609874321c70"},"cell_type":"code","source":"randomForestFrame = pd.DataFrame(columns = ['n_estimators', 'Depth', 'Samples', 'Average', 'Standard Deviation'])\n\nfor n in range(1, 100, 10):\n    for d in range(1, 30, 5):\n        for s in range(2, 5):\n            alg = ensemble.RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s, random_state=0)\n            scores = cross_val_score(alg, X, y, cv = 5)\n            scoreAverage = scores.mean()\n            scoreSTD = scores.std() * 2\n            randomForestFrame.loc[len(randomForestFrame)] = [n, d, s, scoreAverage, scoreSTD]\n\nrandomForest = randomForestFrame.sort_values('Average', ascending=False)\nrandomForest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5308d2d72c93ddaa4a136284624e1775f7fd65b5"},"cell_type":"code","source":"sns.relplot(x = 'n_estimators', y = 'Average', data = randomForestFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0fe860d3b46eb7a7dbc67741ed837787ecb8c65a"},"cell_type":"code","source":"sns.relplot(x = 'Depth', y = 'Average', data=randomForestFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cc78c5685c94b78349a72651817458b7ade48cf"},"cell_type":"code","source":"sns.relplot(x = 'Samples', y = 'Average', data=randomForestFrame)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"165d7f71d8815e3f973fec0054fd0b579d5b0562"},"cell_type":"code","source":"sns.heatmap(randomForestFrame[['Average','n_estimators', 'Depth', 'Samples']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4099dcb9fdf97a689e7af324f09a2c17c95b335a"},"cell_type":"markdown","source":"Looking at the plots we can clearly see that n_estimators should be somewhere between 40 and 80,  max_depth should be above 5 and samples can vary wildly."},{"metadata":{"trusted":true,"_uuid":"8b648714d2a1a8b62bb16778677cbd7c93b48eb1"},"cell_type":"code","source":"randomForestFrame = pd.DataFrame(columns = ['n_estimators', 'Depth', 'Samples', 'Average', 'Standard Deviation'])\n\nfor n in range(40,80,2):\n    for d in range(5,25,5):\n        for s in range(2, 5):\n            alg = ensemble.RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s, random_state=0)\n            scores = cross_val_score(alg, X, y, cv = 5)\n            scoreAverage = scores.mean()\n            scoreSTD = scores.std() * 2\n            randomForestFrame.loc[len(randomForestFrame)] = [n, d, s, scoreAverage, scoreSTD]\n\nrandomForestFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ca06c9bf613be449ede4006e38a17e950ee4d9"},"cell_type":"code","source":"optimalRandomForestN = int(randomForestFrame.sort_values('Average', ascending=False).iloc[0].values[0])\noptimalRandomForestDepth = int(randomForestFrame.sort_values('Average', ascending=False).iloc[0].values[1])\noptimalRandomForestSamples = int(randomForestFrame.sort_values('Average', ascending=False).iloc[0].values[2])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5b6e65e9677fffb563175e89844c430ef7ee90f1"},"cell_type":"code","source":"sns.relplot(x = 'n_estimators', y = 'Average', data=randomForestFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a86c92393318da6604bd806b5e4dbdc32ec0a195"},"cell_type":"markdown","source":"# \t10. Gradient Boosting Classifier\n### How does it work?\nThe Gradient Boosting Classifier builds multiple Decision Trees (just like the Random Forest Classifier), but does this in different stages while trying to optimize the loss function.\n\n### What parameters can we tune?\nThe Gradient Boosting Classifier that we'll tune are: learning_rate, n_estimators, max_depth.\n\nWe have already discussed max_depth in Decision Tree and n_estimators in Extra Trees.\n\n### Default parameters\n* learning_rate = 0.1\n* n_estimators = 10\n* max_depth = None\n\n## learning_rate - How much does a single Decision Tree contribute?\nThe learning_rate parameter shrinks the contribution of each tree by learning_rate.\n\nLet's start by trying N: 1 -> 100, Depth: 1 -> 30 and learning_rate: 0.1 -> 1"},{"metadata":{"trusted":true,"_uuid":"2b1f072f89424d6bd8dd6905cd84ddcb42c7354f"},"cell_type":"code","source":"gradientBoostingTotalFrame = pd.DataFrame(columns = ['n_estimators', 'Depth', 'learning rate', 'Average', 'Standard Deviation'])\n\nfor n in range(1, 100, 10):\n    for d in range(1, 30, 5):\n        for l in range(1, 11):\n            l = l/10\n            alg = ensemble.GradientBoostingClassifier(n_estimators = n, learning_rate = l, max_depth = d, random_state = 0)\n            scores = cross_val_score(alg, X, y, cv = 5)\n            scoreAverage = scores.mean()\n            scoreSTD = scores.std() * 2\n            gradientBoostingTotalFrame.loc[len(gradientBoostingTotalFrame)] = [n, d, l, scoreAverage, scoreSTD]\n\ngradientBoostingTotalFrame.sort_values('Average', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c15beb875aeafec85b100d797e49d97cdcc4d18"},"cell_type":"code","source":"sns.heatmap(gradientBoostingTotalFrame[['Average','n_estimators', 'Depth', 'learning rate']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac119a226d2a018e73833d63a937d97760b4032e"},"cell_type":"code","source":"sns.relplot(x = 'n_estimators', y = 'Average', data=gradientBoostingTotalFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53efed89385f557b0c9daae310d9cfb700130a79"},"cell_type":"code","source":"sns.relplot(x = 'Depth', y = 'Average', data=gradientBoostingTotalFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"147b7f2174d16bb3132216835d31268650322f5c"},"cell_type":"code","source":"sns.relplot(x = 'learning rate', y = 'Average', data=gradientBoostingTotalFrame, kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35af80cb649b7184fd3db52527d8e152e6c5e208"},"cell_type":"markdown","source":"Looking at these results and plots we can see that n_estimators should be somewhere above 60, a max_depth of 1 is optimal and the learning_rate should be somewhere between 0.5 and 1"},{"metadata":{"trusted":true,"_uuid":"f668eef976fb59469f8c78d12b65dd8de9bf7ce8"},"cell_type":"code","source":"gradientBoostingFrame = pd.DataFrame(columns = ['n_estimators', 'learning rate', 'Average', 'Standard Deviation'])\nfor n in range(60, 150):\n    for l in range(1, 11):\n        l = l / 10\n        alg = ensemble.GradientBoostingClassifier(n_estimators = n, learning_rate = l, random_state = 0)\n        scores = cross_val_score(alg, X, y, cv = 5)\n        scoreAverage = scores.mean()\n        scoreSTD = scores.std() * 2\n        gradientBoostingFrame.loc[len(gradientBoostingFrame)] = [n, l, scoreAverage, scoreSTD]\n\ngradientBoostingFrame.sort_values('Average', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa53f48895226f444805304c3e0509919f3c2de"},"cell_type":"code","source":"optimalGBCN = int(gradientBoostingFrame.sort_values('Average', ascending=False).iloc[0].values[0])\noptimalGBCLearningRate = gradientBoostingFrame.sort_values('Average', ascending=False).iloc[0].values[1]\n\noptimalGBCDepth = int(gradientBoostingTotalFrame.sort_values('Average', ascending=False).iloc[0].values[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"857c0cad44c6d6658fbcbe9e71990ec54ba7c549"},"cell_type":"markdown","source":"# 11. Ada Boost Classifier \n### How does it work?\nThe Ada Boost Classifier builds the same model multiple times, but the data gets assigned different weights every time.\n\n### What parameters can we tune?\nWe'll use the following parameters: base_estimator, n_estimators, learning_rate\n\nWe have already discussed n_estimators in Extra Trees and learning_rate in Gradient Boosting.\n\n### Default parameters\n* base_estimator = DecisionTreeClassifier(max_depth=1)\n* n_estimators = 50\n* learning_rate = 1\n\n## base_estimator - What kind of model do I use?\nThe base estimator is the model on which the boost is built."},{"metadata":{"trusted":true,"_uuid":"715dcc259f5c377c52d976733ab532e7fe61c420"},"cell_type":"code","source":"adaBoostFrame = pd.DataFrame(columns = ['Name', 'n_estimators', 'Learning Rate', 'Average', 'Standard Deviation'])\nfor b in [    \n    tree.DecisionTreeClassifier(min_samples_leaf = optimalTreeSamples, max_depth= optimalTreeDepth, random_state=0),\n    ensemble.ExtraTreesClassifier(n_estimators=optimalExtraTreeN, max_depth=optimalExtraTreeDepth, random_state=0),\n    ensemble.RandomForestClassifier(n_estimators=optimalRandomForestN, max_depth=optimalRandomForestDepth, min_samples_split=optimalRandomForestSamples, random_state=0),\n    ensemble.GradientBoostingClassifier(n_estimators = optimalGBCN, learning_rate = optimalGBCLearningRate, max_depth = 1, random_state=0),\n]:\n    for n in range(1,100, 5):\n        for l in range(1,11):\n            l = l/10\n            alg = ensemble.AdaBoostClassifier(n_estimators = n, base_estimator=b, learning_rate = l, random_state = 0)\n            scores = cross_val_score(alg, X, y, cv = 5)\n            scoreAverage = scores.mean()\n            scoreSTD = scores.std() * 2\n            adaBoostFrame.loc[len(adaBoostFrame)] = [b.__class__.__name__, n, l, scoreAverage, scoreSTD]\n\nadaBoostFrame.sort_values('Average', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82d1d0201129bc49357d2d6e41f003f113694f58"},"cell_type":"code","source":"optimalAdaBoostN = int(adaBoostFrame.sort_values('Average', ascending=False).iloc[0].values[1])\noptimalAdaBoostLearningRate = adaBoostFrame.sort_values('Average', ascending=False).iloc[0].values[2]\noptimalAdaBoostBase = tree.DecisionTreeClassifier(min_samples_leaf = optimalTreeSamples, max_depth= optimalTreeDepth, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ecff904f0a556ef0b6e8ab509f18c69b39b722"},"cell_type":"markdown","source":"# 12. Final Score"},{"metadata":{"trusted":true,"_uuid":"de7e0cec4bcee518fdc80fbc7b9fc28ac499c591"},"cell_type":"code","source":"finalScoreFrame = pd.DataFrame(columns = ['Algorithm Name', 'Average', 'Standard Deviation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5125bbb81dc81739cc7de5cfde33f7b62b27124e"},"cell_type":"code","source":"alg = svm.SVC(gamma=optimalSVMGamma, kernel=optimalSVMKernel, degree=optimalSVMDegree, C = optimalSVMPenalty)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final SVM Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d2d480a5a9fd356799ec80109563bca0423b287"},"cell_type":"code","source":"alg = linear_model.Ridge(alpha = optimalRidgeAlpha)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final Ridge Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12d7333a5419fa056323dda54c6486914613a2a"},"cell_type":"code","source":"alg = tree.DecisionTreeClassifier(min_samples_leaf = optimalTreeSamples, max_depth= optimalTreeDepth, random_state=0)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final DecisionTree Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45e7817c95af994749e6afd578bf1cd97a08938c"},"cell_type":"code","source":"alg = neighbors.KNeighborsClassifier(n_neighbors = optimalKNNNeighbors, weights = optimalKNNWeight, algorithm = optimalKNNAlg)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final KNC Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04378e58545cc982a27fff7669887ecd558a4e60"},"cell_type":"code","source":"alg = ensemble.ExtraTreesClassifier(n_estimators=optimalExtraTreeN, max_depth=optimalExtraTreeDepth, random_state=0)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final Extra Trees Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12f38e88ca51d2bbbd84e717fb563fb6e723374a"},"cell_type":"code","source":"alg = ensemble.RandomForestClassifier(n_estimators=optimalRandomForestN, max_depth=optimalRandomForestDepth, min_samples_split=optimalRandomForestSamples, random_state=0)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final Random Forest Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1963b14a75660b976931d6c57d7f92881af88f58"},"cell_type":"code","source":"alg = ensemble.GradientBoostingClassifier(n_estimators = optimalGBCN, learning_rate = optimalGBCLearningRate, max_depth = optimalGBCDepth, random_state = 0)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final Gradient Boosting Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6de0a91b4c0f07151d1634f183ada41026ff441"},"cell_type":"code","source":"alg = ensemble.AdaBoostClassifier(n_estimators = optimalAdaBoostN, base_estimator=optimalAdaBoostBase, learning_rate=optimalAdaBoostLearningRate, random_state = 0)\nscores = cross_val_score(alg, X, y, cv = 5)\nscoreAverage = scores.mean()\nprint('Final AdaBoost Score: {:01.5f}'.format(scoreAverage))\nscoreSTD = scores.std() * 2\nfinalScoreFrame.loc[len(finalScoreFrame)] = [alg.__class__.__name__, scoreAverage, scoreSTD]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14e35182bc19708e0d359e726a83a1030a13b211"},"cell_type":"code","source":"compareScoreFrame = pd.DataFrame(columns = ['Algorithm Name', 'Average', 'Standard Deviation', 'Before/After'])\n\nfor i in range(len(scoreFrame)):\n    row = scoreFrame.loc[i]\n    compareScoreFrame.loc[len(compareScoreFrame)] = [row['Algorithm Name'], row['Average'], row['Standard Deviation'], 'Before']\n    \nfor i in range(len(finalScoreFrame)):\n    row = finalScoreFrame.loc[i]\n    compareScoreFrame.loc[len(compareScoreFrame)] = [row['Algorithm Name'], row['Average'], row['Standard Deviation'], 'After']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c44c25b11d154e7f53e8ed65a5e249e9b11118"},"cell_type":"code","source":"compareScoreFrame.sort_values('Average', ascending=False).head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef325ced41bb658fa51109a085bdecb1646d7d66"},"cell_type":"markdown","source":"So now we can see the results of our hard work. All of the models that we've tuned have higher outcomes than the default models. The most surprising one is the SVM who went up by about 0.3 points of accuracy. The AdaBoostClassifier is the best model however with almost 98% accuracy."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"44aaeba4d563efdafb7e8621d191768ed1ec08eb"},"cell_type":"code","source":"compareScoreFrame.sort_values('Average', ascending=False)\ng = sns.relplot(x = \"Algorithm Name\", y = \"Average\", hue=\"Before/After\", size = 'Standard Deviation', data = compareScoreFrame, sizes = (100,500), height=7) \ng.fig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8016cf32bc3e097ad6d42be6b6fad59e5f4b74"},"cell_type":"markdown","source":"# 13. Conclusion\nBy tuning your models, you can squeeze a bit more performance out of them. However this does require a lot more work and time than just using the default parameters. So if you have the time, I definitely  recommend optimizing your models."},{"metadata":{"_uuid":"0f01860059f631b7035521dc634500611fc76fd5"},"cell_type":"markdown","source":"# 14. Sources\n* *Introduction To Machine Learning by A. C. MÃ¼ller and S. Guido*\n* Scikit-learn: https://scikit-learn.org/stable/index.html\n* SVM Tuning: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769\n* GBC Tuning: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}