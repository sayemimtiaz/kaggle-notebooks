{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Read the data\ndata = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the categories in categorical variables\n\nprint(data['Geography'].unique())\nprint(data['Gender'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find blanks in data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a dict file to convert string variable into numerical one\n# for Gender column\ngender = {'Male':0, 'Female':1}\ndata.Gender = [gender[item] for item in data.Gender]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a dict file to convert string variable into numerical one\n#For contries\ngeo = {'France':1, 'Spain':2, 'Germany':3}\ndata.Geography = [geo[item] for item in data.Geography]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete the unnecessary features from dataset\ndata.pop('CustomerId')\ndata.pop('Surname')\ndata.pop('RowNumber')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations between customer data features and customer churn ###"},{"metadata":{},"cell_type":"markdown","source":"To decide which features of the data to include in our predictive churn model, we’ll examine the correlation between churn and each customer feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})\nheat_map=plt.gcf()\nheat_map.set_size_inches(20,15)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is no high correlation between features. therefore, there is no multicollineality problem"},{"metadata":{},"cell_type":"markdown","source":"### Predictive modelling ###\n\n\nWe will consider several different models to predict customer churn. To ensure we are not over-fitting to our data, we will split the 10,000 customer records into a training and test set, with the test set being 25% of the total records."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ntrain, test = train_test_split(data, test_size = 0.25)\n \ntrain_y = train['Exited']\ntest_y = test['Exited']\n \ntrain_x = train\ntrain_x.pop('Exited')\ntest_x = test\ntest_x.pop('Exited')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression ####\n\nLogistic regression is one of the more basic classification algorithms in a data scientist’s toolkit. It is used to predict a category or group based on an observation. Logistic regression is usually used for binary classification (1 or 0, win or lose, true or false). The output of logistic regression is a probability, which will always be a value between 0 and 1. While the output value does not give a classification directly, we can choose a cutoff value so that inputs with with probability greater than the cutoff belong to one class, and those with less than the cutoff belong to the other.\n\nFor example, if the classifier predicts a probability of customer attrition being 70%, and our cutoff value is 50%, then we predict that the customer will churn. Similarly, if the model outputs a 30% chance of attrition for a customer, then we predict that the customer won’t churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n \nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(X=train_x, y=train_y)\n \ntest_y_pred = logisticRegr.predict(test_x)\nconfusion_matrix = confusion_matrix(test_y, test_y_pred)\nprint('Intercept: ' + str(logisticRegr.intercept_))\nprint('Regression: ' + str(logisticRegr.coef_))\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logisticRegr.score(test_x, test_y)))\nprint(classification_report(test_y, test_y_pred))\n \nconfusion_matrix_df = pd.DataFrame(confusion_matrix, ('No churn', 'Churn'), ('No churn', 'Churn'))\nheatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 78% classification accuracy from our logistic regression classifier. But the precision and recall for predictions in the positive class (churn) are relatively low, which suggests our data set may be imbalanced."},{"metadata":{},"cell_type":"markdown","source":"### handling imbalanced classes\n\nIt is also important to look at the distribution of how many customers churn. If 95% of customers don’t churn, we can achieve 95% accuracy by building a model that simply predicts that all customers won’t churn. But this isn’t a very useful model, because it will never tell us when a customer will churn, which is what we are really interested in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking how many customers exited\ndata['Exited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Up-sampling the minority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n \ndata_majority = data[data['Exited']==0]\ndata_minority = data[data['Exited']==1]\n \ndata_minority_upsampled = resample(data_minority,\nreplace=True,\nn_samples=7963, #same number of samples as majority classe\nrandom_state=1) #set the seed for random resampling\n# Combine resampled results\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled])\n \ndata_upsampled['Exited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a 1:1 ratio for our classes, let’s train another logistic regression model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data_upsampled, test_size = 0.25)\n \ntrain_y_upsampled = train['Exited']\ntest_y_upsampled = test['Exited']\n \ntrain_x_upsampled = train\ntrain_x_upsampled.pop('Exited')\ntest_x_upsampled = test\ntest_x_upsampled.pop('Exited')\n \nlogisticRegr_balanced = LogisticRegression()\nlogisticRegr_balanced.fit(X=train_x_upsampled, y=train_y_upsampled)\n \ntest_y_pred_balanced = logisticRegr_balanced.predict(test_x_upsampled)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logisticRegr_balanced.score(test_x_upsampled, test_y_upsampled)))\nprint(classification_report(test_y_upsampled, test_y_pred_balanced))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall accuracy of the model has decreased, but the precision and recall scores for predicting a churn have improved."},{"metadata":{},"cell_type":"markdown","source":"### Using a different performance metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n \n# Get class probabilities for both models\ntest_y_prob = logisticRegr.predict_proba(test_x)\ntest_y_prob_balanced = logisticRegr_balanced.predict_proba(test_x_upsampled)\n \n# We only need the probabilities for the positive class\ntest_y_prob = [p[1] for p in test_y_prob]\ntest_y_prob_balanced = [p[1] for p in test_y_prob_balanced]\n \nprint('Unbalanced model AUROC: ' + str(roc_auc_score(test_y, test_y_prob)))\nprint('Balanced model AUROC: ' + str(roc_auc_score(test_y_upsampled, test_y_prob_balanced)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the AUROC scores are very similar between the two models. Both are above 0.5 however, suggesting that both models have the ability to distiguish between observations from each class."},{"metadata":{},"cell_type":"markdown","source":"# Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn import tree\n \n# Create each decision tree (pruned and unpruned)\ndecisionTree_unpruned = tree.DecisionTreeClassifier()\ndecisionTree = tree.DecisionTreeClassifier(max_depth = 4)\n \n# Fit each tree to our training data\ndecisionTree_unpruned = decisionTree_unpruned.fit(X=train_x, y=train_y)\ndecisionTree = decisionTree.fit(X=train_x, y=train_y)\n \ntest_y_pred_dt = decisionTree.predict(test_x)\ntest_y_pred_dt = decisionTree_unpruned.predict(test_x)\ntest_y_pred_dt = decisionTree.predict(train_x)\ntest_y_pred_dt = decisionTree_unpruned.predict(train_x)\nprint('Accuracy of unpruned decision tree classifier on train set: {:.2f}'.format(decisionTree_unpruned.score(train_x, train_y)))\nprint('Accuracy of unpruned decision tree classifier on test set: {:.2f}'.format(decisionTree_unpruned.score(test_x, test_y)))\nprint('Accuracy of decision tree classifier on train set: {:.2f}'.format(decisionTree.score(train_x, train_y)))\nprint('Accuracy of decision tree classifier on test set: {:.2f}'.format(decisionTree.score(test_x, test_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exactly as we suspected! The unpruned tree gets a perfect score on the training set, but a relatively lower score (81%) on the test set. Our pruned tree is less accurate on the training set, but performs better when presented with the out-of-sample test data."},{"metadata":{},"cell_type":"markdown","source":"# Cross validation (k_Fold)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, test_x, test_y, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that even KNN is doing better for this classification problem"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Decision tree is outperforming other models considered in this practice. Therefore, Decison tree model could be a better choice."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}