{"cells":[{"metadata":{},"cell_type":"markdown","source":"Notes:\n\n> This model is not that good, needs improvement.\n\n> Without class *Industrial*, model fared much better than current state.\n\n> Will add a few more CNN layers and decrease the Dropout rates.\n\n> Version 1-2: Trained using user-defined model\n\n> Version 4: Testing using ResNet101\n\n> Version 5: Training with ResNet-50\n\n> Version 7-8: Training using the EuroSat-ResNet50 Model imported from TensorFlow Hub\n\nIssues:\n\n> Misclassification with ResNet101 is high with original layers trainability set as false.\n\n> Need to load ImageNet weights"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_hub as hub\nimport os, cv2, json, random, itertools\n\nfrom tqdm import tqdm\nfrom IPython.display import SVG\nfrom tensorflow.keras.utils import plot_model, model_to_dot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom sklearn.metrics import f1_score\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Add, Input, Conv2D, Dropout, Activation, BatchNormalization, MaxPooling2D, ZeroPadding2D, AveragePooling2D, Flatten, Dense)\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, Callback, LearningRateScheduler\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras import backend as K\n\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom tensorflow.keras.applications.resnet import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_final_history(history):\n    \n    plt.style.use(\"ggplot\")\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n    \n    ax[0].set_title('Loss')\n    ax[1].set_title('Accuracy')\n    \n    ax[0].plot(history.history['loss'], \"r-\", label='Training Loss')\n    ax[0].plot(history.history['val_loss'], \"g-\", label='Validation Loss')\n    ax[1].plot(history.history['accuracy'], \"r-\", label='Training Accuracy')\n    ax[1].plot(history.history['val_accuracy'], \"g-\", label='Validation Accuracy')\n    \n    ax[0].legend(loc='upper right')\n    ax[1].legend(loc='lower right')\n    \n    plt.show();\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n    \n    cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar();\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f'\n    thresh = cm.max()/2.0\n    \n    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        \n        plt.text(j,i, format(cm[i,j], fmt),\n                horizontalalignment = \"center\",\n                color = \"white\" if cm[i,j] > thresh else \"black\")\n        pass\n    \n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.grid(False);\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = \"../input/eurosat-dataset/EuroSAT\"\n\ndef data_generator(csv_file, num_classes, batch_size = 10, target_size=64):\n    \n    df = pd.read_csv(csv_file)\n    df.drop(columns=df.columns[0], inplace=True)\n    num_samples = df.shape[0]\n    \n    while True:\n        \n        for offset in range(0, num_samples, batch_size):\n            batch_samples_idx = df.index[offset:offset+batch_size]\n            \n            X = []\n            y = []\n            \n            for i in batch_samples_idx:\n                img_name = df.loc[i,'Filename']\n                img = load_img(os.path.join(base_path,img_name),target_size=(224,224))\n                img = img_to_array(img)\n                \n                img = preprocess_input(img)\n                label = df.loc[i,'Label']\n                \n                X.append(img)\n                y.append(label)\n                pass\n            \n            X = np.array(X)\n            y = np.array(y)\n            y = to_categorical(y, num_classes=num_classes)\n            \n            yield X, y\n            pass\n        pass\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = data_generator(csv_file=\"../input/eurosat-dataset/EuroSAT/train.csv\", num_classes=10, batch_size=10)\nval_generator = data_generator(csv_file=\"../input/eurosat-dataset/EuroSAT/validation.csv\", num_classes=10, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/eurosat-dataset/EuroSAT/label_map.json\",\"r\") as f:\n    class_names_encoded = json.load(f)\n    pass\n\nclass_names = list(class_names_encoded.keys())\nnum_classes = len(class_names)\nclass_names_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Input((224,224,3)),\n    hub.KerasLayer(\"https://tfhub.dev/google/remote_sensing/eurosat-resnet50/1\"),\n    Dropout(0.2),\n    Dense(len(class_names), activation=\"softmax\")\n])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"eurosat_resnet50_model_weights.h5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='max')\nlogs = TensorBoard(\"eurosat_resnet-logs\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/eurosat-dataset/EuroSAT/train.csv\")\ntrain_labels = train_df.loc[:,'Label']\ntrain_labels = np.array(train_labels)\n\nnum_train_samples = train_labels.shape[0]\n\nval_df = pd.read_csv(\"../input/eurosat-dataset/EuroSAT/validation.csv\")\nval_labels = val_df.loc[:,'Label']\nval_labels = np.array(val_labels)\n\nnum_val_samples = val_labels.shape[0]\n\nnum_train_samples, num_val_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train_labels, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_encoded = to_categorical(train_labels,num_classes=10)\n\nclassTotals = train_labels_encoded.sum(axis=0)\nclassWeight = {}\n\nfor i in range(len(classTotals)):\n    classWeight[i] = classTotals[i]/classTotals.max()\n    pass\n\nclassWeight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(Callback):\n    \n    def __init__(self, base_lr=0.0001,max_lr=0.001,step_size=2000.,mode=\"triangular\",\n              gamma=1.,scale_fn=None, scale_mode='cycle'):\n        \n        super(CyclicLR, self).__init__()\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x:1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        \n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history_lr = {}\n        \n        self._reset()\n        pass\n    \n    \n    def _reset(self, new_base_lr=None,new_max_lr=None,new_step_size=None):\n        \n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        \n        self.clr_iterations = 0.\n        pass\n    \n    \n    def clr(self):\n        \n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        pass\n    \n    def on_train_begin(self, logs={}):\n        \n        logs = logs or {}\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n        pass\n    \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        \n        self.history_lr.setdefault('lr',[]).append(K.get_value(self.model.optimizer.lr))\n        self.history_lr.setdefault('iterations', []).append(self.trn_iterations)\n        \n        for (k,v) in logs.items():\n            self.history_lr.setdefault(k, []).append(v)\n            pass\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class StepDecay(LearningRateScheduler):\n#     def __init__(self, initAlpha=0.001, factor=0.1, dropEvery=30):\n#         self.initAlpha = initAlpha\n#         self.factor = factor\n#         self.dropEvery = dropEvery\n#         pass\n    \n#     def __cal__(self, epoch):\n#         exp = np.floor((1+epoch)/self.dropEvery)\n#         alpha = self.initAlpha * (self.factor ** exp)\n        \n#         return float(alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clr = CyclicLR(base_lr=0.0001,max_lr=0.001,step_size=2000.,mode='triangular')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr=1e-5)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nbatchSize = 10\n\nhistory = model.fit(train_generator,\n                   steps_per_epoch= num_train_samples//batchSize,\n                   epochs = epochs,\n                   verbose = 1,\n                   validation_data = val_generator,\n                   validation_steps = num_val_samples//batchSize,\n                   callbacks = [checkpoint, logs],\n                   class_weight=classWeight\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_final_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obtain_images(csv_file):\n    \n    df = pd.read_csv(csv_file)\n    df.drop(columns=df.columns[0], inplace=True)\n    num_samples = df.shape[0]\n        \n    X = []\n    y = []\n\n    for i in tqdm(range(num_samples)):\n        img_name = df.loc[i,'Filename']\n        img = cv2.imread(os.path.join(base_path,img_name))\n        img = cv2.resize(img,(64,64))\n        label = df.loc[i,'Label']\n        \n        X.append(img)\n        y.append(label)\n        pass\n    \n    X = np.array(X)\n    y = np.array(y)\n    \n    return X,y\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images, test_labels = obtain_images(csv_file=\"../input/eurosat-dataset/EuroSAT/test.csv\")\n\ntest_images.shape, test_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded = to_categorical(test_labels,num_classes = 10)\ntest_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(test_images)\ntest_pred = np.argmax(test_pred, axis=1)\ntest_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_evaluate = model.evaluate(test_images, test_encoded)\n\nprint(\"Loss: {}, Accuracy: {}\".format(val_evaluate[0], val_evaluate[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_mat = confusion_matrix(test_labels, test_pred)\n\nplt.figure()\nplot_confusion_matrix(cnf_mat, classes=class_names)\nplt.grid(False);\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f1,class_name in zip(f1_score(test_labels, test_pred, average=None), class_names):\n    print(\"Class name: {}, F1 score: {:.3f}\".format(class_name, f1))\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"eurosat_resnet_transfer_model_v5.h5\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}