{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first split the data into input(X) and oyput(Y) variables.\nx = data.drop(\"Outcome\", axis=1)\ny = data[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Models is keras are defined as Sequence of layers**\n\nSo, we difine a sequential model and add layers one at a time untill we are happy with our network architecture.\n\nThe first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument and setting it to 8 for the 8 input variables.\n\nIn this notebook, we will implement a fully-connnected network structure with 3 layers.\n\n - Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument.\n- We will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n\n**We can piece it all together by adding each layer:**\n- The model expects rows of data with 8 variables(input_dim = 8)\n- First hidden layer has the 12 neurons and activation function as Relu\n- Second hidden layer has the 8 neurons and activation function as Relu.\n- The output layer has one node and uses the sigmoid activation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"#define keras model and add layers with activatin func as discussed.\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step-3) Compile the Keras Model\nCompiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU or GPU or even distributed.\n\nwhen compiling we must specify some additional properties while training the network. Training the network means finding the best set of weights to map inputs to outputs in our dataset.\n\n**Parameters:-** we must specify the loss function to use to evaluate the set of weights, optimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n\nIn this case, we will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as `binary_crossentropy`. You can learn more about choosing loss functions based on your problem here:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#compile the keras model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step-4) Fit the Model\nNow, we have compiled our model successfully, and ready to fit the data.\n\nwe can train model with the loaded data by the help of fit()\n\nTraining occurs over epochs and each epoch is split into batches.\n\n* <b>Epoch:</b> One pass through all of the rows in the training dataset.\n* <b>Batch:</b> One or more samples considered by the model within an epoch before weights are updated.\n\none epoch is comprised of moe or more batches, based on chosen batch_size and model fit for many epochs.\n\nFor this problem, we will run for a small number of epochs (150) and use a relatively small batch size of 10.\n\nThese configurations can be chosen experimentally by trial and error. We want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the keras model on dataset\nmodel.fit(x,y, epochs=150, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step-5) Evaluate the Model\nNow, that we have trained the model and we can evaluate to know the performance that how well our model performed. But has no idea on how model performs on new data. \n\nWe have done this for simplicity, when you are solving any rel-world problem statement, you will split data into train and test set. \n\nWe can evaluate out model on train set using the evaluate() func and pass it the same input and output to train the model.\n\nThis will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n\nThe evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We are only interested in reporting the accuracy, so we will ignore the loss value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate the keras model\n_, accuracy = model.evaluate(x,y)\nprint(\"Model accuracy: %.2f\"% (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step-7) Making Predictions\nmaking a prediction is a simple task using a predict(). we are using a sigmoid activation function in output layer so, prediction will be probability between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a prediction on training dataset, as consider we have not seen it before\npredictions = model.predict(x)\n#round the prediction\nrounded = [round(x[0]) for x in predictions]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Alternately, we can call the predict_classes() function on the model to predict crisp classes directly, for example:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_classes(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thank You!. Please upvote it, it gives huge motivation and appciate to create new notebooks and ace a journey forward towards the data scientist.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}