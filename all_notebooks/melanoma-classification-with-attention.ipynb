{"cells":[{"metadata":{},"cell_type":"markdown","source":"After the paper [Attention is all you need!](https://arxiv.org/abs/1706.03762), the field of NLP has never been the same. While Attention is crucial for most NLP problems, it is still not widely used to solve Computer Vision problems.\n\n\n## Why Attention?\n\n> * When training an image model, we want the model to be able to focus on important parts of the image. One way of accomplishing this is through **trainable attention** mechanisms. \n> * In our case we are dealing with lesion images and it becomes all the more necessary to be able to **interpret** the model.It is important to understand which part of the image contributes more towards the cancer  being classified benign/malignant.\n> * Post-hoc analysis like **Grad-CAM** are not the same as attention. They are not intended to change the way the model learns, or to change what the model learns. They are applied to an already-trained model with fixed weights, and are intended solely to provide insight into the modelâ€™s decisions.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import datasets, models,transforms # add models to the list\n# from torchvision.utils import make_grid\nimport torchvision.utils as utils\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import auc,roc_auc_score\ndevice = torch.device(\"cpu\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport random\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    # random.seed(seed)\n    # os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_dir='../input/melanoma-merged-external-data-512x512-jpeg/512x512-dataset-melanoma/512x512-dataset-melanoma/'\ntest_dir='/kaggle/input/siim-isic-melanoma-classification/jpeg/test/'\ntrain=pd.read_csv('../input/melanoma-merged-external-data-512x512-jpeg/marking.csv')\ntest=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\nsubmission=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare the Data\nWe'll prepare the data as usual, only the modelling part is different. I don't focus much on the data part here, that can be found in this [kernel](https://www.kaggle.com/ibtesama/siim-baseline-keras)\n\nCertain things that I do are:\n* Train on only a sample of data.\n* Resize,Normalize,CenterCrop train and test images.\n* Data Augmentation(Random Rotaion/ Horizontal Flip) on train images.\n\nThanks to [Alex Shonenkov](https://www.kaggle.com/shonenkov) I'm using his external data without duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# balance the data a bit\ndf_0=train[train['target']==0].sample(6000,random_state=42)\ndf_1=train[train['target']==1]\ntrain=pd.concat([df_0,df_1])\ntrain=train.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare train data\nlabels=[]\ndata=[]\nfor i in range(train.shape[0]):\n    data.append(train_dir + train['image_id'].iloc[i]+'.jpg')\n    labels.append(train['target'].iloc[i])\ndf=pd.DataFrame(data)\ndf.columns=['images']\ndf['target']=labels\n\n#Prepare test data\n\ntest_data=[]\nfor i in range(test.shape[0]):\n    test_data.append(test_dir + test['image_name'].iloc[i]+'.jpg')\ndf_test=pd.DataFrame(test_data)\ndf_test.columns=['images']\n\n# Split train into train and val\nX_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = transforms.Compose([\n        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n        transforms.Resize(224),             # resize shortest side to 224 pixels\n        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageDataset(Dataset):\n    \n    def __init__(self,data_paths,labels,transform=None,mode='train'):\n         self.data=data_paths\n         self.labels=labels\n         self.transform=transform\n         self.mode=mode\n    def __len__(self):\n       return len(self.data)\n    \n    def __getitem__(self,idx):\n        img_name = self.data[idx]\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img=Image.fromarray(img)\n        if self.transform is not None:\n          img = self.transform(img)\n        img=img.cuda()\n        \n        if self.mode=='test':\n            return img,img_name\n        else:\n           \n            labels = torch.tensor(self.labels[idx]).cuda()\n\n            return img, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset=ImageDataset(data_paths=X_train.values,labels=y_train.values,transform=train_transform)\nval_dataset=ImageDataset(data_paths=X_val.values,labels=y_val.values,transform=test_transform)\ntest_dataset=ImageDataset(data_paths=df_test['images'].values,labels=None,transform=test_transform,mode='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=100,shuffle=True)\nval_loader=DataLoader(val_dataset,batch_size=50,shuffle=False)\ntest_loader=DataLoader(test_dataset,batch_size=50,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model : VGG16 with Attention \n\n![](https://i.imgur.com/MIT3Wd8.png)\n\n* The architecture implemented here is proposed in [this](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf) paper. VGG-16 is the backbone network without any dense layers.\n* Two attention modules are applied (the gray blocks). The output of intermediate feature maps(pool-3 and pool-4) are used to infer attention maps. Output of pool-5 serves as a form of global-guidance because the last stage feature contains the most abstracted and compressed information over the entire image.\n* The three feature vectors (green blocks) are computed via global average pooling and are concatenated together to form the final feature vector, which serves as the input to the classification layer(not shown here).\n\nIf this is not very clear to you, don't worry I'm going to break it down in the next step.\n\n### Attention Layer\nBelow is a class defining the attention layer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentionBlock(nn.Module):\n    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n        super(AttentionBlock, self).__init__()\n        self.up_factor = up_factor\n        self.normalize_attn = normalize_attn\n        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n    def forward(self, l, g):\n        N, C, W, H = l.size()\n        l_ = self.W_l(l)\n        g_ = self.W_g(g)\n        if self.up_factor > 1:\n            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n        \n        # compute attn map\n        if self.normalize_attn:\n            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n        else:\n            a = torch.sigmoid(c)\n        # re-weight the local feature\n        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n        if self.normalize_attn:\n            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n        else:\n            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n        return a, output\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What goes on inside an Attention Layer can be explained by this figure.\n\n![](https://i.imgur.com/npJMDjq.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The intermediate feature vector(F) is the output of pool-3 or pool-4 and the global feature vector (output of pool-5) are fed as input to the attention layer.\n* Both the feature vectors pass through a convolution layer. When the spatial size of global and intermediate features are different, feature upsampling is done via bilinear interpolation. The *up_factor* determines by what factor is the convoluted global feature vector has to be upscaled.\n* After that an element wise sum is done followed by a convolution operation that just reduces the 256 channels to 1.\n* This is then fed into a Softmax layer, which gives us a normalized Attention map (A).Each scalar element in A represents the degree of attention to the corresponding spatial feature vector in F.\n* The new feature vector $\\hat{F}$ is then computed by *pixel-wise* multiplication. That is, each feature vector $f_{i}$ is multiplied by the attention element $a_{i}$.\n* So, the attention map A and the new feature vector $\\hat{F}$ are the outputs of the Attention Layer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttnVGG(nn.Module):\n    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n        super(AttnVGG, self).__init__()\n        net = models.vgg16_bn(pretrained=True)\n        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n        self.pool = nn.AvgPool2d(7, stride=1)\n        self.dpt = None\n        if dropout is not None:\n            self.dpt = nn.Dropout(dropout)\n        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n        \n       # initialize the attention blocks defined above\n        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n        \n       \n        self.reset_parameters(self.cls)\n        self.reset_parameters(self.attn1)\n        self.reset_parameters(self.attn2)\n    def reset_parameters(self, module):\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.)\n                nn.init.constant_(m.bias, 0.)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0., 0.01)\n                nn.init.constant_(m.bias, 0.)\n    def forward(self, x):\n        block1 = self.conv_block1(x)       # /1\n        pool1 = F.max_pool2d(block1, 2, 2) # /2\n        block2 = self.conv_block2(pool1)   # /2\n        pool2 = F.max_pool2d(block2, 2, 2) # /4\n        block3 = self.conv_block3(pool2)   # /4\n        pool3 = F.max_pool2d(block3, 2, 2) # /8\n        block4 = self.conv_block4(pool3)   # /8\n        pool4 = F.max_pool2d(block4, 2, 2) # /16\n        block5 = self.conv_block5(pool4)   # /16\n        pool5 = F.max_pool2d(block5, 2, 2) # /32\n        N, __, __, __ = pool5.size()\n        \n        g = self.pool(pool5).view(N,512)\n        a1, g1 = self.attn1(pool3, pool5)\n        a2, g2 = self.attn2(pool4, pool5)\n        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n        if self.dpt is not None:\n            g_hat = self.dpt(g_hat)\n        out = self.cls(g_hat)\n\n        return [out, a1, a2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The architecture of VGG16 is kept mostly the same except the Dense layers are removed.\n* We pass pool-3 and pool-4 through the attention layer to get $\\hat{F}_{3}$ and $\\hat{F}_{4}$ .\n* $\\hat{F}_{3}$ , $\\hat{F}_{4}$  and G(pool-5) are concatenated and fed into the final classification layer.\n* The whole network is trained end-to-end.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AttnVGG(num_classes=1, normalize_attn=True)\nmodel=model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm trying Binary Cross Entropy here with Label Smoothing. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try **Cosine Annealing**  this time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = FocalLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = CosineAnnealingLR(optimizer=optimizer,T_max=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()\n\nepochs = 5\n\ntrain_losses = []\ntest_losses = []\ntrain_auc=[]\nval_auc=[]\n\nfor i in range(epochs):\n\n    train_preds=[]\n    train_targets=[]\n    auc_train=[]\n    loss_epoch_train=[]\n    loss_epoch_test=[]\n    # Run the training batches\n    for b, (X_train, y_train) in tqdm(enumerate(train_loader),total=len(train_loader)):\n        \n        b+=1\n        y_pred,_,_=model(X_train)\n        loss = criterion(torch.sigmoid(y_pred.type(torch.FloatTensor)), y_train.type(torch.FloatTensor))   \n        loss_epoch_train.append(loss.item())\n        # For plotting purpose\n        if (i==(epochs-1)):\n            if (b==19):\n                I_train = utils.make_grid(X_train[0:8,:,:,:], nrow=8, normalize=True, scale_each=True)\n                __, a1, a2 = model(X_train[0:8,:,:,:])\n                \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n                 \n    try:\n        auc_train=roc_auc_score(y_train.detach().to(device).numpy(),torch.sigmoid(y_pred).detach().to(device).numpy())\n    except:\n        auc_train=0\n    train_losses.append(np.mean(loss_epoch_train))\n    train_auc.append(auc_train)\n    print(f'epoch: {i:2}   loss: {np.mean(loss_epoch_train):10.8f} AUC  : {auc_train:10.8f} ')\n    # Run the testing batches\n    \n    with torch.no_grad():\n        for b, (X_test, y_test) in enumerate(val_loader):\n            \n            y_val,_,_ = model(X_test)\n            loss = criterion(torch.sigmoid(y_val.type(torch.FloatTensor)), y_test.type(torch.FloatTensor))\n            loss_epoch_test.append(loss.item())\n    try:\n                                           \n        auc_val=roc_auc_score(y_test.detach().to(device).numpy(),torch.sigmoid(y_val).detach().to(device).numpy())\n    except:\n        auc_val=0\n    test_losses.append(np.mean(loss_epoch_test))\n    val_auc.append(auc_val)\n    print(scheduler.get_lr())\n    scheduler.step()\n    print(f'Epoch: {i} Val Loss: {np.mean(loss_epoch_test):10.8f} AUC: {auc_val:10.8f} ')\n    \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Attention\nNow let's visualize the attention maps created by pool-3 and pool-4 to understand which part of the image are responsible for the classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_attention(I_train,a,up_factor,no_attention=False):\n    img = I_train.permute((1,2,0)).cpu().numpy()\n    # compute the heatmap\n    if up_factor > 1:\n        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n    attn = utils.make_grid(a, nrow=8, normalize=True, scale_each=True)\n    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n    attn = np.float32(attn) / 255\n    # add the heatmap to the image\n    img=cv2.resize(img,(466,60))\n    if no_attention:\n        return torch.from_numpy(img)\n    else:\n        vis = 0.6 * img + 0.4 * attn\n        return torch.from_numpy(vis)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig=visualize_attention(I_train,a1,up_factor=2,no_attention=True)\nfirst=visualize_attention(I_train,a1,up_factor=2,no_attention=False)\nsecond=visualize_attention(I_train,a2,up_factor=4,no_attention=False)\n\nfig, (ax1, ax2,ax3) = plt.subplots(3, 1,figsize=(10, 10))\nax1.imshow(orig)\nax2.imshow(first)\nax3.imshow(second)\nax1.title.set_text('Input Images')\nax2.title.set_text('pool-3 attention')\nax3.title.set_text('pool-4 attention')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * **The way this works for malignant images** :- The shallower layer (pool-3) tends to focus on more general and diffused areas, while the deeper layer (pool-4) is more concentrated, focusing on the lesion and avoiding irrelevant objects.\n> * But since most images in our case are benign, pool-3 tries to learn some areas but pool-4 eventually minimizes the activated regions because the image is benign.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Submission \nNow let's make a submission through our trained model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval();\n\nresult = {'image_name': [], 'target': []}\nfor images, image_names in tqdm(test_loader, total=len(test_loader)):\n    preds=[]\n    with torch.no_grad():\n        outputs,_,_ = model(images)\n        a=torch.sigmoid(outputs).cpu().numpy()\n        for i in a:\n            preds.append(i[0])\n\n    result['image_name'].extend(image_names)\n    result['target'].extend(preds)\n\nsubmission = pd.DataFrame(result)\nsubmission['image_name']=[x[58:70] for x in submission['image_name']]\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## End Notes\n\n* This kernel is a basic demonstration of how to use Attention mechanism with pretrained image models.\n* The paper also claims that due to the elimination of Dense Layers, number of parameters are greatly reduced and the network is lighter to train.\n* Tuning the hyperparameters, changing the backbone architecture,increasing training data might yield better results in this competition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### References\n* [https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf)\n* https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1\n* https://github.com/SaoYan/IPMI2019-AttnMel/tree/99e4a9b71717fb51f24d7994948b6a0e76bb8d58\n\n***If you liked the kernel, don't forget to upvote it!***","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}