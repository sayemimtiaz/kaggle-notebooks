{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n* Here is my statistic studies you can find.\n* Subjects have been explained inside of them with examples.\n\n \n <br> <font color='blue'>\n    Content: \n1. [Mean, Median, Mode](#1)\n1. [Range, Variance, Standart Deviation](#2)\n1. [Correlation, Coveriance](#3)\n1. [Pearson's Correlation](#4)\n1. [Spearman's Correlation](#5)\n1. [Probability Distribution](#6)\n    * [Uniform Distribution](#7)\n    * [Binomial Distribution](#8)\n    * [Poisson Distribution](#9)\n    * [Gaussian(Normal) Distribution](#10)\n    * [Z-Score](#11)\n1. [Statistic](#12)\n    * [Sampling](#13)\n    * [Central Limit Theorem](#14)\n    * [Standart Error](#15)\n    * [Hypotesis Testing](#16)\n    * [Type1 and Type2 Error](#17)\n1. [Student's T Test](#18)\n1. [ANOVA](#19)\n1. [Chi Square Analysis](#20)\n1. [Regression](#21)\n    * [Linear Regression](#22)\n    * [Multiple Linear Regression](#23)\n    * [Polynomial Linear Regression](#24)\n    \nNote : DATAI Team informations have been used. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats # this is for mode method for statistics\nimport seaborn as sns # visualization library\nimport matplotlib.pyplot as plt # visualization library\nfrom plotly.offline import init_notebook_mode, iplot # plotly offline mode\ninit_notebook_mode(connected=True) \nimport plotly.graph_objs as go # plotly graphical object\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n# import warnings library\nimport warnings        \n# ignore filters\nwarnings.filterwarnings(\"ignore\") # if there is a warning after some codes, this will avoid us to see them.\nplt.style.use('ggplot') # style of plots. ggplot is one of the most used style, I also like it.\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a>\n## Mean, Median, Mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"#in here we find mean, median and mode values\n#mean value finds the mean value of list, median find the middle value of list, mode value find most used value in list\nage = [1,2,3,5,6,7,7,10,12,13]\n\nmean_age = np.mean(age)\nmedian_age = np.median(age)\nmode_age = stats.mode(age)\n\nprint(mean_age)\nprint(median_age)\nprint(mode_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a>\n## Range, Variance, Standart Deviation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets calculate range, variance and standart deviation(std)\n#we need these because mean, median gives us some information but doesnt give us dispersion information. These 3 expressions give us informaitons about dispersion.\n#range is the value betweeen max(value of list)-min(value of list), std and variance gives us all list values' distance from mean value\nage = [1,2,3,5,6,7,7,10,12,13]\nrange_age = np.max(age) - np.min(age)\nprint(range_age)\n\nvariance_age = np.var(age)\nprint(variance_age)\n\n#this is variance with formula\nvariance_age = sum((age - np.mean(age))**2)/len(age)\nprint(variance_age)\n\nstd_age = np.std(age)\nprint(std_age)\n\n#this is std with formula\nstd_age = np.sqrt(sum((age - np.mean(age))**2)/len(age))\nprint(std_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example and visualize our datas and in graphic we are seeing that mean and median values are similar but they arent the same data, so that we need std,range and variance values\ny = np.random.uniform(5,8,100)\nx1 = np.random.uniform(10,20,100)\nx2 = np.random.uniform(0,30,100)\n\nplt.scatter(x1, y, color =\"black\")\nplt.scatter(x2, y, color =\"orange\")\nplt.xlim([-1,31])\nplt.ylim([2,11])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\nprint(\"x1 mean: {} and meadian: {}\".format(np.mean(x1),np.median(x1)))\nprint(\"x2 mean: {} and meadian: {}\".format(np.mean(x2),np.median(x2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a>\n## Correlation, Coveriance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will look for bivariate and coveriance\n#bivariate compare 2 values and gives if there is a correlation between them\n#correlation gives if there is a connection between variables\n\nf,ax = plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(), annot = True, linewidths = 0.5, fmt = \".1f\", ax=ax)\nplt.xticks(rotation = 90)\nplt.yticks(rotation = 0)\nplt.title(\"Correlation Map\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in here, we have correlation map\nif the values are 1 means 2 variables have straight connection\nif -1 means reverse connection\nif 0 mean no connection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we see the connection between variables\nsns.jointplot(data.radius_mean, data.area_mean, kind = \"reg\")\nsns.jointplot(data.radius_mean, data.fractal_dimension_mean, kind = \"reg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.cov(data.radius_mean, data.area_mean)\nprint(\"Covariance between radius mean and area mean: \",data.radius_mean.cov(data.area_mean))\nprint(\"Covariance between radius mean and fractal dimension se: \",data.radius_mean.cov(data.fractal_dimension_se))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after finding covariance value, we will normalize this value and find pearson(regression) value which shows us the connection between values(correlation)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a>\n## Pearson's Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will normalize our values and find pearson coeffient\n#finding pearson coeffient means dividing coveriance value to standart deviation and this is how we will normalize our value between -1 to +1\np1 = data.loc[:, [\"area_mean\",\"radius_mean\"]].corr(method =\"pearson\")\np2 = data.radius_mean.cov(data.area_mean)/(data.radius_mean.std()*data.area_mean.std())\nprint(\"Pearson correlation: \")\nprint(p1)\nprint(\"Perason correlation: \",p2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see our variables correlation value as 0.98 means our values have a straight connection between"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data.radius_mean, data.area_mean, kind = \"reg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we see there is a straight connection"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a>\n## Spearman's Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#the other coeffient is Spearman rank coefficient\n#first lets understand whats rank\ndata1 = {'name': ['ali', 'veli', 'hakan', 'ayse', 'fatma'],\n        'year' : [2012,2012,2013,2014,2014],\n        'coverage' : [25,94,57,62,70]}\ndf = pd.DataFrame(data1, index = ['Ankara','İstanbul','Sinop','Bolu','İzmir'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['coverageRanked'] = df['coverage'].rank(ascending=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see rank method calculated the coverage values and sorted it"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rank() rank the data\nranked_data = data.rank()\nspearman_corr = ranked_data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method=\"pearson\")\nprint(\"Spearman's correlation: \")\nprint(spearman_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we see correlaiton values is 0.99 now. Pearson correlation can't deal with outliers but Spearman correlation can, thats why its better"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a>\n## PROBABILITY DISTRIBUTION\n* Probablity means for a condition in an event, how much possible we can have our condition. For example, for a dice, we have 1,2,3,4,5,6 and when we role the dice, the condition that we see 1 is 1/6.\n* Probability distribution having the condition more than one time like 1000, 1 million times and see the distribution of our probablities.\n* There are dscrete and continious probability distribution.\n\n\n1. Discrete Probability Distribution\n    * Uniform Distribution\n    * Binomial Distribution\n    * Poisson Distribution\n1. Continous Probability Distribution\n    * Gaussian(Normal) Distribution\n    * Z-Score"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a>\n### Uniform Distribution\n* Uniform distribution means the probablity of the each condition of our event is equal. For example when we roll the dice, probablity of 1,2,3,4,5,6 is equal and 1/6.\n* Also uniform distribution can be found in discrete time or sample. For example we can roll the dice 1 time or 2 times or 1000 times, not like 1.78 times. That makes it certain and discrete."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example and see our graph\n#rnp.random.randint(1,7,60000) means between 1-7(7 not including) generate 60000 values\na = np.random.randint(1,7,60000)  \nprint(\"sample space: \",np.unique(a))\nplt.hist(a, bins = 12)\nplt.ylabel(\"Number of outcomes\")\nplt.xlabel(\"Possible outcomes\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we see that our samples possiblity is equal and uniform"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a>\n### Binomial Distribution\n* Binomial distribution is also a discrete probablity distribution. This helps us the see probablity distribution in 2 sampled event like success-failure or heads-tails.\n* formula = Combination(n,r) * p^r * (1-p)^(n-r)   \n* n = number of trial(test)\n* p = probablity of success\n* r = number of success"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example to see heads and tails binomal distribution\nn = 2 #number of trial, we will flip coin twice\np = 0.5 #probablity of each trial\ns = np.random.binomial(n, p, 10000)  #we will test 10000 times\nweights = np.ones_like(s)/float(len(s))  #we normalize our values\nplt.hist(s, weights=weights)\nplt.xlabel(\"number of success\")\nplt.ylabel(\"probability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we see possibility of heads and tails come twice is 0.25, possibility of one time heads and one time tails come is 0.50"},{"metadata":{"trusted":true},"cell_type":"code","source":"#10 times rolling the dice and whats the probablity of 6 coming 4 times successful\nn = 10\nr = 4\np = 1/6\n\n#from library solution\nfrom scipy.stats import binom\nprint(binom.pmf(r,n,p))\n\n#from formula solution\nimport math\nprint((math.factorial(n)/(math.factorial(n-r)*math.factorial(r)))*(p**r)*(1-p)**(n-r))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we found the probablity with the methods"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a>\n### Poisson Distribution\n* Poisson distribution is also a discrete probablity distribution. This helps us the see probablity distribution in specific time or distance interval like possiblity of trucks coming to base in 1 hour. \n* formula = (lambda^x * exp(-lambda)) / x!\n* lambda = number of occurances/interval(means success in interval)\n* interval = x = time or distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example: possibility of 10 trucks coming to base in 1 hour\nlamda = 10  #our success value\ns1 = np.random.poisson(lamda, 100000)  #100000 times test\nweights1 = np.ones_like(s1)/float(len(s1))\nplt.hist(s1, weights=weights1, bins = 100)\nplt.xlabel(\"number of occurances\")\nplt.ylabel(\"probability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we see that 10 trucks success probability is like 0.12 and also we see that 22 trucks have success probability too even though its very little"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"10\"></a>\n### Gaussian(Normal) Distribution\n* We can name this as PDF(Probability density function) too because it shows the values density in graphic. Gaussian distribution is a continious probability distribution. \n* Gaussian distribution gives examples from real life like IQ level, weight-height distribution of all people in the world.\n* It is symmetrical and it has a mean value and standart deviation shows the distance from mean value to left and right side in the distribution graphic.\n* As example IQ level mean value of world is 110 and there are people have 140-150 and also 50 IQ but this is rare."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example: IQ level of people in the world\n#mean value of IQ level=110, std=20 lets say\nmu, sigma = 110,20 #mean and std\ns = np.random.normal(mu, sigma, 100000)  #100000 test\nprint(\"mean: \", np.mean(s))\nprint(\"std: \", np.std(s))\n\nplt.figure(figsize = (10,7))\nplt.hist(s, 100)\nplt.xlabel(\"IQ\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of IQ\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we see that 110 value is mean value of histogram and +/-20 std = 90-130. Between 90-130 shows us %68 values are inside of this region. That's why its important for future calculations."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"11\"></a>\n### Z-Score\n* Z-Score is actually concept in Gaussian distribution. Normally we have std and mean value and we say that +/-std from mean value has %68 of graphic. From previous example we said 20 std and 110 mean and that means from world %68 people have that 90-130 IQ. But what if we want to find % value of 80-140. Then we need Z-Score in here. \n* Z score formula : z = (x-mean)/std   \n* in our example x=80 and 140 so that     z1 = (80-110)/20=-1.5,    z2 = (140-110)/20=1.5   this means that z1 and z2 is (1.5*std) away from mean value from left and right side.\n* in this point we look for Z table(you can find it by searching in internet) and then 1.5 value is equal to 0.4332 in z table and and this means from left and right side 80 and 140 is %43.32 away from mean value. Then we have to multiply it with 2 for find all region. 0.4332*2=0.8664   so that this region is %86.64\n* this is what z-score helps us for."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"12\"></a>\n## Statistics"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"13\"></a>\n### Sampling\nSampling is chosing one or couple sub groups from a population.\n* Random sampling : Chosing a sub group from population randomly without looking colors or any features.\n* Stratified sampling : First choosing 2 or more groups from population with respect to a feature like color or anything, then analyzing groups indivially. In here groups are more homogenous.\n* Cluster sampling : Taking many sub groups from population. For example education success ration state by state in USA. In here groups are more heteregenous."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"14\"></a>\n### Central Limit Theorem\n* This means population mean value ≈ sub group of population mean value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see it in an example\n#x is our population\nx = np.random.random_integers(10, size = 100000)\nplt.hist(x)\nplt.show()\nprint(\"mean value of population: \", np.sum(x)/len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will take a sample from our population\nimport random\nfrom random import sample \nmean_sample = []\nfor i in range(100):\n    samplex = random.randrange(1,10)   #we take values 1-10\n    mean_sample.append(np.mean(random.sample(list(x), samplex)))     #in here we take between samplex=1-10 samples in list(x) and take mean value of it and append on mean_sampe\nplt.hist(mean_sample, bins=50, color = \"purple\")\nplt.show()\nprint(\"mean value of sample: \", np.sum(mean_sample)/len(mean_sample))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is how randrange works; if you try it, it will take a value randomly between 1-10\nfor i in range(10000):\n    samplex = random.randrange(1,10)\nsamplex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(x, alpha = 1, density = True)\nplt.hist(mean_sample, alpha = 0.8, bins = 50, color = \"purple\", density = True)\nplt.show()\nprint(\"mean value of population: \", np.sum(x)/len(x))\nprint(\"mean value of sample: \", np.sum(mean_sample)/len(mean_sample))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see that mean value of population and sample are almost the same. This proves central limit theorem.\n* almost this part is important: the more we take samples, the more value of population and sample gets closer."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"15\"></a>\n## Standart Error\n* Standart error : When we choose a sample from a population, standart error value shows how good our sample represents our population. If standart error is 0, this means our sample is perfectly represents our population.\n* formula : Standart Error(SE) = std / root(n)    n means sample number\n* When we find SE, we say that our sample is %68 possiblity in region of mean(+/-)SE and %95 possibility in region of mean(+/-)(2*SE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example to understand: our IQ population mean value:100, std:15. We take 10 people from population and mean value of sample:104.\n#lets see how good our sample represents our population.\nmean = 100\nsigma = 15 #std\nn = 10 #sample number\nSE = sigma / np.sqrt(n)\nprint(\"Our sample is %68 possibility in region: {} and {}, also our sample is %95 possibility in region: {} and {}\".format(mean+SE, mean-SE, mean+(2*SE), mean-(2*SE)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"16\"></a>\n## Hypotesis Testing\n* Null hypotesis : First hypotesis we say like world is plain, eggs radius is 3 cm. Null hypotesis is generally the value we dont want and we want to refute it.\n* Alternative hypotesis : If null hypotesis is wrong then we say alternative hypotesis.\n* Process Sequence:<br>\n1) First we make null and alternative hypotesis.<br>\n2) We use z value and find p value from z value in z value table.   Formula : z = (new value we want - old mean value)/(std / root(n))<br>\n3) if p value is smaller than level of significance(means reverse of confidence level if confidence level is %95, level of sig is 0.05) than we say null hyp. is rejected, if p value is greater than level of sig. than we say we fail to reject null hyp. means we accept it.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#example. We have a distance sensor that measures in 3ms and the error ratio is 0.6ms which means this sensor can measure between 2.4-3.6ms. So that we say\n#our mean value: 3ms, std=0.6ms.   Now we want to upgrade our sensor and we want this sensor measures in shorter time and we want %99 confidence level which means\n#we want our datas say to us yes sensor is measuring in %99 confidence level. Engineers calculate and then 50 times more make tests and say new mean value = 2.8ms\n#now we will make our null hyp. and alt. hyp., null hyp should be the value we dont want so that we refute it.\n#null hyp : mean value >= 3 (so that our sensor is not faster)\n#alt hyp: mean value < 3 (so that our sensor is faster)\n#also now we are looking that smaller values from old mean value means we are looking for left side(tail) test\n#now lets find z value\nold_mean_value = 3\nnew_mean_value = 2.8\nstd = 0.6\nn = 50  #number of test\nz = (new_mean_value-old_mean_value)/(std/np.sqrt(n))\nprint(\"z value: \", z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* now we found our z value and we need to look for z table for finding p value, you can find it in internet.\n* when we look for -2.30 in row and 0.05 in columns, the cell intersected between tells us 0.0094 means p value\n* p value = 0.0094 < 0.01(level of sig.) means our null hyp is rejected so that alternative hyp. is true. That means our engineers succeed in %99 possibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets do another example with pyhton methods. We will look for radius_mean and area_mean if there is a difference or not\nstatistic, p_value = stats.ttest_rel(data.radius_mean, data.area_mean)\nprint(\"p_value: \", p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see our p_value is almost 0 and for 0.01 or 0.05 sig. level p_value is smaller which means there is not a significance difference between these 2 datas."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"17\"></a>\n## Type1 and Type2 Error\n* Type1 error we say it for if our null hyp. is true but we reject it. For example null hyp: there is a wolf but we say there is no wolf but there is a wolf.\n* Type2 error we say it for if our null hyp. is false but we accept it. For example null hyp: there is no wolf but we say there is no wolf but there is a wolf."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"18\"></a>\n## Student's t test\n* Student's t test gives us in 2 groups if there is a difference or not. By doing this we look for difference between mean values and variance values means distribution between 2 groups.\n* We can look for 2 groups only and also these 2 groups' target must be similar. For example, if a person has cold and he doesnt use pills or a person has cold and use pills and look for his sickness process and finds if there is a difference between them. Another example, we can say that if there is 2 cancer groups and one group has fresh air and pill, the other group has fresh air and sugar and we observe their lifetime periods.\n* formula : t value = abs(mean value of 1st group - mean value of 2nd group) / root((variance of 1st group/number of 1st group)/(variance of 2nd group/number of 2nd group))"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example\ns1 = np.array([14.22, 8.30, 20.12, 15.3, 2.5, 30.1, 18.3, 22.1, 12.8, 13, 11.2, 10.3, 20.1, 11, 13.22, 17.5])\nprint(\"mean1: \", np.mean(s1))\nprint(\"std1: \", np.std(s1))\nprint(\"variance1: \", np.var(s1))\n\ns2 = np.array([15.22, 12.30, 3.12, 20.3, 8.5, 1.1, 17.3, 15.1, 12.8, 17, 20.2, 10.3, 12.1, 15, 14.22, 17.5])\nprint(\"mean1: \", np.mean(s2))\nprint(\"std1: \", np.std(s2))\nprint(\"variance1: \", np.var(s2))\n\nsns.kdeplot(s1)\nsns.kdeplot(s2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in here we can see our arrays mean and std values, now we will find t_value"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_val = np.abs(np.mean(s1)-np.mean(s2))/np.sqrt((np.var(s1)/len(s1))+(np.var(s2)/len(s2)))\nprint(\"t_value: \", t_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* now we will find our tcritical value from t table. You can find it from internet by searching t table.\n* for finding tcritical value, we need to find degree of freedom(DOF) formula : number of 1st group + number of 2nd group - 2 = 16+16-2 = 30\n* now we will look for t table and DOF is left rows : 30, p level of sig is right columns : 0.05 and the cell where intersected is 2.04 = tcritical\n* if t > tcrit means we reject null hyp. and there is a significant difference and if t < tcrit we accept null hyp. and there is no significance difference.\n* 0.86 < 2.04 means there is no significance difference so that our datas are similar we can say.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"19\"></a>\n## ANOVA(Analysis of Variance)\n* We looked for z and t distribution before and here we will look for F distribution. \n* We will analyzes 2 or more than 2 groups if they have possibility belong to the same population.\n* ANOVA test analyzes variance of groups between groups and within groups.\n    * Variance Between Groups : How far groups' mean values from total mean value\n    * Within Between Groups : How far groups' all values from group's mean value\n* F value is the ratio between these 2 groups. (Variance Between Groups/Within Between Groups)\n* ANOVA test can tell us if the groups are similar and belongs to the same population but if test is fails then it doesnt tell which groups are not the same. We have to look Stduent's t test for finding between 2 groups differences."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example\n#we have middle, high school and university's students worries data from student population, lets make one data for them and analyze if they are similar\nmiddle_school = np.array([40, 50.2, 52.3, 42.1, 43.2, 30.8, 48.3, 41.2, 36.8, 45.7])\nhigh_school = np.array([45, 52, 47, 38.2, 43.2, 58, 44.1, 43, 49.8, 52.2])\nuniversity = np.array([50.8, 50.2, 45, 42.8, 58.3, 55.3, 60.3, 63, 53, 45.7])\n\nprint(\"middle_school mean: {}, high_school mean: {}, university mean: {}\".format(np.mean(middle_school),np.mean(high_school),np.mean(university)))\nprint(\"total_mean: {}\".format((np.mean(middle_school)+np.mean(high_school)+np.mean(university))/3))\n      \nsns.kdeplot(middle_school)\nsns.kdeplot(high_school)\nsns.kdeplot(university)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we can see that university worry level is higher"},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will find F value\nstats.f_oneway(middle_school,high_school,university)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* statistic value = F value = 5.45\n* now we need find Fcritical(Fcrit) value. For finding it, we need to find degrees of freedom for groups and error values.\n* degrees of freedom for groups : number of groups - 1 = 3 - 1 = 2    we have 3 groups\n* degrees of freedom for error : (number of rows - 1) * number of groups = (10-1)*3 = 27    we have 10 rows in each groups\n* now we need to look for F distribution table right tailed(because F dist graphic has bias to right side). You can find it in internet.\n* df1 is degress of freedom for groups : 2, df2 is degrees of freedom for groups : 27. Intersection cell tells us Fcrit = 5.4881\n* if F < Fcrit, we fail to reject null hyp means we accept it, if F > Fcrit, we reject null hyp\n* F < Fcrit >> 5.45 < 5.48 means we fail to reject null hyp means we accept it and as result our groups are similar to each other."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"20\"></a>\n## CHI SQUARE ANALYSIS\n* chi square analysis gives us if there is a high or low correlation between expected and observed values. \n* chi square formula : x^2 = ∑((observed-expected)'2/observed)\n* degree of freedom : number of possible outcomes - 1 (for coin its 2-1=1, for dice its 6-1=5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make an example\n#we have a coin and 10 times we flip it and 9 times we see heads and 1 time tails\n#our null hyp = as a fair coin, 9 times heads coming is statistically logical.(this is what we say and look if its true)\nheads_expected = 5 #normally it has to come 5 times heads and 5 times tails\nheads_observed = 9\ntails_expected = 5\ntails_observed = 1\nchi_value = np.sum((((heads_observed-heads_expected)**2)/heads_expected)+(((tails_observed-tails_expected)**2)/tails_expected))\nprint(\"chi_value: \",chi_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we found chi_value, now we will find degree of freedom   DOF = 2-1 = 1    2 is number of possible outcomes\n* now we need to find critical value, we use chi square table for it, rows show DOF, columns shows p value(significance level) DOF:1, p=0.05 we choose and we see that intersection cell gives 3.841\n* finally if chi_value<critical that means observed and expeceted values are high correlated and null hyp. is true, if reverse low correlated and null hyp. is rejected.\n* chi_value:6.4 > critical:3.841 so that null hyp. is rejected and there is low correlation between observed and expected groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets make another example\n#we have 7 computers and they have the same ratio of collapsing. \n#by observartion : 1st PC 5 times, 2nd PC 7 times, 3rd PC 9 times, 4rd PC 4 times, 5th PC 1 times, 6th PC 10 times, 7th PC 6 times collapsed\n#our null hyp: observation values are stastically %95 possibility makes sense.\n#total collapse = 5+7+9+4+1+10+6 = 42,   expected collapse = 42 / 7 = 6 because collapsing ratio is the same for each computers.\n#DOF = 7-1 = 6\nobservation = np.array([5,7,9,4,1,10,6])\nprint(\"total: \", np.sum(observation))\n\nexpected = np.sum(observation)/len(observation)\nprint(\"expected: \", expected)\n\nchi_value = np.sum(((observation-expected)**2)/expected)\nprint(\"chi_value: \", chi_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* now we have our chi_value, we need to find critical value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#0.05 is significance level, 6 is DOF\nfrom scipy.stats import chi2\nprint(\"critical value: \", chi2.isf(0.05,6))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see chi_value:9.33 < critical:12.59 means we fail to reject null hyp, null hyp is true and there is a high correlation between observed and expected value."},{"metadata":{},"cell_type":"markdown","source":"## as SUMMARY: t test, chi square, ANOVA we always make a null hypotesis and try to find if its true or false"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"21\"></a>\n## REGRESSION\n1. Linear Regression\n1. Multiple Linear Regression\n1. Polynomial Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"22\"></a>\n### Linear Regression\n* Linear regression formula : y = b0 + b1*x\n* we plot our graphic and try to find a function with line which represents graphic.\n* all values and line has a distance and with Mean square error, we try to minimize this error. MSE formule: (1/n)*∑(yi-yi_head)  yi:our values yi_head: line value"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({\"experience\" : [0.5, 0, 1, 5, 8, 4, 15, 7, 3, 2, 12, 10, 14, 6], \"salary\" : [2500, 2250, 2750, 8000, 9000, 6900, 20000, 8500, 6000, 3500, 15000, 13000, 18000, 7500]})\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in here y = b0 + b1*x   >>    salary = b0 + b1 * experience"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data.experience, data.salary)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression  \n\nlinear_reg = LinearRegression()\n\nx = data.experience.values.reshape(-1,1) #we made np array(we could use df too) and without reshape it look like (14,) but for lineerreg we need to make it look like (14,1)\ny = data.salary.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b0 = linear_reg.predict([[0]])\nprint(\"b0: \",b0)\n\nb0_ = linear_reg.intercept_\nprint(\"b0_: \",b0)   #thats how we can find b0 in the same way\n\nb1 = linear_reg.coef_\nprint(\"b1: \",b1) \n\nsalary_11 = b0 + b1 * 11\nprint(\"salary_11: \",salary_11)\n\n#we find the salary of 11 years experienced worker by predict method\nmaas_11 = linear_reg.predict([[11]])\nprint(\"salary_11: \",salary_11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x, y)\n\narray = np.arange(1,15,1).reshape(-1,1)\n\ny_head = linear_reg.predict(array)\n\nplt.plot(array, y_head, color = \"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* here we visualized the our lineer regression graphic and see that the line fits well"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"23\"></a>\n### Multiple Linear Regression\n* Linear regression formula : y = b0 + b1*x + b2*x + ... + bn*x\n* The difference between linear and multiple linear regression, multiple linear regression has many x dependent values. We will see in our example.\n* we plot our graphic and try to find a function with line which represents graphic.\n* all values and line has a distance and with Mean square error, we try to minimize this error. MSE formule: (1/n)*∑(yi-yi_head)  yi:our values yi_head: line value"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"age\"] = [22, 21, 23, 25, 28, 23, 35, 29, 22, 23, 32, 30, 34, 27]\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in here y = b0 + b1*x + b2*x   >>    salary = b0 + b1 * experience + b2 * age"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:,[0,2]].values\ny = data.salary.values\n\nmultiple_linear_reg = LinearRegression()\nmultiple_linear_reg.fit(x,y)\n\nprint(\"b0: \", multiple_linear_reg.intercept_)\nprint(\"b1,b2: \", multiple_linear_reg.coef_)\n\nmultiple_linear_reg.predict(np.array([[10,35],[5,35]])) #we find salary for experience:10years, age:35 and experience:5years, age:35) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* here we can find our b0,b1,b2 values"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"24\"></a>\n### Polynomial Linear Regression\n* Polynomial linear regression formula : y = b0 + b1*x + b2*x^2 + ... + bn*x^n\n* When we can't find our graphic with linear regression, we look for polynomial linear regression.\n* we plot our graphic and try to find a function with line which represents graphic.\n* all values and line has a distance and with Mean square error, we try to minimize this error. MSE formule: (1/n)*∑(yi-yi_head)  yi:our values yi_head: line value"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({\"car_price\" : [60, 70, 80, 100, 120, 150, 200, 250, 300, 400, 500, 750, 1000, 2000, 300], \"car_max_speed\" : [180, 180, 200, 200, 200, 220, 240, 240, 300, 350, 350, 360, 365, 365, 365]})\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.car_max_speed.values.reshape(-1,1) #we made np array(we could use df too) and without reshape it look like (14,) but for lineerreg we need to make it look like (14,1)\nx = data.car_price.values.reshape(-1,1)\n\nplt.scatter(x, y)\nplt.xlabel(\"car_price\")\nplt.ylabel(\"car_max_speed\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_reg = LinearRegression()\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\n\nplt.scatter(x, y)\nplt.xlabel(\"car_price\")\nplt.ylabel(\"car_max_speed\")\nplt.plot(x, y_head, color = \"red\", label = \"linear\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see that linear regression isn't enough"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npolynomial_reg = PolynomialFeatures(degree = 2)\n\nx_poly = polynomial_reg.fit_transform(x)  #in here we x^2 values\n\nlinear_reg2 = LinearRegression()\nlinear_reg2.fit(x_poly, y)\n\ny_head2 = linear_reg2.predict(x_poly)\n\nplt.scatter(x, y)\nplt.xlabel(\"car_price\")\nplt.ylabel(\"car_max_speed\")\nplt.plot(x, y_head2, color = \"black\", label = \"poly2\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npolynomial_reg = PolynomialFeatures(degree = 4)\n\nx_poly = polynomial_reg.fit_transform(x)  #in here we x^4 values\n\nlinear_reg2 = LinearRegression()\nlinear_reg2.fit(x_poly, y)\n\ny_head2 = linear_reg2.predict(x_poly)\n\nplt.scatter(x, y)\nplt.xlabel(\"car_price\")\nplt.ylabel(\"car_max_speed\")\nplt.plot(x, y_head2, color = \"orange\", label = \"poly4\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* as we see polynomial degree = 4 shows better graphic\n* note : there is a problem with plotting as you see, i couldnt find the reason but it shows the difference of degree of polynomial."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}