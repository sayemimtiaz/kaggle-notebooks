{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport json\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import models,datasets,transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport time\nimport copy\n\n#print the pytorch version\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e79b6358d82fc4d48384d04ed1b3ee5199e3e7d"},"cell_type":"code","source":"#ImageNet Transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae98f49932f2b8d2a655b6747c94f0f8ff30d697"},"cell_type":"code","source":"image_transforms = { 'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), \n                    transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n                  \n                   'valid': transforms.Compose([transforms.Resize(256),transforms.CenterCrop(224),\n                    transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"141d6dc515710f24fd0ed388acffca2ef20fba5a"},"cell_type":"code","source":"fd_path = \"../input/flower_data/flower_data\"\ntrain_path = \"../input/flower_data/flower_data/train\"\nvalid_path = \"../input/flower_data/flower_data/valid\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057572dd2a46562da25318f154c36954bfb14082"},"cell_type":"code","source":"train_data = datasets.ImageFolder(train_path, image_transforms['train'])\nvalid_data = datasets.ImageFolder(valid_path, image_transforms['valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b74ee4c3500daf8563bcb7d89a17cc6aad739546"},"cell_type":"code","source":"train_data_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=0)\nvalid_data_loader = DataLoader(valid_data, batch_size=16, shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85cdd8a8d5b01027b7acde315dd3cc6c7dd56fc3"},"cell_type":"code","source":"train_data_loader.dataset.imgs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e232f194c34eb8eee6c21d2b70b9e82d74f5f295"},"cell_type":"code","source":"print('train data size:', len(train_data))\nprint('valid data size:', len(valid_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f84bb663b3045d2166fb533630ba74908196ac9"},"cell_type":"code","source":"class_names = train_data.classes\nwith open('../input/cat_to_name.json', 'r') as f:\n    cat_to_name = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37383b4afa874de152801aba347305b47a616e42"},"cell_type":"code","source":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(train_data_loader))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fac33e9efd76d767748bd7c695ea65d504eae17"},"cell_type":"code","source":"len(train_data), len(valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"511eafb370ae6fc5625185562fd9cfd8299b5f55"},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            print(phase)\n            if phase == 'train':\n                dataloader = train_data_loader\n                dataset_size = len(train_data)\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = valid_data_loader\n                dataset_size = len(valid_data)\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / dataset_size\n            epoch_acc = running_corrects.double() / dataset_size\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2991195f408828b672175efa0c51bca0805aba0"},"cell_type":"code","source":"model_00 = models.resnet152(pretrained=True)\nfor param in model_00.parameters():\n    param.requires_grad = False\n\n\nnum_ftrs = model_00.fc.in_features\nmodel_00.fc = nn.Linear(num_ftrs, 102)\n\nmodel_00 = model_00.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8216d9ce524cf682c34e5eaed33ceabe28b52c95"},"cell_type":"code","source":"#TO UNFREEZE THE WEIGHTS FOR THE SECOND TIME TRAINING AND DON'T FORGET TO CHANGE THE OPTIMIZER TOO!\nfor param in model_00.parameters():\n    param.requires_grad = True\n    \n# train the WHOLE part instead of just the classifier!, THIS IS THE SECOND ITERATION OF THE TRAINING AND REDUCE THE LEARNING RATE TOO!\noptimizer = optim.SGD(model_00.parameters(), lr=0.0002, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa2818f3783319d814dfe7d8fc706a4ee363dd82"},"cell_type":"code","source":"\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_00.fc.parameters(), lr=0.015, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"231ff79efd214a56e8335c8fc2c8f8ae9d981bf7"},"cell_type":"code","source":"model_00 = train_model(model_00, criterion, optimizer_ft, exp_lr_scheduler, num_epochs = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1b3823ec3d36ede2bf7f585de1fb97ceef2425e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}