{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep learning model's performance on tabular data compared to GBDT and TabNet model\n\nRecent years' development of deep learning models is very impressive and without any doubt, it is the state of the art in the area of computer vision (CV) and natural language processing (NLP). However, when it comes to structured data, a gradient boosted decision tree (GBDT) model still seems to be a strong opponent of deep learning models.\n\nThis notebook explores the model performance of GBDT (XGBoost), deep learning model (MLP) and TabNet (deep learning model for tabular data) on the home insurance dataset (structured data). Those models predict whether a home insurance will be lapsed.\n\n## Summary\n### Model performance\nAs mentioned above, here compares the model performance of XGBoost, MLP and TabNet with and without pre-train. The models are evaluated by ROC AUC score and F1 score. F1 scores were calculated at 0.27 as a threshold as I assumed that the distribution of the lapsed insurances is similar to the training distribution. Below is a summary of it.\n\n|                         \t| ROC AUC \t| F1 score \t| Time (sec) \t|\n|:-----------------------:\t|:-------:\t|:--------:\t|:----------:\t|\n|         XGBoost         \t|  0.7706 \t|  0.5591  \t|     500    \t|\n|           MLP           \t|  0.7514 \t|  0.5458  \t|     184    \t|\n| TabNet without pretrain \t|  0.7579 \t|  0.5529  \t|    1464    \t|\n|   TabNet with pretrain  \t|  0.7524 \t|  0.5484  \t|    2370    \t|\n\nAs we can see, in terms of the accuracy of the model, the XGBoost model is the best one, yet other models are also not far behind it. I have used with and without pretraining for the TabNet model (notebook of without pretraining TabNet can be found [here](https://www.kaggle.com/kyosukemorita/home-insurance-pretrained-tabnet). TabNet with pretraining supposed to be having a better result, but in this dataset, it got a slightly worse result than without pretraining. I am not sure what is exactly the reason but I guess this can be improved by appropriate hyperparameters.\n\nWhen we look into the distribution of the predictions of each model, we can observe that there are some degrees of similarity between the XGBoost and TabNet model. I guess it might be because TabNet is also using a tree-based-like algorithm. MLP model has a quite different shape compared to other models.\n\nIn terms of training time, the MLP model was the fastest one. I have used GPU, so that is the main reason why I got this result. Both TabNet models took quite a long time compared to other models. This makes a lot of differences when it comes to hyperparameter tuning. In this experiment, I didn't do any hyperparameter tuning and used arbitrary parameters. Although MLP's training time is almost 1/3 of the XGBoost model, the number of parameters it needs to optimise is easily more than 10 times of the XGBoost, so if I was doing hyperparameter tuning, it might take longer than the XGBoost model's training with hyperparameter tuning.\n\n### Explainability\nExplainability is quite important for some machine learning model business use cases. For example, it is critical to be able to explain why a model is making a particular decision in finance/banking. Imagine that we are deploying a model that can be used for loan approval and a customer wants to know why his application was rejected. Banks can't tell him that we don't know as there are strong regulators in the industry.\nExplainability of the model is one of the drawbacks of MLP models. Although we can still evaluate which features contributed to making predictions by using some ways such as using SHAP, it would be more useful if we can check the feature importance list quickly. In this notebook, I will compare only XGBoost and TabNet models' feature importance.\n\nThe top 5 important features of the XGBoost model are;\n\n- Marital status - Partner\n- Payment method - Non-Direct debit\n- Option \"Emergencies\" included after 1st renewal\n- Building coverage - Self-damage\n- Option \"Replacement of keys\" included before 1st renewal\n\nThe top 5 important features of the TabNet model without pretraining are;\n\n- Property type 21 (Detail not given)\n- \"HP1\" included before 1st renewal\n- Payment method - Pure Direct debit\n- Type of membership 6 (Detail not given)\n- Insurance cover length in years\n\nSurprisingly, those two models' important features are quite different. The important features from XGBoost are more \"understandable and expected\" to me - for example, if a customer has a partner, that person should be financially more responsible, thus, the home insurance will less likely to lapse. On the other hand, important features of TabNet are, I would say, less intuitive. The most important feature is \"property type 21\", where the detail of this feature is not given, so we don't know what is special about this property type. Also the second most important feature, \"HP1\" included before 1st renewal, where again we don't know what is \"HP1\". Perhaps, this can be an advantage of TabNet. As it is a deep learning model, it can explore a non-obvious relationship of the features and uses the optimal feature set, especially like this time, where not all the features' details are given.\n\n### Model selection for deployment in the real-life business\nWhen we want to use a machine learning model in real-life business, we need to select the best way to deploy the model and often there are some trade-offs. For example, it is a known fact that when we built a few models like this time and those models' accuracies are quite similar, ensemble them might increase the accuracy. If this ensemble strategy worked perfectly like improved the F1 score by 10%, then it is absolutely necessary to take this strategy, but if this improvement was only 1%, do we still want to take this strategy? Probably not, right? - as running one more model makes the computation more expensive, so usually if the benefit of deploying one more model surpasses the computation cost, we can take this ensemble strategy, otherwise, it is not optimal in terms of business.\n\nAlso, regarding the model explainability, whereas the XGBoost model used all 115 features, the TabNet model is only using 16 features (the pre-trained model used only 4 features). This is quite a huge difference and also important to understand those differences. As I mentioned above, in some real-life business use cases, it is critical to know how much contribution those features make. So sometimes although the accuracy is quite high, if the model couldn't explain why it's making that decision, it is difficult to convince people to use it in real life, especially in very sensitive business.\n\nConsidering the above 2 points, we would consider that the XGBoost model is superior to other deep learning models in this case. In terms of accuracy, the XGBoost model was slightly better than others (I haven't tried ensembling those predictions from all the models but let's assume, it didn't improve the accuracy much - I might be wrong). And in terms of explainability, as discussed above, the XGBoost model's feature importance list is somewhat we could understand (we can see some logic behind it) and somewhat expected.\n\n### Conclusion\nThis notebook experimentally compared the model performance of XGBoost, MLP and TabNet on tabular data. Here we are using the home insurance dataset to predict its lapse. As the result of this experiment, we have seen that the XGBoost model has slightly better than other deep learning models in terms of accuracy (F1 score and ROC AUC score), but as this experiment used GPU, the MLP model was the fastest to complete its training. Furthermore, we compared their explainability by seeing the feature importance list of the XGBoost model and TabNet model. The XGBoost model's feature importance list was somewhat more understandable and expected, on the other hand, the TabNet model's one was less intuitive. I think this is caused because of the structure of the algorithm - deep learning models, by nature, explores non-obvious relationships of the features and often it is difficult to understand by a human. From this simple experiment, we confirm that although improvement of deep learning models in recent years is impressive and definitely state-of-the-art, on tabular data, GBDT models are still as good as those deep learning models and sometimes even better than them, especially when we would like to deploy a machine learning model in the real-life business.","metadata":{}},{"cell_type":"code","source":"# TabNet\n!pip install pytorch-tabnet","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\nimport xgboost as xgb\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timer(myFunction):\n    def functionTimer(*args, **kwargs):\n        start_time = time.time()\n        result = myFunction(*args, **kwargs)\n        end_time = time.time()\n        computation_time = round(end_time - start_time, 2)\n        print(\"{} is excuted\".format(myFunction.__name__))\n        print('Computation took: {:.2f} seconds'.format(computation_time))\n        return result\n    return functionTimer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@timer\ndef prepareInputs(df: \"pd.dataFrame\") -> \"pd.dataFrame\":\n    \"\"\"Prepare the input for training\n\n    Args:\n        df (pd.DataFrame): raw data\n        \n    Process:\n        1. Exclude missing values\n        2. Clean the target variable\n        3. Create dummy variables for categorical variables\n        4. Create age features\n        5. Impute missing value\n    \n    Return: pd.dataFrame\n    \"\"\"\n    \n    # 1. Exclude missing values\n    df = df[df[\"POL_STATUS\"].notnull()]\n    \n    # 2. Clean the target variable\n    df = df[df[\"POL_STATUS\"] != \"Unknown\"]\n    df[\"lapse\"] = np.where(df[\"POL_STATUS\"] == \"Lapsed\", 1, 0)\n    \n    # 3. Create dummy variables for categorical variables\n    categorical_cols = [\"CLAIM3YEARS\", \"BUS_USE\", \"AD_BUILDINGS\",\n                        \"APPR_ALARM\", \"CONTENTS_COVER\", \"P1_SEX\",\n                        \"BUILDINGS_COVER\", \"P1_POLICY_REFUSED\", \n                        \"APPR_LOCKS\", \"FLOODING\",\n                        \"NEIGH_WATCH\", \"SAFE_INSTALLED\", \"SEC_DISC_REQ\",\n                        \"SUBSIDENCE\", \"LEGAL_ADDON_POST_REN\", \n                        \"HOME_EM_ADDON_PRE_REN\",\"HOME_EM_ADDON_POST_REN\", \n                        \"GARDEN_ADDON_PRE_REN\", \"GARDEN_ADDON_POST_REN\", \n                        \"KEYCARE_ADDON_PRE_REN\", \"KEYCARE_ADDON_POST_REN\", \n                        \"HP1_ADDON_PRE_REN\", \"HP1_ADDON_POST_REN\",\n                        \"HP2_ADDON_PRE_REN\", \"HP2_ADDON_POST_REN\", \n                        \"HP3_ADDON_PRE_REN\", \"HP3_ADDON_POST_REN\", \n                        \"MTA_FLAG\", \"OCC_STATUS\", \"OWNERSHIP_TYPE\",\n                        \"PROP_TYPE\", \"PAYMENT_METHOD\", \"P1_EMP_STATUS\",\n                        \"P1_MAR_STATUS\"\n                        ]\n    \n    for col in categorical_cols:\n        dummies = pd.get_dummies(df[col], \n                                 drop_first = True,\n                                 prefix = col\n                                )\n        df = pd.concat([df, dummies], 1)\n    \n    # 4. Create age features\n    df[\"age\"] = (datetime.strptime(\"2013-01-01\", \"%Y-%m-%d\") - pd.to_datetime(df[\"P1_DOB\"])).dt.days // 365\n    df[\"property_age\"] = 2013 - df[\"YEARBUILT\"]\n    df[\"cover_length\"] = 2013 - pd.to_datetime(df[\"COVER_START\"]).dt.year\n    \n    # 5. Impute missing value\n    df[\"RISK_RATED_AREA_B_imputed\"] = df[\"RISK_RATED_AREA_B\"].fillna(df[\"RISK_RATED_AREA_B\"].mean())\n    df[\"RISK_RATED_AREA_C_imputed\"] = df[\"RISK_RATED_AREA_C\"].fillna(df[\"RISK_RATED_AREA_C\"].mean())\n    df[\"MTA_FAP_imputed\"] = df[\"MTA_FAP\"].fillna(0)\n    df[\"MTA_APRP_imputed\"] = df[\"MTA_APRP\"].fillna(0)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train and test\n@timer\ndef splitData(df: \"pd.DataFrame\", FEATS: \"list\"):\n    \"\"\"Split the dataframe into train and test\n    \n    Args:\n        df: preprocessed dataframe\n        FEATS: feature list\n        \n    Returns:\n        X_train, y_train, X_test, y_test\n    \"\"\"\n    \n    train, test = train_test_split(df, test_size = .3, random_state = 42)\n    train, test = prepareInputs(train), prepareInputs(test)\n    \n    return train[FEATS], train[\"lapse\"], test[FEATS], test[\"lapse\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardise the data sets\n@timer\ndef standardiseNumericalFeats(X_train, X_test):\n    \"\"\"Standardise the numerical features\n    \n    Returns:\n        Standardised X_train and X_test\n    \"\"\"\n\n    numerical_cols = [\n        \"age\", \"property_age\", \"cover_length\", \"RISK_RATED_AREA_B_imputed\", \n        \"RISK_RATED_AREA_C_imputed\", \"MTA_FAP_imputed\", \"MTA_APRP_imputed\",\n        \"SUM_INSURED_BUILDINGS\", \"NCD_GRANTED_YEARS_B\", \"SUM_INSURED_CONTENTS\", \n        \"NCD_GRANTED_YEARS_C\", \"SPEC_SUM_INSURED\", \"SPEC_ITEM_PREM\", \n        \"UNSPEC_HRP_PREM\", \"BEDROOMS\", \"MAX_DAYS_UNOCC\", \"LAST_ANN_PREM_GROSS\"\n    ]\n\n    for col in numerical_cols:\n        scaler = StandardScaler()\n\n        X_train[col] = scaler.fit_transform(X_train[[col]])\n        X_test[col] = scaler.transform(X_test[[col]])\n        \n    return X_train, X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"@timer\ndef trainXgbModel(X_train, y_train, X_test, y_test, FEATS, ROUNDS) -> \"XGBoost model obj\":\n    \"\"\"Train XGBoost model\n    \n    Arg:\n        ROUNDS: Number of training rounds\n    \n    Return:\n        Model object\n    \"\"\"\n    \n    params = {\n                'eta': 0.02,\n                'max_depth': 10,\n                'min_child_weight': 7,\n                'subsample': 0.6,\n                'objective': 'binary:logistic',\n                'eval_metric': 'error',\n                'grow_policy': 'lossguide'\n            }\n    \n    dtrain, dtest = xgb.DMatrix(X_train, y_train, feature_names=FEATS), xgb.DMatrix(X_test, y_test, feature_names=FEATS)\n\n    EVAL_LIST = [(dtrain, \"train\"),(dtest, \"test\")]\n\n    xgb_model = xgb.train(params,dtrain,ROUNDS,EVAL_LIST)\n    \n    return xgb_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1D-CNN","metadata":{}},{"cell_type":"code","source":"@timer\ndef trainD1CnnModel(X_train, y_train):\n    \"\"\"Train D1-CNN model\n    \n    Return:\n        keras model obj\n    \"\"\"\n\n    d1_cnn_model = keras.Sequential([\n        layers.Dense(4096, activation='relu'),\n        layers.Reshape((256, 16)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n        layers.MaxPooling1D(pool_size=2),\n        layers.Flatten(),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(1, activation='sigmoid'),\n    ])\n\n    d1_cnn_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=3e-3),\n        loss='binary_crossentropy',\n        metrics=[keras.metrics.BinaryCrossentropy()]\n    )\n\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=25,\n        min_delta=0.001,\n        restore_best_weights=True,\n    )\n\n    d1_cnn_model.fit(\n        X_train, y_train,\n        batch_size=10000,\n        epochs=5000,\n        callbacks=[early_stopping],\n        validation_data=(X_test, y_test),\n    )\n    \n    return d1_cnn_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TabNet","metadata":{}},{"cell_type":"code","source":"@timer\ndef tabNetPretrain(X_train):\n    \"\"\"Pretrain TabNet model\n    \n    Return:\n        TabNet pretrainer obj\n    \"\"\"\n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                             n_independent=2, n_shared=2,\n                             seed=42, lambda_sparse=1e-3,\n                             optimizer_fn=torch.optim.Adam,\n                             optimizer_params=dict(lr=2e-2,\n                                                   weight_decay=1e-5\n                                                  ),\n                             mask_type=\"entmax\",\n                             scheduler_params=dict(max_lr=0.05,\n                                                   steps_per_epoch=int(X_train.shape[0] / 256),\n                                                   epochs=200,\n                                                   is_batch_level=True\n                                                  ),\n                             scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                             verbose=10\n                        )\n\n    pretrainer = TabNetPretrainer(**tabnet_params)\n\n    pretrainer.fit(\n        X_train=X_train.to_numpy(),\n        eval_set=[X_train.to_numpy()],\n        max_epochs = 100,\n        patience = 10, \n        batch_size = 256, \n        virtual_batch_size = 128,\n        num_workers = 1, \n        drop_last = True)\n    \n    return pretrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@timer\ndef trainTabNetModel(X_train, y_train, pretrainer):\n    \"\"\"Train TabNet model\n    \n    Args:\n        pretrainer: pretrained model. If not using this, use None\n        \n    Return:\n        TabNet model obj\n    \"\"\"\n    \n    tabNet_model = TabNetClassifier(\n                                   n_d=16,\n                                   n_a=16,\n                                   n_steps=4,\n                                   gamma=1.9,\n                                   n_independent=4,\n                                   n_shared=5,\n                                   seed=42,\n                                   optimizer_fn = torch.optim.Adam,\n                                   scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                                   scheduler_fn=torch.optim.lr_scheduler.MultiStepLR\n                                  )\n\n    tabNet_model.fit(\n        X_train = X_train.to_numpy(),\n        y_train = y_train.to_numpy(),\n        eval_set=[(X_train.to_numpy(), y_train.to_numpy()),\n                  (X_test.to_numpy(), y_test.to_numpy())],\n        max_epochs = 100,\n        batch_size = 256,\n        patience = 10,\n        from_unsupervised = pretrainer\n        )\n    \n    return tabNet_model","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Make predictions\ndef makePredictions(X_test, xgb_model, d1_cnn_model, tabNet_model):\n    \"\"\"Make predictions\n    \n    Return:\n        Predictions from each models\n    \"\"\"\n    \n    y_xgb_pred = xgb_model.predict(xgb.DMatrix(X_test, feature_names=FEATS))\n    y_d1_cnn_pred = d1_cnn_model.predict(X_test).reshape(1, -1)[0]\n    y_tabNet_pred = tabNet_model.predict_proba(X_test.to_numpy())[:,1]\n    \n    return [y_xgb_pred, y_d1_cnn_pred, y_tabNet_pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\ndef evaluate(y_xgb_pred, y_d1_cnn_pred, y_tabNet_pred) -> None:\n    \"\"\"Evaluate the predictions\n    \n    Process:\n        Print ROC AUC and F1 score of each models\n    \"\"\"\n    \n    preds = {\"XGBoost\":y_xgb_pred, \"D1 CNN\":y_d1_cnn_pred, \"TabNet\":y_tabNet_pred}\n\n    for key in preds:\n        print(\"The ROC AUC score of \"+ str(key) +\" model is \" +\n              str(round(roc_auc_score(y_test, preds[key]), 4))\n             )\n\n    for key in preds:\n        print(\"The F1 score of \"+ str(key) +\" model at threshold = 0.27 is \" +\n              str(round(f1_score(y_test, np.where(preds[key] > 0.27, 1, 0)), 4))\n             )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot prediction distribution\ndef plotPredictionDistribution(y_xgb_pred, y_d1_cnn_pred, y_tabNet_pred) -> None:\n    \"\"\"Plot histogram of predicted probability distributions of each model\n    \"\"\"\n    \n    preds = {\"XGBoost\":y_xgb_pred, \"D1 CNN\":y_d1_cnn_pred, \"TabNet\":y_tabNet_pred}\n\n    for key in preds:\n        plt.hist(preds[key], bins = 100)\n        plt.title(f\"Predicted probability distribution of {key}\")\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROUNDS = 500\n\nFEATS = [\n         \"CLAIM3YEARS_Y\", \"BUS_USE_Y\", \"AD_BUILDINGS_Y\",\n         \"CONTENTS_COVER_Y\", \"P1_SEX_M\", \"P1_SEX_N\", \"BUILDINGS_COVER_Y\", \n         \"P1_POLICY_REFUSED_Y\", \"APPR_ALARM_Y\", \"APPR_LOCKS_Y\", \"FLOODING_Y\", \n         \"NEIGH_WATCH_Y\", \"SAFE_INSTALLED_Y\", \"SEC_DISC_REQ_Y\", \"SUBSIDENCE_Y\", \n         \"LEGAL_ADDON_POST_REN_Y\", \"HOME_EM_ADDON_PRE_REN_Y\", \n         \"HOME_EM_ADDON_POST_REN_Y\", \"GARDEN_ADDON_PRE_REN_Y\",\n         \"GARDEN_ADDON_POST_REN_Y\", \"KEYCARE_ADDON_PRE_REN_Y\", \n         \"KEYCARE_ADDON_POST_REN_Y\", \"HP1_ADDON_PRE_REN_Y\", \"HP1_ADDON_POST_REN_Y\", \n         \"HP2_ADDON_PRE_REN_Y\", \"HP2_ADDON_POST_REN_Y\", \"HP3_ADDON_PRE_REN_Y\", \n         \"HP3_ADDON_POST_REN_Y\", \"MTA_FLAG_Y\", \"OCC_STATUS_LP\",\n         \"OCC_STATUS_PH\", \"OCC_STATUS_UN\", \"OCC_STATUS_WD\",\n         \"OWNERSHIP_TYPE_2.0\", \"OWNERSHIP_TYPE_3.0\", \"OWNERSHIP_TYPE_6.0\", \n         \"OWNERSHIP_TYPE_7.0\", \"OWNERSHIP_TYPE_8.0\", \"OWNERSHIP_TYPE_11.0\", \n         \"OWNERSHIP_TYPE_12.0\", \"OWNERSHIP_TYPE_13.0\", \"OWNERSHIP_TYPE_14.0\", \n         \"OWNERSHIP_TYPE_16.0\", \"OWNERSHIP_TYPE_17.0\", \n         \"OWNERSHIP_TYPE_18.0\", \"PROP_TYPE_2.0\", \"PROP_TYPE_3.0\", \"PROP_TYPE_4.0\", \n         \"PROP_TYPE_7.0\", \"PROP_TYPE_9.0\", \"PROP_TYPE_10.0\", \n         \"PROP_TYPE_16.0\", \"PROP_TYPE_17.0\", \"PROP_TYPE_18.0\", \"PROP_TYPE_19.0\", \n         \"PROP_TYPE_20.0\", \"PROP_TYPE_21.0\", \"PROP_TYPE_22.0\", \"PROP_TYPE_23.0\", \n         \"PROP_TYPE_24.0\", \"PROP_TYPE_25.0\", \"PROP_TYPE_26.0\", \"PROP_TYPE_27.0\", \n         \"PROP_TYPE_29.0\", \"PROP_TYPE_30.0\", \"PROP_TYPE_31.0\", \n         \"PROP_TYPE_32.0\", \"PROP_TYPE_37.0\", \"PROP_TYPE_39.0\", \n         \"PROP_TYPE_40.0\", \"PROP_TYPE_44.0\", \"PROP_TYPE_45.0\", \"PROP_TYPE_47.0\", \n         \"PROP_TYPE_48.0\", \"PROP_TYPE_51.0\", \"PROP_TYPE_52.0\", \"PROP_TYPE_53.0\", \n         \"PAYMENT_METHOD_NonDD\", \"PAYMENT_METHOD_PureDD\", \"P1_EMP_STATUS_C\", \n         \"P1_EMP_STATUS_E\", \"P1_EMP_STATUS_F\", \"P1_EMP_STATUS_H\", \"P1_EMP_STATUS_I\", \n         \"P1_EMP_STATUS_N\", \"P1_EMP_STATUS_R\", \"P1_EMP_STATUS_S\", \"P1_EMP_STATUS_U\", \n         \"P1_EMP_STATUS_V\", \"P1_MAR_STATUS_B\", \"P1_MAR_STATUS_C\", \"P1_MAR_STATUS_D\", \n         \"P1_MAR_STATUS_M\", \"P1_MAR_STATUS_N\", \"P1_MAR_STATUS_O\", \"P1_MAR_STATUS_P\", \n         \"P1_MAR_STATUS_S\", \"P1_MAR_STATUS_W\", \n         \"age\", \"property_age\", \"cover_length\", \"RISK_RATED_AREA_B_imputed\", \n         \"RISK_RATED_AREA_C_imputed\", \"MTA_FAP_imputed\", \"MTA_APRP_imputed\",\n         \"SUM_INSURED_BUILDINGS\", \"NCD_GRANTED_YEARS_B\", \"SUM_INSURED_CONTENTS\", \n         \"NCD_GRANTED_YEARS_C\", \"SPEC_SUM_INSURED\", \"SPEC_ITEM_PREM\", \n         \"UNSPEC_HRP_PREM\", \"BEDROOMS\", \"MAX_DAYS_UNOCC\", \"LAST_ANN_PREM_GROSS\"\n        ]\n\n\nprint(\"Reading the data\")\ndf = pd.read_csv(\"../input/home-insurance/home_insurance.csv\")\n\nprint(\"Preprocessing the data\")\nX_train, y_train, X_test, y_test = splitData(df, FEATS)\nX_train, X_test = standardiseNumericalFeats(X_train, X_test)\n\nprint(\"The ratio of lapse class in training set is \" +\n      str(round(y_train.sum()/len(y_train) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"The ratio of lapse class in test set is \" +\n      str(round(y_test.sum()/len(y_test) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"Training XGBoost model\")\nxgb_model = trainXgbModel(X_train, y_train, X_test, y_test, FEATS, ROUNDS)\n\nprint(\"Training MLP model\")\nd1_cnn_model = trainD1CnnModel(X_train, y_train)\n\nprint(\"Training TabNet model\")\ntabNet_model = trainTabNetModel(X_train, y_train, None)\n\nprint(\"Making predictions\")\ny_xgb_pred, y_d1_cnn_pred, y_tabNet_pred = makePredictions(X_test, xgb_model, d1_cnn_model, tabNet_model)\n\nprint(\"Evaluation of the model\")\nevaluate(y_xgb_pred, y_d1_cnn_pred, y_tabNet_pred)\n\nprint(\"Prediction distribution\")\nplotPredictionDistribution(y_xgb_pred, y_d1_cnn_pred, y_tabNet_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance","metadata":{}},{"cell_type":"code","source":"# XGBoost model\nimportance_xgb = pd.DataFrame.from_dict(xgb_model.get_score(importance_type=\"gain\"),orient=\"index\").sort_values(0, ascending = False)\nimportance_xgb.columns = [\"importance\"]\nimportance_xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TabNet model\nimportance_tabNet = pd.DataFrame(tabNet_model.feature_importances_,index=X_train.columns).sort_values(0, ascending = False)\nimportance_tabNet.columns = [\"importance\"]\nimportance_tabNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}