{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook contains material from [Kaggle's Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning), [Kaggle's Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) and [Kaggle's Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability).\n\n### Table of Contents\n#### <a href='#partI'> 1. Intro to Machine Learning </a>  \n> <a href='#dataExploration'> 1.1 Basic Data Exploration </a>  \n> <a href='#buildingModel'> 1.2 Building a Model </a>  \n> <a href='#modelValidation'> 1.3 Model Validation </a>  \n> <a href='#underfitting&overfitting'> 1.4 Underfitting and Overfitting </a>  \n> <a href='#randomForests'> 1.5 Random Forests </a>\n\n#### <a href='#partII'> 2. Intermediate Machine Learning </a>  \n> <a href='#missingValues'> 2.1 Missing Values </a>  \n> <a href='#categoricalValues'> 2.2 Categorical Values </a>  \n> <a href='#pipelines'> 2.3 Pipelines </a>  \n> <a href='#crossValidation'> 2.4 Cross-Validation </a>  \n> <a href='#XGBoost'> 2.5 XGBoost </a>  \n> <a href='#dataLeakage'> 2.6 Data Leakage </a>  \n\n#### <a href='#partIII'> 3. Machine Learning Explainability </a>  \n> <a href='#modelInsights'> 3.1 Use cases for Model Insights </a>  \n> <a href='#permutationImportance'> 3.2 Permutation Importance </a>  \n> <a href='#partialDependence'> 3.3 Partial Dependence Plots </a>  \n> <a href='#SHAP'> 3.4 SHAP values </a>  \n\n#### <a href='#references'> References </a> "},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"partI\">1. Intro to Machine Learning</a>\n## <a id=\"dataExploration\">1.1 Basic Data Exploration</a>\nWe have to familiarize ourselves with the data. First, we will use *pandas* to load the *csv* data. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhousing_data_full = pd.read_csv(\"../input/melbourne-housing-snapshot/melb_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we could analyze the data e.g. number of observations, features etc. One convenient method to use is `describe()`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_data_full.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can output the number of features using `columns`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_data_full.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can also remove data with missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_data = housing_data_full.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"buildingModel\">1.2 Building a model</a>\n\nIn our model, we want to predict the price of a house given certain features. Therefore, our target function *y* would be the price:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = housing_data.Price\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we have to choose the different features we will take into account to predict the price of different houses. We have to select them carefully, since they will be the building blocks of the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = [\"Rooms\", \"Bathroom\", \"Landsize\", \"Lattitude\", \"Longtitude\"]\nX = housing_data[selected_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use *decision trees* to model our data. To do so, we import beforehand the *sklearn* module. Then we fit the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nhousing_model = DecisionTreeRegressor(random_state=0)\nhousing_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's carefully compare the true and the predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_values = housing_model.predict(X)\nprint(predicted_values)\nprint(y.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The predicted results match the true values. Note that we have validated our model with training data. That is, the model has to make predictions using data it has already seen. However, what would happen if we give the model a new example? It might happen that the data used for training the model is biased. In other words, we don't take into account all the different possible houses there might be. If that is the case, our model will not **generalize** properly."},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"modelValidation\">1.3 Model Validation</a>\n\nModel validation involves the assessment of the current model if we evaluate it with new data. We define the data used to build the model as the _training set_. The data to test the model is the _test set_. We can split our training set in two different groups: the training set itself, which will be used to train the model; and the validation set, data that is only used to assess the model. The validation set is helpful when the test data is not available (which occurs most of the times). We can split the training data using *sklearn*."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 0)\nhousing_model = DecisionTreeRegressor()\nhousing_model.fit(X_train, y_train)\nval_predictions = housing_model.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can evaluate the performance of the model using different metrics. In this case, we will use the *mean_absolute_error* function from *sklearn*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(y_valid, val_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an absolute error of almost 270,000 dollars (quite terrible!)"},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"underfitting&overfitting\">1.4 Underfitting and Overfitting</a>\n\nBefore, we have talked about the ability of the model to generalize. The more the data, the more theoretical variance the model will account, and hence, the better the generalization of the model. If we use few data or the complexity is the model is to high, we are bound to find **overfitting**: the model will perform well with training data, but it will awful with new observations. Contrarily, if our model is too simple, we will find **underfitting**: the model will be incapable of capturing important patterns within the data. We can observe both phenomena if changing the number of leaves from the decision tree model we used before."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mae_train(max_leaf_nodes, X_train, X_valid, y_train, y_valid):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_train)\n    mae = mean_absolute_error(y_train, preds_val)\n    return(mae)\n\ndef get_mae_test(max_leaf_nodes, X_train, X_valid, y_train, y_valid):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_valid)\n    mae = mean_absolute_error(y_valid, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae_train = get_mae_train(max_leaf_nodes, X_train, X_valid, y_train, y_valid)\n    my_mae_test = get_mae_test(max_leaf_nodes, X_train, X_valid, y_train, y_valid)\n    print(\"Max leaf nodes: %d  \\t\\t MAE train:  %d \\t\\t MAE test:  %d\" %(max_leaf_nodes, my_mae_train, my_mae_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot the training and test error over the different number of leaves. "},{"metadata":{"trusted":true},"cell_type":"code","source":"my_mae_train = []\nmy_mae_test = []\nleaves = range(5,1050,5)\n\nfor idx, max_leaf_nodes in enumerate(leaves):\n    my_mae_train.append(get_mae_train(max_leaf_nodes, X_train, X_valid, y_train, y_valid))\n    my_mae_test.append(get_mae_test(max_leaf_nodes, X_train, X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,5))\n#plt.plot(leaves, my_mae_train)\n#plt.plot(leaves, my_mae_test)\n\nsns.lineplot(x=leaves, y=my_mae_train)\nsns.lineplot(x=leaves, y=my_mae_test)\nplt.xlabel(\"No. leaves\", fontsize=15)\nplt.ylabel(\"MAE\", fontsize=15)\nplt.legend(labels=['Train MAE', 'Test MAE']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot shows how the test error decreases until it reaches a lower-bound at $\\approx$ 150 leaves. The training error keeps decreasing as the complexity increases, since the model keeps capturing all the different patterns of the training data (event those which are particular to the training set)."},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"randomForests\">1.5 Random Forests</a>\nRandom forests look to combine different decision trees models. In this way, we prevent the model from underfitting and overfitting since the prediction is weighed among the different models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X_train, y_train)\npredictionsRF = forest_model.predict(X_valid)\nprint(mean_absolute_error(y_valid, predictionsRF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error has droped considerable to almost 220,000!"},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"partII\">2. Intermediate Machine Learning</a>\n## <a id=\"missingValues\">2.1 Missing Values </a>\nIt might happen that some features of our data are incomplete. In those cases, we have three different options.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and evaluate model in a function\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=50, random_state=1)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n# Retrieve all the dataset and divide it intro train and val\ny = housing_data_full.Price\nX_wo_y = housing_data_full.drop(['Price'], axis=1)\nX = X_wo_y.select_dtypes(exclude=['object'])\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Drop the features with unknown values \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get names of columns with missing values\nfeatures_with_missing = [feature for feature in X_train.columns if X_train[feature].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(features_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(features_with_missing, axis=1)\n\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Imputation (replace the unknown value with the mean over the features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Enhanced imputation (specify in other column weather the value is available or not)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n\n# Make new columns indicating what will be imputed\nfor feature in features_with_missing:\n    X_train_plus[feature + '_was_missing'] = X_train_plus[feature].isnull()\n    X_valid_plus[feature + '_was_missing'] = X_valid_plus[feature].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n\n# Imputation removed column names; put them back\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns\n\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the second and third method achieved a slower error than the first one!"},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"categoricalValues\">2.2 Categorical Values</a>\n\nWe might have some features that include values that are not numbers. For instance, strings such as \"Never\" or \"Always\" or booleans such as \"True\" or \"False\". We can check the categorical values of our data with the following code:\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n# Separate target from predictors\ny = data.Price\nX = data.drop(['Price'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# Drop columns with missing values (simplest approach)\ncols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \nX_train_full.drop(cols_with_missing, axis=1, inplace=True)\nX_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can include this data to the model by mapping those values into numbers. We have three different methods to do that:\n\n### 1. Drop categorical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n\nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. One-hot vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"pipelines\">2.3 Pipelines</a>\n\nThroughout the notebook, we have copied the code for the preprocessing and fit over and over. We can bundle everything together using pipelines, making our code cleaner and more readable.\n### Step 1: Define preprocessing steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Select categorical columns\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Define the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Create and evaluate the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"crossValidation\">2.4 Cross-Validation</a>\n\nIf we perform the train-test split in our data, we might disregard randomness in the data (not used for training but for testing). Cross-validation prevents this bias, by iteratively training and validating the model with different chunks of data. Computationally, it is more expensive than a single training and validation sweep. However, we will be certainly sure that our model does not skim interesting patterns in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"XGBoost\">2.5 XGBoost</a>\n\nRandom forests is an example of ensemble methods. Ensemble methods combine and average predictions of different models. Gradient boosting is a method that iteratively adds new models into an ensemble. The different steps are the following:\n\n1. Calculate the predicitons with the current ensemble\n2. Compute the loss of the ensemble\n3. Fit a new model using the previous loss\n4. Add the new model to the ensemble and repeat\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Read the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n# Select subset of predictors\nfeature_names = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[feature_names]\n\n# Select target\ny = data.Price\n\n# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)\n\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The XGBoost has different parameters to tune: `n_estimators`, that is the number of total models of the ensemble; the `learning_rate`, or how fast will the gradient jump from one iteration to the following; `early_stopping_rounds` which stops the model when there is no gain in the test error; `n_jobs`, which helps to parallelize the tasks in the boosting."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=2)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"dataLeakage\">2.6 Data Leakage</a>\n\nData leakage occurs when your training data contains features that will not be present when the model is used for prediction. There are two main data leakage types: *target leakage* and *train-test contamination*. The first one occurs when the data includes fields that will not be available for prediction tasks (e.g. hindsight events); the train-test contamination occurs when data from the training is introduced in validation."},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"partIII\">3. Machine Learning Explainability</a>\n\n## <a id=\"modelInsights\">3.1 Use cases for Model Insights</a>\nUnderstanding machine learning models is important for learning about the insights underlying prediction in different scenarios â€“ with different data. Some of the ways machine learning explainability are helpful are the following:\n1. Debugging\n2. Informing Feature Engineering\n3. Directing Future Data Collection\n4. Informing Human Decision-Making\n5. Building trust\n\n## <a id=\"permutationImportance\">3.2 Permutation Importance</a>\nFeature importance tries to address the following question: *What features have the biggest impact on our machine learning model when predicting?*\n\nOne of the easiest ways to to quantify feature importance is through *permutation importance*.\n\nIdea: Train a model and perform predictions on a validation set. Then, shuffle the different observations of a feature (column) and assess the performance of your model again. If the predictions are worsen, the feature in discussion affected the model considerably.\n\nWe can make use of the *eli5* library to study permutation importance on our model. We will use the past model trained (XGBoost) with five features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Fill NaN values with the mean\nX_valid = X_valid.fillna(X_valid.mean())\n\n# Perforformance permutation importance\nperm = PermutationImportance(my_model, random_state=1).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results show the most important feature to predict a house price is distance, which refers to the distance from the house to the central business district (CBD). From the five selected features, the year of construction seems to be the least critical.\n\n## <a id=\"partialDependence\">3.3 Partial Dependence Plots</a>\nPartial dependence plots show *how* an individual feature affects predictions. In the same way as permutation importance, partial dependece plots as produced after the model has been trained. By selecting continuously different number of rows, altering the value for one variable and doing predictions on the validation set, we can disentangle the interaction of multiple features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Separate data into training and validation sets\nX = X.fillna(X.mean())\ny_scaled = y.copy()/1000 # scale for visualization \nX_train, X_valid, y_train, y_valid = train_test_split(X, y_scaled)\n\n# Train the model\ntree_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=X_valid, model_features=feature_names, \n                            feature='Distance')\n\n# Plot\npdp.pdp_plot(pdp_goals, 'Distance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can examine how the distance affects the prediction. As we increase the distance, we can see a negative tendency of the price: it decreases as the distance becomes larger. That is, the further is the house from the CDB, the less expensive it becomes. We can analyze other feature, for instance, the year built."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=X_valid, model_features=feature_names, \n                            feature='YearBuilt')\n# Plot\npdp.pdp_plot(pdp_goals, 'YearBuilt');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows how the houses become more afordable as the houses are newer. We can observe the partial dependence of two different features by means of a 2-D partial dependence plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_plot = ['Distance', 'Rooms']\ninter  =  pdp.pdp_interact(model=tree_model, dataset=X_valid, model_features=feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"SHAP\">3.4 SHAP values</a>\nSHAP values (SHappley Additive exPlanations) help to break down the prediction, showing the direct impact from the different features. SHAP values compare the prediction given a value for a feature with a baseline value for the same feature. In this way, it can construct an explanation of why the prediction was off a baseline.  \nWe will use a classification problem to illustrate how the SHAP values work. We will load and use the hospital readmissions dataset. It contains medical information of different patients, together with if they were readmitted to the hospital or not. The task is to predict whether a given patient will need to be rehospitalized or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load Hospital Readmissions dataset\ndata = pd.read_csv(\"../input/hospital-readmissions/train.csv\")\ny = data[\"readmitted\"]\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\nX = X.drop(\"readmitted\", axis=1)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data shows different features, such as time in hospital, the number of procedures or the number of diagnoses. After the model is trained, we can observe the SHAP values to see how each feature affected the final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data and train the classifier\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=1).fit(X_train, y_train)\n\n# Select the row for prediction\nrow_to_show = 15\ndata_for_prediction = X_valid.iloc[row_to_show] \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nmy_model.predict(data_for_prediction_array)\n\n# Calculate and plot SHAP values\nexplainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the summary plot of the first 200 observations\nshap_values = explainer.shap_values(X_valid[:200])\nshap.summary_plot(shap_values[1], X_valid[:200])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot shows on the y-axis the different features of the model. On the x-axis it is ploted the impact of the feature on a given prediction. The color shows the value of the feature itself: low and high."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the dependence plot of the first 200 observations\nshap_values = explainer.shap_values(X[:200])\nshap.dependence_plot('number_inpatient', shap_values[1], X[:200], interaction_index=\"number_diagnoses\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot has an upward tendency. When the feature `number_inpatient` increases, the higher the importance on the model's prediction. The color shows us the value of `number_diagnoses` for a given observation. This dependence plot help us to see the interplay between these two features and the final prediction. Across the observations `number_diagnoses` is considerably high. This help us to devise that the interplay between these two features does not necessary changes the readmission rate."},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"references\">References</a>\n1. [Kaggle's Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) \n2. [Kaggle's Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)\n3. [Kaggle's Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}