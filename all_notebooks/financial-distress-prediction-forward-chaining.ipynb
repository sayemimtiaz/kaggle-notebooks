{"cells":[{"metadata":{"_uuid":"d0490a87d72665fc0e29f3b082bc322c160f019d"},"cell_type":"markdown","source":"# Financial Distress Prediction - Forward Chaining\n\nContrary to Cross-Sectional Data, Longitudinal Data has a time-component that implicitly orders each row. Therefore, it is not a good idea to use K-Fold cross validation to test model performance. The right approach is to use [Forward Chaining](https://robjhyndman.com/hyndsight/tscv/).\n\nPython's scikit-learn module has a [TimeSeriesSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) function that can help run Forward Chaining. However, the function makes an assumption that the entire DataFrame is to be treated as one entity. Whereas, it is possible that the entire DataFrame actually be composed of groups of data (row-wise) each of which group should be treated to Forward Chaining separately. The Financial Distress Prediction Data Set is exactly this type of a data set.\n\nLet me demonstrate what I mean:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load necessary modules\nimport numpy as np \nimport pandas as pd\nimport os\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the Data\n# Read data frame\ndf = pd.read_csv('../input/Financial Distress.csv', index_col=False,\n                 dtype={\n                     'Company': np.uint16,\n                     'Time': np.uint8,\n                     'Financial Distress': np.double\n                 })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5b2d1ea10de9c6821e61b1ae62eb4074222796a"},"cell_type":"code","source":"# Look at the Data\nprint(\"Number of unique companies:\", df.Company.unique().shape[0])  # 422 companies\nprint(\"Number of time periods per company:\")\nprint(pd.crosstab(df.Company, df.Time.sum()))  # Some companies have < 5 time periods","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"205aae08080f80750fd85cf6af76ac87d5356201"},"cell_type":"code","source":"# Take a look at the data based on Groups per Company\ngrouped_company = df.groupby('Company')\n\n# Take first 5 groups\ngroup_gen = ((name, group) for name, group in grouped_company)\nfor name, group in itertools.islice(group_gen, 5):\n    # For each group, print and show the data\n    print('-------------------------------------')\n    print(\"Data of Company\", name)\n    print(group.head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b74a3f62fd01ebe249b6e4fa6d1f0ec023717cd"},"cell_type":"markdown","source":"## Challenges with straightforward Time Series Split:\n\nAs can be seen based on the above code output, although we have 1 common time variable (Time), because we have multiple companies we may have multiple rows belonging to the same Time value (For example, 1 row for Time 1 + Company 1, another row for Time 1 + Company 2 etc).\n\nThis prevents us from using sklearn.model_selection.TimeSeriesSplit, as the assumption of that function is that each row represents a data point from a unique instance of time (and the rows are arranged as per increasing value of time).\n\nWe can still achieve our goal of implementing Forward Chaining by:\n1. Split the Data Set into multiple groups - 1 group per Company\n2. For each group, derive the indexes for Forward Chaining\n3. Combine the list of indexes per group into 1 final index list\n\n## Dealing with Dummy Data:\n\nAs mentioned in the Data Dictionary, one of the features is actually a categorical feature. We will therefore create Dummy Columns:"},{"metadata":{"trusted":true,"_uuid":"dc85ed728ff0998df5523708553ed55a614ad027"},"cell_type":"code","source":"# Dummy Variables\ndummy_cols = pd.get_dummies(df[['x80']], prefix='dummy', columns=['x80'], drop_first=True)\n\nprint(dummy_cols.head())\n\n# Combine dummy_cols back with original data set\nx_cols = [col for col in df.columns if all([col.startswith('x'), col != 'x80'])]\ndf_transformed = pd.concat([df[['Company', 'Time', 'Financial Distress'] + x_cols].reset_index(drop=True),\n                            pd.DataFrame(data=dummy_cols)], axis=1)\n\ndf_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7443cdb0e9f79625ccf40a98d8cc9e16615eb6ef"},"cell_type":"markdown","source":"## Creating Lagged Features:\n\nWith the above pre-processing step out of the way, we will now move on to the 2nd piece of Feature Engineering - Creating lagged features (again, lagged features per group):"},{"metadata":{"trusted":true,"_uuid":"c3f7e422aeac4e540c12283b98311f812ee738ad"},"cell_type":"code","source":"# Helper function to create lagged features\ndef lagged_features(df_long, lag_features, window=2, lag_prefix='lag', lag_prefix_sep='_'):\n    \"\"\"\n    Function calculates lagged features (only for columns mentioned in lag_features)\n    based on time_feature column. The highest value of time_feature is retained as a row\n    and the lower values of time_feature are added as lagged_features\n    :param df_long: Data frame (longitudinal) to create lagged features on\n    :param lag_features: A list of columns to be lagged\n    :param window: How many lags to perform (0 means no lagged feature will be produced)\n    :param lag_prefix: Prefix to name lagged columns.\n    :param lag_prefix_sep: Separator to use while naming lagged columns\n    :return: Data Frame with lagged features appended as columns\n    \"\"\"\n    if not isinstance(lag_features, list):\n        # So that while sub-setting DataFrame, we don't get a Series\n        lag_features = [lag_features]\n\n    if window <= 0:\n        return df_long\n\n    df_working = df_long[lag_features].copy()\n    df_result = df_long.copy()\n    for i in range(1, window+1):\n        df_temp = df_working.shift(i)\n        df_temp.columns = [lag_prefix + lag_prefix_sep + str(i) + lag_prefix_sep + x\n                           for x in df_temp.columns]\n        df_result = pd.concat([df_result.reset_index(drop=True),\n                               df_temp.reset_index(drop=True)],\n                               axis=1)\n\n    return df_result\n\n\n# Now split Data Set into groups (based on Company) and create lagged features for each group\ngrouped_company = df_transformed.groupby('Company')\ncols_to_lag = [col for col in df_transformed.columns if col.startswith('x')]\ndf_cross = pd.DataFrame()\n\nfor name, group in grouped_company:\n    # For each group, calculate lagged features and rbind to df_cross\n    print('----------------------------------------------------')\n    print('Working on group:', name, 'with shape', group.shape)\n    df_cross = pd.concat([df_cross.reset_index(drop=True),\n                          lagged_features(group, cols_to_lag).reset_index(drop=True)],\n                         axis=0)\n    print('Shape of df_cross', df_cross.shape)\n    \n# Remove rows with NAs\ndf_cross = df_cross.dropna()\ndf_cross.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c94187dd3a2e0808de22825d59005479a5addf0"},"cell_type":"markdown","source":"## Time Series Splits per group\n\nNext, we will write a helper function to create time series splits for forward chaining. The function will return a list of tuples. Each tuple will contain 2 values - The train index and the test index.\n\nHere is how the return value will look like:\n\n    > [(Int64Index([10, 11, 12], dtype='int64'), (Int64Index[13, 14], dtype='int64')), # For 1st iteration, train on row-index 10-12. test on row-index 13 and 14\n    > (Int64Index([10, 11, 12, 13, 14), Int64Index([15, 16])),                         # For 2nd iteration, train on row-index 10-14. test on row-index 15 and 16\n    > (Int64Index([10, 11, 12, 13, 14, 15, 16]), Int64Index([17, 18]))]                # For 3rd iteration, train on row-index 10-16. test on row-index 17 and 18"},{"metadata":{"trusted":true,"_uuid":"dd316ccedc65c5cc4b25bfdf9a2f2d6eefc73881"},"cell_type":"code","source":"# Create Time-Series sampling function to draw train-test splits\ndef ts_sample(df_input, train_rows, test_rows):\n    \"\"\"\n    Function to draw specified train_rows and test_rows in time-series rolling sampling format\n    :param df_input: Input DataFrame\n    :param train_rows: Number of rows to use as training set\n    :param test_rows: Number of rows to use as test set\n    :return: List of tuples. Each tuple contains 2 lists of indexes corresponding to train and test index\n    \"\"\"\n    if df_input.shape[0] <= train_rows:\n        return [(df_input.index, pd.Index([]))]\n\n    i = 0\n    train_lower, train_upper = 0, train_rows + test_rows*i\n    test_lower, test_upper = train_upper, min(train_upper + test_rows, df_input.shape[0])\n\n    result_list = []\n    while train_upper < df_input.shape[0]:\n        # Get indexes into result_list\n        # result_list += [([df_input.index[train_lower], df_input.index[train_upper]],\n        #                  [df_input.index[test_lower], df_input.index[test_upper]])]\n        result_list += [(df_input.index[train_lower:train_upper],\n                         df_input.index[test_lower:test_upper])]\n\n        # Update counter and calculate new indexes\n        i += 1\n        train_upper = train_rows + test_rows*i\n        test_lower, test_upper = train_upper, min(train_upper + test_rows, df_input.shape[0])\n\n    return result_list\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6141000ef4a9b5f5dda7ae694a62fdc10a8d5448"},"cell_type":"markdown","source":"## Using ts_sample() per group\n\nThe next step is to use ts_sample **per group** of the data. This will give rise to 1 list of index tuples per group. \n\nMoreover, because the number of time periods per group is not the same, the size of these lengths will also vary. Therefore, we will need a way to **pad the shorter groups**. How this is done is described in the code comments below:"},{"metadata":{"trusted":true,"_uuid":"87354b987c807208a208ee7dc22d62bdbe899d1f"},"cell_type":"code","source":"# For each group, apply function ts_sample\n# Depending on size of group, the output size of ts_sample (which is a list of (train_index, test_index))\n# tuples will vary. However, we want the size of each of these lists to be equal.\n# To do that, we will augment the smaller lists by appending the last seen train_index and test_index\n# For example:\n# group 1 => [(Int64Index([1, 2, 3], dtype='int64'), (Int64Index[4, 5], dtype='int64)),\n#             (Int64Index([1, 2, 3, 4, 5], dtype='int64'), (Int64Index([6], dtype='int64'))]\n# group 2 => [(Int64Index([10, 11, 12], dtype='int64'), (Int64Index[13, 14], dtype='int64')),\n#             (Int64Index([10, 11, 12, 13, 14), Int64Index([15, 16])),\n#             (Int64Index([10, 11, 12, 13, 14, 15, 16]), Int64Index([17, 18]))]\n# Above, group 2 has 3 folds whereas group 1 has 2. We will augment group 2 to also have 3 folds:\n# group 1 => [(Int64Index([1, 2, 3], dtype='int64'), (Int64Index[4, 5], dtype='int64)),\n#             (Int64Index([1, 2, 3, 4, 5], dtype='int64'), (Int64Index([6], dtype='int64')),\n#             (Int64Index([1, 2, 3, 4, 5, 6]), Int64Index([]))]\ngrouped_company_cross = df_cross.groupby('Company')\nacc = []\nmax_size = 0\nfor name, group in grouped_company_cross:\n    # For each group, calculate ts_sample and also store largest ts_sample output size\n    group_res = ts_sample(group, 4, 4)\n    acc += [group_res]\n    # print('Working on name:' + str(name))\n    # print(acc)\n\n    if len(group_res) > max_size:\n        # Update the max_size that we have observed so far\n        max_size = len(group_res)\n\n        # All existing lists (apart from the one added latest)in acc need to be augmented\n        # to match the new max_size by appending the last value in those list (combining train and test)\n        for idx, list_i in enumerate(acc):\n            if len(list_i) < max_size:\n                last_train, last_test = list_i[-1][0], list_i[-1][1]\n                list_i[len(list_i):max_size] = [(last_train.union(last_test),\n                                                 pd.Index([]))] * (max_size - len(list_i))\n\n                acc[idx] = list_i\n\n    elif len(group_res) < max_size:\n        # Only the last appended list (group_res) needs to be augmented\n        last_train, last_test = acc[-1][-1][0], acc[-1][-1][1]\n        acc[-1] = acc[-1] + [(last_train.union(last_test), pd.Index([]))] * (max_size - len(acc[-1]))\n\n\nprint(acc[0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f675edc507367ab8b6c1139668071fa53c184c90"},"cell_type":"code","source":"# acc now contains a list of lists, where each internal list contains tuples of train_index, test_index\n# [[(group_1_train_index1, group_1_test_index1), (group_1_train_index2, group_1_test_index2)],\n#  [(group_2_train_index1, group_2_test_index1), (group_2_train_index2, group_2_test_index2)],\n#  [(group_3_train_index1, group_3_test_index1), (group_3_train_index2, group_3_test_index2)]]\n#\n# Our goal is to drill-down by removing group-divisions:\n# [(train_index1, test_index1), (train_index2, test_index2)]\nflat_acc = []\nfor idx, list_i in enumerate(acc):\n    if len(flat_acc) == 0:\n        flat_acc += list_i\n        continue\n\n    for inner_idx, tuple_i in enumerate(list_i):\n        flat_acc[inner_idx] = (flat_acc[inner_idx][0].union(tuple_i[0]),\n                               flat_acc[inner_idx][1].union(tuple_i[1]))\n\n\nprint(flat_acc[0:2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83d4b0c08c7af4634ddb5cff8566601b85bca347"},"cell_type":"markdown","source":"## Modeling\n\nNow that we have our lagged features as well as the indexes ready for Forward Chaining, we can proceed with modeling.\n\nHowever, one decision that we will need to take is whether we want to treat this as a classification problem or a regression problem. The 'Financial Distress' column is real-valued, containing both positive and negative values. As per the Data Dictionary, we should consider the company financially distressed if the 'Financial Distress' column is <= -0.50. Accordingly, we will convert this problem into a classification problem by using that definition."},{"metadata":{"trusted":true,"_uuid":"01534a4d0b0992ebb398ac4dec29c204bb633e29"},"cell_type":"code","source":"# Convert Financial Distress column into 0 or 1\ndf_model = df_cross.copy()\ndf_model['Financial Distress'] = ['0' if x > -0.50 else '1' for x in df_model['Financial Distress'].values]\n\ndf_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0793ac4fefeaee3c22a9de941509af1761dd25ce"},"cell_type":"code","source":"# For each entry in flat_acc, perform train and test and plot metrics\ndependent_cols = [col for col in df_model.columns if col != 'Financial Distress']\nindependent_col = ['Financial Distress']\nfor idx, tuple_i in enumerate(flat_acc):\n    print('---------------------------------------')\n    X_train, X_test = df_model.loc[tuple_i[0]][dependent_cols], df_model.loc[tuple_i[1]][dependent_cols]\n    y_train, y_test = df_model.loc[tuple_i[0]][independent_col], df_model.loc[tuple_i[1]][independent_col]\n    \n    # Fit logistic regression model to train data and test on test data\n    lr_mod = LogisticRegression(C=0.01, penalty='l2')  # These should be determined by nested cv\n    lr_mod.fit(X_train, y_train)\n    \n    y_pred_proba = lr_mod.predict_proba(X_test)\n    y_pred = lr_mod.predict(X_test)\n    \n    # Print Confusion Matrix and ROC AUC score\n    print('Confusion Matrix:')\n    print(confusion_matrix(y_test, y_pred))\n    \n    print('ROC AUC score:')\n    print(roc_auc_score(y_test['Financial Distress'].astype(int), y_pred_proba[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0505b6e20d04f5e8809c0c057c71ccbb302171bc"},"cell_type":"markdown","source":"## Conclusion\n\nThe aim of this notebook was to show that forward chaining may not always be as straight forward as using TimeSeriesSplit function from sklearn. It is important to understand the structure of the data, and design the cross validation approach appropriately.\n\nOf course, I did gloss over a lot of other aspects in this kernel. We should have also focused on the following:\n1. **Dealing with imbalanced data:** Handling skewed data involves some flavor of undersampling of majority class + oversampling of minority class. We skipped over that here, but doing that is crucial in getting a good classifier. I plan on doing that as a later revision to this notebook.\n2. **Nested Validation to choose hyper parameters (like C value in Logistic Regression):** In practice, we would use nested cross validation to also determine the hyper parameters that we have hard coded here."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}