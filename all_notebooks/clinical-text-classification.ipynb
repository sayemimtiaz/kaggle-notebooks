{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here we look at Medical Transcriptions dataset from Kaggle\nhttps://www.kaggle.com/tboyle10/medicaltranscriptions\n\nThis data was scraped from mtsamples.com\n\nInspiration\nCan you correctly classify the medical specialties based on the transcription text?\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us import all the necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.manifold import TSNE\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A method to get unique words(vocabulary) and sentence count in a list of text ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sentence_word_count(text_list):\n    sent_count = 0\n    word_count = 0\n    vocab = {}\n    for text in text_list:\n        sentences=sent_tokenize(str(text).lower())\n        sent_count = sent_count + len(sentences)\n        for sentence in sentences:\n            words=word_tokenize(sentence)\n            for word in words:\n                if(word in vocab.keys()):\n                    vocab[word] = vocab[word] +1\n                else:\n                    vocab[word] =1 \n    word_count = len(vocab.keys())\n    return sent_count,word_count\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do some exploratory analysis of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clinical_text_df = pd.read_csv(\"/kaggle/input/medicaltranscriptions/mtsamples.csv\")\n\nprint(clinical_text_df.columns)\nclinical_text_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclinical_text_df = clinical_text_df[clinical_text_df['transcription'].notna()]\nsent_count,word_count= get_sentence_word_count(clinical_text_df['transcription'].tolist())\nprint(\"Number of sentences in transcriptions column: \"+ str(sent_count))\nprint(\"Number of unique words in transcriptions column: \"+str(word_count))\n\n\n\ndata_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\ni = 1\nprint('===========Original Categories =======================')\nfor catName,dataCategory in data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\nprint('==================================')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since some catgeories have less than 50 samples i remove them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\nfinal_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\ni=1\nprint('============Reduced Categories ======================')\nfor catName,dataCategory in final_data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\n\nprint('============ Reduced Categories ======================')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot the categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(y='medical_specialty', data = filtered_data_categories )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are interested only in the 'transcription' and 'medical_specialty' columns in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = filtered_data_categories[['transcription', 'medical_specialty']]\ndata = data.drop(data[data['transcription'].isna()].index)\ndata.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets define soome methods for cleaning the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text ): \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text1 = ''.join([w for w in text if not w.isdigit()]) \n    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n    #BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n    \n    text2 = text1.lower()\n    text2 = REPLACE_BY_SPACE_RE.sub('', text2) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    #text2 = BAD_SYMBOLS_RE.sub('', text2)\n    return text2\n\ndef lemmatize_text(text):\n    wordlist=[]\n    lemmatizer = WordNetLemmatizer() \n    sentences=sent_tokenize(text)\n    \n    intial_sentences= sentences[0:1]\n    final_sentences = sentences[len(sentences)-2: len(sentences)-1]\n    \n    for sentence in intial_sentences:\n        words=word_tokenize(sentence)\n        for word in words:\n            wordlist.append(lemmatizer.lemmatize(word))\n    for sentence in final_sentences:\n        words=word_tokenize(sentence)\n        for word in words:\n            wordlist.append(lemmatizer.lemmatize(word))       \n    return ' '.join(wordlist) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets clean the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['transcription'] = data['transcription'].apply(lemmatize_text)\ndata['transcription'] = data['transcription'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets us peform feature extraction using TfidfVectorizer to generate tf-idf features.\nFor more on tf-idf check here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\ntfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\nfeature_names = sorted(vectorizer.get_feature_names())\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize the tf-idf features using t-sne plot. For more on t-sne check here: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.[1] It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\nThe t-sne plot shows that lot of categories are overlapping with each other.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\ntfIdfMatrix = tfIdfMat.todense()\nlabels = data['medical_specialty'].tolist()\ntsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\nplt.figure(figsize=(16,10))\npalette = sns.hls_palette(21, l=.6, s=.9)\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=labels,\n    palette= palette,\n    legend=\"full\",\n    alpha=0.3\n)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us do PCA to reduce dimensionality of features.\nhttps://en.wikipedia.org/wiki/Principal_component_analysis\nPCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\npca = PCA(n_components=0.95)\ntfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\nlabels = data['medical_specialty'].tolist()\ncategory_list = data.medical_specialty.unique()\nX_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us use Logisitic Regression to learn on training data and predict on test data\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us visualize the confusion matrix and the classification results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_test_pred,labels=category_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are quite poor. Let us apply some domain knowledge and see if we can improve the results\nThe surgey category is kind of superset as there can be surgeries belonging to specializations like cardiology,neurolrogy etc. Similarly other categories like Emergency Room Reports, Discharge Summary, Notes also overlap with specialities. Hence i remove them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_data_categories['medical_specialty'] =filtered_data_categories['medical_specialty'].apply(lambda x:str.strip(x))\nmask = filtered_data_categories['medical_specialty'] == 'Surgery'\nfiltered_data_categories = filtered_data_categories[~mask]\nfinal_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\nmask = filtered_data_categories['medical_specialty'] == 'SOAP / Chart / Progress Notes'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Office Notes'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Consult - History and Phy.'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Emergency Room Reports'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'Discharge Summary'\nfiltered_data_categories = filtered_data_categories[~mask]\n\n'''\nmask = filtered_data_categories['medical_specialty'] == 'Pediatrics - Neonatal'\nfiltered_data_categories = filtered_data_categories[~mask]\n'''\nmask = filtered_data_categories['medical_specialty'] == 'Pain Management'\nfiltered_data_categories = filtered_data_categories[~mask]\nmask = filtered_data_categories['medical_specialty'] == 'General Medicine'\nfiltered_data_categories = filtered_data_categories[~mask]\n\n\nmask = filtered_data_categories['medical_specialty'] == 'Neurosurgery'\nfiltered_data_categories.loc[mask, 'medical_specialty'] = 'Neurology'\nmask = filtered_data_categories['medical_specialty'] == 'Nephrology'\nfiltered_data_categories.loc[mask, 'medical_specialty'] = 'Urology'\n\n\ni=1\nprint('============Reduced Categories======================')\nfor catName,dataCategory in final_data_categories:\n    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n    i = i+1\n\nprint('============Reduced Categories======================')\n\n\ndata = filtered_data_categories[['transcription', 'medical_specialty']]\ndata = data.drop(data[data['transcription'].isna()].index)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us use sciscpacy models to detect medical entities in our text\nscispaCy is a Python package containing spaCy models for processing biomedical, scientific or clinical text.\nFor more on scispacy check here:https://allenai.github.io/scispacy/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_bionlp13cg_md-0.2.5.tar.gz\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport en_ner_bionlp13cg_md\nnlp = en_ner_bionlp13cg_md.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_Text( text):\n    wordlist=[]\n    doc = nlp(text)\n    for ent in doc.ents:\n        wordlist.append(ent.text)\n    return ' '.join(wordlist)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us pre-process data using scispacy to detect medical entities in transcriptions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['transcription'] = data['transcription'].apply(process_Text)\ndata['transcription'] = data['transcription'].apply(lemmatize_text)\ndata['transcription'] = data['transcription'].apply(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\nprint('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\nprint('Sample Transcription 3:'+data.iloc[1000]['transcription'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us extract tf-idf features then perform dimensionality reduction on the features using t-sne and plot the t-sne features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75,min_df=5, use_idf=True, smooth_idf=True,sublinear_tf=True, max_features=1000)\ntfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\nfeature_names = sorted(vectorizer.get_feature_names())\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\ntfIdfMatrix = tfIdfMat.todense()\nlabels = data['medical_specialty'].tolist()\ntsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\nplt.figure(figsize=(20,10))\npalette = sns.hls_palette(12, l=.3, s=.9)\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=labels,\n    palette= palette,\n    legend=\"full\",\n    alpha=0.3\n)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=0.95)\ntfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\nlabels = data['medical_specialty'].tolist()\ncategory_list = data.medical_specialty.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create train and test sets.Let us use logistic regression for developing a classification model and then visualize the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \nprint('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\nclf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)\n\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()\nprint(classification_report(y_test,y_test_pred,labels=category_list))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is marked improvement in results. Since some classes are in minority we can use SMOTE(Synthetic Minority Over-sampling Technique\n) to generate more sample form minority class to solve the data imbalance problem. For more on SMOTE check here:https://arxiv.org/pdf/1106.1813.pdf. Let us generate new dataset using SMOTE and then perform classification on them\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smote_over_sample = SMOTE(sampling_strategy='minority')\nlabels = data['medical_specialty'].tolist()\nX, y = smote_over_sample.fit_resample(tfIdfMat_reduced, labels)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)   \nprint('Train_Set_Size:'+str(X_train.shape))\nprint('Test_Set_Size:'+str(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us visualize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = category_list\ncm = confusion_matrix(y_test, y_test_pred, labels)\n\nfig = plt.figure(figsize=(20,20))\nax= fig.add_subplot(1,1,1)\nsns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \nplt.show()\nprint(classification_report(y_test,y_test_pred,labels=category_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still some categories are not getting classofoed properly.Let us look at samples from these classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = filtered_data_categories['medical_specialty'] == 'Radiology'\nradiologyData = filtered_data_categories[mask]\nprint(radiologyData['transcription'].tolist()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = clinical_text_df['medical_specialty'] ==  ' Pediatrics - Neonatal'\npediaData = clinical_text_df[mask]\nprint(pediaData ['transcription'].tolist()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmask = clinical_text_df['medical_specialty'] ==  ' Hematology - Oncology'\noncoData = clinical_text_df[mask]\nprint(oncoData ['transcription'].tolist()[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My learnings from this dataset are:\nThis dataset is very noisy.\n\nLot of text in transcriptions overlaps across categories\n\nWe can apply domain knowledge to reduce the categories\n\nIt is imbalanced dataset and using SMOTE can improve the results\n\nHand coded features may improve results on this dataset but may not apply to generic transcription datasets.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}