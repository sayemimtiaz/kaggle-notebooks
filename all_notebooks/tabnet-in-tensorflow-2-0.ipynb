{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThe world is filled with Tabular Data. Every day databases capture credit card transactions, login events, alarms and customer loyatly card or basket information. This data is unique, it is mixed and often requires a combination of luck and domain expertise to find features useful in regression, classification, survival or ranking problems. Analysts may be working with hundreds or thousands of count, continuous and categorical data sources and derived features, and in many applications, are required for fairness and algorithmic bias to provide global and local explainations about the features used in a given prediction at scale. This isnt the world where a pixel is a pixel is a pixel with edges and local structure, or just a series of word embeddings, there is a lot of garbage data for which there is no off-the-shelf pretrained model to help you. \n\nFor a long time, both in production in many AutoML services and in competition Histogram Gradient Boosting has become popular for its robustness and explainablity- with drawback. These drawbacks are in the ability to handle incredibly sparse information efficiently, multi-output problems and, more recently, continue training on pretrained image and audio backbones which allow the incorperations of other rich data sources. Recently, efforts by [Yandex on NODE](https://research.yandex.com/publications/241) and [Google on TabNet](https://arxiv.org/abs/1908.07442) have looked to push the state of the art, so that deep learning is not just the tool for images, text and audio but also for the wealth of tabular data that dominates make industries.  That is not to say, however, that deep learning cannot or has not shown impressive results in narrow use cases on tabular data: it has. But rather these papers have looked to find general purpose approaches for deep learning on tabualr problems which meet the demands of practitioners in this field. \n\nIn a previous post I looked at existing TabNet implementation and compared them against popular off-the-shelf Boosting and MLP approaches. In this post I take a deeper look at the tricks used in TabNet and offer a detailed and flexible implementation for Tensorflow users looking to give this model a go. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tricks\nSo like any Deep Learning papers, this paper is going to reach into a bag of tricks. Many of these tricks take inspiration from breakthroughs in Natural Language Processing (NLP) and Image Recognition, but are going to useful for improve the stability of our models in training on tabular data, allow for some kind of features selection, like in Tree Ensembles, and fight overfitting. \n  \n__0. Learned Embeddings__  \nSo I nearly didn't include this one. Embeddings are so pervasive these days across deep learning domains that it seems easy to forget their noverly or significance. Embeddings are a nice way of representing categorical data by learning a continuous vector to represent that word, user or product. This means rather than feeding into the first layer of the model a one-hot encoding of our categories, we in effect add an extra 'prelayer' for categorical variables so the first layer in the model sees dense continuous features rather than sparse discrete features. This has proven very effective in a host of applications in particular in unsupervised modelling and transfer learning. In Natural Language these embeddings capture in many cases the symantic meaning of words, which [researchers can later visualize](https://projector.tensorflow.org/) to explore problems such as algorithmic bias. \n![embedding-diagram](https://developers.google.com/machine-learning/crash-course/images/EmbeddingExample2-1.svg)\n  \n__1. Ghost Batch Normalization__  \nTo understand [Ghost Batch Normalization](https://arxiv.org/abs/1705.08741), we need to introduce [Batch Normalization](https://www.youtube.com/watch?v=em6dfRxYkYU). I have linked a video by Andrew Ng which explains very well the advantages in model stability and overfitting of this approach. Using Ghost Batch Normalization we feed large batches into our model, which can help with speed and stability, but at each Batch Normalization layer, split the batch into many smaller virtual-batches upon which to normalize the data seperately before recombining and feeding to next layer. This allows us to train faster and more stably on larger dataset, while still taking advantage of the regularizing effect on Batch Normalization on small batch sizes. \n![types-of-bn](https://i.stack.imgur.com/DLwRc.png)\n  \n__2. Gated Linear Units (GLUs)__  \n[Gated Linear Units](https://arxiv.org/pdf/1612.08083.pdf) can be thought again as a kind of attention mechanism. Similar to LSTMs, the gates formed by this approach involve taking two dense layer outputs, applying a sigmoid activation to one of them, and then multiply the two together. This, authors claim, serves to control the information passed on in the hierarchy depending on what is relevant in some context. \n![GLU-diagram](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQN4fHkyIwoetD75ZvHYXNH-pmotF1DGIHikh3AzRJvRfAxI5o&s)\n\n__3. Attention Mechanisms__  \n[Attention Mechanism can very get complicated](https://www.youtube.com/watch?v=iDulhoQ2pro) but in simple applications are very similar to the GLUs mentioned previously. In this paper the attention mechanism is used to perform some kind of feature selection where a model with a Sparsemax (or Entmax) activation function is used to predict a mask to select only a subset of features for use in later layers of the network. \n![attention-diagram](https://i.imgur.com/1152PYf.png)\n  \n__4. Sparsemax and Entmax__  \n[Sparsemax](https://arxiv.org/abs/1602.02068) and [Entmax](https://arxiv.org/pdf/1905.05702.pdf) are a more extreme kind of softmax activation function which result in a binary mask rather than mask which just sums to one. This has proven useful in conjunction with growing interest into attention mechanisms in Neural Networks and is used in NODE as well to immitate decision trees.   \n![entmax-diagram](https://github.com/deep-spin/entmax/raw/master/entmax.png)\n  \n__5. Skip connections and Residual Networks__  \nThere are a lot of resources on Skip connections. These appoaches have been taken very far in ['ResNet'](https://www.youtube.com/watch?v=GWt6Fu05voI) and [Neural Differential Equations](https://arxiv.org/abs/1806.07366) but are rather simple. In skip connections we take the output of a layer and and its input, $Y = f(X) + X. This has been show to improve the stability of deep models, as in theory layers only learn the changes needed to be made to the input, providing deep models the ability to represent shallow models where optimal. \n![skip connection](https://miro.medium.com/max/570/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n\n  \n\n  \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TabNet\n![tabnet](https://github.com/titu1994/tf-TabNet/raw/master/images/TabNet.png?raw=true)\n  \nThe TabNet Architecture comprises a number of layers and blocks, which together decribe the model. To understand how these pieces fit together we are going to implement them and slowly build up and join each component together. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from typing import Optional, Union, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport tensorflow_addons as tfa\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n\n@tf.function\ndef identity(x):\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__GLU Block__  \nThe first component we are going to need to build is our GLUBlock which complises two fully connected layers, two ghost batch normalization, our identity and sigmoid activation function and multiplication operation. Here we use Tensorflow 2.0 custom layer subclassing to make this layer easy to work with a reusable across the rest of our model. Here I have added a number of type-hints for users to make working with this customer layer easy to follow and apply. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class GLUBlock(tf.keras.layers.Layer):\n    def __init__(self, units: Optional[int] = None,\n                 virtual_batch_size: Optional[int] = 128, \n                 momentum: Optional[float] = 0.02):\n        super(GLUBlock, self).__init__()\n        self.units = units\n        self.virtual_batch_size = virtual_batch_size\n        self.momentum = momentum\n        \n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n            \n        self.fc_outout = tf.keras.layers.Dense(self.units, \n                                               use_bias=False)\n        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n                                                            momentum=self.momentum)\n        \n        self.fc_gate = tf.keras.layers.Dense(self.units, \n                                             use_bias=False)\n        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n                                                          momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n        output = self.bn_outout(self.fc_outout(inputs), \n                                training=training)\n        gate = self.bn_gate(self.fc_gate(inputs), \n                            training=training)\n    \n        return output * tf.keras.activations.sigmoid(gate) # GLU","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Feature Transformer Block__  \nHere we again use subclassing to define a layer to represent either the shared or independent steps to 'Feature Transformer' in the diagram above. This block comprises two GLU Blocks with a skip connection form the output of the first block to the output of the second. Here I have had to add a flag to add a skip connection over the first GLU Block, as the this is only present in the decision step dependent block. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n                 momentum: Optional[float] = 0.02, skip=False):\n        super(FeatureTransformerBlock, self).__init__()\n        self.units = units\n        self.virtual_batch_size = virtual_batch_size\n        self.momentum = momentum\n        self.skip = skip\n        \n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n        \n        self.initial = GLUBlock(units = self.units, \n                                virtual_batch_size=self.virtual_batch_size, \n                                momentum=self.momentum)\n        self.residual =  GLUBlock(units = self.units, \n                                  virtual_batch_size=self.virtual_batch_size, \n                                  momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n        initial = self.initial(inputs, training=training)\n        \n        if self.skip == True:\n            initial += inputs\n\n        residual = self.residual(initial, training=training) # skip\n        \n        return (initial + residual) * np.sqrt(0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Attention Block__  \nThis block is simple to implement and involves prior to the actual mask operation, just a dense layer fed into a batch normalization layer, followed by a sparsemax actication function. The major complication in this block is in how to handle TabNet prior, used to encourage orthogonal feature selection across decision steps. Here we just use it as an input to our layer and reserve to handle the updates to our priors in our TabNet step layer. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentiveTransformer(tf.keras.layers.Layer):\n    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n                 momentum: Optional[float] = 0.02):\n        super(AttentiveTransformer, self).__init__()\n        self.units = units\n        self.virtual_batch_size = virtual_batch_size\n        self.momentum = momentum\n        \n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n            \n        self.fc = tf.keras.layers.Dense(self.units, \n                                        use_bias=False)\n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n                                                     momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n        feature = self.bn(self.fc(inputs), \n                          training=training)\n        if priors is None:\n            output = feature\n        else:\n            output = feature * priors\n        \n        return tfa.activations.sparsemax(output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__TabNetStep__  \nIn this TabNetStep Block I take a nunmber of design decision to make implmentation and reusability simpler.  At this layer we take as inputs our batch normalized features, the output of our shared feature transformer, and our priors of the current step and output the features embedding at our split point, the masked feature to used in the shared feature transfomer black of the next step and the mask used in our attention operation. This mask will be important as we most though layers in ensuring new features are selected across steps and providing local and global feature attributions for each output. This block comprises our FeatureTransformerBlock and Attention Transfomer block and starts to piece all our components together. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabNetStep(tf.keras.layers.Layer):\n    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n                 momentum: Optional[float] =0.02):\n        super(TabNetStep, self).__init__()\n        self.units = units\n        self.virtual_batch_size = virtual_batch_size\n        self.momentum = momentum\n        \n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n        \n        self.unique = FeatureTransformerBlock(units = self.units, \n                                              virtual_batch_size=self.virtual_batch_size, \n                                              momentum=self.momentum,\n                                              skip=True)\n        self.attention = AttentiveTransformer(units = input_shape[-1], \n                                              virtual_batch_size=self.virtual_batch_size, \n                                              momentum=self.momentum)\n        \n    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n        split = self.unique(shared, training=training)\n        keys = self.attention(split, priors, training=training)\n        masked = keys * inputs\n        \n        return split, masked, keys","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__TabNetEncoder__  \nI opted to present the entire model architecture as a layer. This makes this easier to work with between use cases, as we apply TabNet in unsupervised, self-supervised and multiple supervised domains without having to rewrite large tracts of code each time. You will see here, we accumulate our feature embeddings at each decision step, update our priors and compute out entropy loss used to limit how often features are reused across steps.  This makes for a complicated layer, but in many ways adds modularity which is very useful going forward. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabNetEncoder(tf.keras.layers.Layer):\n    def __init__(self, units: int =1, \n                 n_steps: int = 3, \n                 n_features: int = 8,\n                 outputs: int = 1, \n                 gamma: float = 1.3,\n                 epsilon: float = 1e-8, \n                 sparsity: float = 1e-5, \n                 virtual_batch_size: Optional[int]=128, \n                 momentum: Optional[float] =0.02):\n        super(TabNetEncoder, self).__init__()\n        \n        self.units = units\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.virtual_batch_size = virtual_batch_size\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.sparsity = sparsity\n        \n    def build(self, input_shape: tf.TensorShape):            \n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n                                                     momentum=self.momentum)\n        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n                                                    virtual_batch_size=self.virtual_batch_size, \n                                                    momentum=self.momentum)        \n        self.initial_step = TabNetStep(units = self.n_features, \n                                       virtual_batch_size=self.virtual_batch_size, \n                                       momentum=self.momentum)\n        self.steps = [TabNetStep(units = self.n_features, \n                                 virtual_batch_size=self.virtual_batch_size, \n                                 momentum=self.momentum) for _ in range(self.n_steps)]\n        self.final = tf.keras.layers.Dense(units = self.units, \n                                           use_bias=False)\n    \n\n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n        entropy_loss = 0.\n        encoded = 0.\n        output = 0.\n        importance = 0.\n        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n        \n        B = prior * self.bn(X, training=training)\n        shared = self.shared_block(B, training=training)\n        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n\n        for step in self.steps:\n            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n            importance += keys\n            \n            shared = self.shared_block(masked, training=training)\n            split, masked, keys = step(B, shared, prior, training=training)\n            features = tf.keras.activations.relu(split)\n            \n            output += features\n            encoded += split\n            \n        self.add_loss(self.sparsity * entropy_loss)\n          \n        prediction = self.final(output)\n        return prediction, encoded, importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\nWe will be looking at a customer churn classification problem for broadband internet customers. The aim of this notebook is not to explore many complicated approaches to featur engineering but to explore the inner workings of tabnet. The main aim in choosing a dataset was for it to be reasonably large, at around 510125 observations, and to have mixed categorical, count and continuous data, as it common to tabular datasets. The only operations we performed to clean and resample the data was to ensure there was class balance. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORICAL_COLUMNS = ['line_stat', 'serv_type', 'serv_code',\n                       'bandwidth', 'term_reas_code', 'term_reas_desc',\n                       'with_phone_service', 'current_mth_churn']\nNUMERIC_COLUMNS = ['contract_month', 'ce_expiry', 'secured_revenue', 'complaint_cnt']\n\ndf = pd.read_csv('/kaggle/input/broadband-customers-base-churn-analysis/bbs_cust_base_scfy_20200210.csv').assign(complaint_cnt = lambda df: pd.to_numeric(df.complaint_cnt, 'coerce'))\ndf.loc[:, NUMERIC_COLUMNS] = df.loc[:, NUMERIC_COLUMNS].astype(np.float32).pipe(lambda df: df.fillna(df.mean())).pipe(lambda df: (df - df.mean())/df.std())\ndf.loc[:, CATEGORICAL_COLUMNS] = df.loc[:, CATEGORICAL_COLUMNS].astype(str).applymap(str).fillna('')\ndf = df.groupby('churn').apply(lambda df: df.sample(df.churn.value_counts().min()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be taking a simple randomized test-train split approach to cross-validation, though in other applications k-fold, stratified k-fold or backtesting may be more appropriate in a competition or production application. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef get_labels(x: pd.Series) -> pd.Series:\n    \"\"\"\n    Converts strings to unqiue ints for use in Pytorch Embedding\n    \"\"\"\n    labels, levels = pd.factorize(x)\n    return pd.Series(labels, name=x.name, index=x.index)\n\nX, E, y = (df\n           .loc[:, NUMERIC_COLUMNS]\n           .astype('float32')\n           .join(pd.get_dummies(df.loc[:, CATEGORICAL_COLUMNS])),\n           df\n           .loc[:, NUMERIC_COLUMNS]\n           .astype('float32')\n           .join(df.loc[:, CATEGORICAL_COLUMNS].apply(get_labels).add(1).astype('int32')),\n           df.churn == 'Y')\n\nX_train, X_valid, E_train, E_valid, y_train, y_valid = train_test_split(X.to_numpy(), E, y.to_numpy(), train_size=250000, test_size=250000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I wrote some simple helpers to convert of Pandas DataFrame to TF Data records for easy and flexible use with our DenseFeature layer for embeddings in TF2. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature(x: pd.DataFrame, dimension=1) -> Union[tf.python.feature_column.NumericColumn, tf.python.feature_column.EmbeddingColumn]:\n    if x.dtype == np.float32:\n        return tf.feature_column.numeric_column(x.name)\n    else:\n        return tf.feature_column.embedding_column(\n        tf.feature_column.categorical_column_with_identity(x.name, num_buckets=x.max() + 1, default_value=0),\n        dimension=dimension)\n    \ndef df_to_dataset(X: pd.DataFrame, y: pd.Series, shuffle=False, batch_size=50000) -> tf.python.data.ops.dataset_ops.TensorSliceDataset:\n    ds = tf.data.Dataset.from_tensor_slices((dict(X.copy()), y.copy()))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(X))\n    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ncolumns = [get_feature(f) for k, f in E_train.iteritems()]\nfeature_column = tf.keras.layers.DenseFeatures(columns, trainable=True)\n\ntrain, valid = df_to_dataset(E_train, y_train), df_to_dataset(E_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised Learning\nThe first application we will be looking at is in supervised learning this is a primary aim of TabNet so is one we will explore. Here I tried to trick to a number of hyperparameter defaults found in other implementation, exploring only a smaller feature vector size for the purpose of visualization later on.  In my experiment this hampers the formance of the model greatly but, in my implementation, reduces greatly the overall footprint of the model given the use of weight sharing across the steps.   \nHere I use Tensorflow 2's model subclassing approach to make explainations and feature visualization easier later on. For production use, the subclassing API does have some limitation in how model can be serialized and unserialized- some of which have been adressed in Tensorflow 2.2 and 2.3 releases. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabNetClassifier(tf.keras.Model):\n    def __init__(self, outputs: int = 1, \n                 n_steps: int = 3, \n                 n_features: int = 8,\n                 gamma: float = 1.3, \n                 epsilon: float = 1e-8, \n                 sparsity: float = 1e-5, \n                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n                 pretrained_encoder: Optional[tf.keras.layers.Layer] = None,\n                 virtual_batch_size: Optional[int] = 128, \n                 momentum: Optional[float] = 0.02):\n        super(TabNetClassifier, self).__init__()\n        \n        self.outputs = outputs\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.feature_column = feature_column\n        self.pretrained_encoder = pretrained_encoder\n        self.virtual_batch_size = virtual_batch_size\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.sparsity = sparsity\n        \n        if feature_column is None:\n            self.feature = tf.keras.layers.Lambda(identity)\n        else:\n            self.feature = feature_column\n            \n        if pretrained_encoder is None:\n            self.encoder = TabNetEncoder(units=outputs, \n                                        n_steps=n_steps, \n                                        n_features = n_features,\n                                        outputs=outputs, \n                                        gamma=gamma, \n                                        epsilon=epsilon, \n                                        sparsity=sparsity,\n                                        virtual_batch_size=self.virtual_batch_size, \n                                        momentum=momentum)\n        else:\n            self.encoder = pretrained_encoder\n\n    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n        X = self.feature(X)\n        output, encoded, importance = self.encoder(X)\n          \n        prediction = tf.keras.activations.sigmoid(output)\n        return prediction, encoded, importance\n    \n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        prediction, _, _ = self.forward(X)\n        return prediction\n    \n    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        _, encoded, _ = self.forward(X)\n        return encoded\n    \n    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        _, _, importance = self.forward(X)\n        return importance    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"m = TabNetClassifier(outputs=1, n_steps=3, n_features = 2, feature_column=feature_column, virtual_batch_size=250)\nm.compile(tf.keras.optimizers.Adam(learning_rate=0.025), tf.keras.losses.binary_crossentropy)\nm.fit(train, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We show the performance in terms of accuracy in terms of both our training and validation samples to determine the impact of overfitting. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_tabnet_y_pred = m.predict(train)\n\naccuracy_score(y_train, tf_tabnet_y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_tabnet_y_pred = m.predict(valid)\n\naccuracy_score(y_valid, tf_tabnet_y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visualize our models feature space to analyze class seperation. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import holoviews as hv\nhv.extension('bokeh')\n\nZ_train = m.transform(dict(E_train)).numpy()\n\nhv.Scatter(pd.DataFrame(Z_train, columns=['Component 1', 'Component 2'])\n .assign(label=y_train.astype(str))\n .sample(1000),\n  kdims='Component 1', vdims=['Component 2', 'label']).opts(color='label', cmap=\"Category10\", title='Latent feature space')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use of learned masks to determine local and global feature importances or attributions for the model. I don't this the local attributions follow the same sensitivity and interprettations of other model agnostic or tree-based methods but are, in previous experiment, similar to those of common Boosting approaches. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"A_train = m.explain(dict(E_train)).numpy()\n\npd.Series(A_train.mean(0), index=E.columns).plot.bar(title='Global Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unsupervised Pretraining\nThe TabNet authors see their approach particularly valuable in unsupervised or self-supervised learning applications where models can be pre-trained across large amounts of unlabelled data and then fine-tuned on labelled examples. To allow for such as approach, they define a decoder architures which takes in the encoders feature space of the encoder model and passes this input through a number of step of Feature Tansformer Blocks and Dense Layers. This decoder then returns the original feature input of the encoder model as output for use in training. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabNetDecoder(tf.keras.layers.Layer):\n    def __init__(self, units=1, \n                 n_steps = 3, \n                 n_features = 8,\n                 outputs = 1, \n                 gamma = 1.3,\n                 epsilon = 1e-8, \n                 sparsity = 1e-5, \n                 virtual_batch_size=128, \n                 momentum=0.02):\n        super(TabNetDecoder, self).__init__()\n        \n        self.units = units\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.virtual_batch_size = virtual_batch_size\n        self.momentum = momentum\n        \n    def build(self, input_shape: tf.TensorShape):\n        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n                                                    virtual_batch_size=self.virtual_batch_size, \n                                                    momentum=self.momentum)\n        self.steps = [FeatureTransformerBlock(units = self.n_features,\n                                              virtual_batch_size=self.virtual_batch_size, \n                                              momentum=self.momentum) for _ in range(self.n_steps)]\n        self.fc = [tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n    \n\n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        decoded = 0.\n        \n        for ftb, fc in zip(self.steps, self.fc):\n            shared = self.shared_block(X, training=training)\n            feature = ftb(shared, training=training)\n            output = fc(feature)\n            \n            decoded += output\n        return decoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Where this encoder-decoder approach differs from many autoencoders is in its use of masks. The encoder is fed features with certain features masks with zero or their mean and is required to then predict these masked features. This implementation is complicated and see I naively decided to define an internal loss funciton to perform this operation with a dummy loss function used an model compile time, rather than try to handle this in TF Data. This provides room for further work, but does make the model easier for novices unfamiliar with custom loss function and TFData. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabNetAutoencoder(tf.keras.Model):\n    def __init__(self, outputs: int = 1, \n                 inputs: int = 12,\n                 n_steps: int  = 3, \n                 n_features: int  = 8,\n                 gamma: float = 1.3, \n                 epsilon: float = 1e-8, \n                 sparsity: float = 1e-5, \n                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n                 virtual_batch_size: Optional[int] = 128, \n                 momentum: Optional[float] = 0.02):\n        super(TabNetAutoencoder, self).__init__()\n        \n        self.outputs = outputs\n        self.inputs = inputs\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.feature_column = feature_column\n        self.virtual_batch_size = virtual_batch_size\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.sparsity = sparsity\n        \n        if feature_column is None:\n            self.feature = tf.keras.layers.Lambda(identity)\n        else:\n            self.feature = feature_column\n            \n        self.encoder = TabNetEncoder(units=outputs, \n                                    n_steps=n_steps, \n                                    n_features = n_features,\n                                    outputs=outputs, \n                                    gamma=gamma, \n                                    epsilon=epsilon, \n                                    sparsity=sparsity,\n                                    virtual_batch_size=self.virtual_batch_size, \n                                    momentum=momentum)\n        \n        self.decoder = TabNetDecoder(units=inputs, \n                                     n_steps=n_steps, \n                                     n_features = n_features,\n                                     virtual_batch_size=self.virtual_batch_size, \n                                     momentum=momentum)\n        \n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n                                                     momentum=momentum)\n        \n        self.do = tf.keras.layers.Dropout(0.25)\n\n    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n        X = self.feature(X)\n        X = self.bn(X)\n        \n        # training mask\n        M = self.do(tf.ones_like(X), training=training)\n        D = X*M\n        \n        #encoder\n        output, encoded, importance = self.encoder(D)\n        prediction = tf.keras.activations.sigmoid(output)        \n        \n        return prediction, encoded, importance, X, M\n    \n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        # encode\n        prediction, encoded, _, X, M = self.forward(X)\n        T = X * (1 - M)\n\n        #decode\n        reconstruction = self.decoder(encoded)\n        \n        #loss\n        loss  = tf.reduce_mean(tf.where(M != 0., tf.square(T-reconstruction), tf.zeros_like(reconstruction)))\n        \n        self.add_loss(loss)\n        \n        return prediction\n    \n    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        _, encoded, _, _, _ = self.forward(X)\n        return encoded\n    \n    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n        _, _, importance, _, _ = self.forward(X)\n        return importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef dummy_loss(y, t):\n    return 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"ae = TabNetAutoencoder(outputs=1, inputs=12, n_steps=3, n_features = 2, feature_column=feature_column, virtual_batch_size=250)\nae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=dummy_loss)\nae.fit(train, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ae.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visual the latent space described by the model for used in unsupervised applciations. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import holoviews as hv\nhv.extension('bokeh')\n\nZ_train = ae.transform(dict(E_train)).numpy()\n\nhv.Scatter(pd.DataFrame(Z_train, columns=['Component 1', 'Component 2'])\n .assign(label=y_train.astype(str))\n .sample(1000),\n  kdims='Component 1', vdims=['Component 2', 'label']).opts(color='label', cmap=\"Category10\", title='Latent feature space')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unlike many unsupervised autoencoder model, we get some kind of feature importances to the model without having to rely on model agnostic explainations or gradient-based explainations. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"AE_train = ae.explain(dict(E_train)).numpy()\n\npd.Series(AE_train.mean(0), index=E.columns).plot.bar(title='Global Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using this pretrained layer now for use in our next fine-tuning experiment. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ae.layers[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Self-supervised Fine-tuning\nGiven the major motivation of this paper, the performance and flexibility of this approach in fine-tuning appears critical. Here we initialize our classifier with our pretrained TabNet encoder layer and continue training on our labelled data. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pm = TabNetClassifier(outputs=1, n_steps=3, n_features = 2, feature_column=feature_column, pretrained_encoder=ae.layers[1], virtual_batch_size=250)\npm.compile(tf.keras.optimizers.Adam(learning_rate=0.05), tf.keras.losses.binary_crossentropy)\npm.fit(train, epochs=150) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In my experiments pretraining had limited impacy on the performance of the model and exhibitted very different training characteristics requiring a much higher learning rate. This can be typical in transfer learning application and is reason for which many researchers have experimented with partial weight reinitialization by readding some noise to the model at this step. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_tabnet_y_pred = pm.predict(train)\n\naccuracy_score(y_train, tf_tabnet_y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_tabnet_y_pred = pm.predict(valid)\n\naccuracy_score(y_valid, tf_tabnet_y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we visualize the latent feature representation of our model and the learned feature importances. I think this is an interesting application of TabNet which may in time, make it an important tool in solving particular tabular data problems. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Z_train = pm.transform(dict(E_train)).numpy()\n\nhv.Scatter(pd.DataFrame(Z_train, columns=['Component 1', 'Component 2'])\n .assign(label=y_train.astype(str))\n .sample(1000),\n  kdims='Component 1', vdims=['Component 2', 'label']).opts(color='label', cmap=\"Category10\", title='Latent feature space')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AE_train = pm.explain(dict(E_train)).numpy()\n\npd.Series(AE_train.mean(0), index=E.columns).plot.bar(title='Global Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nTabNet is exciting. I think it is too early to know its impact. For now we will have to stay tuned to Winner's Posts and industry Blog Posts to see its usability and real-world performance. I think, in theory, this approach may unlock new domains or approaches to modelling on large mixed datasets. I think, as with many deep learning approaches, there are some challenges in automation which require resiliant infrastucture. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}