{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T12:43:04.995495Z","iopub.execute_input":"2021-08-11T12:43:04.995839Z","iopub.status.idle":"2021-08-11T12:43:05.000504Z","shell.execute_reply.started":"2021-08-11T12:43:04.995809Z","shell.execute_reply":"2021-08-11T12:43:04.999601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Binary classification vs multiclass classification ðŸ¥\n\nBinary classification are those tasks where examples are assigned exactly one of two classes. Multi-class classification is those tasks where examples are assigned exactly one of more than two classes","metadata":{}},{"cell_type":"code","source":"fit, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=2, random_state=0)\nax[0].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g'])[y])\nax[0].title.set_text('Binary classification, only 2 classes')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[1].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g', 'b'])[y]);\nax[1].title.set_text('Multiclass, 3 classes')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T12:43:05.001951Z","iopub.execute_input":"2021-08-11T12:43:05.002233Z","iopub.status.idle":"2021-08-11T12:43:05.304321Z","shell.execute_reply.started":"2021-08-11T12:43:05.002206Z","shell.execute_reply":"2021-08-11T12:43:05.303333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression ðŸš¨","metadata":{}},{"cell_type":"code","source":"class LogisticRegression:\n    \n    def __init__(self, lr=0.1, n_iters=10000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n    \n    def fit(self,X,y):\n        #init parameters\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        #gradient descent\n        for _ in range(self.n_iters):\n            linear_model = np.dot(X, self.weights) + self.bias\n            hx = self._sigmoid(linear_model)\n            \n            dw = (X.T * (hx - y)).T.mean(axis=0)\n            db = (hx - y).mean(axis=0)\n\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db \n\n    def predict(self,X):\n        linear_model = np.dot(X,self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        return y_predicted\n  \n    def _sigmoid(self,x):\n        return(1/(1+np.exp(-x)))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:05.306102Z","iopub.execute_input":"2021-08-11T12:43:05.306663Z","iopub.status.idle":"2021-08-11T12:43:05.315627Z","shell.execute_reply.started":"2021-08-11T12:43:05.306522Z","shell.execute_reply":"2021-08-11T12:43:05.314741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intuition behind algorithm ðŸ‡\n\nWe already know how to separate 2 classes using LogisticRegression. What we're going to do is take our training set and turn this into three separate binary classification problems. We'll turn this into three separate two class classification problems.","metadata":{}},{"cell_type":"code","source":"fit, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[0].scatter(X[:, 0], X[:, 1], c=np.array(['r', 'black', 'black'])[y]);\nax[0].title.set_text('Red class vs green and blue')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[1].scatter(X[:, 0], X[:, 1], c=np.array(['black', 'g', 'black'])[y]);\nax[1].title.set_text('Green class vs red and blue')\n\nX, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nax[2].scatter(X[:, 0], X[:, 1], c=np.array(['black', 'black', 'b'])[y]);\nax[2].title.set_text('Blue class vs red and green')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T12:43:05.317094Z","iopub.execute_input":"2021-08-11T12:43:05.317567Z","iopub.status.idle":"2021-08-11T12:43:05.705614Z","shell.execute_reply.started":"2021-08-11T12:43:05.317519Z","shell.execute_reply":"2021-08-11T12:43:05.704234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we want to learn our model for each of these classification problems. For class that we are trying to predict we assign positive class, 1, in case of first plot it will be red class, and others will be negative class, 0.\n\nSo to summarize, what we will be doing is, fitting three classifiers, and than for each X value we will predict probabilty of how this X value is similar to class Red, class Blue and class Green. After this, we will look at the higher probablity and assign class with higher probablity! Simple!","metadata":{}},{"cell_type":"markdown","source":"# Generate Data ðŸ‰","metadata":{}},{"cell_type":"code","source":"X, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\nplt.scatter(X[:, 0], X[:, 1], c=np.array(['r', 'g', 'b'])[y]);","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:05.707162Z","iopub.execute_input":"2021-08-11T12:43:05.707533Z","iopub.status.idle":"2021-08-11T12:43:05.859751Z","shell.execute_reply.started":"2021-08-11T12:43:05.707499Z","shell.execute_reply":"2021-08-11T12:43:05.858782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiclass Classification ðŸ","metadata":{}},{"cell_type":"code","source":"class MulticlassClassification:\n    \n    def __init__(self):\n        self.models = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits each model\n        \"\"\"\n        for y_i in np.unique(y):\n            # y_i - positive class for now\n            # All other classes except y_i are negative\n\n            # Choose x where y is positive class\n            x_true = X[y == y_i]\n            # Choose x where y is negative class\n            x_false = X[y != y_i]\n            # Concatanate\n            x_true_false = np.vstack((x_true, x_false))\n\n            # Set y to 1 where it is positive class\n            y_true = np.ones(x_true.shape[0])\n            # Set y to 0 where it is negative class\n            y_false = np.zeros(x_false.shape[0])\n            # Concatanate\n            y_true_false = np.hstack((y_true, y_false))\n\n            # Fit model and append to models list\n            model = LogisticRegression()\n            model.fit(x_true_false, y_true_false)\n            self.models.append([y_i, model])\n\n\n    def predict(self, X):\n        y_pred = [[label, model.predict(X)] for label, model in self.models]\n\n        output = []\n\n        for i in range(X.shape[0]):\n            max_label = None\n            max_prob = -10**5\n            for j in range(len(y_pred)):\n                prob = y_pred[j][1][i]\n                if prob > max_prob:\n                    max_label = y_pred[j][0]\n                    max_prob = prob\n            output.append(max_label)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:05.861202Z","iopub.execute_input":"2021-08-11T12:43:05.861573Z","iopub.status.idle":"2021-08-11T12:43:05.870043Z","shell.execute_reply.started":"2021-08-11T12:43:05.861534Z","shell.execute_reply":"2021-08-11T12:43:05.869066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy score ðŸš”","metadata":{}},{"cell_type":"code","source":"model = MulticlassClassification()\nmodel.fit(X, y)\naccuracy_score(y, model.predict(X))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:05.871825Z","iopub.execute_input":"2021-08-11T12:43:05.872093Z","iopub.status.idle":"2021-08-11T12:43:06.791742Z","shell.execute_reply.started":"2021-08-11T12:43:05.872067Z","shell.execute_reply":"2021-08-11T12:43:06.79055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iris Dataset ðŸŒ·","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/iris/Iris.csv')\n\ndata","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:06.793165Z","iopub.execute_input":"2021-08-11T12:43:06.793458Z","iopub.status.idle":"2021-08-11T12:43:06.821213Z","shell.execute_reply.started":"2021-08-11T12:43:06.793428Z","shell.execute_reply":"2021-08-11T12:43:06.820132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']], data['Species']\n\nlabel_encoding = LabelEncoder()\ny = label_encoding.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:06.822426Z","iopub.execute_input":"2021-08-11T12:43:06.8229Z","iopub.status.idle":"2021-08-11T12:43:06.830517Z","shell.execute_reply.started":"2021-08-11T12:43:06.822857Z","shell.execute_reply":"2021-08-11T12:43:06.829531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = MulticlassClassification()\n\nclf.fit(X_train, y_train)\n\naccuracy_score(y_test, clf.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:43:06.83166Z","iopub.execute_input":"2021-08-11T12:43:06.831911Z","iopub.status.idle":"2021-08-11T12:43:07.831481Z","shell.execute_reply.started":"2021-08-11T12:43:06.831888Z","shell.execute_reply":"2021-08-11T12:43:07.830555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}