{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"pip install lifelines","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom lifelines import KaplanMeierFitter\n\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n###\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report,accuracy_score,recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g_H = df.loc[df['ejection_fraction'] > 55]\nb_H = df.loc[df['ejection_fraction'] < 55]\nf_d = df.loc[df['sex'] == 0]\nm_d = df.loc[df['sex'] == 1]\n\ncnd_d = df.loc[df['diabetes'] == 0]\ncd_d = df.loc[df['diabetes'] == 1]\n\nns_d = df.loc[df['smoking'] == 0]\ns_d = df.loc[df['smoking'] == 1]\n\ncnbp_d = df.loc[df['high_blood_pressure'] == 1]\ncbp_d = df.loc[df['high_blood_pressure'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g_H['DEATH_EVENT'].value_counts().plot(kind = 'bar', title = 'Good Ejection fraction', xlabel = 'Death', legend = 'Death_event')\nplt.ylabel('Count')\nplt.legend({'0: Alive, 1: Dead'})\nplt.show()\nb_H['DEATH_EVENT'].value_counts().plot(kind = 'bar', title = 'Bad Ejection fraction', xlabel = 'Death')\nplt.ylabel('Count')\nplt.legend({'0: Alive, 1: Dead'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(x = 'sex', y ='ejection_fraction', data = df)\nplt.legend({'0: Female', '1: Male'})\nplt.ylabel(\"Ejection Fraction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.hist(df.ejection_fraction)\nplt.title('Ejection Fraction of Patients')\nplt.xlabel('Ejection Fraction')\nplt.ylabel('count')\nplt.show()\n\nsns.distplot(m_d.ejection_fraction)\nplt.title('Ejection Fraction of Males')\nplt.show()\nsns.distplot(f_d.ejection_fraction)\nplt.title('Ejection Fraction of Females')\nplt.show()\nsns.distplot(ns_d.ejection_fraction)\nplt.title('Ejection Fraction of Non-Smokers')\nplt.show()\nsns.distplot(s_d.ejection_fraction)\nplt.title('Ejection Fraction of Smokers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Null Hypothesis** - There is no difference in Ejection Fraction between Genders. \n\n**Alternative Hypothesis** - There is a difference in Ejection Fraction between Genders. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import scipy.stats as ss \nt_stat, p_val= ss.ttest_ind(m_d.ejection_fraction,f_d.ejection_fraction)\nprint('P-Value of ejection between Genders - ', p_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a P-Value less than .05 we can **reject** the **Null Hypothesis** and **accept** the **Alternative Hypothesis** that there is a difference in EF between Genders."},{"metadata":{},"cell_type":"markdown","source":"# Survial Analysis"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"kmf = KaplanMeierFitter()\nkmf_g = KaplanMeierFitter()\nkmf_ba = KaplanMeierFitter()\nkmf_f = KaplanMeierFitter()\nkmf_m = KaplanMeierFitter()\n\nkmf_nd = KaplanMeierFitter()\nkmf_d = KaplanMeierFitter()\nkmf_nb = KaplanMeierFitter()\nkmf_b = KaplanMeierFitter()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kmf.fit(df['time'], df['DEATH_EVENT'], label= 'All Patients')\nkmf.plot()\nplt.title(\"The Kaplan-Meier Estimate\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_g.fit(g_H['time'], g_H['DEATH_EVENT'], label = 'Good Ejection Fraction')\nkmf_ba.fit(b_H['time'], b_H['DEATH_EVENT'], label = 'Bad Ejection Fraction')\nkmf_g.plot()\nkmf_ba.plot()\nplt.title(\"Ejection Fraction - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_g.fit(f_d['time'], f_d['DEATH_EVENT'], label = 'Female')\nkmf_b.fit(m_d['time'], m_d['DEATH_EVENT'], label = 'Male')\nkmf_g.plot()\nkmf_b.plot()\nplt.title(\"Sex - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\n\nkmf_nd.fit(cnd_d['time'],cnd_d['DEATH_EVENT'], label = 'No Diabetes')\nkmf_d.fit(cd_d['time'], cd_d['DEATH_EVENT'], label = 'Diabetes')\nkmf_nd.plot()\nkmf_d.plot()\nplt.title(\"Diabetes - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_nb.fit(cnbp_d['time'], cnbp_d['DEATH_EVENT'], label = 'No BP')\nkmf_b.fit(cbp_d['time'], cbp_d['DEATH_EVENT'], label = 'BP')\nkmf_nb.plot()\nkmf_b.plot()\nplt.title(\"BP - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df2 = df\ny = df2['DEATH_EVENT'].values\nX = df2.drop('DEATH_EVENT', axis= 1).values\n\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify= y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised Models"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Evaluate test-set roc_auc_score\nKNN_6_Classifier_roc = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(KNN_6_Classifier_roc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc2 = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"KNN-6-Classifier, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Create the classifier: logreg\nlogreg = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nLogReg_ROC = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_ROC))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc2 = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid=param_grid, cv = 5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"y_pred = logreg_cv.predict(X_test)\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n\nLogReg_CV = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_CV))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression_CV, auc=\"+str(LogReg_CV))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('logreg', LogisticRegression())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = knn_scaled.predict(X_test)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n\nLogReg_Scaled = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_Scaled))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression_Scaled, auc=\"+str(LogReg_Scaled))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada_dt = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada_dt.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_prob_dt = ada_dt.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_dt_roc_auc = roc_auc_score(y_test, y_pred_prob_dt)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_dt_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_prob_dt)\nauc1 = roc_auc_score(y_test, y_pred_prob_dt)\nplt.plot(fpr,tpr,label=\"Ada_boost_DT, auc=\"+str(auc1))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\n# Instantiate ada\nada_rf = AdaBoostClassifier(base_estimator=rfc, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada_rf.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_prob_rf = ada_rf.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_rf_roc_auc = roc_auc_score(y_test, y_pred_prob_rf)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_rf_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_prob_rf)\nauc2 = roc_auc_score(y_test, y_pred_prob_rf)\nplt.plot(fpr,tpr,label=\"Ada_boost_RF, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unsupervised Models"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=3)\n\n# Fit dt to the training set\n\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_test, y_pred)\nprint(\"Test set accuracy: {:.2f}\".format(acc))\n\n# Evaluate test-set roc_auc_score\nDT_AUC = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(DT_AUC))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"Ada_boost_RF, auc=\"+str(DT_AUC))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn import tree\ndf2 = df\ny = df2['DEATH_EVENT']\nX = df2.drop('DEATH_EVENT', axis= 1)\nfig = plt.figure(figsize=(25,20))\n\n_ = tree.plot_tree(dt, \n                   feature_names=X.columns,  \n                   class_names='AD',\n                   filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As Many know a low EF is a potential factor for a Cardiac Event. Looking at the DT above, we see Serum_Creatinine & platelets levels are another possible factor.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}