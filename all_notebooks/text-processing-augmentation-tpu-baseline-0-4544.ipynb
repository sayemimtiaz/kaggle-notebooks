{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"blue\"><b>Natural Language Processing is Fun!</b></font><br><a id=\"1\"></a>\n\nHi, welcome to my kernel. I just want to share this fun NLP notebook to you. I believe you can do better than me, 加油！\n\n> I'm still new to data science, so don't trust me right away, but do an experiment first.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nplt.rcParams['figure.figsize'] = [14, 8]\n\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/train.csv')\ntest_df = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/sampleSubmission.csv')\n\ntrain_df.drop('review_id', axis=1, inplace=True)\n\nprint('Train shape:', train_df.shape)\nprint('Test shape:', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Are emoji's important?\n\n![emoji](https://miro.medium.com/max/4000/0*a_PRfzH2Qrjn3Pnj.png)\n\nIn today’s online communication, **emoji** becoming the primary language that allows us to **communicate with anyone globally** when you need to be quick and precise. **Emoji** also playing an essential part in **sentiment analysis**, so **removing them might not be a right solution**.\n\nIf a company wants to find out how people are feeling about their product, they can capture people’s emotions by analyzing emojis. This will provide an essential piece of information, and it is vital for companies to understand their customer’s feelings better.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import emoji  # https://pypi.org/project/emoji/\n\nhave_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(train_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_emoji_percentage = round(len(have_emoji_train_idx) / train_df.shape[0] * 100, 2)\nprint(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) / test_df.shape[0] * 100, 2)\nprint(f'Test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Emoji Cleaning pipeline\n\n1. Change emoji to text\n1. Delete repeated emoji\n1. Change _ and - with whitespace\n\n> For better understanding about this pipeline, i suggest you to try by yourself","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Without demojize: ', train_df.loc[70266, 'review'])\nprint('\\nUsing demojize: ', emoji.demojize(train_df.loc[70266, 'review']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def emoji_cleaning(text):\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_original = train_df.copy()\ntest_df_original = test_df.copy()\n\n# emoji_cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'] = train_df.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)\ntest_df.loc[have_emoji_test_idx, 'review'] = test_df.loc[have_emoji_test_idx, 'review'].apply(emoji_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before cleaning\ntrain_df_original.loc[have_emoji_train_idx, 'review'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text cleaning\n\nText cleaning is an important step for natural language processing tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\n### In the dataset, i found a lot of:\n* emoticon\n* repeated word characters (especially in bahasa)\n* punctuation\n* shortened words \n* noisy text\n\n> Let's try to clean them one by one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_cleaning(text):\n    \n    # delete lowercase and newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    # delete punctuation\n    text = re.sub('[^a-z0-9 ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['review'] = train_df['review'].apply(review_cleaning)\ntest_df['review'] = test_df['review'].apply(review_cleaning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which rows have repeated words?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"repeated_rows_train = []\nrepeated_rows_test = []\n\nfor idx, review in enumerate(train_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_test.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Total {len(repeated_rows_train)} rows')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete repeated words characters (problem)\n\n* I usually use regex code below to delete repeated characters in bahasa indonesia\n* But many english words have a repeated characters, example: good, less, will etc, it will damage these words\n* You can look example below..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = \"UUUURRGGGEENNTTT\"\nprint(re.sub(r'(\\w)\\1+', r'\\1', test1))\n\ntest2 = \"good product quality good value for money \"\nprint(re.sub(r'(\\w)\\1+', r'\\1', test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Solution\n\n* Repeated character in bahasa indonesia **usually repeated more than 2**, example: bagussssssssssssssssssss, tipisssss\n* The solution is: **just preprocess more than 2 repeated character**, this way will not damage some english words\n\n> Notes: some of repeated character only repeated twice, so this is NOT PERFECT way","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test3 = \"bagussssssssssssssssssss bagusssssssssssssssssss real pict\"\nprint(re.sub(r'(\\w)\\1{2,}', r'\\1', test3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_repeated_char(text):\n    \n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[repeated_rows_train, 'review'] = train_df.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)\ntest_df.loc[repeated_rows_test, 'review'] = test_df.loc[repeated_rows_test, 'review'].apply(delete_repeated_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before: ', train_df_original.loc[92129, 'review'])\nprint('After: ', train_df.loc[92129, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[56938, 'review'])\nprint('After: ', train_df.loc[56938, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[72677, 'review'])\nprint('After: ', train_df.loc[72677, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[36558, 'review'])\nprint('After: ', train_df.loc[36558, 'review'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete the row that has noisy text\n\n* Some of text are noisy, it will useless for training\n* But some of text is not noisy, it just repeated\n* If you understand many languages (just use google translate), you can choose which one are **noisy** or **just repeated**?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# example\ntrain_df_original.loc[[92129, 71640, 76275, 10409], 'review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the row that has noisy text or mislabeled rating\nnoisy_row = [31, 50, 2235, 5244, 10409, 11748, 12384, 14395, 15215, 17629, 20819, 23691, 32089, 39532, 40530, 43954, 48186, 50500, 55834, 60088,\n             60442, 61095, 62982, 63803, 67464, 70791, 74861, 73636, 74119, 76275, 79789, 85745, 91058, 91663, 91800, 93204, 99295, 100903, 101177, 103155,\n             109166, 109566, 109651, 109724, 110115, 110441, 111461, 113175, 115782, 116903, 118099, 118328, 118414, 119071, 125338, 125340, 129496, 129640, \n             132027, 138212, 131626, 134715, 133248, 136217, 141377, 143707, 145045, 146485, 37301]\n\ntrain_df.drop(noisy_row, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recover shortened words to original form\n\n* Perhaps you can get a better indonesian slang dictionary for processing this shortened words\n* Since this review mostly written in english, this won't help a lot\n* This is just a fun experiment :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def recover_shortened_words(text):\n    \n    # put \\b (boundary) for avoid the characters in the word to be replaced\n    # I only make a few examples here, you can add if you're interested :)\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df['review'] = train_df['review'].apply(recover_shortened_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before: ', train_df_original['review'][4912])\nprint('\\nAfter: ', recover_shortened_words(train_df['review'][4912]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before: ', train_df_original['review'][9035])\nprint('\\nAfter: ', recover_shortened_words(train_df['review'][9035]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud + EDA\n\nLet's compare the words in the reviews between rating 1, 3 and 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_1 = train_df[train_df['rating'] == 1]['review']\nrating_3 = train_df[train_df['rating'] == 3]['review']\nrating_5 = train_df[train_df['rating'] == 5]['review']\n\nrating_1_text = ' '.join([text for text in rating_1])\nrating_3_text = ' '.join([text for text in rating_3])\nrating_5_text = ' '.join([text for text in rating_5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rating 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, ImageColorGenerator\n\nrating_1_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='magma', max_words=80).generate(rating_1_text)\n\nplt.imshow(rating_1_wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rating 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\n# Load image\nmask = np.array(Image.open('/kaggle/input/masks/masks-wordclouds/upvote.png'))\n\nrating_3_wordcloud = WordCloud(background_color='black', max_font_size=300, colormap='Pastel1', mask=mask).generate(rating_3_text)\n\nplt.imshow(rating_3_wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rating 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load image\nmask = np.array(Image.open('/kaggle/input/masks/masks-wordclouds/star.png'))\n\nrating_5_wordcloud = WordCloud(background_color='black', max_font_size=300, colormap='Pastel1', mask=mask).generate(rating_5_text)\n\nplt.imshow(rating_5_wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rating Comparison\n\n* Class imbalanced\n* Rating 1 + 2 (negative) - only 19% from the entire dataset\n* Rating 4 + 5 (positive) - 56.77% from the entire dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(train_df['rating'])\n\nfor val in ax.patches:\n    pct = '{:.2f}%'.format(100 * val.get_height() / train_df.shape[0])\n    xpos = val.get_x() + val.get_width() / 2.\n    ypos = val.get_height()\n    ax.annotate(pct, (xpos, ypos), ha='center', va='center', fontsize=14, xytext=(0, 12), textcoords='offset points')\n    \nplt.title('Rating comparison', fontsize=24, pad=15)\nplt.xlabel('rating', labelpad=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Usually more data we have, better performance we can achieve\n\nDeep Learning models typically have millions of parameters, thus require large amounts of data for training in order for over-fit avoidance and better model generalization. However, collecting a large annotated data samples is time-consuming and expensive. One technique aiming to address such a data hungry problem is data augmentation.\n\nProper data augmentation is useful to boost up your model performance. Augmentation is very popular in computer vision area. Image can be augmented easily by flipping, rotation, random cropping etc. It is proved that augmentation is one of the anchor to success of computer vision model.\n\nIn Natural Language Processing field, it is hard to augmenting text due to high complexity of language. Not every word we can replace it by others such as a, an, the. Also, not every word has synonym. Even changing a word, the context will be totally difference.\n\nThere are several techniques to do **text augmentation**:\n* Synonym Replacement: Randomly replace n words in the sentences with their synonyms\n* Random Insertion: Insert random synonyms of words in a sentence, this is done n times\n* Random Deletion: Random removal for each word in the sentence with a probability p\n* Contextualized Word Embeddings\n* Text Generation (GPT-2 or XLNet)\n\n> For detail you can [check here](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28). In this notebook we'll use [nlpaug](https://pypi.org/project/nlpaug/) library. If you're interested, you also can check their tutorial example.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nlpaug -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Try Contextualized Word Embeddings\n\nActually this is my first time using text augmentation, so i choosed **Contextualized Word Embeddings** as an experiment. We can try Synonym Replacement, Random Deletion etc\n\nRemember we have class imbalance problem. We also can add rating 1 & 2 training dataset using this technique. Maybe we can get a better score.\n\n> It will took approximate 3 hours if you run code below. If you don't want to wait, you can use [my augmented data here](https://www.kaggle.com/indralin/text-augmented)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# review_augmented = []\n\n# # action = \"insert\" or 'substitute (default)'\n# aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', action='insert')\n# # aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', action='substitute', aug_p=0.5) # higher aug_p = substitute more words\n\n# for review in train_df['review']:\n#     augmented_text = aug.augment(review)\n#     review_augmented.append(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_aug = pd.DataFrame({'review': review_augmented,\n#                           'rating': train_df['rating']})\n\n# # filter rows that have words less than 10\n# train_aug[train_aug['review'].str.len() >= 10].to_csv('review_aug_insert.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_aug =  pd.read_csv('/kaggle/input/text-augmented/review_aug_insert.csv')\n\nprint('train aug shape:', train_aug.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original:')\nprint(train_df['review'].loc[12])\n\nprint('\\nAugmented:')\nprint(train_aug['review'].loc[12])\n\nprint('\\nOriginal:')\nprint(train_df['review'].loc[9])\n\nprint('\\nAugmented:')\nprint(train_aug['review'].loc[9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train_df, train_aug], axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token) # 5 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMODEL = 'jplu/tf-xlm-roberta-large' # bert-base-multilingual-uncased","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since tf.keras reads your data take 0 as the reference, our category should start from 0 not 1\nrating_mapper_encode = {1: 0,\n                        2: 1,\n                        3: 2,\n                        4: 3,\n                        5: 4}\n\n# convert back to original rating after prediction later(dont forget!!)\nrating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train_df['rating'], num_classes=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=2020)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192\n\nX_train = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df['review'].values, tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_train.shape[0] // BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# Get training and test loss histories\ntraining_loss = train_history.history['loss']\ntest_loss = train_history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_dataset, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# don't forget to save for ensemble\nnp.save('xlm-roberta', pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_sentiment = np.argmax(pred, axis=1)\n\nprint(pred_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'review_id': test_df['review_id'],\n                           'rating': pred_sentiment})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last, don't forget to convert back our rating values to 1 - 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['rating'] = submission['rating'].map(rating_mapper_decode)\n\nsubmission.to_csv('submission', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\n* [Emoticon and Emoji in Text Mining](https://medium.com/towards-artificial-intelligence/emoticon-and-emoji-in-text-mining-7392c49f596a)\n* [Data Augmentation in NLP](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}