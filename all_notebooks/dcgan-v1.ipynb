{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nfrom IPython import display\n\nimport os\n\n# Keras functions\nimport tensorflow as tf\n\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Conv2D, Conv2DTranspose, Reshape, Flatten\nfrom keras.layers import Dropout, LeakyReLU, BatchNormalization\nfrom keras.layers import Activation, ZeroPadding2D, UpSampling2D\nfrom keras.layers import Input, Reshape\nfrom matplotlib import pyplot\nfrom IPython.display import clear_output\n\n# Numpy functions\nimport numpy\nimport numpy as np\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import vstack\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\n\n#Torchvision for fast and easy loading and resizing\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\n#for efficient data loading and manipulation, the images are save as binary file\n#to reproduce the file the  code is below\n#but if the saved file is already present the function will load it and not\n#create it from scratch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change these paths accordingly\ndataset_path = \"../input/best-artworks-of-all-time/resized\" \nsaved_binary_file = \"/kaggle/working/training_data.npy\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WIDTH = 64\nHEIGHT = 64\nIMG_SIZE = (WIDTH,HEIGHT)\nEPOCHS = 250","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(path_of_file , path_of_data):\n    #Look for saved file to save time loading and processing images between runs\n    print(\"Looking for saved binary file...\")\n\n    if not os.path.isfile(path_of_file):\n        print(\"\\n File not found, creating  new file...\\n\")\n        dataset = []\n        transform_ds = transforms.Compose([transforms.Resize(IMG_SIZE),]) #define transformation\n        \n        image_folder = torchvision.datasets.ImageFolder(root=path_of_data,\n                                        transform=transform_ds)\n\n        print('Number of artworks found: ',len(image_folder))\n        \n        \n        print(\"Converting images, this will take a few minutes\")\n        for i in range (len(image_folder)):\n            image_array = numpy.array(image_folder[i][0])\n            dataset.append(image_array)\n            if (i%500 == 0):\n                print(\"Pictures processed: \", i)\n                \n        print(\"Saving dataset binary file...\")\n        dataset = np.array(dataset, dtype=np.float32)\n        dataset = (dataset - 127.5) / 127.5 #Normalize to [-1 , 1]\n        numpy.save(path_of_file, dataset)  #Save processed images as npy file\n\n    else:\n        print(\"Data found, loading..\")\n        dataset = np.load(path_of_file) \n\n    print(\"Dataset length: \", len(dataset))\n\n    return dataset\n\ndataset = prepare_data(saved_binary_file , dataset_path)\n\n#Using a TensorFlow Dataset to manage the images for easy shuffling, dividing etc\nBATCH_SIZE = 128\n\ntraining_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(60000).batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_random_art(dataset):\n    \"\"\"\n    function to plot some random images from the data set \n\n    \"\"\"\n    np.random.shuffle(dataset) #Shuffle the images\n\n    fig = plt.figure(figsize=(12,12))\n    for i in range(1,37):\n        fig.add_subplot(6,6,i)\n        plt.imshow(dataset[i])\n        plt.axis('off')\n\nvisualize_random_art(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_generator_model(seed_size, channels):\n    \"\"\"\n    This function builds the generator for DCGAN \n    Parameters:\n        seed_size:according to authors of DCGAN , Generator takes this random seed and generates an image\n        channels : output channels that image will have\n    \"\"\"\n    model = tf.keras.Sequential()\n    \n    model.add(Dense(4*4*512,activation=\"relu\",input_dim=seed_size)) #64x64 units\n    model.add(Reshape((4,4,512)))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha = 0.2))\n\n\n    model.add(Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha =0.3))\n    model.add(Dropout(0.4))\n\n    model.add(Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha = 0.2))\n    model.add(Dropout(0.4))\n\n    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha =0.3))\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False  ))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha = 0.2))\n\n    model.add(Conv2DTranspose(channels, (5, 5), strides=(1, 1), padding='same', use_bias=True, activation='tanh'))\n\n \n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_discriminator_model(image_shape):\n    \"\"\"\n    This function builds a discriminator which is a CNN based image classifier\n    and will output probability values of what it thinks is fake or real with  values close to\n    0 being fake and 1 being real.\n\n    Parameters :\n        image_shape : input_image shape (h x w x c)\n    \"\"\"\n    model = tf.keras.Sequential()\n    model.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(64, 64, 3), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n    model.add(BatchNormalization(momentum=0.9))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))\n    model.add(BatchNormalization(momentum=0.9))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))\n    model.add(BatchNormalization(momentum=0.9))\n    model.add(LeakyReLU(alpha=0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED_SIZE = 100\nIMAGE_CHANNELS = 3\nnoise = tf.random.normal([1,SEED_SIZE])\nimage_shape = (HEIGHT, HEIGHT, IMAGE_CHANNELS)\ncross_entropy = tf.keras.losses.BinaryCrossentropy()\n\n\n#uncomment these lines only if model is to be trained from scratch\n\ngenerator = make_generator_model(SEED_SIZE ,IMAGE_CHANNELS)\ngenerated_image = generator(noise, training=False) # untrained generator output\nplt.imshow(generated_image[0, :, :, 0])\ndiscriminator = make_discriminator_model(image_shape) #untrained discriminator output\nprint(discriminator(generated_image))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    \"\"\"\n    The discriminators loss is based on its ability to distinguish real images from fakes.\n    It compares its predictions on real images to an array of ones (remember 1 being real)\n    and its predictions on fake images to an array of zeros (0 being fake). \n    The goal is to classify all real images as 1 and all fakes as 0. \n    The total loss is then these two losses added together.\n\n    Parameters : \n        real_output  : real image from the dataset \n        fake_output  : fake image from the dataset\n    \"\"\"\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(fake_output):\n    \"\"\"\n    The generators loss is a measurement of how good it performed at fooling the discriminator. \n    If the discriminator classifies the fake images as 1, the generator did a good job.\n\n    Parameters :\n     fake_output : fake image from generator\n    \"\"\"\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The two models optimizers are separated because we train them separately.\n# I found a slightly lower generator LR to be beneficial.\n#Beta value of 0.5 generates more stable models as per the findings in the paper \n#\"Unsupervised representation learning with deep convolutional generative adversarial networks\"\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./training_checkpoints\n\n#checkpoint for saving a model\n\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_images(generated_images2):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n    generated_images2 =0.5 * generated_images2 + 0.5\n\n    fig = plt.figure(figsize=(10,10))\n    for i in range(1,21):\n        fig.add_subplot(5,5,i)\n        plt.imshow(generated_images2[i])\n        plt.axis('off')\n    plt.savefig(\"Images.png\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 150\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# Reuse this seed overtime\n# to visualize progress in the animated GIF\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    \"\"\"\n    The training begins by providing a random seed to the generator, which is then used to generate\n    an image. The discriminator then classifies images from both the fake and real dataset.\n    The loss is calculated separately for each model and the gradients are updated.\n\n    Parameters  :\n    images : images to be trained on\n    \"\"\"\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n      #generate_images(generated_images)\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss , disc_loss\n\ndef train(dataset, epochs):\n  for epoch in range(epochs):\n    gen_loss_list = []\n    disc_loss_list = []\n    start = time.time()\n\n    for image_batch in dataset:\n        t =  train_step(image_batch)\n        gen_loss_list.append(t[0])\n        disc_loss_list.append(t[1])\n    # Produce images for the GIF as you go\n    # display.clear_output(wait=True)\n    g_loss = sum(gen_loss_list) / len(gen_loss_list) #calculate losses\n    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n    \n    print(f'Epoch {epoch+1}, gen loss = {g_loss}, disc loss = {d_loss}')\n  # Generate after the final epoch\n  display.clear_output(wait=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\ntrain(training_dataset, EPOCHS) # uncomment to train model from scratch or from a previous checkpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model():\n    generator.save(\"trained_generator.h5\")\nsave_model() #uncomment to save a newly trained model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(path_of_saved_h5_file):\n    import keras\n    generator = keras.models.load_model(path_of_saved_h5_file)\n\nload_model(\"./trained_generator.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise = tf.random.normal([22,SEED_SIZE])\ngenerated_image = generator(noise, training=False)\ngenerate_images(generated_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}