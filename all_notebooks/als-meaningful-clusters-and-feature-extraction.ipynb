{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Packages and Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # data analysis\nimport pandas as pd # data analysis\nimport matplotlib.pyplot as plt # data vizualization\nimport seaborn as sns # data visualization\nfrom sklearn.model_selection import train_test_split # machine learning\nfrom sklearn.ensemble import RandomForestClassifier # machine learning\nfrom sklearn.cluster import KMeans\n\nexample_clinical_data_path_1 = '/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/Demographics.csv'\nexample_clinical_data_path_2 = '/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/ALSFRS_R.csv'\nexample_transcriptomics_DESEQ2_data_path_1 = '/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/bulbar_vs_limb.csv'\nexample_transcriptomics_DESEQ2_data_path_2 = '/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/ctrl_vs_case.csv'\nexample_transcriptomics_3counts_data_path = '/kaggle/input/end-als/end-als/transcriptomics-data/L3_counts/CASE-NEUZX521TKK/CASE-NEUZX521TKK-5793-T/CASE-NEUZX521TKK-5793-T_P85.exon.txt'\n\ndemographics = pd.read_csv(example_clinical_data_path_1)\ndemographics.to_csv('/kaggle/working/demographics.csv')\nalsfrs_scores = pd.read_csv(example_clinical_data_path_2)\nalsfrs_scores.to_csv('/kaggle/working/alsfrs_scores.csv')\nbulbar_vs_limb = pd.read_csv(example_transcriptomics_DESEQ2_data_path_1)\nbulbar_vs_limb.to_csv('/kaggle/working/bulbar_vs_limb.csv')\nctrl_vs_case = pd.read_csv(example_transcriptomics_DESEQ2_data_path_2)\nctrl_vs_case.to_csv('/kaggle/working/ctrl_vs_case.csv')\nexample_transcriptomics_3counts_data = pd.read_csv(example_transcriptomics_3counts_data_path,delim_whitespace=True,skiprows=1,low_memory=False)\nexample_transcriptomics_3counts_data.to_csv('/kaggle/working/L3_counts.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def sort_feature_importances(df, visualize = False):\n    '''\n    Adapted from https://github.com/WillKoehrsen/feature-selector\n    '''\n    #Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    #Normalise the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    #Make a horizontal bar chart of feature importances\n    \n    if(visualize):\n        plt.figure(figsize = (10,6))\n        ax = plt.subplot()\n        #Need to reverse the index to plot most important on top\n        ax.barh(list(reversed(list(df.index[:15]))),\n               df['importance_normalized'].head(15),\n               align = 'center', edgecolor = 'k')\n        #Set the yticks and labels\n        ax.set_yticks(list(reversed(list(df.index[:15]))))\n        ax.set_yticklabels(df['feature'].head(15))\n        #Plot labeling\n        plt.xlabel('Normalized Importance'); plt.title('Feature Importance')\n        plt.show()\n    \n    return df\n\ndef important_clusters(XClusterLabels, Y, numClusters, threshold = 0.8, labelOfInterest = 1):\n    \"\"\" Check which clusters express a given class label in a ratio greater than a threshold\n    \n    Arguments:\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        Y: ndarray of shape (n_samples,), training labels\n        numCluster: an integer representing the number of clusters\n        threshold: a float representing the ratio threshold for a cluster to be significant, defaults to 0.8\n        labelOfInterest: an integer representing the class label of interest\n    \n    Returns:\n        An ndarray containing 0 (not exceeding the threshold) or 1 (exceeding the threshold) for each cluster,\n        and an ndarray containing the ratio for each cluster\n    \"\"\"\n    meaningfulList = np.zeros((numClusters))\n    ratioList = np.zeros((numClusters))\n    \n    for i in np.arange(numClusters):\n        YClusterLabels = Y[XClusterLabels == i]\n        ratio = YClusterLabels[YClusterLabels == labelOfInterest].shape[0] / YClusterLabels.shape[0]\n        if ratio >= threshold:\n            meaningfulList[i] = 1\n        ratioList[i] = ratio\n    return meaningfulList, ratioList\n\ndef extract_important_features(X, XClusterLabels, clusterOfInterest, numFeatures=5000, visualize=False):\n    \"\"\" Find which features are important in a random forest classifier with two classes: \n    being in the cluster of interest, and not being in it.\n\n    Arguments:\n        X: Pandas DataFrame containing the training data\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        clusterOfInterest: an integer representing the cluster of interest\n        numFeatures: an integer representing the number of important features to return, defaults to 5000\n        visualize: a boolean representing whether to visualize the important features, defaults to False\n\n    Returns:\n        A Pandas DataFrame containing the top numFeatures most important features\n    \"\"\"\n    \n    clf = RandomForestClassifier()\n    newClusterLabels = np.zeros(XClusterLabels.shape)\n    newClusterLabels[XClusterLabels == clusterOfInterest] = 1\n    clf.fit(X, newClusterLabels)\n\n    feature_importance_values = clf.feature_importances_\n    features = list(X.columns)\n    feature_importances = pd.DataFrame({'feature': features, 'importance':feature_importance_values})\n    return sort_feature_importances(feature_importances, visualize)[:numFeatures]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run","metadata":{}},{"cell_type":"code","source":"training_dataTest = bulbar_vs_limb.drop(['SiteOnset_Class','Participant_ID'],axis=1)\nlabelsTest = bulbar_vs_limb['SiteOnset_Class']\nX_train, X_test, y_train, y_test = train_test_split(training_dataTest, labelsTest, train_size=0.9)\n\nnumClusters=10\nkmeans = KMeans(n_clusters=numClusters).fit(X_train)\nXclusterLabels = kmeans.predict(X_train)\nmeaningList, ratioList = important_clusters(XclusterLabels, y_train, numClusters, threshold = 0.9)\n\nfor i in np.arange(numClusters):\n    if meaningList[i] == 1:\n        test = extract_important_features(X_train, XclusterLabels, i, visualize=True)\n        print(test[:10])","metadata":{},"execution_count":null,"outputs":[]}]}