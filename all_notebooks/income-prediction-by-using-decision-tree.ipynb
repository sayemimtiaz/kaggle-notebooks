{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction Income: Using Decision Tree Algorithm\n\n**```Use Case:```**\n\nIn this case study, we will build a decision tree to predict the income of a given population, which is labelled as <=$50K and >$50K. The attributes (predictors) are age, working class type, marital status, gender, race etc.\n\n\nIn the following sections, we'll:\n- clean and prepare the data, \n- build a decision tree with default hyperparameters, \n- understand all the hyperparameters that we can tune, and finally\n- choose the optimal hyperparameters using grid search cross-validation.\n"},{"metadata":{},"cell_type":"markdown","source":"## Loading,Understanding and Cleaning Data\n\n** Loading required python packages for data analysis,visualization and model prediction **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To ignore warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Adult dataset to Predict Income"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Adult dataset path\nadult_dataset_path = \"../input/adult_dataset.csv\"\n\n# Function for loading adult dataset\ndef load_adult_data(adult_path=adult_dataset_path):\n    csv_path = os.path.join(adult_path)\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling load adult function and assigning to a new variable df\ndf = load_adult_data()\n# load top 3 rows values from adult dataset\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis : EDA(Exploratory Data Analysis)"},{"metadata":{},"cell_type":"markdown","source":"- The info() method : Used to get a quick description of the data, in particular the total number of rows,columns and each attribute's type and number of non-null values...etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's understand the type of values present in each column of our adult dataframe 'df'.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's look at the other fields. describe() method shows a summary of the numerical attributes(numerical columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical feature of summary/description \ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's get insight of the dataset little bit more and understand them. how they are distributed? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# pull top 5 row values to understand the data and how it's look like\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- You can observe that the columns **workclass** and **occupation** consist of **missing values which are represented as '?' in the  dataframe.**\n\n- On looking a bit more closely, you will also find that whenever workclass is having a missing value, occupation is also missing in that row. Let's check how may rows are missing."},{"metadata":{},"cell_type":"markdown","source":"**Pulling total missing value \"?\" present in \"workclass\" feature **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking \"?\" total values present in particular 'workclass' feature\ndf_check_missing_workclass = (df['workclass']=='?').sum()\ndf_check_missing_workclass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pulling total missing value \"?\" present in \"Occupation\" feature **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking \"?\" total values present in particular 'occupation' feature\ndf_check_missing_occupation = (df['occupation']=='?').sum()\ndf_check_missing_occupation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** check \"?\" values, how many are there in the whole dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking \"?\" values, how many are there in the whole dataset\ndf_missing = (df=='?').sum()\ndf_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**find out percentage of \"?\" value present across the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = (df=='?').sum() * 100/len(df)\npercent_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are 1836 rows with missing values, which is about 5% of the total data. We choose to simply drop these rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find total number of rows which doesn't contain any missing value as '?'\ndf.apply(lambda x: x !='?',axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the rows having missing values in workclass\ndf = df[df['workclass'] !='?']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's see whether any other columns contain a \"?\". Since \"?\" is a string, we can apply this check only on the categorical columns."},{"metadata":{},"cell_type":"markdown","source":"### Analysis on Categorical features/columns is they contain any missing value as'?'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select all categorical variables\ndf_categorical = df.select_dtypes(include=['object'])\n\n# checking whether any other column contains '?' value\ndf_categorical.apply(lambda x: x=='?',axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Thus, the columns **occupation and native.country** contain some **\"?\"s**. Let's get rid of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the \"?\"s from occupation and native.country\ndf = df[df['occupation'] !='?']\ndf = df[df['native.country'] !='?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the dataset whether cleaned or not?\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n- Now we have a clean dataframe which is ready for model building.\n- Next, let's get ride of dummy variable. where ever having categorical value convert those into dummy variable. let's do some data preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\n\nThere are a number of preprocessing steps we need to do before building the model. \n\nFirstly, note that we have both categorical and numeric features as predictors. In previous models such as linear and logistic regression, we had created **dummy variables** for categorical variables, since those models (being mathematical equations) can  process only numeric variables.\n\nAll that is not required in decision trees, since they can process categorical variables easily. However, we still need to **encode the categorical variables** into a standard format so that sklearn can understand them and build the tree. We'll do that using the ```LabelEncoder()``` class, which comes with ```sklearn.preprocessing```. \n\nYou can read the documentation of ```LabelEncoder``` <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\">here</a>.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\n# encode categorical variables using label Encoder\n\n# select all categorical variables\ndf_categorical = df.select_dtypes(include=['object'])\ndf_categorical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Apply label encoder to df_categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply label encoder to df_categorical\nle = preprocessing.LabelEncoder()\ndf_categorical = df_categorical.apply(le.fit_transform)\ndf_categorical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's Contenate df_categorical with master/original dataframe df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, Concatenate df_categorical dataframe with original df (dataframe)\n\n# first, Drop earlier duplicate columns which had categorical values\ndf = df.drop(df_categorical.columns,axis=1)\ndf = pd.concat([df,df_categorical],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at column type\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next, Since here we have income as target/predicted variable we can see it's showing integer though we need to figure out  labelled as <=50ùêæ ùëéùëõùëë >50K and >50K as categorical.\n\n- Let's convert target class/variable int32 to categorical( labelled as <=50ùêæùëéùëõùëë>50Kand>50K)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert target variable income to categorical\ndf['income'] = df['income'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check df info again whether everything is in right format or not\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Now all the categorical columns are successfully encoded. Let's build the model.**"},{"metadata":{},"cell_type":"markdown","source":"## Model Building and Evaluation\n\n**```Let's first build a decision tree with default hyperparameters. Then we'll use cross-validation to tune them.```**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing train_test_split\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting independent variables/features to X\nX = df.drop('income',axis=1)\n\n# Putting response/dependent variable/feature to y\ny = df['income']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=99)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5)\ndt_default.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the evaluation metrics of our default model\n\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# making predictions\ny_pred_default = dt_default.predict(X_test)\n\n# Printing classifier report after prediction\nprint(classification_report(y_test,y_pred_default))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing confusion matrix and accuracy\nprint(confusion_matrix(y_test,y_pred_default))\nprint(accuracy_score(y_test,y_pred_default))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the Decision Tree\n\nTo visualise decision trees in python, you need to install certain external libraries. You can read about the process in detail here: http://scikit-learn.org/stable/modules/tree.html\n\nWe need the ```graphviz``` library to plot a tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to add missing packages that are available in pip add a single cell at the beginning of your kernel with just this line:\n\n!pip install my-package\n\nFor this to work you have to enable temporarily internet access for your kernel. It is important that you check the availability of the package in pip. Execute one time that cell and after it finishes disable internet access if not required. The rest of cells in your kernel will have access to the package from now on.\n\nAn example kernel that uses this approach you will find it here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus,graphviz\n\n# Putting features\nfeatures = list(df.columns[1:])\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**:<br>\nPython requires library pydot and an external software graphviz to visualize the decision tree. If you are on wondows, you'll need to specify the path for the pydot library to access dot file from graphviz.\n\nPlease read the downloadable instructions to install graphviz. For Mac users, one way is to:\n- Install the python graphviz module: ```pip install graphviz```\n- Then install the Graphviz software on Mac, you do this using homebrew:\n    - Install homebrew: https://docs.brew.sh/Installation\n    - ```brew install graphviz```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you're on windows:\n# Specifing path for dot file.\n# import os\n# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/graphviz-2.38/release/bin/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting tree with max_depth=3\ndot_data = StringIO()  \nexport_graphviz(dt_default, out_file=dot_data,\n                feature_names=features, filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"The default tree is quite complex, and we need to simplify it by tuning the hyperparameters. \n\nFirst, let's understand the parameters in a decision tree. You can read this in the documentation using ```help(DecisionTreeClassifier)```.\n\n\n- **criterion** (Gini/IG or entropy): It defines the function to measure the quality of a split. Sklearn supports ‚Äúgini‚Äù criteria for Gini Index & ‚Äúentropy‚Äù for Information Gain. By default, it takes the value ‚Äúgini‚Äù.\n- **splitter**: It defines the strategy to choose the split at each node. Supports ‚Äúbest‚Äù value to choose the best split & ‚Äúrandom‚Äù to choose the best random split. By default, it takes ‚Äúbest‚Äù value.\n- **max_features**: It defines the no. of features to consider when looking for the best split. We can input integer, float, string & None value.\n    - If an integer is inputted then it considers that value as max features at each split.\n    - If float value is taken then it shows the percentage of features at each split.\n    - If ‚Äúauto‚Äù or ‚Äúsqrt‚Äù is taken then max_features=sqrt(n_features).\n    - If ‚Äúlog2‚Äù is taken then max_features= log2(n_features).\n    - If None, then max_features=n_features. By default, it takes ‚ÄúNone‚Äù value.\n- **max_depth**: The max_depth parameter denotes maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. By default, it takes ‚ÄúNone‚Äù value.\n- **min_samples_split**: This tells above the minimum no. of samples reqd. to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then it shows percentage. By default, it takes ‚Äú2‚Äù value.\n- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. If an integer value is taken then consider - -min_samples_leaf as the minimum no. If float, then it shows percentage. By default, it takes ‚Äú1‚Äù value.\n- **max_leaf_nodes**: It defines the maximum number of possible leaf nodes. If None then it takes an unlimited number of leaf nodes. By default, it takes ‚ÄúNone‚Äù value.\n- **min_impurity_split**: It defines the threshold for early stopping tree growth. A node will split if its impurity is above the threshold otherwise it is a leaf.<br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Tuning max_depth\n\nLet's first try to find the optimum values for max_depth and understand how the value of max_depth affects the decision tree.\n\nHere, we are creating a dataframe with max_depth in range 1 to 80 and checking the accuracy score corresponding to each max_depth. \n\nTo reiterate, a grid search scheme consists of:\n\n    - an estimator (classifier such as SVC() or decision tree)\n    - a parameter space\n    - a method for searching or sampling candidates (optional) \n    - a cross-validation scheme, and\n    - a score function (accuracy, roc_auc etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now let's visualize how train and test score changes with max_depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Tuning min_samples_leaf\n\nThe hyperparameter **min_samples_leaf** indicates the minimum number of samples required to be at a leaf.<br>\n\nSo if the values of min_samples_leaf is less, say 5, then the will be constructed even if a leaf has 5, 6 etc. observations (and is likely to overfit).<br>\n\nLet's see what will be the optimum value for min_samples_leaf."},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning min_samples_split\n\nThe hyperparameter **min_samples_split** is the minimum no. of samples required to split an internal node. Its default value is 2, which means that even if a node is having 2 samples it can be furthur divided into leaf nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to Find Optimal Hyperparameters\n\n- We can now use GridSearchCV to find multiple optimal hyperparameters together. Note that this time, we'll also specify the criterion (gini/entropy or IG).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nn_folds = 5\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\ngrid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n                          cv = n_folds, verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\nprint(\"best accuracy\", grid_search.best_score_)\nprint(grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Running the model with best parameters obtained from grid search.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model with optimal hyperparameters\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy score\nclf_gini.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the tree\ndot_data = StringIO()  \nexport_graphviz(clf_gini, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- You can see that this tree is too complex to understand. Let's try reducing the max_depth and see how the tree looks."},{"metadata":{"trusted":true},"cell_type":"code","source":"# tree with max_depth = 3\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=3, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\n\n# score\nprint(clf_gini.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting tree with max_depth=3\ndot_data = StringIO()  \nexport_graphviz(clf_gini, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classification metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\ny_pred = clf_gini.predict(X_test)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Work in Progress..."},{"metadata":{},"cell_type":"markdown","source":"### More to come. Stay tuned.!"},{"metadata":{},"cell_type":"markdown","source":"#### Please Upvote! If you really find this notebook is useful! Thanks,Happy learning!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}