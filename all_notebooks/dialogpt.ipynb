{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\nHola! Today we will be creating a chatbot, but not just any chatbot. In this tutorial, you will create your own open-dialog chatbot, one that doesn't just have premade responses to very specific questions or commands!\n\nThe overall goal of this tutorial is to create a language learning companion where you can practice simple conversations in a language you care about. We will focus on the beautiful Spanish language in this series as I have been trying to learn the language for the past 5 years, however, you should be able to adapt this tutorial to other languages as well.\n\nFirst we are going to cover some of the background material for how all this works (if you are already familiar with the GPT2 model, go ahead and skip this background section). Let's get to it!","metadata":{"id":"pq3dZlNehoxJ"}},{"cell_type":"markdown","source":"# Background","metadata":{"id":"U5iDZXnFhoxK"}},{"cell_type":"markdown","source":"## What is GPT2?\nIn this post, we are going to use the GPT2 model (Generative Pre-Training 2), from the amazing paper [\"Language Models are Unsupervised Multitask Learners\"](https://openai.com/blog/better-language-models/) by Alex Radford et. al. I will be giving a brief overview of this model. However, if you want a more in-depth explanation I highly recommend the blog post [\"The Illustrated GPT-2\"](http://jalammar.github.io/illustrated-gpt2/) by Jay Alammar.\n\nGPT2 is what is called an autoregressive language model. This may sound complicated, but it is actually quiet simple, so lets break down what this means. Autoregressive means that the output of the model is fedback into the model as input. Here is a nice example of how that works:","metadata":{"id":"THybpQmXhoxK"}},{"cell_type":"markdown","source":"![](https://github.com/ncoop57/i-am-a-nerd/blob/master/images/autoregressive.gif?raw=1)\n","metadata":{"id":"IRKfps4EJgJV"}},{"cell_type":"markdown","source":"\n> Image From [Deepmind](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)\n\n\nNow, a language model is usually some statistical model that gives the probability of some word given the context. So, take the following example:\n\n\n> *An [blank] a day keeps the doctor away*\n\n\nA good language model would give higher probability to the word \"apple\" occuring in the [blank] than say the word \"crocodile\" since most likely encountering a crocodile daily would probably have the opposite effect.\n\nPutting them together, we get an autoregressive language model where given some context\n\n> *How much wood could a woodchuck chuck, if a woodchuck could [blank]*\n\nThe statistical model then gives some probability to what the next word will be, which we will use in selecting the word. Once we have the selection we add it to our sentence and repeat the whole process again!\n\n> *How much wood could a woodchuck chuck, if a woodchuck could chuck [blank]*\n\nNow, to train our autoregressive language model we just need to get a bunch of example sentences or just chunks of text, hide the last word, and use these sentences with the missing word as our inputs and the last words as the target. This is essentially the whole idea behind GPT2 and many other autoregressive language models, where they learn how language works by using the context to infer the next word.","metadata":{"id":"EESZ4ViwJkw4"}},{"cell_type":"markdown","source":"## GPT2 as a chatbot\nGreat, so you may be asking yourself, \"how do we use GPT2 as a chatbot?\" To answer this question we need to turn our attention to another paper, [\"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation\"](https://arxiv.org/abs/1911.00536). To see how we can repurpose this generator, GPT2, look at the following example:\n\n> *Hi, how are you? [end_of_turn] I'm good, what about you? [end_of_turn] Not so good, lots of long nights at work. [end_of_turn] Darn, that sucks :( [end_of_conversation]*\n\n\nThis is a sample conversation between two speakers. What's special about it is that there are special tokens that signify when one of the speakers has finished talking, which we in the biz call a turn. If we treat this example like our previous one with the autorgressive language mode, we can do some interesting things:\n\n> *Hi, how are you? [end_of_turn] [blank]*\n\nIf we use the same logic as we did previously, it is easy to see how we can now use GPT2 to guess the next word in this conversation.\n\n> *Hi, how are you? [end_of_turn] I'm [blank]*\n\nWe keep feeding back the prediction of our model and there ya have it! A chatting GPT2, where all we need to do is show the model a bunch of these example conversations and have it predict the next word in the conversation.\n\nI think that is plenty of background, we will revisit exactly how we design a system where we actually hold a conversation with GPT2 once we have the model trained ;).","metadata":{"id":"kSyY-qvkhoxL"}},{"cell_type":"code","source":"","metadata":{"id":"bBpwY0UJ7UOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip -q install transformers==2.9.0 gdown","metadata":{"id":"OxGpp1jWhoxL","outputId":"b61fa75f-02f5-41ca-ee06-235d536d7c99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n# Imports\n\n\n\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.notebook import tqdm, trange\n\nfrom pathlib import Path\n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\n# Configs\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"id":"uYQHZPV4hoxP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define to configuration variables so we don't have a bunch of magic numbers and strings!","metadata":{"id":"CULz6g_7hoxS"}},{"cell_type":"code","source":"#collapse\n# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 3\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()","metadata":{"id":"RPl9JlsDhoxS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Data!\nTo train our chatbot we will be using conversations scraped from subtitles of Spanish TV shows and movies. I've gone ahead and formated the data for us already, however, if you would like to use a different language to train your chatbot you can use [this script](https://colab.research.google.com/drive/1kKErlSSpewQbWexFPEj1rPWsYpMx69ZS?usp=sharing) to generate a csv with the same format I am going to use in the rest of this tutorial.","metadata":{"id":"wTMpgb2ehoxV"}},{"cell_type":"code","source":"! gdown https://drive.google.com/uc?id=1Lp-diuMohUTGyB9BSTFgeGZyY3dkNuEg","metadata":{"id":"aH3suSNchoxW","outputId":"ea83fe0d-b1a9-4d3b-c933-f4c71f06cb4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('final_es_conv.csv')\ndf = df.dropna()\ntrn_df, val_df = train_test_split(df, test_size = 0.2)\ntrn_df.head()","metadata":{"id":"GWC3IpQDhoxY","outputId":"47234310-f885-4779-ea40-02da00294398","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trn_df), len(val_df)","metadata":{"id":"JWEMSNSuhoxc","outputId":"56433cfe-cf76-4d5c-9d86-d0683d5fa107","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\nfrom collections import Counter\nfrom statistics import mean, median, stdev\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"bhPUWF-zhoxf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#collapse\ndef get_counter_and_lens(data, tokenizer):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    toks = [tokenizer.tokenize(x) for x in data]\n    \n    return list(map(len, toks)), Counter(flatten(toks)), Counter(' '.join(data).split())","metadata":{"id":"KQgPi1oPhoxi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\nlens, tok_cnt, word_cnt = get_counter_and_lens(trn_df[df.columns].apply(lambda x: ' '.join(x.astype(str)), axis = 1), tokenizer)","metadata":{"id":"g73rIIzwhoxl","outputId":"52e04782-66cc-4707-ff89-8c68c8833696","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#collapse\ndef plot_counts(counts, top_k = 30):\n    labels, values = zip(*counts.most_common()[:top_k])\n\n    indexes = np.arange(len(labels))\n    width = 1\n    plt.figure(num=None, figsize=(22, 4), dpi=60, facecolor='w', edgecolor='k')\n    plt.bar(indexes, values, width)\n    plt.xticks(indexes + width * 0.5, labels)\n    plt.show()","metadata":{"id":"qzuDY3Ichoxn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_counts(tok_cnt, top_k = 30)\nplot_counts(word_cnt, top_k = 30)","metadata":{"id":"z2Xqryf8hoxq","outputId":"97676f5c-a3a2-493b-d597-fdb4e6d25a82","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#collapse\ndef plot_hist(lens, n_bins = 50):\n    n, bins, patches = plt.hist(lens, n_bins, facecolor='blue', alpha=0.9)\n    plt.show()","metadata":{"id":"rIPN-JQ1hoxt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Mean: {mean(lens)}, Median: {median(lens)}, Standard Deviation: {stdev(lens)}, 90th Percentile: {np.percentile(lens, 100)}')\nplot_hist(lens)","metadata":{"id":"4kEPTCw7hoxw","outputId":"7ff7eb79-7518-43c7-a0ec-1c1422344b98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get our data into a format that we can feed into our model using Pytorch's Dataset and Dataloader API. All these methods do are convert our dataframes where we have multiple historical dialog, i.e., context, and a response, into a single conversation string that is separated a special token that tells our model when a person is finished speaking.\n\nThese conversation strings are then tokenized using HuggingFace's awesome tokenizers into their numerical representation that our model actual understands!","metadata":{"id":"kjkSIjEihoxz"}},{"cell_type":"code","source":"#collapse\ndef construct_conv(row, tokenizer, eos = True):\n    # from: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                if len(conv) > block_size: continue\n                self.examples.append(conv)\n\n            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n            # If your dataset is small, first you should loook for a bigger one :-) and second you\n            # can change this behavior by adding (model specific) padding.\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"id":"t1CoZwUbhox0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n# Cacheing and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","metadata":{"id":"7YuM1dWohox2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluating\nNow that we have THE DATA we can finally create our model and start training it! The training and evaluation loop are quite simple. We simplely take a batch of examples from our dataloader and use it both as our inputs and labels. We do this because GPT2 is an auto-regressive model, meaning it uses some context to predict the next token. This prediction is then added to the original context and fed back in as the new context for generating the next token.\n\nTo evaluate our model, we use the metric perplexity, which is a simple, but powerful metric. Perplexity is a measure of how unsure the model is in its choice of the next token. The more unsure our model is, the higher its perplexity. One fascinating thing about perplexity is that it correlates very well with what humans think of when it comes to coherent and specific natural conversations, which was shown in the amazing paper [\"Towards a Human-like Open-Domain Chatbot\"](https://arxiv.org/abs/2001.09977) by Daniel Adiwardana, et. al.","metadata":{"id":"cpvLi9BQhox5"}},{"cell_type":"code","source":"#collapse\n# Training of model\n\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n# Evaluation of some model\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","metadata":{"id":"gi6Q7v1Ihox5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's put it all together into our runner function and let our baby cook away!","metadata":{"id":"iVN93qofhox8"}},{"cell_type":"code","source":"#collapse\n# Main show runner\n\ndef main(df_trn, df_val):\n    args = Args()\n    \n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelWithLMHead.from_pretrained(\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n    \n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results","metadata":{"id":"FKnrDiKDhox9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%reload_ext tensorboard\n\n%tensorboard --logdir runs","metadata":{"id":"B2hFgUU4hox_","outputId":"7f5b2ed0-502f-4cb0-8c25-b120b252e484","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we run our model! I found this can take anywhere from an hour to three hours depending on the GPU Google give to you to finish training a model that can sort of hold a coherent conversation for the Spanish language. If you are using a different language, you'll have to play around with how long to cook your model for. ","metadata":{"id":"B82oIIPWhoyE"}},{"cell_type":"code","source":"main(trn_df, val_df)","metadata":{"scrolled":true,"id":"TnFa5PhOhoyE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chatting with our Model\n\nNow that we have our model trained, let's it out for a spin and have our first conversation with it!\n\nIn order to allow us to chitchat with our new bot we need to figure out when the model has finished its turn, i.e. when it has generated the [end_of_turn] token. When the model generates this token, we can switch back control of the conversation to the user so they can respond. Luckily, this is very easy to do with the Huggingface framework!\n\nThe below code is copied pretty much verbatim from the creators of the DialoGPT model, which you can find [here](https://huggingface.co/microsoft/DialoGPT-small).","metadata":{"id":"635gO8WshoyH"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('output')\n\n# Let's chat for 5 lines\nfor step in range(6):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=1000,\n        pad_token_id=tokenizer.eos_token_id,\n        top_p=0.92, top_k = 50\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","metadata":{"scrolled":true,"id":"uieRDgEmhoyH","outputId":"6c70ef84-194b-45eb-911c-6bb46f331089","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, it ain't the best, however, training it for longer or using the DialogGPT-medium instead of DialogGPT-small does improve results, at least in my experiments. I decided to only include the DialogGPT-small in this tutorial due to the limited (But still AMAZING) resources of Google Colab. I've went ahead and trained a bigger DialogGPT-medium model for longer and have uploaded it to Huggingface for anyone to try out! Find the [model card here](https://huggingface.co/ncoop57/DiGPTame-medium).","metadata":{"id":"Mth30hl4jr_I"}},{"cell_type":"markdown","source":"# Conclusion\n\nIn this tutorial, you learned how to train an Open-Dialog chatbot in any language we want to practice with! This involved learning about the amazing [`transformers`](https://huggingface.co/transformers/index.html) library by Huggingface that has seen a lot of popularity recently. You've also learned what an Open-Dialog chatbot is and some of the difficulties that come with training them such as constructing training examples and generating repetitive text.\n\nThis is just part one in what I am hoping will be a three part series! In the next part, we will take our model and integrate it into a web app using the awesome [Streamlit](https://www.streamlit.io/) library. Finally, part three will then be generating an Android application for chatting with your new language companion!","metadata":{"id":"yT53W8PDhoyJ"}},{"cell_type":"markdown","source":"# PS\nIf you do train a new chatbot for a language of your interest, please share it! I'd love to hear about your progress with it and I'm sure others would be also interested in it as these models can be quite expensive to train.\n\nIf you want an ease way to share it, I suggest submitting your trained model to Huggingface's model zoo, where others can view and download your model to use as a starting point for their applications! Here is a simple way for taking the model trained in this tutorial and uploading it to Hugginface's website following the instructions on the Huggingface [website](https://huggingface.co/transformers/model_sharing.html):\n\nFirst make sure you have a Huggingface account: https://huggingface.co/join. Next Run the following code snippets and that's it!","metadata":{"id":"9MzzMyM8hoyK"}},{"cell_type":"code","source":"# First remove checkpoint files from being submit to huggingface\n! rm -rf output/checkpoint-*\n! mv output <name_of_model>","metadata":{"id":"CkAuqtephoyK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! transformers-cli login\n# log in using the same credentials as on huggingface.co","metadata":{"id":"lFNLgj4_hoyO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! transformers-cli upload <name_of_model>","metadata":{"id":"lDCtXkJmhoyQ","trusted":true},"execution_count":null,"outputs":[]}]}