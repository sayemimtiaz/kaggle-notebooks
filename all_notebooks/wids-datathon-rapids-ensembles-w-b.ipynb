{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/EPVALzO.png\">\n\n<h1><center>Predicting Diabetes Mellitus</center></h1>\n\n# 1. Introduction\n\n> For more information about [RAPIDS click here](https://rapids.ai/).\n\n### W&B Setup\n\n1. Create an account on https://wandb.ai (it's free)\n2. Install `wandb`\n3. Input your personal key of the project ( mine will be secret, as it is confidential :) )"},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\nimport os\nos.environ[\"WANDB_SILENT\"] = \"true\"\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\npersonal_key_for_api = user_secrets.get_secret(\"key_for_api\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! wandb login $personal_key_for_api","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Libraries üìö"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CPU Libraries\nimport os\nimport random\nimport warnings\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom fancyimpute import KNN, IterativeImputer, SimpleFill, SoftImpute\\\n                        ,IterativeSVD, MatrixFactorization\\\n                        ,NuclearNormMinimization, BiScaler\n\n# GPU Libraries\nimport cudf\nimport cupy\nfrom cuml.experimental.preprocessing import MinMaxScaler\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nicecream = [\"#CCD4BF\", \"#E7CBA9\", \"#EEBAB2\", \"#F5F3E7\", \"#F5E2E4\"]\nsns.palplot(sns.color_palette(icecream))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Functions Below ‚¨á"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n    \ndef heatmap(x, y, size, color):\n    fig, ax = plt.subplots()\n    \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n        marker='s' # Use square as scatterplot marker\n    )\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    ax.grid(False, 'major')\n    ax.grid(True, 'minor')\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n    \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    \n    n_colors = 256 # Use 256 colors for the diverging color palette\n    palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\n    color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n\n    def value_to_color(val):\n        val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n        ind = int(val_position * (n_colors - 1)) # target index in the color palette\n        return palette[ind]\n\n    ax.scatter(\n        x=x.map(x_to_num),\n        y=y.map(y_to_num),\n        s=size * size_scale,\n        c=color.apply(value_to_color), # Vector of square color values, mapped to color palette\n        marker='s'\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inspect_missing_data(df, data_type=\"Train\"):\n    '''Insect missing patterns from data to assess next steps.'''\n    \n    print(\"--- {} ---\".format(data_type))\n    \n    missing_data = df.isna().sum().reset_index().sort_values(by=0, ascending=False)\n    no_missing = missing_data[missing_data[0] != 0].shape[0]\n    total_cols = df.shape[1]\n    total_rows = df.shape[0]\n    \n    missing_data.columns = [\"name\", \"missing appearences\"]\n    missing_data[\"%missing from total\"] = missing_data[missing_data[\"missing appearences\"]!=0][\"missing appearences\"]/total_rows\n    \n    too_much_miss = missing_data[missing_data[\"%missing from total\"] > 0.5].shape[0]\n    to_drop = missing_data[missing_data[\"%missing from total\"] > 0.5][\"name\"].to_array()\n    \n    print(\"There are {}/{} columns with missing data.\".format(no_missing, total_cols))\n    print(\"There are {}/{} columns with more than 50% missing data (these columns will be dropped)\".format(too_much_miss,\n                                                                                                           no_missing))\n    \n    return missing_data, to_drop\n\n\n\ndef distplot_features(data, feature, categorical=False):\n    '''Takes a column from the GPU dataframe and plots the distribution (after count).\n    data: train + test vector'''\n    \n    values = []\n    for df in data:\n        if categorical:\n            values.append(cupy.asnumpy(df[feature].value_counts().values))\n        else:\n            values.append(cupy.asnumpy(df[feature].values))\n\n    print('Mean : train {:,} | test {:,}'.format(np.mean(values[0]), np.mean(values[1])), \"\\n\"\n      'Median : train {:,} | test {:,}'.format(np.median(values[0]), np.median(values[1])), \"\\n\"\n      'Min: : train {:,} | test {:,}'.format(np.min(values[0]), np.min(values[1])), \"\\n\"\n      'Max: : train {:,} | test {:,}'.format(np.max(values[0]), np.max(values[1])))\n    \n    plt.figure(figsize = (18, 3))\n    \n    fig = sns.distplot(values[0], hist=False, color = icecream[0], kde_kws = {'lw':4}, label=\"Train\")\n    fig = sns.distplot(values[1], hist=False, color = icecream[2], kde_kws = {'lw':4}, label=\"Test\")\n    \n    plt.title(f'{feature}', fontsize=15)\n    plt.legend()\n    plt.show();\n    \n    # Save to W&B\n    wandb.log({feature + \" Distribution\": fig})\n    \n    \ndef to_int_cuda(x):\n    return cupy.int32(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Import Data üìÄ\n\n* `TrainingWiDS2021.csv` - train data.\n* `UnlabeledWiDS2021.csv` - test data; the variable to predict is `diabetes_mellitus` (on encounters).\n* `SolutionTemplateWiDS2021.csv` - a list of all the rows (and encounters) that should be in your submissions.\n* `DataDictionaryWiDS2021.csv` - columns descriptors.\n\n> There are 181 columns in total, with 160/181 containing some sort of missing information. Moreover, 74 out of these 160 columns have missing data more than 50% of the cases. Hence, we'll drop these, as attempting to do any sort of imputation might induce a big bias in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in data\ntrain = cudf.read_csv(\"../input/widsdatathon2021/TrainingWiDS2021.csv\")\ntest = cudf.read_csv(\"../input/widsdatathon2021/UnlabeledWiDS2021.csv\")\ndata = [train, test]\n\nprint(\"Train : {:,} rows and {:,} columns\".format(train.shape[0], train.shape[1]), \"\\n\" + \n      \"Test : {:,} rows and {:,} columns\".format(test.shape[0], test.shape[1]), \"\\n\")\n\n\n# ~~~~~~~~~~~~~~~~~~~~~\n# Drop unwanted columns\n# ~~~~~~~~~~~~~~~~~~~~~\n_, to_drop_train = inspect_missing_data(df=train, data_type=\"Train\")\n_, to_drop_test = inspect_missing_data(df=test, data_type=\"Test\")\n# There is 1 more colum in test but not in train; we'll have to drop that as well in both datasets\ndiff_test = next(iter(set(to_drop_test) - set(to_drop_train)))\nprint(\"! {} has more than 50% missingness in test, but not in train; nevetheless, we'll drop it in both.\".format(diff_test))\n\n# Drop columns with more than 50% missingness\ntrain.drop(labels=to_drop_test, axis=1, inplace=True)\ntest.drop(labels=to_drop_test, axis=1, inplace=True)\n# Drop other unnecessary columns\nto_drop = [\"Unnamed: 0\", \"hospital_id\"]\ntrain.drop(labels=to_drop, axis=1, inplace=True)\ntest.drop(labels=to_drop, axis=1, inplace=True)\n\n# Save up the encounter_id and diabetes_mellitus (these are needed intact and will be appended later)\nencounter_id_train = train[\"encounter_id\"]\nencounter_id_test = test[\"encounter_id\"]\ndiabetes_mellitus_train = train[\"diabetes_mellitus\"]\n\ndiabetes_mellitus_train = cudf.DataFrame(diabetes_mellitus_train, \n                                         columns=[\"diabetes_mellitus\"])\ndiabetes_mellitus_train.to_parquet(\"target_train.parquet\")\n\ntrain.drop(labels=[\"encounter_id\", \"diabetes_mellitus\"], axis=1, inplace=True)\ntest.drop(labels=[\"encounter_id\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Let's save the original datasets!\n### wel'll save them as artifacts:\n### artifacts can be datasets, models and much more!\nrun = wandb.init(project='wids-datathon-kaggle', name='save-original-data')\nartifact = wandb.Artifact(name='original-train-test-data', \n                          type='dataset')\n\nartifact.add_file(\"../input/widsdatathon2021/TrainingWiDS2021.csv\")\nartifact.add_file(\"../input/widsdatathon2021/UnlabeledWiDS2021.csv\")\nartifact.add_file(\"../input/wids-datathon-2021-preprocessed-data/target_train.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Label Encoding\n\n> There are 6 columns that are of type `object`. Hence, before doing any alteration to the dataset, we'll have to encode them (transforming words into encodings/ numbers).\n\n> Difference between LE and OHE:\n<img src=\"https://miro.medium.com/max/2736/0*T5jaa2othYfXZX9W.\" width=400>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encoder_train_test(train, test, method=\"LabelEncoder\", prints=True):\n    '''Encode object columns in Train and Test, leaving NAs to be imputed later.\n    Uses Label Encoder from Sklearn or Dummy Encoding from Pandas.\n    Train and Test need to be Pandas dataframes.\n    Returns the train and test dataframes with encoded column + a list of \n    the columns what have been encoded.'''\n    # Convert to CPU dataframe\n    train = train.to_pandas()\n    test = test.to_pandas()\n    \n    # Select all columns with type == \"object\"\n    train_cols = [col for col in train.columns if train[col].dtype == 'object']\n    test_cols = [col for col in test.columns if test[col].dtype == 'object']\n    string_cols = list(set(train_cols+test_cols))\n    if prints:\n        print(\"Train: {} columns to encode.\".format(len(train_cols)), \"\\n\"\n              \"Test: {} columns to encode.\".format(len(test_cols)), \"\\n\")\n    \n        \n    # --- Label Encoder ---\n    if method == \"LabelEncoder\":\n        \n        for df, cols in zip([train, test], [train_cols, test_cols]):\n            for col in cols:\n                encoder = LabelEncoder()\n\n                ### select all values to encode but NAs (we'll impute these later)\n                fit_by = pd.Series([i for i in df[col].unique() \n                                    if type(i) == str])\n                encoder.fit(fit_by)\n                ### encode the column, leaving NAs untouched\n                df[col] = df[col].apply(lambda x: encoder.transform([x])[0] \n                                        if type(x) == str else x)\n    \n    \n    # --- Dummy Encoder ---          \n    if method == \"Dummy\":\n    \n        # Create Dummy Variables\n        encoded_train = pd.get_dummies(train[train_cols])\n        encoded_test = pd.get_dummies(test[test_cols])\n        \n        # Strip columns of leading/trailing spaces\n        encoded_train.columns = encoded_train.columns.str.strip()\n        encoded_test.columns = encoded_test.columns.str.strip()\n        # Erase train or test columns that might be found in the other df\n        ### this can and it happens: there are categories in train data \n        ### that aren't found in the test data\n        ### nevertheless, they need to be found in both (or not)\n        in_train_not_test = list(set(encoded_train.columns) - set(encoded_test.columns))\n        in_test_not_train = list(set(encoded_test.columns) - set(encoded_train.columns))       \n        \n        # Drop old columns and replace with encoded ones\n        train.drop(columns=train_cols, axis=1, inplace=True)\n        test.drop(columns=test_cols, axis=1, inplace=True)\n\n        train = pd.concat([train, encoded_train], axis=1)\n        test = pd.concat([test, encoded_test], axis=1)\n\n        if in_train_not_test:\n            train.drop(columns=in_train_not_test, axis=1, inplace=True)\n\n        if in_test_not_train:\n            test.drop(columns=in_test_not_train, axis=1, inplace=True)\n        \n        # Get all categ columns again (updated with latest changes)\n        if in_train_not_test:\n            new_train_cols = list(set(encoded_train.columns) - set(in_train_not_test))\n        else:\n            new_train_cols = list(encoded_train.columns)\n        if in_test_not_train:\n            new_test_cols = list(set(encoded_test.columns) - set(in_test_not_train))\n        else:\n            new_test_cols = list(encoded_test.columns)\n        \n        string_cols = list(set(new_train_cols + new_test_cols))\n    \n    \n    # Convert back to GPU dataframe\n    train = cudf.DataFrame.from_pandas(train)\n    test = cudf.DataFrame.from_pandas(test)\n            \n    print(\"Encoding finished.\")\n    # Categ columns are needed to be properly shifted to int32 later\n    return train, test, string_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain_le, test_le, string_cols_le = encoder_train_test(train=train, \n                                                       test=test,\n                                                       method=\"LabelEncoder\")\n# OHE doesn't perform better\n# train_ohe, test_ohe, string_cols_ohe = encoder_train_test(train=train, \n#                                                           test=test,\n#                                                           method=\"Dummy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Important TRAIN vs TEST differences üîé\n\n* `encounter_id` - unique on each row (it is used in submission, but NOT during training)\n* `hospital_id` - completely different between train and test (columns needs to be dropped)\n* `age` - impute NAs; distributions are fairly similar, but with more senior patients in TEST set\n* `bmi` & `weight` - fairly similar distributions\n* `height` & `age` - little fluctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize new experiment\nrun = wandb.init(project=\"wids-datathon-kaggle\", name=\"train-test-diff\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas = [train[train[\"age\"].isna() == False], test[test[\"age\"].isna() == False]]\ndistplot_features(data=datas, feature=\"age\", categorical=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas = [train[train[\"height\"].isna() == False], test[test[\"height\"].isna() == False]]\ndistplot_features(data=datas, feature=\"height\", categorical=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We've logged in the plots, so now we can finish this run (or experiment)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finish this experiment\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation ‚öõ\n\n> There are many variables that are highly correlated between themselves. Moreover, the number of features in the dataset is quite big itself (~100 feaures). A PCA might be of help in here and might even drive the ROC curve value up."},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_network(data, corr_interval=[-0.8, 0.8], data_type=\"Train\",\n                        node_color=icecream[1]):\n    '''Returns the correlation Matrix between the variables with highest correlation coefficient.'''\n    \n    # .corr() in cudf's version doesn't support NAs, so we'll use the Pandas version\n    data = data.to_pandas()\n    corr = data.corr()\n    corr = pd.melt(corr.reset_index(), id_vars='index')\n    corr.columns = ['x', 'y', 'value']\n\n    # Filter only high correlations\n    high_corr = corr[((corr['value'] <= corr_interval[0]) | (corr['value'] >= corr_interval[1]))]\n    high_corr = high_corr[(high_corr['value'] != 1)].reset_index(drop=True)\n    \n    #~~~~~~~~~~~~~~~~~\n    #    The Graph\n    #~~~~~~~~~~~~~~~~~\n    \n    sources = list(high_corr.x.unique())\n    targets = list(high_corr.y.unique())\n    \n\n    plt.figure(figsize=(16, 12))\n    # GRAPH OBJECT\n    ### easier shorter way to create the nodes & edges object\n    g = nx.from_pandas_edgelist(high_corr, source='x', target='y') \n\n    # Layout\n    layout = nx.spring_layout(g, iterations=50, k=0.6, seed=seed)\n    # A list of sizes, based on g.degree\n    target_size = [g.degree(t) * 80 for t in targets]\n\n    nx.draw_networkx_nodes(g, \n                           layout, \n                           nodelist=targets, \n                           node_size=target_size, \n                           node_color=node_color)\n    # Draw every connection\n    nx.draw_networkx_nodes(g, layout, nodelist=sources, node_color=node_color, node_size=800, alpha=0.5)\n\n    nx.draw_networkx_edges(g, layout, width=1, edge_color=icecream[1])\n\n    target_dict = dict(zip(targets, targets))\n    nx.draw_networkx_labels(g, layout, labels=target_dict)\n\n    plt.axis('off')\n    plt.title(f\"{data_type}: Highest Correlations Network\", fontsize=25)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_network(data=train, corr_interval=[-0.8, 0.8], data_type=\"Train\", node_color=icecream[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_network(data=test, corr_interval=[-0.8, 0.8], data_type=\"Test\", node_color=icecream[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Imputation üóÑ\n\n> `fancyimpute` is a library for missing data imputation algorithms, which uses machine learning algorithms to impute missing values. There are [many algorithms](https://pypi.org/project/fancyimpute/) that can be tried out:\n* **KNN**: Nearest neighbor imputations which weights samples using the mean squared difference on features for which two rows both have observed data.\n* **IterativeImputer**: A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n* **SoftImpute**: Matrix completion by iterative soft thresholding of SVD decompositions.\n* etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fancy_imputer_train_test(train, test, string_cols, impute_type=\"mice\"):\n    '''Imputes the train and test datasets and returns the result.\n    string_cols: columns encoded with the label encoder (to be converted as well as int)\n    impute_type: could be 'mice' or 'knn' '''\n    # Convert to CPU dataframe\n    train = train.to_pandas()\n    test = test.to_pandas()\n    \n    print(\"Imputation has started...\")\n    if impute_type == \"mice\":\n        mice_imputer = IterativeImputer()\n        train_imputed = mice_imputer.fit_transform(train)\n        test_imputed = mice_imputer.transform(test)\n        \n        print(\"Data Cleanup has started...\")\n        train_finished = adjust_imputed_data(imputed_data=train_imputed, column_names=train.columns,\n                                             string_cols=string_cols)\n        test_finished = adjust_imputed_data(imputed_data=test_imputed, column_names=test.columns,\n                                             string_cols=string_cols)\n        \n    \n    elif impute_type == \"soft\":\n        imputer = SoftImpute()\n        train_imputed = imputer.fit_transform(train)\n        test_imputed = imputer.fit_transform(test)\n        \n        print(\"Data Cleanup has started...\")\n        train_finished = adjust_imputed_data(imputed_data=train_imputed, column_names=train.columns,\n                                             string_cols=string_cols)\n        test_finished = adjust_imputed_data(imputed_data=test_imputed, column_names=test.columns,\n                                             string_cols=string_cols)\n        \n    elif impute_type == \"svd\":\n        imputer = IterativeSVD(verbose=False)\n        train_imputed = imputer.fit_transform(train)\n        test_imputed = imputer.fit_transform(test)\n        \n        print(\"Data Cleanup has started...\")\n        train_finished = adjust_imputed_data(imputed_data=train_imputed, column_names=train.columns,\n                                             string_cols=string_cols)\n        test_finished = adjust_imputed_data(imputed_data=test_imputed, column_names=test.columns,\n                                             string_cols=string_cols)\n     \n    # No GPU conversion needed, as they are already GPU dataframes from adjust_imputed_data() function\n    print(\"Imputation has finished.\")\n    return train_finished, test_finished\n\n\n\ndef adjust_imputed_data(imputed_data, column_names, string_cols):\n    '''Adjusts the output of the imputed data.\n    The input is a CPU df and the output is a GPU df.\n    Also transforms all initial categorical columns to int64.'''\n    data = cudf.DataFrame(imputed_data)\n    data.columns = column_names\n\n#     # Transform to int all columns that were int/string in the beginning\n#     int64_cols = [col for col in data.columns if data[col].dtype == \"int64\"]\n#     int64_cols.extend(string_cols)\n#     for col in int64_cols:\n#         data[col] = data[col].applymap(to_int_cuda)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the purpose of this notebook running faster, I'll use only\n# a fraction of the total data\n# to preprocess all, set n = len(train_le) OR n = len(test_le)\n\ntrain_le_sample = train_le.sample(n=100)\ntest_le_sample = test_le.sample(n=100)\n\n%time\ntrain_le_mice, test_le_mice = fancy_imputer_train_test(train=train_le_sample, \n                                                       test=test_le_sample,\n                                                       string_cols=string_cols_le,\n                                                       impute_type=\"mice\")\n\n# You can try using OHE instead of LE and other imputation methods ;)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> You can see the saved artifacts [here](https://wandb.ai/andrada/wids-datathon-kaggle/artifacts)."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Save to W&B as artifacts\nrun = wandb.init(project='wids-datathon-kaggle', name='le+mice_process')\nartifact = wandb.Artifact(name='preprocessed', \n                          type='dataset')\n\nartifact.add_file(\"../input/wids-datathon-2021-preprocessed-data/train_le_mice.parquet\")\nartifact.add_file(\"../input/wids-datathon-2021-preprocessed-data/test_le_mice.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Scaling ‚öñÔ∏è\n\n> Scaling makes all data uniform (and helps model training)\n<img src=\"https://miro.medium.com/max/3316/1*yR54MSI1jjnf2QeGtt57PA.png\" width=300>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(train, test):\n    '''Scales the data using MinMaxScaler from the cuml library.\n    Returns the 2 scaled train & test dataframes.'''\n\n    scaler = MinMaxScaler()\n\n    new_train = scaler.fit_transform(train)\n    new_test = scaler.transform(test)\n\n    new_train.columns = train.columns\n    new_test.columns = test.columns\n    \n    print(\"Scaling has finished.\")\n    return new_train, new_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_le_mice_scaled, test_le_mice_scaled = scale_data(train=train_le_mice, \n                                                       test=test_le_mice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Save data üìÄ\n\n> The last step in the preprocessing part is to save the newly processed data. We'll save the data as `.parquet`, as it reads 6 times faster than a `.csv` would.\n\n> You can see the saved artifacts [here](https://wandb.ai/andrada/wids-datathon-kaggle/artifacts)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Add encounter_id and diabetes_mellitus back to the dataframes\ntrain_le_mice_scaled[\"encounter_id\"] = encounter_id_train\ntest_le_mice_scaled[\"encounter_id\"] = encounter_id_test\n\ntrain_le_mice_scaled[\"diabetes_mellitus\"] = diabetes_mellitus_train\n\n# Save data to .parquet (6x faster than .csv)\ntrain_le_mice_scaled.to_parquet('train_le_mice_scaled.parquet')\ntest_le_mice_scaled.to_parquet('test_le_mice_scaled.parquet')\n\n# Save to W&B as artifacts\nrun = wandb.init(project='wids-datathon-kaggle', name='le+mice_scale_process')\nartifact = wandb.Artifact(name='preprocessed', \n                          type='dataset')\n\nartifact.add_file(\"../input/wids-datathon-2021-preprocessed-data/train_le_mice_scaled.parquet\")\nartifact.add_file(\"../input/wids-datathon-2021-preprocessed-data/test_le_mice_scaled.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> My Artifacts section after the runs looks like this:\n<img src=\"https://i.imgur.com/K17RgUy.png\" width=600>\n\n<hr style=\"height:3px;border:none;color:#CCD4BF;background-color:#CCD4BF;\" />\n<center><h1>- Training the Model -</h1></center><hr style=\"height:3px;border:none;color:#CCD4BF;background-color:#CCD4BF;\" />\n\n# 1. Libraries and Data Import\n\n### Libraries üìö"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GPU Libraries\nimport cuml\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing.model_selection import train_test_split\nimport xgboost\n# Important\nfrom wandb.xgboost import wandb_callback\n\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Import ‚á¢"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data\ntrain = cudf.read_parquet(\"../input/wids-datathon-2021-preprocessed-data/train_le_mice.parquet\")\ntest = cudf.read_parquet(\"../input/wids-datathon-2021-preprocessed-data/test_le_mice.parquet\")\ntarget_column = cudf.read_parquet(\"../input/wids-datathon-2021-preprocessed-data/target_train.parquet\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions üóÇ"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission(predictions, file_name):\n    '''Creates a .csv submission file.'''\n\n    # Create submission\n    sample_submission = cudf.read_csv(\"../input/widsdatathon2021/UnlabeledWiDS2021.csv\")\n    IDs = sample_submission[\"encounter_id\"]\n\n    to_submit = {'encounter_id': IDs, 'diabetes_mellitus': predictions}\n    df_to_submit = cudf.DataFrame(to_submit).set_index(['encounter_id'])\n    \n    df_to_submit.to_csv(f\"{file_name}.csv\")\n    print(\"Submission ready.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. XGBoost ‚è≥"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_xgb_model(X_train, X_test, y_train, y_test, params, \n                    details=\"default\", prints=True, step=1):\n    \n    '''Trains an XGB and returns the trained model + ROC value.'''\n    run = wandb.init(project='wids-datathon-kaggle', name=f'xgboost_run_{step}',\n                     config=params)\n    wandb.log(params)\n    \n    # Create DMatrix - is optimized for both memory efficiency and training speed.\n    train_matrix = xgboost.DMatrix(data = X_train, label = y_train)\n    \n    # Create & Train the model\n    model = xgboost.train(params, dtrain = train_matrix,\n                          callbacks=[wandb_callback()],\n                         )\n\n    # Make prediction\n    predicts = model.predict(xgboost.DMatrix(X_test))\n    roc = roc_auc_score(y_test.astype('int32'), predicts)\n    wandb.log({'roc':roc})\n\n    if prints:\n        print(details + \" - ROC: {:.5}\".format(roc))\n    \n    wandb.finish()\n    return model, roc\n\n\ndef train_xgb_entire_data(X, y, params):\n    '''Use this when you are ready to submit.\n    Hence, you can train your tuned model 1 more time, but on the entire available labeled data.\n    Returns the trained model.'''\n    run = wandb.init(project='wids-datathon-kaggle', name='xgboost_run_entire_data',\n                     config=params)\n    wandb.log(params)\n    \n    # Create DMatrix - is optimized for both memory efficiency and training speed.\n    train_matrix = xgboost.DMatrix(data = X, label = y)\n    \n    # Create & Train the model\n    model = xgboost.train(params, dtrain = train_matrix,\n                          callbacks=[wandb_callback()],\n                         )\n    \n    return model, run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Establish X and y\ny = target_column\nX = train\n\n# Simple split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    shuffle=False, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"params1 = {\n    'max_depth' : 4,\n    'max_leaves' : 2**4,\n    'tree_method' : 'gpu_hist',\n    'objective' : 'reg:logistic',\n    'grow_policy' : 'lossguide',\n    'colsample_bynode': 0.8,\n}\n\nmodel1, roc1 = train_xgb_model(X_train, X_test, y_train, y_test, \n                               params1, details=\"Baseline Model\", step=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> After running this cell, a new Experiment should appear in you W&B dashboard. To see full dashboard, [click here](https://wandb.ai/andrada/wids-datathon-kaggle?workspace=user-andrada).\n<img src=\"https://i.imgur.com/wi5aIxZ.png\" width=900>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~~~~~~~~~\n#   TUNED\n# ~~~~~~~~~\n\nparams_final = {\n    'max_depth' : 6,\n    'max_leaves' : 14,\n    'tree_method' : 'gpu_hist',\n    'objective' : 'reg:logistic',\n    'grow_policy' : 'lossguide',\n    'eta' : 0.6,\n}\n\n_, _ = train_xgb_model(X_train, X_test, y_train, y_test, \n                               params_final, details=\"Final Model\", step=2)\n\n# Train on the entire dataset\nxgb_final_model, run = train_xgb_entire_data(X, y, params=params_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> After parameter tunning, 2 more experiments should appear in the main panel. You can compare hyperparameters and ROC values between the 3 models (`xgboost_run_1`, `xgboost_run_2` and `xgboost_run_entire_data`). To see full dashboard, [click here](https://wandb.ai/andrada/wids-datathon-kaggle?workspace=user-andrada).\n<img src=\"https://i.imgur.com/DD8QVIt.png\" width=800> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save final model\npickle.dump(xgb_final_model, open(\"xgb_le_mice.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Save Artifact\n# run = wandb.init(project='wids-datathon-kaggle', name='save_xgb_model')\n# artifact = wandb.Artifact(name='XGBoost-models', \n#                           type='model')\n# artifact.add_dir(\"./xgb_le_mice.pickle.dat\")\n# run.log_artifact(artifact)\n# wandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Submission üì§"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\nxgb_final_preds = xgb_final_model.predict(xgboost.DMatrix(test))\n\nmake_submission(predictions=xgb_final_preds, file_name=\"xgb_le_mice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance üìä"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(xgb_final_model.get_fscore().items(), \n                           columns=['feature','importance']).\\\n                            sort_values('importance', ascending=False)\nfeature_imp = feature_imp.reset_index(drop=True).head(10)\n\nplt.figure(figsize=(16, 9))\nax = sns.barplot(data=feature_imp, x='importance', y='feature',\n                 palette=\"spring_r\")\nax.tick_params(axis='both', which='both', labelsize=14)\nax.tick_params(axis='both', which='both', labelsize=14)\nax.set_xlabel('Importance',fontsize=15, weight=\"bold\")\nax.set_ylabel('Feature',fontsize=15,weight=\"bold\")\nplt.title(\"XGBoost - Feature Importance\", size=20, weight=\"bold\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Light GBM ‚è≥"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn import metrics\nimport lightgbm as lgbm\nfrom wandb.lightgbm import wandb_callback","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables\nfeatures = train.columns.tolist()\ntarget = \"diabetes_mellitus\"\n\ntrain[\"diabetes_mellitus\"] = target_column\n\n# Train & Test\ntrain_lgbm, test_lgbm = train_test_split(train, test_size=0.05, shuffle=False)\n\ntrain_lgbm = train_lgbm.to_pandas()\ntest_lgbm = test_lgbm.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light GBM Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_lgbm(train_lgbm, test_lgbm, features, target, param,\n                  n_splits=5, stop_round=100, num_rounds=1000, verbose=False, \n                  tuned=\"None\", val=None, return_model=False, step=1):\n    \n    '''Trains LGBM model.'''\n    run = wandb.init(project='wids-datathon-kaggle', name=f'lgbm_run_{step}',\n                     config=param)\n    wandb.log(param)\n\n    # ~~~~~~~\n    #  KFOLD\n    # ~~~~~~~\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n\n    oof = np.zeros(len(train_lgbm))\n    predictions = np.zeros(len(test_lgbm))\n\n    # Convert Train to Train & Validation\n    skf_split = skf.split(X=train_lgbm[features], y=train_lgbm[target].values)\n    \n    # ~~~~~~~\n    #  TRAIN\n    # ~~~~~~~\n    counter = 1\n\n    for train_index, valid_index in skf_split:\n        print(\"==== Fold {} ====\".format(counter))\n\n        lgbm_train = lgbm.Dataset(data = train_lgbm.iloc[train_index, :][features].values,\n                                  label = train_lgbm.iloc[train_index, :][target].values,\n                                  feature_name = features,\n                                  free_raw_data = False)\n\n        lgbm_valid = lgbm.Dataset(data = train_lgbm.iloc[valid_index, :][features].values,\n                                  label = train_lgbm.iloc[valid_index, :][target].values,\n                                  feature_name = features,\n                                  free_raw_data = False)\n\n        lgbm_1 = lgbm.train(params = param, train_set = lgbm_train, valid_sets = [lgbm_valid],\n                            early_stopping_rounds = stop_round, num_boost_round=num_rounds, \n                            verbose_eval=verbose, callbacks=[wandb_callback()])\n\n\n        # X_valid to predict\n        oof[valid_index] = lgbm_1.predict(train_lgbm.iloc[valid_index][features].values, \n                                          num_iteration = lgbm_1.best_iteration)\n        predictions += lgbm_1.predict(test_lgbm[features], \n                                      num_iteration = lgbm_1.best_iteration) / n_splits\n\n        counter += 1\n        \n        \n    # ~~~~~~~~~~~\n    #   OOF EVAL\n    # ~~~~~~~~~~~\n    print(\"============================================\")\n    print(\"Splits: {} | Stop Round: {} | No. Rounds: {} | {}: {}\".format(n_splits, stop_round, \n                                                                            num_rounds, tuned, val))\n    print(\"CV ROC: {:<0.5f}\".format(metrics.roc_auc_score(test_lgbm[target], predictions)))\n    print(\"\\n\")\n    wandb.log({'oof_roc': metrics.roc_auc_score(test_lgbm[target], predictions)})\n    wandb.finish()\n    \n    if return_model:\n        return lgbm_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model (After Tuning)\n\n> I've done my tunning on my personal Z8 Desktop and here are the results after that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_params = {'learning_rate': 0.1,\n               'objective': 'binary',\n               'metric': 'auc',\n               'boosting_type': 'gbdt',\n               'max_depth': 6,\n\n               'subsample': 0.2,\n               'colsample_bytree': 0.3,\n               'reg_alpha': 0.54,\n               'reg_lambda': 0.4,\n               'min_split_gain': 0.7,\n               'min_child_weight': 26,\n               'nthread':-1,\n\n               'save_binary': True,\n               'seed': 1337, 'feature_fraction_seed': 1337,\n               'bagging_seed': 1337, 'drop_seed': 1337, \n               'data_random_seed': 1337,\n               'verbose': -1, \n               'is_unbalance': True,\n}\n\ntraining_lgbm(train_lgbm, test_lgbm, features, target, param=final_params,\n              n_splits=5, stop_round=100, num_rounds=1000, verbose=200,\n              step=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light GBM Final Model & Submission üì§"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on more data\ntrain_lgbm, test_lgbm = train_test_split(train, test_size=0.05, shuffle=False)\n\ntrain_lgbm = train_lgbm.to_pandas()\ntest_lgbm = test_lgbm.to_pandas()\n\nfinal_lgbm = training_lgbm(train_lgbm, test_lgbm, features, target, \n                           param=final_params, n_splits=5, stop_round=100, \n                           num_rounds=1000, verbose=400, return_model=True, step=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Again, let's observe our 2 experiments in the W&B dashboard. To see full dashboard, [click here](https://wandb.ai/andrada/wids-datathon-kaggle?workspace=user-andrada).\n<img src=\"https://i.imgur.com/O1YbAHr.png\" width=800>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(final_lgbm, open(\"lgbm_le_mice.pickle.dat\", \"wb\"))\n\n# Predict test\nlgbm_final_preds = final_lgbm.predict(test.to_pandas(), num_iteration = final_lgbm.best_iteration)\n\nmake_submission(lgbm_final_preds, file_name=\"lgbm_le_mice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. SVR ‚è≥"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GPU Libraries\nimport cuml\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing.model_selection import train_test_split\n\nimport pickle\n\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.svm import SVC, SVR\nfrom cuml.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom cuml.linear_model import MBSGDClassifier as cumlMBSGDClassifier\nfrom cuml.naive_bayes import MultinomialNB\n\n\n# --- Functions ---\ndef train_model(model, X_train, y_train, X_test, y_test, name=\"default\"):\n    model.fit(X_train, y_train)\n    predicts = model.predict(X_test)\n    \n    roc = roc_auc_score(y_test, predicts)\n    print(\"Model: {} | ROC: {}\".format(name, roc))\n    \n    \ndef train_entire_data(model, X, y):\n    model.fit(X, y)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Establish X and y\ny = target_column\nX = train.drop(labels=[\"diabetes_mellitus\"], axis=1)\n\n# Simple split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    shuffle=False, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating and fitting the model\nsvr_tuned = SVR(kernel='rbf', gamma='scale', C=1, epsilon=0.3)\ntrain_model(model=svr_tuned, X_train=X_train, \n            y_train=y_train, X_test=X_test, \n            y_test=y_test, name=\"SVR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the entire dataset\nsvr_final = SVR(kernel='rbf', gamma='scale', C=1, epsilon=0.3)\nsvr_final = train_entire_data(model=svr_final, X=X, y=y)\n\n# Save final model\npickle.dump(svr_final, open(\"svr_le_mice.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission üì§"},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_predictions = svr_final.predict(test)\nmake_submission(predictions=svr_predictions, file_name=\"svr_le_mice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Blend the models üî¨"},{"metadata":{"trusted":true},"cell_type":"code","source":"blended_preds = 0.1*xgb_final_preds + 0.9*lgbm_final_preds\nmake_submission(blended_preds, file_name=\"blend_xgb_lgbm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **‚ùóÔ∏èNote‚ùóÔ∏è**: I've made many more experiments on my local machine, playing around with types of encoders, imputations, models and blends. So far, my best score was a combination of **LE + SVD imputation + multiple tunned LGBMs**."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_blend = pd.read_csv(\"../input/wids-datathon-2021-preprocessed-data/blend05.csv\")\nbest_blend.to_csv(\"best_blend.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/cUQXtS7.png\">\n\n# Specs on how I prepped & trained ‚å®Ô∏èüé®\n### *(on my local machine)*\n* Z8 G4 Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA Quadro RTX 8000 üéÆ\n* RAPIDS version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}