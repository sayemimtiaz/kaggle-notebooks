{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Extractive Summarization using Text Rank Algorithm\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing spacy library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Input files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='iso-8859-1')\n#View labels of input\nsummary.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary['ctext']\n#viewing the contents inside summary's 5th element","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.iloc[286,5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=(len(summary))\nprint(count)\nrange(count-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import data or fill text","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#!/usr/bin/env python\n# encoding: utf-8\n\n#PyTextRank Algorithm\n\nfrom collections import defaultdict, OrderedDict\nfrom math import sqrt\nfrom operator import itemgetter\nfrom spacy.tokens import Doc\nimport graphviz\nimport json\nimport logging\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport spacy\nimport string\nimport sys\nimport time\nimport unicodedata\n\n\n######################################################################\n## utility functions\n######################################################################\n\nPAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\nPAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\nPAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n\n\ndef split_grafs (lines):\n    \"\"\"\n    segment raw text, given as a list of lines, into paragraphs\n    \"\"\"\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield \"\\n\".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield \"\\n\".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    \"\"\"\n    filter the quoted text out of a message\n    \"\"\"\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, re.M)\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(\"\\n\"):\n        if line.startswith(\">\"):\n            lines.append(\"\")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\ndef maniacal_scrubber (text):\n    \"\"\"\n    it scrubs the garble from its stream...\n    or it gets the debugger again\n    \"\"\"\n    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n\n    x = x.replace('“', '\"').replace('”', '\"')\n    x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\").replace(\"`\", \"'\")\n    x = x.replace(\"…\", \"...\").replace(\"–\", \"-\")\n\n    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n\n    # some web content returns \"not string\" ?? ostensibly no longer\n    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n    # have been found in the wild -- YMMV\n\n    try:\n        assert type(x).__name__ == \"str\"\n    except AssertionError:\n        print(\"not a string?\", type(line), line)\n\n    return x\n\n\ndef default_scrubber (text):\n    \"\"\"\n    remove spurious punctuation (for English)\n    \"\"\"\n    return text.lower().replace(\"'\", \"\")\n\n\n######################################################################\n## class definitions\n######################################################################\n\nclass CollectedPhrase:\n    \"\"\"\n    represents one phrase during the collection process\n    \"\"\"\n\n    def __init__ (self, chunk, scrubber):\n        self.sq_sum_rank = 0.0\n        self.non_lemma = 0\n        \n        self.chunk = chunk\n        self.text = scrubber(chunk.text)\n\n\n    def __repr__ (self):\n        return \"{:.4f} ({},{}) {} {}\".format(\n            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n        )\n\n\n    def range (self):\n        \"\"\"\n        generate the index range for the span of tokens in this phrase\n        \"\"\"\n        return range(self.chunk.start, self.chunk.end)\n\n\n    def set_key (self, compound_key):\n        \"\"\"\n        create a unique key for the the phrase based on its lemma components\n        \"\"\"\n        self.key = tuple(sorted(list(compound_key)))\n\n\n    def calc_rank (self):\n        \"\"\"\n        since noun chunking is greedy, we normalize the rank values\n        using a point estimate based on the number of non-lemma\n        tokens within the phrase\n        \"\"\"\n        chunk_len = self.chunk.end - self.chunk.start + 1\n        non_lemma_discount = chunk_len / (chunk_len + (2.0 * self.non_lemma) + 1.0)\n\n        # normalize the contributions of all the kept lemma tokens\n        # within the phrase using root mean square (RMS)\n\n        self.rank = sqrt(self.sq_sum_rank / (chunk_len + self.non_lemma)) * non_lemma_discount\n\n\nclass Phrase:\n    \"\"\"\n    represents one extracted phrase\n    \"\"\"\n\n    def __init__ (self, text, rank, count, phrase_list):\n        self.text = text\n        self.rank = rank\n        self.count = count\n        self.chunks = [p.chunk for p in phrase_list]\n\n\n    def __repr__ (self):\n        return self.text\n\n\nclass TextRank:\n    \"\"\"\n    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n    used to extract the top-ranked phrases from a text document\n    \"\"\"\n    _EDGE_WEIGHT = 1.0\n    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n    _TOKEN_LOOKBACK = 3\n    \n\n    def __init__ (\n            self,\n            edge_weight=_EDGE_WEIGHT,\n            logger=None,\n            pos_kept=_POS_KEPT,\n            scrubber=default_scrubber,\n            token_lookback=_TOKEN_LOOKBACK\n    ):\n        self.edge_weight = edge_weight\n        self.logger = logger\n        self.pos_kept = pos_kept\n        self.scrubber = scrubber\n        self.stopwords = defaultdict(list)\n        self.token_lookback = token_lookback\n\n        self.doc = None\n        self.reset()\n\n\n    def reset (self):\n        \"\"\"\n        initialize the data structures needed for extracting phrases\n        removing any state\n        \"\"\"\n        self.elapsed_time = 0.0\n        self.lemma_graph = nx.Graph()\n        self.phrases = defaultdict(list)\n        self.ranks = {}\n        self.seen_lemma = OrderedDict()\n\n\n    def load_stopwords (self, path=\"stop.json\"):\n        \"\"\"\n        load a list of \"stop words\" that get ignored when constructing\n        the lemma graph -- NB: be cautious when using this feature\n        \"\"\"\n        stop_path = None\n\n        # check if the path is fully qualified, or if the file is in\n        # the current working directory\n\n        if os.path.isfile(path):\n            stop_path = path\n        else:\n            cwd = os.getcwd()\n            stop_path = os.path.join(cwd, path)\n\n            if not os.path.isfile(stop_path):\n                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n                stop_path = os.path.join(loc, path)\n\n        try:\n            with open(stop_path, \"r\") as f:\n                data = json.load(f)\n\n                for lemma, pos_list in data.items():\n                    self.stopwords[lemma] = pos_list\n        except FileNotFoundError:\n            pass\n\n\n    def increment_edge (self, node0, node1):\n        \"\"\"\n        increment the weight for an edge between the two given nodes,\n        creating the edge first if needed\n        \"\"\"\n        if self.logger:\n            self.logger.debug(\"link {} {}\".format(node0, node1))\n    \n        if self.lemma_graph.has_edge(node0, node1):\n            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n        else:\n            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n\n\n    def link_sentence (self, sent):\n        \"\"\"\n        link nodes and edges into the lemma graph for one parsed sentence\n        \"\"\"\n        visited_tokens = []\n        visited_nodes = []\n\n        for i in range(sent.start, sent.end):\n            token = self.doc[i]\n\n            if token.pos_ in self.pos_kept:\n                # skip any stop words...\n                lemma = token.lemma_.lower().strip()\n\n                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n                    continue\n\n                # ...otherwise proceed\n                key = (token.lemma_, token.pos_)\n\n                if key not in self.seen_lemma:\n                    self.seen_lemma[key] = set([token.i])\n                else:\n                    self.seen_lemma[key].add(token.i)\n\n                node_id = list(self.seen_lemma.keys()).index(key)\n\n                if not node_id in self.lemma_graph:\n                    self.lemma_graph.add_node(node_id)\n\n                if self.logger:\n                    self.logger.debug(\"visit {} {}\".format(\n                        visited_tokens, visited_nodes\n                    ))\n                    self.logger.debug(\"range {}\".format(\n                        list(range(len(visited_tokens) - 1, -1, -1))\n                    ))\n            \n                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                    if self.logger:\n                        self.logger.debug(\"prev_tok {} {}\".format(\n                            prev_token, (token.i - visited_tokens[prev_token])\n                        ))\n                \n                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n                        self.increment_edge(node_id, visited_nodes[prev_token])\n                    else:\n                        break\n\n                if self.logger:\n                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n                    ))\n\n                visited_tokens.append(token.i)\n                visited_nodes.append(node_id)\n\n\n    def collect_phrases (self, chunk):\n        \"\"\"\n        collect instances of phrases from the lemma graph\n        based on the given chunk\n        \"\"\"\n        phrase = CollectedPhrase(chunk, self.scrubber)\n        compound_key = set([])\n\n        for i in phrase.range():\n            token = self.doc[i]\n            key = (token.lemma_, token.pos_)\n        \n            if key in self.seen_lemma:\n                node_id = list(self.seen_lemma.keys()).index(key)\n                rank = self.ranks[node_id]\n                phrase.sq_sum_rank += rank\n                compound_key.add(key)\n        \n                if self.logger:\n                    self.logger.debug(\" {} {} {} {}\".format(\n                        token.lemma_, token.pos_, node_id, rank\n                    ))\n            else:\n                phrase.non_lemma += 1\n    \n        phrase.set_key(compound_key)\n        phrase.calc_rank()\n\n        self.phrases[phrase.key].append(phrase)\n\n        if self.logger:\n            self.logger.debug(phrase)\n\n\n    def calc_textrank (self):\n        \"\"\"\n        iterate through each sentence in the doc, constructing a lemma graph\n        then returning the top-ranked phrases\n        \"\"\"\n        self.reset()\n        t0 = time.time()\n\n        for sent in self.doc.sents:\n            self.link_sentence(sent)\n\n        if self.logger:\n            self.logger.debug(self.seen_lemma)\n\n        # to run the algorithm, we use PageRank – i.e., approximating\n        # eigenvalue centrality – to calculate ranks for each of the\n        # nodes in the lemma graph\n\n        self.ranks = nx.pagerank(self.lemma_graph)\n\n        # collect the top-ranked phrases based on both the noun chunks\n        # and the named entities\n\n        for chunk in self.doc.noun_chunks:\n            self.collect_phrases(chunk)\n\n        for ent in self.doc.ents:\n            self.collect_phrases(ent)\n\n        # since noun chunks can be expressed in different ways (e.g., may\n        # have articles or prepositions), we need to find a minimum span\n        # for each phrase based on combinations of lemmas\n\n        min_phrases = {}\n\n        for phrase_key, phrase_list in self.phrases.items():\n            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n            best_phrase = phrase_list[0]\n            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n\n        # yield results\n\n        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n\n        phrase_list = [\n            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n        ]\n\n        t1 = time.time()\n        self.elapsed_time = (t1 - t0) * 1000.0\n\n        return phrase_list\n\n\n    def write_dot (self, path=\"graph.dot\"):\n        \"\"\"\n        output the lemma graph in Dot file format\n        \"\"\"\n        keys = list(self.seen_lemma.keys())\n        dot = graphviz.Digraph()\n\n        for node_id in self.lemma_graph.nodes():\n            text = keys[node_id][0].lower()\n            rank = self.ranks[node_id]\n            label = \"{} ({:.4f})\".format(text, rank)\n            dot.node(str(node_id), label)\n\n        for edge in self.lemma_graph.edges():\n            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n\n        with open(path, \"w\") as f:\n            f.write(dot.source)\n\n\n    def summary (self, limit_phrases=10, limit_sentences=4):\n        \"\"\"\n        run extractive summarization, based on vector distance \n        per sentence from the top-ranked phrases\n        \"\"\"\n        unit_vector = []\n\n        # construct a list of sentence boundaries with a phrase set\n        # for each (initialized to empty)\n\n        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n\n        # iterate through the top-ranked phrases, added them to the\n        # phrase vector for each sentence\n\n        phrase_id = 0\n\n        for p in self.doc._.phrases:\n            unit_vector.append(p.rank)\n\n            if self.logger:\n                self.logger.debug(\n                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n                )\n    \n            for chunk in p.chunks:\n                for sent_start, sent_end, sent_vector in sent_bounds:\n                    if chunk.start >= sent_start and chunk.start <= sent_end:\n                        sent_vector.add(phrase_id)\n\n                        if self.logger:\n                            self.logger.debug(\n                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n                                )\n\n                        break\n\n            phrase_id += 1\n\n            if phrase_id == limit_phrases:\n                break\n\n        # construct a unit_vector for the top-ranked phrases, up to\n        # the requested limit\n\n        sum_ranks = sum(unit_vector)\n        unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n\n        # iterate through each sentence, calculating its euclidean\n        # distance from the unit vector\n\n        sent_rank = {}\n        sent_id = 0\n\n        for sent_start, sent_end, sent_vector in sent_bounds:\n            sum_sq = 0.0\n    \n            for phrase_id in range(len(unit_vector)):\n                if phrase_id not in sent_vector:\n                    sum_sq += unit_vector[phrase_id]**2.0\n\n            sent_rank[sent_id] = sqrt(sum_sq)\n            sent_id += 1\n\n        # extract the sentences with the lowest distance\n\n        sent_text = {}\n        sent_id = 0\n\n        for sent in self.doc.sents:\n            sent_text[sent_id] = sent\n            sent_id += 1\n\n        # yield results, up to the limit requested\n\n        num_sent = 0\n\n        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n            yield sent_text[sent_id]\n            num_sent += 1\n\n            if num_sent == limit_sentences:\n                break\n\n\n    def PipelineComponent (self, doc):\n        \"\"\"\n        define a custom pipeline component for spaCy and extend the\n        Doc class to add TextRank\n        \"\"\"\n        self.doc = doc\n        Doc.set_extension(\"phrases\", force=True, default=[])\n        Doc.set_extension(\"textrank\", force=True, default=self)\n        doc._.phrases = self.calc_textrank()\n\n        return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take input for number of sentences\n#length= input(\"How many sentences do you want to extract? \")\nlength=\"1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"tr = TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n\nrows=[]\nfor n in range(count-1):\n    text = summary.iloc[n,5]\n    if type(text) == float:\n        break\n    doc = nlp(text)\n\n    # Examine the results: a list of top-ranked phrases in the document\n    for p in doc._.phrases:\n       print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n       print(p.chunks)\n\n    #Construct a list of the sentence boundaries with a phrase vector (initialized to empty set) for each...\n    sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n    sent_bounds\n\n\n    #Iterate through the top-ranked phrases, added them to the phrase vector for each sentence...\n    limit_phrases = 4\n\n    phrase_id = 0\n    unit_vector = []\n\n    for p in doc._.phrases:\n        print(phrase_id, p.text, p.rank)\n        \n        unit_vector.append(p.rank)\n    \n        for chunk in p.chunks:\n            print(\" \", chunk.start, chunk.end)\n            \n            for sent_start, sent_end, sent_vector in sent_bounds:\n                if chunk.start >= sent_start and chunk.start <= sent_end:\n                    print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n                    sent_vector.add(phrase_id)\n                    break\n\n        phrase_id += 1\n\n        if phrase_id == limit_phrases:\n            break\n        \n#Let's take a look at the results...\n    sent_bounds\n    for sent in doc.sents:\n        print(sent)\n    \n#We also construct a `unit_vector` for all of the phrases, up to the limit requested...\n    unit_vector\n\n    sum_ranks = sum(unit_vector)\n    unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n    unit_vector\n\n#Iterate through each sentence, calculating its *euclidean distance* from the unit vector...\n    from math import sqrt\n\n    sent_rank = {}\n    sent_id = 0\n\n    for sent_start, sent_end, sent_vector in sent_bounds:\n        print(sent_vector)\n        sum_sq = 0.0\n    \n        for phrase_id in range(len(unit_vector)):\n            print(phrase_id, unit_vector[phrase_id])\n        \n            if phrase_id not in sent_vector:\n                sum_sq += unit_vector[phrase_id]**2.0\n\n        sent_rank[sent_id] = sqrt(sum_sq)\n        sent_id += 1\n\n    print(sent_rank)\n\n#Sort the sentence indexes in descending order\n    from operator import itemgetter\n    sorted(sent_rank.items(), key=itemgetter(1)) \n\n#Extract the sentences with the lowest distance, up to the limite requested...\n\n    limit_sentences = int(length)\n\n    sent_text = {}\n    sent_id = 0\n    final_summary= \"Extractive Summary:\"\n\n    for sent in doc.sents:\n        sent_text[sent_id] = sent.text\n        sent_id += 1\n\n    num_sent = 0\n\n    for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n        print(sent_id, sent_text[sent_id])\n        final_summary= final_summary + ' ' + sent_text[sent_id]\n        num_sent += 1\n    \n        if num_sent == limit_sentences:\n            rows.append([text, final_summary])\n         #   extractive_summary = pd.DataFrame({\n          #  'text': text,\n           # 'summary': summary,\n            #    index=[n]\n           # })\n            break\n    print(final_summary)\n#print(rows)\ndataframe = pd.DataFrame(rows, columns=[\"Original\", \"Extractive Summary\"])\n#print(dataframe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.to_csv('file_save.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}