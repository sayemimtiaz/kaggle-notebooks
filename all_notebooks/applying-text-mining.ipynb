{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\nBasic Text Mining Methods\n\n   * Text can be sentences, strings, words, characters and large documents\n   * Now lets create a sentence to understand basics of text mining methods.\n   * Our sentece is \"no woman no cry\" from Bob Marley.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a text\ntext = \"No woman no cry\"\n\n# length of text ( includes spaces)\nprint(\"length of text :\", len(text))\n\n# split the text\nsplitted_text = text.split()\n\nprint(\"splitted text :\", splitted_text)\n# each word is called token in text maning world.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find specific words with list comprehension method\nspecific_words = [word for word in splitted_text if(len(word)>2)]\nprint(\"Specific words :\", specific_words)\n\n# capitalized words with istitle() method that finds capitalized words\ncapital_words = [word for word in splitted_text if word.istitle()]\nprint(\"Capatal_words :\", capital_words)\n\n\n# words which end with \"o\": endswith() method finds last letter of word\nwords_end_with_o = [word for word in splitted_text if word.endswith('o')]\nprint(\"Word_end_with_o :\", words_end_with_o)\n\n# words which starts with \"w\": startswith() method\nword_start_with_w = [word for word in splitted_text if word.startswith(\"w\")]\nprint(\"Word Start with w\", word_start_with_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique with set() method\nprint(\"uniquw words :\", set(splitted_text))\n\n# make all letters lowercase with lower() method\nlower_text = [word.lower() for word in splitted_text]\n\n# then find uniques again with set() method\nprint(\"unique lower words :\", lower_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# chech words includes or not includes particular substring or letter\nprint(\"Is w letter in woman word:\", \"w\" in \"woman\")\n\n# check words are upper case or lower case\nprint(\"Is word uppercase:\", \"WOMAN\".isupper())\nprint(\"Is word lowercase:\", \"cry\".islower())\n\n# check words are made of by digits or not\nprint(\"Is word made of by digits: \",\"12345\".isdigit())\n\n# get rid of from white space characters like spaces and tabs or from unwanted letters with strip() method\nprint(\"00000000No cry: \",\"00000000No cry\".strip(\"0\"))\n\n# find particular letter from front \nprint(\"Find particular letter from back: \",\"No cry no\".find(\"o\"))  # at index 1\n\n# find particular letter from back  rfind = reverse find\nprint(\"Find particular letter from back: \",\"No cry no\".rfind(\"o\"))  # at index 8\n\n# replace letter with letter\nprint(\"Replace o with 3 \", \"No cry no\".replace(\"o\",\"3\"))\n\n# find each letter and store them in list\nprint(\"Each letter: \",list(\"No cry\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning text\ntext1 = \"     Be fair and tolerant    \"\nprint(\"Split text1 :\", text1.split(\" \"))\n\n# get rid of from these unnecassary white spaces with strip() method then split\nprint(\"Cleaned text :\", text1.strip().split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading files line by line\nf = open('../input/religious-and-philosophical-texts/35895-0.txt')\n\n# read first line\nprint(f.readline())\n\n# length of data\ntext3 = f.read()\nprint(\"length of text :\", len(text3))\n\n# Number of lines with splitlines() method\nlines = text3.splitlines()\nprint(\"Number of lines :\", len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndata = pd.read_csv(\"../input/ben-hamners-tweets/benhamner.csv\", encoding = \"latin-1\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find which entries contain the word 'appointment'\nprint(\"In his tweets, the rate of occuring kaggle word is: \", sum(data.text.str.contains('kaggle'))/len(data))\n\n# text\ntext = data.text[1]\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find regular expression on text\n# import regular expression package\nimport re\n\n# find callouts that starts with @\ncallouts = [word for word in text.split(\" \") if re.search(\"@[A-Za-z0-9_]+\", word)]\nprint(\"callouts: \", callouts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n    Lets look at this @[A-Za-z0-9_]+ expression more detailed\n        @: we say that our searched word start with @\n        [A-Za-z0-9_]: @ is followed by upper or lower case letters, digits or underscore\n        +: there can be more than one @. So with \"+\" sign we say that the words which start with @ can be occured more than one times.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# continue finding regular expressions\n# [A-Za-z0-9_] =\\w\n# We will use \"\\w\" to find callouts and our result will be same because \\w matches with [A-Za-z0-9_]\ncallouts1 = [word for word in text.split(\" \") if re.search(\"@\\w+\", word)]\nprint(\"callouts: \", callouts1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find specific characters like \"w\"\nprint(re.findall(r\"w\", text))\n\n# \"w\"ith, \"w\"indo\"w\", sho\"w\"ing, s\"w\"itches \n\n# do not find specific character like \"w\". We will use \"^\" symbol\nprint(re.findall(r\"[^w]\", text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at regular expressions for date\n    We will use \"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\" expression to find dates\n        d{1,2}: first number can be 1 or 2 digit\n        [/-]: between digits there can be \"/\" or \"-\" symbols\n        d{1,4}: last number can be between 1 and 4\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regular expressions for Dates\ndate = \"15-10-2000\\n09/10/2005\\n15-05-1999\\n05/05/99\\n\\n05/05/199\\n\\n05/05/9\"\nre.findall(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\",date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Natural Language Process (NLP)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Natural language is any language that is used by people in everyday like English or Spanish\n* Natural language processing is that any computation and manipulation of natural language to get inside about * how words mean and how sentences are contructed is natural language processing.\n* Natural languages are in change like new words tweets\n* Natural language process tasks are counting words, finding unique words and sentence boundaries, identify   \\ * semantic rules and entities in a sentence\n* We will use natural language tool kit that is open source library in python.\n* It supports most of the NLP tasks\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import natural language tool kit\nimport nltk as nlp\n\n# counting vocabulary of words\ntext = data.text[1]\nsplitted = text.split(\" \")\nprint(\"number of words: \", len(splitted))\n\n# counting unique vocabulary of words\ntext = data.text[1]\nprint(\"number of unique words: \", len(set(splitted)))\n\n# print first five unique words\nprint(\"first 5 unique words: \",list(set(splitted))[:5])\n\n# frequency of words\ndist = nlp.FreqDist(splitted)\nprint(\"frequency of words: \", dist)\n\n# look at keys in dist\nprint(\"words in text: \", dist.keys())\n\n# count how many time a particalar value occurs. Lets look at \"box\"\nprint(\"the word box is occured how many times:\",dist[\"box\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization and Stemming words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n* Normalization is different forms of the same word like have and having\n* Stemming is finding a root of the words like having => have\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nword = \"task Tasked tasks tasking\"\nword_list = word.lower().split(\" \")\nprint(\"Normalized words: \", word_list)\n\n# stemming\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in word_list]\nprint(\"roots of task Tasked tasks tasking: \", roots)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* It is also stemming but resulting stems are all valid words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# stemming\nstemming_word_list =  [\"Universal\",\"recognition\",\"Become\",\"being\",\"happened\"]\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in stemming_word_list]\nprint(\"result of stemming: \",roots)\n\n\n# lemmatization\nlemma = nlp.WordNetLemmatizer()\nlemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]\nprint(\"result of lemmatization: \",lemma_roots)\n      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization\n\n* Splitting a sentece into words(tokes)\n* Learn tokenize with nltk","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_t = \"Youâ€™re in the right place!\"\nprint(\"split the sentece: \", text_t.split(\" \"))  # 5 words\n\n# tokenization with nltk\nprint(\"tokenize with nltk: \",nlp.word_tokenize(text_t))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you like this kernel, don't forget to upvote...!\nThanks in Advance.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}