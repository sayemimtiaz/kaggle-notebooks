{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn import preprocessing\nfrom keras.preprocessing import sequence\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, SpatialDropout1D, MaxPooling1D, Embedding, Conv1D, Flatten, Dropout\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, LSTM\nfrom keras.optimizers import Adam\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n\n# load data\ninput_file = \"../input/imdb-review-dataset/imdb_master.csv\"\n\n# comma delimited is the default\ndata = pd.read_csv(input_file, header = 0, encoding='ISO-8859-1', engine='python')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"650f345a9a89f53a3c08d8ef0108d1dc0ec05f72"},"cell_type":"code","source":"\n# show data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dadb820ef0bdaa422cff3993f6a78cbc7ee6e66"},"cell_type":"code","source":"# we will divide data on train and test sets\n\nindexes_train = np.where(data['type'] == 'train')\nindexes_test = np.where(data['type'] == 'test')\n\nX_train = data['review'][indexes_train[0]].values\nY_train = data['label'][indexes_train[0]].values\n\nX_test = data['review'][indexes_test[0]].values\nY_test = data['label'][indexes_test[0]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99b0ce63c0fe06bd2e982f40b297caa8e5d3abea"},"cell_type":"code","source":"# certain indices of positive and negative reviews\n\nindex_unsup = np.where(Y_train == 'unsup')\nY_train = np.delete(Y_train, index_unsup)\nX_train = np.delete(X_train, index_unsup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bd38c71092acf35420d4fb568faad277964f2be"},"cell_type":"code","source":"# example review\ndata['review'][55000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1999f7a2137e4506934ee0e6b85c0202cfcfb42"},"cell_type":"code","source":"# Analysis of the test and training sample\n\ndef buil_hist(data):\n    lenf_data = []\n    for i in data:\n        lenf_data.append(i)\n    return lenf_data\n    \nlenf_train = buil_hist(Y_train)\nlenf_test = buil_hist(Y_test)   \n\nplt.subplot(1, 2, 1)\nplt.title('Train')\nplt.hist(lenf_train)\nplt.subplot(1, 2, 2)\nplt.hist(lenf_test)\nplt.title('Test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5317852572b4d45664dc2e2e301ad0723e9bb1d6"},"cell_type":"code","source":"# transform catecorical labels in num labels\nle = preprocessing.LabelEncoder()\nle.fit(Y_train)\n\nprint('Lisr unique labels: {}'.format(le.classes_))\n\nY_train_encod = le.transform(Y_train) \nY_test_encod = le.transform(Y_test) \n\n# inverse transform\n# list(le.inverse_transform(Y_test_encod))\n\n# shufle train data set\nX_train, Y_train_encod = shuffle(X_train, Y_train_encod)\n\n# check a shuffle\nprint(Y_train_encod[10], '\\n', X_train[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"805433643d341a9e5c20e08b80e64106f1d686bd"},"cell_type":"code","source":"# HYPERPARAMETERS\nmax_features = 10000\nmaxlen = 100\nembedding_dimenssion = 100\n\nVALIDATION_SPLIT = 0.1\nCLASSES = 1\nNB_EPOCH = 20\nBATCH_SIZE = 64\nOPTIMIZER = Adam(lr=0.001)\n\n# Tokenization and encoding text corpus\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(X_train)\nX_train_en = tk.texts_to_sequences(X_train)\nX_test_en = tk.texts_to_sequences(X_test)\n\n# dictionaries\nword2index = tk.word_index\nindex2word = tk.index_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2fecf30d1bf85a106d24fe277303c1c3f1b4994"},"cell_type":"code","source":"# check the correctness of the encoding\nprint('Orginal \\n{}'.format(X_train[2]))\n\nprint('\\nDecoding')\nfor index in X_train_en[2]:\n    x = index2word.get(index)\n    print(x, end=' ')\nprint('\\nVerify coding fidelity. Click continue to continue.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63bf7e5f446b322518b1ee929e05951b2b0d50de"},"cell_type":"code","source":"# Ðnalysis of the length of each review\nlenf_reviews = list(map(len, X_train_en))\n\nplt.hist(lenf_reviews)\nplt.title('lenf_reviews')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e1bd019ed2863426321b246fce78a79d2120b21"},"cell_type":"code","source":"# we give feedback to the same dimension\nX_train_new = sequence.pad_sequences(X_train_en, maxlen=maxlen)\nX_test_new = sequence.pad_sequences(X_test_en, maxlen=maxlen)\nprint('\\nLed examples of texts to the general dimension. Click continue to continue')\n\n# path to the pre-trained word vectors or download the link\n# https://nlp.stanford.edu/projects/glove/\nglove_dir = ''.join(['../input/glove-vectors/glove.6B.', str(embedding_dimenssion),'d.txt']) # This is the folder with the dataset\n\nembeddings_index = {} # We create a dictionary of word -> embedding\n\nwith open(glove_dir, encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0] # The first value is the word, the rest are the values of the embedding\n        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n\nprint('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))\nprint('\\nLoaded the pre-trained word vectors in English: Click continue to continue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ad9fb02fd30c213a3ff1a30425f046a74534b3f"},"cell_type":"code","source":"embedding_matrix = np.zeros((max_features, embedding_dimenssion))\n\n# The vectors need to be in the same position as their index.\n# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n\n# Loop over all words in the word index\nfor word, i in word2index.items():\n    # If we are above the amount of words we want to use we do nothing\n    if i >= max_features:\n        break\n    # Get the embedding vector for the word\n    embedding_vector = embeddings_index.get(word)\n    # If there is an embedding vector, put it in the embedding matrix\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint('Creating your own dictionary of word vectors from Glove is over. Click continue.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5e994c1aa83850080dec1b509b3920864ab7280"},"cell_type":"code","source":"# checking the dimension of arrays\nprint(embedding_matrix.shape, X_train_new.shape, Y_train_encod.shape, X_test_new.shape, Y_test_encod.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86cf02b6a3c3d11f0a462b4339c651b8c693c4c8"},"cell_type":"code","source":"print('Click continue to learn the model')\n\nmodel = Sequential()\n\n# LSTM version 1\nmodel.add(Embedding(max_features, embedding_dimenssion, input_length=maxlen,\n                    weights=[embedding_matrix], trainable=False))\nmodel.add(LSTM(125, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.summary()\n\n\n# compile the model\n\nmodel.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n\n# train model\nmodel.fit(X_train_new, Y_train_encod, batch_size=BATCH_SIZE, epochs=10, validation_split=VALIDATION_SPLIT, verbose=1)\n\n# evaluate the quality of the system using accuracy\nscores = model.evaluate(X_test_new, Y_test_encod)\nprint('losses: {}'.format(scores[0]))\nprint('TEST accuracy: {}'.format(scores[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"476e4188225c7fb45938d9ce332ceb41e055e839"},"cell_type":"code","source":"# predicted labels on test\nY_predicted_test = model.predict_classes(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5605f2dbbf3db819828b96c1286578fb3401cd9"},"cell_type":"code","source":"# evaluate the quality of the system using f1-score and confusion matrix\nprint('F1-score: {0}'.format(f1_score(Y_predicted_test, Y_test_encod)))\nprint('Confusion matrix:')\nconfusion_matrix(Y_predicted_test, Y_test_encod)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}