{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\n\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n\nINDUS - proportion of non-retail business acres per town.\n\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n\nNOX - nitric oxides concentration (parts per 10 million)\n\nRM - average number of rooms per dwelling\n\nAGE - proportion of owner-occupied units built prior to 1940\n\nDIS - weighted distances to five Boston employment centres\n\nRAD - index of accessibility to radial highways\n\nTAX - full-value property-tax rate per $10,000\n\nPTRATIO - pupil-teacher ratio by town\n\nN - 1000(N - 0.63)^2 where N is the proportion of Non-Americans by town\n\nLSTAT - % lower status of the population\n\nMEDV - Median value of owner-occupied homes in $1000's\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target is MEDV (Median value of owner-occupied homes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will use deep learning method and for scaling will use minmaxscaler\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\n#firt of all i will make the dataset ready to model.\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'N', 'LSTAT', 'MEDV']\nhousing = pd.read_csv(\"/kaggle/input/boston-house-prices/housing.csv\", header=None, delimiter=r\"\\s+\", names=column_names)\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimension of the dataset\nprint(np.shape(housing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarized statistics of data\nprint(housing.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#while prediction of MEDV , the columns ZN and CHAS is not necessary \n#and in the features above 50.00 in MEDV columns are not necessary \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in housing.items():\n    sns.boxplot(y=v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    for k, v in housing.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 / np.shape(housing)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n#there are outliers in the columns CRIM,ZN,RM and B seemed in the graphs above. \n#lets see the percentages of them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets remove features of MEDV columns below 50\nhousing = housing[~(housing['MEDV'] >= 50.0)]\nprint(np.shape(housing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the new graphs as plot\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in housing.items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the correlation\nplt.figure(figsize=(20, 10))\nsns.heatmap(housing.corr().abs(),  annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#in the matrix above it is seemed that TAX and RAD coloumns are highly correlated but LSTAT,INDUS,RM,TAX,NOX,PTRAIO columns have low correlation.\nfrom sklearn import preprocessing\n# Let's scale the columns before plotting them against MEDV\nmin_max_scaler = preprocessing.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = housing.loc[:,column_sels]\ny = housing['MEDV']\nx = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.regplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets remove the skewness of the data\ny =  np.log1p(y)\nfor col in x.columns:\n    if np.abs(x[col].skew()) > 0.3:\n        x[col] = np.log1p(x[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#starting the model. split the train and test datas. after that i will make fit transfer at the same time\nfrom sklearn.model_selection import train_test_split\n\ntrain_test_split(x, y)\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 20)\nscaler = MinMaxScaler((-1,1))\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\n# we use elastic net. alpha is the total penalty parameter and l1_ratio is the proportion of alpha to be given to l1 norm\nelastic = ElasticNet(alpha = 0.05 , l1_ratio= 0.5, max_iter = 1000) \n\n#fit the model with train data. \nmodel = elastic.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import ridge from linear model\nfrom sklearn.linear_model import Ridge\n\n# we use ridge instead of linear regression. \nridge = Ridge(alpha = 0.3) \n\n\n#column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n#x = housing.loc[:,column_sels]\n#y = housing['MEDV']\n\n\n#fit the model with train data. \nmodel = ridge.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression() \n#fit the model with train data. \nmodel = lr.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nlinear_coef = model.coef_\nlinear_coef","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\n\nx_train_transformed = scaler.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n#lets first train a linear regression model\n#try: alpha=0, max_iter = 10000\n#predictor = SGDRegressor(alpha=0) \n\npredictor = SGDRegressor(alpha=0, max_iter = 10000) \n\n\n#fit the model with train data. \nmodel = predictor.fit(x_train_transformed,y_train)\n\nx_test_transformed = scaler.transform(x_test)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train_transformed,y_train)\nr2_test = model.score(x_test_transformed,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}