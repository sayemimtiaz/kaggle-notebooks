{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://housepriceprediction.com/wp-content/uploads/2018/07/11679.4c96032809c10d54e3e216015aecf32a_XL-768x450.jpg\" height=600 width=1000>","metadata":{}},{"cell_type":"markdown","source":"Linear regression can be considered as a grandfather of regression models. It’s simple yet incredibly useful. It can be used in a variety of domains. It has a nice closed form solution, which makes model training a super-fast non-iterative process.\n\nRecently for one of my projects I was looking for some kaggle kernels validating important assumptions of linear regression, but not found any. It made me write a kernel on Linear regression along with validating its assumptions.\n\nIf our data satisfies the assumptions that the Linear Regression model, specifically the Ordinary Least Squares Regression (OLSR) model makes, in most cases you need look no further. Before moving further, Let us discuss important assumptions of Linear regression.\n\n\n** <font color='blue'> Note:   This is not an end to end solution for the problem. But I will try to validate the assumptions of linear regression </font> **\n\n\n## Assumptions:\n\n**1. <font color='red'> Linear relationship between target and independent variables:</font>** The response variable y should be a linearly related to the explanatory variables X.\n\n**2. <font color='red'> No or Little multicollinearity between independent variables:</font>** Linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs when the independent variables are too highly correlated with each other.\n\n**3. <font color='red'> Residual errors must be normally distributed:</font>** The residual errors should be normally distributed.\n\n**4. <font color='red'> Residual errors must be homoscedacitic:</font>** The residual errors should have constant variance. Otherwise it is known as hetroscedacity.\n\n\n\nEventhough on internet we can see more assumptions like normal distribution of input variables and target and others, the above are the most important ones. So we will go with it.\n\n\n**Let's get our hands dirty with data and we will check the assumptions as we will go further.**\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport matplotlib.style as style\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nimport scipy\nimport matplotlib.gridspec as gridspec\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/housesalesprediction/kc_house_data.csv')\nprint(data.shape)\ndata.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us go through what each feature is:\n\n* ID - Unique ID for each home sold\n* date- Date of the home sale\n* price - Price of each home sold\n* bedrooms - Number of bedrooms\n* bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n* sqft_living - Square footage of the apartments interior living space\n* sqft_lot - Square footage of the land space\n* floors - Number of floors\n* waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not\n* view - An index from 0 to 4 of how good the view of the property was\n* condition - An index from 1 to 5 on the condition of the apartment,\n* average level of construction and design, and 11-13 have a high quality level of construction and design.\n* sqft_above - The square footage of the interior housing space that is above ground level\n* sqft_basement - The square footage of the interior housing space that is below ground level\n* yr_built - The year the house was initially built\n* yr_renovated - The year of the house’s last renovation\n* zipcode - What zipcode area the house is in\n* lat - Lattitude\n* long - Longitude\n* sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\n* sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors ","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n","metadata":{}},{"cell_type":"markdown","source":"### Checking for missing values","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have not any missing values.We are ready to go","metadata":{}},{"cell_type":"markdown","source":"### Analyzing target: Price","metadata":{}},{"cell_type":"code","source":"#helper function\n#function for ploting Histogram,Q-Q plot and \n# Box plot of target and also print skewness\ndef target_analysis(target):\n    fig = plt.figure(constrained_layout=True, figsize=(14,10))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(target,norm_hist=True,ax=ax1)\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('Q-Q Plot')\n    stats.probplot(target,plot=ax2)\n    ax3 = fig.add_subplot(grid[:,2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(target,orient='v',ax=ax3)\n    print(f'skweness is { target.skew()}')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_analysis(data['price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_analysis(np.log1p(data['price']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its better if we make the target normal. Eventhough it is not an assumption of linear regression, it will helps to better analysis with our residuals later. Actaully our target is right skwedwith skweness of about  4.024. By taking log of ot we are able to reduce the skweness and make it normal to ceratin extend. We can do the reverse after prediction.","metadata":{}},{"cell_type":"code","source":"# transforming logprice\ndata['log_price'] = np.log1p(data['price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking distribution of independent Features\n\n","metadata":{}},{"cell_type":"code","source":"df_num = data[['sqft_living','sqft_lot','sqft_basement','sqft_above','sqft_living15','sqft_lot15','floors','grade',\n             'bedrooms','bathrooms','yr_built','yr_renovated', 'condition','log_price','zipcode']]\n\nmulticoll_pairs = df_num.drop(columns=['log_price']).columns.to_list()\n\nfig,axes = plt.subplots(7,2,figsize=(15,20))\n\ndef plot_two(feat,i,j):\n    sns.boxplot(x=df_num[feat],ax=axes[i,j])\n    fig.tight_layout(pad=5.0)\n\n    \n\nfor i,feat in enumerate(multicoll_pairs):\n    j = i%2 #0 or 1\n    plot_two(feat,i//2,j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see some outlier kind of points in all most of the features. Removing all of them can make loss of information. So as of now we will keep it.","metadata":{}},{"cell_type":"markdown","source":"### <font color='red'>Checking Linearity: (Assumption 1)</font>\n\nLinearity of independent variables is an important assumption of linear regression. Let us see how much linearity is there.","metadata":{}},{"cell_type":"code","source":"# df_num = df[['sqft_living','sqft_lot','sqft_basement','sqft_above','sqft_living15','sqft_lot15','bedrooms','bathrooms','yr_built','yr_renovated','log_price']]\n\nmulticoll_pairs = df_num.drop(columns=['log_price']).columns.to_list()\n\nfig,axes = plt.subplots(7,2,figsize=(15,20))\n\ndef plot_two(feat,i,j):\n    sns.regplot(x=df_num[feat], y=df_num['log_price'], ax=axes[i,j])\n    sns.scatterplot(y=df_num['log_price'],x=df_num[feat],color=('orange'),ax=axes[i,j])   \n    fig.tight_layout(pad=5.0)\n    \n\nfor i,feat in enumerate(multicoll_pairs):\n    j = i%2 #0 or 1\n    plot_two(feat,i//2,j)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see that sqft_living, sqft_basement, sqft_above, sqft_living15, shows good linearity.\n\n* We can see that sqft_lot and sqft_lot15 is not that much linear. We can feature transform them or drop them.\n\n* Number of bathrooms and bedrooms,floors, grades are also showing some linearity. So I think we do not need to convert them to categorical.\n\n* yr_built and yr_renovated doesnot show any sort of linearity.We will further feature engineer them.\n\n\n* Features like view, condition,zipcode behave categorical. So we will one hot encode them and use them as categorical features.\n\n* From boxplot it is clear that there is an outlier in bedrooms and we will remove it after analyzing if needed.\n\n> It is important to note that here by linearity we donot assume that our features must be perfectly linear. Almost linear is suffiecient for us to proceed further.","metadata":{}},{"cell_type":"code","source":"data = data.drop(columns=['sqft_lot15','sqft_lot'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing bedrooms","metadata":{}},{"cell_type":"code","source":"data['bedrooms'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['bedrooms'] == 33]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will drop that row\ndata = data[data['bedrooms'] != 33]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing Year of sale and Age of  building\n\nFrom date column we can easily get year of sale","metadata":{}},{"cell_type":"code","source":"data['yr_sale'] = data['date'].apply(lambda x: int(str(x)[0:4]))\n\nsns.distplot(data['yr_sale'])\nplt.show()\ndata['yr_sale'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all sales occured in year 2014 and 2015. We can also create a new feature Age of house at the time of sale from this.","metadata":{}},{"cell_type":"code","source":"data['age'] = -(data['yr_built'] - data['yr_sale'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data['age'])\nplt.show()\ndata['age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us check the linearity of data","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data['age'],data['log_price'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is no direct connecting between price and age. So we will convert it categorical.","metadata":{}},{"cell_type":"code","source":"bins = [-30,0,20,40,60,80,100,120]\nlabels = ['<1','0-20','20-40','40-60','60-80','80-100','>100']\ndata['age_binned'] = pd.cut(data['age'], bins=bins, labels=labels)\n\nsns.countplot(data['age_binned'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing year_built and year_renovated","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['yr_built'])\ndata['yr_built'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All our year_bult data  ranges betwwen 1900 to 2015. So we will custom encode it by dividing it to span of 10 years. for eg: if year_built between 1900-1910 -> 1, 1910-1920 -> 2 and likewise.","metadata":{}},{"cell_type":"code","source":"yr = [i for i in range(1900,2020)]\nvals = [i for i in range(1,13) for j in range(10)]\n\n\ndict_yr = { k:v for k,v in zip(yr,vals)}\ndata['yr_built_cat'] = data['yr_built'].map(dict_yr)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similary we will create a categorical column if the house is renovated","metadata":{}},{"cell_type":"code","source":"data['is_renovated'] = data['yr_renovated'].apply(lambda x: 1 if x>0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='red'>Checking for multicollinearity of independent features (Assumption 2)</font>\n\nNext we will check multicollinearity of independent features and we will remove if we found any. We will use pearsons correlation for that.","metadata":{}},{"cell_type":"code","source":"df = data[['sqft_living','sqft_basement','sqft_above','sqft_living15','bathrooms','bedrooms','floors','grade',\n    'waterfront','view','condition','zipcode','yr_built_cat','is_renovated','age_binned','log_price']]\ncorr = df.corr()\n# 'sqft_lot15','sqft_lot',\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap=sns.diverging_palette(20, 220, n=200))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no strong negative coorelation. There exist a somewhat strong positive coorelation between sqft_living and sqft_above.  Let us see how strong it is.","metadata":{}},{"cell_type":"code","source":"df[['sqft_living','sqft_above','log_price']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both of them maintains strong positive coorelation with each other. So its better we drop one feature. We will drop sqft_above (As it has lesser coorelation with target compared to sqft_living.)","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['sqft_above'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train test split","metadata":{}},{"cell_type":"code","source":"num_cols = ['sqft_living','sqft_basement','sqft_living15','bathrooms','bedrooms','floors','grade']\n# 'sqft_lot15','sqft_lot'\n\nX = df.drop(columns=['log_price'])\ny = df['log_price']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One hot encoding","metadata":{}},{"cell_type":"markdown","source":"Now we will one hot encode categorical features. For reading more about one onehot encoding refer: https://www.kaggle.com/arunmohan003/encoding-categorical-variables-tutorial","metadata":{}},{"cell_type":"code","source":"\nfeats = ['waterfront','view','condition','yr_built_cat','is_renovated','age_binned','zipcode']\none_hot_tr = pd.get_dummies(X_train[feats])\none_hot_test = pd.get_dummies(X_test[feats])\ncat_train,cat_test = one_hot_tr.align(one_hot_test,join='left',axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization\n\nIt is important that we must standardize the data before fitting to regression models","metadata":{}},{"cell_type":"code","source":"std = StandardScaler()\nstd.fit(X_train[num_cols])\nX_train[num_cols] = std.transform(X_train[num_cols])\nX_test[num_cols] = std.transform(X_test[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.concat((X_train[num_cols],cat_train),axis=1)\nX_test = pd.concat((X_test[num_cols],cat_test),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model - Linear regression","metadata":{}},{"cell_type":"code","source":"reg = LinearRegression()\nreg.fit(X_train,y_train)\n\ntrain_pred = reg.predict(X_train)\ntest_pred = reg.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train mse: {np.sqrt(mean_squared_error(y_train,train_pred))}')\nprint(f'Test mse: {np.sqrt(mean_squared_error(y_test,test_pred))}')\nprint('-'*50)\nprint(f'Train R2: {r2_score(y_train,train_pred)}')\nprint(f'Test R2: {r2_score(y_test,test_pred)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='red'>Checking Residual errors are normally distributed (Assumption 3)</font>\n\nThis is not a strict assumption of linear regression. Nothing will go horribly wrong with your regression model if the residual errors ate not normally distributed. Normality is only a desirable property.\n\nWhat’s normally is telling us is that most of the prediction errors from your model are zero or close to zero and large errors are much less frequent than the small errors. Q-Q plot is a effective way to check normality.","metadata":{}},{"cell_type":"code","source":"residuals = y_train - train_pred\ntarget_analysis(residuals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thats great , we got an approximate normal distribution. (it is evident from Q-Q plot)","metadata":{}},{"cell_type":"markdown","source":"### <font color='red'> Checking for Homoscedacity (Assumption 4) </font>\n\n","metadata":{}},{"cell_type":"markdown","source":"Homoscedacity means equal variance distribution and hetrosceadacity means unequal variance distribution.\nWe will check the nature between residuals and fitted value to check for heteroscedacity. Usually if it is heteroscedacitic, we will get a funnel shaped plot.\n\n> Heteroscedastic errors frequently occur when a linear model is fitted to data in which the fluctuation in the response variable y is some function of the current value y, for e.g. it is a percentage of the current value of y. \n\n>  The presence of non-constant variance in the error terms results in heteroskedasticity. Generally, non-constant variance arises in presence of outliers or extreme leverage values. Look like, these values get too much weight, thereby disproportionately influences the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.\n\n\nTo check homoscedacity, one can look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern (shown in next section).","metadata":{}},{"cell_type":"code","source":"residuals = y_train - train_pred\n\nsns.scatterplot(y_train,residuals)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident that hetroscedacity is present in our data. It violates the assumption of linear regresion. It is not a surpricing factor because in earlier box plots itself we had seen some outliers present in data. I donot removed it because removing all outliers is removing an entire portion of useful data.So it is not recommended. So either we can transform those features(taking log, sqrt etc) or based on domain expertise we can remove some outliers and reduce hetroscedacity.\n\n> How to fix hetroscedacity (general)?\n> \n> * Transform the dependent variable so as to linearize it and dampen down the heteroscedastic variance. Commonly used transforms are log(y) and square-root(y).\n> * Identify important variables that may be missing from the model, and which are causing the variance in the errors to develop a pattern, and add those variables into the model. Alternately, stop using the linear model and switch to a completely different model such as a Generalized Linear Model, or a neural net model.\n> * Simply accept the heteroscedasticity present in the residual errors.","metadata":{}},{"cell_type":"markdown","source":"Here we will just accept the hetroscedacity. You are always welcome to transform those features and see tha result.\n","metadata":{}},{"cell_type":"markdown","source":"## Scope of Improvement\n\n* There are several other ways to validate the assumptions. I just proposed the simpler ways.\n\n* There is more scope of feature engineering here. Still you can transform features in to square, log etc of independent features and try out it in linear models(Also it can help to remove hetroscedacity).\n\n* You can also try non linear regression models and tree based models which can improve perfomance to great extend.\n\n## <font color='red'> If you find my kernel useful please do Upvote </font>\n","metadata":{}}]}