{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"### Recently I started with NLP and after going through several articles and algorithms I thought to implement it. So in this kernel I am try to implement them. Data is of Stack OverFlow and I'll try to understand what the key words are in the coupus. "},{"metadata":{},"cell_type":"markdown","source":"![stack-overflow-for-teams.webp](attachment:stack-overflow-for-teams.webp)","attachments":{"stack-overflow-for-teams.webp":{"image/webp":"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"}}},{"metadata":{},"cell_type":"markdown","source":"## Table -- \n1. [ Getting Started](#The_Data) \n    * [Import Liberay](#Lib)\n    * [Explore Data](#Display)\n2. [Pre-Processing](#Pre-Processing) \n    * [Cleaning Text](#Cleaning)\n    * [Bigrams](#Bigrams)\n    * [Most Frequent Words](#Frequent)\n3. [Training the model](#Model)\n    * [The parameters](#parms)\n    * [Building the Vocabulary Table](#Vocab)\n    * [Training of the model](#Training)\n4. [Exploring the model](#Exploring)\n    * [Most similar to](#similar)\n    * [t-SNE visualizations](#t-SNE)\n5. [PLOTS](#PLOTS)\n6. [Conclusion](#Conclusion)"},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"The_Data\">The Data : </a>\n\nIts a collection of 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories:\n\nHQ: High-quality posts with 30+ score and without a single edit.\nLQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\nLQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n"},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"Lib\">Import Liberay </a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"Display\"> Let's look at the Data</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.info(),data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Pre-Processing'>Pre-Processing</a> "},{"metadata":{},"cell_type":"markdown","source":"Note:- We are currently going to use only two columns but we are still going to keep 2 more columns just for later use\n<ul>\n  <li>1. <code> Id , CreationDate </code> were not usefull at the current point. </li>\n  <li>2. And let's transform the <code> 'Y' </code> column into \" 0,1,2 \" </li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_data = data[['Title','Body','Tags','Y']]\ndummy_data['Y'] = dummy_data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndummy_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Again let's combine <code> 'Text' = Title and Body </code> "},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_data['text'] = dummy_data['Title'] + ' ' + dummy_data['Body']\ndummy_data = dummy_data.drop(['Title', 'Body'], axis=1)\ndummy_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Cleaning'>Cleaning Text</a> "},{"metadata":{},"cell_type":"markdown","source":"### We are lemmatizing and removing the stopwords and non-alphabetic characters for each line . \n### For this purpose we are using SpaCy and its build-in pipeline for this purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\ndef cleaning(doc):\n    #remove stopwords + Lemmitze them \n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    \n    # word2vec uses context words to learn the vector representation of a terget word \n    # if sentence is only one or two worlds long,\n    # the benifit for training them is small thus we may drop them \n    if len(txt)>2:\n        return ' '.join(txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing characters other than alpha numeric characters "},{"metadata":{"trusted":true},"cell_type":"code","source":"init_cleaning = (re.sub(\"[^A-Za-z]+\",' ',str(row)).lower() for row in dummy_data['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\n#batch size used if a document is having words more than 5000 treat it as a seperate doc\ntxt = [cleaning(doc) for doc in nlp.pipe(init_cleaning,batch_size=5000,n_threads=-1)]\n\nprint('Time too clean up everything: {} mins'.format(round((time() - t)/60,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaned Data: \nNow that we have cleaned data lets store it in a new dataframe and drop any null/ missing values or duplicates "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = pd.DataFrame({'clean':txt})\ndf_clean = df_clean.dropna().drop_duplicates()\ndf_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Bigrams'>Bigrams</a>\nWe are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phrases,Phraser","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As Phrases() takes a list of list of words as input:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent = [row.split() for row in df_clean['clean']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creates the relevant phrases from the list of sentences:"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = Phrases(sent,min_count=30, progress_per=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = Phraser(phrases)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the corpus based on the bigrams detected:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = bigram[sent]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Frequent'>Most Frequent Words:</a>\nMainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Model'>Training the model</a>\n## Gensim Word2Vec Implementation:\nWe use Gensim implementation of word2vec:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model in 3 Steps: \n\n1. `Word2Vec()`: \n>In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n2. `.build_vocab()`: \n>Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. \n3. `.train()`:\n>Finally, trains the model.<br>\nThe loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count() # Count the number of cores in a computer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='parms'> The parameters </a> :\n\n* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n\n\n* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n\n\n* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n\n\n* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n\n\n* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Vocab'> Building the Vocabulary Table:</a>\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='Training'> Training of the model:</a>\n_Parameters of the training:_\n* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='Exploring'>Exploring the model</a>\n## <a id='similar'>Most similar to:</a>\n\nHere, we will ask our model to find the word most similar to some of the words in the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"java\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='t-SNE'>t-SNE visualizations:</a>\nt-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\nHere is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=20).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='PLOTS'>Let's check the 20 similar words to Java and plot it. </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'java',[t[0] for t in w2v_model.wv.most_similar(positive=[\"java\"], topn=20)][10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As being from a coding background I can find some words and terms similar to Java. "},{"metadata":{},"cell_type":"markdown","source":"## Let's see the difference as to how top 30 similar words make a difference"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'java',[t[0] for t in w2v_model.wv.most_similar(positive=[\"java\"], topn=30)][10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again I can find words that are related to Java Which is pretty good.  "},{"metadata":{},"cell_type":"markdown","source":"## Let's see if it works for other words"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'swift',[t[0] for t in w2v_model.wv.most_similar(positive=[\"swift\"], topn=20)][10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is again giving great results."},{"metadata":{},"cell_type":"markdown","source":"## Let's see if it works for something random"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'child',[t[0] for t in w2v_model.wv.most_similar(positive=[\"child\"], topn=20)][10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are great it actually found similar words to Child and it was able to comprehend it with  \"Classes and Child\" which is great. "},{"metadata":{},"cell_type":"markdown","source":"## <a id='Conclusion'>Conclusion:</a>\nWell this is not the end of this kernel. Next I will include the part where I will try classifing it into their respective categories."},{"metadata":{},"cell_type":"markdown","source":"## Hope it was great and you learned something. Please UpVote and Comment if you like.Thank YouüòÉüôè"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}