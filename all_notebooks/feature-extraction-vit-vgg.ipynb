{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pytorch-pretrained-vit\n!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt -y install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\nplt.rcParams.update(plt.rcParamsDefault)\n\nplt.matplotlib.rc('figure', figsize=(10, 6))\nplt.matplotlib.rc('grid', linestyle = 'dotted', linewidth=1.5, alpha = 0.25)\nplt.matplotlib.rc('text', usetex=True)\nplt.matplotlib.rc('font', family='serif', size=15)\nplt.matplotlib.rc('legend', fontsize=15)\n\n# Change ticks\nplt.rcParams['xtick.major.size'] = 7.0\nplt.rcParams['xtick.minor.size'] = 4.0\nplt.rcParams['xtick.direction'] = 'inout'\nplt.rcParams['ytick.major.size'] = 7.0\nplt.rcParams['ytick.minor.size'] = 4.0\nplt.rcParams['ytick.direction'] = 'inout'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.autonotebook import tqdm\nfrom pytorch_pretrained_vit import ViT\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torchsummary import summary\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nPATH = '../input/covidxct'\nIMAGES_PATH = f'{PATH}/2A_images'\n\nIMAGE_DIM = 384\n\nCLASSES = {\n    0: 'Normal', \n    1: 'Sick', \n}\n\n# lock the seed\nSEED = 211\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# device - cpu or gpu?\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    \n# create a folder to save the model\nif not os.path.isdir('CHECKPOINTS'):\n    os.mkdir('CHECKPOINTS')\n    \n    \ndef elapsed_time(start_time):\n    # source: https://stackoverflow.com/a/27780763\n    hours, rem = divmod(time.time() - start_time, 3600)\n    minutes, seconds = divmod(rem, 60)\n    duration = \"{:0>2}:{:0>2}:{:02d}\".format(int(hours), int(minutes), int(seconds))\n    return duration","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-24T06:04:31.824972Z","iopub.execute_input":"2021-06-24T06:04:31.825237Z","iopub.status.idle":"2021-06-24T06:04:33.312365Z","shell.execute_reply.started":"2021-06-24T06:04:31.825207Z","shell.execute_reply":"2021-06-24T06:04:33.31152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Costum Dataloader\n\n> **Based on:** https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, label_file:str, transform_img=False):\n        self.label_file = label_file\n        self.transform_img = transform_img\n        fnames, classes, bboxes = self.load_labels()\n        \n        self.data = []\n        self.positive_idxs = []\n        self.negative_idxs = []\n        for i in range(len(fnames)):\n            self.data.append([f'{IMAGES_PATH}/{fnames[i]}', classes[i], bboxes[i]])\n            if classes[i] == 0:\n                self.negative_idxs.append(i)\n            if classes[i] == 1:\n                self.positive_idxs.append(i)\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_path, label, bbox = self.data[idx]\n        \n        class_id = torch.tensor([label])\n        img = cv2.imread(img_path)\n        \n        # crop to fit in bounding box\n        img = img[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n        \n        # resize to fit in model input\n        img = cv2.resize(img, (IMAGE_DIM, IMAGE_DIM))\n        \n        #convert image to tensor\n        img_tensor = torch.from_numpy(img)\n        img_tensor = img_tensor.permute(2, 0, 1).float()\n        \n        if self.transform_img:\n            img_transforms = transforms.Compose([\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n            img_tensor = img_transforms(img_tensor)\n        \n        return img_tensor, class_id\n    \n    def load_labels(self):\n        \"\"\"Loads image filenames, classes, and bounding boxes\"\"\"\n        fnames, classes, bboxes = [], [], []\n        with open(self.label_file, 'r') as f:\n            for line in f.readlines():\n                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n                cls = int(cls)\n                if (cls == 0) | (cls == 2): # remove pneumania class\n                    fnames.append(fname)\n                    classes.append(0 if cls == 0 else 1)\n                    bboxes.append((int(xmin), int(ymin), int(xmax), int(ymax)))\n        return fnames, classes, bboxes\n    \n\ndef get_dataloader(label_file, batch_size, shuffle=True, num_workers=1, max_images=-1, transform_img=False):\n    \"\"\"Returns a Dataloader object, which supports using only part of the data\"\"\"\n    dataset = CustomDataset(label_file, transform_img)\n    \n    loader_settings = {\n        'dataset': dataset,\n        'batch_size': batch_size,\n        'shuffle': shuffle,\n        'num_workers': num_workers,\n        'pin_memory': torch.cuda.is_available()\n    }\n    \n    \n    if 0 < max_images < len(dataset):        \n        negative_idxs = dataset.negative_idxs\n        positive_idxs = dataset.positive_idxs\n        \n        np.random.shuffle(negative_idxs)\n        np.random.shuffle(positive_idxs)\n        \n        # don't use all the images in the dataset, equal parts positive and negative\n        negative_idxs = negative_idxs[:int(max_images/2)]\n        positive_idxs = positive_idxs[:int(max_images/2)]\n        \n        idxs = np.concatenate((positive_idxs, negative_idxs))\n        np.random.shuffle(idxs)\n        \n        indices = torch.from_numpy(idxs)\n        loader_settings['sampler'] = SubsetRandomSampler(indices)\n        loader_settings['shuffle'] = False\n        \n    return DataLoader(**loader_settings)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:14:21.029975Z","iopub.execute_input":"2021-06-24T06:14:21.030399Z","iopub.status.idle":"2021-06-24T06:14:21.054187Z","shell.execute_reply.started":"2021-06-24T06:14:21.030357Z","shell.execute_reply":"2021-06-24T06:14:21.053349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Review Dataset\n\n> **Source:** https://www.kaggle.com/hgunraj/covidx-ct-starter-code","metadata":{}},{"cell_type":"code","source":"pd.read_csv('../input/covidxct/metadata.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:04:33.332164Z","iopub.execute_input":"2021-06-24T06:04:33.332547Z","iopub.status.idle":"2021-06-24T06:04:33.399094Z","shell.execute_reply.started":"2021-06-24T06:04:33.332507Z","shell.execute_reply":"2021-06-24T06:04:33.398377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set paths\nimage_dir = '/kaggle/input/covidxct/2A_images'\nlabel_file = '/kaggle/input/covidxct/val_COVIDx_CT-2A.txt'\n\n# Load labels\nfnames, classes, bboxes = CustomDataset(label_file).load_labels()\n\n#####################################################################\n#                        Preview Some Images\n#####################################################################\n\n# Select cases to view\nindices = np.random.choice(list(range(len(fnames))), 9)\n\n# Show a grid of 9 images\nfig, axes = plt.subplots(3, 3, figsize=(16, 16))\nclass_names = ('Normal', 'COVID-19')\nfor index, ax in zip(indices, axes.ravel()):\n    # Load the CT image\n    image_file = os.path.join(image_dir, fnames[index])\n    image = cv2.imread(image_file, cv2.IMREAD_UNCHANGED)\n\n    # Overlay the bounding box\n    image = np.stack([image]*3, axis=-1)  # make image 3-channel\n    bbox = bboxes[index]\n    cv2.rectangle(image, bbox[:2], bbox[2:], color=(255, 0, 0), thickness=3)\n\n    # Display\n    cls = classes[index]\n    ax.imshow(image)\n    ax.set_title('Class: {} ({})'.format(class_names[cls], cls))\nplt.savefig('example.png', dpi=300)\nplt.show()\n\n#####################################################################\n#                      Preview the Preprocessing\n#####################################################################\nprint('\\n\\nPreprocessing:')\n\nindex = indices[3]\n\n# Load the CT image\nimage_file = os.path.join(image_dir, fnames[index])\nimage = cv2.imread(image_file)\n\n# Overlay the bounding box\nbbox = bboxes[index]\nimage_with_bb = np.copy(image)\ncv2.rectangle(image_with_bb, bbox[:2], bbox[2:], color=(255, 0, 0), thickness=3)\n\n# crop\nimage = image[::,::,::-1]\nbbox #xmin ymin xmax ymax\ncropped_im = image[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n\n# resize\ndim = (384, 384)\nresized_im = cv2.resize(cropped_im, dim, interpolation=cv2.INTER_AREA)\n\nplt.figure(figsize=(10, 4), dpi=100)\n\nplt.subplot(1,3,1)\nplt.imshow(image_with_bb)\nplt.axis('off')\nplt.title('Raw image')\n\nplt.subplot(1,3,2)\nplt.imshow(cropped_im)\nplt.axis('off')\nplt.title('Cropped image')\n\nplt.subplot(1,3,3)\nplt.imshow(resized_im)\nplt.axis('off')\nplt.title('Cropped + Resized image')\n\nplt.tight_layout()\nplt.savefig('preprocessing.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:04:33.400241Z","iopub.execute_input":"2021-06-24T06:04:33.400581Z","iopub.status.idle":"2021-06-24T06:04:43.761253Z","shell.execute_reply.started":"2021-06-24T06:04:33.400545Z","shell.execute_reply":"2021-06-24T06:04:43.760383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preview_dataset(plot_batch_size=4):\n    plot_label_file = f'{PATH}/train_COVIDx_CT-2A.txt'\n    plot_dataset = CustomDataset(plot_label_file)\n    plot_dataloader = DataLoader(\n        plot_dataset, batch_size=plot_batch_size, shuffle=True\n    )\n\n    plt.figure(figsize=(10,3), dpi=100)\n\n    for imgs, labels in plot_dataloader:\n        print(f\"Batch of images has shape: {imgs.shape}\")\n        print(f\"Batch of labels has shape: {labels.shape}\")\n        for i in range(len(imgs)):\n            plt.subplot(1, plot_batch_size, i+1)\n            img = imgs[i].permute(1, 2, 0)\n            img = img.numpy().astype(np.uint8)\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(f'{labels[i].numpy()[0]}')\n        break\n\n    plt.tight_layout()\n    plt.show()\n    \npreview_dataset()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:04:43.762843Z","iopub.execute_input":"2021-06-24T06:04:43.763203Z","iopub.status.idle":"2021-06-24T06:04:44.924108Z","shell.execute_reply.started":"2021-06-24T06:04:43.763162Z","shell.execute_reply":"2021-06-24T06:04:44.923235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Vision Transformer\n\n> **Source:** https://github.com/lukemelas/PyTorch-Pretrained-ViT","metadata":{}},{"cell_type":"code","source":"class PretrainedViT:\n    def __init__(self, model_name='B_16_imagenet1k'):\n        self.model = ViT(model_name, pretrained=True)\n        self.freeze_all_layers()\n\n    def freeze_all_layers(self):\n        \"\"\"Freeze the model so it's won't change the pretrained whights\"\"\"\n        for key, module in self.model._modules.items():\n            for param in module.parameters():\n                param.requires_grad = False\n\n    def unfreeze_fully_connected(self):\n        \"\"\"Unfreeze the whights of the last fully connected layer\"\"\"\n        self.model.fc.weight.requires_grad = True\n        self.model.fc.bias.requires_grad = True\n\n    def replace_fully_connected(self, block):\n        \"\"\"Replace the last fully connected layer with a given block\"\"\"\n        self.model.fc = block\n","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:04:44.926685Z","iopub.execute_input":"2021-06-24T06:04:44.927055Z","iopub.status.idle":"2021-06-24T06:04:44.933492Z","shell.execute_reply.started":"2021-06-24T06:04:44.92701Z","shell.execute_reply":"2021-06-24T06:04:44.93259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"############################################################\n#                    Initialize the Model\n############################################################\nvit = PretrainedViT()\nvit.unfreeze_fully_connected()\n\n# the original model have 1000 classes. Here only 1 are needed so \n# the shape of the last fully-connected layer need to change.\nvit.replace_fully_connected(\n    nn.Sequential(\n        nn.Linear(vit.model.fc.in_features, 2048),\n        nn.ReLU(inplace=True),\n        nn.Linear(2048, 1),\n        nn.Sigmoid()\n    )\n)\n\nmodel = vit.model.to(device)\nmodel_name = 'pretrained_ViT'\n\nsummary(model, (3, IMAGE_DIM, IMAGE_DIM))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T08:09:13.878402Z","iopub.execute_input":"2021-06-18T08:09:13.878759Z","iopub.status.idle":"2021-06-18T08:09:15.745494Z","shell.execute_reply.started":"2021-06-18T08:09:13.878723Z","shell.execute_reply":"2021-06-18T08:09:15.74467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def performance_metrics(model, dataloader):\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for data in tqdm(dataloader, leave=False):\n            val_images, val_labels = data\n\n            val_images = val_images.to(device).to(torch.float32)\n            val_labels = val_labels.to(device).to(torch.float32)\n\n            y_pred = model(val_images)\n            y_true = val_labels\n\n            predictions += list((y_pred > 0.5).to(torch.int32).squeeze().cpu().numpy())\n            true_labels += list(y_true.squeeze().to(torch.int32).cpu().numpy())\n\n    N = len(true_labels)\n    true = np.array(predictions) == np.array(true_labels)\n    false = np.array(predictions) != np.array(true_labels)\n    positive = np.array(predictions) > 0.5\n    negative = np.array(predictions) < 0.5\n    tn = 100 * np.sum(true * negative) / N\n    tp = 100 * np.sum(true * positive) / N\n    fn = 100 * np.sum(false * negative) / N\n    fp = 100 * np.sum(false * positive) / N\n    accuracy = np.sum(true) / N\n    \n    return tn, tp, fn, fp, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:11:25.452296Z","iopub.execute_input":"2021-06-18T07:11:25.452663Z","iopub.status.idle":"2021-06-18T07:11:25.463722Z","shell.execute_reply.started":"2021-06-18T07:11:25.452623Z","shell.execute_reply":"2021-06-18T07:11:25.46292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################################\n#                      Dataloaders\n############################################################\n\nbatch_size = 4\n\ntrain_label_file = f'{PATH}/train_COVIDx_CT-2A.txt'\ntrain_dataloader = get_dataloader(label_file=train_label_file, \n                                  batch_size=batch_size,\n                                  max_images=10000)\n    \nval_label_file = f'{PATH}/val_COVIDx_CT-2A.txt'\nval_dataloader = get_dataloader(label_file=val_label_file, \n                                batch_size=batch_size,\n                                max_images=10000)\n\ntest_label_file = f'{PATH}/test_COVIDx_CT-2A.txt'\ntest_dataloader = get_dataloader(label_file=test_label_file, \n                                 batch_size=batch_size,\n                                 max_images=10000)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:02:05.054312Z","iopub.execute_input":"2021-06-19T08:02:05.054702Z","iopub.status.idle":"2021-06-19T08:02:05.854378Z","shell.execute_reply.started":"2021-06-19T08:02:05.054669Z","shell.execute_reply":"2021-06-19T08:02:05.853318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################################\n#                      Hyperparametrs\n############################################################\nepochs = 3\n\nlearning_rate = 2e-3\n\n# select loss\ncriterion = nn.BCELoss()\n\n# select optimizer\noptimizer = torch.optim.Adam(\n    params=model.parameters(),\n    lr=learning_rate,\n    amsgrad=True\n)\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T08:11:05.747144Z","iopub.execute_input":"2021-06-18T08:11:05.747476Z","iopub.status.idle":"2021-06-18T08:11:06.468641Z","shell.execute_reply.started":"2021-06-18T08:11:05.747444Z","shell.execute_reply":"2021-06-18T08:11:06.467747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nepochs_log = []\n\nfor epoch in range(1, epochs+1):\n    running_loss = 0.0\n    epoch_time = time.time()\n\n    with tqdm(train_dataloader, unit=\"batch\", leave=False) as tepoch:\n        for images, labels in tepoch:\n            tepoch.set_description(f\"{elapsed_time(start_time)} | Epoch {epoch:02d}/{epochs}\")\n            \n            images = images.to(device).to(torch.float32)\n            labels = labels.to(device).to(torch.float32).squeeze()\n            \n            pred = model(images).squeeze()       # forward pass\n            loss = criterion(pred, labels)       # calculate the loss\n            optimizer.zero_grad()                # zero the parameter gradients\n            loss.backward()                      # backpropagation\n            optimizer.step()                     # update parameters\n\n            running_loss += loss.data.item()\n        \n        # update learning rate\n        curr_lr = scheduler.get_last_lr()\n        scheduler.step()                         \n        \n        # calculate loss and accuracy\n        running_loss /= len(train_dataloader)\n\n        tn, tp, fn, fp, accuracy = performance_metrics(model, val_dataloader)\n\n        # show results to tqdm\n        tepoch.refresh()\n        tepoch.set_description(f\"{elapsed_time(start_time)} | Epoch {epoch:02d}/{epochs}\")\n        \n        epoch_results = [\n            f'{elapsed_time(start_time)}',\n            f'Epoch {epoch:02d}/{epochs}',\n            f'lr: {curr_lr[0]:.1e}',\n            f'Loss: {running_loss:.3f}',\n            f'Validation  -  accuracy: {100*accuracy:.02f}%,  tp: {tp:2.01f}%,  tn: {tn:.01f}%,  fp: {fp:.01f}%,  fn: {fn:.01f}%'\n        ]\n        epochs_log.append(epoch_results)\n        print(*epoch_results, sep=' | ')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T08:11:25.125639Z","iopub.execute_input":"2021-06-18T08:11:25.125973Z","iopub.status.idle":"2021-06-18T10:32:17.040148Z","shell.execute_reply.started":"2021-06-18T08:11:25.125944Z","shell.execute_reply":"2021-06-18T10:32:17.039278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp = [float(i[-1].split('tp: ')[1].split(' ')[0].replace('%,', '')) for i in epochs_log]\ntn = [float(i[-1].split('tn: ')[1].split(' ')[0].replace('%,', '')) for i in epochs_log]\nfp = [float(i[-1].split('fp: ')[1].split(' ')[0].replace('%,', '')) for i in epochs_log]\nfn = [float(i[-1].split('fn: ')[1].split(' ')[0].replace('%' , '')) for i in epochs_log]\n\nA = np.array(tp)\nB = np.array(tn)\nC = np.array(fp)\nD = np.array(fn)\n\nTotal = A + B + C + D\n\nnorm_A = A / Total * 100\nnorm_B = B / Total * 100\nnorm_C = C / Total * 100\nnorm_D = D / Total * 100\n\nPos = range(1, epochs+1)\n\ncolors = plt.get_cmap('RdYlGn')(np.linspace(0.15, 0.85, 8))\n        \n\n# plt.figure(figsize=(8,5), dpi=100)\nfig, ax = plt.subplots()\n\nplt.bar(Pos, norm_A, color=colors[7], edgecolor='white', label='TP')\nplt.bar(Pos, norm_B, bottom = norm_A, color=colors[6], edgecolor='white', label='TN')\nplt.bar(Pos, norm_C, bottom = norm_A + norm_B, color=colors[1], edgecolor='white', label='FP')\nplt.bar(Pos, norm_D, bottom = norm_A + norm_B + norm_C, color=colors[0], edgecolor='white', label='FN')\nplt.plot(Pos, norm_A+norm_B, color='black', label='Accuracy', marker='+', linewidth=2)\n\nplt.xlabel('Epoch')\nplt.ylabel('Validation Performance')\nax.yaxis.set_minor_locator(MultipleLocator(10))   # select minor ticks on y axis\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=5, mode=\"expand\", borderaxespad=0., frameon=False)\n     \nplt.savefig('results.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:46:04.149217Z","iopub.execute_input":"2021-06-14T20:46:04.149593Z","iopub.status.idle":"2021-06-14T20:46:04.821335Z","shell.execute_reply.started":"2021-06-14T20:46:04.149563Z","shell.execute_reply":"2021-06-14T20:46:04.820522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_label_file = f'{PATH}/test_COVIDx_CT-2A.txt'\ntest_dataloader = get_dataloader(label_file=test_label_file, \n                                 batch_size=batch_size,\n                                 max_images=5000)\n\ntn, tp, fn, fp, accuracy = performance_metrics(model, test_dataloader)\nprint(f'Test  -  accuracy: {100*0.7816:.02f}%,  tp: {39.54:2.01f}%,  tn: {38.62:.01f}%,  fp: {11.38:.01f}%,  fn: {10.46:.01f}%')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:51:09.043378Z","iopub.execute_input":"2021-06-14T20:51:09.043755Z","iopub.status.idle":"2021-06-14T20:51:09.049196Z","shell.execute_reply.started":"2021-06-14T20:51:09.043724Z","shell.execute_reply":"2021-06-14T20:51:09.048196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract Features","metadata":{}},{"cell_type":"markdown","source":"### VGG Features","metadata":{}},{"cell_type":"code","source":"############################################################\n#                    Initialize the Model\n############################################################\nmodel = models.vgg16(pretrained=True).to(device)\nfor param in model.parameters():\n    param.requires_grad = False\n\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = nn.Identity()\n\n############################################################\n#                   Initialize the Dataset\n############################################################\ntrain_label_file = f'{PATH}/train_COVIDx_CT-2A.txt'\ntrain_dataloader = get_dataloader(label_file=train_label_file, \n                                  batch_size=1,\n                                  max_images=60000,\n                                  transform_img=True)\n\nval_label_file = f'{PATH}/val_COVIDx_CT-2A.txt'\nval_dataloader = get_dataloader(label_file=val_label_file, \n                                batch_size=batch_size,\n                                max_images=10000,\n                                transform_img=True)\n\n############################################################\n#                     Extract Features\n############################################################\npredictions = np.zeros((10000, num_features))\ntrue_values = np.zeros((10000, 1))\n\nwith torch.no_grad():\n    for i, data in enumerate(tqdm(val_dataloader)):\n        images, labels = data\n\n        images = images.to(torch.float32)\n        labels = labels.to(torch.float32)\n\n        y_pred = model(images.to(device)).squeeze().cpu().numpy()\n        y_true = labels.to(device).squeeze().cpu().data.item()\n        \n        predictions[i] = y_pred\n        true_values[i] = y_true","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:53:09.207069Z","iopub.execute_input":"2021-06-24T06:53:09.207414Z","iopub.status.idle":"2021-06-24T06:57:03.209422Z","shell.execute_reply.started":"2021-06-24T06:53:09.207379Z","shell.execute_reply":"2021-06-24T06:57:03.208582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ViT Features","metadata":{}},{"cell_type":"code","source":"############################################################\n#                    Initialize the Model\n############################################################\nnum_features = 768\nvit = PretrainedViT()\nvit.replace_fully_connected(nn.Identity())\nmodel = vit.model.to(device)\n\n############################################################\n#                   Initialize the Dataset\n############################################################\ntrain_label_file = f'{PATH}/train_COVIDx_CT-2A.txt'\ntrain_dataloader = get_dataloader(label_file=train_label_file, \n                                  batch_size=1,\n                                  max_images=60000)\n\nval_label_file = f'{PATH}/val_COVIDx_CT-2A.txt'\nval_dataloader = get_dataloader(label_file=val_label_file, \n                                batch_size=batch_size,\n                                max_images=10000)\n\n############################################################\n#                     Extract Features\n############################################################\npredictions = np.zeros((10000, num_features))\ntrue_values = np.zeros((10000, 1))\n\nwith torch.no_grad():\n    for i, data in enumerate(tqdm(val_dataloader)):\n        images, labels = data\n\n        images = images.to(torch.float32)\n        labels = labels.to(torch.float32)\n\n        y_pred = model(images.to(device)).squeeze().cpu().numpy()\n        y_true = labels.to(device).squeeze().cpu().data.item()\n        \n        predictions[i] = y_pred\n        true_values[i] = y_true","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Save Features:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(predictions).to_csv(\"val_predictions_vgg.csv\", header=None, index=None)\npd.DataFrame(true_values).to_csv(\"val_true_values_vgg.csv\", header=None, index=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:04:39.013342Z","iopub.execute_input":"2021-06-24T07:04:39.013735Z","iopub.status.idle":"2021-06-24T07:05:25.601559Z","shell.execute_reply.started":"2021-06-24T07:04:39.013697Z","shell.execute_reply":"2021-06-24T07:05:25.600663Z"},"trusted":true},"execution_count":null,"outputs":[]}]}