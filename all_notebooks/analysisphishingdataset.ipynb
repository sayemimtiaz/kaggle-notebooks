{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n \nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import friedmanchisquare\nfrom scipy import stats\nfrom sklearn.metrics import confusion_matrix\n \n \nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/website-phishing-data-set/Website Phishing.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset description","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the phishy class by the value 2\n# I decided to use MLP so it would be better if\n# the classes were all non negative values\n\ndf['Result'] = df['Result'].astype(str).replace('-1','2').astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the data\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Calculating if there are null or na values in the dataset\nprint('Verifying null and na data')\nprint()\nprint(df.isna().any())\n\nprint()\nprint(df.isnull().any())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtaining some additional information about the dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graphics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to plot the class distribution by feature\n\ndef plot_class_distribution(feature, color, data, labels):\n\n  class_info = data[feature].value_counts().sort_index()\n  \n  #x = class_info.index\n  x = labels\n  x_pos = [i for i, _ in enumerate(x)]\n\n  y = class_info.values\n\n\n  fig, ax = plt.subplots()\n  rects1 = ax.bar(x_pos, y, color=color)\n  # helper function to show the number of examples in each bar\n  def autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%.f' % float(height),\n        ha='center', va='bottom')\n  autolabel(rects1)\n\n\n  plt.ylabel(\"Number of Examples\")\n  plt.title(feature + \" examples distribution\\n\")\n  plt.xticks(x_pos, x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"legit = len(df[df['Result'] == 1])\nsusp = len(df[df['Result'] == 0])\nphishy = len(df[df['Result'] == 2])\n\nsize=[legit, susp, phishy]\nnames = ['Legitimate', 'Suspicious', 'Phishy']\n \n# Create a circle for the center of the plot\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\n\nplt.pie(size, labels=names, colors=['blue','pink','red'])\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.title('Class Distribution')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class distribution\n\nlegit = len(df[df['Result'] == 1])\nsusp = len(df[df['Result'] == 0])\nphishy = len(df[df['Result'] == 2])\n\nlabels = 'Legitimate', 'Suspicious', 'Phishy'\nsizes = [legit, susp, phishy]\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')  \nplt.title('Class Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class distribution\n\nlegit = len(df[df['Result'] == 1])\nsusp = len(df[df['Result'] == 0])\nphishy = len(df[df['Result'] == 2])\n\nx = ['Legitimate', 'Suspicious', 'Phishy']\nx_pos = [i for i, _ in enumerate(x)]\n\ny = [legit, susp, phishy]\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x_pos, y, color='lightblue')\n\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of Examples\")\nplt.title(\"Class distribution\\n\")\nplt.xticks(x_pos, x)\n\ndef autolabel(rects):\n\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%.f' % float(height),\n        ha='center', va='bottom')\nautolabel(rects1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the values in each feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nThe columns in the dataframe represent different features related to phishing. The values associated with each one of the columns are detailed in the [article](https://www.sciencedirect.com/science/article/abs/pii/S0957417414001481) associated with its Kaggle database, and they are described below:\n\n1.   SFH - Server Form Handler After the information is send the website sends them to a server to process the data. Phishy websites usuallu let the SFH field blank or redirects to another domain.\n2.   Pop up Window -  Legitimate websites don't use pop up windows to validate users' information. \n3.   SSL final state - Reliable webpages use the HTTP protocol, on the other hand malicious websites may use a fake HTTP procotol or not use it at all.\n4.   Request URL - Malicious websites usually load the page content from a different URL than the original website URL.\n5.   URL of anchor - Malicious websites usually have links that point to different webpages.\n6.   Web traffic - Legitimate websites usually have a lower number of visits than the malicious ones.  **\n7.   URL length - URLs with length bigger than 75 characters  are considered features from phishy websites.\n8.   Age of domain - Websites with less than a year of existece are considered suspicious **\n9.   Having IP Address - The presence of IP address in the website URL is associated with malicious websites.\n\n** Some of the features were defined in a differet way in the description of the features from the article than the logical rules in it, in these cases I considered the description instead of the logical rule.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the column values according to the description from the paper\n\n\nsfh_labels = ['Empty SFH', 'SFH different domain', 'Valid SFH']\nplot_class_distribution('SFH', 'silver', df, sfh_labels)\n\npop_labels = ['Rightclick disabled', 'Rightclicl with alert', 'No pop-up']\nplot_class_distribution('popUpWidnow', 'gold', df, pop_labels)\n\nssl_labels = ['Nor HTTP nor trusted', 'HTTP and nottrusted', 'HTTP and trusted']\nplot_class_distribution('SSLfinal_State', 'silver', df, ssl_labels)\n\nrequest_labels = ['req_URL > 61%',  '22 <= req_URL <= 61%', 'req_URL < 22%']\nplot_class_distribution('Request_URL', 'gold', df, request_labels)\n\nanchor_labels = [ 'Acr_URL>67%',' 31%<=Acr_URL<=67%', 'Acr_URL<31%']\nplot_class_distribution('URL_of_Anchor', 'silver',df, anchor_labels)\n\nweb_labels = ['wtraffic>150K', 'wtraffic<=150K', 'wtraffic<150K']\nplot_class_distribution('web_traffic', 'gold', df, web_labels)\n\nurl_labels = ['len > 75', '54 <= len <= 75', 'len < 54']\nplot_class_distribution('URL_Length', 'silver', df, url_labels)\n\nage_labels = ['age < 1 year', 'age > 1 year']\nplot_class_distribution('age_of_domain', 'lightblue', df, age_labels)\n\nip_labels = ['No IPAdress URL','URL IPaddress']\nplot_class_distribution('having_IP_Address', 'lightblue', df, ip_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph grouping the presence of IP address with the classes\n \ndfip = df[df['having_IP_Address'] == 1]\ndfnoip = df[df['having_IP_Address'] == 0]\n \nlabelsip = dfip['Result'].value_counts().index\nvaluesip = dfip['Result'].value_counts().values\n \nlabelsnoip = dfnoip['Result'].value_counts().index\nvaluesnoip = dfnoip['Result'].value_counts().values\n \n \nbarWidth = 0.25\n \nbars1 = [ valuesip[0], valuesnoip[0]]\nbars2 = [valuesip[1], valuesnoip[1]]\nbars3 = [valuesip[2], valuesnoip[2]]\n \n \n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \n# Make the plot\nplt.bar(r1, bars1, color='#E4D9FF', width=barWidth, edgecolor='white', label='Phishy')\nplt.bar(r2, bars2, color='lightblue', width=barWidth, edgecolor='white', label='Legitimate')\nplt.bar(r3, bars3, color='silver', width=barWidth, edgecolor='white', label='Suspicious')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Class distribution by presence of IP addres in the URL', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(bars1))],['With IPaddress', 'Without IPaddress'] )\n \n \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph with the relation between the Phishy class and the feature having IP address, and \n# URL length\ndfphishy = df[df['Result'] == 2]\n\ndfphishy_ip = dfphishy[dfphishy['having_IP_Address'] == 1]\ndfphishy_noip = dfphishy[dfphishy['having_IP_Address'] == 0]\n\n\nip_values_url1_phis = list(dfphishy_ip['URL_Length'].value_counts().values)\nnoip_values_url1_phis = list(dfphishy_noip['URL_Length'].value_counts().values)\n\nlabels = ['URL < 54', '54 <= URL <= 75', 'URL > 75']\nnoip = noip_values_url1_phis\nip = ip_values_url1_phis\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, ip, width, label='With IPaddress')\nrects2 = ax.bar(x + width/2, noip, width, label='Without IPaddress')\n\n\nax.set_ylabel('Examples')\nax.set_title('Data from the Phishy class')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dflegitimate = df[df['Result'] == 1]\n\ndflegit_ip = dflegitimate[dflegitimate['having_IP_Address'] == 1]\ndflegit_noip = dflegitimate[dflegitimate['having_IP_Address'] == 0]\n\n\nip_values_url1_legit = list(dflegit_ip['URL_Length'].value_counts().values)\nnoip_values_url1_legit = list(dflegit_noip['URL_Length'].value_counts().values)\n#ip_values_url1_legit.append(0)\n\nlabels = ['URL < 54', '54 <= URL <= 75', 'URL > 75']\nnoip = noip_values_url1_legit\nip = ip_values_url1_legit\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, ip, width, label='With IPaddress')\nrects2 = ax.bar(x + width/2, noip, width, label='Without IPaddress')\n\n\nax.set_ylabel('Examples')\nax.set_title('Data from Legitimate class')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfsuspicious = df[df['Result'] == 0]\n\ndfsusp_ip = dfsuspicious[dfsuspicious['having_IP_Address'] == 1]\ndfsusp_noip = dfsuspicious[dfsuspicious['having_IP_Address'] == 0]\n\n\nip_values_url1_susp = list(dfsusp_ip['URL_Length'].value_counts().values)\nnoip_values_url1_susp = list(dfsusp_noip['URL_Length'].value_counts().values)\nip_values_url1_susp.append(0)\n\nlabels = ['URL < 54', '54 <= URL <= 75', 'URL > 75']\nnoip = noip_values_url1_susp\nip = ip_values_url1_susp\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, ip, width, label='With IPaddress')\nrects2 = ax.bar(x + width/2, noip, width, label='Without IPaddress')\n\n\nax.set_ylabel('Examples')\nax.set_title('Data from the Suspicious class')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Helper functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"######## Classification Functions ##########\n \n \n# Function to load the classifiers\n \ndef initialize_classifiers():\n  # KNN k = 3\n  knn3 = KNeighborsClassifier(n_neighbors=3)\n \n  # KNN k = 5\n  knn5 = KNeighborsClassifier(n_neighbors=5)\n \n  # KNN k = 7\n  knn7 = KNeighborsClassifier(n_neighbors=7)\n \n  # KNN ponderado k = 3\n  knnp3 = KNeighborsClassifier(n_neighbors=3, weights='distance',metric='euclidean')\n \n  \n  # KNN ponderado k = 5\n  knnp5 = KNeighborsClassifier(n_neighbors=5, weights='distance',metric='euclidean')\n \n \n  # KNN ponderado k = 7\n  knnp7 = KNeighborsClassifier(n_neighbors=7, weights='distance',metric='euclidean')\n \n  # Support Vector Machine - função de kernel linear\n  svmLinear = SVC(kernel='linear')\n \n  # Support Vector Machine - função de kernel RBF\n  svmRBF = SVC(kernel='rbf')\n  \n  # Decision Tree \n  decisionTree = DecisionTreeClassifier()\n \n  # Random Forest\n  randomForest = RandomForestClassifier()\n \n  # Naïve Bayes\n  naiveBayes = GaussianNB()\n  \n  # Logistic Regression\n \n  logisticRegression = LogisticRegression()\n \n  # MLP \n  modelo = Sequential()\n  modelo.add(Dense(units=64, activation='relu',kernel_initializer='random_uniform', input_dim=9))\n  modelo.add(Dense(units=32, activation='relu',kernel_initializer='random_uniform'))\n  modelo.add(Dense(units=16, activation='relu',kernel_initializer='random_uniform'))\n  modelo.add(Dense(units=3, activation='softmax'))\n \n  optimizer = keras.optimizers.Adam(lr = 0.001, decay =0.0001, clipvalue= 0.5)\n  modelo.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n \n \n  return [knn3, knn5, knn7,svmLinear, svmRBF, decisionTree, randomForest, naiveBayes, logisticRegression, modelo,knnp3,knnp5,knnp7]\n \n# Function to train and store the evaluation results from the classifiers \n \ndef train_and_evaluate(class_train, noclass_train, class_test, noclass_test, alg, dicio, epochs = 400, batch_size = 64, verb = False):\n \n  if dicio['modelname'] == 'MLP':\n    \n    class_train_categorical = keras.utils.to_categorical(class_train, num_classes=3)\n    class_test_categorical = keras.utils.to_categorical(class_test, num_classes=3)\n    tempo_inicial = time.time()\n    alg.fit(noclass_train, class_train_categorical,batch_size=batch_size, epochs=epochs, verbose= verb)\n    tempo_fim = time.time()\n    predicted = [np.argmax(pred) for pred in alg.predict(noclass_test)]\n \n  else:\n    tempo_inicial = time.time()\n    alg.fit(noclass_train, class_train)\n    tempo_fim = time.time()\n \n    predicted = alg.predict(noclass_test)\n \n  dicio['acc'].append(accuracy_score(class_test, predicted))\n  dicio['fscore'].append(f1_score(class_test, predicted, average='macro'))\n  dicio['precision'].append(precision_score(class_test, predicted, average='macro'))\n  dicio['recall'].append(recall_score(class_test, predicted, average='macro'))\n  dicio['tempo'].append(tempo_fim - tempo_inicial)\n  dicio['cm'].append(confusion_matrix(class_test,predicted))\n \n# Creating dictionaies to store the metric data to each one of the classifiers\n \ndef create_dictios():\n \n  knn3_dc = {'modelname':'KNN3','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  knn5_dc = {'modelname':'KNN5','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  knn7_dc = {'modelname':'KNN7','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  knnp3_dc = {'modelname':'KNNP3','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  knnp5_dc = {'modelname':'KNNP5','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  knnp7_dc = {'modelname':'KNNP7','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  svmLinear_dc = {'modelname':'SVML','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  svmRBF_dc = {'modelname':'SVMR','acc': [], 'fscore': [], 'precision': [], 'recall': [] , 'tempo':[], 'cm': []}\n  decisionTree_dc = {'modelname':'DT','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  randomForest_dc = {'modelname':'RF','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  naive_dc = {'modelname':'NB','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  logreg_dc = {'modelname':'LR','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n  mlp_dc = {'modelname':'MLP','acc': [], 'fscore': [], 'precision': [], 'recall': [], 'tempo':[], 'cm': []}\n \n  return [knn3_dc, knn5_dc, knn7_dc,svmLinear_dc, svmRBF_dc, decisionTree_dc, randomForest_dc,  naive_dc, logreg_dc, mlp_dc, knnp3_dc,knnp5_dc,knnp7_dc]\n \n \n######## Data visualization functions ##########\n \n# Creating boxplot with the data from a metric\n \ndef create_boxplot(df, metric_name):\n  df.boxplot(column= list(df.columns) , figsize=(12, 8))\n  plt.xlabel('Algorithms')\n  plt.ylabel(metric_name)\n  plt.show()\n \n  print()\n  print()\n  print()\n \n# Creating the statistic test to a pair of classifiers\n \ndef create_statistic_test(measure, df):\n \n  two_bests = list(df.mean().sort_values(ascending = False).index[:2])\n  best = df[two_bests[0]].values\n  secondbest = df[two_bests[1]].values\n \n  pvalue = stats.ttest_ind(best, secondbest)\n  if pvalue[1] < 0.05:\n    print('The algorithms '+ two_bests[0] + ' and ' + two_bests[1] + ' are statistically different using the metric ' + measure)\n    print('P-value: ' + str(pvalue[1]))\n  else:\n    print('The algorithms '+ two_bests[0] + ' and ' + two_bests[1] + ' are  NOT statistically different using the metric '  + measure)\n    print('P-value: ' + str(pvalue[1]))\n \n# Plotting the mean, median and standard deviation to each one of the classifiers used\n \ndef plot_stats_values(df, plot_name):\n  media = df.mean()\n  mediana = df.median()\n  desviopadrao = df.std()\n \n  barWidth = 0.25\n \n  bars1 = media.values\n  bars2 = mediana.values\n  bars3 = desviopadrao.values\n \n  labels = media.index\n  \n  # Set position of bar on X axis\n  r1 = np.arange(len(bars1))\n  r2 = [x + barWidth for x in r1]\n  r3 = [x + barWidth for x in r2]\n  \n  # Make the plot\n  plt.bar(r1, bars1, color='#E4D9FF', width=barWidth, edgecolor='white', label='Mean')\n  plt.bar(r2, bars2, color='lightblue', width=barWidth, edgecolor='white', label='Median')\n  plt.bar(r3, bars3, color='silver', width=barWidth, edgecolor='white', label='Std')\n  \n  # Add xticks on the middle of the group bars\n  plt.xlabel(plot_name, fontweight='bold')\n  plt.xticks([r + barWidth for r in range(len(bars1))],labels )\n  \n  plt.rcParams[\"figure.figsize\"] = (20,3)\n  plt.legend()\n  plt.show()\n \n# Plotting the metrics of accuracy, precision, fscore, recall and execution time to\n# the three best classifiers\n\ndef plot_best_algs_result(df_acc, df_precision, df_fscore, df_recall, df_tempo):\n  \n  # searching fot the best classifiers using the accuracy mean\n  bests = list(df_acc.mean().sort_values(ascending = False).index[:3])\n  df_mean = df_acc.mean()\n  media_precision = df_precision.mean()\n  media_fscore = df_fscore.mean()\n  media_recall = df_recall.mean()\n  \n  best = df_mean[bests[0]]\n  secondbest = df_mean[bests[1]]\n  thirdbest = df_mean[bests[2]]\n \n \n  bests_acc = []\n  bests_acc.append(best)\n  bests_acc.append(secondbest)\n  bests_acc.append(thirdbest)\n \n  bests_precision = []\n  bests_precision.append(media_precision[bests[0]])\n  bests_precision.append(media_precision[bests[1]])\n  bests_precision.append(media_precision[bests[2]])\n \n  bests_recall = []\n  bests_recall.append(media_recall[bests[0]])\n  bests_recall.append(media_recall[bests[1]])\n  bests_recall.append(media_recall[bests[2]])\n \n  bests_fscore = []\n  bests_fscore.append(media_fscore[bests[0]])\n  bests_fscore.append(media_fscore[bests[1]])\n  bests_fscore.append(media_fscore[bests[2]])\n  \n \n  barWidth = 0.15\n \n  bars1 = bests_acc\n  bars2 = bests_precision\n  bars3 = bests_fscore\n  bars4 = bests_recall\n \n  labels = bests\n \n  # Set position of bar on X axis\n  r1 = np.arange(len(bars1))\n  r2 = [x + barWidth for x in r1]\n  r3 = [x + barWidth for x in r2]\n  r4 = [x + barWidth for x in r3]\n \n \n  # Make the plot\n  plt.bar(r1, bars1, color='#E4D9FF', width=barWidth, edgecolor='white', label='Accuracy')\n  plt.bar(r2, bars2, color='lightblue', width=barWidth, edgecolor='white', label='Precision')\n  plt.bar(r3, bars3, color='silver', width=barWidth, edgecolor='white', label='F1-score')\n  plt.bar(r4, bars4, color='gold', width=barWidth, edgecolor='white', label='Recall')\n \n \n  # Add xticks on the middle of the group bars\n  plt.xlabel(xlabel='Best Algorithms', fontweight='bold')\n  plt.xticks([r + barWidth for r in range(len(bars1))],labels )\n \n \n \n  plt.rcParams[\"figure.figsize\"] = (20,10)\n  plt.legend()\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the classifiers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into the features and classes\ndf_noclass = df.iloc[:, 0:9]\n\ndf_class = df.iloc[:, 9]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification with k-fold cross validation\n\n# Initializing the classifiers\nlistmodels = initialize_classifiers()\n\n# Creating the dictionaries that will store the classifiers' results \nlistofdicts = create_dictios()\n\n# Initializing the K-fold\nkfold = StratifiedKFold(10, True, 1)\n\nc = kfold.split(df_noclass, df_class)\n\nfor train_index, test_index in c:\n\n  noclass_train, noclass_test =np.array(df_noclass.iloc[train_index]) ,np.array(df_noclass.iloc[test_index])\n  class_train, class_test = np.array(df_class.iloc[train_index]), np.array(df_class.iloc[test_index])\n\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[0], listofdicts[0])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[1], listofdicts[1])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[2], listofdicts[2])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[3], listofdicts[3])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[4], listofdicts[4])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[5], listofdicts[5])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[6], listofdicts[6])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[7], listofdicts[7])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[8], listofdicts[8])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[9] , listofdicts[9], epochs = 400, batch_size = 64, verb = False)\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[10], listofdicts[10])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[11] , listofdicts[11])\n  train_and_evaluate(class_train, noclass_train, class_test, noclass_test, listmodels[12] , listofdicts[12])\n\n# creating the dataframes that will store the evaluation metrics results\n\ndf_acc = pd.DataFrame(np.array([dic['acc'] for dic in listofdicts]).T, columns= [dic['modelname'] for dic in listofdicts])\n\ndf_fscore = pd.DataFrame(np.array([dic['fscore'] for dic in listofdicts]).T, columns=[dic['modelname'] for dic in listofdicts])\n\ndf_recall = pd.DataFrame(np.array([dic['recall'] for dic in listofdicts]).T, columns=[dic['modelname'] for dic in listofdicts])\n\ndf_precision = pd.DataFrame(np.array([dic['precision'] for dic in listofdicts]).T, columns=[dic['modelname'] for dic in listofdicts])\n\ndf_time = pd.DataFrame(np.array([dic['tempo'] for dic in listofdicts]).T, columns=[dic['modelname'] for dic in listofdicts])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistic test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the statistic test to each one of the evaluation metrics\ncreate_statistic_test('f1-score', df_fscore)\nprint()\n\ncreate_statistic_test('recall', df_recall)\nprint()\n\ncreate_statistic_test('precision', df_precision)\nprint()\n\ncreate_statistic_test('accuracy', df_acc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graphs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphs with the mean, median, and standart deviation to each metric used to all the \n# tested classifirers\n\nplot_stats_values(df_time, 'Time')\n\nplot_stats_values(df_acc, 'Accuracy')\n\nplot_stats_values(df_fscore, 'F1-score')\n\nplot_stats_values(df_precision, 'Precision')\n\nplot_stats_values(df_recall, 'Recall')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot with the algorithms results to each one of the metrics used\ncreate_boxplot(df_acc, metric_name='Accuracy')\n\ncreate_boxplot(df_fscore, metric_name='F1-score')\n\ncreate_boxplot(df_precision, metric_name='Precision')\n\ncreate_boxplot(df_recall, metric_name='Recall')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_best_algs_result(df_acc, df_precision, df_fscore, df_recall, df_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean accuracy of the best algorithms\nmedia = df_acc.mean()\n\nmlp = media['MLP']\nrf = media['RF'] \ndt = media['DT']\n\nx = ['MLP', 'Random Forest', 'Decision Tree']\nx_pos = [i for i, _ in enumerate(x)]\n\ny = [mlp, rf, dt]\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x_pos, y, color='pink')\n\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy mean\")\nplt.title(\"Best Algorithms\\n\")\nplt.xticks(x_pos, x)\n\ndef autolabel(rects):\n\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.00*height,\n                '%.3f' % float(height),\n        ha='center', va='bottom')\nautolabel(rects1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix to the MLP algorithm (best algorithm)\n\ncm = listofdicts[9]['cm'][3]\nsns.heatmap(cm,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the K-means algorithm to cluster the data\nkmeans3 = KMeans(n_clusters = 3)\n\n# Using Principal Component Analysis to plot a 2D graph of the data\npca = PCA(n_components=2).fit(df_noclass)\npca_2d = pca.transform(df_noclass)\n\narray_classe = np.array(df_class)\n\ny_km = kmeans3.fit_predict(df_noclass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the result of the K-means \n\nplt.scatter(\n    pca_2d[y_km == 0, 0], pca_2d[y_km == 0, 1],\n    s=50, c='lightgreen',\n    marker='o', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    pca_2d[y_km == 1, 0], pca_2d[y_km == 1, 1],\n    s=50, c='gold',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\nplt.scatter(\n    pca_2d[y_km == 2, 0], pca_2d[y_km == 2, 1],\n    s=50, c='red',\n    marker='o', edgecolor='black',\n    label='cluster 3'\n)\n\nplt.scatter(\n    kmeans3.cluster_centers_[:, 0], kmeans3.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='black', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotando a distribuição de classes original\nplt.scatter(\n    pca_2d[array_classe == 0, 0], pca_2d[array_classe == 0, 1],\n    s=50, c='lightgreen',\n    marker='o', edgecolor='black',\n    label='Suspicious'\n)\n\nplt.scatter(\n    pca_2d[array_classe == 1, 0], pca_2d[array_classe == 1, 1],\n    s=50, c='gold',\n    marker='o', edgecolor='black',\n    label='Legitimate'\n)\n\nplt.scatter(\n    pca_2d[array_classe == 2, 0], pca_2d[array_classe == 2, 1],\n    s=50, c='red',\n    marker='o', edgecolor='black',\n    label='Phishy'\n)\n\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}