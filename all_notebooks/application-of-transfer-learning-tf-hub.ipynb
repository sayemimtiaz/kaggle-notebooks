{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div style=\"background-color:#3c7852; display:block; padding:10px;\"><h1 style=\"color:#fff\">Multiclass Text Classification using Tf.Hub</h1></div>\n<div style=\"padding:3px;\">&nbsp;</div>\n\n## What is Transfer Learning?\n\nTransfer learning is a process of using pre-trained model on similar type of data(text, images). TensorFlow has a framework to leverage the pre-trained model network and components in a new model to get trained and receive more knowledge about the data. \n\n## Dataset\n\nIn this kernel we are going to explore a problem on multiclass text classification with Deep Learning model. If the target or response variable contains more than one class label then the data is considered as multinomial or multiclass dataset. \n\n\nThe dataset is a collection of various consumer complaints about finance products and services sent to companies for response. \n\n## Key Variables\n\nIn this dataset,  `Issue` is a textual description field which conveys the complaints about the finance product and service. The `product` is a target variable which will be classified based on the consumer issue description. \n\n\n<div style=\"background-color:#e0d52f; display:block; padding:10px;margin-botton:4px;\"><h2 style=\"color:#000\">Table of content</h2></div>\n<div style=\"padding:3px;\">&nbsp;</div>\n\n* [Load and extract dataset](#load_data)\n* [Split Train/Holdout and Dev Set](#split_data)\n* [Handling Imbalanced Data](#compute_weights)\n* [Data to Tensors](#data_to_tensor)\n* [Target Encoding](#target_encoding)\n* [Transfer Learning](#transfer_learning)\n* [Train Model](#train_model)\n* [Predict Data](#predict_data)\n"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tensorflow packages\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\n\n# SKlearn packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle, class_weight\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# setting max width option\npd.set_option('display.max_colwidth', -1)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\ndef dir_watch(dirname):\n    for dirname, _, filenames in os.walk(dirname):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n# input dir\ndir_watch('/kaggle/input/')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load_data\"></a>\n\n## Load and Extract Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the dataset from csv file\ncfpb_data = pd.read_csv('/kaggle/input/us-consumer-finance-complaints/consumer_complaints.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfpb_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract Data\n\nWe will extract notnull values of consumer complaint narrative records for the training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"non_na_complaints = np.where(~cfpb_data['consumer_complaint_narrative'].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(non_na_complaints[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfpb_extract = cfpb_data.loc[non_na_complaints]\n\n# Reset the index\ncfpb_extract.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfpb_extract.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interested fields\nkey_cols = ['product', 'consumer_complaint_narrative']\n\ncfpb_extract[key_cols][:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfpb_extract['product'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the target variable\ncfpb_extract['product'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"split_data\"></a>\n\n\n## Split the dataset\n\n### Train, Holdout and Dev Split\nThe dataset will be splited into 3 portions as 60/20/20 ratio. One for train the model, one for validation(holdout) and one for test(dev) the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test data will be taken as 80/20 ratio\nX_train_full, X_test_full = train_test_split(cfpb_extract[key_cols], test_size=0.2, random_state=111)\n\n# Split the train data into further as 60/20 ratio\nX_train, X_valid = train_test_split(X_train_full, test_size=0.2, random_state=111)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of X_train: {X_train.shape}, X_valid: {X_valid.shape}\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"compute_weights\"></a>\n## Handling imbalanced class data\n\nOne of the key techniques to handle imbalanced class data is, **computing the class weights**. We can compute the class weights. The weightage of the class is given based on the number of samples available in the dataset. We will use the `sklearn.utils.class_weight` modules `compute_class_weight` method to calculate the weights of the class.\n\nThe higher sample classes will have lesser weight and lower sampled classes will have higher weights. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = list(class_weight.compute_class_weight('balanced',\n                                                      np.unique(cfpb_extract['product']),\n                                                      cfpb_extract['product']))\n\n\nclass_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting list to dictionary object\nweights = {}\n\nfor inx, weight in enumerate(class_weights):\n    weights[inx] = weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['consumer_complaint_narrative'][:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data_to_tensor\"></a>\n## Convert Dataset into Tensors\n\nIn this step, we are converting the data into a tensors. Tensor datastructure is required for training the neural network model.\n\n`tf.data.Dataset.from_tensor_slices(tuple)` [Click here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more on `tf.data`\n\nOur dependent variable is `product` and the independent variable is `consumer_complaint_narrative`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor = tf.data.Dataset.from_tensor_slices((X_train['consumer_complaint_narrative'].values, X_train['product'].values))\ntest_tensor = tf.data.Dataset.from_tensor_slices((X_test_full['consumer_complaint_narrative'].values, X_test_full['product'].values))\nvalid_tensor = tf.data.Dataset.from_tensor_slices((X_valid['consumer_complaint_narrative'].values, X_valid['product'].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for corpus, target in train_tensor.take(5):\n    print(\"\\nTarget: {} \\nData: {}\".format(target, corpus))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"target_encoding\"></a>\n## Target Encoding\n\nWe will create a [**StaticHashTable**](https://gist.github.com/venkat-krish/a21808db141c58bea87bc309fccaa042) for our target variables. A sample code for creation of static hash table can be found [here](https://gist.github.com/venkat-krish/a21808db141c58bea87bc309fccaa042)"},{"metadata":{"trusted":true},"cell_type":"code","source":"products = np.unique(cfpb_extract['product'])\n\nproducts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Method to define target static hash\ndef target_encoding(unique_targets):\n    \n    key_tensor = tf.constant(unique_targets) # class names in text format\n    value_tensor = tf.constant(np.arange(0, len(unique_targets))) # index values from 0 to length of the classes\n    \n    hash_table = tf.lookup.StaticHashTable(\n                    tf.lookup.KeyValueTensorInitializer(\n                        keys = key_tensor, \n                        values = value_tensor), -1\n                )\n    \n    return hash_table\n\n# Target encoded table\ntarget_encoded = target_encoding(products)\n\n# TF function will get build in the TensorFlow graph\n@tf.function\ndef target_enc(t):\n    return target_encoded.lookup(t)\n\n\ndef display_batchwise(dataset, bsize=5):\n    for data, label in dataset.take(bsize):\n        print(\"Data:{}\\nTarget:{}\\n\".format(data.numpy(), label.numpy()))\n        \ndef one_hot_labelencoding(text, label):\n    return text, tf.one_hot(target_enc(label), 11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(train_tensor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the labels into binary variables\ntrain_data_f = train_tensor.map(one_hot_labelencoding)\nvalid_data_f = valid_tensor.map(one_hot_labelencoding)\ntest_data_f = test_tensor.map(one_hot_labelencoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, train_labels = next(iter(train_data_f.batch(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"transfer_learning\"></a>\n## Transfer Learning using TF.Hub\n\n\nTensorflow Hub is a way to share pre-trained model components. In this notebook we will use the **NNLM English 128 dim** ([source](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1)) model for embedding our text corpus data."},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_url = 'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1'\n\n# Hub layer for embedding the text corpus\nhub_layer = hub.KerasLayer(pretrained_url, output_shape=[128], \n                          input_shape=[], \n                          dtype=tf.string, \n                          trainable=True)\n\n# Look at the hub layer\nhub_layer(train_data[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embed_layer, output_shape):\n    model = tf.keras.Sequential()\n    \n    model.add(embed_layer)\n    \n    for unit in [128, 128, 64, 32]:\n        model.add(tf.keras.layers.Dense(unit, activation='relu'))\n        model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Dense(output_shape, activation='softmax'))\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_shape = len(products)\n\n# NN model\nmodel = build_model(hub_layer, output_shape)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"train_model\"></a>\n\n## Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model with train and validation set\n\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n             optimizer='adam',\n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the train data\n# shuffle_buffer_size = 50000\ntrain_data_f = train_data_f.shuffle(60000).batch(512) \nvalid_data_f = valid_data_f.shuffle(20000).batch(512)\ntest_data_f = test_data_f.batch(512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the data on the model\nhistory = model.fit(train_data_f,\n                    epochs=10,\n                    validation_data=valid_data_f,\n                    class_weight=weights,\n                   verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(test_data_f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"predict_data\"></a>\n## Predict the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data, test_labels = next(iter(test_data_f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_labels.numpy().argmax(axis=1), y_preds.argmax(axis=1)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}