{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Kmeans Clustering \n* Kmeans clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using \nk-medians and k-medoids.[Wikipedia] \n<br><br>\n* Clustering is a type of unsupervised machine learning algorithms used to separate data points into groups or clusters. Clustering is a process in which we inccrease similarity between the members inside a cluster, and in the meanwhile, we decrease it between members of diffirent groups or clusters. \n<br><br>\n* The kind of problems using clustering techniques such as Kmeans deals with an input space of data (features) without any predefined target, are known as unsupervised learning problems. In these problems, we only have the independent variables and no target variable.\n<br><br>\n* The quality of the cluster assignments is determined by computing the sum of the squared error (SSE) after the centroids converge, or match the previous iteration’s assignment. The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. \n<br><br>\n* Clustering Process with Kmeans : <br>  <br>\n 1. Specify the number of K clusters.\n 2. Randomly choose K centroids \n 3. Repeat this:<br>\n  3.1 : Assign each data point to its nearset cenroids <br>\n  3.2 : Recompute each centroid (mean of data points ) of each cluster.\n 4. Untill : the clusters don't change. \n"},{"metadata":{},"cell_type":"markdown","source":"### Imoprts"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom kneed import KneeLocator\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt \nimport seaborn as sb\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/customer-segmentation-tutorial-in-python\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{},"cell_type":"markdown","source":"* We are working on a mall customers data known as market basket analysis offered by Kaggle based on  data features such as age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\n* Our first task is to explore the dataset and anlyze it (EDA) to get some meaningful insights out of it. This will lead us to know relations between variales , and select only the important features before feeding it to the kmeans algorithm. \n* Our second task is to apply the Kmeans technique to segment customers into different clusters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#get data \ndata=pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get some informations on our data \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show the shape of the datafrme \ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe data\ndata[data.columns.difference(['CustomerID'])].describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if there any null values \ndata.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No Null Values. Let's proceed.. "},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"male=data[data.Gender=='Male']\nfemale=data[data.Gender=='Female']\nprint(f'Percentage of Male Customers : {len(male)/data.shape[0]*100} %')\nprint(f'Percentage of Female Customers : {len(female)/data.shape[0]*100} %')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count customers by gender \nplt.figure(2 , figsize = (8 , 5))\nsb.countplot(x = 'Gender' , data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the pairplot for the dataset scattered by Gender\nsb.set_style('whitegrid')\nplt.figure(1 , figsize = (15 , 6))\nsb.pairplot(data.drop('CustomerID', axis=1), hue='Gender', aspect=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see from the pairplot above, gender has no effect on clustering since our data does't show any patterns based on customer's sex. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#so let's select only the important features \nfeatures=['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']\nsb.set_style('whitegrid')\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor i in features:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sb.distplot(data[i] , bins = 20)\n    plt.title(f' {i} Distplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting regression plot to show relation within variables of the dataset.\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor i in features:\n    for j in features:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sb.regplot(x = i , y = j , data = data)\n        plt.ylabel(j.split()[0]+' '+j.split()[1] if len(j.split()) > 1 else j )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Age' , y = 'Annual Income (k$)' , data = data[data['Gender'] == gender] ,\n                s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Age'), \nplt.ylabel('Annual Income (k$)') \nplt.title('Age vs Annual Income grouped by Gender')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Annual Income (k$)',y = 'Spending Score (1-100)' ,\n                data = data[data['Gender'] == gender] ,s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)') \nplt.title('Annual Income vs Spending Score grouped by Gender')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Age , Annual Income and Spending Score by Gender\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor i in features:\n    n += 1 \n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sb.violinplot(x = i , y = 'Gender' , data = data , palette = 'vlag')\n    sb.swarmplot(x = i , y = 'Gender' , data = data)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Boxplots & Swarmplots' if n == 2 else '')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering using Kmeans"},{"metadata":{},"cell_type":"markdown","source":"#### Clustering with Annual Income and Spending Score"},{"metadata":{},"cell_type":"markdown","source":"#### The Elbow Method :\n* The Elbow Method is one of the most popular methods to determine this optimal value of k, k as number of clusters we want. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE(Sum of Squared Errors). \n* When you plot SSE as a function of the number of clusters, notice that SSE continues to decrease as you increase k. As more centroids are added, the distance from each point to its closest centroid will decrease. There’s a sweet spot where the SSE curve starts to bend known as the elbow point"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clustering with Annual Income and spending Score\nX = data[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\n#scaled the features \nscaler=StandardScaler()\nX_scaled= scaler.fit_transform(X)\n#begin saerching for the best k \nsse = []\nfor n in range(1 , 11):\n    model = KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') \n    model.fit(X_scaled)\n    sse.append(model.inertia_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting k Clusters based on the SSE (Sum of Squared Errors using the elbow method\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , sse , 'o')\nplt.plot(np.arange(1 , 11) , sse , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters')\nplt.ylabel('SSE')\nplt.title('The Elbow Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to detect the elbow point \nkl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n#get the elbow point  \nprint(f'The Elbow Point matches : {kl.elbow}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can see that number of clusters equals 5\nmodel = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nmodel.fit(X)\nlabels = model.labels_\ncentroids = model.cluster_centers_\nprint(f'Init Centroids:\\n {np.unique(centroids)}')\nprint(f'Clustering Labels: \\n{np.unique(labels)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max,0.02 ), np.arange(y_min, y_max, 0.02))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nX_preds = model.predict(np.c_[xx.ravel(), yy.ravel()])\nX_preds = X_preds.reshape(xx.shape)\nplt.imshow( X_preds, interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = data , c = labels , \n            s = 200 )\nplt.scatter(x = centroids[: , 0] , y =  centroids[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score')  \nplt.xlabel('Annual Income')\nplt.title('Plotting the Clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clustering with Annual Income and Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clustering with age and Annual Income\nZ = data[['Annual Income (k$)' , 'Age']].iloc[: , :].values\n#scale data \nZ_scaled=scaler.fit_transform(Z)\nsse=[]\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(Z_scaled)\n    sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the ELBOW method to show the elbow point \nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , sse , 'o')\nplt.plot(np.arange(1 , 11) , sse , '-' , alpha = 0.5)\nplt.title('The Elbow Method')\nplt.xlabel('# of clusters')\nplt.ylabel('SSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to detect the elbow point \nkl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n#get the elbow point  \nprint(f'The Elbow Point matches : {kl.elbow}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as we can see, the elbow poit matches k=3 \nkmeans_model = KMeans(n_clusters= 3, init='k-means++', random_state=0)\nZ_preds= kmeans_model.fit_predict(Z_scaled)\n#plot the clusters \nplt.figure(1 , figsize = (15 ,6))\nplt.scatter(Z[Z_preds == 0, 0], Z[Z_preds == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(Z[Z_preds == 1, 0], Z[Z_preds == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(Z[Z_preds == 2, 0], Z[Z_preds == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(kmeans_model.cluster_centers_[:, 0], kmeans_model.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Age')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If you Like my Notebook Please Upvote !! "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}