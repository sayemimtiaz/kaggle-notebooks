{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div style=\"background: #e6e6d8 url('https://dl.dropboxusercontent.com/s/t0gu051d08sei65/bg-retro-noise.png');\n  color: #121212; min-height:300px;\">\n    \n<section style=\"position: absolute;\n  width: 100%;\n  min-width: 800px;\n  text-align: center;\n  top: 50%;\n  margin-top: -55px;\">\n    \n  <h3 style=\"transform: matrix(1, -0.15, 0, 1, 0, 0);\n  -ms-transform: matrix(1, -0.10, 0, 1, 0, 0);\n  -webkit-transform: matrix(1, -0.15, 0, 1, 0, 0);\n  text-transform: uppercase;\n  font-weight: 400;\n  font-size: 70px;\n  text-shadow: 4px 5px #e6e6d8, 6px 7px #c6a39a;\">Chinese MNIST</h3>\n    <p style=\"text-align:center;\">@koayhongvin</p>\n</section>\n    </div>"},{"metadata":{},"cell_type":"markdown","source":"<ul class=\"list-group\">\n  <li class=\"list-group-item active\">Notebook Content</li>\n    \n  <li class=\"list-group-item d-flex justify-content-between align-items-center\" ><span class=\"badge badge-primary badge-pill\">1</span> <a href=\"#1\">\n    Insights of Data\n    </a>\n  </li>\n  <li class=\"list-group-item d-flex justify-content-between align-items-center\"> <span class=\"badge badge-primary badge-pill\">2</span> <a href=\"#2\">\n    VGG-like model\n     </a>\n  </li>\n  <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">3</span> <a href=\"#3\">\n    No-Dense Model\n    </a>\n  </li>\n    <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">4</span> <a href=\"#4\">\n    Simpler Model\n    </a>\n  </li>\n    <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">5</span> <a href=\"#5\">\n    Visualizing the intermidate layer (Simpler Model)\n    </a>\n  </li>\n</ul>\n\n<div>\n<a href=\"https://khvmaths.medium.com/chinese-digit-mnist-1b46f51e8f75\"><p style=\"text-align:center;\">Medium Article:<img src=\"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiPjxwYXRoIGQ9Ik0xMiAwYy02LjYyNyAwLTEyIDUuMzczLTEyIDEyczUuMzczIDEyIDEyIDEyIDEyLTUuMzczIDEyLTEyLTUuMzczLTEyLTEyLTEyem0wIDJjNS41MTQgMCAxMCA0LjQ4NiAxMCAxMHMtNC40ODYgMTAtMTAgMTAtMTAtNC40ODYtMTAtMTAgNC40ODYtMTAgMTAtMTB6bS0yLjQyNiAxNC43NDFoLTMuNTc0di0uMjAybDEuMjYxLTEuNTI5Yy4xMzQtLjEzOS4xOTUtLjMzNS4xNjItLjUyNnYtNS4zMDRjLjAxNS0uMTQ3LS4wNDEtLjI5My0uMTUxLS4zOTJsLTEuMTIxLTEuMzV2LS4yMDFoMy40NzlsMi42ODkgNS44OTcgMi4zNjQtNS44OTdoMy4zMTd2LjIwMWwtLjk1OC45MTljLS4wODMuMDYzLS4xMjQuMTY2LS4xMDYuMjY5djYuNzQ4Yy0uMDE4LjEwMy4wMjMuMjA2LjEwNi4yNjlsLjkzNi45MTl2LjIwMWgtNC43MDZ2LS4yMDFsLjk2OS0uOTQxYy4wOTUtLjA5NS4wOTUtLjEyMy4wOTUtLjI2OXYtNS40NTVsLTIuNjk1IDYuODQ0aC0uMzY0bC0zLjEzNy02Ljg0NHY0LjU4N2MtLjAyNi4xOTMuMDM4LjM4Ny4xNzQuNTI2bDEuMjYgMS41Mjl2LjIwMnoiLz48L3N2Zz4=\"></p></a></div>"},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"#1\">Good Practices: The insights of the data</h1>"},{"metadata":{},"cell_type":"markdown","source":"**Check for distribution of data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_df=pd.read_csv('../input/chinese-mnist/chinese_mnist.csv')\ndata_df.groupby([\"value\",\"character\"]).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for missing data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Well, no missing data, but how about missing images?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimage_files = list(os.listdir(\"../input/chinese-mnist/data/data\"))\nprint(\"Number of image files: {}\".format(len(image_files)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add the image path and sizes to the dataframe!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import skimage.io\nimport numpy as np\n\ndef image_files(x):\n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name\ndata_df[\"file\"] = data_df.apply(image_files, axis=1)\n\ndef image_sizes(file_name):\n    image = skimage.io.imread(\"../input/chinese-mnist/data/data/\" + file_name)\n    return list(image.shape)\n\nimage_size = np.stack(data_df['file'].apply(image_sizes))\nimage_size_df = pd.DataFrame(image_size,columns=['w','h'])\ndata_df = pd.concat([data_df,image_size_df],axis=1, sort=False)\n\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df[\"code\"].values)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"code\"].values)\n\nprint(\"Train set: {}\".format(train_df.shape[0]))\nprint(\"Test set: {}\".format(test_df.shape[0]))\nprint(\"Validation set: {}\".format(val_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Encode the categories**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import skimage.transform\n\ndef read_image(file_name):\n    image = skimage.io.imread(\"../input/chinese-mnist/data/data/\" + file_name)\n    image = skimage.transform.resize(image, (64, 64, 1), mode='reflect')\n    return image\n\ndef categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y\n\nX_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"#2\">VGG-like Model</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\n\nmodel=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=10,\n                  verbose=1,\n                  validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"#2\">No-Dense Model</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import GlobalAveragePooling2D, Activation\n\nmodel=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\n# model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n# model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(y_train.columns.size, kernel_size=3, activation='relu', padding='same'))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Activation('softmax'))\n# model.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=50,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"#4\">Simpler Model</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\n# model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n# model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=50,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"#5\">Visualizing the Model</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\n\nlayer_outputs = [layer.output for layer in model.layers][0:6]\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs) \nactivations = activation_model.predict(X_test[0:1])\n\nlayer_names = []\nfor layer in model.layers:\n    layer_names.append(layer.name)\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image /= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. / size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"jumbotron\">\n  <h1 class=\"display-4\">Thank you for making till the end!</h1>\n  <p class=\"lead\">Please upvote if you find this notebook helps you. Any comments are welcomed. I'd love to get feedbacks!</p>\n  \n</div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}