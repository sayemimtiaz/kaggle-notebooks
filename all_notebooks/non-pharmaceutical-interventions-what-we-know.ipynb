{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook is written to analyze the information available in **“Open Research Dataset Challenge (CORD-19)”** related to **Non Pharmaceutical Interventions**. This task requires mining of information related to specific phrases and keywords. AI Related tools and technologies have been used to achieve the objective. Details and findings are available in subsequent sections. Happy reading!\n> Note: Non Pharmaceutical Interventions are abbreviated as **NPI**.\n\n### Contents\n\nThis notebook is organized in following sections. If you want to jump to findings, click  [Findings](#Findings)\n\n- [Methodology](#Methodology)\n- [Tools Used](##Tools/-Packages-used)\n- [The Code](#The-code)\n- [Usage Instruction](#Instruction-of-usage)\n- [Input Area](#Input-Area)\n- [Processing Summary](#Processing-Summary)\n- [Findings](#Findings)\n- [Important Output for Biorxiv Articles](#Important-Biorxiv/Medrxiv-articles-related-to-NPI)\n- [Important Output for Non Comm Articles](#Important-Non-Comm-articles-related-to-NPI)\n- [Important Output for Comm Articles](#Important-Comm-articles-related-to-NPI)\n- [Important Output for Custom License Articles](#Important-Non-Comm-articles-related-to-NPI)\n- [Complete List of Biorxiv/Medrxiv Articles](#Complete-List---Biorxiv/Medrxiv)\n- [Complete List of Non-Comm Articles](#Complete-List---Non-Comm)\n- [Complete List of Comm Articles](#Complete-List---Comm)\n- [Complete List of Custom License Articles](#Complete-List---Custom-License)"},{"metadata":{},"cell_type":"markdown","source":"# The Task\nIn this notebook, the task **“What do we know about non-pharmaceutical interventions?”** is being attempted.\n\n# Data Details\n\nThe CORD-19 dataset is an open-access collection of academic publications on COVID-19 and coronavirus research.\nThere are four datasets available which are as follows. \n- Biorxiv/ Medrxiv\n- Comm Subset\n- Non Comm Use Subset and \n- Custom License\n\n# Tools/ Packages used\n\nFor this task, the NLP library of Spacy is used. Spacy is a modern NLP library designed to make NLP related tasks easy to solve. This library supports both Rule Based models and Statistical Models. One of the important features used here is **sentence boundary detection** or (SBD), which appears to be better than regex based extraction methods and provides rich information about the data. Ease of Use and Clean API provided by Spacy is also handy in this case.\n\n\n# Methodology\n\n## Mining data: Obtaining relevant information from articles. \n\n- First, the Title and article text(subset) is searched for most relevant articles. The text of the article is truncated to perform initial finding quickly. These are deeply mined later on.\n- Second, a deep dive on filtered articles is done where more relevant information from filtered articles (above step) is extracted. This is done to enrich the information for the articles which are already identified. In these two steps, some statistics are generated related to an article.\n- A scoring methodology is applied, which helps to filter out the relevant articles, especially when the number of articles found is high. This helps in ensuring that misinformation is kept to a minimum.\n- The scoring model has been developed using capabilities of spacy search. The articles containing instances of relevant texts such as non, pharmaceutical, interventions and their variations have been identified and the scoring model ensures that most relevant articles are filtered.\n\n## Descriptive Output\nA high level overview of findings is presented to the user in a tabular format and with the help of a bar chart. This tells about how many articles were available, how many of them were processed, how many relevant articles are found, etc.\n\n## Detailed output\nFor highly relevant articles, detailed output is provided in the following format. Articles, which have higher score, typically above 10 (there is an additional filter which enables showing an article with low score as well) are shown to the user with the following information.\n\n## Complete List\nWhile the Detailed output gives the details of high ranking and relevant articles, the complete list is the one where every article, whether high or low on relevance, is listed for the user. The idea is that if the end user wants to see, (s)he can look at the article through the URL, if available.\n\n# The code\n\n## Main Functions and Class module \n- Function `process_a_module` - Takes a path containing json files and returns two datasets - one containing the relevant statistics and other containing the raw data. \n- Class module `Article` -  This is initiated with the dataset obtained from above function. The helper function for this is `get_objectlist_from_df` which returns a list of Article objects.\n- Function `display_data_processing_info` - Displays information on processing of data.\n- Function `display_article` - This is the function which displays information for every article.\n\n## How the output is refined?\n- `process_a_module` generates the basic filtering criteria for choosing an article/paper.\n-  Once the articles are selected, deep mining of data happens which considers the whole text of data. This helps to find out more information about an article. Class `Article` encapsulates the logic.\n- At the time of outputting article related information, filters are used to suppress the articles which are not significant.\n- The output has been examined by undergoing few iteration and filters are applied by observing various statistics and then using them.\n\n### Detailed Output Format\n  Results are presented in the following format\n- Title of the article\n- Score\n- URL\n- Date\n- Author Information\n- Relevant sentences\n- Sentences containing keywords - \"rapid\", \"design\", \"experiments\", \"assessment\" (and/or)\n- Sentences containing keywords - \"cost\", \"benefits\" (and/or)\n- Sentences containing keywords - \"funding\", \"infra\",\"authorities\" (and/or)   \n- All urls which appear in the article.    \n\n### Complete List Format\nThis contains the list of all articles, section wise in a tabular format. Name of the article, paper id, url (wherever applicable), etc are listed.\n\n# Observations related to processing\n- Relevant articles are found for Biorxiv/ Medrxiv datasets. This is determined by finding a variable called [Hit Ratio](#Rate-of-finding-Articles-and-useful-snippets-(Hit-Ratios)). Hit Ratios find out that how often an article of interest is found from the given dataset. On top of that, manual inspection is also performed. All the articles are listed under **Complete List** as a reference to the user. For this reason, Biorxiv/Medrxiv database is used as a primary source for analysis.\n- Due to computational limits, not all articles are mined for finding relevant information, especially `custom license` dataset. This has turned out to be not an issue, as apart from Biorxiv/ Medrxiv dataset, no dataset exhibited relevant information.\n- The approach is generally showing good results. However, couple of situations were found where results are not appropriate. The paper id corresponding to those are as following.\n\n    * 3d437a83aac3593fcea6eb2a6f6f5e56fc13b7ee\n    * 0b6a65192c25d1f4fe2a17090a5994c7f499d42a\n\n# Instruction of usage\n\nThere are four separate databases which have a huge number of articles. To deal with, four variables in [input area](#Input-Area) are provided. If I do not want to process say Biorxiv Articles, then set value of `SAMPLE_SIZE_BIORXIV` to -1. A value of 1000 indicates that the program will attempt to mine 1000 articles of Biorxiv. If I set `SAMPLE_SIZE_COMM` to 250 and `SAMPLE_SIZE_BIORXIV` to 250, the program will mine 250 and 200 articles respectively from these two databases. Initial settings are set in the way so that it can cover maximum number of articles which are likely to be relevant. The dataset size is also increasing as new articles are added to them. Setting corresponding SAMPLE_SIZE to a higher number does the trick."},{"metadata":{},"cell_type":"markdown","source":"# Input Area"},{"metadata":{"trusted":true},"cell_type":"code","source":"# INPUT AREA, Provide -1 if you do not want to process data for a module \n# A module is one of Biorxiv, Comm, Non comm and Custom License. Please read Instruction of Usage above.\n\nSAMPLE_SIZE_BIORXIV =  2000\nSAMPLE_SIZE_COMM =     2500\nSAMPLE_SIZE_NON_COMM =  2500\nSAMPLE_SIZE_CUSTOM_LICENSE = 3000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Findings\n\n* Of all the articles filtered, Social distancing has been discussed widely with respect to Non Pharmaceutical Interventions.\n* Early detection and isolation is likely to prevent more infections than contact reduction and social distancing across the country (5-fold versus 2.6-fold), but without the intervention of contact reductions, in the longer term, the epidemics would increase exponentially.\n* Non essential contact, travel , avoiding pubs, theatres, restaurants should be avoided.\n* It is also suggested to harness the power of Smartphone technology.\n* Usage of face masks is also suggested. Use of face masks, at least in healthcare settings, is a rapid adoption of NPI which can have major impacts on COVID-19 related morbidity and mortality in Canada and elsewhere.\n* Symptom based isolation with a smaller period (1 day) does not seem to be effective enough.\n* No benefit was found from increasing the post-fever isolation period beyond one day.\n* Results also suggest that the social distancing intervention should be continued for the next few months in China to prevent case numbers increasing again after travel restrictions were lifted on February 17, 2020.\n* It is estimated that Lock down results in 50% or higher in reduction of COVID-19 cases.\n* The implementation of NPIs has coincided with the rapid decline in the number of new cases across China, albeit at high economic and social costs.\n* The early detection and isolation of cases was estimated to prevent more infections than travel restrictions and contact reductions, but integrated NPIs would achieve the strongest and most rapid effect.\n* Early NPI measures taken by states and cities such as the total closure of schools, universities and non-essential services, the social distancing and isolation of individuals above 60 years and the voluntary home quarantine have already lead to significant reduction in the number of infections as well as delaying the time for the peak of contamination.\n* In particular, this is noticed that changing the current NPI to SD60+ (f social distancing of people above 60 years old), is completely catastrophic.\n* Despite substantial uncertainties surrounding the biology of the disease and intervention costs and effects, most models arrived at the same qualitative finding: left unchecked, the spread of COVID-19 would overwhelm hospital capacity to care for acute and critical cases, resulting in intolerable potential morbidity and mortality.\n* In the short-term, results suggest that given baseline knowledge of the number of tests and number of hospitalizations, short-term planning on the scale of weeks would benefit most from understanding the impact of NPIs.\n\n\n## Summary: \n\nThough effectiveness of Social distancing measures are difficult to predict because of two factors, A - the time they are introduced and B - Implementation is a  result of complex deliberative processes.The literature indicates that overall compliance with NPI can decline through time but that changes are complex and difficult to predict. Despite the above, results are reassuring.\n\n\nFollowing articles have been primarily used to arrive at findings.\n\n* https://doi.org/10.1101/2020.03.30.20047597\n* https://doi.org/10.1101/2020.04.03.20052498\n* https://doi.org/10.1101/2020.04.06.20052506 \n* https://doi.org/10.1101/2020.03.03.20029843 \n* https://doi.org/10.1101/2020.03.26.20044750 \n* https://doi.org/10.1101/2020.03.24.20042424\n* https://doi.org/10.1101/2020.03.21.20040667 \n* https://doi.org/10.1101/2020.04.01.20050039 \n\n\n\n### Note on Costs: An article writes about costs which is as following:\n**Inclusive Costs of NPI Measures for COVID-19 Pandemic: Three Approaches** https://doi.org/10.1101/2020.03.26.20044552 \n\nIn the herd immunity approach, the state imposes multiple NPI in order to bring R0 down to the level slightly above one.The herd immunity approach applies non-pharmaceutical measures to keep the number of new cases at the maximum of healthcare capacity.\n\nThen, after the number of infections gets too close to the healthcare capacity and while the proportion of immune individuals in the population grows, it slowly dismantled NPI measures in order to keep the number of new infections at the level of healthcare capacity.\n\nThe third aggressive approach uses the most aggressive non-pharmaceutical suppression and mitigation measures to reduce the number of new cases and then tries to eliminate the virus through extensive testing, case tracking and case isolation. These measures included extensive testing, mask wearing, shutting down most non-essential businesses and strict movement restrictions (shelter-in-place, lock-down policies).\nAssuming that we test all residents experiencing common cold or flu symptoms which happens about 3 times per year (Arroll, 2011) and using the Medicare reimbursement rate of 4 bln per month.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.matcher import Matcher\nnlp = spacy.load('en_core_web_sm')\nimport torch\nimport scipy.spatial\nimport time\nimport gc\nimport plotly.express as px\nfrom IPython.core.display import display, HTML\n\nRUN_MODE =   \"SUBSET\" #\"ALL\" \nRUN_SAMPLES = 100\nBIORXIV = \"biorxiv\"\nCOMM = \"comm\"\nNON_COMM = \"non_comm\"\nCUSTOM_LICENSE = \"custom_license\"\n\nFILE_BIORXIV_ARTICLES_INFO             = \"biorxiv_article_info.txt\"\nFILE_COMM_ARTICLES_INFO                = \"comm_article_info.txt\"\nFILE_NON_COMM_ARTICLES_INFO            = \"non_comm_article_info.txt\"\nFILE_CUSTOM_LICENSE_ARTICLES_INFO      = \"custom_license_article_info.txt\"\n\nlst_url_exclusions = ['//github.com', 'https://doi.org','https://doi.org/10','perpetuity.is', 'https://doi.org/10.1101/2020.03', 'https://doi.org/10.1101/2020.04']\n\n\n## Functions to generate CSVs from Json files, four types of datasets are available here.\ndef save_article_info(obj, filename):\n    with open(filename, 'a') as the_file:\n        the_file.write(\"# PAPER_ID ----- : \"  + obj.paper_id + \"\\n\")\n        the_file.write(\"# TITLE -----------: \"  + obj.title + \"\\n\")\n        the_file.write(\"# RELEVANT SENTENCES ----------:\")\n        the_file.write(\"\\n\")\n        for item in obj.lst_sentences:\n            the_file.write(\"\\n ==>\")\n            the_file.write(\"%s \" % item)\n            the_file.write(\"\\n\")\n        \n        if (len(obj.lst_rapid_assessment_sentences) > 0):\n            the_file.write(\"# ASSESSMENT RELATED SENTENCES ----------:\")\n            the_file.write(\"\\n\")\n            for item in obj.lst_rapid_assessment_sentences:\n                the_file.write(\"\\n ==>\")\n                the_file.write(\"%s \" % item)\n            the_file.write(\"\\n\")\n        \n        if (len(obj.lst_rapid_design_sentences) > 0):\n            the_file.write(\"# DESIGN RELATED SENTENCES ----------:\")\n            the_file.write(\"\\n\")\n            for item in obj.lst_rapid_design_sentences:\n                the_file.write(\"\\n ==>\")\n                the_file.write(\"%s \" % item)\n            the_file.write(\"\\n\")      \n        \n        if (len(obj.lst_design_experiments_sentences) > 0):\n            the_file.write(\"# EXPERIMENT RELATED SENTENCES ----------:\")\n            the_file.write(\"\\n\")\n            for item in obj.lst_design_experiments_sentences:\n                the_file.write(\"\\n ==>\")\n                the_file.write(\"%s \" % item)\n            the_file.write(\"\\n\")     \n        \n        the_file.write(\"# URL -------------:\")\n        for item in obj.lst_urls:\n            the_file.write(\"\\n ==>\")\n            the_file.write(\"%s \" % item)\n        if (len(obj.lst_urls)==0):\n            the_file.write(\"No urls found.\")\n        the_file.write(\"\\n\")\n        author_out = obj.authors\n        if (obj.authors.strip() == \"\"):\n            author_out = \"NOT_FOUND\"\n        the_file.write(\"\\n\")\n        the_file.write(\"# AUTHORS -----------: \"  + obj.authors + \"\\n\")\n        the_file.write(\"# SCORE -----------: \"  + str(obj.score) + \"\\n\")\n        the_file.write(\"# =========================================================: \"  + \"\\n\")\n\n\n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        if (section.strip() != \"\"):\n            body += section.upper()\n            body += \" : \"\n        body += text\n        body += \".\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname, SAMPLE_SIZE = 50):\n    filenames = os.listdir(dirname)\n    lst_orig_count = len(filenames)\n    raw_files = []\n    if (RUN_MODE == \"SUBSET\"):\n        filenames = filenames[0: SAMPLE_SIZE]\n    \n    for filename in (filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return (raw_files, lst_orig_count)\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in (all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\ndef find_phrases_in_title_npi(doc , span_start = 5 , span_end = 5):\n    matcher = Matcher(nlp.vocab)\n    pattern1= [{'LOWER': 'non'}, {'LOWER': 'pharmaceutical'}, {'LOWER': 'intervention'}]\n    pattern2 = [{'LOWER': 'non'}, {'LOWER': 'pharmaceutical'}, {'LOWER': 'interventions'}]\n    pattern3 = [{'LOWER': 'non'}, {'IS_PUNCT': True, 'OP' : '*'} , {'LOWER': 'pharmaceutical'}, {'IS_PUNCT': True, 'OP' : '*'}, {'LOWER': 'interventions'}]\n    pattern4 = [{'LOWER': 'non'}, {'IS_PUNCT': True, 'OP' : '*'} , {'LOWER': 'pharmaceutical'}, {'IS_PUNCT': True, 'OP' : '*'}, {'LOWER': 'intervention'}]\n    lst_spans = []\n    #matcher.add('titlematcher', None, *phrase_patterns)\n    matcher.add('titlematcher', None,  pattern1, pattern2, pattern3, pattern4)\n    found_matches = matcher(doc)\n    find_count = len(found_matches)\n    for match_id, start, end in found_matches:\n        string_id = nlp.vocab.strings[match_id]\n        end = min(end + span_end, len(doc))\n        start = max(start - span_start,0)\n        span = doc[start:end]\n        lst_spans.append(span.text)\n    snippets = '| '.join([lst for lst in lst_spans])\n    return find_count, snippets\n\ndef prepare_dataframe_for_nlp(df, nlp):\n    df.fillna('', inplace=True)\n    return(df)\n\ndef get_sents_from_snippets(lst_snippets, nlpdoc, paper_id):\n    \"\"\"\n    Finding full sentences when snippets are passed to this function.\n    \"\"\"\n    phrase_patterns = [nlp(text) for text in lst_snippets]\n    matcher = PhraseMatcher(nlp.vocab)\n    matcher.add('xyz', None, *phrase_patterns)\n    sentences = nlpdoc\n    res_sentences = []\n    for sent in sentences.sents:\n        found_matches = matcher(nlp(sent.text))\n        find_count = len(found_matches)\n        if len(found_matches) > 0:\n            res_sentences.append(sent.text)\n    res_sentences = list(set(res_sentences))\n    return(res_sentences)\n\ndef limit_text_size(text):\n#     if (len(text) > (10000)):\n    text = text[0:40000]\n    return(text)\n\ndef find_phrases_in_text(doc , phrase_list, span_start = 5 , span_end = 5):\n    matcher = PhraseMatcher(nlp.vocab)\n    #print(phrase_list)\n    lst_spans = []\n    phrase_patterns = [nlp(text) for text in phrase_list]\n    matcher.add('covidmatcher', None, *phrase_patterns)\n    found_matches = matcher(doc)\n    find_count = len(found_matches)\n            \n    for match_id, start, end in found_matches:\n        string_id = nlp.vocab.strings[match_id]\n        end = min(end + span_end, len(doc) - 1)\n        start = max(start - span_start,0)\n        span = doc[start:end]\n        lst_spans.append(span.text)\n        #print(\"found a match.\", span.text)\n    snippets = '| '.join([lst for lst in lst_spans])\n    ret_list = list(set(lst_spans))\n    return(find_count, ret_list)\n\ndef generate_data(dir_path, SAMPLE_SIZE = 50):\n    _files, count_files_orig = load_files(dir_path, SAMPLE_SIZE)\n    df = generate_clean_df(_files)\n    return(df, count_files_orig)\n\ndef add_lists(lst1, lst2, lst3, lst4):\n    lst_final = list(lst1) + list(lst2) + list(lst3) + list(lst4)\n    return(lst_final)\n\ndef do_scoring_npi(title_find_count\n               , text_find_count_in\n               , text_find_count_ph\n               , text_find_count_non\n               , text_find_count_npi):\n    if ((text_find_count_in > 0) & (text_find_count_ph > 0) & (text_find_count_non > 0)):\n        ret = 30 * title_find_count + 10 * text_find_count_npi + text_find_count_in + text_find_count_non + text_find_count_ph\n    else:\n        ret = 30 * title_find_count + 10 * text_find_count_npi \n    return(ret)\n\ndef process_url(url):\n    ret = url\n    #print(url in lst_url_exclusions)\n    if url in lst_url_exclusions:\n        ret = ''\n    return(ret)\n\ndef is_main_url(d):\n    if d.startswith('https://doi.org/'): # Could use /10.1101\n        return (True)\n    else:\n        return (False)\ndef find_url_in_text(doc):\n    main_url = \"NOT FOUND\"\n    lst_urls = []\n    matcher = Matcher(nlp.vocab)\n    pattern = [{'LIKE_URL': True}]\n    matcher.add('url', None,  pattern)\n    found_matches = matcher(doc)\n    #print(found_matches)\n    for match_id, start, end in found_matches:\n        url = doc[start:end]\n        url = process_url(url.text)\n        #print(url)\n        if (url != \"\"):\n            lst_urls.append(url)\n        if is_main_url(url):\n            main_url = url\n    return(main_url , list(set(lst_urls)))\n\ndef get_summary_row_for_df(processed_articles, count_find, count_fund_infra, count_cost_benefits, module):\n    dict_row ={\"Module\":module, \"Processed\": processed_articles, \"Found\": count_find\n               , \"Found Funding and Infrastructure\": count_fund_infra\n               , \"Count Cost benefits\": count_cost_benefits}\n    return(dict_row)\n\ndef process_a_module(path, SAMPLE_SIZE = 50, MODULE = \"provide-module\"):\n    \n    df_data, count_orig_files = generate_data(path, SAMPLE_SIZE)\n    df_master = df_data.copy()[[\"paper_id\", \"title\"]]\n    df_data = prepare_dataframe_for_nlp(df_data, nlp)\n    df_data['small_text'] = list(map(limit_text_size, (df_data['text'])))\n    \n    df_data['nlp_title'] = list(map(nlp, (df_data['title'])))\n    \n    with nlp.disable_pipes(\"tagger\", \"parser\", \"ner\"):\n        df_data['nlp_snall_text'] = list(map(nlp, (df_data['small_text'])))\n    \n    \n    df_master['title_find_count'], df_master['title_found_snippets'] = zip(*df_data['nlp_title'].apply(lambda title: find_phrases_in_title_npi((title))))\n    \n    \n    phrase_list = [u\"intervention\"]\n    df_master['text_find_count_in'], df_master['text_found_snippets_in'] = zip(*df_data['nlp_snall_text'].apply(lambda nlptext: find_phrases_in_text((nlptext), phrase_list)))\n\n\n    phrase_list = [u\"pharmaceutical\"]\n    df_master['text_find_count_ph'], df_master['text_found_snippets_ph'] = zip(*df_data['nlp_snall_text'].apply(lambda nlptext: find_phrases_in_text((nlptext), phrase_list)))\n\n\n    phrase_list = [u\"non\"]\n    df_master['text_find_count_non'], df_master['text_found_snippets_non'] = zip(*df_data['nlp_snall_text'].apply(lambda nlptext: find_phrases_in_text((nlptext), phrase_list)))\n\n\n    phrase_list = [u\"NPI\"]\n    df_master['text_find_count_npi'], df_master['text_found_snippets_npi'] = zip(*df_data['nlp_snall_text'].apply(lambda nlptext: find_phrases_in_text((nlptext), phrase_list)))\n    \n    df_master['lst_snippets']  = list(map(add_lists\n                                              , (df_master['text_found_snippets_ph'])\n                                              , (df_master['text_found_snippets_npi'])\n                                              , (df_master['text_found_snippets_non'])\n                                              , (df_master['text_found_snippets_in'])\n                                             ) )\n\n    df_master[\"score\"] = list(map(do_scoring_npi\n                                     , df_master['title_find_count']\n                                     , df_master['text_find_count_in']\n                                     , df_master['text_find_count_ph']\n                                     , df_master['text_find_count_non']\n                                     , df_master['text_find_count_npi']\n                                     ))\n    \n    df_master    = df_master.sort_values('score', ascending = False)\n    df_master['module'] = MODULE\n    df_data['module'] = MODULE\n    _ = gc.collect()\n    return(df_master, df_data, count_orig_files)\n\n\ndef get_paper_info(paper_id, journal):\n    if (journal == BIORXIV):\n        df = df_biorxiv[df_biorxiv['paper_id'] == paper_id]\n    if (journal == COMM):\n        df = df_comm[df_comm['paper_id'] == paper_id]\n    if (journal == NON_COMM):\n        df = df_non_comm[df_non_comm['paper_id'] == paper_id]\n    if (journal == CUSTOM_LICENSE):\n        df = df_custom_license[df_custom_license['paper_id'] == paper_id]\n    text = df.iloc[0]['text']#[0:5000]\n    title = df.iloc[0]['title']\n    authors = df.iloc[0]['authors']\n    return(text, title, authors)\n\ndef print_list(lst, number_to_print = 5, shuffle = True):\n    if len(lst) < number_to_print:\n        number_to_print = len(lst)\n    for i in range(-1*number_to_print, 0):\n        print( lst[i])\n        \ndef get_stats_from_articles(lst_articles):\n    count_articles = 0\n    count_cost_benefits = 0\n    count_fund_infra = 0\n    lst_cost_benefits = []\n    lst_fund_infra = []\n    for obj in lst_articles:\n        count_articles = count_articles + 1\n        if len(obj.lst_cost_benefits_sentences) > 0:\n            count_cost_benefits = count_cost_benefits + 1\n            lst_cost_benefits.append((obj.title, obj.lst_urls, obj.score))\n        if len(obj.lst_funding_infra_sentences) > 0:\n            count_fund_infra = count_fund_infra + 1\n            lst_fund_infra.append((obj.title, obj.lst_urls, obj.score))\n    return(count_articles,  count_cost_benefits, count_fund_infra, lst_cost_benefits)\n\ndef create_file(filename):\n    with open(filename, 'w') as the_file:\n        the_file.close()\n        \ndef write_to_file(filename, Text):\n    with open(filename, 'a') as the_file:\n        the_file.write(Text)\n        the_file.close()\n        \ndef get_nlp_text_for_paper_id(paper_id, module):\n    text, x, y = get_paper_info(paper_id, module)\n    with nlp.disable_pipes(\"tagger\", \"parser\", \"ner\"):\n        return(nlp(text))\n    \ndef get_td_string():\n    tdstring = '<td style=\"text-align: left; vertical-align: middle; font-size:1.2em;\">'\n    return(tdstring)\n\ndef get_sentence_tr(sent):\n    \n    row = get_td_string() + f'{sent}</td></tr>'\n    return(row)\n    #return(  f'<tr>' + f'<td align = \"left\">{sent}</td>' + '<td>&nbsp;</td></tr>')\n    \ndef display_article(serial , title, url , sentences, score , lst_other_keywords \n                    , lst_cost_benefits, lst_funding_infra_sentences\n                    , lst_all_urls, authors, publish_date, npi_count, paper_id):\n    if (publish_date == NOT_FOUND):\n        publish_date = \"N/A\"\n    if (url != \"NOT FOUND\"):\n        link_text = f'<a href=\"{url}\" target=\"_blank\">{url}</a>'\n    else:\n        link_text = \"N/A\"\n    text =  f'<h3>{serial}: {title}</h3><table border = \"1\">'\n    tdstring = get_td_string() #'<td style=\"text-align: left; vertical-align: middle;\">'\n    text_info = f'&nbsp;&nbsp;&nbsp;&nbsp;<b>Score:</b> {score} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Date:</b> {publish_date} NPI Count:{npi_count}'\n    text_1 = '<tr>' + tdstring  + '<b>URL:</b>' + link_text + f'{text_info}</td>' + '</tr>'\n    text_paper = '<tr>' + tdstring  + '<b>Paper ID:</b>'+ f'{paper_id}</td>' + '</tr>'\n    text_author = '<tr>' + tdstring + f'<b>Author(s): </b>{authors}</td></tr>'\n    text = text + text_1 + text_paper + text_author\n    #text += ''.join([f'<td><b>{col}</b></td>' for col in df.columns.values]) + '</tr>'\n    i = 0\n    if (len(sentences) > 0):\n        #text +=  f'<tr><td align =\"left\"><b>Relevant Sentences</b></td></tr>'\n        text +=  tdstring + '<b>Relevant Sentences</b></td></tr>'\n        for sent in sentences:\n            i = i + 1  \n            text +=  get_sentence_tr(sent)\n    \n    if (len(lst_other_keywords) > 0):\n        text +=  tdstring + '<b>Sentences containing keywords - \"rapid\", \"design\", \"experiments\", \"assessment\" (and/or)</b></td></tr>'\n        for sent in lst_other_keywords:\n            i = i + 1  \n            text +=  get_sentence_tr(sent)\n    \n    if (len(lst_cost_benefits) > 0):\n        text +=  tdstring + '<b>Sentences containing keywords - \"cost\", \"benefits\" (and/or)</b></td></tr>'\n        for sent in lst_cost_benefits:\n            i = i + 1  \n            text +=  get_sentence_tr(sent)\n    \n    if (len(lst_funding_infra_sentences) > 0):\n        text +=  tdstring + '<b>Sentences containing keywords - \"funding\", \"infra\",\"authorities\" (and/or) </b></td></tr>'\n        for sent in lst_funding_infra_sentences:\n            i = i + 1  \n            text +=  get_sentence_tr(sent)  \n    if (len(lst_all_urls) > 0):\n        text +=  tdstring + '<b>All urls which appear in the article</b></td></tr>'\n        str_urls = '<br> '.join([u for u in lst_all_urls])\n        text +=  get_sentence_tr(str_urls)  \n    \n    text += '</table>'\n    display(HTML(text))\n    \ndef get_df_from_article_list(lst_articles): \n    lst = []\n    serial = 0\n    for l in lst_articles:\n        serial +=1\n        str_rel = \"Low\"\n        url = l.main_url\n        if (l.main_url == \"NOT FOUND\"):\n            url = \"N/A\"\n        if (l.npi_count > 0):\n            str_rel = \"High\"\n        dict_row = {\"Serial\": serial, \"Title\":l.title, \"URL\": url, \"Score\": l.score , \"Relevance\": str_rel, \"PaperID\": l.paper_id}\n        lst.append(dict_row)\n    #print(len(lst_articles), len(lst))\n    return(pd.DataFrame(lst))\n\ndef get_processing_flag(module):\n    retval = False\n    if (module == BIORXIV):\n        if (SAMPLE_SIZE_BIORXIV != -1): retval = True\n    if (module == COMM):\n        if (SAMPLE_SIZE_COMM != -1): retval = True\n    if (module == NON_COMM):\n        if (SAMPLE_SIZE_NON_COMM != -1): retval = True      \n    if (module == CUSTOM_LICENSE):\n        if (SAMPLE_SIZE_CUSTOM_LICENSE != -1): retval = True\n    return(retval)\n\ndef print_user_message(mess = \"Pl provide\" ):\n    m = f'<font  size=4 , color=grey >{mess}</font>'\n    display(HTML(m))\n    \ndef display_dataframe(df, title = \"\"):\n    #tdstring = f'<td style=\"text-align: left; vertical-align: middle; font-size:1.2em;\">{v}</td>'\n    if (title != \"\"):\n        text = f'<h2>{title}</h2><table><tr>'\n    else:\n        text = '<table><tr>'\n    text += ''.join([f'<td style=\"text-align: left; vertical-align: middle; font-size:1.2em;\"><b>{col}</b></td>' for col in df.columns.values]) + '</tr>'\n    for row in df.itertuples():\n        #text +=  '<tr>' + ''.join([f'<td valign=\"top\">{v}</td>' for v in row[1:]]) + '</tr>'\n        text +=  '<tr>' + ''.join([ f'<td style=\"text-align: left; vertical-align: middle; font-size:1.1em;\">{v}</td>' for v in row[1:]]) + '</tr>'\n    text += '</table>'\n    display(HTML(text))\n    \ndef start_td():\n    tdstring = '<td style=\"text-align: center; vertical-align: middle; font-size:1.2em;\">'\n    return(tdstring)\ndef end_td():\n    tdstring = '</td>'\n    return(tdstring)\ndef get_bolded(tstr):\n    tdstring = '<b>'+ tstr + '</b>'\n    return(tdstring)\n\ndef get_sentence_tr_vs(sent):\n    row = get_td_string() + f'{sent}</td></tr>'\n    return(row)\n    #return(  f'<tr>' + f'<td align = \"left\">{sent}</td>' + '<td>&nbsp;</td></tr>')\n    \ndef display_data_processing_info():\n    text =  f'<h3>Table: Data Processing Information</h3><table border = \"1\">'\n    \n    td_header_1 = start_td() + get_bolded(\"Module\") + end_td() \n    td_header_2 = start_td() + get_bolded(\"Total articles\") + end_td() \n    td_header_3 = start_td() + get_bolded(\"Processed articles\") + end_td() \n    td_header_4 = start_td() + get_bolded(\"Number of articles of interest\") + end_td() \n    td_header_5 = start_td() + get_bolded(\"Excerpts of interest\") + end_td() \n    \n    text_header = \"\\n<tr>\" + td_header_1 + td_header_2  + td_header_3 + td_header_4 + td_header_5+ \"</tr>\\n\"\n    #text_header = text_header +  \"<tr>\" + start_td() + get_bolded(\"total articles\") + end_td() + \"</tr>\\n\"\n    \n    text = text + text_header\n    \n    if get_processing_flag(BIORXIV):   \n        td_data_1 = start_td() + \"Biorxiv/Medrxiv\" + end_td()\n        td_data_2 = start_td() + str(count_biorxiv_orig) + end_td()\n        td_data_3 = start_td() + str(df_biorxiv.shape[0]) + end_td()\n        td_data_4 = start_td() + str(df_biorxiv_filter.shape[0]) + end_td()\n        td_data_5 = start_td() + str(sum_bio_sents) + end_td()\n        text_row  = \"\\n<tr>\" + td_data_1 +  td_data_2 + td_data_3 + td_data_4 + td_data_5 + \"</tr>\\n\"\n        text = text +  text_row\n        \n    if get_processing_flag(NON_COMM):   \n        td_data_1 = start_td() + \"Non Comm\" + end_td()\n        td_data_2 = start_td() + str(count_non_comm_orig) + end_td()\n        td_data_3 = start_td() + str(df_non_comm.shape[0]) + end_td()\n        td_data_4 = start_td() + str(df_non_comm_filter.shape[0]) + end_td()\n        td_data_5 = start_td() + str(sum_non_comm_sents) + end_td()\n        text_row  = \"\\n<tr>\" + td_data_1 +  td_data_2 + td_data_3 + td_data_4 + td_data_5 + \"</tr>\\n\"\n        text = text +  text_row\n        \n    if get_processing_flag(COMM):   \n        td_data_1 = start_td() + \"Comm\" + end_td()\n        td_data_2 = start_td() + str(count_comm_orig) + end_td()\n        td_data_3 = start_td() + str(df_comm.shape[0]) + end_td()\n        td_data_4 = start_td() + str(df_comm_filter.shape[0]) + end_td()\n        td_data_5 = start_td() + str(sum_comm_sents) + end_td()\n        text_row  = \"\\n<tr>\" + td_data_1 +  td_data_2 + td_data_3 + td_data_4 + td_data_5 + \"</tr>\\n\"\n        text = text +  text_row    \n        \n    if get_processing_flag(CUSTOM_LICENSE):   \n        td_data_1 = start_td() + \"Custom License\" + end_td()\n        td_data_2 = start_td() + str(count_custom_license_orig) + end_td()\n        td_data_3 = start_td() + str(df_custom_license.shape[0]) + end_td()\n        td_data_4 = start_td() + str(df_custom_license_filter.shape[0]) + end_td()\n        td_data_5 = start_td() + str(sum_cl_sents) + end_td()\n        text_row  = \"\\n<tr>\" + td_data_1 +  td_data_2 + td_data_3 + td_data_4 + td_data_5 + \"</tr>\\n\"\n        text = text +  text_row   \n    text += '\\n</table>'\n    display(HTML(text))\n\nNOT_FOUND = \"<not found>\"\ndef add_list(*lsts):\n    retlist = []\n    for l in lsts:\n        retlist =retlist + l\n    return(list(set(retlist)))\n\ndef get_date(paper_url):\n    retval = NOT_FOUND\n    if paper_url.startswith('https://doi.org/'): # Could use /10.1101\n        retval = paper_url.replace('https://doi.org/10.1101/', '')[0:10]\n    return(retval)\n\nclass article():\n    def __init__(self, paper_id, score, journal, lst_snippets, npi_count):\n        self.publish_date = \"\"\n        self.npi_count = npi_count\n        self.paper_id = paper_id\n        self.main_url = \"\"\n        self.score = score\n        self.journal = journal\n        self.lst_sentences = []\n        self.lst_snippets = lst_snippets\n        self.nlp_text = None\n        self.text = None\n        self.lst_urls = []\n        self.title = None\n        self.authors = None\n        self.lst_funding_infra_snippets = []\n        self.lst_funding_infra_sentences = []\n        self.lst_cost_benefits_snippets =[]\n        self.lst_cost_benefits_sentences = []\n        self.lst_all_sentences = []\n        self.lst_other_keywords_snippets  = []\n        self.lst_other_keywords_sentences = []\n        self.count_sentences = 0\n        self.initialize()\n        self.consolidate_all_sentences()\n        \n    def consolidate_all_sentences(self):\n        self.lst_all_sentences =  add_list(self.lst_sentences\n                                         , self.lst_funding_infra_sentences\n                                         , self.lst_cost_benefits_sentences\n                                         , self.lst_other_keywords_sentences)   \n        self.count_sentences = len(self.lst_all_sentences)\n    def save_biorxiv_all_info(self):\n        write_to_file(FILE_BIORXIV_ARTICLES_INFO, \"=====================  START ===========================\\n\")\n        write_to_file(FILE_BIORXIV_ARTICLES_INFO, \"TITLE:\" + self.title + \"\\n\")\n        write_to_file(FILE_BIORXIV_ARTICLES_INFO, \"SENTENCES:\" + ' \\n'.join(self.lst_sentences))\n        write_to_file(FILE_BIORXIV_ARTICLES_INFO, \"=====================  END ===========================\\n\")\n    def find_url_in_text(self):\n        self.main_url , self.lst_urls = find_url_in_text(self.nlp_text)\n    def initialize(self):\n        self.text, self.title, self.authors = get_paper_info(self.paper_id, self.journal)\n        self.nlp_text = nlp(self.text)\n        self.find_url_in_text()\n        self.get_sents_from_snippets()\n        self.get_cost_benefits_info()\n        self.get_funding_infra_info()\n        self.get_other_keywords_info()\n        self.publish_date = get_date(self.main_url)\n    def get_sents_from_snippets(self):\n        self.lst_sentences = []\n        self.lst_sentences = get_sents_from_snippets(self.lst_snippets, self.nlp_text, self.paper_id)\n    def get_funding_infra_info(self):\n        phrase_list = [ u\"funding\", u\"fund\"\n                       , u\"authorities\"\n                       , u\"infrastructure\"]\n        count, snippets = find_phrases_in_text(self.nlp_text, phrase_list, span_start = 1, span_end = 1 )\n        self.lst_funding_infra_snippets = snippets\n        if (count > 0):\n            self.lst_funding_infra_sentences = get_sents_from_snippets(self.lst_funding_infra_snippets, self.nlp_text, self.paper_id) \n    \n    def get_other_keywords_info(self):\n        phrase_list = [ u\"experiment\", u\"rapid\", u\"assesment\", u\"design\"]\n        count, snippets = find_phrases_in_text(self.nlp_text, phrase_list, span_start = 1, span_end = 1 )\n        self.lst_other_keywords_snippets = snippets\n        if (count > 0):\n            self.lst_other_keywords_sentences = get_sents_from_snippets(snippets, self.nlp_text, self.paper_id)\n    \n    def get_cost_benefits_info(self):\n        phrase_list = [u\"cost\", u\"benefit\"]\n        count, snippets = find_phrases_in_text(self.nlp_text, phrase_list, span_start = 1, span_end = 1 )\n        self.lst_cost_benefits_snippets = snippets\n        if (count > 0):\n            self.lst_cost_benefits_sentences = get_sents_from_snippets(self.lst_cost_benefits_snippets, self.nlp_text, self.paper_id)  \n    def info_cost_benefits(self):\n        if ((len(self.lst_cost_benefits_sentences) > 0) & (len(self.lst_cost_benefits_sentences) < 10)):\n            self.print_header()\n            print(\"Cost Benefits Information:\", self.lst_cost_benefits_sentences)\n            print(\"Number of cost benefits sentences found:\", len(self.lst_cost_benefits_sentences))\n            self.print_footer()\n    def print_header(self):\n        strformat = \"================== START ===========================\\n TITLE: {} \\n\".format(self.title)\n        print(strformat)\n    def print_footer(self):\n        strformat = \"RELEVANT URLS:\\n {} \\n PAPER ID {}\".format(self.lst_urls, self.paper_id)\n        print(strformat)\n        print(\"PaperID: \", self.paper_id , \"  Score:\" , self.score)\n        print(\"======================= END  ==========================================\\n\")\n    def print_1_basic_article_information(self):\n        self.print_header()\n        print(\" -------------- PRINTING SOME EXTRACTED SENTENCES (MAX 5) Related to NPI -------------- \")\n        if len(self.lst_sentences) > 5:\n            print_list(self.lst_sentences[0:5])\n            #print(self.lst_sentences[0:5])\n        else:\n            print_list(self.lst_sentences)\n        self.print_footer()\n\ndef get_objectlist_from_df(df):\n    lst_objs = list(map(article, \n                          (df['paper_id']) ,\n                          df['score'], \n                          df['module'],\n                          df['lst_snippets'],\n                          df['text_find_count_npi']))\n    #print(\"sorting\")\n    #lst_objs.sort(key=lambda x: x.score, reverse=True)\n    return (lst_objs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing Summary\n\n## Summary of Articles of interest found for: Non Pharmaceutical Interventions\n\nFollowing gives an initial idea of how many articles of interest found after initial screening. The final list of articles will be lesser in number as there will be further screening of articles. Relevant articles are available in [Detailed Output](notebook#Detailed-Output) section."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Processing Happens here\nif get_processing_flag(BIORXIV):\n    path = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\n    start_time = time.time()\n    df_biorxiv_master, df_biorxiv, count_biorxiv_orig = process_a_module(path, SAMPLE_SIZE = SAMPLE_SIZE_BIORXIV, MODULE = BIORXIV)\n    df_biorxiv_filter = df_biorxiv_master[df_biorxiv_master['score'] > 0].reset_index()#.sort_values(['score'], ascending = False)\n    lst_obj_biorxiv = get_objectlist_from_df(df_biorxiv_filter)\n    lst_obj_biorxiv.sort(key=lambda x: x.score, reverse=True)\n    sum_bio_sents = sum(c.count_sentences for c in lst_obj_biorxiv)\n    #print(sum_bio_sents)\n    \nif get_processing_flag(COMM):    \n    path = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'\n    start_time = time.time()\n    df_comm_master, df_comm, count_comm_orig = process_a_module(path, SAMPLE_SIZE_COMM, COMM)\n    df_comm_filter    = df_comm_master[df_comm_master['score'] > 0].reset_index()\n    lst_obj_comm = get_objectlist_from_df(df_comm_filter)\n\n    lst_obj_comm.sort(key=lambda x: x.score, reverse=True)\n    sum_comm_sents = sum(c.count_sentences for c in lst_obj_comm)\n    #print(sum_bio_sents)\n    \nif get_processing_flag(NON_COMM):    \n    path = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/'\n    start_time = time.time()\n    df_non_comm_master, df_non_comm, count_non_comm_orig = process_a_module(path, SAMPLE_SIZE_NON_COMM, NON_COMM)\n    df_non_comm_filter = df_non_comm_master[df_non_comm_master['score'] > 0].reset_index()\n    lst_obj_non_comm = get_objectlist_from_df(df_non_comm_filter)\n    lst_obj_non_comm.sort(key=lambda x: x.score, reverse=True)\n    sum_non_comm_sents = sum(c.count_sentences for c in lst_obj_non_comm)\n    #len(lst_obj_non_comm)\n    #print(sum_non_comm_sents)\n    \nif get_processing_flag(CUSTOM_LICENSE):   \n    path = \"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/\"\n    start_time = time.time()\n    df_custom_license_master, df_custom_license, count_custom_license_orig = process_a_module(path, SAMPLE_SIZE_CUSTOM_LICENSE, CUSTOM_LICENSE)\n    df_custom_license_filter  = df_custom_license_master[df_custom_license_master['score'] > 0].reset_index()\n    lst_obj_custom_license = get_objectlist_from_df(df_custom_license_filter)\n    lst_obj_custom_license.sort(key=lambda x: x.score, reverse=True)\n    sum_cl_sents = sum(c.count_sentences for c in lst_obj_custom_license)\n    #print(sum_cl_sents)\n\ndisplay_data_processing_info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rate of finding Articles and useful snippets (Hit Ratios)\n\nHit ratios determine how relevant a database is in finding article of interests. A higher hit ratio is desired."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#df_data = pd.DataFrame()\nlst = []\nlst_hit_ratio = []\nif get_processing_flag(BIORXIV):\n    \n    dict_hit_ratio = { \"module\": \"biorxiv\", \"ratio\": df_biorxiv_filter.shape[0]/df_biorxiv.shape[0], \"type\" : \"article_hit_ratio\" }    \n    lst_hit_ratio.append(dict_hit_ratio)\n    dict_hit_ratio = { \"module\": \"biorxiv\", \"ratio\": sum_bio_sents/df_biorxiv.shape[0], \"type\" : \"snippets_hit_ratio\" }\n    lst_hit_ratio.append(dict_hit_ratio)\n    \n    \n    dict_row = {\"count\": count_biorxiv_orig , \"module\" : \"biorxiv\", \"type\" :\"total\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_biorxiv.shape[0] , \"module\" : \"biorxiv\", \"type\" :\"processed\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_biorxiv_filter.shape[0] , \"module\" : \"biorxiv\", \"type\" :\"found\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": sum_bio_sents , \"module\" : \"biorxiv\", \"type\" :\"excerpts\"}\n    lst.append(dict_row)\n\nif get_processing_flag(NON_COMM):\n    \n   \n    dict_hit_ratio = { \"module\": \"non_comm\", \"ratio\": df_non_comm_filter.shape[0]/df_non_comm.shape[0], \"type\" : \"article_hit_ratio\" }    \n    lst_hit_ratio.append(dict_hit_ratio)\n    dict_hit_ratio = { \"module\": \"non_comm\", \"ratio\": sum_non_comm_sents/df_non_comm.shape[0], \"type\" : \"snippets_hit_ratio\" }\n    lst_hit_ratio.append(dict_hit_ratio)\n    \n    \n    dict_row = {\"count\": count_non_comm_orig , \"module\" : \"non_comm\", \"type\" :\"total\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_non_comm.shape[0] , \"module\" : \"non_comm\", \"type\" :\"processed\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_non_comm_filter.shape[0] , \"module\" : \"non_comm\", \"type\" :\"found\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": sum_non_comm_sents , \"module\" : \"non_comm\", \"type\" :\"excerpts\"}\n    lst.append(dict_row)\n\nif get_processing_flag(COMM):\n    \n    dict_hit_ratio = { \"module\": \"comm\", \"ratio\": df_comm_filter.shape[0]/df_comm.shape[0], \"type\" : \"article_hit_ratio\" }    \n    lst_hit_ratio.append(dict_hit_ratio)\n    dict_hit_ratio = { \"module\": \"comm\", \"ratio\": sum_comm_sents/df_comm.shape[0], \"type\" : \"snippets_hit_ratio\" }\n    lst_hit_ratio.append(dict_hit_ratio)\n    \n    dict_row = {\"count\": count_comm_orig , \"module\" : \"comm\", \"type\" :\"total\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_comm.shape[0] , \"module\" : \"comm\", \"type\" :\"processed\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_comm_filter.shape[0] , \"module\" : \"comm\", \"type\" :\"found\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": sum_comm_sents , \"module\" : \"comm\", \"type\" :\"excerpts\"}\n    lst.append(dict_row)\n\nif get_processing_flag(CUSTOM_LICENSE):\n    \n\n    dict_hit_ratio = { \"module\": \"custom_license\", \"ratio\": df_custom_license_filter.shape[0]/df_custom_license.shape[0], \"type\" : \"article_hit_ratio\" }    \n    lst_hit_ratio.append(dict_hit_ratio)\n    dict_hit_ratio = { \"module\": \"custom_license\", \"ratio\": sum_cl_sents/df_custom_license.shape[0], \"type\" : \"snippets_hit_ratio\" }\n    lst_hit_ratio.append(dict_hit_ratio)\n    \n    \n    dict_row = {\"count\": count_custom_license_orig , \"module\" : \"custom_license\", \"type\" :\"total\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_custom_license.shape[0] , \"module\" : \"custom_license\", \"type\" :\"processed\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": df_custom_license_filter.shape[0] , \"module\" : \"custom_license\", \"type\" :\"found\"}\n    lst.append(dict_row)\n    dict_row = {\"count\": sum_cl_sents , \"module\" : \"custom_license\", \"type\" :\"excerpts\"}\n    lst.append(dict_row)\n\ndf_data = pd.DataFrame(lst)\ndf_hit_ratio = pd.DataFrame(lst_hit_ratio)\n#df = df_data\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.bar(df_hit_ratio, x=\"module\", y=\"ratio\", color='type', barmode='group',  title = \"Hit Ratio of various modules\", template = \"plotly_dark\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.bar(df_data, x=\"type\", y=\"count\", color='module', barmode='group', log_y= True, title = \"Various stats from articles after initial screening\",  template = \"plotly_dark\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Detailed Output\nIn this section, detailed output of high relevance articles are provided. The whole list is available in **Complete List** (Database wise)."},{"metadata":{},"cell_type":"markdown","source":"# Important Biorxiv/Medrxiv articles related to NPI"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if get_processing_flag(BIORXIV):\n    serial = 0\n    for l in lst_obj_biorxiv:\n        if (l.npi_count > 0):\n            serial = serial + 1\n            display_article(serial, l.title, l.main_url, l.lst_sentences, l.score\n                        , l.lst_other_keywords_sentences\n                        , l.lst_cost_benefits_sentences\n                        , l.lst_funding_infra_sentences\n                        , l.lst_urls\n                        , l.authors\n                        , l.publish_date\n                        , l.npi_count\n                        , l.paper_id)\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important Non Comm articles related to NPI"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if get_processing_flag(NON_COMM):\n    serial = 0\n    for l in lst_obj_non_comm:\n        if (l.npi_count > 0):\n            serial = serial + 1\n            display_article(serial, l.title, l.main_url, l.lst_sentences, l.score\n                        , l.lst_other_keywords_sentences\n                        , l.lst_cost_benefits_sentences\n                        , l.lst_funding_infra_sentences\n                        , l.lst_urls\n                        , l.authors\n                        , l.publish_date\n                        , l.npi_count\n                        , l.paper_id)\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important Comm articles related to NPI"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(COMM):\n    serial = 0\n    for l in lst_obj_comm:\n        if (l.npi_count > 0):\n            serial = serial + 1\n            display_article(serial, l.title, l.main_url, l.lst_sentences, l.score\n                        , l.lst_other_keywords_sentences\n                        , l.lst_cost_benefits_sentences\n                        , l.lst_funding_infra_sentences\n                        , l.lst_urls\n                        , l.authors\n                        , l.publish_date\n                        , l.npi_count\n                        , l.paper_id)\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important Custom articles related to NPI"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(CUSTOM_LICENSE):\n    serial = 0\n    for l in lst_obj_custom_license:\n        if (l.npi_count > 0):\n            serial = serial + 1\n            display_article(serial, l.title, l.main_url, l.lst_sentences, l.score\n                        , l.lst_other_keywords_sentences\n                        , l.lst_cost_benefits_sentences\n                        , l.lst_funding_infra_sentences\n                        , l.lst_urls\n                        , l.authors\n                        , l.publish_date\n                        , l.npi_count\n                        , l.paper_id)\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Complete List - Biorxiv/Medrxiv"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(BIORXIV):\n    display_dataframe(get_df_from_article_list(lst_obj_biorxiv))\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Complete List - Non Comm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(NON_COMM):\n    display_dataframe(get_df_from_article_list(lst_obj_non_comm))\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Complete List - Comm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(COMM):\n    display_dataframe(get_df_from_article_list(lst_obj_comm))\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Complete List - Custom License"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if get_processing_flag(CUSTOM_LICENSE):\n    display_dataframe(get_df_from_article_list(lst_obj_custom_license))\nelse:\n    print_user_message(\"This module has not been processed. Please set Sample size other than -1 in the Input area for this module to be processed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Credit to https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/ - This kernel has helped in processing data from json files to respective datasets."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}