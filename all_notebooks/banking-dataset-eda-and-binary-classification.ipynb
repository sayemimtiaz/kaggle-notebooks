{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About Dataset\nThere has been a revenue decline in a Portuguese Bank and they would like to know what actions to take. After investigation, they found that the root cause was that their customers are not investing enough for long term deposits. So the bank would like to identify existing customers that have higher chance to subscribe for a long term deposit and focus marketing efforts on such customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import data modelling libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset\ndata= pd.read_csv(\"../input/banking-dataset-classification/new_train.csv\")\n\n# check shape of dataset\nprint(\"shape of the data:\", data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types of all columns\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### check missing data \nOne of the main steps in data preprocessing is handling missing data. Missing data means absence of observations in columns that can be caused while procuring the data, lack of information, incomplete results etc. Feeding missing data to your machine learning model could lead to wrong prediction or classification. Hence it is necessary to identify missing values and treat them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### check for class imbalance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# target class count\ndata[\"y\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[\"y\"])\nplt.title(\"target variable\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of class present in target variable(y) \nprint(\"percentage of NO and YES\\n\",data[\"y\"].value_counts()/len(data)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The class distribution in the target variable is ~89:11 indicating an imbalance dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n### univariate analysis of categorical variables","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# indentifying the categorical variables\ncat_var= data.select_dtypes(include= [\"object\"]).columns\nprint(cat_var)\n\n# plotting bar chart for each categorical variable\nplt.style.use(\"ggplot\")\n\nfor column in cat_var:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    data[column].value_counts().plot(kind=\"bar\")\n    plt.xlabel(column)\n    plt.ylabel(\"number of customers\")\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n- The top three professions that our customers belong to are - administration, blue-collar jobs and technicians.\n- A huge number of the customers are married.\n- Majority of the customers do not have a credit in default\n- Many of our past customers have applied for a housing loan but very few have applied for personal loans.\n- Cell-phones seem to be the most favoured method of reaching out to customers.\n- Many customers have been contacted in the month of **May**.\n- The plot for the target variable shows heavy imbalance in the target variable. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The missing values in some columns have been represented as `unknown`. `unknown` represents missing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing \"unknown\" with the mode\nfor column in cat_var:\n    mode= data[column].mode()[0]\n    data[column]= data[column].replace(\"unknown\", mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis of Numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# indentifying the numerical variables\nnum_var= data.select_dtypes(include=np.number)\nnum_var.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# plotting histogram for each numerical variable\nplt.style.use(\"ggplot\")\nfor column in [\"age\", \"duration\", \"campaign\"]:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.distplot(data[column], kde=True)\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation :\n- As we can see from the histogram, the features `age`, `duration` and `campaign` are heavily skewed and this is due to the presence of outliers as seen in the boxplot for these features. \n- Looking at the plot for `pdays`, we can infer that majority of the customers were being contacted for the first time because as per the feature description for `pdays` the value 999 indicates that the customer had not been contacted previously. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since `pdays` and `previous` consist majorly only of a single value, their variance is quite less and hence we can drop them since technically will be of no help in prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=[\"pdays\", \"previous\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis of  Categorical Columns","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\nfor column in cat_var:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.countplot(data[column], hue=data[\"y\"])\n    plt.title(column)    \n    plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- Customers having administrative jobs form the majority amongst those who have subscirbed to the term deposit.\n- They are married \n- They hold a university degree\n- They do not hold a credit in default\n- Housing loan doesn't seem a priority to check for since an equal number of customers who have and have not subscribed to it seem to have subscribed to the term deposit.\n- Cell-phones should be the preferred mode of contact for contacting customers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Handling Outliers\nOutliers cause significant impact on the Mean and Variance.It becomes necessary to treat the outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`age` `duration` and `campaign` are skewed towards right, we will compute the IQR and replace the outliers with the lower and upper boundaries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute interquantile range to calculate the boundaries\nlower_boundries= []\nupper_boundries= []\nfor i in [\"age\", \"duration\", \"campaign\"]:\n    IQR= data[i].quantile(0.75) - data[i].quantile(0.25)\n    lower_bound= data[i].quantile(0.25) - (1.5*IQR)\n    upper_bound= data[i].quantile(0.75) + (1.5*IQR)\n    \n    print(i, \":\", lower_bound, \",\",  upper_bound)\n    \n    lower_boundries.append(lower_bound)\n    upper_boundries.append(upper_bound)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_boundries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_boundries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace the all the outliers which is greater then upper boundary by upper boundary\nj = 0\nfor i in [\"age\", \"duration\", \"campaign\"]:\n    data.loc[data[i] > upper_boundries[j], i] = int(upper_boundries[j])\n    j = j + 1  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, \n- for `age` the lower boundary (9.5) < minimum value (17)\n- for `duration` and `campaigh` the lower boundaries are negative (-221.0), (-2.0) resp.<br>\nreplacing outliers with the lower boundary is not required","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# without outliers\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After replacing the outliers with the upper boundary, the maximum values has been changed without impacting any other parameters like mean, standard deviation and quartiles.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical Features\nMachine learning algorithm can only read numerical values. It is therefore essential to encode categorical features into numerical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical features\ncat_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check categorical class\nfor i in cat_var:\n    print(i, \":\", data[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features like `job` `education` `month` `day_of_week ` has so many categories, we will Label Encode them as One Hot Encoding would create so many columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing label encoder\nle= LabelEncoder()\n\n# iterating through each categorical feature and label encoding them\nfor feature in cat_var:\n    data[feature]= le.fit_transform(data[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoded dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating independent and dependent variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature variables\nx= data.iloc[:, :-1]\n\n# target variable\ny= data.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Correlation of feature variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.heatmap(data.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no features that are highly correlated and inversely correlated. If we had, we could have written the condition that if the correlation is higher than 0.8 (or can be any threshold value depending on the domain knowledge) and less than -0.8, we could have drop those features. Because those correlated features would have been doing the same job.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Handling imbalanced dataset\nSince the class distribution in the target variable is ~89:11 indicating an imbalance dataset, we need to resample it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialising oversampling\nsmote= SMOTETomek(0.75)\n\n#implementing oversampling to training data\nx_sm, y_sm= smote.fit_sample(x,y)\n\n# x_sm and y_sm are the resampled data\n\n# target class count of resampled dataset\ny_sm.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting resampled data in train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test= train_test_split(x_sm, y_sm, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gridsearch and hyperparameter tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the classifier\nlog_reg= LogisticRegression()\n\n# selecting hyperparameter tuning\nlog_param= {\"C\": 10.0**np.arange(-2,3), \"penalty\": [\"l1\", \"l2\"]}\n\n# defining stratified Kfold cross validation\ncv_log= StratifiedKFold(n_splits=5)\n\n# using gridsearch for respective parameters\ngridsearch_log= GridSearchCV(log_reg, log_param, cv=cv_log, scoring= \"f1_macro\", n_jobs=-1, verbose=2)\n\n# fitting the model on resampled data\ngridsearch_log.fit(x_train, y_train)\n\n# printing best score and best parameters\nprint(\"best score is:\" ,gridsearch_log.best_score_)\nprint(\"best parameters are:\" ,gridsearch_log.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking model performance\ny_predicted= gridsearch_log.predict(x_test)\n\ncm= confusion_matrix(y_test, y_predicted)\nprint(cm)\nsns.heatmap(cm, annot=True)\nprint(accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest\nrf= RandomForestClassifier()\n\nrf_param= { \n           \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n           \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n#            \"max_depth\": [4,5,6,7,8],\n           \"max_depth\": [int(x) for x in np.linspace(start=5, stop=30, num=6)],\n           \"min_samples_split\": [5,10,15,100],\n           \"min_samples_leaf\": [1,2,5,10],\n           \"criterion\":['gini', 'entropy'] \n          }\n\ncv_rf= StratifiedKFold(n_splits=5)\n\nrandomsearch_rf= RandomizedSearchCV(rf, rf_param, cv=cv_rf, scoring= \"f1_macro\", n_jobs=-1, verbose=2, n_iter=10)\n\nrandomsearch_rf.fit(x_train, y_train)\n\nprint(\"best score is:\", randomsearch_rf.best_score_)\nprint(\"best parameters are:\", randomsearch_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking model performance\ny_predicted_rf= randomsearch_rf.predict(x_test)\n\nprint(confusion_matrix(y_test, y_predicted_rf))\nsns.heatmap(confusion_matrix(y_test, y_predicted_rf), annot=True)\nprint(accuracy_score(y_test, y_predicted_rf))\nprint(classification_report(y_test, y_predicted_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction on the Test dataset\nWe have to perform the same preprocessing operations on the test data that we have performed on the train data. But here we already have preprocessed data which is present in the csv file new_test.csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data= pd.read_csv(\"../input/banking-dataset-classification/new_test.csv\")\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest classifier has given the best metric score on the validation data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting the test data\ny_predicted= randomsearch_rf.predict(test_data)\ny_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset of predicted values for target variable y\nprediction= pd.DataFrame(y_predicted, columns=[\"y_predicted\"])\nprediction_dataset= pd.concat([test_data, prediction], axis=1)\nprediction_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}