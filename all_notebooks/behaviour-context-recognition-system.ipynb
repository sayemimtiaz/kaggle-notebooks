{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Behavioral context recognition system in a wild.\n\nThis Kaggle note used ExtraSensory Dataset and refinign the python code in the website http://extrasensory.ucsd.edu/ and showing different machine learning algorithms used to deal with system recognitions data.\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports\nImport pandas, numpy, matplotlib,and seaborn. Then set %matplotlib inline"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom io import StringIO;\nimport os;\nimport os.path;\nimport glob as glob\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\n\n    o   Defining features\n    o   Defining context labels\n    o   Identifying relationship between features and context labels\n    o   Identifying relationship between labels\n    o   Select the features to work with for target label\n    o   Defining 1 target labels to work with.\n    o   Cleaning missing labels and replacing NaN values with dataset samples\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sensor_names_from_features(feature_names):\n    feat_sensor_names = np.array([None for feat in feature_names]);\n    for (fi,feat) in enumerate(feature_names):\n        if feat.startswith('raw_acc'):\n            feat_sensor_names[fi] = 'Acc';\n            pass;\n        elif feat.startswith('proc_gyro'):\n            feat_sensor_names[fi] = 'Gyro';\n            pass;\n        elif feat.startswith('raw_magnet'):\n            feat_sensor_names[fi] = 'Magnet';\n            pass;\n        elif feat.startswith('watch_acceleration'):\n            feat_sensor_names[fi] = 'WAcc';\n            pass;\n        elif feat.startswith('watch_heading'):\n            feat_sensor_names[fi] = 'Compass';\n            pass;\n        elif feat.startswith('location'):\n            feat_sensor_names[fi] = 'Loc';\n            pass;\n        elif feat.startswith('location_quick_features'):\n            feat_sensor_names[fi] = 'Loc';\n            pass;\n        elif feat.startswith('audio_naive'):\n            feat_sensor_names[fi] = 'Aud';\n            pass;\n        elif feat.startswith('audio_properties'):\n            feat_sensor_names[fi] = 'AP';\n            pass;\n        elif feat.startswith('discrete'):\n            feat_sensor_names[fi] = 'PS';\n            pass;\n        elif feat.startswith('lf_measurements'):\n            feat_sensor_names[fi] = 'LF';\n            pass;\n        else:\n            raise ValueError(\"!!! Unsupported feature name: %s\" % feat);\n\n        pass;\n\n    return feat_sensor_names;  \n\n'''\nRead the data (precomputed sensor-features and labels) for a user.\nThis function assumes the user's data file is present.\n'''\ndef validate_column_names_are_consistent(old_column_names,new_column_names):\n    if len(old_column_names) != len(new_column_names):\n        raise ValueError(\"!!! Inconsistent number of columns.\");\n        \n    for ci in range(len(old_column_names)):\n        if old_column_names[ci] != new_column_names[ci]:\n            raise ValueError(\"!!! Inconsistent column %d) %s != %s\" % (ci,old_column_names[ci],new_column_names[ci]));\n        pass;\n    return;\ndef get_label_pretty_name(label):\n    if 'FIX_walking' in label:\n        return 'Walking';\n    if 'FIX_running' in label:\n        return 'Running';\n    if 'LOC_main_workplace' in label:\n        return 'At main workplace' ;\n    if 'OR_indoors' in label:\n        return 'Indoors';\n    if 'OR_outside' in label:\n        return 'Outside';\n    if 'LOC_home' in label:\n        return 'At home';\n    if 'FIX_restaurant' in label:\n        return 'At a restaurant';\n    if 'OR_exercise' in label:\n        return 'Exercise';\n    if  'LOC_beach' in label:\n        return 'At the beach';\n    if 'OR_standing' in label:\n        return 'Standing';\n    if 'WATCHING_TV' in label:\n        return 'Watching TV'\n    else:\n        label.replace('label:', '')\n    if label.endswith('_'):\n        label = label[:-1] + ')';\n        pass;\n    \n    label = label.replace('__',' (').replace('_',' ');\n    label = label[0] + label[1:].lower();\n    label = label.replace('i m','I\\'m');\n    return label;\n\ndef get_phone_label(label):\n    if label == 'FIX_walking':\n        return 'Walking';\n    if label == 'FIX_running':\n        return 'Running';\n    if label == 'LOC_main_workplace':\n        return 'At main workplace';\n    if label == 'OR_indoors':\n        return 'Indoors';\n    if label == 'OR_outside':\n        return 'Outside';\n    if label == 'LOC_home':\n        return 'At home';\n    if label == 'FIX_restaurant':\n        return 'At a restaurant';\n    if label == 'OR_exercise':\n        return 'Exercise';\n    if label == 'LOC_beach':\n        return 'At the beach';\n    if label == 'OR_standing':\n        return 'Standing';\n    if label == 'WATCHING_TV':\n        return 'Watching TV'\n    \n    if label.endswith('_'):\n        label = label[:-1] + ')';\n        pass;\n    \n    label = label.replace('__',' (').replace('_',' ');\n    label = label[0] + label[1:].lower();\n    label = label.replace('i m','I\\'m');\n    \n    # if lable is phone related then return the label\n    if \"Phone\" not in label: \n        return label\n    else:\n        return False\n\n\ndef get_sensor_names_from_features(feature_names):\n    feat_sensor_names = np.array([None for feat in feature_names]);\n    for (fi,feat) in enumerate(feature_names):\n        if feat.startswith('raw_acc'):\n            feat_sensor_names[fi] = 'Acc';\n            pass;\n        elif feat.startswith('proc_gyro'):\n            feat_sensor_names[fi] = 'Gyro';\n            pass;\n        elif feat.startswith('raw_magnet'):\n            feat_sensor_names[fi] = 'Magnet';\n            pass;\n        elif feat.startswith('watch_acceleration'):\n            feat_sensor_names[fi] = 'WAcc';\n            pass;\n        elif feat.startswith('watch_heading'):\n            feat_sensor_names[fi] = 'Compass';\n            pass;\n        elif feat.startswith('location'):\n            feat_sensor_names[fi] = 'Loc';\n            pass;\n        elif feat.startswith('location_quick_features'):\n            feat_sensor_names[fi] = 'Loc';\n            pass;\n        elif feat.startswith('audio_naive'):\n            feat_sensor_names[fi] = 'Aud';\n            pass;\n        elif feat.startswith('audio_properties'):\n            feat_sensor_names[fi] = 'AP';\n            pass;\n        elif feat.startswith('discrete'):\n            feat_sensor_names[fi] = 'PS';\n            pass;\n        elif feat.startswith('lf_measurements'):\n            feat_sensor_names[fi] = 'LF';\n            pass;\n        else:\n            raise ValueError(\"!!! Unsupported feature name: %s\" % feat);\n\n        pass;\n\n    return feat_sensor_names;  \n\ndef get_features_from_data(users_df):\n    for (ci,col) in enumerate(users_df.columns):\n        if col.startswith('label:'):\n            first_label_ind = ci;\n            break;\n    pass;\n    feature_names = users_df.columns[1:first_label_ind];\n    return np.array(feature_names)\n\ndef project_features_to_selected_sensors(feature_names,sensors_to_use):\n\n    feature_names_arr = []\n    for sensor in sensors_to_use:\n        if sensor == 'Acc':\n            for feature in feature_names:\n                #print (type(feature))\n                if (feature.startswith('raw_acc')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'WAcc':\n            for feature in feature_names:\n                if (feature.startswith('watch_acceleration')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'Gyro':\n            for feature in feature_names:\n                if (feature.startswith('proc_gyro')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'Magnet':\n            for feature in feature_names:\n                if (feature.startswith('raw_magnet')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'Compass':\n            for feature in feature_names:\n                if (feature.startswith('watch_heading')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'Loc':\n            for feature in feature_names:\n                if (feature.startswith('location')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'Aud':\n            for feature in feature_names:\n                if (feature.startswith('audio_naive')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'AP':\n            for feature in feature_names:\n                if (feature.startswith('audio_properties')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'PS':\n            for feature in feature_names:\n                if (feature.startswith('discrete')):\n                    feature_names_arr.append(feature)\n        elif sensor == 'LF':\n            for feature in feature_names:\n                if (feature.startswith('lf_measurements')):\n                    feature_names_arr.append(feature)\n                    \n    return feature_names_arr\n\ndef estimate_standardization_params(X):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        mean_vec = np.nanmean(X,axis=0);\n        std_vec = np.nanstd(X,axis=0);\n        return (mean_vec,std_vec);\n\ndef standardize_features(X,mean_vec,std_vec):\n    # Subtract the mean, to centralize all features around zero:\n    X_centralized = X - mean_vec.reshape((1,-1));\n    # Divide by the standard deviation, to get unit-variance for all features:\n    # * Avoid dividing by zero, in case some feature had estimate of zero variance\n    normalizers = np.where(std_vec > 0., std_vec, 1.).reshape((1,-1));\n    X_standard = X_centralized / normalizers;\n    return X_standard;\n\ndef get_label_names(users_df):\n    # Search for the column of the first label:\n    for (ci,col) in enumerate(users_df.columns):\n        if col.startswith('label:'):\n            first_label_ind = ci;\n            break;\n        pass;\n\n    label_names = np.array(users_df.columns[first_label_ind:-1]);\n    for (li,label) in enumerate(label_names):\n        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n        assert label.startswith('label:');\n        #label_names[li] = label.replace('label:','');\n        pass;\n    \n    return (list(label_names));\n\ndef print_accuracy_repoprt(predictions, y_test):\n\n    accuracy = np.mean(predictions == y_test);\n    # Count occorrences of true-positive, true-negative, false-positive, and false-negative:\n    tp = np.sum(np.logical_and(predictions,y_test));\n    tn = np.sum(np.logical_and(np.logical_not(predictions),np.logical_not(y_test)));\n    fp = np.sum(np.logical_and(predictions,np.logical_not(y_test)));\n    fn = np.sum(np.logical_and(np.logical_not(predictions),y_test));\n\n    # Sensitivity (=recall=true positive rate) and Specificity (=true negative rate):\n    sensitivity = float(tp) / (tp+fn);\n    specificity = float(tn) / (tn+fp);\n\n    # Balanced accuracy is a more fair replacement for the naive accuracy:\n    balanced_accuracy = (sensitivity + specificity) / 2.;\n\n    # Precision:\n    # Beware from this metric, since it may be too sensitive to rare labels.\n    # In the ExtraSensory Dataset, there is large skew among the positive and negative classes,\n    # and for each label the pos/neg ratio is different.\n    # This can cause undesirable and misleading results when averaging precision across different labels.\n    precision = float(tp) / (tp+fp);\n    accuracy_list = [accuracy,sensitivity,specificity,balanced_accuracy,precision]\n    print(\"-\"*10);\n    print('Accuracy*:         %.2f' % accuracy);\n    print('Sensitivity (TPR): %.2f' % sensitivity);\n    print('Specificity (TNR): %.2f' % specificity);\n    print('Balanced accuracy: %.2f' % balanced_accuracy);\n    print('Precision**:       %.2f' % precision);\n    print(\"-\"*10);\n    #print('* The accuracy metric is misleading - it is dominated by the negative examples (typically there are many more negatives).')\n    #print('** Precision is very sensitive to rare labels. It can cause misleading results when averaging precision over different labels.')\n    return accuracy_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_X_Y_for_ML(users_df):\n    # prepare data for machine learning\n    # 1. get all features available\n    feature_names = get_features_from_data(users_df)\n\n    # 2. get the features sensors feat from features\n    feat_sensor_names = get_sensor_names_from_features(feature_names);\n        \n    # 3. select the sensors to use in the machine learning\n    #sensors_to_use = ['Acc','WAcc'];\n\n    # 4. get Data accoring to selected sensors with feaures;\n    #feature_names_arr = []\n    #feature_names_arr = project_features_to_selected_sensors(feature_names, sensors_to_use)\n    X = users_df[feature_names]\n    # 5. stanrdize the features substracting the mean value and dividing by standard deviation\n    # so that all their values will be roughly in the same range:\n    (mean_vec,std_vec) = estimate_standardization_params(X);\n    X = standardize_features(X,mean_vec,std_vec);\n    X[np.isnan(X)] = 0.\n    # 6. X is ready for training\n    # 7. Prepare Y target lables for training\n    label_names = get_label_names(users_df)\n    Y = users_df[label_names]\n    # 8. clean nan values and converted to binary labels\n    # Read the binary label values, and the 'missing label' indicators:\n    trinary_labels_mat = users_df[label_names]; # This should have values of either 0., 1. or NaN\n    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n    Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n    y_df = pd.DataFrame(Y)\n    y_df.rename(columns=dict(enumerate(label_names, 0)), inplace = True)\n    return (X,y_df,M,feature_names,label_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GET the data\nTry reading sample user in order to extract features and label names"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#sample_user = pd.read_csv('61359772-D8D8-480D-B623-7C636EAD0C81.features_labels.csv')\nsample_user = pd.read_csv('/kaggle/input/extrasensory-dataset/1155FF54-63D3-4AB2-9863-8385D0BD0A13.features_labels.csv')\n#sample_user = pd.read_csv('1155FF54-63D3-4AB2-9863-8385D0BD0A13.features_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's check the features data columns before that has NAN values in certain columns \"lf_measurements:temperature_ambient\"\nsample_user.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardize sensor features measurements by substracting mean and deviding by SD and context lable to have only bolean variables**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# process the data to get features data and context label data\n(X,Y,M,feature_names,label_names) = prepare_X_Y_for_ML(sample_user)\nXY = pd.concat([X, Y], axis=1, sort=False)\nXY.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Displaying all columns in the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of columns in the database =\", sample_user.columns.size)\nfor column in sample_user.columns:\n    print (column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Listing feature names available in the dataset.\nfeatures columns are all columns except the timestamp and context columns and they are 225 column"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Number of features columns in the dataset =\",feature_names.size)\nprint (feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsample_user[feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.distplot(sample_user,x=sample_user.index,y='raw_acc:magnitude_stats:mean', color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sensor features data after standrization process\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the features data after standarization, it has been set to 0 instead of NaN\n#sns.displot(data=X,x=X.index,y='raw_acc:magnitude_stats:mean')\nX[feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"if we compare the histogram for the column \"raw_acc:magnitude_stats:mean\" it shows that standardization process worked fine and we have all values close to zero\n\n# getting relations between context labels if any for that sample user"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = Y[label_names].corr().sort_values(by=label_names, ascending=False)\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have 10 labels for this users that are missing data or have NaN values, so we will ignore them in our next step\n\n# removing Nan from the view to focus on the interesting labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.dropna(how='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# showing a heatmap for correlation between labels will list 41 label instead of 51 for this sample user"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.dropna(how='all').style.background_gradient(cmap='coolwarm', axis=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As per the heatmap shown for this sample user, there is strong coorelation between certain labels for example Lying Down, comes always with sleeping , Running comes with excercise, indoor comes with location at home, Bathing Shower comes with singing. However there are certain labels that comes only with itself and there are no chance it happens with other label, like Resturant.\n* But in order to get more correlation for the whole population and not only this sample user. we read the whole users dataset all at once of 60 users with 300K+"},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n%%time\npath = r'C:\\Users\\amrabouelenein\\Desktop\\BHCS\\ExtraSensory' # use your path\ncsv_files = glob.glob(path + \"/*.csv\")\n\nfifty_users_files = []\nten_users_files = [] \nlist_user_files = []\ni = 1\nfor file in csv_files:\n    user_data = pd.read_csv(file)\n    list_user_files.append(user_data)\n    if i <= 50:\n        fifty_users_files.append(user_data)\n    else:\n        ten_users_files.append(user_data)\n    i = i + 1\n\nall_users_data = pd.concat(list_user_files, axis=0, ignore_index=True)\nfifty_users_data = pd.concat(fifty_users_files, axis=0, ignore_index=True)\nten_users_data = pd.concat(ten_users_files, axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n(X_all,Y_all,M_all,feature_names,label_names) = prepare_X_Y_for_ML(all_users_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncorr_all = Y_all[label_names].corr().sort_values(by=label_names, ascending=False)\ncorr_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# from the table above we can see that the missing labels in the sample users are filled and have relation with other labels, so we can plot at heatmap to visualise the whole dataset of all 60 users"},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n%%time\ncorr_all.style.background_gradient(cmap='coolwarm', axis=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There are still labeles that come togeather with reasonble propablity like sleepting and Lying down. Byceling comes with excercise and label:STAIRS_-_GOINGUP label:STAIRS-_GOING_DOWN. the rest of labels doesn't seems to always happens togeather, and this maybe due to user didn't report multiple context label\n1. let's get the most lables reported among all users to consider them in our target labels for prediction in machine learining process\n1. between label we need to check the correlation between sensors features and context labels in order to use them for the maching learning excercise"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_examples_per_label = np.sum(np.array(Y),axis=0)\nlabels_and_counts = zip(label_names,n_examples_per_label)\nsorted_labels_and_counts = sorted(labels_and_counts,reverse=True,key=lambda pair:pair[1])\nprint (\"number of examples for every context label:\")\nprint (\"-\"*20)\ni = 0\nlabel_x_arr = []\nlabel_y_arr = []\nfor (label,count) in sorted_labels_and_counts:\n    i = i +1\n    label_x_arr.append(label)\n    label_y_arr.append(count)\n    print (\" %i : %s - %d minutes\"  % (i,label,count))\n    pass;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X.isnull().values.any()\nlabels_df = pd.DataFrame(sorted_labels_and_counts)\nlabels_df.rename(columns = {0:'label'}, inplace = True) \nlabels_df.rename(columns = {1:'count'}, inplace = True) \nlabels_df.plot(x='label', y='count', kind='bar', legend=False, grid=True, figsize=(20, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* selecting the top labels with many examples recorded in our dataset.\n* Choosing Walking as target context label and the 14 top lable to be included in our Multiple label classifcation stage later on\n* label:OR_indoors - 184692 minutes\n* label:LOC_home - 152892 minutes\n* label:SITTING - 136356 minutes\n* label:PHONE_ON_TABLE - 115037 minutes\n* label:LYING_DOWN - 104210 minutes\n* label:SLEEPING - 83055 minutes\n* label:AT_SCHOOL - 42331 minutes\n* label:COMPUTER_WORK - 38081 minutes\n* label:OR_standing - 37782 minutes\n* label:TALKING - 36293 minutes\n* label:LOC_main_workplace - 33944 minutes\n* label:WITH_FRIENDS - 24737 minutes\n* label:PHONE_IN_POCKET - 23401 minutes\n* label:FIX_walking - 22136 minutes\n* label:SURFING_THE_INTERNET - 19416 minutes\n* listing the availble sensors for the dataset, we extract the name from column name\n* 1. Acc stands for acceloremeter\n* 2. Gyro stands for Gyroscope\n* 3. Magnet stands for Magnetormeter\n* 4. WAcc stands for Watch acceloremeter\n* 5. Compass stands for Compass\n* 6. Loc stands for Location\n* 7. Aud stands for Audio\n* 8. AP stands for Application\n* 9. PS stands for Phone State\n* 10. LFstands for LF"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sensor_names = get_sensor_names_from_features(feature_names)\nprint(pd.unique(feat_sensor_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Displaying a list of columns features related to certain features for acceloremeter (Acc) and Watch acceloremeter (WAcc) that can coorelate with Walking activity\n1. checking the values of acceloremeters with the corresponding labels to see any coorelation between multiple lables and sensors in the same time"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_of_selected_sensors =  project_features_to_selected_sensors(feature_names,['Acc', 'WAcc'])\nfeatures_of_selected_sensors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Maching learining Part\n\n1. logistic regression model\n2. KNN model\n3. Linear model\n\n**Logistic Regression Model with label WALKING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X[features_of_selected_sensors], Y['label:FIX_walking'], test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlogmodel = LogisticRegression(max_iter=200)\nlogmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\npredictions = logmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(classification_report(y_test,predictions))\nlogmodel_results =  print_accuracy_repoprt(predictions,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**logistic regression has very good accuracy but low sensetivity for this label .69 which is not high sensetive for the label Walking.\nLinear Regression\ntesting the model with sample user**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(X[features_of_selected_sensors], Y[\"label:FIX_walking\"], test_size=0.20, shuffle=True, random_state=42)    \nlm.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate these metrics by hand!\npredictions = lm.predict(X_test)\n\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.displot((y_test-predictions),bins=50);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(classification_report(y_test,predictions.round()))\nlm_results =  print_accuracy_repoprt(predictions,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# linear regression model is not the right one for label Walking.\n# Trying KNearest Neighbour model for label WALKING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(XY[features_of_selected_sensors])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nscaled_features = scaler.transform(XY[features_of_selected_sensors])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_feat = pd.DataFrame(scaled_features,columns=features_of_selected_sensors)\ndf_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,XY['label:FIX_walking'],test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nknn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,pred))\nprint(classification_report(y_test,pred))\nknn_results =  print_accuracy_repoprt(pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choosing a K Value\n# Let's go ahead and use the elbow method to pick a good K Value!\n\n Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. Refer to the lecture if you are confused on this step."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# NOW WITH K=30\nknn = KNeighborsClassifier(n_neighbors=17)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=17')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))\nknn_results =  print_accuracy_repoprt(pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN has an excelent score for true value which is better than logistic model for the label Walking.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"single_label_models_results = [logmodel_results, lm_results, knn_results]\ndata = single_label_models_results\nx_axis = np.arange(5)\n\n\naccuracy_list = ['Accuracy', 'Sensitivity (TPR)', 'Specificity (TNR)', 'Balanced_accuracy', 'Precision']\n\nwidth = 0.35  # the width of the bars\nfig, ax = plt.subplots(figsize=(15, 7))\nrects1 = ax.bar(x_axis - width/2 , np.round(knn_results, decimals=2) , width, label='KNN')\n#rects2 = ax.bar(x_axis + 00.25, np.round(lm_results, decimals=2) , width, label='Linear')\nrects2 = ax.bar(x_axis + width/2, np.round(logmodel_results, decimals=2) , width, label='Logistic Model')\n\nax.set_xticks(x_axis)\nax.set_ylabel('Scores')\nax.set_title('Scores by by every model for single label walking')\nax.set_xticklabels(accuracy_list)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN show much more better results over logistic regression model\n# Multiple Label Classification\n\niterating through multiple label at one time\nselection of the below labels\n\nlabel:LYING_DOWN\n\nlabel:LOC_home\n\nlabel:LOC_main_workplace\n\nlabel:SITTING\n\nlabel:OR_standing\n\nlabel:FIX_walking\n\nlabel:SLEEPING\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arr_labels = ['label:FIX_walking','label:LYING_DOWN','label:LOC_home','label:LOC_main_workplace','label:SITTING','label:OR_standing', 'label:SLEEPING']\nmlp_logmodel_results = list()\nfor label in arr_labels:\n    X_train, X_test, y_train, y_test = train_test_split(X[features_of_selected_sensors], Y[label], test_size=0.30, random_state=42)    \n    logmodel = LogisticRegression(max_iter=200)\n    logmodel.fit(X_train,y_train)\n    predictions = logmodel.predict(X_test)\n    print (\"********************** \"+label+\" ******************\")\n    print(classification_report(y_test,predictions))\n    mlp_logmodel_results.append(print_accuracy_repoprt(predictions,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that label:SLEEPING has the highest accurace among the other label with 93%\n\n# Using pipeline for applying logistic regression and one vs rest classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X[features_of_selected_sensors], Y, test_size=0.30, random_state=42)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Using pipeline for applying logistic regression and one vs rest classifier\nLogReg_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),])\nfor label in arr_labels:\n    print('**Processing {} context label...**'.format(label))\n    \n    # Training logistic regression model on train data\n    LogReg_pipeline.fit(X_train, y_train[label])\n    \n    # calculating test accuracy\n    predictions = LogReg_pipeline.predict(X_test)\n    print('Test accuracy is {}'.format(accuracy_score(y_test[label], predictions)))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLP using KNN model to predict the top selected labels\n# we will try with all users dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(XY[features_of_selected_sensors])\nscaled_features = scaler.transform(XY[features_of_selected_sensors])\ndf_feat = pd.DataFrame(scaled_features,columns=features_of_selected_sensors)\ndf_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#arr_labels = ['label:LYING_DOWN','label:LOC_home','label:LOC_main_workplace','label:SITTING','label:OR_standing','label:FIX_walking', 'label:SLEEPING']\nmlp_knn_results = list()\nfor label in arr_labels:\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features,XY[label],test_size=0.30, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=17)\n    knn.fit(X_train,y_train)\n    pred = knn.predict(X_test)\n    print (\"********************** \"+label+\" ******************\")\n    print('Test accuracy is {}'.format(accuracy_score(pred,y_test)))\n    mlp_knn_results.append(print_accuracy_repoprt(pred,y_test))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#single_label_models_results = [logmodel_results, lm_results, knn_results]\n#data = single_label_models_results\nx_axis = np.arange(7)\n\n\n#arr_labels = ['LYING_DOWN','LOC_home','LOC_main_workplace','SITTING','OR_standing','FIX_walking', 'SLEEPING']\n\nwidth = 0.35  # the width of the bars\nfig, ax = plt.subplots(figsize=(15, 7))\n\nrects1 = ax.bar(x_axis - width/2 , np.round(np.array(mlp_knn_results)[:,0], decimals=3) , width, label='MLP KNN')\nrects2 = ax.bar(x_axis + 00.25, np.round(np.array(mlp_logmodel_results)[:,0], decimals=3) , width, label='MLP Logistic')\n#rects2 = ax.bar(x_axis + width/2, np.round(logmodel_results, decimals=2) , width, label='Logistic Model')\n\nax.set_xticks(x_axis)\nax.set_ylabel('Scores')\nax.set_title('Scores of accuracy  by every model for every labels')\nax.set_xticklabels(arr_labels)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per the bar chart shown above, the accuracy score is better for KNN in certain labels like Walking, Standing and setting, however the logistic regression has better score for sleeping and . KNN and logitistc regression model shows pretty similar results accross the targeted labels. so let's check the other evaluators for the models TPR and TNR"},{"metadata":{"trusted":true},"cell_type":"code","source":"#single_label_models_results = [logmodel_results, lm_results, knn_results]\n#data = single_label_models_results\nx_axis = np.arange(7)\n\n\n#arr_labels = ['LYING_DOWN','LOC_home','LOC_main_workplace','SITTING','OR_standing','FIX_walking', 'SLEEPING']\n\nwidth = 0.35  # the width of the bars\nfig, ax = plt.subplots(figsize=(15, 7))\n\nrects1 = ax.bar(x_axis - width/2 , np.round(np.array(mlp_knn_results)[:,1], decimals=3) , width, label='MLP KNN')\nrects2 = ax.bar(x_axis + 00.25, np.round(np.array(mlp_logmodel_results)[:,1], decimals=3) , width, label='MLP Logistic')\n#rects2 = ax.bar(x_axis + width/2, np.round(logmodel_results, decimals=2) , width, label='Logistic Model')\n\nax.set_xticks(x_axis)\nax.set_ylabel('Scores')\nax.set_title('Scores of ** Sensetivity (TPR) **  by every model for every labels')\nax.set_xticklabels(arr_labels)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#single_label_models_results = [logmodel_results, lm_results, knn_results]\n#data = single_label_models_results\nx_axis = np.arange(7)\n\n\n#arr_labels = ['LYING_DOWN','LOC_home','LOC_main_workplace','SITTING','OR_standing','FIX_walking', 'SLEEPING']\n\nwidth = 0.35  # the width of the bars\nfig, ax = plt.subplots(figsize=(15, 7))\n\nrects1 = ax.bar(x_axis - width/2 , np.round(np.array(mlp_knn_results)[:,2], decimals=3) , width, label='MLP KNN')\nrects2 = ax.bar(x_axis + 00.25, np.round(np.array(mlp_logmodel_results)[:,2], decimals=3) , width, label='MLP Logistic')\n#rects2 = ax.bar(x_axis + width/2, np.round(logmodel_results, decimals=2) , width, label='Logistic Model')\n\nax.set_xticks(x_axis)\nax.set_ylabel('Scores')\nax.set_title('Scores of ** Specificity (TNR) **  by every model for every labels')\nax.set_xticklabels(arr_labels)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(mlp_knn_results)[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}