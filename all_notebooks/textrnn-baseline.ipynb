{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TextRNN\n\nTextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。\n\n![TextRNN](img/rnn.jpeg)","metadata":{}},{"cell_type":"code","source":"import logging\nimport random\n\nimport numpy as np\nimport torch\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\n# set seed \nseed = 666\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\n\n# set cuda\ngpu = 0\nuse_cuda = gpu >= 0 and torch.cuda.is_available()\nif use_cuda:\n    torch.cuda.set_device(gpu)\n    device = torch.device(\"cuda\", gpu)\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Use cuda: %s, gpu id: %d.\"%(use_cuda, gpu))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:45:24.692324Z","iopub.execute_input":"2021-07-09T15:45:24.692819Z","iopub.status.idle":"2021-07-09T15:45:24.706265Z","shell.execute_reply.started":"2021-07-09T15:45:24.692781Z","shell.execute_reply":"2021-07-09T15:45:24.705217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_set = '/kaggle/input/nlp-news-text/train_set.csv'\ntest_set_a = '/kaggle/input/nlp-news-text/test_a.csv'\ntest_set_b = '/kaggle/input/nlp-news-text/test_b.csv'","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:45:24.708234Z","iopub.execute_input":"2021-07-09T15:45:24.708887Z","iopub.status.idle":"2021-07-09T15:45:24.724294Z","shell.execute_reply.started":"2021-07-09T15:45:24.708829Z","shell.execute_reply":"2021-07-09T15:45:24.723103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data to 10 fold\nfold_num = 10\ndata_file = data_set\nimport pandas as pd\n\n\ndef all_data2fold(fold_num, num=10000):\n    fold_data = []\n    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n    texts = f['text'].tolist()[:num]\n    labels = f['label'].tolist()[:num]\n\n    total = len(labels)\n\n    index = list(range(total))\n    np.random.shuffle(index)\n\n    all_texts = []\n    all_labels = []\n    for i in index:\n        all_texts.append(texts[i])\n        all_labels.append(labels[i])\n\n    label2id = {}\n    for i in range(total):\n        label = str(all_labels[i])\n        if label not in label2id:\n            label2id[label] = [i]\n        else:\n            label2id[label].append(i)\n\n    all_index = [[] for _ in range(fold_num)]\n    for label, data in label2id.items():\n        # print(label, len(data))\n        batch_size = int(len(data) / fold_num)\n        other = len(data) - batch_size * fold_num\n        for i in range(fold_num):\n            cur_batch_size = batch_size + 1 if i < other else batch_size\n            # print(cur_batch_size)\n            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n            all_index[i].extend(batch_data)\n\n    batch_size = int(total / fold_num)\n    other_texts = []\n    other_labels = []\n    other_num = 0\n    start = 0\n    for fold in range(fold_num):\n        num = len(all_index[fold])\n        texts = [all_texts[i] for i in all_index[fold]]\n        labels = [all_labels[i] for i in all_index[fold]]\n\n        if num > batch_size:\n            fold_texts = texts[:batch_size]\n            other_texts.extend(texts[batch_size:])\n            fold_labels = labels[:batch_size]\n            other_labels.extend(labels[batch_size:])\n            other_num += num - batch_size\n        elif num < batch_size:\n            end = start + batch_size - num\n            fold_texts = texts + other_texts[start: end]\n            fold_labels = labels + other_labels[start: end]\n            start = end\n        else:\n            fold_texts = texts\n            fold_labels = labels\n\n        assert batch_size == len(fold_labels)\n\n        # shuffle\n        index = list(range(batch_size))\n        np.random.shuffle(index)\n\n        shuffle_fold_texts = []\n        shuffle_fold_labels = []\n        for i in index:\n            shuffle_fold_texts.append(fold_texts[i])\n            shuffle_fold_labels.append(fold_labels[i])\n\n        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n        fold_data.append(data)\n\n    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n\n    return fold_data\n\n\nfold_data = all_data2fold(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:45:24.726578Z","iopub.execute_input":"2021-07-09T15:45:24.727137Z","iopub.status.idle":"2021-07-09T15:45:55.755088Z","shell.execute_reply.started":"2021-07-09T15:45:24.727088Z","shell.execute_reply":"2021-07-09T15:45:55.754041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build train, dev, test data\nfold_id = 9\n\n# dev\ndev_data = fold_data[fold_id]\n\n# train\ntrain_texts = []\ntrain_labels = []\nfor i in range(0, fold_id):\n    data = fold_data[i]\n    train_texts.extend(data['text'])\n    train_labels.extend(data['label'])\n\ntrain_data = {'label': train_labels, 'text': train_texts}\n\n# test\ntest_data_file = test_set_a\nf = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\ntexts = f['text'].tolist()\ntest_data = {'label': [0] * len(texts), 'text': texts}","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:45:55.757354Z","iopub.execute_input":"2021-07-09T15:45:55.75805Z","iopub.status.idle":"2021-07-09T15:46:02.861387Z","shell.execute_reply.started":"2021-07-09T15:45:55.757993Z","shell.execute_reply":"2021-07-09T15:46:02.860437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build vocab\nfrom collections import Counter\nfrom transformers import BasicTokenizer\n\nbasic_tokenizer = BasicTokenizer()\n\n\nclass Vocab():\n    def __init__(self, train_data):\n        self.min_count = 5\n        self.pad = 0\n        self.unk = 1\n        self._id2word = ['[PAD]', '[UNK]']\n        self._id2extword = ['[PAD]', '[UNK]']\n\n        self._id2label = []\n        self.target_names = []\n\n        self.build_vocab(train_data)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._word2id = reverse(self._id2word)\n        self._label2id = reverse(self._id2label)\n\n        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n\n    def build_vocab(self, data):\n        self.word_counter = Counter()\n\n        for text in data['text']:\n            words = text.split()\n            for word in words:\n                self.word_counter[word] += 1\n\n        for word, count in self.word_counter.most_common():\n            if count >= self.min_count:\n                self._id2word.append(word)\n\n        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n\n        self.label_counter = Counter(data['label'])\n\n        for label in range(len(self.label_counter)):\n            count = self.label_counter[label]\n            self._id2label.append(label)\n            self.target_names.append(label2name[label])\n\n    def load_pretrained_embs(self, embfile):\n        with open(embfile, encoding='utf-8') as f:\n            lines = f.readlines()\n            items = lines[0].split()\n            word_count, embedding_dim = int(items[0]), int(items[1])\n\n        index = len(self._id2extword)\n        embeddings = np.zeros((word_count + index, embedding_dim))\n        for line in lines[1:]:\n            values = line.split()\n            self._id2extword.append(values[0])\n            vector = np.array(values[1:], dtype='float64')\n            embeddings[self.unk] += vector\n            embeddings[index] = vector\n            index += 1\n\n        embeddings[self.unk] = embeddings[self.unk] / word_count\n        embeddings = embeddings / np.std(embeddings)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._extword2id = reverse(self._id2extword)\n\n        assert len(set(self._id2extword)) == len(self._id2extword)\n\n        return embeddings\n\n    def word2id(self, xs):\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.unk) for x in xs]\n        return self._word2id.get(xs, self.unk)\n\n    def extword2id(self, xs):\n        if isinstance(xs, list):\n            return [self._extword2id.get(x, self.unk) for x in xs]\n        return self._extword2id.get(xs, self.unk)\n\n    def label2id(self, xs):\n        if isinstance(xs, list):\n            return [self._label2id.get(x, self.unk) for x in xs]\n        return self._label2id.get(xs, self.unk)\n\n    @property\n    def word_size(self):\n        return len(self._id2word)\n\n    @property\n    def extword_size(self):\n        return len(self._id2extword)\n\n    @property\n    def label_size(self):\n        return len(self._id2label)\n\n\nvocab = Vocab(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:02.86357Z","iopub.execute_input":"2021-07-09T15:46:02.86396Z","iopub.status.idle":"2021-07-09T15:46:05.912331Z","shell.execute_reply.started":"2021-07-09T15:46:02.863918Z","shell.execute_reply":"2021-07-09T15:46:05.911449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build module\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        self.weight.data.normal_(mean=0.0, std=0.05)\n\n        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n        b = np.zeros(hidden_size, dtype=np.float32)\n        self.bias.data.copy_(torch.from_numpy(b))\n\n        self.query = nn.Parameter(torch.Tensor(hidden_size))\n        self.query.data.normal_(mean=0.0, std=0.05)\n\n    def forward(self, batch_hidden, batch_masks):\n        # batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)\n        # batch_masks:  b x len\n\n        # linear\n        key = torch.matmul(batch_hidden, self.weight) + self.bias  # b x len x hidden\n\n        # compute attention\n        outputs = torch.matmul(key, self.query)  # b x len\n\n        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n\n        attn_scores = F.softmax(masked_outputs, dim=1)  # b x len\n\n        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n\n        # sum weighted sources\n        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  # b x hidden\n\n        return batch_outputs, attn_scores\n\n\n# build word encoder\nword2vec_path = '/kaggle/input/nlp-news-text/word2vec100.txt'\ndropout = 0.15\nword_hidden_size = 128\nword_num_layers = 2\n\n\nclass WordLSTMEncoder(nn.Module):\n    def __init__(self, vocab):\n        super(WordLSTMEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.word_dims = 100\n\n        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=0)\n\n        extword_embed = vocab.load_pretrained_embs(word2vec_path)\n        extword_size, word_dims = extword_embed.shape\n        logging.info(\"Load extword embed: words %d, dims %d.\" % (extword_size, word_dims))\n\n        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=0)\n        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))\n        self.extword_embed.weight.requires_grad = False\n\n        input_size = self.word_dims\n\n        self.word_lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=word_hidden_size,\n            num_layers=word_num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n    def forward(self, word_ids, extword_ids, batch_masks):\n        # word_ids: sen_num x sent_len\n        # extword_ids: sen_num x sent_len\n        # batch_masks   sen_num x sent_len\n\n        word_embed = self.word_embed(word_ids)  # sen_num x sent_len x 100\n        extword_embed = self.extword_embed(extword_ids)\n        batch_embed = word_embed + extword_embed\n\n        if self.training:\n            batch_embed = self.dropout(batch_embed)\n\n        hiddens, _ = self.word_lstm(batch_embed)  # sen_num x sent_len x  hidden*2\n        hiddens = hiddens * batch_masks.unsqueeze(2)\n\n        if self.training:\n            hiddens = self.dropout(hiddens)\n\n        return hiddens\n\n\n# build sent encoder\nsent_hidden_size = 256\nsent_num_layers = 2\n\n\nclass SentEncoder(nn.Module):\n    def __init__(self, sent_rep_size):\n        super(SentEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.sent_lstm = nn.LSTM(\n            input_size=sent_rep_size,\n            hidden_size=sent_hidden_size,\n            num_layers=sent_num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n    def forward(self, sent_reps, sent_masks):\n        # sent_reps:  b x doc_len x sent_rep_size\n        # sent_masks: b x doc_len\n\n        sent_hiddens, _ = self.sent_lstm(sent_reps)  # b x doc_len x hidden*2\n        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n\n        if self.training:\n            sent_hiddens = self.dropout(sent_hiddens)\n\n        return sent_hiddens","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:05.91425Z","iopub.execute_input":"2021-07-09T15:46:05.91465Z","iopub.status.idle":"2021-07-09T15:46:05.941552Z","shell.execute_reply.started":"2021-07-09T15:46:05.914607Z","shell.execute_reply":"2021-07-09T15:46:05.939055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model\nclass Model(nn.Module):\n    def __init__(self, vocab):\n        super(Model, self).__init__()\n        self.sent_rep_size = word_hidden_size * 2\n        self.doc_rep_size = sent_hidden_size * 2\n        self.all_parameters = {}\n        parameters = []\n        self.word_encoder = WordLSTMEncoder(vocab)\n        self.word_attention = Attention(self.sent_rep_size)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.word_encoder.parameters())))\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.word_attention.parameters())))\n\n        self.sent_encoder = SentEncoder(self.sent_rep_size)\n        self.sent_attention = Attention(self.doc_rep_size)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n\n        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n\n        if use_cuda:\n            self.to(device)\n\n        if len(parameters) > 0:\n            self.all_parameters[\"basic_parameters\"] = parameters\n\n        logging.info('Build model with lstm word encoder, lstm sent encoder.')\n\n        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n\n    def forward(self, batch_inputs):\n        # batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len\n        # batch_masks : b x doc_len x sent_len\n        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n\n        batch_hiddens = self.word_encoder(batch_inputs1, batch_inputs2,\n                                          batch_masks)  # sen_num x sent_len x sent_rep_size\n        sent_reps, atten_scores = self.word_attention(batch_hiddens, batch_masks)  # sen_num x sent_rep_size\n\n        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  # b x doc_len x sent_rep_size\n        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  # b x doc_len x max_sent_len\n        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n\n        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  # b x doc_len x doc_rep_size\n        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  # b x doc_rep_size\n\n        batch_outputs = self.out(doc_reps)  # b x num_labels\n\n        return batch_outputs\n\n\nmodel = Model(vocab)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:05.943312Z","iopub.execute_input":"2021-07-09T15:46:05.94377Z","iopub.status.idle":"2021-07-09T15:46:06.903534Z","shell.execute_reply.started":"2021-07-09T15:46:05.943724Z","shell.execute_reply":"2021-07-09T15:46:06.902472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build optimizer\nlearning_rate = 2e-4\ndecay = .75\ndecay_step = 1000\n\n\nclass Optimizer:\n    def __init__(self, model_parameters):\n        self.all_params = []\n        self.optims = []\n        self.schedulers = []\n\n        for name, parameters in model_parameters.items():\n            if name.startswith(\"basic\"):\n                optim = torch.optim.Adam(parameters, lr=learning_rate)\n                self.optims.append(optim)\n\n                l = lambda step: decay ** (step // decay_step)\n                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n                self.schedulers.append(scheduler)\n                self.all_params.extend(parameters)\n\n            else:\n                Exception(\"no nameed parameters.\")\n\n        self.num = len(self.optims)\n\n    def step(self):\n        for optim, scheduler in zip(self.optims, self.schedulers):\n            optim.step()\n            scheduler.step()\n            optim.zero_grad()\n\n    def zero_grad(self):\n        for optim in self.optims:\n            optim.zero_grad()\n\n    def get_lr(self):\n        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n        lr = ' %.5f' * self.num\n        res = lr % lrs\n        return res","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:06.905141Z","iopub.execute_input":"2021-07-09T15:46:06.90555Z","iopub.status.idle":"2021-07-09T15:46:06.925533Z","shell.execute_reply.started":"2021-07-09T15:46:06.905505Z","shell.execute_reply":"2021-07-09T15:46:06.923566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build dataset\ndef sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n    words = text.strip().split()\n    document_len = len(words)\n\n    index = list(range(0, document_len, max_sent_len))\n    index.append(document_len)\n\n    segments = []\n    for i in range(len(index) - 1):\n        segment = words[index[i]: index[i + 1]]\n        assert len(segment) > 0\n        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n        segments.append([len(segment), segment])\n\n    assert len(segments) > 0\n    if len(segments) > max_segment:\n        segment_ = int(max_segment / 2)\n        return segments[:segment_] + segments[-segment_:]\n    else:\n        return segments\n\n\ndef get_examples(data, vocab, max_sent_len=256, max_segment=8):\n    label2id = vocab.label2id\n    examples = []\n\n    for text, label in zip(data['text'], data['label']):\n        # label\n        id = label2id(label)\n\n        # words\n        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)\n        doc = []\n        for sent_len, sent_words in sents_words:\n            word_ids = vocab.word2id(sent_words)\n            extword_ids = vocab.extword2id(sent_words)\n            doc.append([sent_len, word_ids, extword_ids])\n        examples.append([id, len(doc), doc])\n\n    logging.info('Total %d docs.' % len(examples))\n    return examples","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:06.927378Z","iopub.execute_input":"2021-07-09T15:46:06.928002Z","iopub.status.idle":"2021-07-09T15:46:06.944066Z","shell.execute_reply.started":"2021-07-09T15:46:06.927952Z","shell.execute_reply":"2021-07-09T15:46:06.942924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build loader\n\ndef batch_slice(data, batch_size):\n    batch_num = int(np.ceil(len(data) / float(batch_size)))\n    for i in range(batch_num):\n        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n\n        yield docs\n\n\ndef data_iter(data, batch_size, shuffle=True, noise=1.0):\n    \"\"\"\n    randomly permute data, then sort by source length, and partition into batches\n    ensure that the length of  sentences in each batch\n    \"\"\"\n\n    batched_data = []\n    if shuffle:\n        np.random.shuffle(data)\n\n        lengths = [example[1] for example in data]\n        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n        sorted_indices = np.argsort(noisy_lengths).tolist()\n        sorted_data = [data[i] for i in sorted_indices]\n    else:\n        sorted_data = data\n\n    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n\n    if shuffle:\n        np.random.shuffle(batched_data)\n\n    for batch in batched_data:\n        yield batch","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:06.949012Z","iopub.execute_input":"2021-07-09T15:46:06.949361Z","iopub.status.idle":"2021-07-09T15:46:06.962006Z","shell.execute_reply.started":"2021-07-09T15:46:06.949309Z","shell.execute_reply":"2021-07-09T15:46:06.960967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some function\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n\ndef get_score(y_ture, y_pred):\n    y_ture = np.array(y_ture)\n    y_pred = np.array(y_pred)\n    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n    p = precision_score(y_ture, y_pred, average='macro') * 100\n    r = recall_score(y_ture, y_pred, average='macro') * 100\n\n    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n\n\ndef reformat(num, n):\n    return float(format(num, '0.' + str(n) + 'f'))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:06.964879Z","iopub.execute_input":"2021-07-09T15:46:06.965617Z","iopub.status.idle":"2021-07-09T15:46:06.976995Z","shell.execute_reply.started":"2021-07-09T15:46:06.965572Z","shell.execute_reply":"2021-07-09T15:46:06.975851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build trainer\n\nimport time\nfrom sklearn.metrics import classification_report\n\nclip = 5.0\nepochs = 10\nearly_stops = 3\nlog_interval = 200\n\ntest_batch_size = 32\ntrain_batch_size = 32\n\nsave_model = './rnn.bin'\nsave_test = './rnn.csv'\n\n\nclass Trainer():\n    def __init__(self, model, vocab):\n        self.model = model\n        self.report = True\n\n        self.train_data = get_examples(train_data, vocab)\n        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n        self.dev_data = get_examples(dev_data, vocab)\n        self.test_data = get_examples(test_data, vocab)\n\n        # criterion\n        self.criterion = nn.CrossEntropyLoss()\n\n        # label name\n        self.target_names = vocab.target_names\n\n        # optimizer\n        self.optimizer = Optimizer(model.all_parameters)\n\n        # count\n        self.step = 0\n        self.early_stop = -1\n        self.best_train_f1, self.best_dev_f1 = 0, 0\n        self.last_epoch = epochs\n\n    def train(self):\n        logging.info('Start training...')\n        for epoch in range(1, epochs + 1):\n            train_f1 = self._train(epoch)\n\n            dev_f1 = self._eval(epoch)\n\n            if self.best_dev_f1 <= dev_f1:\n                logging.info(\n                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n                torch.save(self.model.state_dict(), save_model)\n\n                self.best_train_f1 = train_f1\n                self.best_dev_f1 = dev_f1\n                self.early_stop = 0\n            else:\n                self.early_stop += 1\n                if self.early_stop == early_stops:\n                    logging.info(\n                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n                    self.last_epoch = epoch\n                    break\n\n    def test(self):\n        self.model.load_state_dict(torch.load(save_model))\n        self._eval(self.last_epoch + 1, test=True)\n\n    def _train(self, epoch):\n        self.optimizer.zero_grad()\n        self.model.train()\n\n        start_time = time.time()\n        epoch_start_time = time.time()\n        overall_losses = 0\n        losses = 0\n        batch_idx = 1\n        y_pred = []\n        y_true = []\n        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n            torch.cuda.empty_cache()\n            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n            batch_outputs = self.model(batch_inputs)\n            loss = self.criterion(batch_outputs, batch_labels)\n            loss.backward()\n\n            loss_value = loss.detach().cpu().item()\n            losses += loss_value\n            overall_losses += loss_value\n\n            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n            y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n                optimizer.step()\n                scheduler.step()\n            self.optimizer.zero_grad()\n\n            self.step += 1\n\n            if batch_idx % log_interval == 0:\n                elapsed = time.time() - start_time\n\n                lrs = self.optimizer.get_lr()\n                logging.info(\n                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n                        epoch, self.step, batch_idx, self.batch_num, lrs,\n                        losses / log_interval,\n                        elapsed / log_interval))\n\n                losses = 0\n                start_time = time.time()\n\n            batch_idx += 1\n\n        overall_losses /= self.batch_num\n        during_time = time.time() - epoch_start_time\n\n        # reformat\n        overall_losses = reformat(overall_losses, 4)\n        score, f1 = get_score(y_true, y_pred)\n\n        logging.info(\n            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n                                                                                  overall_losses,\n                                                                                  during_time))\n        if set(y_true) == set(y_pred) and self.report:\n            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n            logging.info('\\n' + report)\n\n        return f1\n\n    def _eval(self, epoch, test=False):\n        self.model.eval()\n        start_time = time.time()\n        data = self.test_data if test else self.dev_data\n        y_pred = []\n        y_true = []\n        with torch.no_grad():\n            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n                torch.cuda.empty_cache()\n                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n                batch_outputs = self.model(batch_inputs)\n                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n                y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            score, f1 = get_score(y_true, y_pred)\n\n            during_time = time.time() - start_time\n            \n            if test:\n                df = pd.DataFrame({'label': y_pred})\n                df.to_csv(save_test, index=False, sep=',')\n            else:\n                logging.info(\n                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n                                                                              during_time))\n                if set(y_true) == set(y_pred) and self.report:\n                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n                    logging.info('\\n' + report)\n\n        return f1\n\n    def batch2tensor(self, batch_data):\n        '''\n            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n        '''\n        batch_size = len(batch_data)\n        doc_labels = []\n        doc_lens = []\n        doc_max_sent_len = []\n        for doc_data in batch_data:\n            doc_labels.append(doc_data[0])\n            doc_lens.append(doc_data[1])\n            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n            max_sent_len = max(sent_lens)\n            doc_max_sent_len.append(max_sent_len)\n\n        max_doc_len = max(doc_lens)\n        max_sent_len = max(doc_max_sent_len)\n\n        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n        batch_labels = torch.LongTensor(doc_labels)\n\n        for b in range(batch_size):\n            for sent_idx in range(doc_lens[b]):\n                sent_data = batch_data[b][2][sent_idx]\n                for word_idx in range(sent_data[0]):\n                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n                    batch_masks[b, sent_idx, word_idx] = 1\n\n        if use_cuda:\n            batch_inputs1 = batch_inputs1.to(device)\n            batch_inputs2 = batch_inputs2.to(device)\n            batch_masks = batch_masks.to(device)\n            batch_labels = batch_labels.to(device)\n\n        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:06.981465Z","iopub.execute_input":"2021-07-09T15:46:06.981934Z","iopub.status.idle":"2021-07-09T15:46:07.021888Z","shell.execute_reply.started":"2021-07-09T15:46:06.981899Z","shell.execute_reply":"2021-07-09T15:46:07.020738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\ntrainer = Trainer(model, vocab)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:46:07.02543Z","iopub.execute_input":"2021-07-09T15:46:07.025782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntrainer.test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**关于Datawhale：**\n\n> Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale 以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时 Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n\n本次新闻文本分类学习，专题知识将在天池分享，详情可关注Datawhale：\n\n ![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279172547/1584432602983_kAxAvgQpG2.jpg)","metadata":{}}]}