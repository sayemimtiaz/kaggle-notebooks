{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> Table of Contents </h1>\n\n#### 1) Load Required Libraries\n\n#### 2) iris\n\n>       2.1) Finding Important Features\n\n>       2.2) Generating the Model on Selected Features\n\n#### 3) HR Analytics\n\n>       3.1) Read Data\n\n>       3.2) EDA (Exploratory Data Analysis)\n\n>            3.2.1) Drop Unwanted Features\n\n>       3.3) Model building and Evaluation \n\n>            3.3.1) Random Forest\n\n>            3.3.2) LGBM\n\n#### 4) Boston\n\n>       a) Xgboost Built-in Feature Importance\n\n>       b) Permutation Based Feature Importance\n\n>       c) Feature Importance Computed with SHAP Values","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 1) Load Required Libraries </h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost\nfrom xgboost import XGBClassifier\n\nimport lightgbm\nfrom lightgbm import LGBMClassifier\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) iris </h1>","metadata":{}},{"cell_type":"code","source":"iris = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\niris.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iris.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import scikit-learn dataset library\nfrom sklearn import datasets\n\n# Load dataset\niris = datasets.load_iris()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the label species(setosa, versicolor,virginica)\nprint(iris.target_names)\n\n# print the names of the four features\nprint(iris.feature_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the iris data (top 5 records)\nprint(iris.data[0:5])\n\n# print the iris labels (0:setosa, 1:versicolor, 2:virginica)\nprint(iris.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a DataFrame of given iris dataset.\n\ndata = pd.DataFrame({\n    'sepal length':iris.data[:,0],\n    'sepal width':iris.data[:,1],\n    'petal length':iris.data[:,2],\n    'petal width':iris.data[:,3],\n    'species':iris.target\n})\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\ny = data['species']                                                       # Labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)    # 70% training and 30% test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 2.1) Finding Important Features </h1>\n\n - Random forest uses **gini** importance or **mean decrease in impurity (MDI)** to calculate the importance of each feature.","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.Series(clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 2.2) Generating the Model on Selected Features </h1>\n\n- Here, we can remove the **\"sepal width\"** feature because it has **very low importance,** and select the 3 remaining features.","metadata":{}},{"cell_type":"code","source":"# Split dataset into features and labels\nX = data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\"\ny = data['species']                                       \n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=5) # 70% training and 30% test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\n# prediction on test set\ny_pred=clf.predict(X_test)\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that after **removing** the least important features (sepal length), the accuracy **increased.** This is because you removed misleading data and noise, resulting in an increased accuracy. A lesser amount of features also reduces the training time.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) HR Analytics </h1>","metadata":{}},{"cell_type":"markdown","source":"|Feature                   |Description                                                |\n|--------------------------|-----------------------------------------------------------|\n|enrollee_id               |Unique ID for candidate                                    |\n|city                      |City code                                                  |\n|city_development_index    |Development index of the city (scaled)                     |\n|gender                    |Gender of candidate                                        |\n|relevent_experience       |Relevant experience of candidate                           |\n|enrolled_university       |Type of University course enrolled if any                  |\n|education_level           |Education level of candidate                               |\n|major_discipline          |Education major discipline of candidate                    |\n|experience                |Candidate total experience in years                        |             \n|company_size              |Number of employees in current employer's company          |\n|company_type              |Type of current employer                                   |\n|lastnewjob                |Difference in years between previous job and current job   |\n|training_hours            |Training hours completed                                   |\n|target                    |0 – Not looking for job change                             |\n|                          |1 – Looking for a job change                               |","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1) Read Data </h1>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv\")\nsub = pd.read_csv(\"/kaggle/input/hr-analytics-job-change-of-data-scientists/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head(3))\ndisplay(test.head(3))\ndisplay(sub.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.shape)\ndisplay(test.shape)\ndisplay(sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.info())\ndisplay(test.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainoriginal = train.copy()\ntestoriginal = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.1) target </h1>","metadata":{}},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = ['Looking for job change', 'Not looking for job change']\ny = train['target'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('Survived Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('Survived', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.2) gender </h1>","metadata":{}},{"cell_type":"code","source":"train['gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = ['Male', 'Female', 'Other']\ny = train['gender'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('gender Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('gender', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.3) education_level </h1>","metadata":{}},{"cell_type":"code","source":"train['education_level'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nx = ['Graduate', 'Masters', 'High School', 'Phd', 'Primary School']\ny = train['education_level'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('education_level Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('education_level', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.4) enrolled_university </h1>","metadata":{}},{"cell_type":"code","source":"train['enrolled_university'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nx = ['no_enrollment', 'Full time course', 'Part time course']\ny = train['enrolled_university'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('enrolled_university Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('enrolled_university', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.5) relevent_experience </h1>","metadata":{}},{"cell_type":"code","source":"train['relevent_experience'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nx = ['Has relevent experience', 'No relevent experience']\ny = train['relevent_experience'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('relevent_experience Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('relevent_experience', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.6) experience </h1>","metadata":{}},{"cell_type":"code","source":"train['experience'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.7) company_size </h1>","metadata":{}},{"cell_type":"code","source":"train['company_size'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2) EDA (Exploratory Data Analysis) </h1>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.1) Drop Unwanted Features </h1>","metadata":{}},{"cell_type":"code","source":"train = train.drop(['enrollee_id', 'city'], axis=1)\ntest = test.drop(['enrollee_id', 'city'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Random forests can also handle missing values. ","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2) Missing Values </h1>\n\n#### Visualize missing values (NaN) values using Missingno Library\n\na) Visualize missing values as a matrix\n\nb) Visualize missing values as a barplot\n\nc) Visualize missing values as a heatmap\n\nd) Visualize missing values as a dendrogram","metadata":{}},{"cell_type":"code","source":"import missingno as msno","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.isnull().sum())\nprint('-'*40)\ndisplay(test.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.1) Visualize missing values as a matrix </h1>","metadata":{}},{"cell_type":"code","source":"# Visualize missing values as a matrix\n# msno.matrix(train,figsize=(11,7), sparkline=False, fontsize=12, color=(0.27, 0.52, 1.0));\n# msno.matrix(train,figsize=(11,7), sparkline=False, fontsize=12, color=(0,.3,.3));\nmsno.matrix(train,figsize=(11,7), fontsize=12, color=(1, 0.38, 0.27));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.2) Visualize missing values as a barplot </h1>","metadata":{}},{"cell_type":"code","source":"# Visualize the number of missing values as a bar chart\n# color=\"dodgerblue\" \"orangered\"\nmsno.bar(train, color=\"dodgerblue\", sort=\"ascending\", figsize=(13,7), fontsize=12);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(1,2,1)\nmsno.bar(train, color=\"tomato\", fontsize=12, ax=ax1);\n\nax2 = fig.add_subplot(1,2,2)\nmsno.bar(train, log=True, color=\"tab:green\", fontsize=12, ax=ax2);\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing Values\nNaN = np.sum(train.isnull())\nNaN_Col = NaN.loc[(NaN != 0)].sort_values(ascending=False)\n\nplt.figure(figsize=(8,6))\nsns.barplot(x = NaN_Col.index, y = NaN_Col)\nplt.ylabel(\"Missing Value Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.3) Visualize missing values as a heatmap </h1>","metadata":{}},{"cell_type":"code","source":"# Visualize the correlation between the number of missing values in different columns as a heatmap\nmsno.heatmap(train, cmap=\"RdYlGn\", figsize=(10,5), fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.4) Visualize missing values as a dendogram </h1>","metadata":{}},{"cell_type":"code","source":"msno.dendrogram(train, figsize=(12,7), fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(1,2,1)\nmsno.dendrogram(train, orientation=\"right\", method=\"centroid\", fontsize=12, ax=ax1);\n\nax2 = fig.add_subplot(1,2,2)\nmsno.dendrogram(train, orientation=\"top\", method=\"ward\", fontsize=12, ax=ax2);\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(train.corr(), vmax=.8, square=True, annot=True, cmap='Blues');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3) Model building and Evaluation </h1>","metadata":{}},{"cell_type":"code","source":"# Transform discrete values to columns with 1 and 0s\ntrain_OHE = pd.get_dummies(train)\n\n# Do the same for competition data\ntest_OHE = pd.get_dummies(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = train_OHE.drop('target', axis=1)  # Features\ny1 = train_OHE['target']               # Labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2) # 80% training and 20% test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.1) Random Forest </h1>","metadata":{}},{"cell_type":"code","source":"# Create a Gaussian Classifier\nrf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nrf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf.predict(X_test)\ny_pred_test_rf = rf.predict(test_OHE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp1 = pd.Series(rf.feature_importances_,index=X1.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(15,20))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp1, y=feature_imp1.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.2) XGBoost </h1>","metadata":{}},{"cell_type":"markdown","source":"xgb1 = XGBClassifier()\nxgb1.fit(X_train, y_train)","metadata":{}},{"cell_type":"markdown","source":"# make predictions for test set\ny_pred_xgb1 = xgb1.predict(X_test)\npredictions = [round(value) for value in y_pred_xgb1]","metadata":{}},{"cell_type":"markdown","source":"accuracy = accuracy_score(y_test, predictions)\naccuracy","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.3) LGBM </h1>","metadata":{}},{"cell_type":"code","source":"lgbm_parameters = {\n    'reg_alpha': 0.00388218567052311,\n    'reg_lambda': 8.972335390951376e-05,\n    'colsample_bytree': 0.18375780999902297,\n    'subsample': 0.013352256062576087,\n    'learning_rate': 0.002597839272059483,\n    'max_depth': 44,\n    'num_leaves': 15,\n    'min_child_samples': 89,\n    'cat_smooth': 56, \n    'cat_l2': 22.375773634793603,\n    'max_bin': 33, \n    'min_data_per_group': 89\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_parameters['metric'] = 'binary_logloss'\nlgbm_parameters['objective'] = 'binary'\nlgbm_parameters['n_estimators'] = 15000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model = LGBMClassifier(**lgbm_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lgbm_model.predict(X_test)\ny_pred_test_rf = lgbm_model.predict(test_OHE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (12, 22)\nlightgbm.plot_importance(lgbm_model, max_num_features = 60, height=.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Boston </h1>","metadata":{}},{"cell_type":"code","source":"import shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nfrom sklearn.inspection import permutation_importance\n\nfrom xgboost import XGBRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBRegressor(n_estimators=100)\nxgb.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:left;\"> 4.1) Xgboost Feature Importance Computed in 3 Ways with Python </h1>\n\n#### a) Feature Importance built-in the Xgboost algorithm,\n\n#### b) Feature Importance computed with Permutation method,\n\n#### c) Feature Importance computed with SHAP values.","metadata":{}},{"cell_type":"markdown","source":"### About Xgboost Built-in Feature Importance\n\n- There are several types of importance in the Xgboost. It can be computed in several different ways. The **default** type is **gain** if you construct model with scikit-learn like API (docs). When you access Booster object and get the importance with **get_score** method, then default is **weight**. You can check the type of the importance with **xgb.importance_type.**\n\n- The **gain** type shows the average gain across all splits where feature was used.\n\n- The **weight** shows the number of times the feature is used to split data. This type of feature importance can favourize **numerical and high cardinality features**.\n\n- There are also **cover, total_gain, total_cover** types of importance.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> a) Xgboost Built-in Feature Importance </h1>","metadata":{}},{"cell_type":"code","source":"# Random Forest we would do the same to get importances\nprint(xgb.feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot\nplt.figure(figsize=(9,8))\nplt.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9,8))\nplt.barh(boston.feature_names, xgb.feature_importances_);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To have even better plot, let’s sort the features based on importance value:\n\nplt.figure(figsize=(9,8))\nsorted_idx = xgb.feature_importances_.argsort()\nplt.barh(boston.feature_names[sorted_idx], xgb.feature_importances_[sorted_idx]);\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> b) Permutation Based Feature Importance </h1>","metadata":{}},{"cell_type":"code","source":"# This permutation method will randomly shuffle each feature and compute the change in the model’s performance. The features which impact the performance the most are the most important one.\n\nperm_importance = permutation_importance(xgb, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9,10))\nsorted_idx = perm_importance.importances_mean.argsort()\nplt.barh(boston.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\nplt.xlabel(\"Permutation Importance\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The permutation based importance is computationally expensive (for each feature there are several repeast of shuffling). The permutation based method can have problem with highly-correlated features. Let’s check the correlation in our dataset:","metadata":{}},{"cell_type":"code","source":"def correlation_heatmap(train):\n    correlations = train.corr()\n\n    fig, ax = plt.subplots(figsize=(13,13))\n    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}\n                )\n    plt.show();\n    \ncorrelation_heatmap(X_train[boston.feature_names[sorted_idx]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on above results, I would say that it is **safe to remove: ZN, CHAS, AGE, INDUS.**\n\n- Their importance based on permutation is very low and they are **not highly correlated with other features (abs(corr) < 0.8).**","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> c) Feature Importance Computed with SHAP Values </h1>\n\n- The third method to compute feature importance in Xgboost is to use SHAP package. It is model-agnostic and using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To visualize the feature importance we need to use summary_plot method:\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.dependence_plot(\"LSTAT\", shap_values, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Submission </h1>","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\"enrollee_id\": testoriginal[\"enrollee_id\"], \"target\": y_pred_test_rf})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}