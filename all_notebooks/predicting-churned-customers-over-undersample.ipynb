{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Version 3: Using Catboost to train the data without SMOTE\n# Version 2: Using OneHotEncoder to deal with Unknown Data\n# Version 1: Using LabelEncoder to deal with Unknown Data "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, make_scorer, recall_score\n#!pip install imblearn\nfrom imblearn.over_sampling import SMOTE #Here SMOTE is used to generate samples.\n#!pip install xgboost\nfrom xgboost.sklearn import XGBClassifier\n!pip install catboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"**1. This is a classification problem, the first thing is to check whether the samples are balanced.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set the style of seaborn ploting\nsns.set_style(\"white\")\nsns.set_context(\"talk\", font_scale = 1.8) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(data[\"Attrition_Flag\"]) #Check the labels whether they are balanced or imbalanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use pie chart to demonstrate\nchurn = pd.value_counts(data[\"Attrition_Flag\"]).tolist()\nplt.figure(figsize = (20,11))\nplt.title(\"The Ratio of Normal and Churned Customers\")\nplt.pie(x = churn, labels = [\"Existing Customers\", \"Attrited Customers\"], autopct='%.2f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We could find that the data is imbalanced.**"},{"metadata":{},"cell_type":"markdown","source":"**2. Age with Churned or not**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (28,15))\nplt.title(\"Age with Churned or not\", fontsize = 30)\nsns.countplot(data = data, x = data[\"Customer_Age\"], hue = \"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This looks like a Gussian distribution.**"},{"metadata":{},"cell_type":"markdown","source":"**3. Gender with Churned or not**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#countplot\nplt.figure(figsize = (28,15))\nplt.title(\"Gender with Churned or not\")\nsns.countplot(data = data, x = data[\"Gender\"], hue = \"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pie\ngender_normal = data.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Gender\"]].value_counts().tolist()\ngender_churned = data.loc[data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Gender\"]].value_counts().tolist()\nfig, ax = plt.subplots(1, 2, dpi = 200, figsize = (28,15))\nax[0].set_title(\"Gender in Normal Customers\")\nax[0].pie(x = gender_normal, labels = [\"Female\", \"Male\"], autopct='%.2f%%')\nax[1].set_title(\"Gender in Churned Customers\")\nax[1].pie(x = gender_churned, labels = [\"Female\", \"Male\"], autopct='%.2f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It seems like that female people and male people both have a similar ratio to churn or to stay.**"},{"metadata":{},"cell_type":"markdown","source":"**4. Education Level with Churned or not**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#countplot\nplt.figure(figsize = (28,15))\nplt.title(\"Education Level with Churned or not\")\nsns.countplot(data =data, x = data[\"Education_Level\"], hue = \"Attrition_Flag\",\n             order = [\"Unknown\",\"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pie\ngender = data.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Education_Level\"]].value_counts()\ngender_normal = data.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Education_Level\"]].value_counts().tolist()\ngender_churned = data.loc[data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Education_Level\"]].value_counts().tolist()\nfig, ax = plt.subplots(1, 2, dpi = 200, figsize = (32,20))\nax[0].set_title(\"Education Level in Normal Customers\")\nax[0].pie(x = gender_normal, labels = gender.index, autopct='%.2f%%')\nax[1].set_title(\"Education Level in Churned Customers\")\nax[1].pie(x = gender_churned, labels = gender.index, autopct='%.2f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Marriage Status with Churned or not**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#countplot\nplt.figure(figsize = (28,15))\nplt.title(\"Marriage Status with Churned or not\")\nsns.countplot(data =data, x = data[\"Marital_Status\"], hue = \"Attrition_Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pie\nmarriage = data.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Marital_Status\"]].value_counts()\nmarriage_normal = data.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Marital_Status\"]].value_counts().tolist()\nmarriage_churned = data.loc[data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Marital_Status\"]].value_counts().tolist()\nfig, ax = plt.subplots(1, 2, dpi = 200, figsize = (32,18))\nax[0].set_title(\"Marital Status in Normal Customers\", fontsize = 30)\nax[0].pie(x = marriage_normal, autopct='%.2f%%', labels = marriage.index)\nax[1].set_title(\"Marital Status in Churned Customers\", fontsize = 30)\nax[1].pie(x = marriage_churned, autopct='%.2f%%', labels = marriage.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The ratio is quite similar- single people, married people and divorced people would churn mostly unrelated with their marital status.**"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"**So in this notebook, logistic regression is used to compare the results of undersampling, oversampling and samples without sampling. This might give you some ideas about dealing with imbalanced data.**"},{"metadata":{},"cell_type":"markdown","source":"**Change str type data to numeric data/ Dealing with Unkown data using OneHotEncoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Attrition_Flag/ Label\ndata.loc[data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Attrition_Flag\"]] =0\ndata.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Attrition_Flag\"]] =1\ndata[\"Attrition_Flag\"] = data[\"Attrition_Flag\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OneHotEncoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#OneHotEncoder\ndata = data.join(pd.get_dummies(data[\"Gender\"], prefix = \"Gender\"))\ndata.drop(\"Gender\", axis = 1, inplace = True)\ndata = data.join(pd.get_dummies(data[\"Education_Level\"], prefix = \"Education_Level\"))\ndata.drop(\"Education_Level\", axis = 1, inplace = True)\ndata = data.join(pd.get_dummies(data[\"Marital_Status\"], prefix = \"Marital_Status\"))\ndata.drop(\"Marital_Status\", axis = 1, inplace = True)\ndata = data.join(pd.get_dummies(data[\"Income_Category\"], prefix = \"Income_Category\"))\ndata.drop(\"Income_Category\", axis = 1, inplace = True)\ndata = data.join(pd.get_dummies(data[\"Card_Category\"], prefix = \"Card_Category\"))\ndata.drop(\"Card_Category\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardization\nss = StandardScaler()\ncols_for_scaler = [\"Customer_Age\", \"Dependent_count\", \"Months_on_book\", \"Total_Relationship_Count\",\n                   \"Months_Inactive_12_mon\", \"Contacts_Count_12_mon\", \"Credit_Limit\", \"Total_Revolving_Bal\",\n                   \"Avg_Open_To_Buy\", \"Total_Amt_Chng_Q4_Q1\", \"Total_Trans_Amt\", \"Total_Trans_Ct\",\n                   \"Total_Ct_Chng_Q4_Q1\", \"Avg_Utilization_Ratio\"]\ndata[cols_for_scaler] = ss.fit_transform(data[cols_for_scaler])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Other Things**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop two columns because these two columns are not features(check the dataset description)\ndata.drop(\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\", axis = 1, inplace = True)\ndata.drop(\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\", axis = 1, inplace = True)\ndata #check what the data looks now\n\n#Using client number as index\ndata = data.set_index(\"CLIENTNUM\") #This should help you quickly spot customers using their client number\n\n#Check what data looks like now\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make independent/dependent variables\n#Make train/test data\nx = data.loc[:, data.columns != \"Attrition_Flag\"]\ny = data.loc[:, data.columns == \"Attrition_Flag\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 66)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. No Sampling**\n\n**It means we would use the original imbalanced data to predict who would possibly churn.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#This method is to print the best C for logisticregression \ndef printing_Kfold_scores(x_train_data,y_train_data):\n    #kfold cross valiidation\n    fold = KFold(n_splits=5,shuffle=False)\n    #different C params\n    c_param_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n    results_table['C_parameter'] = c_param_range\n    #Two list would be given using KFold Cross Validationï¼štrain_indices = indices[0], test_indices = indices[1]\n    j = 0\n    for c_param in c_param_range:\n        print('-------------------------------------------')\n        print('C parameter: ', c_param)\n        print('-------------------------------------------')\n        print('')\n        recall_accs = []\n        for iteration,indices in enumerate(fold.split(x_train_data)):\n            #solver = lbfgs, L2 Regularization is used\n            lr = LogisticRegression(C = c_param,penalty = 'l2',solver='lbfgs', max_iter = 10000)\n            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n            #Recall score calculation\n            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n            recall_accs .append(recall_acc)\n            print('Iteration ', iteration,': recall score = ', recall_acc)\n        #calculate the average recall score\n        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n        j += 1\n        print('')\n        print('Mean recall score ', np.mean(recall_accs))\n        print('')\n    best_c = results_table.loc[results_table['Mean recall score'].values.argmax()]['C_parameter']\n    #Print the best c\n    print('*********************************************************************************')\n    print('Best model to choose from cross validation is with C parameter = ', best_c)\n    print('*********************************************************************************')\n    return best_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This method is to print the scores used in classification\ndef as_rs_ps(y_test, y_pred):\n    print(\"The Accuracy is:\", metrics.accuracy_score(y_test, y_pred))    #Accuracy Score: TP + TN / TP + TN + FP + FN\n    print(\"The Recall Score is:\", metrics.recall_score(y_test, y_pred))    #Recall Score:  TP/ TP + FN    \n    print(\"The Precision Score is:\", metrics.precision_score(y_test, y_pred))    #Precision Score:  TP/ TP + FP\n    print(\"The F1 Score is:\", metrics.f1_score(y_test, y_pred)) #F1 Score/ A combination of Recall Score and Precision Score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This method could help you to draw the confusion matrix\ndef ploting_confusion_matrix(cm, title, cmap= \"GnBu_r\"):\n    plt.figure(figsize = (20,12))\n    plt.title(title)   \n    sns.heatmap(cm, annot=True, fmt =\"d\", cmap=\"GnBu_r\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No sampling\nbest_c = printing_Kfold_scores(x_train, y_train) #get the best C\nlr = LogisticRegression(solver = \"lbfgs\", penalty= \"l2\", C = best_c, max_iter = 99999) #use the best C to build the model\nlr.fit(x_train, y_train.values.ravel()) #use the model to fit the data\npredictions_lr = lr.predict(x_test) #get the predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. The best C for no-sampling model is 10.**\n\n**2. If there are many features, L2 regularization usually has better results than L1 regularization because L2 regularization has a stronger penalty in preventing from overfitting.**\n\n**3. L1 penalty is usually used in features choosing (filtering features that are useless or meanless).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the score and the confusion matrix\nas_rs_ps(y_test, predictions_lr) \ncm_lr = confusion_matrix(y_test, predictions_lr) \nploting_confusion_matrix(cm_lr, \"Confusion Matrix Result without Sampling Using Logistic Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No Sampling Method Is Used**\n\n**1. The accuracy score is good (90%).**\n\n**2. The recall score is relatively low as well as the precision score(60%/77%). This is a relatively bad recall score.**"},{"metadata":{},"cell_type":"markdown","source":"**2. Undersampling**\n\n**This means we randomly choose samples whose label is \"Existing Customer\"(8500 samples -> 1627 samples) and then merge these samples with samples whose label are \"Attrited Customer\"(1627 samples).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts() #8500 Existing Customer/ 1627 Attrited Customer\nnumber_of_churned = len(y[y.Attrition_Flag == 1]) \nnormal_indices = y[y[\"Attrition_Flag\"] == 0].index #get the indices of existing customers\nchurned_indices = y[y[\"Attrition_Flag\"] == 1].index #get the indices of attrited customers\nrandom_normal_indices = np.random.choice(normal_indices, number_of_churned) #randomly select 1627 existing customers from 8500 existing customers\nundersample_indices = np.concatenate([churned_indices,random_normal_indices]) #concatenate these two sets of indices\ndata_undersample = data.loc[undersample_indices,:] #pick up undersampling data using their indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make dependent/independent variables in undersample data\nx_undersample = data_undersample.loc[:, data_undersample.columns != \"Attrition_Flag\"]\ny_undersample = data_undersample.loc[:, data_undersample.columns == \"Attrition_Flag\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make train/test data in undersample data\nx_train_undersample, x_test_undersample, y_train_undersample, y_test_undersample =train_test_split(\n    x_undersample, y_undersample, test_size = 0.3, random_state = 66)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build the model\nbest_c= printing_Kfold_scores(x_train_undersample, y_train_undersample)\nlr_undersample = LogisticRegression(solver = \"lbfgs\", penalty = \"l2\", C = best_c, max_iter = 99999)\nlr_undersample.fit(x_train_undersample, y_train_undersample.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the result\n#Here we are now using the undersample test data to test in the undersample data.\nprediciton_lr_undersample = lr_undersample.predict(x_test_undersample)\ncm_lr_undersample_undersample = confusion_matrix(y_test_undersample, prediciton_lr_undersample)\nas_rs_ps(y_test_undersample, prediciton_lr_undersample) \nploting_confusion_matrix(cm_lr_undersample_undersample, \"Confusion Matrix of Undersample\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This could suggest that the undersample model is not bad when testing the undersample test data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#What could we get if we use the whole test data to test in model built by undersample data?\npredictions_sample = lr_undersample.predict(x_test)\ncm_lr_undersample_sample = confusion_matrix(y_test, predictions_sample)\nas_rs_ps(y_test, predictions_sample) \nploting_confusion_matrix(cm_lr_undersample_sample, \"Confusion Matrix of Data Test in Undersample\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. The accuracy is not bad(tips: each time you would possibly get a sligtly different value of accuracy/recall score/precision score/F1 score because undersamples are merged or concatenated with random samples sampled from 8500 existing customers.**\n\n**2. The recall score is improved compared to the result in which no sampling method is used. However, we get a bad precision score.**\n\n**3. Mostly we should improve our recall score because this kind of error generally affects a business or project to a larger extent than an precision error does. Things go like that we have to first make customers stay because left customers mean less profit or other direct loss. But for customers that we mistakenly think they would leave, we just have to do some precautions like offering some benefits. Different business or project may attach different importance to these two statistical errors(F-Score could be used to assign the weights of the recall score and the precision score). Generally, achieving a high recall score is more important than getting a high precision score.**\n\n**Notice: The result of the whole test set in undersampling data might be misleading. What we have to notice is that we actually first splited the training and test set, then we processed the undersampling. There are identical samples that are both in whole test set and undersample training set. A more precise or rigorous way to get this prediction is to exclude the data that has already been put into the whole test data when processing undersampling.**\n"},{"metadata":{},"cell_type":"markdown","source":"**3. Oversampling (SMOTE is used)**\n\n**This means we generate samples whose label is \"attrited customer\" using features that are quite similar to those data whose label is \"attrited customer\"(This description may have flaws and be misleading).**\n\n**Learn more about SMOTE: https://github.com/scikit-learn-contrib/imbalanced-learn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#make train/test data\n#using SMOTE to generate data whose label is \"Attrited Customer\"\noversampler = SMOTE(random_state = 66)\nx_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(x, y, test_size = 0.3,\n                                                                            random_state = 66)\nx_oversample, y_oversample = oversampler.fit_sample(x_train_smote, y_train_smote) #we should make the training data balance.\ny_oversample.value_counts() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we have a balanced data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get best C in oversampling data\nbest_c = printing_Kfold_scores(x_oversample, y_oversample)\nlr_oversample = LogisticRegression(solver = \"lbfgs\", penalty = \"l2\", C = best_c, max_iter = 99999)\nlr_oversample.fit(x_oversample, y_oversample.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the result and confusion matrix\nprediction_lr_oversample = lr_oversample.predict(x_test_smote)\ncm_lr_oversample = confusion_matrix(y_test_smote, prediction_lr_oversample)\nas_rs_ps(y_test_smote, prediction_lr_oversample) \nploting_confusion_matrix(cm_lr_oversample, \"Confusion Matrix of Oversample\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. The accuracy score is relatively good(each time we would get the same score(s) because of SMOTE's generating rules).**\n\n**2. The recall score is relatively lower than the recall score achieved when undersampling is used(because undersampling cheated in some way. [See the notice above in the undersamping section! or this recall score should be quit similar to the recall score achieved by undersampling and I have tested for several times]).**\n\n**3. The precision score is slightly improved(Actually I have done several projects, sometimes the precision score would be improved significantly when using the oversampling method. But for this data/case, it is just slightly improved).**\n\n**Therefore, a comparision among no-sampling method, undersampling method, oversampling method would suggest that oversampling data might be a good choice when dealing with imbalanced data(labels).**"},{"metadata":{},"cell_type":"markdown","source":"# Using xgboost to predict the churned customers with oversampling data"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc = XGBClassifier() #substantialize the model\nparams_xgbc = {\"learning_rate\": [0.05],\n               \"max_depth\": [9],\n               \"min_child_weight\": [1],\n               \"subsample\": [0.8, 0.9],\n               \"colsample_bytree\": [0.8, 0.9],\n               \"lambda\": [0.1],\n               \"gamma\": [0.1, 0.2,]}  #set the params of xgbc for grid search\nrecallscore = make_scorer(recall_score) #ranking as recall score/ we have to use \"make_score\" to accomplish ranking as the recall score\ngrid_xgbc = GridSearchCV(xgbc, params_xgbc, cv = 5, scoring = recallscore) \ngrid_xgbc.fit(x_oversample, y_oversample.values.ravel()) #grid search for best params/This may take several minutes to get results.\ngrid_xgbc.best_params_ #get the best params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**best params: (If you run and wait, you should get this result.)**\n\n\n{'colsample_bytree': 0.8,\n 'gamma': 0.2,\n 'lambda': 0.1,\n 'learning_rate': 0.05,\n 'max_depth': 9,\n 'min_child_weight': 1,\n 'subsample': 0.9}"},{"metadata":{"trusted":true},"cell_type":"code","source":"#use the best params to fit the xgbc model\nxgbc = XGBClassifier(colsample_bytree = 0.8, #the ratio of features used in each tree\n                     learning_rate = 0.05, #just learning rate\n                     max_depth = 9, #just max_depth\n                     min_child_weight = 1, #the minimum sum of instance weight\n                     subsample = 0.9, #the ratio of samples used when fitting a tree\n                     reg_lambda = 0.1, #L2 regularization param/reg_lambda = lambda\n                     gamma = 0.2, #the minimum loss function decrease value needed to split\n                    ) \nxgbc.fit(x_oversample, y_oversample.values.ravel()) #apply the xgbc model to data\npredictions_xgbc_oversample = xgbc.predict(x_test_smote) #get the prediction\n#check the score and plot the confusion matrix\ncm_xgbc_oversample = confusion_matrix(y_test_smote, predictions_xgbc_oversample) \nas_rs_ps(y_test_smote, predictions_xgbc_oversample)\nploting_confusion_matrix(cm_xgbc_oversample, \"Confusion Matrix for XGBClassifier in Oversampling Data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. The accuracy score is very good (around 96.5%).**\n\n**2. The recall score is good (nearly 90%), and the precision score is good (89%).**\n\n**3. Generally, we have got a relatively good prediction result using XGBClassifier.**"},{"metadata":{},"cell_type":"markdown","source":"# Version 3 Starts Here"},{"metadata":{},"cell_type":"markdown","source":"**What I found was that using SMOTE could cause great overfitting in this dataset, so I abandoned SMOTE as a way to sampling. Catboost gives us a way to set the param of the imbalanced ratio of dataset like auto_class_weights = \"Balanced\" (other models do offer these parameters too), so I'll just use this param instead of using SMOTE**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Import the dataset\ndata = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\n#Drop two columns because these two columns are not features(check the dataset description)\ndata.drop(\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\", axis = 1, inplace = True)\ndata.drop(\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\", axis = 1, inplace = True)\n#Encode the Label/Dealing with unknown data with meadians(because the distplot suggests to use median to fill the Nan)\n#Labels\ndata.loc[data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Attrition_Flag\"]] =0\ndata.loc[data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Attrition_Flag\"]] =1\ndata[\"Attrition_Flag\"] = data[\"Attrition_Flag\"].astype(int)\nle = LabelEncoder()\n#Gender\ndata[\"Gender\"] = le.fit_transform(data[\"Gender\"])\n#Education_Level\ndata[\"Education_Level\"] = le.fit_transform(data[\"Education_Level\"])\ndata.loc[data[\"Education_Level\"] == 6, [\"Education_Level\"]] = np.nan\ndata[\"Education_Level\"].fillna(data[\"Education_Level\"].median(), inplace = True)\n#Marital Status\ndata[\"Marital_Status\"] = le.fit_transform(data[\"Marital_Status\"])\ndata.loc[data[\"Marital_Status\"] == 3, [\"Marital_Status\"]] = np.nan\ndata[\"Marital_Status\"].fillna(data[\"Marital_Status\"].median(), inplace = True)\n#Income_Category\ndata[\"Income_Category\"] = le.fit_transform(data[\"Income_Category\"])\ndata.loc[data[\"Income_Category\"] == 5, [\"Income_Category\"]] = np.nan\ndata[\"Income_Category\"].fillna(data[\"Income_Category\"].median(), inplace = True)\n#Card_Category\ndata[\"Card_Category\"] = le.fit_transform(data[\"Card_Category\"])\n#Standardization and using the client numbers as indices\nss = StandardScaler()\ndata = data.set_index(\"CLIENTNUM\")\ncols_standard = ['Customer_Age', 'Dependent_count', \n       'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\ndata[cols_standard] = ss.fit_transform(data[cols_standard])\n#split the train and test set\nx = data.loc[:, data.columns != \"Attrition_Flag\"]\ny = data.loc[:, data.columns == \"Attrition_Flag\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 66)\n#Using the Catboost to train and validate\nfrom catboost import CatBoostClassifier\ncbc = CatBoostClassifier(loss_function=\"Logloss\", #using logloss as the loss of binary classification\n                         eval_metric= \"Recall\", #ranking as the recall score\n                         custom_metric= \"Recall\", #showing as the recall score\n                         one_hot_max_size= 10, #If a feature has less then 10 unique elements, it would be OneHotEncoded.\n                         #cat_features = [1, 3, 4, 5, 6],\n                         early_stopping_rounds = 50, #stop early to prevent from overfitting\n                         auto_class_weights = \"Balanced\", #balance the ratio of the imbalanced data weights\n                         depth = 4, #just depth\n                         l2_leaf_reg = 12, #L2 regularization param\n                         learning_rate = 0.2, #just learning_rate\n                         rsm = 0.6, #AKa colsample_bylevel\n                         subsample = 0.3, #what is the ratio of samples to be randomly used to train each tree.\n                         )\n\n#You can explore your own params using the gridsearch\n#GridSearch for the best parameters within the model\n#params_cbc = {\"depth\": [3, 4, 5],\n#              \"subsample\": [0.2, 0.3, 0.4],\n#              \"learning_rate\": [0.1, 0.2, 0.3],\n#              \"l2_leaf_reg\": [11, 12, 13],\n#              'colsample_bylevel':[0.5, 0.6, 0.7],\n#             }\n#grid_search_result = cbc.grid_search(params_cbc, \n#                                     X= x_train, \n#                                     y= y_train, \n#                                     plot=True,\n#                                     cv = 5,)\n\n#When the model is done with gridsearching, the model would be already to use.\n#cbc.get_params() #you could check the model's parameters\ncbc.fit(x_train, y_train.values.ravel()) #using the model trained to fit the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the score and plot the confusion matrix\npredictions_cbc = cbc.predict(x_test) #receive the prediction result\nas_rs_ps(y_test, predictions_cbc) #check the score\ncm_cbc_no_sampling = confusion_matrix(y_test, predictions_cbc) #confusion matrix\nploting_confusion_matrix(cm_cbc_no_sampling, \"Confusion Matrix for Catboost Without SMOTE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Accuracy is: 0.9776242184929254**\n\n**The Recall Score is: 0.9337349397590361**\n\n**The Precision Score is: 0.93**\n\n**The F1 Score is: 0.9318637274549098**\n\n**Generally this is a good result with a relatively higher recall score and remains with a high accuracy score and F1 score.**"},{"metadata":{},"cell_type":"markdown","source":"**The result suggests that sampling might not be neccessary all the time because some sampling method would cause overfitting. It might be better to set the imbalanced ratio to deal with the imbalanced dataset.**"},{"metadata":{},"cell_type":"markdown","source":"# Thanks\n"},{"metadata":{},"cell_type":"markdown","source":" **If I could find an another model to get a better recall score, I would update this notebook.**\n \n \n **This is my first try on Kaggle.I would apppreciate it very much if you like or upvote this notebook:)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}