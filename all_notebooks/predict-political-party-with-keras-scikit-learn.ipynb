{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Predict political party based on votes\n\nAs a fun little example, I'll use a public data set of how US congressmen voted on 17 different issues in the year 1984. Let's see if we can figure out their political party based on their votes alone, using a deep neural network!\n\nFor those outside the United States, our two main political parties are \"Democrat\" and \"Republican.\" In modern times they represent progressive and conservative ideologies, respectively.\n\nPolitics in 1984 weren't quite as polarized as they are today, but you should still be able to get over 90% accuracy without much trouble.\n\nSince the point of this exercise is implementing neural networks in Keras, I'll help you to load and prepare the data.\n\nLet's start by importing the raw CSV file using Pandas, and make a DataFrame out of it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_path = '../input/congressional-voting-records-data-set/house-votes-84.data.csv'\nvoting_data = pd.read_csv(data_path, na_values=['?'])\nvoting_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use describe() to get a feel of how the data looks in aggregate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there's some missing data to deal with here; some politicians abstained on some votes, or just weren't present when the vote was taken. We will just drop the rows with missing data to keep it simple, but in practice you'd want to first make sure that doing so didn't introduce any sort of bias into your analysis (if one party abstains more than another, that could be problematic for example.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_data.dropna(inplace=True)\nvoting_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our neural network needs normalized numbers, not strings, to work. So let's replace all the y's and n's with 1's and 0's, and represent the parties as 1's and 0's as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_data.replace(('y', 'n'), (1, 0), inplace=True)\nvoting_data.replace(('democrat', 'republican'), (1, 0), inplace=True)\nvoting_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let's extract the features and labels in the form that Keras will expect:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = voting_data.drop('Class Name', axis=1).values\nall_classes = voting_data['Class Name'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, so have a go at it! There are only two parties, so this is a binary problem. This also saves us the hassle of representing classes with \"one-hot\" format like we had to do with [MNIST](https://www.kaggle.com/alirezanematolahy/handwriting-recognition-with-keras); our output is just a single 0 or 1 value.\n\nAlso we use cross_val_score to evaluate our resulting model with 10-fold cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential(\n    [\n        #16 feature inputs (votes) going into an 32-unit layer\n        Dense(32, input_dim = 16,activation='relu'),\n        # Another hidden layer of 16 units\n        Dense(16, activation='relu'),\n        # Output layer with a binary classification (Democrat or Republican political party)\n        Dense(1, activation='sigmoid')   \n    ])\n    \n    # Compile model\n    model.compile(\n        loss='binary_crossentropy', \n        optimizer='adam', \n        metrics=['accuracy']\n    )\n    \n    return model\n    \n\n# Wrap our Keras model in an estimator compatible with scikit_learn\nestimator = KerasClassifier(\n    build_fn=create_model, \n    epochs = 100, \n    verbose=0\n)\n\n# Now we can use scikit_learn's cross_val_score to evaluate this model identically to the others\ncv_scores = cross_val_score(estimator,\n                            all_features, \n                            all_classes, \n                            cv=10\n                           )\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"94% without even trying too hard! Did you do better? Maybe more neurons, more layers, or Dropout layers would help even more."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}