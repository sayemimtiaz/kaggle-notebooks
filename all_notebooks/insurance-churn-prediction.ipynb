{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\n\nfrom tqdm import tqdm\nfrom glob import glob\nimport gc\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport shap\n\nplt.rcParams[\"figure.figsize\"] = (12,8)\nplt.rcParams['axes.titlesize'] = 16\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(base_dir + 'Train.csv')\nprint(f'Number of rows in trainset: {train.shape[0]} \\nNumber of columns in trainset: {train.shape[1]}')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(base_dir + 'Test.csv')\nprint(f'Number of rows in testset: {test.shape[0]} \\nNumber of columns in testset: {test.shape[1]}')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q openpyxl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_excel(base_dir + 'sample_submission.xlsx')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Feature0 to feature6 are numerical features with dtype float64\n- Feature7 to feature15 seems to be categorical with dtype int64"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Check for NaNs in train and test__"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum(), test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are no NaNs in the dataset"},{"metadata":{},"cell_type":"markdown","source":"__Count plot of target__"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(data = train, x = 'labels', palette = 'Set3')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100 * p.get_height() / len(train)), (p.get_x() + 0.1, p.get_height() + 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the countplot its clear that dataset is unbalanced."},{"metadata":{},"cell_type":"markdown","source":"__Boxplots of numerical features__"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train.columns)\nfeatures.remove('labels')\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = [c for c in train.columns if train[c].dtype == 'float64']\nnumerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(numerical_features):\n    sns.boxplot(x = train[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize = (20, 15))\nax = ax.flatten()\nfor i, c in enumerate(numerical_features):\n    sns.histplot(x = train[c], ax = ax[i], kde = True)\nplt.suptitle('Histogram Plot', fontsize = 25)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Numerical features seems to have lots of outliers."},{"metadata":{},"cell_type":"markdown","source":"__Value counts of int64 features__"},{"metadata":{"trusted":true},"cell_type":"code","source":"int64_cols = [c for c in train.columns if train[c].dtype == 'int64']\nprint(f'There are {len(int64_cols)} features with int64 dtype: \\n{int64_cols}')\n\n#Check their unique values\n\nprint('Unique number of values in int64 features:')\nfor c in int64_cols:\n    if c != 'labels':\n        print(f'{c.upper()}: {train[c].nunique()}, {test[c].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5, 2, figsize = (10, 20))\nax = ax.flatten()\nfor i, c in enumerate(int64_cols):\n    a = sns.countplot(x = train[c], ax = ax[i], palette = 'Set3', hue = train['labels'])\n    for p in a.patches:\n        a.annotate('{:.1f}%'.format(100 * p.get_height() / len(train)), (p.get_x() + 0.1, p.get_height() + 5))\nplt.suptitle('Count Plot of Categorical Features', fontsize = 20)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Standardize Num Features and Label Encode Cat Features__"},{"metadata":{"trusted":true},"cell_type":"code","source":"scl = StandardScaler()\ntrain[numerical_features] = scl.fit_transform(train[numerical_features])\ntest[numerical_features] = scl.transform(test[numerical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int64_cols.remove('labels')\nfor c in int64_cols: \n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].astype(str).values) + list(test[c].astype(str).values)) #Takes care of cardinality mismatch\n    train[c] = lbl.transform(list(train[c].astype(str).values))\n    test[c] = lbl.transform(list(test[c].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Feature Selection using Forward Propagation__"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('labels', axis = 1)\ny = train['labels']\n\nsfs = SFS(LogisticRegression(class_weight = 'balanced'),\n           k_features = 10,\n           forward = True,\n           floating = False,\n           scoring = 'f1',\n           cv = 2)\n\nsfs.fit(X,y)\n\nprint(f'Top 10 features selected using Forward Propagation: \\n{sfs.k_feature_names_}')\nprint(f'Score: {sfs.k_score_}')\n\nselected_features = list(sfs.k_feature_names_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot_sfs(sfs.get_metric_dict(), kind = 'std_dev')\nplt.title('Sequential Forward Selection')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above plot f1 score tend to flatten after 8th feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, valid_df = train_test_split(train, test_size = 0.2, random_state = 2021, stratify = train['labels'])\n\nXtrain = train_df[selected_features]\nytrain = train_df['labels']\nXvalid = valid_df[selected_features]\nyvalid = valid_df['labels']\nprint(Xtrain.shape, ytrain.shape, Xvalid.shape, yvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pos_samples = train['labels'].value_counts().values[1]\ntotal_samples = len(train['labels'])\nscale_pos_weight = 100 - ( (num_pos_samples / total_samples) * 100 )\nscale_pos_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\n\nparams = {'num_leaves': 7,  # 2^max_depth - 1\n          'min_child_samples': 100,\n          'objective': 'binary',\n          #'scale_pos_weight': scale_pos_weight, #99,\n          'is_unbalance': 'true',\n          'max_depth': 3,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.7,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'seed': 2021\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Training LightGBM..')\nltrain = lgbm.Dataset(Xtrain, label = ytrain)\nlvalid = lgbm.Dataset(Xvalid, label = yvalid)\n\nnum_rounds = 10000\nclf = lgbm.train(params, ltrain, num_rounds, valid_sets = [ltrain, lvalid], verbose_eval = 50, \n                    early_stopping_rounds = 100)\n\ntrain_preds = clf.predict(Xtrain, num_iteration = clf.best_iteration)\nprint(f'Training ROC_AUC_SCORE: {roc_auc_score((train_preds > 0.5), ytrain)}')\nprint(f'Training F1 SCORE: {f1_score((train_preds > 0.5), ytrain)}')\n\nvalid_preds = clf.predict(Xvalid, num_iteration = clf.best_iteration)\nprint(f'Validation ROC_AUC_SCORE: {roc_auc_score((valid_preds > 0.5), yvalid)}')\nprint(f'Validation F1 SCORE: {f1_score((valid_preds > 0.5), yvalid)}')\n\ntest_preds = clf.predict(test[selected_features], num_iteration = clf.best_iteration)\nprint(test_preds[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['labels'] = (test_preds > 0.5).astype(int)\nax = sns.countplot(data = sub, x = 'labels', palette = 'Set3')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100 * p.get_height() / len(train)), (p.get_x() + 0.1, p.get_height() + 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}