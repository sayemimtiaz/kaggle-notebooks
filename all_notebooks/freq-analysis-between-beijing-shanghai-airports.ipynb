{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import linear_model, metrics, model_selection, preprocessing, multiclass, svm\nfrom scipy import interp\nimport os, datetime , time, sys\nfrom itertools import cycle\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"The analysis below is performed for only one dataset i.e. sha_pek.csv.\n\nThe ML models used are: Linear and Logistic Regression (OVR and Multinomial)\n\nEvaluation Metrics: AUC-ROC, F1 Score, Mean Absolute Square, MSE, RMSE, R^2 Score, Confusion Matrix\n\nAny additional comments or suggestions are welcome.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sha_pek = pd.read_csv('/kaggle/input/air-tickets-between-shanghai-and-beijing/sha-pek.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sha_pek.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for Valid and Invalid Data\n\nThere were 175 rows of invalid data observed in this dataset.\n\nDetermination: EDA was performed and there were flights with booking date(CreateDate) after the departure date (departureDate) which is not possible unless someone booked a flight after it's departure. Another reason that can be possible, is the delay in entering the data into the system by the airlines.\n\nWe consider the column dateDifference to perform our analysis. The rows with dateDifference less than 0 were marked as invalid. Apart from that, the data where the dateDifference was 0 and the dates of createDate and departureDate were the same but there was a time difference, were also considered invalid. Ex:\n\ndepartureDate: 2015-08-01 16:00:00\ncreateDate: 2015-08-01 22:30:00","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sha_pek['createDate'] = pd.to_datetime(sha_pek['createDate'])\nsha_pek['departureDate'] = pd.to_datetime(sha_pek['departureDate'])\nsha_pek_invalid =  sha_pek[((sha_pek['departureDate']-sha_pek['createDate'])/np.timedelta64(1,'s'))<0]\nsha_pek_invalid.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reason for invalid dateDifference could be:\n\n1. Tickets bought over the counter and recorded late into the system\n2. Agent purchase which was recorded late in the system\n3. Tickets purchased after the flight departed (highly unlikely but possible)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sha_pek_valid = sha_pek[((sha_pek['departureDate']-sha_pek['createDate'])/np.timedelta64(1,'s'))>0]\nmin((sha_pek_valid['departureDate']-sha_pek_valid['createDate'])/np.timedelta64(1,'s'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Minimum Time Difference between Create Date and Departure Date is 155 seconds, realistically impossible as 155 seconds is an impossible time difference because of airport protocols. We can consider this as invalid data, but there is an exception here, through research, it was found that many third party online booking websites show the booking of a flight even before the exact time of it's departure. For the analysis below this was the reason considered for these close bookings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA of whether the price column is actually the discounted price.\n\nsingleFlight_Filter = sha_pek_valid.loc[sha_pek_valid.flightNumber.isin(['MU5389']) & \n                  sha_pek_valid.cabinClass.isin(['C']) & \n                  sha_pek_valid.departureDate.isin(['2019-07-21 07:20:00'])]\n\nsingleFlight_Filter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It was observed that the column rate lies between 0 and 1, the lower the rate, the higher the discounted price.\n\nFor rate = 1.0, no discount was observed, and that price was treated as original price of the flight. In the example above flightNumber: 'MU5389' had two prices 1,210 and 5,660 at rates 0.22 and 1.0 respectively for cabin class 'C'. We believe the price 5,660 was the original price of the flight. Similar assumptions were made for other flights as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#CALCULATING DEPARTURE TIME FROM THE DEPARTURE DATE COLUMN\nsha_pek_valid['departureTime'] =pd.to_datetime(sha_pek_valid['departureDate']).dt.strftime('%H:%M')\n\n#SELECTING NECESSARY COLUMNS FROM THE VALID DATASET\ntabular_subset = sha_pek_valid[['flightNumber','departureTime','cabinClass','price','rate']]\n\n#DETERMINING ORIGINAL PRICE OF EACH FLIGHT WHERE RATE = 1.0 which means no discount\ntabular_subset = sha_pek_valid.loc[sha_pek_valid['rate']==1.0]\n\n#FINAL OUTPUT\ntabular_subset[['flightNumber','departureTime','cabinClass','price']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above grouping is done to display the original price of every flightNumber grouped by their cabinClass and departureTime. \n\nNote: departureTime was sliced from departureDate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LINEAR REGRESSION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below we will first analyse some relationships between a few columns. We will start with departureTime and price.\n\nTo determine the relation between departureTime and price according to the flight names, our strategy will be to first check the peak hours and frequency of flights in a day during all of their time intervals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##MAKING TIME EXECUTABLE FOR REGRESSION\n\ntabular_subset['departureTime']=tabular_subset['departureTime'].str.replace(':','.')\n\n#DETERMINING THE FLIGHT NAME BY TAKING THE FIRST TWO LETTERS ACCORDING TO THEIR NAMING CONVENTION\ntabular_subset['flightType']=tabular_subset['flightNumber'].str.slice(0,2)\n\n#GENERATING GRID PLOTS ACCORDING TO DIFFERENT FLIGHT NAMES COMPUTED FROM ABOVE\n\nairlines = tabular_subset.flightType.to_list()\nairlines = set(airlines)\n\nfig = plt.figure(figsize=(100,100))\n\nfor al,num in zip(airlines,range(1,7)):\n    al_flights = tabular_subset.loc[tabular_subset['flightType']==al].sort_values(by=['departureTime'])\n    ax=fig.add_subplot(20,20,num)\n    ax.plot(al_flights['departureTime'], al_flights['price'])\n    ax.set_title(al)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the dataset, above observations have been recorded on the departureTime vs price relation. The Graphs are classified into 6 representing each flight in the 'valid' data.\n\n1. Flight CA is spread throughout the day with peak prices throughout morning to evening.\n2. Flight MU is also spread throughout the day with peak price in late morning and early evening.\n3. Flight CZ runs only on two times both ranging from 11:45 to 12:30, hitting its peak price at 12.30.\n4. Flight FM runs one flight in the afternoon while three in the nights, with the peak in the afternoon.\n5. Flight HO runs only twice in the night at times 21:45 and 22:00 with peak prices.\n6. Flight HU runs through different intervals thoughout the day, with peak prices throughout those intervals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Regression for Q5 (DEPARTURE TIME vs PRICE)\n\nx=tabular_subset[['departureTime']].values\ny=tabular_subset['price'].values\nrm = linear_model.LinearRegression()\nrm.fit(x,y)\nsst = np.sum((y-np.mean(y))**2)\nssr = np.sum((rm.predict(x)-np.mean(y))**2)\nsse = np.sum((rm.predict(x)-y)**2)\nprint('The Coefficient is: ', rm.coef_)\nprint('The Intercept is: ', rm.intercept_)\nprint('The Coefficient is: ', rm.coef_)\nprint('The Intercept is: ', rm.intercept_)\nprint('The Total Sum of Squares is: ', sst)\nprint('The Residual Sum of Squares is: ',sse)\nprint('The Explained Sum of Squares is: ', ssr)\nprint('The R^2 from regressor: ', rm.score(x,y))\nprint('The R^2 from ssr/sst: ', ssr/sst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above stats, we can see that there isn't a significant R^2 for the relation between departureTime and price. However, when taking a deeper dive into different airlines (as shown in the grid graphs above), it shows different stories for different flights.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next we analyse the relation between dateDifference and rate\n\nTo analyse the relation between dateDifference and Rate, we need to first plot the datedifference vs rate plot to observe their relation, and then perform linear regression to calculate the R^2 evaluation metric to determine a conclusion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting datedifference vs rate\n\ncabins = sha_pek_valid.cabinClass.to_list()\ncabins = set(cabins)\n\nfig, ax = plt.subplots()\n\nsha_pek_valid.plot(x='dateDifference',y='rate',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows a **'not-so'** significant relation between the two attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Regression Date Difference vs Rate\n\nx=sha_pek_valid[['dateDifference']].values\ny=sha_pek_valid['rate'].values\nrm = linear_model.LinearRegression()\nrm.fit(x,y)\nsst = np.sum((y-np.mean(y))**2)\nssr = np.sum((rm.predict(x)-np.mean(y))**2)\nsse = np.sum((rm.predict(x)-y)**2)\nprint('The Coefficient is: ', rm.coef_)\nprint('The Intercept is: ', rm.intercept_)\nprint('The Coefficient is: ', rm.coef_)\nprint('The Intercept is: ', rm.intercept_)\nprint('The Total Sum of Squares is: ', sst)\nprint('The Residual Sum of Squares is: ',sse)\nprint('The Explained Sum of Squares is: ', ssr)\nprint('The R^2 from regressor: ', rm.score(x,y))\nprint('The R^2 from ssr/sst: ', ssr/sst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Relation between dateDifference and rate isn't significant as shown from the graph and the R^2 proves it with a smaller value. We can see from the data that, the lowest and the highest rates are available at both the maximum and minimum dateDifference.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next we perfomed the impact of different attributes on the columns: price and rate.\n\nFor Linear Regression Model, we will use flightNumber and cabinClass as two attributes to determine the model on price and rate\nrespectively.\n\nWe will change the flightNumber and cabinClass into categorical dummy variables (OneHotEncoding) in order to regress against the price, since both flightNumber and cabinClass are strings\n\nPRICE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LINEAR REGRESSION OF THE ATTRIBUTES flightNumber, cabinClass, departureTime on Target: Price\n\nlrAttributes = sha_pek_valid[['flightNumber','price','cabinClass','departureTime']]\n\n#MAKING TIME EXECUTABLE FOR REGRESSION\nlrAttributes['departureTime'] = lrAttributes['departureTime'].str.replace(':','.')\n\n#CATEGORIZING OHC \ncabinClassEnc = pd.get_dummies(lrAttributes['cabinClass'])\nflightNumberEnc = pd.get_dummies(lrAttributes['flightNumber'])\n\n#CONCATENATING THE OHC to DATASET\nlrAttributes = pd.concat([lrAttributes,cabinClassEnc,flightNumberEnc],axis=1)\n\n#DELETING UNNECESSARY COLUMNS\nlrAttributes = lrAttributes.drop(['flightNumber','cabinClass'],axis=1)\n\n#SEPARATING PRICE vs THE REST\nyAttributes = ['price']\nxAttributes = list(set(list(lrAttributes.columns))-set(yAttributes))\nxPrice = lrAttributes[xAttributes].values\nyPrice = lrAttributes[yAttributes].values\nxTrainPrice, xTestPrice, yTrainPrice, yTestPrice = model_selection.train_test_split(xPrice, \n                                                                                    yPrice, \n                                                                                    test_size=0.2, \n                                                                                    random_state = 2020)\n\n#LINEAR REGRESSION\nrm = linear_model.LinearRegression()\nrm.fit(xTrainPrice,yTrainPrice)\ntrainPredPrice = rm.predict(xTrainPrice)\ntestPredPrice = rm.predict(xTestPrice)\n\n#EVALUATION METRICS\nprint('R^2 for Training Data: ', rm.score(xTrainPrice,yTrainPrice))\nprint('R^2 for Test Data: ', rm.score(xTestPrice,yTestPrice))\nprint('Explained Metrics Score Test Data: ', metrics.explained_variance_score(yTestPrice,testPredPrice))\nprint('Mean Absolute Error Test Data: ', metrics.mean_absolute_error(yTestPrice,testPredPrice))\nprint('Mean Squared Error Test Data: ', metrics.mean_squared_error(yTestPrice,testPredPrice))\nprint('Root Mean Squared Error Test Data: ', np.sqrt(metrics.mean_squared_error(yTestPrice,testPredPrice)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R^2 for the Training Data is 70.83% while the R^2 for the Test Data is 70.65% which shows a good performance.\n\nRATE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LINEAR REGRESSION OF THE ATTRIBUTES flightNumber, cabinClass, departureTime on Target: Rate\n\nlrAttributes = sha_pek_valid[['flightNumber','rate','cabinClass','departureTime']]\n\n#MAKING TIME EXECUTABLE FOR REGRESSION\nlrAttributes['departureTime'] = lrAttributes['departureTime'].str.replace(':','.')\n\n#CATEGORIZING OHC\ncabinClassEnc = pd.get_dummies(lrAttributes['cabinClass'])\nflightNumberEnc = pd.get_dummies(lrAttributes['flightNumber'])\n\n#CONCATENATING THE OHC to DATASET\nlrAttributes = pd.concat([lrAttributes,cabinClassEnc,flightNumberEnc],axis=1)\n\n#DELETING UNNECESSARY COLUMNS\nlrAttributes = lrAttributes.drop(['flightNumber','cabinClass'],axis=1)\n\n#SEPARATING PRICE vs THE REST\nyAttributes = ['rate']\nxAttributes = list(set(list(lrAttributes.columns))-set(yAttributes))\nxRate = lrAttributes[xAttributes].values\nyRate = lrAttributes[yAttributes].values\nxTrainRate, xTestRate, yTrainRate, yTestRate = model_selection.train_test_split(xRate, \n                                                                                    yRate, \n                                                                                    test_size=0.2, \n                                                                                    random_state = 2020)\n\n#LINEAR REGRESSION\nrm = linear_model.LinearRegression()\nrm.fit(xTrainRate,yTrainRate)\ntrainPredRate = rm.predict(xTrainRate)\ntestPredRate = rm.predict(xTestRate)\n\n#EVALUATION METRICS\nprint('R^2 for Training Data: ', rm.score(xTrainRate,yTrainRate))\nprint('R^2 for Test Data: ', rm.score(xTestRate,yTestRate))\nprint('Explained Metrics Score Test Data: ', metrics.explained_variance_score(yTestRate,testPredRate))\nprint('Mean Absolute Error Test Data: ', metrics.mean_absolute_error(yTestRate,testPredRate))\nprint('Mean Squared Error Test Data: ', metrics.mean_squared_error(yTestRate,testPredRate))\nprint('Root Mean Squared Error Test Data: ', np.sqrt(metrics.mean_squared_error(yTestRate,testPredRate)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R^2 for the Training Data is 7.33% and the Test Data is 7.28% which shows bad performance.\n\n<b>The Performance of the two models are significantly different.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION\n\nNext we analyse the impact of attributes: flightNumber, cabinClass, traAirport and priceClass over the column of rate. We use the categorization of the column rate as:\n\n- rate = 1.0, no discount (new value = 0)\n- rate < 1.0, discount (new value = 1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TAKING RELEVANT COLUMNS INTO CONSIDERATION\n\nbinAttributes = sha_pek_valid[['flightNumber','traAirport','cabinClass','priceClass','rate']]\nbinAttributes['rate'][binAttributes['rate']!=1]=0\n\n#CATEGORIZING ONE HOT ENCODER VALUES FOR CLASSIFICATION\nfNumberEnc = pd.get_dummies(binAttributes['flightNumber'])\ntraAirportEnc = pd.get_dummies(binAttributes['traAirport'])\ncClassEnc = pd.get_dummies(binAttributes['cabinClass'])\npriceClassEnc = pd.get_dummies(binAttributes['priceClass'])\n\n#COMBINING OHC VALUES TO THE ORIGINAL DATASET\nbinAttributes = pd.concat([binAttributes,fNumberEnc,traAirportEnc,cClassEnc,priceClassEnc], axis=1)\n#DELETING UNNECESSARY COLUMNS\nbinAttributes = binAttributes.drop(['flightNumber','traAirport','cabinClass','priceClass'], axis=1)\n\n#SEPARATING RATE vs THE REST\nyAttribute = ['rate']\nxAttribute = list(set(list(binAttributes.columns))-set(yAttribute))\n\n#LOGISTIC REGRESSION\n\nxR = binAttributes[xAttribute].values\nyR = binAttributes[yAttribute].values.astype(int)\n\nxTrain, xTest, yTrain, yTest = model_selection.train_test_split(xR,yR,test_size=0.25,random_state=2021)\n\nlogR = linear_model.LogisticRegression(solver='lbfgs')\nlogR.fit(xTrain,yTrain)\n\ntestPred = logR.predict(xTest)\nprint('The Accuracy Score is: ', metrics.accuracy_score(yTest,testPred)) #0.97216","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> The accuracy score out of the regression was observed to be <i>0.97216</i> approximately.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The analyses below are done using OneVsRestRegression (OVR) and Multinomial models. Again we are categorizing rate into different classes. However, this time, we are not considering the no discount rates i.e. rate = 1.0. For the OVR analysis we will be categorizing data into three classes:\n\n- if 0.75 < rate < 1.0, then class 1 (new value = 1.0)\n- if 0.5 < rate <= 0.75, the class 2 (new value = 2.0)\n- if rate <= 0.5, then class 3 (new value = 3.0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TAKING NECESSARY COLUMNS INTO CONSIDERATION\n\nrocAttributes = sha_pek_valid[['flightNumber','traAirport','cabinClass','priceClass','rate']]\nrocAttributes = rocAttributes.loc[rocAttributes['rate'] != 1]\n\n#CATEGORIZING DATA ACCORDING TO THE QUESTION\n\nfor i in range(0,len(rocAttributes['rate'].values)):\n    if rocAttributes['rate'].values[i] < 1 and rocAttributes['rate'].values[i] > 0.75:\n        rocAttributes['rate'].values[i] = 1\n    elif rocAttributes['rate'].values[i] <= 0.75 and rocAttributes['rate'].values[i] > 0.5:\n        rocAttributes['rate'].values[i] = 2\n    elif rocAttributes['rate'].values[i] <=0.5:\n        rocAttributes['rate'].values[i] = 3\n\n#CREATING ONE HOT ENCODER VALUES FOR CLASSIFICATION\nfNumberEnc_roc = pd.get_dummies(rocAttributes['flightNumber'])\ntraAirportEnc_roc = pd.get_dummies(rocAttributes['traAirport'])\ncClassEnc_roc = pd.get_dummies(rocAttributes['cabinClass'])\npriceClassEnc_roc = pd.get_dummies(rocAttributes['priceClass'])\n\n#DELETING REDUNDANT DATA\nrocAttributes = pd.concat([rocAttributes,fNumberEnc_roc,traAirportEnc_roc,cClassEnc_roc,priceClassEnc_roc], axis=1)\nrocAttributes = rocAttributes.drop(['flightNumber','traAirport','cabinClass','priceClass'], axis=1)\n\n#SEPARATING RATE VS THE REST OF THE ARGUMENTS\nyAttr = ['rate']\nxAttr = list(set(list(rocAttributes.columns))-set(yAttribute))\n\n#ADDING NOISY FEATURES\nxR1 = rocAttributes[xAttr].values\nyR1 = rocAttributes[yAttr].values.astype(int)\nyR1 = preprocessing.label_binarize(yR1,classes=[1,2,3])\n\nnClasses = yR1.shape[1]\nrandomState = np.random.RandomState(0)\n\n#SPLITTING TRAIN AND TEST BY 70:30 RATIO\nxTrainR, xTestR, yTrainR, yTestR = model_selection.train_test_split(xR1, yR1, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OVR LOGISTIC REGRESSION AND TEST PREDICTION\n\ncfier = multiclass.OneVsRestClassifier(svm.LinearSVC(random_state=0))\nyScore = cfier.fit(xTrainR,yTrainR).decision_function(xTestR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#COMPUTING ROC CURVE AND ROC AREA FOR EACH CLASS\n\nfpr=dict()\ntpr=dict()\nroc_auc=dict()\n\n\nfor i in range(nClasses):\n    fpr[i], tpr[i], _ = metrics.roc_curve(yTestR[:,i],yScore[:,i])\n    roc_auc[i] = metrics.auc(fpr[i],tpr[i])\n\n    \n#AGGREGATE ALL FALSE POSITIVE RATES    \nallFpr = np.unique(np.concatenate([fpr[i] for i in range(nClasses)]))\n\n#INTERPOLATE ALL ROC CURVES AT THESE POINTS\nmeanTpr = np.zeros_like(allFpr)\nfor i in range(nClasses):\n    meanTpr += interp(allFpr, fpr[i], tpr[i])\n\n#AVERAGE IT AND CALCULATE AUC\nmeanTpr /= nClasses\n\nfpr[\"macro\"] = allFpr\ntpr[\"macro\"] = meanTpr\nroc_auc[\"macro\"] = metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(yTestR.ravel(), yScore.ravel())\nroc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n\n\n#ROC Curves\nplt.figure()\nlw=2\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.2f})' ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.2f})' ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(nClasses), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Multi_Class ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the plot that:\n\n1. Micro-Average ROC Curve (AREA): 0.95\n2. Macro-Average ROC Curve (AREA) = 0.94\n3. ROC Curve Class 0 (AREA) = 0.98\n4. ROC Curve Class 1 (AREA) = 0.91\n5. ROC Curve Class 2 (AREA) = 0.94","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Multinomial Analysis (Stacking the binary and Multi Class classifiers from the above analyses.The updated classes:\n\n- if rate = 1.0, class 0 (new value = 0.0)\n- if 0.75 < rate < 1.0, class 1 (new value = 1.0)\n- if 0.5 < rate <= 0.75, class 2 (new value = 2.0)\n- if rate <= 0.5, class 3 (new value = 3.0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TAKING NECESSARY COLUMNS INTO CONSIDERATION\nstackAttributes = sha_pek_valid[['flightNumber','traAirport','cabinClass','priceClass','rate']]\n\n#CATEGORIZING DATA ACCORDING TO THE QUESTION\n\nfor i in range(0,len(stackAttributes['rate'].values)):\n    if stackAttributes['rate'].values[i] == 1:\n        stackAttributes['rate'].values[i] = 0\n    elif stackAttributes['rate'].values[i] < 1 and stackAttributes['rate'].values[i] > 0.75:\n        stackAttributes['rate'].values[i] = 1\n    elif stackAttributes['rate'].values[i] <= 0.75 and stackAttributes['rate'].values[i] > 0.5:\n        stackAttributes['rate'].values[i] = 2\n    elif stackAttributes['rate'].values[i] <=0.5:\n        stackAttributes['rate'].values[i] = 3\n\n#CREATING ONE HOT ENCODER VALUES FOR CLASSIFICATION\n\nfNumberEnc_stack = pd.get_dummies(stackAttributes['flightNumber'])\ntraAirportEnc_stack = pd.get_dummies(stackAttributes['traAirport'])\ncClassEnc_stack = pd.get_dummies(stackAttributes['cabinClass'])\npriceClassEnc_stack = pd.get_dummies(stackAttributes['priceClass'])\n\n#DELETING REDUNDANT DATA TO AVOID PERFORMANCE ISSUES\nstackAttributes = pd.concat([stackAttributes,fNumberEnc_stack,traAirportEnc_stack,cClassEnc_stack,priceClassEnc_stack], axis=1)\nstackAttributes = stackAttributes.drop(['flightNumber','traAirport','cabinClass','priceClass'], axis=1)\n\n#SEPARATING RATE VS THE REST OF THE ARGUMENTS\nyAttr2 = ['rate']\nxAttr2 = list(set(list(stackAttributes.columns))-set(yAttribute))\n\n#LOGISTIC REGRESSION\nxR2 = stackAttributes[xAttr].values\nyR2 = stackAttributes[yAttr].values.astype(int)\n\nxTrainS, xTestS, yTrainS, yTestS = model_selection.train_test_split(xR2, yR2, test_size=0.3, random_state=0)\n\nclsfier = linear_model.LogisticRegression(solver='lbfgs',multi_class='multinomial')\nclsfier.fit(xTrainS,yTrainS)\n\ntestPredS = clsfier.predict(xTestS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"confMat = metrics.confusion_matrix(yTestS,testPredS,[0,1,2,3])\nconfMat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax=fig.add_subplot(111)\ncax=ax.matshow(confMat)\nfig.colorbar(cax)\nplt.title('Confusion Matrix')\nax.set_xticklabels(['']+[0,1,2,3])\nax.set_yticklabels(['']+[0,1,2,3])\nplt.xlabel('Predictions')\nplt.ylabel('Actuals')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision, Recall and F1 Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(yTestS, testPredS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance Measurement between OVR and Multinomial","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Training Score of OVR is: ',cfier.score(xTrainR,yTrainR))\nprint('The Training Score of Multinomial is: ', clsfier.score(xTrainS,yTrainS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Therefore we can see that Multinomial Logistic Regression has performed better compared to the OVR Logistic Regression.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}