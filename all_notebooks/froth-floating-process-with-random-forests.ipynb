{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting the effectiveness of a Froth floating process\n\n\n**June 2019**\n\n\n**Project Description:**\n\nWe will use this dataset to analyse and predict the Froth floating process having the two aims:\n* What is the best predictor for the iron concentration of the product?\n* Can the data set be used to predict the impurity of the product (by silicate concentration)?\n\n**Data Description:** \n\nThis notebook deals with the analysis of a reverse cationic flotation process of a real production environment. The data (including its documentation) is accessible through kaggle: https://www.kaggle.com/edumagalhaes/quality-prediction-in-a-mining-process\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## The Froth flotation process\n\nThe froth floatation is used to seperate the iron contents in the ore from other contaminations. The whole process usually contains for steps:\n\n1. Contioning of the ore feed pulp (mixture of ore and water) and other reagents\n2. Separation of hydrophobic and hydrophilic materials: binding particles attach to the bubbles\n3. The bubbles transport the particles upwards until they float on the surface (froth)\n4. Collection of the froth by mechanical separation (e.g. by an impeller)"},{"metadata":{},"cell_type":"markdown","source":"---\n\n## Data Analysis\n\nWe start our analysis by importing required libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\nfrom sklearn import metrics\n\n# include fasti.ai libraries\nfrom fastai.tabular import *\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom IPython.display import display\npd.set_option('display.max_columns', None) # display all columns\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/MiningProcess_Flotation_Plant_Database.csv\", parse_dates = True, index_col = 'date',decimal=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if we have missing (nan) values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"shape1 = df.shape\ndf = df.dropna()\nshape2 = df.shape\nif shape1 == shape2:\n    print('Data contains no nan values.')\nelse:\n    print('Data contains nan values.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good. Let's look at the first couple of rows in our dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe contains data about:\n* Quality measures of the iron ore pulp before feeding it into the process (inputs)\n* Features that can effect the quality of the product (process parameters)\n* Quality measures of the iron ore pulp as product of the process (outputs)"},{"metadata":{},"cell_type":"markdown","source":"OK, this looks good so far. Lets start with visualizing the data to see flaws in the data. We start by plotting our most improtant variables '% Iron Concentrate' and '% Silica Concentrate':"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nplt.subplot(1, 2, 1)\nplt.plot(df['% Iron Concentrate']);\nplt.xlabel('Date')\nplt.title('Iron Concentrate in %')\nplt.subplot(1, 2, 2)\nplt.plot(df['% Silica Concentrate']);\nplt.xlabel('Date')\nplt.title('Silica Concentrate in %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our data misses data packages of a couple of days. Based on the documentation at Kaggle, this was caused by a production shutdown. In order to rule out any influences from potentially corrupted data, we will remove the data earlier of the restart of production (\"2017-03-29 12:00:00\").\n\nWe can also see that the quality of the products does not seem to follow a clear temporal dependency."},{"metadata":{"trusted":true},"cell_type":"code","source":"sep_date = \"2017-03-29 12:00:00\"\nind_date = df.index<sep_date #boolean of earlier dates\ndf.drop(df.index[ind_date],inplace=True)\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we quickly look at pearson correlations between our features (independent variables) to get a better understanding of our dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 25))\np = sns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, that revealed a high (negative) correlations between the 'Iron Feed' and 'Silica Feed' (both Inputs of the process) as well as 'Iron Concentrate' and 'Silica Concentrate' (both Outputs of the process from the lab measurement). The later basically says, the higher the quality of the Iron, the smaller the less Silica it contains."},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\nNow let's apply a model to check if we can predict the dependent variable '% Concentrate Silica'. First, we split our dataframe into train and validation set (train: first 80% of dataframe, test: last 20% of dataframe). Then we need to remove our '% Silica Concentrate' and '% Iron Concentrate' columns, since the first one is the dependent variable and the later is not available for the online implementation, as these values come from a lab measurement and takes roughly 1h 40 minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.2)\nx = train.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny = train['% Silica Concentrate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, min_samples_leaf=1, max_features=None, n_jobs=-1)\nmodel.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These settings can be tuned a little bit to improve performance..."},{"metadata":{},"cell_type":"markdown","source":"**Check Train Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model.predict(x)\nmse = metrics.mean_squared_error(y,y_hat)\nprint('Train Set')\nprint('RMSE:',math.sqrt(mse),'   R2:',model.score(x,y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Test Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny_test = test['% Silica Concentrate']\ny_hat_test = model.predict(x_test)\nmse_test = metrics.mean_squared_error(y_test,y_hat_test)\nprint('TEST Set')\nprint('RMSE:',math.sqrt(mse_test),'   R2:',model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model interpretation"},{"metadata":{},"cell_type":"markdown","source":"**Feature importance**\n\nLet's look at the importance of each feature and plot the 10 most important features:[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(model.feature_importances_, index=df.columns[:-2])\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the ten most important features, '% Iron Feed' and '% Silica Feed' as well as the pH level of the ore pulp seem to be substantial parameters to control. "},{"metadata":{},"cell_type":"markdown","source":"**Identifiying redundant features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc\ncorr = np.round(scipy.stats.spearmanr(df).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above shows how each feature correlates with the others. The features which merge on the right hand side are closer to each other, suggesting that potentially only one of both would be sufficient for training the model. This could be useful to reduce the colinearity of different features."},{"metadata":{},"cell_type":"markdown","source":"------"},{"metadata":{},"cell_type":"markdown","source":"## Averaging dataset to account for differently sampled features"},{"metadata":{},"cell_type":"markdown","source":"Based on the documentation of the dataset, some columns are sampled every 20 seconds, some every hour. For instance, the feature 'Ore Pulp Flow' changes continously during the process while the features '% Iron Feed' and '% Silica Feed' are sample only every hour. Thus, I think it is not really helpful to use every row (sampled every 20s) including the less sampled features (held constant over the duration of one hour), since this assumes that every row is an individual observation - which it isnt. Using all samples to train our model does not really represent the reality. What we can try to do is to mean the rows (observations) for every hour and create a new dataframe which uses the average of the 20s samples. This however, will strongly reduce our data size (by factor 180). What we can do to not lose all information of the 20s sampled features, is to also include their variations during one hour (e.g. by calculating also the standard deviation of the meaned columns)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mean = df.copy()\nmean_grpby = df_mean.groupby(['date']).mean() # calculate mean\nstd_grpby = df_mean.groupby(['date']).std() # calculate std\nstd_grpby = std_grpby.loc[:, (std_grpby != 0).any(axis=0)] # delete null columns (columns with zero variance)\nstd_grpby = std_grpby.add_prefix('STD_') # add prefix to column names\ndf_merge = pd.merge(mean_grpby, std_grpby, on='date') # merge both dataframes\ndf_merge.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeat training model with new dataframe 'df_merge':"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df_merge, test_size=0.2)\nx_aver = train.drop(['% Silica Concentrate','% Iron Concentrate','STD_% Silica Concentrate','STD_% Iron Concentrate'], axis=1)\ny_aver = train['% Silica Concentrate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, min_samples_leaf=1, max_features=None, n_jobs=-1)\nmodel.fit(x_aver,y_aver)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Averaged Train Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_aver_hat = model.predict(x_aver)\nmse = metrics.mean_squared_error(y_aver,y_aver_hat)\nprint('Train Set')\nprint('RMSE:',math.sqrt(mse),'   R2:',model.score(x_aver,y_aver))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Averaged Test Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_aver_test = test.drop(['% Silica Concentrate','% Iron Concentrate','STD_% Silica Concentrate','STD_% Iron Concentrate'], axis=1)\ny_test = test['% Silica Concentrate']\ny_hat_test = model.predict(x_aver_test)\nmse_test = metrics.mean_squared_error(y_test,y_hat_test)\nprint('TEST Set')\nprint('RMSE:',math.sqrt(mse_test),'   R2:',model.score(x_aver_test,y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}