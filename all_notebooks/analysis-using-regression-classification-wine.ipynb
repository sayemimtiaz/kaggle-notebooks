{"cells":[{"metadata":{"id":"YVGiZLs8koS-"},"cell_type":"markdown","source":"#   **Prediction of Wine quality using regression and classification**\n\n","execution_count":null},{"metadata":{"id":"Xgt0cB-jtIok"},"cell_type":"markdown","source":"## Aim and a little about the data: \nThis is a learning project where we conduct multiple methods of regression and classification on the same dataset to learn more about a wide range of  methods and its use cases as well as the various aspects of a data science project including EDA, preprocessing, modelling etc. We will be using the dataset \"Wine Quality\" provided in UCI's Machine Learning repository for our work.  \n\nOur final aim is to be able to make prediction on the quality of wine given to us based on the parameters of our data set which will be done as two parts, one as classification where the integers from 0-10 as well as a binary classification of \"excellent wine\" and \"average wine\" are the classes for the quality of wine and the other part being regression where our final answer value can range from 0-10. This provides us the opportunity to get to know more about the various classification methods as well as regression methods.\n\nWe will be going through each step in high detail so as to cement all the concepts that have been learned both from a coding standpoint as well as a statistical standpoint making this python notebook quite apt for learning purposes. Note that this notebook is being done using \"Google Colab\"'s which is based on Jupyter Notebooks. We will mostly be using Pandas(for dataframe manipulation), numpy(array manipulation), seaborn and matplotlib(for plotting) and scikit-learn( for preprocessing,modelling and prediction).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"ztC_7hFAg_mW","trusted":true},"cell_type":"code","source":"# Importing necessary libraries\nimport os  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"7lfk0_9jKQu3","outputId":"4dec09b8-e4b6-4e0c-e79b-2ef7cbf2ffa8","trusted":true},"cell_type":"code","source":"# Loading the dataset in python with the name df and displaying first 20 rows: \ndf=pd.read_csv(\"/kaggle/input/wine-quality/winequalityN.csv\")\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"X2IP6SSXAdwi"},"cell_type":"markdown","source":"# Data cleaning/wrangling: \n\nHere we take the make the dataset usable for our work. This involves taking care of missing values, making sure all the object types are correct etc.","execution_count":null},{"metadata":{"id":"f919aTq6g-TT","outputId":"3d4801c7-99f0-4717-a592-20d4fa2bc3e0","trusted":true},"cell_type":"code","source":"#Getting an overall picture of the data types and shape of our dataset :\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"l8nzbcOjiRFN","outputId":"4bfab699-6309-4421-c9fa-0718a989ac30","trusted":true},"cell_type":"code","source":"# Taking sum of all missing values in each column:\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"eYAeI-eIA8jE"},"cell_type":"markdown","source":"Above, we can see that there are 13 columns and 6497 observations/rows. We also note that there are quite a few missing values. All the data types are correctly put however, we can change quality into a float object for regression.","execution_count":null},{"metadata":{"id":"T773m8FuBW2V"},"cell_type":"markdown","source":"### Handling missing variables: \nThere are multiple ways of handling missing variables from removing all the columns to complex prediction methods for the missing variables. However, we will simply assume them to be the mean of the entire column. This allows us to retain the rest of the information of the observation with the missing data while making a reasonable assumption for the missing value.","execution_count":null},{"metadata":{"id":"p0ufa8ytpEml","trusted":true},"cell_type":"code","source":"df=df.fillna(df.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"cVSb5M4saKRQ"},"cell_type":"markdown","source":"We will change the object type whenever depending on the model we are going to use and we will do it in our modelling section. Hence we have completed our cleaning section of our project. Next we will do a exploratory data analysis to see which variables are useful and each of their relationships etc. We will be using matplotlib as well as seaborn to conduct our EDA.","execution_count":null},{"metadata":{"id":"mOa-G7dwanxs"},"cell_type":"markdown","source":"# **Exploratory Data Analysis**","execution_count":null},{"metadata":{"id":"SpcJrVSca6dQ"},"cell_type":"markdown","source":"There are only 12 variables hence conducting an EDA is not that hard and we will be able to look at most variables in depth. First, we will take a  look at the summary statistics of the dataset.","execution_count":null},{"metadata":{"id":"iaUPM1DrYMyU","outputId":"4c43a206-5f8d-4492-c931-9a3970cf923e","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"a8exBCsJbdQ2"},"cell_type":"markdown","source":"One thing that we can take note of is that the different variables are on different scales, and hence we might be better of rescaling all of the data which we will explore in the next section of preprocessing.\n\nNext we will take a look at the correlation between all the variables by looking at the scatter plots between them.","execution_count":null},{"metadata":{"id":"Jy7jC7y2ZfeU","outputId":"14352ce9-5692-4933-d555-971d223b023c","trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\ncorr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sipWGCAorcXj"},"cell_type":"markdown","source":"One point we can note is the fact that there  is a good amount of multicollinearity, which could be potentially taken care of by using methods such as Ridge Regression or lasso regression.\n\nNext we will take a look at whether the color of the wine has a role in its quality. For that we will use boxplots.","execution_count":null},{"metadata":{"id":"bOOEVSa4J-Yc","outputId":"c3d526a7-42b2-4b0e-ad71-553e62200499","trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"type\",y=\"quality\",data=df, palette=\"dark\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9R8GLqUrK9jt"},"cell_type":"markdown","source":"Here, we can see that there is no significant difference between the ratings of white and red wine and hence in the next section we can remove this column as it provides no additional information for predicting the quality of wine.\n\nAnd finally we will take a look at the plots of the response variables and the various explanotory variables in our next part.\n","execution_count":null},{"metadata":{"id":"rh8-jX26Z-Ea","outputId":"33cbfea2-c5ec-4f94-c332-9232e9eddc81","trusted":true},"cell_type":"code","source":"g = sns.PairGrid(df, y_vars=[\"quality\"], x_vars=list(df)[1:-6],palette=\"GnBu_d\",hue=\"type\")\ng.map(sns.regplot)\ng.set(ylim=(-1, 11), yticks=[0, 5, 10]);\ng.add_legend()\nplt.show()\n\na = sns.PairGrid(df, y_vars=[\"quality\"], x_vars=list(df)[-6:-1],palette=\"GnBu_d\",hue=\"type\")\na.map(sns.regplot)\na.set(ylim=(-1, 11), yticks=[0, 5, 10]);\na.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YGyVhrXjdG-m"},"cell_type":"markdown","source":"One major point to note from the graph above is the categorical/ordinal nature of the response variable. This could mean that regression might not be the most optimal method for predicting values. Hence we will be comparing both, classification as well as regression models for our problems.Also note that even though the type of wine doesnt seem to have any notable impact on our response variable, it does seem to have a impact on the various independant variables. \n\n\nWith this we conclude with our EDA section and move to our next portion that is data preprocessing using sk-learn.","execution_count":null},{"metadata":{"id":"NgsDZbueJRuR"},"cell_type":"markdown","source":"# Data preprocessing \n\nHere we will prepare for our data for the modelling. First, we will remove the 'type' column as has shown to have no influence on our quality value.","execution_count":null},{"metadata":{"id":"FqlZNSBfcpkT","trusted":true},"cell_type":"code","source":"df=df[df.columns.drop('type')]","execution_count":null,"outputs":[]},{"metadata":{"id":"Ls_5zOPaNcV2"},"cell_type":"markdown","source":"One thing we could do is eliminate some more columns which are highly correlated with each other, however, since we will be using lasso as well as ridge regression we will not make any further adjustments.\n\nNext,we will normalize the independant variables. so as to reduce bias towards any particular variables. This is quite useful as the range of the variables are highly varied between the variables and name our independant variables x and target variable y. We will scale the values of all our independant variables to a range of 0-1. We will leave our target variable as is for convenience.","execution_count":null},{"metadata":{"id":"ahh-bNz3ksBs","trusted":true},"cell_type":"code","source":"x=df[df.columns.drop(\"quality\")]\nnormalized_x=preprocessing.minmax_scale(x)\ny=df[\"quality\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"JUxuvPpQVxfW"},"cell_type":"markdown","source":"And we have prepared our data as required, now, finally all we need to do is split the dataset into a training and testing dataset.","execution_count":null},{"metadata":{"id":"EHL--j3py_Mt","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(normalized_x, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"R_Icn3UXesFr"},"cell_type":"markdown","source":"We have now divided the data into a test set as well as a training set and hence we have come to an end of this section. \nNext we will move on to modelling the problem and testing.\n\n# Modelling and testing: \nWe will model using both classification and regression with multiple methods in each to give an overall idea.\n\n## Modelling and testing for regression:\nFirst we will start with linear regression and will be using Lasso, Ridge as well as ordinary linear regression for our data and compare the results and get an inference. We will be using the scikit Learn library for our regression as well as testing.\n\n### Multiple linear regression with OLS(ordinary least squares): \nFirstly we will do basic linear regression without any penalty on complexity.\n\n\n","execution_count":null},{"metadata":{"id":"LRLQvCYYfwbq","outputId":"8f7a3d68-6d38-4179-c3a2-9dedf05a055c","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nclf= LinearRegression().fit(X_train,y_train)\nclf.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"7fg2Hds2f-0k"},"cell_type":"markdown","source":"The score above gives us the R^2 value for our dataset which simply means the amount of variability that is explained by our model. Note that higher could mean better, but often times this could be an indication of over fitting. We will take a look at the root mean square error of our model on the test set to have a better idea of the predicting power of the model.","execution_count":null},{"metadata":{"id":"ljeusSeZgdNs","outputId":"a2b303c3-492a-4de2-968c-f26a64c27dfb","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\na=clf.predict(X_train)\ntrain_rmse= (mean_squared_error(a,y_train)) ** 0.5\nprint(train_rmse)\nb=clf.predict(X_test)\ntest_rmse= (mean_squared_error(b,y_test)) ** 0.5\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{"id":"bLyZulwwzoIR"},"cell_type":"markdown","source":"We got a RMSE value of 0.7141, which tells about how accurate our values are. Again, the RMSE value is not an absolute value of and needs comparision to truly know how well our model performs, however, considering that our models output ranges from 0-10, an approx. average error of 0.74 is a respectable value.\n\nOne important thing to note is that the RMSE of the test set is smaller than that of the training set which is a good sign as it indicates that overfitting is unlikely which means that lasso regression as well as ridge regression will not necessarily give us any better answers.\n\nAnother small observation I had made was that normalizing the data did not give any substantial Reduction in RMSE.","execution_count":null},{"metadata":{"id":"JfRJhrH31vZp"},"cell_type":"markdown","source":"Next we will take a quick look at ridge regression and lasso regression and compare results from the three. Again, since there wasn't any signs of overfitting there is a good chance that ridge and lasso regressions will not give any better results and possibly even worse. \n\n### Multiple linear regression with Ridge regression:\nRidge regression based on L2 regularisation is a regression method that places a penalty on complexity of our model. What it essentially does is in addition to regular cost function of OLS, Ridge regression places a penalty on the size of the coefficients. This reduces the complexity by inducing a bias resulting in a lower variance. Note that we use RidgeCV(cross validation) for finding an optimal alpha which is the one hyperparameter of our model.","execution_count":null},{"metadata":{"id":"YmisxlCbeTyo","outputId":"d534c9ee-776c-4d38-eacf-c2d817f17152","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nalpha= np.arange(0.01,10,0.1).tolist()\nclf = RidgeCV(alphas=alpha).fit(X_train, y_train)\nscore=clf.score(X_train, y_train)\nprint(\"R^2 =\", score)\na=clf.predict(X_train)\ntrain_rmse= (mean_squared_error(a,y_train)) ** 0.5\nprint(\"train_rmse = \", train_rmse)\nb=clf.predict(X_test)\ntest_rmse= (mean_squared_error(b,y_test)) ** 0.5\nprint(\"test_rmse = \", test_rmse)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jUAwMyOM4TZ7"},"cell_type":"markdown","source":"We can see that the values are exactly the same as Ridge regression converges towards a penalty of 0 for complexity and mimics regular linear regression.\nNext we can take a look at lasso regression in the same manner.\n\n### Multiple linear regression with Lasso regression: \n\nLasso regression based on L1 regularisation is a regression method that places a penalty on the complexity not just by reducing the coefficient size but by reducing it to zero and hence effectively acting as a variables selector. This method is especially helpful if there are a large number of independant variables and we need to filter out some. However, again, since the model complexity is not high the prediction is not going to be any better than linear regression. Similar to Ridge regression, alpha is the hyperparameter.","execution_count":null},{"metadata":{"id":"iuOuVqmLeXKN","outputId":"30ae05a8-eaba-43c8-fa37-76bb2ac78962","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\nalpha= np.arange(0.01,10,0.1).tolist()\nclf = LassoCV(alphas=alpha).fit(X_train, y_train)\nscore=clf.score(X_train, y_train)\nprint(\"R^2 =\", score)\na=clf.predict(X_train)\ntrain_rmse= (mean_squared_error(a,y_train)) ** 0.5\nprint(\"train_rmse = \", train_rmse)\nb=clf.predict(X_test)\ntest_rmse= (mean_squared_error(b,y_test)) ** 0.5\nprint(\"test_rmse = \", test_rmse)","execution_count":null,"outputs":[]},{"metadata":{"id":"WXgKf3R65ajA"},"cell_type":"markdown","source":"We can see that we have got substantially worse results here as some of the coefficients values have been reduced to zero and resulting in less accuracy. ","execution_count":null},{"metadata":{"id":"mMub2vev5nmO"},"cell_type":"markdown","source":"With this we come to an end of the regression section and we have found regular linear regression to be the best model out of the three. This can indicate that more complex models do not necessarily mean better models and models should be selected based on our knowledge.\n\nNext we will take approach the same problem via classification in our next subsection.","execution_count":null},{"metadata":{"id":"2r0wrISb27o4"},"cell_type":"markdown","source":"## Modelling and testing for classification:\nWe will be doing quite a few models for classification as well as a basic understanding of why the accuracy and predictions are the way they are for the various methods.\n\n### Classification with  K-Nearest Neighbours:\n\nThis is a fairly simple straightforward model which classifies each outcome based on its k nearest values. We will be choosing k =80( which is the sqrt of n), however, further research on optimal values can give us better values.","execution_count":null},{"metadata":{"id":"PSzQzvOffc5_","outputId":"df03c58c-0adf-4f63-a438-5cf48a1737dc","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nknn=KNeighborsClassifier(n_neighbors=80)\nknn.fit(X_train,y_train)\ng=knn.predict(X_test)\nmetrics.accuracy_score(y_test,g)\nmetrics.f1_score(y_test,g,average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{"id":"WMuTCxpfy7Q5"},"cell_type":"markdown","source":"We have formed the model and fitted it and got its F1-score which is a metric that combines precision as well as recall. And we have found it to be 0.544, which could be quite a bit better. We will be discussing possible reasons for the relatively low scores later.\n\n### Classification with Support Vector Machines:\nThis is quite an elegant method which is quite effective however, performs its best in binary classification without many outliers and hence may not be able to perform its best here. We use the RBF kernal here as it a generally well suited kernal.","execution_count":null},{"metadata":{"id":"AHMb-ssTgoQz","outputId":"32a500f1-6f00-427b-a8c4-1ca38250b7d5","trusted":true},"cell_type":"code","source":"from sklearn import svm\nSV=svm.SVC(C=1,kernel='rbf')\nSV.fit(X_train,y_train)\ng=SV.predict(X_test)\nmetrics.accuracy_score(y_test,g)\nmetrics.f1_score(y_test,g,average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{"id":"SOBGjXWF0Cxt"},"cell_type":"markdown","source":"The f1 score only shows marginal improvement.\n\n### Classification with Naive Bayes: \nThis is an extremely simple method that uses basic bayes theorem to get its outcome. A major advantage being its simplicity and efficieny. However, the model performs poorly when there is multicollinearity and works best with categorical values.\n We do not expect very high performance metrics for this particular method.","execution_count":null},{"metadata":{"id":"2XoQLHsphZ6B","outputId":"e080eeae-cfa3-4548-ad36-5e8f3614e0f1","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(X_train,y_train)\ng=nb.predict(X_test)\nmetrics.accuracy_score(y_test,g)\nmetrics.f1_score(y_test,g,average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{"id":"uuZ3JiOj07_C"},"cell_type":"markdown","source":"As expected, the score is considerably worse than the above two.\n\n### Classification with Logistic regression:\n Here, we will be using logistic regression to predict the classes. Note that logistic regression works best with binary values as logistics regression has two outcomes. However, it can be used to predict multiclass variables by modifying the model.","execution_count":null},{"metadata":{"id":"5pbwFSOij48E","outputId":"aa04a98b-d6b5-429e-b086-1d65725603c4","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nf=LogisticRegression(max_iter=10000)\nf.fit(X_train,y_train)\ng=f.predict(X_test)\nmetrics.accuracy_score(y_test,g)\nmetrics.f1_score(y_test,g,average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{"id":"QULPLZkm1r2h"},"cell_type":"markdown","source":"The model performs in line with the rest of the model.\n\n### Classification with Decision tree classifier :\nDecision tree classifiers is a highly intuitive model which performs admirably with a large number of independant variables.","execution_count":null},{"metadata":{"id":"yR-WcjQpmOnB","outputId":"7420604e-538b-4592-c08a-a4bc72642416","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nf= DecisionTreeClassifier()\nf.fit(X_train,y_train)\ng=f.predict(X_test)\nprint(metrics.accuracy_score(y_test,g))\nprint(metrics.f1_score(y_test,g,average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"saJKh_5j2Iaf"},"cell_type":"markdown","source":"Decision tree classifier performs markedly better than the rest of the models.\n\n### Classification with RandomForest Classifier: \nThis is an esemble model( of sorts) which essentially combines a variety of decision trees so as to form models which are less sensitive to outliers.","execution_count":null},{"metadata":{"id":"Uxu_oizSwB7w","outputId":"30aa44db-b01f-46ee-8ed2-15bb491abfe6","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nf= RandomForestClassifier()\nf.fit(X_train, y_train)\ng=f.predict(X_test)\nprint(metrics.accuracy_score(y_test,g))","execution_count":null,"outputs":[]},{"metadata":{"id":"hQfk173A2fpB"},"cell_type":"markdown","source":"This model performs the best compared to the rest of our models.\nOne important point to note is that there is no best model for all situations. Just because RandomForest worked best for our problem does not mean it will work the best in every situation hence it is critical to have a basic theoritical understanding of all models we work with. These are few classification models. \nOne of the reasons as to why our models perform poorly is due to the highly subjective nature of the dataset. We can also possibly improve these results by using different models or tweaking the hyperparameters of our current models.\nNext,we will classify the wine into \"bad wine\", \"good wine\" and \"excellent wine\".\n\n## New criteria for classification: \nIt has been shown that accurate predictions for a scale of 1-10 is fairly difficult to predict for our given models with high amount of accuracy. One way to make it easier is by reducing the number of outcomes or classes. We ensure these classes are still meaningful to the observer. We group the wines with a quality of 1-3 in \"bad wine\", 4-7 in \"good wine\" and 8-10 in excellent wine.\nWe will start of making a new column in our dataset namely \"wine_qual\" for this new data.\n\n","execution_count":null},{"metadata":{"id":"4MziK7s1wi0T","outputId":"ba99da7a-80b6-4590-b994-28b3f3539b7d","trusted":true},"cell_type":"code","source":"def qual(a):\n  if 1<=a[\"quality\"]<=3 :\n    return \"bad wine\"\n  elif 4<=a[\"quality\"]<=7 :\n    return \"good wine\"\n  else :\n    return \"excellent wine\"\ndf[\"wine_qual\"]=df.apply(qual,axis=1)\ndf.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ArNKR9do_o60"},"cell_type":"markdown","source":"Now that we have got our desired dataframe we can now classify them using some of the algorithms used above.\n\n# Classification with reduced dataset: \nWe will be using RandomForest and SVM for our classification this time.","execution_count":null},{"metadata":{"id":"aP2ECRcDxfJV","outputId":"4a720cb3-b766-4788-84b0-3062a9363ede","trusted":true},"cell_type":"code","source":"y=df[\"wine_qual\"]\nX_train, X_test, y_train, y_test = train_test_split(normalized_x, y, test_size=0.33, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nf= RandomForestClassifier()\nf.fit(X_train, y_train)\ng=f.predict(X_test)\nprint(\"accuracy for RandomForestClassifier:\", metrics.accuracy_score(y_test,g))","execution_count":null,"outputs":[]},{"metadata":{"id":"OpHAOIJLATfb","outputId":"7b356324-c64a-44ca-be12-213011d37f5f","trusted":true},"cell_type":"code","source":"from sklearn import svm\nSV=svm.SVC(C=1,kernel='rbf')\nSV.fit(X_train,y_train)\ng=SV.predict(X_test)\nmetrics.accuracy_score(y_test,g)\nprint(\"accuracy for Support Vector Machines: \", metrics.f1_score(y_test,g,average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"OZ9-KHkgBIk5"},"cell_type":"markdown","source":"We can see a massive increase in accuracy when we reduced the number of classes while still managing to retain meaning of the output. With this we have come to an end to this project.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}