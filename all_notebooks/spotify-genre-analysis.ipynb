{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Most Complete Spotify Genre Analysis\n\n![](https://images.unsplash.com/photo-1513829596324-4bb2800c5efb?ixlib=rb-1.2.1&auto=format&fit=crop&w=750&q=80)\n\n### In this notebook, we will explore the hidden facts about Spotify genres and answer the following questions:\n\n### * How do genres look like?\n\nWe will explore the hidden truths behind the data by creating\nheatmaps, scatterplots, barplots, word cloud etc.\n\n### * Can we predict genres at all?\n \nWe are gonna test the K-Nearest Neighbors and Cosine Similarity Algorithm\non how good they are at imputing missing genres\n\n### * Can we cluster genres into bigger genre groups?\n\nHow could we teach machines to tell the difference between (canadian pop, swedish pop, dance pop) \n and (blues, country, folk music) genre groups? Without any doubt, K-Means Clustering is the way to do!\n \n### * Are the newly discovered clusters that different?\n\nIn order to evaluate our new findings, we are gonna visualize results, calculate silhouette scores and make t-tests and one-way ANOVA tests for each \"number of clusters\" selection. \n\n### And much more... So let's get started!"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#basic libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\n!pip install ppscore\nwarnings.filterwarnings(\"ignore\")\nimport ppscore as pps\nimport ast\nfrom tqdm.notebook import tqdm\nimport math\nfrom collections import Counter\n\n#visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\nplt.style.use(\"ggplot\")\n\n#statistical analysis & machine learning\nfrom sklearn.cluster import KMeans as KM\nfrom sklearn.metrics import silhouette_score as score\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.model_selection import train_test_split as splitter\nfrom sklearn.model_selection import cross_val_score as validator\nfrom statsmodels.stats import power as sms\nfrom scipy.stats import pearsonr, shapiro, ttest_ind, f_oneway, levene\n\n#text preprocessing\nimport nltk\nfrom collections import Counter\nimport string\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#string representation of list -> list\ndef str2list(x):\n    try:\n        return ast.literal_eval(x)\n    except:\n        return np.nan\n\n#accuracy metric\ndef accuracy(true, pred):\n    return sum(true == pred)/len(pred)\n\n#calculate accuracy of knn model (imputing algorithm 1)\ndef knn_score(k, X_known, y_known):\n    model = KNN(n_neighbors = k)\n    return validator(model, X_known, y_known, cv=5)\n\n#calculate accuracy of many models\ndef many_knn_score(start_n, end_n, X_known, y_known):\n    scores = []\n    for i in range(start_n, end_n):\n        scores.append(knn_score(i, X_known, y_known).mean())\n    progress = pd.Series(scores, index = np.arange(start_n, end_n))\n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.subplots()\n    progress.plot(ax=ax, kind=\"line\")\n    ax.set_ylabel(\"accuracy\")\n    ax.set_xlabel(\"n_neighbors\")\n    plt.show()\n\n#cosine similarity\ndef cosine_similarity(x1, x2):\n    return np.dot(x1, x2)/(np.linalg.norm(x1)*np.linalg.norm(x2))\n\n#imputing algorithm 2\ndef similarity_algorithm(X_train):\n    similarity_lists = []\n    for x in tqdm(range(len(X_train))):\n        similarity_list = []\n        for genre in range(len(genre_profile)):\n            similarity = cosine_similarity(X_train.iloc[x], genre_profile.iloc[genre])\n            similarity_list.append(similarity)\n        similarity_lists.append(similarity_list)\n    df = pd.DataFrame(similarity_lists, columns=popular_genres) \n    return df.transpose()\n\n#silhouette score\ndef calc_silhouette(X, y):\n    return score(X, y)\n\n#determine clusters and find out how good model is using silhouette score, t-test and/or anova\ndef k_means(data, n_clusters, n_components=2):\n    model = KM(n_clusters=n_clusters)\n    model.fit(df_std)\n    preds = model.predict(data)\n    decomposer = PCA(n_components=n_components)\n    decomposer.fit(df_std)\n    data_for_plot = decomposer.transform(data)\n    cols = [\"x\"+str(x+1) for x in range(n_components)]\n    df_genre_profile = pd.DataFrame(data_for_plot, index=data.index, columns=cols)\n    df_genre_profile[\"cluster\"] = preds\n    cluster_score = calc_silhouette(data, preds)\n    tester = Test(alpha=0.05)\n    anova_score = tester.anova(*[df_genre_profile[df_genre_profile[\"cluster\"] == x][\"x1\"].values for x in range(n_clusters)])\n    anova_score = anova_score[\"anova_stat\"] if anova_score[\"test_is_accepted\"] else None\n    ttest_score = None\n    if n_clusters == 2:\n        ttest_score = tester.ttest(df_genre_profile[df_genre_profile[\"cluster\"] == 0][\"x1\"].values,\n                                   df_genre_profile[df_genre_profile[\"cluster\"] == 1][\"x1\"].values)\n        ttest_score = ttest_score[\"ttest_stat\"] if ttest_score[\"test_is_accepted\"] else None\n    return df_genre_profile, cluster_score, anova_score, ttest_score\n\n#plot a kmeans model\ndef plot_k_means(data, n_clusters):\n    df_genre_profile, cluster_score, anova_score, ttest_score = k_means(data=data, n_clusters=n_clusters)\n    rgb_colormap = np.random.randint(0, 255, size=(n_clusters, 3))/255\n    rgb_values = rgb_colormap[df_genre_profile[\"cluster\"]]\n    \n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.subplots()\n    df_genre_profile.plot(ax=ax, x=\"x1\", y=\"x2\", kind = \"scatter\", c = rgb_values)\n    string = f\"n_clusters: {n_clusters}, silhouette: {cluster_score:.4f}\"\n    if anova_score:\n        string += f\", t-test: {list(test_score)[0]:.4f}\"\n    if ttest_score:\n        string += f\", anova: {list(anova_score)[0]:.4f}\"\n    ax.set_title(string)\n    return ax\n\n#plot many k-means models based on different n_clusters\ndef k_many_clusters(data, start_n, end_n):\n    for i in range(start_n, end_n):\n        plot_k_means(data, i)\n        plt.show()\n        \n#Test class includes both ttest and anova tests\nclass Test():\n    def __init__(self, alpha, power=0.90, only_result=True, ind_limit=0.20):\n        \"\"\"alpha and power required for identifying the min. sample size\n        and ind_limit that defines the dependence using correlation coefficient\"\"\"\n        self.alpha = alpha\n        self.power = power\n        self.only_result = only_result\n        self.ind_limit = ind_limit\n    def ttest(self, a, b):\n        \"\"\"min. sample size, shapiro, pearsonr and ttest, and their corresponding p-values\"\"\"\n        only_result = self.only_result\n        power, alpha = self.power, self.alpha\n        p_a = a.mean()\n        p_b = b.mean()\n        n_a = len(a)\n        n_b = len(b)\n        effect_size = (p_b-p_a)/a.std()\n        n_req = int(sms.TTestPower().solve_power(effect_size=effect_size, power=power, alpha=alpha))\n        if len(a) > len(b):\n            a = a[:len(b)]\n        elif len(a) < len(b):\n            b = b[:len(a)]\n        stat1, p1 = shapiro(a)\n        stat2, p2 = shapiro(b)\n        stat3, p3 = pearsonr(a, b)\n        stat4, p4 = ttest_ind(b, a)\n        \n        result_dict = {\"power\": power, \"alpha\": alpha, \"n_req\": n_req,\n                       \"n_control\": n_a, \"n_test\": n_b, \"shapiro_control_stat\": stat1,\n                       \"shapiro_control_p\": p1, \"shapiro_test_stat\": stat2, \"shapiro_test_p\": p2,\n                       \"pearsonr_stat\": stat3, \"pearsonr_p\": p3, \"ttest_stat\": stat4,\n                       \"ttest_p\": p4, \"ind_limit\": self.ind_limit, \"very_low_number\": n_req > n_a or n_req > n_b,\n                       \"control_is_normal\": alpha < p1, \"test_is_normal\": alpha < p2,\n                       \"very_low_correlation\": self.ind_limit > abs(stat3), \"very_high_dependence\": p3 < alpha,\n                       \"no_difference\": p4 > alpha, \"test_is_bigger\": stat4 > 0, \"control_is_bigger\": stat4 < 0}\n        \n        accepted = all([\n            not(result_dict[\"very_low_number\"]),\n            (result_dict[\"control_is_normal\"] and result_dict[\"test_is_normal\"]),\n            (not(result_dict[\"very_high_dependence\"]) or result_dict[\"very_low_correlation\"])\n        ])\n        \n        result_dict.update({\"test_is_accepted\": accepted})\n        result_dict = {key: result_dict[key] for key in [\"ttest_stat\", \"ttest_p\", \"test_is_accepted\"]} if only_result else result_dict\n        return result_dict\n    def anova(self, *args):\n        \"\"\"shapiro, levene, one-way anova and their corresponding p-values\"\"\"\n        only_result = self.only_result\n        alpha = self.alpha\n        normality = [shapiro(x) for x in args]\n        every_group_is_normal = True if all([x[1] > alpha for x in normality]) else False\n        stat1, p1 = levene(*args)\n        equal_variance = False if(p1 < alpha) else True\n        stat2, p2 = f_oneway(*args)\n        equal_means = False if (p2 < alpha) else True\n        accepted = all([every_group_is_normal, equal_variance, not(equal_means)])\n        result_dict = {\"alpha\": alpha, \"normality\":every_group_is_normal, \"levene_stat\":stat1, \"levene_p\": p1,\n                       \"homogenity\": equal_variance, \"anova_stat\": stat2, \"anova_p\": p2, \n                       \"groups_are_different\": not(equal_means), \"test_is_accepted\": accepted}\n        result_dict = {key: result_dict[key] for key in [\"anova_stat\", \"anova_p\", \"test_is_accepted\"]} if only_result else result_dict\n        return result_dict\n    \n#predictions over new data\ndef predict_cluster(sample):\n    sample = sample.copy()\n    for i in range(len(in_cols)):\n        col = in_cols[i]\n        sample[col] = (sample[col]-genre_means[i])/genre_stds[i]\n    df_genre_profile = k_means(sample, 4)[0]\n    return df_genre_profile[\"cluster\"].to_dict()\n\n#generate fake name for clusters\ndef create_cluster_name(artists_clusters):\n    counts = []\n    artists_data = [artists_clusters.query(\"cluster == \"+str(x)).index for x in range(4)]\n    for data in artists_data:\n        data_string = \" \".join(data).lower()\n        tokens = nltk.word_tokenize(data_string)\n        stopset = set(stopwords.words('english') + list(string.punctuation) + [\"orchestra\", \"band\", \"symphony\"])\n        data = [token for token in tokens if token not in stopset and len(token) > 2]\n        count = pd.Series(dict(Counter(data)))\n        counts.append(count.sort_values(ascending=False)[:3].to_dict())\n    name_dict = {}\n    for i, count in enumerate(counts):\n        indices = np.random.permutation(len(count))\n        count = np.array(list(count.keys()))\n        count = count[indices]\n        genre_name = \" \".join(count)\n        name_dict.update({\"Cluster \"+str(i): genre_name})\n    return name_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/spotify-dataset-19212020-160k-tracks/data_by_genres.csv\")\ndf_2 = pd.read_csv(\"/kaggle/input/spotify-dataset-19212020-160k-tracks/data_w_genres.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_cols = [\"genres\", \"artists\", \"mode\", \"count\", \"key\"]\nin_cols = [x for x in df.columns if x not in out_cols] \n\ndf = df.set_index(\"genres\")[in_cols].drop(\"[]\", 0)\ndf #genre data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill nan values by 0\ndf_2.set_index(\"artists\", inplace=True)\ndf_2[\"genres\"][df_2[\"genres\"] == \"[]\"] = np.nan\ndf_2[\"genres\"] = df_2[\"genres\"].fillna(0)\ndf_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardize data\ndf_2_std = df_2.copy()\nfor col in in_cols:\n    df_2_std[col] = (df_2[col]-df_2[col].mean())/df_2[col].std()\n       \n#extract individual genres from genre lists\ndf_2_std.reset_index(inplace = True)\ncollist = list(df_2_std.columns)\nnew_rows = []\nfor index in tqdm(range(len(df_2_std))):\n    row = df_2_std.iloc[index]\n    genre_list = str2list(row[\"genres\"])\n    row = pd.DataFrame(row).transpose()\n    if(not(isinstance(genre_list, list) and len(genre_list) != 0)):\n        pass\n    else:\n        if(len(genre_list) == 1):\n            row[\"genres\"] = genre_list[0]\n            new_rows.append(list(row.values[0]))\n        else:\n            row = pd.concat([row for i in range(len(genre_list))], 0)\n            row[\"genres\"] = genre_list\n            for i in range(len(genre_list)):\n                new_rows.append(list(row.values[i]))\n                \ndf_known = pd.DataFrame(new_rows, columns = collist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#export\ndf_known.to_csv(\"data_each_genres.csv\")\ndf_known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_known = df_known[in_cols]\ny_known = df_known[\"genres\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ndf_unknown = df_2_std[df_2_std[\"genres\"] == 0]\ndf_unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_unknown = df_unknown[in_cols]\ny_unknown = df_unknown[\"genres\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = pps.matrix(df_known.reset_index()[[\"artists\", \"genres\"]])\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.subplots()\nsns.heatmap(pd.DataFrame(correlations[\"ppscore\"].values.reshape(2, 2),\n                         columns = [\"artists\", \"genres\"], index = [\"artists\", \"genres\"]),\n                         cmap = \"Wistia\", axes = ax)\nax.set_title(\"Predictive Power Score of Artists and Genres\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> The artists feature isn't a good indicator (its predictive power score is nearly 0), so we cannot really predict genres by artists."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_known.value_counts()[:25].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10, 10))\nax = fig.subplots()\ny_known.value_counts()[:25].plot(ax=ax, kind = \"pie\")\nax.set_ylabel(\"\")\nax.set_title(\"Top 25 most popular genres\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> There is a huge bias towards the most popular genres, especially Pop and Rock. In the following sections, we can increase the accuracy of missing data imputation with a simple yet clever trick: Only include the most popular ones!"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 400\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', max_words = max_words, colormap=\"viridis\",\n                min_font_size = 10).generate(\" \".join(df.index))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud)\nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\nplt.title(f\"The most {max_words} frequent words in genres\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> The most frequent words are mostly words from the most popular genres. Pop and Rock, which are the top 2 most popular genres, are also the top 2 most frequent words, so we can conclude that there may be too many subgenres of these genres (e.g. Canadian Pop => Pop, Alternative Rock => Rock)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nax = fig.subplots()\nax.set_title(\"Top 25 artists having the most genres\")\nax.set_ylabel(\"Genres\")\nax.set_xlabel(\"Artists\")\ndf_known[\"artists\"].value_counts()[:25].plot(ax=ax, kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> There are nearly 10k artists who do not have any genre at all, and there are artists who have too many genres (Deerhunter (23) followed by Wire(20)). The second type of artists are pretty rare, which leads to too much sparsity. That's why artists' predictive power score is too low. \n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1200px-Long_tail.svg.png)\n\n### -> Visualization of \"Long Tail\" phenomenon, X=number of genres, Y=number of artists"},{"metadata":{"trusted":true},"cell_type":"code","source":"many_knn_score(1, 6, X_known, y_known)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> When we simply impute missing genres using KNN, it produces results with a very low accuracy score. The more neighbors are calculated, the more accurate the results are. N_neighbors = 100 => Acc = 0.04, N_neighbors = 500 => Acc = 0.06. The most probable reason it might be increasing is that the data in general is biased towards the most frequent genres. The solution is definitely gonna be including only the most popular genres. "},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_genres = list(y_known.value_counts()[:25].index)\ndf_known_w_populars = df_known[df_known[\"genres\"].isin(popular_genres)]\nX_known_w_populars = df_known_w_populars[in_cols]\ny_known_w_populars = df_known_w_populars[\"genres\"]\n\nmany_knn_score(1, 26, X_known_w_populars, y_known_w_populars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> When we only use the most popular genres in the knn imputing and increase the number of neighbors to 26, the performance rises to 0.22: What a great improvement! Let's increase the number of clusters!"},{"metadata":{"trusted":true},"cell_type":"code","source":"many_knn_score(250, 260, X_known_w_populars, y_known_w_populars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> It looks like the accuracy does not improve any more... It is not a clever idea to impute missing genres because only 1 of 5 results is correct. "},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_profile = df_known_w_populars[[\"genres\", *in_cols]].groupby(\"genres\").mean()\nsimilarity_matrix=similarity_algorithm(X_known)\n\npreds=list(similarity_matrix.index[similarity_matrix.values.argmax(axis=0)])\n\naccuracy(preds, y_known)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> As it turns out, cosine similarity isn't a good measurement for this specific purpose, as its accuracy score is the worst among others (5e-3). It is even worse than filling all missing genres with the most popular genre Pop, which results in 9e-3. By the way, it is time-consuming and requires too much memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_means, genre_stds = [], []\ndf_std = df.copy()\nfor col in in_cols:\n    mean = df_std[col].mean()\n    std = df_std[col].std()\n    genre_means.append(mean)\n    genre_stds.append(std)\n    df_std[col] = (df_std[col] - mean) / std\ndf_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_many_clusters(df_std, 2, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> Observations: \n### ---> As the number of clusters increases, silhouette score decreases (N_clusters = 2 => Silhouette = 0.30, N_clusters = 9 => Silhouette = 0.15), which means the goodness of model decreases\n### ---> One-Way ANOVA and T-Test are rejected due to the lack of at least one of the following assumptions:\n\n### -------> T-Test: a) Normality, b) Different Means, c) No Correlation\n### -------> One-Way ANOVA: a) Normality, b) Different Means, c) Equal Variance\n\n### The data is not normally distributed, because in fact at the beginning we are transforming a multi-dimensional normal data into a single-dimensional non-normal data, which I believe it is to a certain extent not a huge deal-breaker at all. So let's do calculation!"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 2\ntester = Test(alpha = 0.05, only_result=False)\ndf_genre_profile = k_means(df_std, 2)[0]\nttest_result = tester.ttest(df_genre_profile[df_genre_profile[\"cluster\"] == 0][\"x1\"].values, df_genre_profile[df_genre_profile[\"cluster\"] == 1][\"x1\"].values)\n\nttest_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> Given that we pick 2 as the number of clusters, T-Test is rejected due to the lack of normality. Otherwise, the results indicate a very strong mean difference. (Group 1 has a lot bigger mean than Group 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"anova_results = []\nfor i in range(2, 9):\n    tester = Test(alpha=0.05, only_result=False)\n    df_genre_profile = k_means(df_std, i)[0]\n    test_data = [df_genre_profile[df_genre_profile[\"cluster\"] == x][\"x1\"] for x in range(i)]\n    anova_results.append(tester.anova(*test_data))\n    \npd.DataFrame(anova_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> Normality and homogenity are not fulfilled, so the test was rejected every time. However interestingly, the groups are always different. As the number of clusters increases, the mean difference decreases (6130 -> 1667), so picking a low number is a better option. The distribution is the most homogenous if the number of clusters is equal to 4 or 5 (levene-p is high and variance score is low). As you may notice in the plots above, 4 would be the best option! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions over new samples\n#e.g. predict_cluster(df.iloc[:350])\n\n#prediction over all data\n{k: v for i, (k, v) in enumerate(predict_cluster(df).items()) if i%100==0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict_cluster(df)\n\ndf_known_new = df_known.copy()\ndf_known_new[\"cluster\"] = df_known_new[\"genres\"].map(lambda x: pred[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#artists - clusters (group by artists and find the most frequent clusters)\nartists_clusters = df_known_new[[\"cluster\", \"artists\"]].groupby(\"artists\").agg(lambda x: x.value_counts().index[0])\nartists_clusters.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10, 10))\nax = fig.subplots()\nax.set_title(\"The distribution of clusters w.r.t. artists\")\nax.set_xlabel(\"Clusters\")\nartists_clusters.value_counts().plot(ax=ax, kind=\"pie\", ylabel=\"Percentage\", legend=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> Distribution is nearly uniform, except for one cluster dominates nearly the half of artists."},{"metadata":{"trusted":true},"cell_type":"code","source":"create_cluster_name(artists_clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -> Machine-generated names for possible genre groups. It is created by picking the top 3 most frequent words in the names of artists which are not stopwords like \"the\" or \"a\", and ordering them randomly. Can you see how amazing it is!"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n### -> The data is biased towards the most popular genres, mostly Pop and Rock, and which is why it is more difficult to predict less popular genres. \n### ---> Suggestion: Try over- or undersampling. \n\n### -> The imputing seems to work poorly both for K-Nearest Neighbors and Cosine Similarity Matcher.\n\n### ---> Suggestion: Play around with the KNN hyperparameters such as distance criterion. Try a neural network regressor with tuned hyperparameters, maybe MissForest imputer or even a NLP classifier which analyzes the artists' names. If it goes well, impute data and repeat the analysis.\n### ---> Hint: Classification results will be more accurate, as only the most popular genres are included in the labels. With this clever trick, the performance of has increased from 2-3%s to 22-23%s! (n_clusters in 2-9 interval)\n\n### -> Both silhouette scores are too low and we can not prove significant group differences, which is why K-Means Clustering didn't work as expected.\n### ---> Suggestion: Play around with the hyperparameters in K-Means model (e.g. set distance criterion to \"weighted\" instead of \"uniform\"(default))or try other clustering algorithms such as DBSCAN and Mean Shift.\n\n### -> According to statistical tests, the most suitable pick for number of clusters is 4.\n\n# The ball is on your court, try it yourself. By the way, don't forget to upvote my notebook :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}