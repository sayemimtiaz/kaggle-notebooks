{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\n$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install treelib\n#!pip install text_to_image\n#!pip install pystemmer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pystemmer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport os\nimport random\nimport time\n\n%matplotlib inline\nimport matplotlib.pyplot as plt, time\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.datasets import load_files\nfrom sklearn.externals import joblib #in place of pickling, its better\nfrom sklearn.decomposition import PCA  #use for similarity anlysis plots\nfrom sklearn.manifold import TSNE #use for similarity analysis plots\nimport seaborn as sns\n\n\n# NLP Packages\nimport scipy.io as spio\nimport spacy\nfrom scipy import ndimage\n#nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\nnlp = spacy.load(\"en_core_web_sm\")\nfrom spacy import displacy #this allows the visualizations to be printed in jupyter\n\n #py stemmer is a better implementation of snowball than nltk\nimport Stemmer\nstemmer = Stemmer.Stemmer('english')\n\nimport nltk\nfrom nltk import sent_tokenize, word_tokenize, regexp_tokenize\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer() # Use the wordnet corpus to define the lemma\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus.reader.wordnet import NOUN\n\nimport xml.dom.minidom\nfrom IPython.display import HTML\nimport re #regular expression tokenizer \n\nimport markovify #Markov Chain Generator\n\n# Neural Network Packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import text_to_word_sequence\n\nfrom itertools import chain  #flatten list of list\n\nfrom nltk.probability import FreqDist\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"tocheading\">Table of Contents</h1>\n<div id=\"toc\"></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# USER CONFIGURATION PARAMETERS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## SELECT CORPUS TO USE IN ANALYSIS\ncorpusname='lyrics'  # OPTIONS: lyrics, cbt, movies\ncorpussize= 'all'  # number or \"all\"\nbase = 'stem'   #'lemma'  or 'stem'  This also selects which base to use in the corpus cleaning for consist\n\nnew_model=True ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in Raw Data\n## Read in Affect Word Lists\n<a id='readinaffect'></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from treelib import Node, Tree\n#Read in Wordnet Affect\n\ndef readxml(filename):\n    #use the parse() function to load and parse the xml file\n    doc = xml.dom.minidom.parse(filename)\n    \n    #print out the document node and the name of the first child tag\n    print(doc.nodeName)\n    print(doc.firstChild.tagName)\n    \n    #get a list of xml tags from the document and print them\n    categ = doc.getElementsByTagName(\"categ\")\n        \n    data = []\n    for c in categ:\n        n=c.getAttribute(\"name\")\n        #print(n)\n        p=c.getAttribute(\"isa\")\n        data.append([n,p])\n    return(data)\n\n\n#emthyarch= readxml('../DataSets/wordnet-affect/wn-domains-3.2/wn-affect-1.1/a-hierarchy.xml')\nemthyarch = readxml('../input/wordnetaffect/a-hierarchy.xml')\neffdf = pd.DataFrame(emthyarch, columns=['Emotion','Parent'])\neffdf=effdf.drop([88,266,277,300,305,307]) #these are all loops the parent and child are same\n#wnemots =effdf['Emotion'].tolist()\n\n# Create tree object\ntree = Tree()\ntree.create_node('root','root') #root\n#tree.create_node(\"Joy\", \"joy\",parent='positive')\n\nfor i, r in effdf.iloc[1:].iterrows():\n    #print (i,'Emotion', r[\"Emotion\"],'Parent', r[\"Parent\"])\n    tree.create_node(r['Emotion'],r['Emotion'],parent=r['Parent'])\n\npos_t = tree.subtree('positive-emotion')\nneg_t = tree.subtree('negative-emotion')\namb_t = tree.subtree('ambiguous-emotion')\nemtwn = tree.subtree('emotion')\n\nprint('Here is an example of the positive list under the calmness branch:\\n')\npos_t.subtree('calmness').show()\nprint(emtwn.show())\n\nwnemots=(','.join([emtwn[node].tag for node in \\\n            emtwn.expand_tree(mode=Tree.DEPTH)]))\nwnemots=wnemots.split(',')\nprint(wnemots)\n\n\n#print('\\nLets look at the tree where our classifed word is:')\n#tree.subtree('emotion').subtree('gloom').show()\n#tree.show()\n#print(','.join([tree[node].tag for node in tree.rsearch('sad')]))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# 28 Russell terms\nrussell=['happy','delighted','excited','astonished','aroused','tense','alarmed','angry','afraid','annoyed','distressed','fustrated','misearable','sad','gloomy','depressed','bored','droopy','tired','sleepy','calm','relaxed','satisfied','at ease','content','serene','glad','pleased']\n# labels used in the images\n# note: 'Having Cold' was changed to cough\nilabels=['Anger','love','Fear','Failing','Guilt','Anxiety','Stress','Exhaustion','Tiredness','Mannia','Defecation','Heartbeat','Movement','Pride','Relaxation','Succeeding','Wanting','Orgasm','Craving','Feeling touch','Social Longing',\n        'Hositity','Losing','Shame','Despair','Nervousness','Shriving','Itching','Numbness','Acceleration','Eating','Breathing','Smelling','Winning','Laughing','Happiness','Pleasure','Love','Sympathy','Sexual Arousal','Lounging for',\n       'Despise','Disgust','Disappointment','Sadness','Lonliness','Aching','Flu','Coughing','Hotness','Thirst','Satiation','Tasting','Sleeping','Dazzled','Daydreaming','Gratefulness','Self-regulation','Surprise','Closeness','Togetherness',\n       'Ostractism','Panic','Social exclusion','Feeling pain','Depression','Toothache','Fever','cough','Coldness','Hunger','Urination','Hearing','Vitality','Imagining','Thinking','Being Conscious','Attending','Memorizing','Remembering','Forgetting',\n       'Drunkenness','Hangover','Suffocation','Headache','Nauseous','Stomach flu','Heartburn','Dizziness','Sneezing','Sweating','Drinking','Seeing','Speaking','Reading','Reasoning','Inferring','Estimating','Recognition','Recollection','Waiting']\n\nemotwords = wnemots + russell + ilabels  #Join all my emotion words together so that the model is filtered by all of them # or emots\nprint('There are ',len(emotwords),'emotional words in our emots list made up from Russells 28 original, the labels from the 100 body sensation maps and the ',len(wnemots),'affect wordnet terms\\n',emotwords[:20]) \n#some functions use emotwords - some use emots - need to clean up\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Affect Word Lists","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## CLEANING FUNCTIONS\n\ndef unique(list1):    # function to check for duplicates\n    list_set = set(list1)   # insert the list to the set; SETS can only have unique values\n    unique_list = (list(list_set))  # convert the set back to a list \n    return(unique_list)\n\n\ndef get_wordnet_pos(word):\n    #\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wn.ADJ,\n                \"N\": wn.NOUN,\n                \"V\": wn.VERB,\n                \"R\": wn.ADV}\n    return tag_dict.get(tag, wn.NOUN)  #was wn.NOUN\n\ndef lemmaemots(word):\n    #print('PRE LEMMA: ',word)\n    cleanemots= wnl.lemmatize(word, get_wordnet_pos(word))  #wordnet lemmetizer\n    #print('POST LEMMA:',cleanemots)\n    #cleanemots = cleanemots.lower()\n    return cleanemots\n\n\ndef stememots(emotwords):   #read in a list of words\n    #print('PRE STEM: ',word)\n    cleanemots = [stemmer.stemWord(word) for word in emotwords]  #make a list of stemmed words\n    cleanemots = [cw.lower() for cw in cleanemots]  # make a list of lower case stemmed words\n    cleanemots=unique(cleanemots)  # return a list of just unique stemmed lowercase words\n    #print('POST STEM:',cleanemots)\n    return cleanemots  #this is a list\n\nlemots=[lemmaemots(word) for word in emotwords]\nlemots=unique(lemots)\n\n\nstemots=stememots(emotwords)\n\n\n#print('\\nRAW:',len(emotwords), emotwords[:10])\n#print('\\nLEMMA & UNIQUE:',len(lemots), lemots[:10])\n#print('\\nSTEMMED & UNIQUE:',len(stemots))\n\n# SELECT LEMMA OR STEM\nif base=='lemma':\n    cleanemots=lemots\nelse:\n    cleanemots=stemots\n\n#cleanemots=cleanemots[0]  #get rid of list of list for no reason\n\nprint('\\nI have ',len(stemots),\"in the STEMMED & UNIQUE list.  The\",base,\"list has been selected. \\nHere's an example:\")\nprint(cleanemots[:10])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"APPEND 'IN' WORDS TO STEMMER\nThere are a lot of words in lyrics that are not real \"lovin\" \"diein\" to catch these append the stemmer & lemmer to recongize them ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets see if using the stemed words provide a bit more clustering, we'll have to use stem in the corpus too","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Read in Corpus Data\nuse this to select which corpus data to read in","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Children's Book Test Corpus\nWe'll read in a corpus to train our similarity analysis on","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Read in Childrens book corpus\nif corpusname == 'cbtnm':\n    \n    f = open(r\"../input/childrensbooks/cbt_train.txt\",\"r\")\n\n    cbt = f.read()\n    print(type(cbt),'example',corpus[:30])\n\n    cbtnm = 'cbt'\n\n    # split into sentences\n    #sentences = sent_tokenize(corpus) # or f if cbt\n    #sentences = unique(sentences) #unique sents NOT SURE I WANT TO DO THIS\n    print('There are ',len(corpus),'characters in the',corpusname,' corpus')\n    print('There are ',len(sentences),'sentences in the',corpusname,' corpus')\n\n    #print(type(sentences))\n    #sentences[:5]\n    corpus=cbt\nelse: print('CBT Corpus Not Selected')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Movie Corpus\nif corpusname == 'movies':\n    f = open(r'../input/movie-dialog-corpus/movie_lines.tsv')\n    movies=f.read()\n\n    print(type(cbt),'example',corpus[:30])\n\n    movnm = 'movies'\n\n    # split into sentences\n   # sentences = sent_tokenize(movies) \n   # sentences = unique(sentences) #unique sents NOT SURE I WANT TO DO THIS\n   # print('There are ',len(movies),'characters in the',movnm,' corpus')\n   # print('There are ',len(sentences),'sentences in the',movnm,' corpus')\n\n    #print(type(sentences))\n   # sentences[:5]\n    corpus=movies\nelse: print('Movie Corpus Not Selected')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Song Lyrics Corpus\nSo the CBT corpus is not as valuable as I thought it would be.  It turns out the language used is sort of odd, and doesn't use as many emotional words as I thought it would.  \n\nSongs are full of emotion! Lets use that data set. ","execution_count":null},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"songdata=[]\nif corpusname == 'lyrics':\n    songdata = pd.read_csv(\"../input/150k-lyrics-labeled-with-spotify-valence/labeled_lyrics_cleaned.csv\")\n    lyrics=songdata['seq']  #returns a list of lyrics 1 item per song\n    lyricsnm='spotify'\n    corpus = lyrics  \n    print(type(corpus),'\\nExample of corpus:\\n',lyrics[:5])\n\nelse: print('Song Lyrics Corpus Not Selected')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('************************ MY SELECTED CORPUS IS ',corpusname,'*******************************')\n#print(corpus[0])\nprint(type(corpus))\nprint(len(corpus))\nif corpussize=='all':\n    corpussize=len(corpus)\n    print(corpussize)\ncorpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Corpus\n\n#### CLEANING\n* STOP WORDS - not removed because the downsampling of frequent words will get rid of them anyway - the sample parameter = 0.001 is the pruner.  It may give better results with since we are looking for neighbors for context.  \n* LOWER CASE -  since i don't expect there to be too many proper names (apple vs Apple) or acrynoms in my corpus I'll strip caps  \n* PUNCTUATION -  I dont care about punctuation except at end of sentences; the end of sentence punctuation will be used to split by sents.   \n* LEMMA - NLTK wordnet lemmetizer takes input string; if you pass a word it'll treat it like a noun; to lemma a sentence/lyrics/pharse: 1st convert sent to touples with word and pos;  use the converter to account for the pos by nltk.pos_tag() and what's expected by wordnetLemmetizer().  For lyrics the entire song is sent - for movies would need to convert to sent lemma. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n### Emotional Based Corpus\nNext, we limit the corpus to only include sentences which have at least one emotional word it.  We use the combined all emots list to filter the corpus by\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s=set(cleanemots)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\ndef nltk2wn_tag(nltk_tag):  #accounts for diffences in nltk tagging vs wordnet.lemmetizer\n    if nltk_tag.startswith('J'):\n        return wn.ADJ\n    elif nltk_tag.startswith('V'):\n        return wn.VERB\n    elif nltk_tag.startswith('N'):\n        return wn.NOUN\n    elif nltk_tag.startswith('R'):\n        return wn.ADV\n    else:                    \n        return None\n\ndef findemotlines(text):  #use this type of function to limit each line with emotional terms\n    emotlines=[]\n    st=text.splitlines()        #SPLIT LYRIC INTO LINES\n    #print('ST: ',st)\n    #print('CLEANEMOTS: ',cleanemots[:5],len(cleanemots))\n    for line in st:                    # evaluate each sentence in the cleansents list - this loops through the cleansents list for each emotion \n        #CHECK IF LINE HAS WORD IN CLEAN EMOTS.\n        x=set(line.split()) & set(cleanemots)   #use set because word list is long and need to do this operation alot\n        if x!=set():  # if it has an emotional word grab the entire line\n            emotlines.append(line)              \n    return(\" \".join(emotlines))\n\ndef lemmatize_sentence(sentence):\n    print('*** TIME TO LEMMA ***')\n    tokens=regexp_tokenize(sentence,pattern='\\s+',gaps=True)  #use REGEXP_TOKENIZER - ITS FASTER THAN WORD_TOKENIZER\n    nltk_tagged = nltk.pos_tag(tokens)    #tokenize then tag\n    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)  #translates between nltk & wn tags\n    lemma_words = []\n    for word, tag in wn_tagged:\n        #print(word, tag)\n        if tag is None:                        \n          lemma_words.append(word)\n        else:\n          lemma_words.append(wnl.lemmatize(word, tag))  #replace word with lemma word using the wn tags\n    #return \" \".join(lemma_words)\n    return lemma_words\n\ndef clean_text(text,emotional=False):   \n    print('>>>>>>>>>>>>>>>>> CLEANING ONE LYRIC AT A TIME <<<<<<<<<<<<<')\n    # change all in' to ing as in lovin' to loving\n    #print(text)\n    text=text.replace(\"in'\", \"ing\")\n    #print(text)\n    #ONLY keep EACH LINE if has emotional word in it -> emotionally strong corpus\n    print('\\n\\nbefore:',len(text),'\\n')\n    if emotional:\n        #print('check to see if any of these words are in my lyric:')\n        text= findemotlines(text) \n    print('AFTER: ',len(text),'\\n')  \n    \n    #replace new line & carriage retuns with space\n    text=text.replace(\"\\n\", \" \").replace(\"\\r\",\" \")\n    #print('new line',text)\n    \n    #replace numbers and punctuation  with space except '  # can do this during tokeniization in lemma??\n    punc_list = '\"?!#$%&().*+, -./:;<=>@[\\]^_`{|}~' + '1234567890'\n    t=str.maketrans(dict.fromkeys(punc_list,\" \"))\n    text=text.translate(t)\n\n    \n    #replace single quotes wtith empty set \n    t=str.maketrans(dict.fromkeys(\"'\",\"\"))\n    text=text.translate(t)\n\n    #make all lower caps\n    text = text.lower() \n    \n    # lemma each lyric - maybe want to split by sentence: \n    if base=='lemma':\n        tokens = lemmatize_sentence(text)\n    elif base=='stem':\n        tokens=regexp_tokenize(text,pattern='\\s+',gaps=True) \n        tokens = [stemmer.stemWord(word) for word in tokens]  \n    #print('LENGTH OF TOKENS', len(tokens))\n    return tokens\n\nclean_text(\"I be lovin' these cakes!\\nlovin' lovin' wantin' more cake and coffee!!\\nOh yea more cake and coffee!\",True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CORPUS SIZE',corpussize)\nprint('STEM or Lemma:', base)\ncleancorp=[]\ntokens=[]\n\nfor corpitem in corpus[:corpussize]:\n        #print('*****CORPUS ITEM ******:\\n ',corpitem[:50])\n        tok=clean_text(corpitem, emotional=True) #call tokenizer\n        text=\" \".join(tok)\n        print('>>>>> CLEANED TEXT: >>>>',text[:50])\n        print('>>>>> CLEANED TOKENS: >>>>',tok[:50])\n        cleancorp.append(text)\n        tokens.append(tok)\n\n#print('CLEAN - STEMMED AND TOKENIZED ')  #nltk regexp is way faster!\n#print(len(cleancorp))\n#print('example of cleaned corp\\n',cleancorp[:3])\n#print('\\nexample of tokens\\n',tokens[:3],'\\nTOKEN LENGTH:',len(tokens))  #list of lists\n\nalltokens=list(chain.from_iterable(tokens))  #single list of all tokens\nprint('alltokens has',len(alltokens), ' and look like this: [token1,token2,token3,token4,...]')\n\nprint('\\CLEANED EMOT WORDS',type(cleanemots), len(cleanemots),'\\n')\n\n#Join the emotionwords with the corpus list so model has every emotional words\n# +, extend, append, Insert, [list1,list2} and #tokens[len(tokens):len(tokens)] = emotwords  didn't work - lets do it mannually\ntok_emots=[]\nfor i in tokens:\n    tok_emots.append(i)\ntok_emots.append(cleanemots)  #emots are cleaned\nprint('New List to model on length',len(tok_emots), '\\nIt looks like this: [[lyric1],[lyric2],[lyric3],[],[],[emotwords]] where each lyric is tokenized [word1,word2,word3]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency Distribution of Tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist(alltokens)\n#print(fdist)\n\nprint(fdist)\nimport matplotlib.pyplot as plt\nfdist.plot(30,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simialarity Analysis\n## Word2Vec Model\nLatent Semantic Analysis (LSA) is the most popular technique of Corpus-Based similarity. LSA assumes that words that are close in meaning will occur in similar pieces of text.  One type of LSA is Word2vec, a word embedding model, originally published by \\cite{Milkov} is 2-layer neural network vectorizes words from a corpus and returns a set of vectors representing the words.  Word2vec is able to discern meanings of words by the context in which they are used. There are two methods used, either a continuous bag of words or a continuous skip-gram algorithm.  \n\nGreat internal what its doing reference: http://jalammar.github.io/illustrated-word2vec/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the model\nimport gensim, os\nfrom gensim.models.word2vec import Word2Vec\n\nnew_model=True      # new_model=True runs a new model; new_model=False loads the most recent kaggle version\n# Using params from Word2Vec_FastText_Comparison\nparams = {\n    'alpha': 0.025,  # alpha (float, optional) – The initial learning rate.\n    'size': 25,     #The size of the dense vector to represent each token or word (i.e. the context or neighboring words\n    'window': 5,     #The maximum distance between the target word and its neighboring word.\n    'iter': 5,       #Number of iterations (epochs) over the corpus\n    'min_count': 1,  #Minimum frequency count of words. The model would ignore words that do not satisfy the min\\_count.\n    'workers': 2,     #,   #How many threads to use behind the scenes\n    'sg': 1,        # sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n#    'hs': 0,        # hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n#    'negative': 5   # negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n\n}\n\nif new_model:\n    model = Word2Vec(tok_emots, **params)  # wordlist token list\n    # Save the entire model as a SavedModel in HDF5 format.\n    model.save('AffectSimialirty_Model.h5') \n    # To export our model as text just to inspect it \n    model.wv.save_word2vec_format('wvmodel_vectorsT.txt', binary=False)\n    print('New Model Created and saved')\n    print(model)\nif not new_model:\n    # Load the Model back from file\n    print(\"loading model...\")\n    model = gensim.models.Word2Vec.load('../input/affectsimilarities/AffectSimialirty_Model.h5')\n    print(model)\n    print('complete')\n\n\n# contains the list of all unique words in pre-trained word2vec vectors\n# summarize vocabulary\nmodelwords = list(model.wv.vocab)\nprint('An example of the words in the Word2Vec model are:',modelwords[:100])\n\n# access vector for one word\n#print('\\nlower',text,'\\n')\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Lets test it!')\n# lemma each lyric - maybe want to split by sentence: \nif base=='lemma':\n    hap = wnl.lemmatize('happy', wn.ADJ)\nelif base=='stem':\n    hap = [stemmer.stemWord('happy')]  \nprint(base,':',hap)\nprint(\"Happy looks like this\",model[hap][:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have word2vec word embeddings - aka vectors by meaning trained on our corpus. We can now find similar words and append each of the words embedding vector to a matrix; \napply TSNE to the matrix to project the word into a 2D space. AND plot the 2D position of the word with a label. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n#### What is Annoy?\n\"Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data.\" \n\nAnnoy is an open source library to search for points in space that are close to a given query point. For our purpose, it is used to find similarity between words or documents in a vector space. https://markroxor.github.io/gensim/static/notebooks/annoytutorial.html\n\n#### Lets use Annoy Indexer\nWhy use Annoy?\nThe current implementation for finding k nearest neighbors in a vector space in gensim has linear complexity via brute force in the number of indexed documents, although with extremely low constant factors. The retrieved results are exact, which is an overkill in many applications: approximate results retrieved in sub-linear time may be enough. Annoy can find approximate nearest neighbors much faster.\n\nThe Annoyindexer creates K-means clusters to be created using approximate nearest neighbors rather then the word2vec built in K-means exact method, which can be approximately 65 times faster.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set up the model and vector that we are using in the comparison\ntry:\n    from gensim.similarities.index import AnnoyIndexer\nexcept ImportError:\n    raise ValueError(\"SKIP: Please install the annoy indexer\")\n    \nmodel.init_sims()\n\nif new_model:\n    annoy_index = AnnoyIndexer(model, 500)  \n    annoy_index.save('annoyindex.index')\n    print('New Annoy Index Created and saved')\n    \nif not new_model:\n    # Load the Model back from file\n    print(\"loading indexer...\")\n    annoy_index = AnnoyIndexer()\n    annoy_index.load('../input/affectsimilarities/annoyindex.index.index')\n    annoy_index.model = model  #assign index model\n    #create and save Annoy Index from a loaded `KeyedVectors` object (with 100 trees) \n    annoy_index = AnnoyIndexer(model, 500)  # number of trees does not have to match saved version, (not sure of the # tree tradeoff)   # To  get a  metric='angular' error\n    print(annoy_index.model)\n    print('Annoy Index Loaded')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#Word2Vec.most_similar like we would traditionally, but with an added parameter, indexer. The only supported indexer in gensim as of now is Annoy.\n# Derive the vector for the word \"science\" in our model\ndef getmynegh(word,exact,num):\n    vector = model.wv[word][0]\n    neighbors=[]\n    if exact:\n        # The instance of AnnoyIndexer we just created is passed \n        approximate_neighbors = model.wv.most_similar([vector], topn=num, indexer=annoy_index)\n\n        # Neatly print the approximate_neighbors and their corresponding cosine similarity values\n        print(\"\\nApproximate Neighbors for the word \",word,\":\")\n        for neighbor in approximate_neighbors:\n            neighbors.append(neighbor)\n            print('{:20}: {:.5}'.format(neighbor[0],neighbor[1]))\n   \n    if not exact: \n        normal_neighbors = model.wv.most_similar([vector], topn=num)\n        print(\"\\nNormal (not Annoy-indexed) Neighbors for the word \",word,\":\")\n        for neighbor in normal_neighbors:\n            neighbors.append(neighbor)\n            print('{:20}: {:.5}'.format(neighbor[0],neighbor[1]))\n    return(neighbors)\n        \ngetmynegh(hap,exact=True, num=15)\ngetmynegh(hap,exact=False, num=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Metrics & Initialization Time\nRelationship between num_trees and initialization time:\nInitialization time of the annoy indexer increases in a linear fashion with num_trees. Initialization time will vary from corpus to corpus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# If you want exact results instead of approximate cheange this line for the approxmate results line in the for statement\nexact_results = [element[0] for element in model.wv.most_similar([model.wv.vectors_norm[0]], topn=100)]\n\nx_values = []\ny_values_init = []\ny_values_accuracy = []\n\nfor x in range(1, 300, 10):\n    x_values.append(x)\n    start_time = time.time()\n    annoy_index = AnnoyIndexer(model, x)\n    y_values_init.append(time.time() - start_time)\n    approximate_results = model.wv.most_similar([model.wv.vectors_norm[0]], topn=100, indexer=annoy_index)\n    top_words = [result[0] for result in approximate_results]\n    y_values_accuracy.append(len(set(top_words).intersection(exact_results)))\n\nplt.figure(1, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(x_values, y_values_init)\nplt.title(\"num_trees vs initalization time\")\nplt.ylabel(\"Initialization time (s)\")\nplt.xlabel(\"num_trees\")\nplt.subplot(122)\nplt.plot(x_values, y_values_accuracy)\nplt.title(\"num_trees vs accuracy\")\nplt.ylabel(\"% accuracy\")\nplt.xlabel(\"num_trees\")\nplt.tight_layout()\nplt.show()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Visualization - Emotional Terms\nThere are multiple ways to map the multi-dimensional space of vectors onto a 2D plot.  We'll inspect 2 of them: \n\n1. tsne_plots\n1. PCA \n\nBut first, we need to limit our vector space to a subset of words - we only care about emotional words. While, we may want the full model to do word prediction, the maps should just be emotional terms. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function limits the words to be displayed by some emotwords list\ndef smallermodel(emotwords):\n    smmodel={}\n    for key, val in model.wv.vocab.items():\n        if key in emotwords:\n            #print('my emot is in the model:',key)\n            smmodel[key] = val\n    return(smmodel)\n\ndef getwords(dictmodel):\n    return dictmodel.keys()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# limit vectors to Russell affect terms\nprint(russell[:10])\ncrussell=stememots(russell) \n#crussell=[review_to_words(text,base) for text in russell]  #clean the 28 russel terms\nprint('CLEANED RUSSELL:\\n',crussell)\n\n# Use the emotional word list and find the 10 most simiar words then map those... it should give us about 200 words on our map\nrussmodel={}\nrussmodel = smallermodel(crussell)\nprint(type(russmodel),len(russmodel))\n\n# limit vectors to BSM100 Labels \nclabels = stememots(ilabels)  # Labels from image classifications (SEE code block 2) \nprint(print('CLEANED BSM100 LABELS:\\n',clabels[:5]))\n\nbsm100model={}\nbsm100model = smallermodel(clabels)\n\nprint(type(bsm100model),len(bsm100model))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Similarity mapping of just the affect terms\nNext, we'll only keep the words in our model results that are in our wordnet affect sysnets. This is a list of 300 emotional words so it should be fairly comprehensive.   \nA principal component analysis will be performed to map the relationships between the words. \n\nRemember our sentence... Lets use this model and see if we can do better: \n** HELP **\nummm we need to actually update the model... not sure how to do that.  I don' want to strip all the non emotional words out before I train... I feel like that would loose something. \nBut I can't strip it out after because I just get a dict. umm. \n\nInteresting results.  I'm surprised \"bang\" and \"chill\" are in the list. \n\nLets try the 28 words that James Russell originally used.  Again, we'll limit our model results to just these words.  Lets see what that looks like for comparision reasons to his model, remember not all the words will show up. Only the words that are both in our corpus and our emotional list.\n\nThe russell list is defined in <a href='#readinaffect'> affect load list section</a>\n\n### Dimensionality reduction using PCA\nPCA is a technique for reducing the number of dimensions in a dataset whilst retaining most information. It is using the correlation between some dimensions and tries to provide a minimum number of variables that keeps the maximum amount of variation or information about how the original data is distributed. It does not do this using guesswork but using hard mathematics and it uses something known as the eigenvalues and eigenvectors of the data-matrix. These eigenvectors of the covariance matrix have the property that they point along the major directions of variation in the data. These are the directions of maximum variation in a dataset.  (ref: https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit a 2d PCA model to the vectors, #this function creates a PCA visualization with a list of words and their simiarities as inputs\ndef pcavis(modelwords,modelname,ncomps):\n    X = model[modelwords]\n    #print(X[:5])  #prints 5 arrays of pcas\n    savename='PCA and TNSE'+str(modelname)+' with '+str(ncomps)+'components.png'\n    ## ADD DF.CLASS - USE THE BSM100 DATA TO CATEGORIZE EMOTIONS AS POS, NEG OR NEUTRAL USE THE CLASS AS HUE\n    \n    pca = PCA(n_components=ncomps)\n    result = pca.fit_transform(X)  #returns the principal components, the number returned is identified in n_compontents\n        \n    #convert our words and the vectors into a df\n    df = pd.DataFrame(list(modelwords),columns=['Words'])\n    df['vec']=list(X)\n    #print(df)\n    #append the first 2 pca results to the df\n    df['pca_one']=result[:,0]\n    df['pca_two']=result[:,1]\n    #df['pca_three']=result[:,2]\n    #df['pca_four']=result[:,3]\n    \n    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n    \n    #TNSE:\n    time_start = time.time()\n    tsne = TSNE(n_components=2, verbose=1, perplexity=4, n_iter=300)\n    tsne_results = tsne.fit_transform(X)\n    #print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n    df['tsne_one'] = tsne_results[:,0]\n    df['tsne_two'] = tsne_results[:,1]\n    return df\n\n#=========================================\ndef mkpcaplot(df,nm,x1,x2,y1,y2):\n    # Seaborn Plots\n    # Define plot space\n    plt.figure(figsize=(8,6))\n    s1=sns.scatterplot(\n        x=\"pca_one\", y=\"pca_two\",  #hue=\"y\",\n        palette=sns.color_palette(\"bright\"),\n        data=df,\n        #legend=\"full\",\n        alpha=0.75\n    )      \n    #add annotation:\n    for i, row in enumerate(df):\n        xy = (df.pca_one[i], df.pca_two[i])\n        name = df.Words[i]\n        plt.annotate(name, xy=xy)\n    s1.set(title='PCA First two components',\n           xlim=(x1, x2),\n            ylim=(y1, y2))\n    plt.show()\n    fname=nm+'PCA'\n    plt.savefig(fname=fname,format='png')\n\ndef mktsneplot(df,nm,x1,x2,y1,y2):\n    plt.figure(figsize=(8,6))\n\n    s2=sns.scatterplot(\n        x=\"tsne_one\", y=\"tsne_two\", #hue=y,\n        palette=sns.color_palette(\"bright\"),\n        data=df,\n        legend=\"full\",\n        alpha=0.3,\n    )\n    #add annotation:\n    for i in range(1,df.shape[0]):\n           s2.text(df.tsne_one[i]+0.2, df.tsne_two[i], df.Words[i], horizontalalignment='left', size='x-small', color='black')\n    s2.set(title='T-SNE Plots',\n            xlim=(x1, x2),\n            ylim=(y1, y2))\n    plt.show()\n    fname=nm+'T-sne'\n    plt.savefig(fname=fname,format='png')\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Lets visualize all of the terms in our model... be patient this takes a while')\nemotmodel = smallermodel(cleanemots)  #limit the model by the cleanemots emotional word list from wordnet\nprint(type(emotmodel),len(emotmodel))\n\nemotdf=pcavis(modelwords=model.wv.vocab,modelname='main',ncomps=2)  #model names: main, wordnet,  modelinput: model.wv.vocab, emotmodel, russmodel etc.\n#mkplts(emotdf)\nname='allwords'\nnamez='allwords - Zoomed'\ndf=emotdf\nmkpcaplot(df,name,-.05,.05,-.05,.05)\nmktsneplot(df,name,-30,30,-30,30)\nmkpcaplot(df,namez,-.02,.02,-.02,.02)\nmktsneplot(df,namez,-30,30,-30,30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Russell Affect Terms\nrussdf=pcavis(modelwords=russmodel, modelname='Russell 28 Terms', ncomps=2)  #model names: main, wordnet,  modelinput: model.wv.vocab, emotmodel, russmodel etc. \nname='Russell'\ndf=russdf\nnamez='Russell - Zoomed'\nmkpcaplot(df,name,-.05,.05,-.05,.05)\nmktsneplot(df,name,-30,30,-30,30)\nmkpcaplot(df,namez,-.02,.02,-.02,.02)\nmktsneplot(df,namez,-30,30,-30,30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot BSM100 Affect Terms\nbsmdf=pcavis(modelwords=bsm100model,modelname='Image Labels',ncomps=2)  #model names: main, wordnet,  modelinput: model.wv.vocab, emotmodel, russmodel etc. \nname='BSM100'\nnamez='BSM100 - Zoomed'\ndf=bsmdf\nmkpcaplot(df,name,-.05,.05,-.05,.05)\nmktsneplot(df,name,-30,30,-30,30)\nmkpcaplot(df,namez,-.02,.02,-.02,.02)\nmktsneplot(df,namez,-30,30,-30,30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Generation\nLets use 4 approachs\n1. Fill in the blank\n1. Synonym from wordnet \n1. Meaning from wordNet\n1. Model based - using our corpus driven model\n1. Markov Generator\n\nVisually inspect the model based using displacy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('******* Reading in the classified emotion *********')\n# Reload data from imageclassification by clicking add data select the Kernel output files> my work\nimport _pickle as cPickle\nimport csv\ntry:\n    with open(r\"../input/imageclassificationv2/myemtdata.pickle\", \"rb\") as input_file:\n        dat = cPickle.load(input_file)\n    csvemot = pd.readcsv(\"../input/imageclassificationv2/myemot.csv\") #read data from csv file\n    print(csvemot)\n    print('this data comes from my classifed emotion model:')\n    classemot=dat[\"myemot\"]\n    print(classemot)\nexcept:\n    classemot='sad'\n    print(classemot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_closestwords_tsnescatterplot(model, word):\n    \n    arr = np.empty((0,300), dtype='f')\n    word_labels = [word]\n\n    # get close words\n    close_words = model.similar_by_word(word)\n    \n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get Synonyms from all of wordnet - use this to compare with model output\nsynonyms = [] \nantonyms = [] \n  \ndef getnewword(classemot):\n    for syn in wn.synsets(classemot):   #for each of the synsets of our classified word\n        for l in syn.lemmas():                #get the lemma\n            if l.name()!= classemot:\n                synonyms.append(l.name())          #append the lemma to a list\n            if l.antonyms():            # Get the list of antonyms\n                antonyms.append(l.antonyms()[0].name())   #append the antoynms to a list ... just because \n    return(random.choice(synonyms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. READ IN MY CLASSIFIED EMOTION\nfrom nltk.stem import *\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n\n\nprint('My Classified Emotion: ',classemot)\nprint('We need to clean it')\nclassemot = classemot.lower()  #lower case\nclassemotl =wnl.lemmatize(classemot, pos='v')  #lets lemmatize it!\nclassemots = stemmer.stem(classemot)\nprint('The lemma of our word is:',classemotl,'\\nThe stem of our word is:',classemots)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sen='I feel '+classemotl\nprint(sen)\n\nnewemot=getnewword(classemotl)\n\nprint('\\n** USING wordnet synonyms **')\nnewsent=\"I feel \"+ newemot\nprint('This uses just a synonym from wordnet:\\n',newsent)\n\nsyn = wn.synsets(newemot)\nelabsent=\"I am \"+(syn[0].definition())\nprint('Here is an elaborate sentence:\\n',elabsent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nNow lets get the most similar word to our classified word using our model:')\nneighbors=getmynegh(classemots,model,5) # I NEED TO ITERATE ON THE CLASSIFIED EMOTION, OR MOVE UP THE WORDNET TREE UNTIL I GET A WORD IN THE MODEL\nprint(type(neighbors))\nsmartemot=neighbors[1][0]\nprint(type(smartemot), smartemot)\nsmartsent='I feel '+ smartemot\nprint('\\nModel Generated Sentence:\\n',smartsent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next let’s use the displaCy library to visualize the parse tree for that sentence:\n\nGenSent=nlp(smartsent)\nprint(type(GenSent))\nprint('Dependency Analysis')\n#displacy.render(GenSent, style='dep', jupyter=True, options={'dimension':150,'compact':True})\n#sentence_spans= doc.sents\n#print(sentence_spans)\ndisplacy.render(GenSent, style=\"dep\", jupyter=True, options={'dimension':150,'compact':True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the Marvovian model based off of our Corpus text.\ntext_model = markovify.Text(cleancorp)\n\nprint('\\n\\nHere are 3 randomly-generated long sentences:\\n')\nfor i in range(3):\n    print(text_model.make_sentence())\n    \nprint('\\n\\nHere are 3 randomly-generated sentences of no more than 120 characters:\\n')\nfor i in range(3):\n    print(text_model.make_short_sentence(120))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## COMPARISON USING VECTORS FROM GLOVE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## READ IN EMOT TERMS AND USE T-SNE PLOTS WITH PRETRAINED VECTORS IN GLOVE\nimport numpy as np\nfrom scipy import spatial\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\nembeddings_dict = {}\nwith open(\"../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if word in emotwords:                 #check to see if the word is an emotional word\n            #print('FOUND ONE ', word)\n            vector = np.asarray(values[1:], \"float32\")\n            embeddings_dict[word] = vector\n\ndef find_closest_embeddings(embedding):\n    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n\ntsne = TSNE(n_components=2, random_state=0)\nwords =  list(embeddings_dict.keys())\n\nvectors = [embeddings_dict[word] for word in words]\n\nY = tsne.fit_transform(vectors)\n\nplt.scatter(Y[:, 0], Y[:, 1])\n\nfor label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#zoomed in\nplt.scatter(Y[:, 0], Y[:, 1])\n\nfor label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n    plt.annotate(label, xy=(x, y), xytext=(0, 5), textcoords=\"offset points\")\n\n    \nplt.axis([-5,5,-5,5])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topic Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Appendix A\n## TFIDF Generator \ncreate a instead of using our model\ninsiration: https://www.kaggle.com/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# number of topics to extract\nn_topics = 15\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nvec = TfidfVectorizer(max_features=5000, stop_words=\"english\", max_df=0.95, min_df=2)\nfeatures = vec.fit_transform(emotcorpus)\n\nfrom sklearn.decomposition import NMF\nrandom_state=0\ncls = NMF(n_components=n_topics, random_state=random_state)\ncls.fit(features)\n\n# list of unique words found by the vectorizer\nfeature_names = vec.get_feature_names()\n\n# number of most influencing words to display per topic\nn_top_words = 15\n\ntopics=[]\nfor i, topic_vec in enumerate(cls.components_):\n    x=[]\n    print(i, end=' ')\n    # topic_vec.argsort() produces a new array\n    # in which word_index with the least score is the\n    # first array element and word_index with highest\n    # score is the last array element. Then using a\n    # fancy indexing [-1: -n_top_words-1:-1], we are\n    # slicing the array from its end in such a way that\n    # top `n_top_words` word_index with highest scores\n    # are returned in desceding order\n    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:\n        print(feature_names[fid], end=' ')\n        x.append(feature_names[fid])\n    topics.append(x)\n    print()\n    \nemottopics=[]\n# Now change each list of words into a string of words\nfor t in topics: \n    seperator = ' '\n    emottopics.append(seperator.join(t))\n\nfor i in emots:\n    print('I:',i)\n    for t in emottopics:\n        print('topic string:',t)\n        if i in emotwords: #not sure I'm just returning my emotionaal words, it should be. \n            print('my emotion is in my topic:',t)\n            emottopics.append(t)\nemottopics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Results\nSo this is very interesting.  First we found that the childrens boook corpus was pretty useless, as it didn't have very many of the words that we use in our language today.  I find it interesting that the words in our affect list, are located where they are.  I would have thought that laughter, pride,  and love would have been closer related.  Of course, my thinking is influenced by the various frameworks in place today.  I have a mental image of the James Russell Circumplex of Affect model with affect terms laid out in a arousal vs pleasure scale.  As Dr. Peter refered to, that we discussed in chapter 2, previous models were based on the need for studying physiology or neuroscience, not HCI.  It may be worth considering that moving forward a model based on language corpus is used for HCI needs. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References\nhttps://rare-technologies.com/word2vec-tutorial/ - word2vec turtorial\nhttps://medium.com/@shubhamagarwal328/playing-around-with-word2vec-natural-language-processing-ccd10a044b1 - word2vec\n\nword2vec explained\nhttps://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n\nmoreW2V - w1-w2 simialarity stuff < - READ MORE\nhttps://www.freecodecamp.org/news/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3/\n\nYEP USED THIS TOO\nhttp://www.pitt.edu/~naraehan/presentation/word2vec-try.html\n\nNLP REFERENCES\nhttps://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n\nMAYBE USEFUL\nhttp://www.lumenai.fr/blog/quick-review-on-text-clustering-and-text-similarity-approaches\n\nAH THE ANNOY MODEL \nhttps://markroxor.github.io/gensim/static/notebooks/annoytutorial.html\n\nOH CAN I USE THE MARKOVIAN CHAIN <- DO MORE HERE\nhttps://towardsdatascience.com/nlg-for-fun-automated-headlines-generator-6d0459f9588f\n\n\nTOPIC MODELING\nhttps://sanjayasubedi.com.np/nlp/nlp-with-python-topic-modeling/\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TO DO - COOL STUFF\n\nDO THAT LANGUAGE MAPPING - WORD TYPE STUFF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Now use SpaCy to do some NLP exploration\ncorpus=getcorp()\n\ndoc = nlp(corpus[:200000])\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n\ncols = (\"Word\", \"Lemma\",\"Tag\",\" POS\",\"Tag - Long\", \"Stopword\") \nrows = []\n\n\n\n\nprint('\\nSending the selected corpus through spacy we get the Lemma, Part of Speech, and Stopword')\n\ncor_pos = getpos(doc)\ncor_pos[:10]\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets do a language map of our sentence to make sure it has all the sentence parts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}