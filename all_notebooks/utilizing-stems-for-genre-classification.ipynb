{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GTZAN Stems\n## Overview\nThis notebook presents an examle usage of **gtzan-stems** dataset. As well as that, it compares the performance of the 3 CNN models:\n1. **Vanilla** model trained using extracted audio features from original song files\n2. **Enhanced** model trained using extracted audio features from stems that make up a song\n3. **Merged** model trained using extracted audio features from original song files _and_ stems \n\nThis notebook also introduces a compact class for extracting any audio features from labeled songs or multiple stems that make up a song. It is very easy to extend the class by adding custom extractions. This notebook only uses **MFCC** vector extraction for multiple segemnts of an audio file. Extracted vectors are interpreted as images by the CNN models and a stem represents a channel. \n\n## What to take from this notebook\nIt is adviced to simply look at the results of the last cell without getting into too much detail how everything works. The main purpose of this notebook is to show how stems can contribute to model's performence. From the experiments I've done with different seeds and input structures, there is no big difference between training a model using original songs and training a model using stems, sometimes one approach beats another. However, it is almost always the case that the merged model has a higher performance than the first 2 models.\n\nMore training data allows to determine more stable results - it can be observed that the model trained on stems tends to score a higher accuracy by a few percents than the one trained using original songs. Also note that some stems may be empty - for example, if lyrics are not present in the song, the audio file is almost silent and it may not be useful for the model. However it can still learn the patterns about genres from other stems.","metadata":{}},{"cell_type":"code","source":"import os          # traversing directories\nimport json        # saving data in json format\nimport math        # allows to perform ceiling\nimport librosa     # extracting audio features\nimport numpy as np # shaping multidimensional data\nfrom tqdm.notebook import tqdm # progress bars\n\n\nclass AudioFeatureExtractor():\n    \"\"\"Class to preprocess raw audio files.\n    \n    This class allows to extract custom audio features (e.g., amplitude envelope, bandwidth,\n    Mel-spectrograms) from labeled audio files. Each file can further be segmented for data\n    augmentation. This class supports reading directories of stems instead of audio files too.\n    \n    Attributes:\n        duration (int): The duration of each audio file to be processed (in seconds)\n        sample_rate (int): The sample rate used (`22050` by default)\n        num_segments (int): The number of segments to divide each audio file (`5` by default)\n        samples_per_label (int): The number of samples to process pre label (`-1` by default)\n        \n        samples_per_track (int): The total number of samples per track (calculated)\n        samples_per_segment (int): The total number of samples per segment (calculated)\n        \n        _DEFAULT_CONSTANTS (dict): The default parameters the extraction functions will use\n            All the possibly used conctants are put in the dictionary for any extraction:\n            * `n_mfcc`: The number of Mel-frequency cepstral coefficients to extract at each frame\n            * `n_fft`: The frame size (number of Fast Fourier Transformations)\n            * `hop_length`: The step size when shifting to a new frame\n    \"\"\"\n    \n    def __init__(self, duration, sample_rate=22050, num_segments=5, samples_per_label=-1):\n        \"\"\"Initializes the Audio Feature Extractor.\n        \n        Note:\n            Keeping `samples_per_label` initialized to `-1` will result in processing every\n            available track in the dataset path.\n        \"\"\"\n        # Initialize the passed parameters\n        self.duration = duration\n        self.sample_rate = sample_rate\n        self.num_segments = num_segments\n        self.samples_per_label = samples_per_label\n        \n        # Calculate dependent attributes\n        self.samples_per_track = duration * sample_rate\n        self.samples_per_segment = self.samples_per_track // num_segments\n        \n        # Define default constants\n        self._DEFAULT_CONSTANTS = {\n            \"n_mfcc\": 13,\n            \"n_fft\": 2048,\n            \"hop_length\": 512,\n        }\n    \n    \n    def _validate_requests(self, requests):\n        \"\"\"Validates the parameters for feature extraction request.\n        \n        This method checks if the requested feature extraction is available, parses the parameters\n        (or uses the defaut ones) that should be applied to each extraction and returns a map with\n        keys as feature extraction functions and values as parameters for them.\n        \n        Args:\n            requests (list): The list of strings/tuples corresponding to the desired extraction\n        \n        Returns:\n            valid_requests (dict): The dictionary which maps extraction function to its parameters\n        \n        Raises:\n            AttributeError: If no extraction is specified or if it is not supported\n        \"\"\"\n        # Map available extractions to functions \n        mapping = {\n            \"mfcc\": self._extract_mfcc,\n        }\n        \n        # Define requests\n        valid_requests = {}\n        requests = [(req, {}) if isinstance(req, str) else req for req in requests]\n        \n        if len(requests) == 0:\n            # If no request is specified, raise an error\n            raise AttributeError(\"At least 1 feature extraction request must be specified\")\n            \n        # Loop through every request\n        for request_name, params in requests:\n            if request_name not in mapping.keys():\n                # If request name is not valid, raise an error\n                raise AttributeError(f\"'{request_name}' is not a valid feature extraction request\")\n            \n            # Define items for `valid_requests`\n            func = mapping[request_name]\n            args = dict(self._DEFAULT_CONSTANTS)\n            args.update(params)\n            \n            # Add valid request to dictionary\n            valid_requests[func] = args\n        \n        return valid_requests\n    \n    \n    def _extract_mfcc(self, signal, params):\n        \"\"\"Extracts MFCC vectors for the given signal.\n\n        This method uses ``librosa`` library to extract Mel-frequency cepstral coefficients for the\n        given waveform. It also ensures that the number of vectors is of the expected length, i.e.,\n        it must be equal to `samples_per_segment` divided by the `hop_length`.\n        \n        Args:\n            signal (ndarray): The waveform of the signal to preprocess\n            params (dict): Parameters to use for MFCC feature extraction\n            \n        Returns:\n            mfcc (list): extracted MFCC vectors\n        \"\"\"\n        # Extract MFCCs using librosa\n        mfcc = librosa.feature.mfcc(signal,\n                                    sr=self.sample_rate,\n                                    n_mfcc=params[\"n_mfcc\"],\n                                    n_fft=params[\"n_fft\"],\n                                    hop_length=params[\"hop_length\"])       \n        \n        # The expected number of generated mfcc vectors (required to ensure consistency)\n        expected_n_vectors = math.ceil(self.samples_per_segment / params[\"hop_length\"])\n        \n        if mfcc.shape[1] != expected_n_vectors:\n            # Repeat the last col if it doesn't exist (may occur occasionally)\n            mfcc = np.resize(mfcc, (params[\"n_mfcc\"], expected_n_vectors))\n        \n        return mfcc.T.tolist() # more comforatable representation and parsable by JSON\n    \n    \n    def _segmented_extraction(self, filepath, extractions):\n        \"\"\"Performs feature extraction for every track segment.\n        \n        Args:\n            filepath (str): The path to audio file\n            extractions (dict): The dictionary which maps extraction function to its parameters\n            \n        Returns:\n            feature_segments (list): The list of preprocessed track segments with features\n        \"\"\"\n        # Load the sound file and initialize segment list\n        signal, sr = librosa.load(filepath, sr=self.sample_rate)\n        feature_segments = []\n        \n        # Loop through the specified number of segments\n        for segment in range(self.num_segments):\n            # Take a segment from signal to preprocess it\n            start_sample = self.samples_per_segment * segment\n            end_sample = start_sample + self.samples_per_segment\n            signal_segment = signal[start_sample:end_sample]\n            features_group = []\n            \n            # Loop through every extraction request\n            for func, params in extractions.items():\n                # Generate features for the given segment and append to the group\n                features_group.append(func(signal_segment, params))\n            \n            # Append the group of features to the segment\n            feature_segments.append([features_group])\n            \n        return feature_segments\n    \n    \n    def _walk_extract(self, dataset_path, extractions):\n        \"\"\"Traverses directories and performs segmented extraction for each audio file.\n        \n        This method walks through labeled directories and for each audio file (or a folder of stems)\n        it performs segmented feature extraction. The number of segments and the extractions to be\n        performed are known in prior.\n        \n        Note:\n            The file structure is important. For example, if \"path/to/labels\" is passed as an argument\n            for `dataset_path`, then `labels` directory should contain labeled subdirectories for each\n            song (or a folder of stems).\n        \n        Args:\n            dataset_path (str): The path to a dataset directory\n            extractions (dict): The dictionary which maps extraction function to its parameters\n            \n        Returns:\n            data (dict): The dictionary containg labels and inputs. The entires are as follows:\n                * `semantic_label`: The name for each label\n                * `targets`: Numeric representation of semantic labels\n                * `features`: The list of extracted features of the shape (L, N, M, S, F, *F.shape)\n                    * `L`: The number of labels\n                    * `N`: The number of tracks\n                    * `M`: The number of segments\n                    * `S`: The number of stems\n                    * `F`: The number of requested feature extractions\n                    * `F.shape`: The shape of the feature tensor (may not exist if it's a single value)\n        \"\"\"\n        # Define the data dictionary containg labels and inputs\n        data = {\"semantic_labels\": [], \"targets\": [], \"features\": []}\n        \n        # Loop through every labeled directory\n        for i, f in enumerate(os.scandir(dataset_path)):\n            print(f\"Processing target {i} ({f.name})\")\n            \n            # Save semantic and target labels\n            data[\"semantic_labels\"].append(f.name)\n            data[\"targets\"].append(i)\n            features = []\n            \n            # Sort the file paths in case the songs are sequence-dependent data per label\n            samples = [sample for sample in os.scandir(f.path)]\n            samples.sort(key=lambda x: x.name)\n            \n            # Loop through the generated paths (up till `samples_per_label`)\n            for sample in tqdm(samples[:self.samples_per_label]):\n                if sample.is_file():\n                    # If it is a file, generate segmented features for a single stem\n                    feature_segments = self._segmented_extraction(sample.path, extractions)\n                \n                if sample.is_dir():\n                    # If it is a folder with stems, generate segmented features for each stem\n                    feature_segments = [self._segmented_extraction(os.path.join(sample.path, stem),\n                                        extractions) for stem in sorted(os.listdir(sample.path))]\n                    feature_segments = np.concatenate(feature_segments, axis=1).tolist()\n                \n                # Append segment-based features\n                features.append(feature_segments)\n            \n            # Append song-based features\n            data[\"features\"].append(features)\n        \n        return data\n    \n    \n    def extract_features(self, dataset_path, *args):\n        \"\"\"Extracts requested features from a dataset.\n        \n        This method validates the requests for feature extractions and calls ``self.walk_extract()``\n        method to traverse through directories in `dataset_path` and perform segmented extractions.\n        \n        Args:\n            dataset_path (str): The path to the root directory of the labeled dataset files\n            *args: The requested feature extractions.\n                Each argument is a tuple taking 2 values - the name of the feature extraction and the\n                parameters to use for that extraction (e.g., \"hop_length\", \"n_fft\") or 1 value - just\n                the name. Extraction parameters that are not provided are set to default values.\n        \"\"\"\n        extractions = self._validate_requests(args)\n        return self._walk_extract(dataset_path, extractions)\n    \n    \n    def restructure(self, inputs, targets, meanings, label_level=2, feature_level=None, squeeze=True, tolist=False):\n        \"\"\"Restructures the axes of the data.\n\n        Consider the feature leveling:\n            - Level 0: genres\n            - Level 1: tracks\n            - Level 2: segments\n            - level 3: stems\n            - level 4: features\n            \n        This method performs 2 transformations on the inputs and labels based on the feature leveing:\n        \n            1. **Labeling** - by default, only genres are labeled; this transformation allows to\n               increase the labeling level, meaning that labels will be present for all the dimensions\n               up to that level. The feature dimensions belonging to that level will in turn be\n               concatinated and the number of samples will increase. For instance, if _label_ level is\n               `1`, every track will have a label and there will be as many samples as there are tracks.\n               \n            2. **Refeaturing** - by default, features are positioned at the last level; this transformation\n               allows to move the feature axis to a lower level (not lower than _label_ level), meaning\n               that a label at a certain level will correspond to all the features at that level (unless\n               _label_ level is set to `4`). For instance, if _feature_ level is `3` < _label_ level, then\n               each segemnt will have multiple sets of stems, each represented by a certain feature.\n        \n        Note:\n            * If `feature_level` is `None`, it is automatically set to `label_level`, meaning that for every\n              feature, the same set of labels will apply.\n            * The lowest _feature_ level is actually the _label_ level. E.g., if we have labels for segments,\n              the lowest level for features is F extractions each covering a set of segments.\n            * If there is only `1` feature, the samples are concatinated to not contain an empty dimension if\n              `squeeze` is set to true. This only applies to feature dimension.\n        \n        Args:\n            inputs (ndarray): The extracted features for each track\n            targets (ndarray): Numeric representation of semantic labels\n            meanings (ndarray): The name for each label\n            label_level (int): The level at which each input dimension should have a label (`2` by default)\n            feature_level (int): The level at which an input dimension is split to features (`None` by default)\n            squeeze (bool): Whether to squeeze the feature dimension if it is of length one (`True` by default)\n            tolist (bool): Whether to return the outputs as python list objects (`False` by default)\n            \n        Returns:\n            tuple: A tuple of 3 entries as described in ``self.walk_extract``:\n                * `inputs`: The restructured extracted features for each track\n                * `targets`: The restructured numeric representation of semantic labels\n                * `meanings`: The restructured list of names for each label\n        \"\"\"\n        if feature_level is None:\n            # Default feature level\n            feature_level = label_level\n        \n        # Feature level cannot be lower than label level\n        assert feature_level >= label_level \n        \n        # Apply labeling transformation\n        targets = np.resize(targets, inputs.shape[label_level::-1]).T.reshape(-1)\n        meanings = np.resize(meanings, inputs.shape[label_level::-1]).T.reshape(-1)\n        inputs = inputs.reshape(-1, *inputs.shape[label_level+1:])\n        \n        # Apply \"refeaturing\" transformation\n        inputs = np.moveaxis(inputs, 4 - label_level, feature_level - label_level)\n        \n        if inputs.shape[feature_level - label_level] == 1 and squeeze:\n            # If the feature dimension is unnecessary, remove it\n            inputs = inputs.squeeze(axis=feature_level-label_level)\n        \n        if tolist:\n            # Convert to list object\n            inputs = inputs.tolist()\n            targets = targets.tolist()\n            meanings = meanings.tolist()\n        \n        return inputs, targets, meanings\n        \n        \n    def save_data(self, data, json_path):\n        \"\"\"Saves the data as a JSON file.\n        \n        Note:\n            The `data` must be interpretable by JSON parser, i.e., a python list object. It should\n            contain 3 entries as described in ``sef.walk_extract``, of any shape.\n        \n        Args:\n            data (dict): The dictionary containg labels and inputs\n            json_path (str): The path to the JSON file. Must end with a file name, e.g., \"data.json\"\n        \"\"\"\n        # Create output dir if it doesn't exist\n        if os.path.exists(os.path.dirname(json_path)):\n            os.makedirs(os.path.dirname(json_path))\n        \n        # Save the data to a json file\n        with open(json_path, 'w') as fp:\n            json.dump(data, fp, indent=4)\n            \n        print(\"Data successfully saved as a JSON file.\")\n            \n    \n    def load_data(self, json_path):\n        \"\"\"Loads the data from a JSON file.\n        \n        Note:\n            The JSON file must contain 3 entries described in ``sef.walk_extract``, of the exact shape.\n        \n        Returns:\n            tuple: A tuple of 3 entries as described in ``self.walk_extract``:\n                * `inputs`: The extracted features for each track\n                * `targets`: Numeric representation of semantic labels\n                * `meanings`: The name for each label\n        \"\"\"\n        # Load the file into memory\n        with open(json_path, 'r') as fp:\n            data = json.load(fp)\n        \n        # Each entry should be a numpy array\n        inputs = np.array(data[\"features\"])\n        targets = np.array(data[\"targets\"])\n        meanings = np.array(data[\"semantic_labels\"])\n        \n        return inputs, targets, meanings\n\n    \n    def extract_save(self, dataset_path, json_path, *args):\n        \"\"\"Generic method to extract and save the features.\n        \n        Args:\n            dataset_path (str): The path to the root directory of the labeled dataset files\n            json_path (str): The path to the JSON file. Must end with a file name, e.g., \"data.json\"\n            *args: The requested feature extractions.\n                Each argument is a tuple taking 2 values - the name of the feature extraction and the\n                parameters to use for that extraction (e.g., \"hop_length\", \"n_fft\") or 1 value - just\n                the name. Extraction parameters that are not provided are set to default values.\n        \"\"\"\n        extracted_features = self.extract_features(dataset_path, *args)\n        self.save_data(extracted_features, json_path)\n\n        \n    def load_restructure(self, json_path, label_level=2, feature_level=None, squeeze=True, tolist=False):\n        \"\"\"Generic method to load and restructure the features.\n            \n        Args:\n            json_path (str): The path to the JSON file. Must end with a file name, e.g., \"data.json\"\n            label_level (int): The level at which each input dimension should have a label (`2` by default)\n            feature_level (int): The level at which an input dimension is split to features (`None` by default)\n            squeeze (bool): Whether to squeeze the feature dimension if it is of length one (`True` by default)\n            tolist (bool): Whether to return the outputs as python list objects (`False` by default)\n                \n        Returns:\n            tuple: A tuple of 3 entries as described in ``self.load_data``:\n        \"\"\"\n        extracted_features = self.load_data(json_path)\n        return self.restructure(*extracted_features, label_level, feature_level, squeeze, tolist)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-09-11T17:03:19.520048Z","iopub.execute_input":"2021-09-11T17:03:19.520574Z","iopub.status.idle":"2021-09-11T17:03:22.255699Z","shell.execute_reply.started":"2021-09-11T17:03:19.520441Z","shell.execute_reply":"2021-09-11T17:03:22.25432Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File path constants\nSONGS_DATA_PATH = \"../input/gtzan-stems/Data/genres_original\"\nSTEMS_DATA_PATH = \"../input/gtzan-stems/Data/genres_stems\"\nSONGS_JSON_PATH = \"data_songs.json\"\nSTEMS_JSON_PATH = \"data_stems.json\"\n\n# Our extractor object\nextractor = AudioFeatureExtractor(30, num_segments=10, samples_per_label=20)\n\n# Feature extraction for songs ~40 sec\nprint(\"Processing original songs\\n\")\nextractor.extract_save(SONGS_DATA_PATH, SONGS_JSON_PATH, \"mfcc\")\n\n# Feature extraction for stems ~20 min\nprint(\"\\nProcessing song stems\\n\")\nextractor.extract_save(STEMS_DATA_PATH, STEMS_JSON_PATH, \"mfcc\")\n\n# Targets and labels will be the same for each segment\ninputs_songs, targets, meanings = extractor.load_restructure(SONGS_JSON_PATH)\ninputs_stems, _, _ = extractor.load_restructure(STEMS_JSON_PATH)\n\n# Confirm the shapes are what we expect them to be\nprint(\"\\nSong features shape:\", inputs_songs.shape)\nprint(\"Stem features shape:\", inputs_stems.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T20:28:12.180271Z","iopub.execute_input":"2021-09-11T20:28:12.180708Z","iopub.status.idle":"2021-09-11T20:50:52.093227Z","shell.execute_reply.started":"2021-09-11T20:28:12.180661Z","shell.execute_reply":"2021-09-11T20:50:52.091813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow.keras as keras\nimport tensorflow as tf\nimport random\n\nSEED = 54321 # gobal seed\n\ndef reset_random_seeds():\n    \"\"\"Restes the random seeds for reproducable results.\"\"\"\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    tf.random.set_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    \ndef split_data(X, y, test_size=.2, validation_size=.2):\n    \"\"\"Splits the data into train, validation and test sets.\n    \n    Args:\n        X (ndarray): The input data\n        y (ndarray): The target labels\n        test_size (float): The proportion of the test set\n        validation_size (float): The proportion of the validation set\n    \n    Returns:\n        tuple: A tuple of train, validation and test inputs and labels\n    \"\"\"\n    # Channel should be moved to the last dimension\n    X = np.moveaxis(X, 1, 3)\n    \n    # Perform the splits\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=SEED)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n\ndef build_model(input_shape):\n    \"\"\"Creates a CNN model.\n    \n    Args:\n        input_shape (tuple): The shape of the input dimensions of teh form (H, W, C)\n    \n    Returns:\n        tuple: the created model and an optimizer (Adam) for the model\n    \"\"\"\n    # Create a model\n    model = keras.Sequential()\n    \n    # Add first conv layer\n    model.add(keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=input_shape))\n    model.add(keras.layers.MaxPool2D(3, 2, \"same\"))\n    model.add(keras.layers.BatchNormalization())\n    \n    # Add second conv layer\n    model.add(keras.layers.Conv2D(32, 3, activation=\"relu\"))\n    model.add(keras.layers.MaxPool2D(3, 2, \"same\"))\n    model.add(keras.layers.BatchNormalization())\n    \n    # Add third conv layer\n    model.add(keras.layers.Conv2D(32, 2, activation=\"relu\"))\n    model.add(keras.layers.MaxPool2D(2, 2, \"same\"))\n    model.add(keras.layers.BatchNormalization())\n    \n    # Flatten the output and add a dense layer\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64, activation=\"relu\"))\n    model.add(keras.layers.Dropout(.3))\n    \n    # Output layer\n    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n    \n    # Create an optimizer for the model\n    optimizer = keras.optimizers.Adam(learning_rate=2e-3)\n    \n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2021-09-11T21:09:46.184446Z","iopub.execute_input":"2021-09-11T21:09:46.184901Z","iopub.status.idle":"2021-09-11T21:09:46.21245Z","shell.execute_reply.started":"2021-09-11T21:09:46.184864Z","shell.execute_reply":"2021-09-11T21:09:46.210818Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assure deterministic results\nreset_random_seeds()\n\n# Create a merged representation of inputs\ninputs_merge = np.concatenate((inputs_songs, inputs_stems), axis=1)\n\n# Generate train, validation and test data\nX_train_songs, X_val_songs, X_test_songs, y_train_songs, y_val_songs, y_test_songs = split_data(inputs_songs, targets)\nX_train_stems, X_val_stems, X_test_stems, y_train_stems, y_val_stems, y_test_stems = split_data(inputs_stems, targets)\nX_train_merge, X_val_merge, X_test_merge, y_train_merge, y_val_merge, y_test_merge = split_data(inputs_merge, targets)\n\n# Build the models\nmodel_songs, opt_songs = build_model(X_train_songs.shape[1:])\nmodel_stems, opt_stems = build_model(X_train_stems.shape[1:])\nmodel_merge, opt_merge = build_model(X_train_merge.shape[1:])\n\n# Compile the models\nmodel_songs.compile(optimizer=opt_songs, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel_stems.compile(optimizer=opt_stems, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel_merge.compile(optimizer=opt_merge, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Train the models\nprint(\"Training the model using original tracks as inputs...\")\nmodel_songs.fit(X_train_songs, y_train_songs, validation_data=(X_val_songs, y_val_songs), batch_size=32, epochs=20)\nprint(\"\\nTraining the model using stems as inputs...\")\nmodel_stems.fit(X_train_stems, y_train_stems, validation_data=(X_val_stems, y_val_stems), batch_size=32, epochs=20)\nprint(\"\\nTraining the model using merged inputs...\")\nmodel_merge.fit(X_train_merge, y_train_merge, validation_data=(X_val_merge, y_val_merge), batch_size=32, epochs=20)\n\n# Get the accuracies\nprint(\"\\nThe final accuracies for original, stems, merged respectively:\")\ntest_error_songs, test_accuracy_songs = model_songs.evaluate(X_test_songs, y_test_songs)\ntest_error_stems, test_accuracy_stems = model_stems.evaluate(X_test_stems, y_test_stems)\ntest_error_merge, test_accuracy_merge = model_merge.evaluate(X_test_merge, y_test_merge)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T21:09:49.194887Z","iopub.execute_input":"2021-09-11T21:09:49.195287Z","iopub.status.idle":"2021-09-11T21:11:32.904736Z","shell.execute_reply.started":"2021-09-11T21:09:49.195252Z","shell.execute_reply":"2021-09-11T21:11:32.903399Z"},"trusted":true},"execution_count":null,"outputs":[]}]}