{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/janatahack-demand-forecasting-analytics-vidhya/train.csv')\ntest = pd.read_csv('/kaggle/input/janatahack-demand-forecasting-analytics-vidhya/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.distplot(train[\"units_sold\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"skewness = \", train['units_sold'].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = train.corr()\nplt.figure(figsize= (15,12))\nsns.heatmap(cor, annot = True,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_feature = ['total_price','base_price']\ncat_feature = ['is_featured_sku','is_display_sku']\nfig,ax = plt.subplots(1,2,figsize = (18,5))\ni = 221\nfor f in cont_feature:\n    plt.subplot(i)\n    sns.distplot(train[f])\n    i += 1\n# #     plt.subplot(i)\n# #     train[f].plot.bar()\n#     i += 1\n\n#train['total_price'].plot.bar()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(18,5))\ni = 121\nfor f in cat_feature:\n    plt.subplot(i)\n    train[f].value_counts().plot.bar(title = f)\n    #plt.hist(train[f])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# * **Bivariant analysis*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(18,5))\ni = 121\nfor f in cat_feature:\n    plt.subplot(i)\n    train.groupby(f)['units_sold'].mean().plot.bar()\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig,ax = plt.subplots(1,2,figsize=(18,5))\ni = 121\nfor f in cont_feature:\n    plt.subplot(i)\n    plt.scatter(train[f],train['units_sold'],label = f)\n    plt.xlabel(f)\n    plt.ylabel(\"units_sold\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#will see what is that highest outliered row\n\ntrain[train[\"units_sold\"] == train[\"units_sold\"].max()]\n#May be because of discount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['week']= pd.to_datetime(train['week'])\ntrain.groupby('week').sum()['units_sold'].plot(figsize = (20,8))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Units Sold\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature engineering****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = pd.DatetimeIndex(train['week']).year\ntrain.groupby('year').sum()['units_sold'].plot(figsize = (20,8))\ntest['year'] = pd.DatetimeIndex(test['week']).year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['month'] = pd.DatetimeIndex(train['week']).month\ntest['month'] = pd.DatetimeIndex(test['week']).month\ntrain.groupby('month').sum()['units_sold'].plot(figsize = (20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['year','month']).sum()['units_sold'].plot(figsize = (20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['discount'] = train['base_price'] - train['total_price']\ntest['discount'] = test['base_price'] - test['total_price']\nplt.scatter(train['discount'],train['units_sold'],label = 'discount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***NULL values*****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['total_price'].isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"total_price\"].fillna(train[train['sku_id']== 245338]['total_price'].mean(),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***OUTLIERS Treatment***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['units_sold_log'] = np.log(train['units_sold'])\ntrain['units_sold_log'].hist(bins=20) \n#test['units_sold_log'] = np.log(test['units_sold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['total_price_log'] = np.log(train['total_price'])\ntrain['total_price_log'].hist(bins=20) \ntest['total_price_log'] = np.log(test['total_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['base_price_log'] = np.log(train['base_price'])\ntrain['base_price_log'].hist(bins=20) \ntest['base_price_log'] = np.log(test['base_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time Series","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime    # To access datetime \nfrom pandas import Series\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/janatahack-demand-forecasting-analytics-vidhya/train.csv')\ntest = pd.read_csv('/kaggle/input/janatahack-demand-forecasting-analytics-vidhya/test.csv')\ntrain_original=train.copy() \ntest_original=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['week']= pd.to_datetime(train['week'])\ntest['week']= pd.to_datetime(test['week'])\ntrain_original['week']= pd.to_datetime(train_original['week'])\ntest_original['week']= pd.to_datetime(test_original['week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in (train, test, test_original, train_original):\n    i['year']=i['week'].dt.year \n    i['month']=i['week'].dt.month\n    i['week_number']=i['week'].dt.week\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.index = train['week'] # indexing the Datetime to get the time period on the x-axis. \ndf=train.drop('record_ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['units_sold'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Units Sold') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Units Sold\") \nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('year')['units_sold'].mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('month')['units_sold'].mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train.groupby(['year', 'month'])['units_sold'].mean() \ntemp.plot(figsize=(15,5), title= 'Units Sold (Monthwise)', fontsize=14)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Timestamp = pd.to_datetime(train[\"week\"]) \ntrain.index = train.Timestamp \n\nweekly = train.resample('W').mean() \n# # Converting to monthly mean \nmonthly = train.resample('M').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekly[\"units_sold\"].plot(figsize=(15,8), title= 'Weekly', fontsize=14, ) \nmonthly[\"units_sold\"].plot(figsize=(15,8), title= 'Monthly', fontsize=14) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"week\"].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train=train.loc['2011-01-08':'2013-09-03'] \nvalid=train.loc['2013-09-04':'2013-12-03']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(Train[\"units_sold\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train['units_sold'].plot(figsize=(15,8), title= 'units sold', fontsize=14, label='train') \nvalid['units_sold'].plot(figsize=(15,8), title= 'units sold', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"units sold\") \nplt.legend(loc='best') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Moving Average***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['units_sold'].rolling(10).mean().iloc[-1] # average of last 10 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['units_sold'], label='Train') \nplt.plot(valid['units_sold'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['units_sold'].rolling(20).mean().iloc[-1] # average of last 20 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['units_sold'], label='Train') \nplt.plot(valid['units_sold'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['units_sold'].rolling(50).mean().iloc[-1] # average of last 50 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['units_sold'], label='Train') \nplt.plot(valid['units_sold'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations') \nplt.legend(loc='best') \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import mean_squared_log_error as msle\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(valid.units_sold, y_hat_avg.moving_avg_forecast)) \nprint(\"rms: \",rms)\nrmsle = sqrt(msle(valid.units_sold, y_hat_avg.moving_avg_forecast)) \nprint(\"rmsle: \",rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \ny_hat_avg = valid.copy() \nfit2 = SimpleExpSmoothing(np.asarray(Train['units_sold'])).fit(smoothing_level=0.6,optimized=False) \ny_hat_avg['SES'] = fit2.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['units_sold'], label='Train') \nplt.plot(valid['units_sold'], label='Valid') \nplt.plot(y_hat_avg['SES'], label='SES') \nplt.legend(loc='best') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.units_sold, y_hat_avg.SES)) \nprint(rms)\nrmsle = sqrt(msle(valid.units_sold, y_hat_avg.SES)) \nprint(rmsle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***# Holtâ€™s Linear Trend Model***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm \nplt.figure(figsize=(16,8)) \nsm.tsa.seasonal_decompose(Train[\"units_sold\"],period = 3).plot() \nresult = sm.tsa.stattools.adfuller(train.units_sold) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_avg = valid.copy() \nfit1 = Holt(np.asarray(Train['units_sold'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1) \ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['units_sold'], label='Train') \nplt.plot(valid['units_sold'], label='Valid') \nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \nplt.legend(loc='best') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.units_sold, y_hat_avg.Holt_linear)) \nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}