{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overfitting in Neural Network Classification\nThis notebook provides a brief illustration of the problem of overfitting in neural network classification, showing that 'dense-er is not always better'. Specifically, we will be using the *Human Activity Recognition* dataset, composed of smartphone accelerometer readings of individuals performing one of six distinct activities.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom scipy import stats\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom tqdm import tqdm\n\nnp.random.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device: {}\".format(device))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\nInspecting the data reveals that 561 continuous-valued predictors are available, as well as an ID variable describing the particular individual performing the activity. This latter variable will be omitted, as we are more interested in generalised activity recognition. Conveniently, all of the remaining predictors have already been scaled to lie between $-1$ and $1$."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have around 7,300 training datapoints and 3,000 test datapoints. This represents a relatively small dataset, and should render most computation highly feasible."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training dataset size: {}'.format(train_data.shape))\nprint('test dataset size: {}'.format(test_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importantly, the classes are uniformly distributed throughout the data. This will facilitate training, and ensure our model performs equally across all inputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize = (20,6))\n\nsns.countplot(train_data['Activity'], ax=ax[0])\nsns.countplot(test_data['subject'], ax=ax[1])\nax[0].tick_params(axis='x', rotation=45)\nax[0].set_ylabel('Number of observations')\nax[0].set_title('Observations by activity')\nax[1].set_ylabel('Number of observations')\nax[1].set_title('Observations by subject')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\nBefore we can train a model, some elementary coding of the response variable is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n\nX_train = train_data.iloc[:,0:(train_data.shape[1]-2)].values\ny_train = le.fit_transform(train_data.iloc[:,train_data.shape[1]-1].values)\n\nX_test = test_data.iloc[:,0:(test_data.shape[1]-2)].values\ny_test = le.transform(test_data.iloc[:,test_data.shape[1]-1].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA reveals that the first 100 principal components explain 97% of the variance in the data. However, since the dataset is already relatively small, we will preserve all 561 features for modelling purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=X_train.shape[1])\n\npca.fit(X_train)\ncumulative_variance = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(X_train.shape[1])]\n\nfig, ax = plt.subplots(figsize=(12,6))\nax.plot(cumulative_variance)\nax.set_title('Cumulative proportion of variance explained by principal components')\nax.set_xlabel('Number of components')\nax.set_ylabel('Proportion of variance explained')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before proceeding, let's experiment with a $t$-SNE embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE()\n\nX_reduced = tsne.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_data = pd.DataFrame(\n    {'X':X_reduced[:,0], 'Y':X_reduced[:,1], \n     'activity':train_data['Activity']})\n\nactivities = list(tsne_data['activity'].unique())\ncolormap = ['b', 'g', 'r', 'c', 'm', 'y']\n\nfig, ax = plt.subplots(figsize=(12,12))\nfor i in range(len(activities)):\n    plot_data = tsne_data.loc[tsne_data['activity'] == activities[i]]\n    ax.scatter('X', 'Y', data=plot_data, color=colormap[i], label=activities[i])\nax.set_title('2-dimensional $t$-SNE embedding of 561-dimensional accelerometer data')\nax.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This $t$-SNE transformation suggests that some points should be simple to classify, while others are almost indistinguishable from similar activities. We should not be surprised: sitting and standing are likely to generate almost identical accelerometer readings. Nevertheless, let's see how modelling fares."},{"metadata":{},"cell_type":"markdown","source":"## Neural Network Models\nWe now build a neural network clasification model, using varying architectures."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will help to write a simple ```Torch``` class that takes a list of layer sizes as an argument, and constructs a corresponding network with batch normalisation and dropout for each layer. We can also write a function to randomly generate architectures given a number of layers (or to generate an entirely random network)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuickModel(nn.Module):\n    def __init__(self, layer_sizes):\n        super().__init__()\n        \n        self.layer_sizes = layer_sizes\n        \n        structure = []\n        for i in range(len(layer_sizes)):\n            if i == len(layer_sizes) - 2:\n                structure.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n                break\n            else:\n                structure.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n                structure.append(nn.BatchNorm1d(layer_sizes[i+1]))\n                structure.append(nn.ReLU())\n                structure.append(nn.Dropout(0.5))\n                \n        self.layers = nn.ModuleList(structure)\n        \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n    \ndef generateRandomArchitecture(num_layers=None, max_layers=10):\n    np.random.seed(42)\n    if num_layers is None:\n        if max_layers <= 3:\n            max_layers = 4\n        num_layers = np.random.randint(low=3, high=max_layers)\n    layer_sizes = []\n    for i in range(num_layers):\n        if i == 0:\n            layer_sizes.append(561)\n        elif i == num_layers - 1:\n            layer_sizes.append(6)\n        else:\n            layer_max = min(int(layer_sizes[-1] * 1.5),2000)\n            layer_min = int(layer_sizes[-1] / 3)\n            if layer_min < 6:\n                layer_size = 6\n            else:\n                layer_size = np.random.randint(low=layer_min, high=layer_max)\n            layer_sizes.append(layer_size)\n    return layer_sizes, num_layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can instantiate a set of models to experiment with."},{"metadata":{"trusted":true},"cell_type":"code","source":"nets = []\n\nnets.append(QuickModel([561, 250, 6]).double())\nnets.append(QuickModel([561, 124, 32, 16, 6]).double())\nnets.append(QuickModel([561, 256, 256, 124, 64, 32, 16, 6]).double())\nnets.append(QuickModel(generateRandomArchitecture(num_layers=12)[0]).double())\nnets.append(QuickModel(generateRandomArchitecture(num_layers=15)[0]).double())\nnets.append(QuickModel(generateRandomArchitecture(num_layers=20)[0]).double())\n\nn_nets = len(nets)\nnets_details = [(len(net.layer_sizes), net.layer_sizes) for net in nets]\n\nfor ii, net in enumerate(nets_details):\n    print(\"Net {}: {} layers - {}\".format(ii+1, net[0], net[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having done so, we then run the training loop, updating the entire set of models concurrently."},{"metadata":{"trusted":true},"cell_type":"code","source":"for net in nets:\n    net.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizers = [optim.SGD(net.parameters(), lr=0.003, momentum=0.9) for net in nets]\n\nepochs = 800\ntrain_losses = [[] for net in nets]\ntest_losses = [[] for net in nets]\ntest_accuracies = [[] for net in nets]\nensemble_accuracies, ensemble_aucs = [], []\n\nbest = [{} for net in nets]\n\nfor e in tqdm(range(epochs)):\n    for net in nets:\n        net.train()\n    \n    for ii, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        for optimizer in optimizers:\n            optimizer.zero_grad()\n\n        logits = [net(inputs) for net in nets]\n        losses = [criterion(logit, labels) for logit in logits]\n        \n        for loss in losses:\n            loss.backward()\n        \n        for optimizer in optimizers:\n            optimizer.step()\n        \n        for i in range(n_nets):\n            train_losses[i].append(losses[i].item() / inputs.shape[0])\n    \n    test_loss = [0 for net in nets]\n    accuracy = [0 for net in nets]\n    \n    for net in nets:\n        net.eval()    \n    for ii, (inputs, labels) in enumerate(test_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        with torch.no_grad():\n            logits = [net(inputs) for net in nets]\n            losses = [criterion(logit, labels) for logit in logits]\n            \n            for i in range(n_nets):\n                test_loss[i] += losses[i].item() / inputs.shape[0]\n            \n            preds = [logit.argmax(dim=1) for logit in logits]\n            for i in range(n_nets):\n                correct = (preds[i] == labels).type(torch.FloatTensor)\n                accuracy[i] += torch.mean(correct).item()\n    \n    for i in range(n_nets):\n        if e == 0:\n            best[i]['epoch'] = e\n            best[i]['state_dict'] = nets[i].state_dict()\n            best[i]['accuracy'] = accuracy[i] / len(test_dataloader)\n        elif accuracy[i] / len(test_dataloader) > max(test_accuracies[i]):\n            best[i]['epoch'] = e\n            best[i]['state_dict'] = nets[i].state_dict()\n            best[i]['accuracy'] = accuracy[i] / len(test_dataloader)\n        \n        test_losses[i].append(test_loss[i] / len(test_dataloader))\n        test_accuracies[i].append(accuracy[i] / len(test_dataloader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results of the training algorithm are visualised below, after first smoothing the training losses into an average over 250-batch sequences."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses_averaged = [[] for net in nets]\ninterval = 250\nfor n in range(len(train_losses)):\n    for i in range(0,len(train_losses[n]),interval):\n        try:\n            train_losses_averaged[n].append(np.mean(train_losses[n][i:i+interval]))\n        except IndexError:\n            train_losses_averaged[n].append(np.mean(train_losses[n][i:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,12))\ngs = GridSpec(2, 2, figure=fig)\nax1 = plt.subplot(gs[0, :])\nax2 = plt.subplot(gs[1, 0])\nax3 = plt.subplot(gs[1, 1])\n\nfor i in range(n_nets):\n    ax1.plot(train_losses_averaged[i], \n             label=\"Net {}: {} layers\".format(i+1, nets_details[i][0]))\n    ax2.plot(test_losses[i], alpha=0.6)\n    ax3.plot(test_accuracies[i], alpha=0.6)\n\nax1.set_title('Training loss (mean per 250 batches)')\nax1.set_xlabel('Batch')\nax1.set_ylabel('Loss')\nax1.legend(loc='upper right')\n\nax2.set_title('Test loss (per epoch)')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\n\nax3.set_title('Test accuracy (per epoch)')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Accuracy')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the training loss curves, we see starkly different profiles for each of the models. The simplest model trained the fastest, as would be expected, while the most complex model had relatively little success in fitting the training data. It is also interesting to note that several models plateauded for hundreds of epochs before jumping up in accuracy again. Clearly one must be careful about terminating the training algorithm too early! Overall however, it was the simplest models that appear to have performed best. This is confirmed by checking the maximum test accuracy attained by each model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(n_nets):\n    print(\"Net {}: {} layers, maximum test accuracy of {:.4f}\".format(\n        i+1, nets_details[i][0], best[i]['accuracy']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is clear evidence that dense models are a poor choice for the activities classification problem. But what has gone wrong with these complex models, and why haven't our simple models been able to reach 100% accuracy?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(n_nets):\n    nets[i].load_state_dict(best[i]['state_dict'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What's going wrong?\nLet's investigate further the points that our models are misclassifying."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE()\n\nX_test_reduced = tsne.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_data = pd.DataFrame(\n    {'X':X_test_reduced[:,0], 'Y':X_test_reduced[:,1], \n     'activity':test_data['Activity'], 'label':y_test})\n\nactivities = list(tsne_data['activity'].unique())\ncolormap = ['b', 'g', 'r', 'c', 'm', 'y']\n\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30,30))\nax = ax.flatten()\n\nfor i in range(n_nets):\n    predictions = nets[i](\n        torch.from_numpy(X_test).to(device)\n    ).argmax(dim=1).cpu().numpy()\n    correct_predictions = predictions == y_test\n\n    for j in range(len(activities)):\n        data = tsne_data.loc[tsne_data['activity'] == activities[j]]\n        ax[i].scatter('X', 'Y', \n                      data=data, color=colormap[j], label=activities[j])\n    misclassified_points = tsne_data[np.logical_not(correct_predictions)]\n    ax[i].scatter('X', 'Y', \n                  data=misclassified_points, color='black', label='WRONG')\n    ax[i].set_title(\"Model {}: {} layers ({:.2f}% accuracy)\".format(\n        i+1, nets_details[i][0], best[i]['accuracy']*100))\n    ax[i].legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that all models struggle to differentiate between sitting and standing, and this may well be a fundamental limitation of the dataset. Note however that the simple models have nevertheless correctly identified many instances of sitting and standing, even where the two classes appear to overlay one another. It is likely that in the original 561-dimensional space, a simple hyperplane suffices for distinguishing the majority of sitting and standing observations, and that, thanks to its overly-complex decision boundary, the densest network was unable to settle upon this simple approximation.\n\nOtherwise, the models attain similar performance on the remaining activities. Laying should be easy for any competent network to recognise, and the general activity of 'walking' (upstairs, downstairs, or regular) is classified about as well. Further disparities arise in the delineation of different types of walking; once again, the simpler networks, with necessarily cruder approximations of the optimal decision boundary, perform better. It is interesting to note that the denser networks appear to have almost perfectly classified regular walking, yet have entirely failed to distinguish walking up and down stairs. My guess would be that this is similarly due to a messy, convoluted decision boundary that never achieves a perfect split, and so consistently remains terrible.\n\nA brief aside regarding ensemble models: it may seem in this case that an ensemble of the three highest-performing models could offer a boost in accuracy. This is however not the case, and looking at the above $t$-SNE embeddings, it is easy to see why. All three of the models misclassify the same type of datapoint, meaning that no increase in performance is possible by combining their outputs in a voting arrangement. If all of your ensemble members are always wrong in the same way, the average of their predictions will be just as wrong - one would be better off investing effort in training a single good classifier. This contrasts with the randomness that may arise in other contexts, where different members of an ensemble may be biased in different directions. \n\nThe moral of the story: denser is not always better."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}