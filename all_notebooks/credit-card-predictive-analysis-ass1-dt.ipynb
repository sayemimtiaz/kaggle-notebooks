{"cells":[{"metadata":{},"cell_type":"markdown","source":"source: https://www.kaggle.com/umerkk12/credit-card-predictive-analysis"},{"metadata":{},"cell_type":"markdown","source":"# Credit Card Prediction Analysis!"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"data:image/jpeg;base64,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\">\n\n# Context\n\nCredit score cards are a common risk control method in the financial industry. It uses personal information and data submitted by credit card applicants to predict the probability of future defaults and credit card borrowings. The bank is able to decide whether to issue a credit card to the applicant. Credit scores can objectively quantify the magnitude of risk.\n\nGenerally speaking, credit score cards are based on historical data. Once encountering large economic fluctuations. Past models may lose their original predictive power. Logistic model is a common method for credit scoring. Because Logistic is suitable for binary classification tasks and can calculate the coefficients of each feature. In order to facilitate understanding and operation, the score card will multiply the logistic regression coefficient by a certain value (such as 100) and round it.\n \nAt present, with the development of machine learning algorithms. More predictive methods such as Boosting, Random Forest, and Support Vector Machines have been introduced into credit card scoring. However, these methods often do not have good transparency. It may be difficult to provide customers and regulators with a reason for rejection or acceptance.\n \n\n\n# Task\nBuild a machine learning model to predict if an applicant is 'good' or 'bad' client, different from other tasks, the definition of 'good' or 'bad' is not given. You should use some techique, such as vintage analysis to construct you label.\n\n# Following main steps were used:\n* Fill missing values with mode\n* Find correlation between features\n* Oversample data set\n* Use scaling (StandardScaler)\n* Do ramdomizedSearchCV to select initialized parameters\n* Plot learning curves over multiple iterations\n* Plot validation curves over multiple iterations\n"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracting data using two data sources"},{"metadata":{"trusted":true},"cell_type":"code","source":"app = pd.read_csv(\"../input/credit-card-approval-prediction/application_record.csv\")\ncrecord = pd.read_csv(\"../input/credit-card-approval-prediction/credit_record.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using different methods to understand data\n* data is complex and both dataset need some kind of transformation before analysis\n* datasets are indivudally dealt with and then eventually compiled using joins"},{"metadata":{"trusted":true},"cell_type":"code","source":"app.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app['ID'].nunique() # the total rows are 438,557. This means it has duplicates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord['ID'].nunique() \n# this has around 43,000 unique rows as there are repeating entries for different monthly values and status.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(crecord['ID']).intersection(set(app['ID']))) # checking to see how many records match in two datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(app.isnull()) # checking for null values. Seems like occupation_type has many","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(crecord.isnull()) # checking for null values. All good here!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app = app.drop_duplicates('ID', keep='last') \n# we identified that there are some duplicates in this dataset\n# we will be deleting those duplicates and will keep the last entry of the ID if its repeated.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app.drop('OCCUPATION_TYPE', axis=1, inplace=True) \n#we identified earlier that occupation_type has many missing values\n# we will drop this column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ot = pd.DataFrame(app.dtypes =='object').reset_index()\nobject_type = ot[ot[0] == True]['index']\nobject_type\n#we are filtering the columns that have non numeric values to see if they are useful","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_type = pd.DataFrame(app.dtypes != 'object').reset_index().rename(columns =  {0:'yes/no'})\nnum_type = num_type[num_type['yes/no'] ==True]['index']\n#HAVE CREATED SEPARATE LIST FOR NUMERIC TYPE INCASE IT WILL BE NEEDED IN FURTHER ANALYSIS\n# IT IS NEEDED IN FURTHER ANALYSIS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = app[object_type]['CODE_GENDER'].value_counts()\nb = app[object_type]['FLAG_OWN_CAR'].value_counts()\nc = app[object_type]['FLAG_OWN_REALTY'].value_counts()\nd = app[object_type]['NAME_INCOME_TYPE'].value_counts()\ne = app[object_type]['NAME_EDUCATION_TYPE'].value_counts()\nf = app[object_type]['NAME_FAMILY_STATUS'].value_counts()\ng = app[object_type]['NAME_HOUSING_TYPE'].value_counts()\n\nprint( a,\"\\n\",b,'\\n', c, '\\n', d, '\\n', e, '\\n', f, '\\n', g)\n\n#this is just to see what each column is. \n#It seems that all of them are important since there is very fine classifcation in each column.\n# their effectiveness cannot be judged at this moment so we convert all of them to numeric values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n# oe = OneHotEncoder(handle_unknown='ignore')\n# for x in object_type:\n#     app[x] = oe.fit_transform(app[x])\n# we have transformed all the non numeric data columns into data columns\n\n#LabelEncoder:\nle = LabelEncoder()\nfor x in app:\n    if app[x].dtypes=='object':\n        app[x] = le.fit_transform(app[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app[num_type].head()\n# We will look at numeric columns and see if there is anything that needs to be changed. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax= plt.subplots(nrows= 3, ncols = 3, figsize= (14,6))\n\nsns.scatterplot(x='ID', y='CNT_CHILDREN', data=app, ax=ax[0][0], color= 'orange')\nsns.scatterplot(x='ID', y='AMT_INCOME_TOTAL', data=app, ax=ax[0][1], color='orange')\nsns.scatterplot(x='ID', y='CNT_FAM_MEMBERS', data=app, ax=ax[0][2], color= 'orange')\nsns.scatterplot(x='ID', y='DAYS_EMPLOYED', data=app, ax=ax[1][0])\nsns.scatterplot(x='ID', y='FLAG_MOBIL', data=app, ax=ax[1][1])\nsns.scatterplot(x='ID', y='FLAG_WORK_PHONE', data=app, ax=ax[1][2])\nsns.scatterplot(x='ID', y='FLAG_PHONE', data=app, ax=ax[2][0])\nsns.scatterplot(x='ID', y='FLAG_EMAIL', data=app, ax=ax[2][1])\nsns.scatterplot(x='ID', y='DAYS_BIRTH', data=app, ax=ax[2][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in 3 columns.\n1. CNT_CHILDREN\n2. AMT_INCOME_TOTAL\n3. CNT_FAM_MEMBERS"},{"metadata":{},"cell_type":"markdown","source":"* We need to remove these outliers to make sure they do not affect our model results. \n* We will now remove these outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# FOR CNT_CHILDREN COLUMN\nq_hi = app['CNT_CHILDREN'].quantile(0.999)\nq_low = app['CNT_CHILDREN'].quantile(0.001)\napp = app[(app['CNT_CHILDREN']>q_low) & (app['CNT_CHILDREN']<q_hi)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FOR AMT_INCOME_TOTAL COLUMN\nq_hi = app['AMT_INCOME_TOTAL'].quantile(0.999)\nq_low = app['AMT_INCOME_TOTAL'].quantile(0.001)\napp= app[(app['AMT_INCOME_TOTAL']>q_low) & (app['AMT_INCOME_TOTAL']<q_hi)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FOR CNT_FAM_MEMBERS COLUMN\nq_hi = app['CNT_FAM_MEMBERS'].quantile(0.999)\nq_low = app['CNT_FAM_MEMBERS'].quantile(0.001)\napp= app[(app['CNT_FAM_MEMBERS']>q_low) & (app['CNT_FAM_MEMBERS']<q_hi)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax= plt.subplots(nrows= 3, ncols = 3, figsize= (14,6))\n\nsns.scatterplot(x='ID', y='CNT_CHILDREN', data=app, ax=ax[0][0], color= 'orange')\nsns.scatterplot(x='ID', y='AMT_INCOME_TOTAL', data=app, ax=ax[0][1], color='orange')\nsns.scatterplot(x='ID', y='CNT_FAM_MEMBERS', data=app, ax=ax[0][2], color= 'orange')\n\nsns.scatterplot(x='ID', y='DAYS_EMPLOYED', data=app, ax=ax[1][0])\nsns.scatterplot(x='ID', y='FLAG_MOBIL', data=app, ax=ax[1][1])\nsns.scatterplot(x='ID', y='FLAG_WORK_PHONE', data=app, ax=ax[1][2])\nsns.scatterplot(x='ID', y='FLAG_PHONE', data=app, ax=ax[2][0])\nsns.scatterplot(x='ID', y='FLAG_EMAIL', data=app, ax=ax[2][1])\nsns.scatterplot(x='ID', y='DAYS_BIRTH', data=app, ax=ax[2][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord['Months from today'] = crecord['MONTHS_BALANCE']*-1\ncrecord = crecord.sort_values(['ID','Months from today'], ascending=True)\ncrecord.head(10)\n# we calculated months from today column to see how much old is the month\n# we also sort the data according to ID and Months from today columns. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord['STATUS'].value_counts() \n# performed a value count on status to see how many values exist of each type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord['STATUS'].replace({'C': 0, 'X' : 0}, inplace=True)\ncrecord['STATUS'] = crecord['STATUS'].astype('int')\ncrecord['STATUS'] = crecord['STATUS'].apply(lambda x:1 if x >= 2 else 0)\n# replace the value C and X with 0 as it is the same type\n# 1,2,3,4,5 are classified as 1 because they are the same type\n# these will be our labels/prediction results for our model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecord['STATUS'].value_counts(normalize=True) \n# there is a problem here\n# the data is oversampled for the labels\n# 0 are 99%\n# 1 are only 1% in the whole dataset\n# we will need to address the oversampling issue in order to make sense of our analysis\n# this will be done after when we combine both the datasets\n# so first we will join the datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crecordgb = crecord.groupby('ID').agg(max).reset_index()\ncrecordgb.head() \n#we are grouping the data in crecord by ID so that we can join it with app","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = app.join(crecordgb.set_index('ID'), on='ID', how='inner')\ndf.drop(['Months from today', 'MONTHS_BALANCE'], axis=1, inplace=True)\ndf.head()\n# no that this is joined, we will solve over sampling issue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"df.info() # checking for number of rows. \n# there are 9516 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # checking for number of rows. \n# there are 9516 rows.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# find correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,1:-1] # X value contains all the variables except labels\ny = df.iloc[:,-1] # these are the labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n# we create the test train split first","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_scaled = pd.DataFrame(ss.fit_transform(X_train), columns=X_train.columns)\nX_test_scaled = pd.DataFrame(ss.transform(X_test), columns=X_test.columns)\n# we have now fit and transform the data into a scaler for accurate reading and results.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled.std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled.std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_balanced, y_balanced = oversample.fit_resample(X_scaled, y_train)\nX_test_balanced, y_test_balanced = oversample.fit_resample(X_test_scaled, y_test)\n# we have addressed the issue of oversampling here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_balanced.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_balanced.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We notice in the value counts above that label types are now balanced\n* the problem of oversampling is solved now\n* we will now implement different models to see which one performs the best"},{"metadata":{},"cell_type":"markdown","source":"# Algorithms code start below\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import validation_curve\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n!pip install pydotplus\nimport pydotplus\nfrom IPython.display import Image\nfrom sklearn.model_selection import learning_curve \nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# classifiers = {\n#     \"LogisticRegression\" : LogisticRegression(),\n#     \"KNeighbors\" : KNeighborsClassifier(),\n#     \"SVC\" : SVC(),\n#     \"DecisionTree\" : DecisionTreeClassifier(),\n#     \"RandomForest\" : RandomForestClassifier(),\n#     \"XGBoost\" : XGBClassifier(),\n#     \"MLPClassifier\" : MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1, max_iter=10000)\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# train_scores = []\n# test_scores = []\n\n# for key, classifier in classifiers.items():\n#     classifier.fit(X_balanced, y_balanced)\n#     train_score = classifier.score(X_balanced, y_balanced)\n#     train_scores.append(train_score)\n#     test_score = classifier.score(X_test_balanced, y_test_balanced)\n#     test_scores.append(test_score)\n\n# print(train_scores)\n# print(test_scores)\n\n# [0.635392829900839, 0.984515636918383, 0.9175438596491228, 0.9954233409610984, 0.9954233409610984, 0.9950419527078566, 0.7806254767353166]\n# [0.5962633451957295, 0.8119217081850534, 0.8597864768683274, 0.8282918149466192, 0.8572953736654805, 0.9403914590747331, 0.697508896797153]\n# CPU times: user 27.2 s, sys: 256 ms, total: 27.5 s\n# Wall time: 18.7 s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision tree related assignment tasks"},{"metadata":{},"cell_type":"markdown","source":"doing random search first step\nsource: https://gist.github.com/otaviomguerra/51df7a4cff28f92de7105f12a0724115\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nparam_dist = {\"max_depth\": randint(1, 30),\n              \"max_features\": randint(1, 16),\n              \"min_samples_leaf\": randint(1, 16),\n              \"criterion\": [\"gini\"]}\ntree = DecisionTreeClassifier(random_state=0)\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=3)\ntree_cv.fit(X_balanced,y_balanced)\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building initial tree based on parameters from randomsearchcv"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprediction = tree_cv.predict(X_test_balanced)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source:https://www.geeksforgeeks.org/using-learning-curves-ml/\n# Learning curve: iteration 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator):\n    sizes, training_scores, testing_scores , fit_times, _= learning_curve(estimator, X_balanced, y_balanced, cv=3, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 100), return_times=True) \n    # Mean and Standard Deviation of training scores \n    mean_training = np.mean(training_scores, axis=1) \n    Standard_Deviation_training = np.std(training_scores, axis=1) \n\n    # Mean and Standard Deviation of testing scores \n    mean_testing = np.mean(testing_scores, axis=1) \n    Standard_Deviation_testing = np.std(testing_scores, axis=1) \n    \n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n    \n    _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n    # dotted blue line is for training scores and green line is for cross-validation score \n    axes[0].plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\") \n    axes[0].plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\") \n\n    # Drawing plot \n#     plt.title(\"LEARNING CURVE FOR MLP Classifier\") \n    axes[0].set_title(\"LEARNING CURVE FOR DT Classifier\")\n    axes[0].set_xlabel(\"Training Set Size\"), axes[0].set_ylabel(\"Accuracy Score\"), axes[0].legend(loc=\"best\") \n    \n    axes[1].grid()\n#     axes[1].plot(fit_times_mean, mean_testing, 'o-')\n#     axes[1].set_xlabel(\"fit_times\")\n#     axes[1].set_ylabel(\"Score\")\n\n    axes[1].plot(sizes, fit_times_mean, 'o-')\n    axes[1].set_xlabel(\"Training Set Size\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Performance of the model\")\n    \n    \n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport sklearn.tree as tree\nfig = plt.figure(figsize=(25,20), dpi=800)\n#source: https://datascience.stackexchange.com/questions/47852/visualizing-decision-tree-with-feature-names\n# tree.plot_tree(model) \n# fig.savefig(\"decistion_tree.png\")\n\nmodel = tree_cv.best_estimator_\n\ndt_feature_names = list(X_balanced.columns)\ndt_target_names = [str(s) for s in y_balanced.unique()]\ntree.export_graphviz(model, out_file='tree_Iteration1_after_randomizedsearchcv.png', \n    feature_names=dt_feature_names, class_names=dt_target_names,\n    filled=True)  \ngraph = pydotplus.graph_from_dot_file('tree_Iteration1_after_randomizedsearchcv.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(tree_cv.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying validation curve for depth parameter\nsource: https://datascience.stackexchange.com/questions/26918/validation-curve-unlike-sklearn-sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_validation_curve(param, param_range,estimator):\n#     param_range = np.arange(1, 41, 2)\n    train_scores, test_scores = validation_curve(estimator, X_balanced, y_balanced, param_name=param, cv=10, param_range=param_range,n_jobs=-1, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.title(\"Validation Curve with DecisionTree\")\n    plt.xlabel(param)\n    plt.ylabel(\"Score\")\n    plt.ylim(0.0, 1.1)\n    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n                 color=\"r\")\n    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n                 color=\"g\")\n\n    plt.legend(loc=\"best\")\n    param_range = np.arange(1, param_range.max(), 2)\n#     plt.xticks(param_range)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparam_range = np.arange(1, 41, 2)\nparam_name=\"max_depth\"\nplot_validation_curve(param_name,param_range,tree_cv.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Findings: it seems deepth more than 21 doesnt improve score significantly so next iteration i can try changing depth to 21."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntree_cv.best_params_['max_depth'] = 5\ndt_iter1=DecisionTreeClassifier(random_state=0)\ndt_iter1.set_params(**tree_cv.best_params_)\nmodel = dt_iter1.fit(X_balanced, y_balanced)\nprediction = dt_iter1.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntree_cv.best_params_['max_depth'] = 15\ndt_iter1=DecisionTreeClassifier(random_state=0)\ndt_iter1.set_params(**tree_cv.best_params_)\nmodel = dt_iter1.fit(X_balanced, y_balanced)\nprediction = dt_iter1.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntree_cv.best_params_['max_depth'] = 21\ndt_iter1=DecisionTreeClassifier(random_state=0)\ndt_iter1.set_params(**tree_cv.best_params_)\nmodel = dt_iter1.fit(X_balanced, y_balanced)\nprediction = dt_iter1.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy has improved as mentioned above"},{"metadata":{},"cell_type":"markdown","source":"# Iteration 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(dt_iter1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparam_range = np.arange(1, 16, 1)\nparam_name=\"max_features\"\nplot_validation_curve(param_name,param_range,dt_iter1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Findings: As learning curve shows its a good fit and validation curve shows for max_features that its improves at 10 and after that it decreases. so i will choose the min value here which is 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter1.get_params()\nparams['max_features']=4\ndt_iter2=DecisionTreeClassifier(random_state=0)\ndt_iter2.set_params(**params)\nmodel = dt_iter2.fit(X_balanced, y_balanced)\nprediction = dt_iter2.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter1.get_params()\nparams['max_features']=10\ndt_iter2=DecisionTreeClassifier(random_state=0)\ndt_iter2.set_params(**params)\nmodel = dt_iter2.fit(X_balanced, y_balanced)\nprediction = dt_iter2.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter1.get_params()\nparams['max_features']=14\ndt_iter2=DecisionTreeClassifier(random_state=0)\ndt_iter2.set_params(**params)\nmodel = dt_iter2.fit(X_balanced, y_balanced)\nprediction = dt_iter2.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Iteration 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(dt_iter2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparam_range = np.arange(1, 40, 1)\nparam_name=\"min_samples_leaf\"\nplot_validation_curve(param_name,param_range,dt_iter2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Findings: As learning curve shows its a good fit and validation curve shows for min_samples_leaf that score decreases as min_samples_leaf increases. so i will choose min_samples_leaf=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter2.get_params()\nparams['min_samples_leaf']=15\ndt_iter3=DecisionTreeClassifier(random_state=0)\ndt_iter3.set_params(**params)\nmodel = dt_iter3.fit(X_balanced, y_balanced)\nprediction = dt_iter3.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter2.get_params()\nparams['min_samples_leaf']=25\ndt_iter3=DecisionTreeClassifier(random_state=0)\ndt_iter3.set_params(**params)\nmodel = dt_iter3.fit(X_balanced, y_balanced)\nprediction = dt_iter3.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter2.get_params()\nparams['min_samples_leaf']=1\ndt_iter3=DecisionTreeClassifier(random_state=0)\ndt_iter3.set_params(**params)\nmodel = dt_iter3.fit(X_balanced, y_balanced)\nprediction = dt_iter3.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(dt_iter3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Iteration 4\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# plot_learning_curve(dt_iter3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparam_range = np.arange(0, 600, 50)\n\n# print(type(out1))\nparam_name=\"max_leaf_nodes\"\nplot_validation_curve(param_name,param_range,dt_iter3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"max_leaf_nodes with 200 reveals best as ideally its plaeatuing after 200"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter3.get_params()\nparams['max_leaf_nodes']=100\ndt_iter4=DecisionTreeClassifier(random_state=0)\ndt_iter4.set_params(**params)\nmodel = dt_iter4.fit(X_balanced, y_balanced)\nprediction = dt_iter4.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter3.get_params()\nparams['max_leaf_nodes']=300\ndt_iter4=DecisionTreeClassifier(random_state=0)\ndt_iter4.set_params(**params)\nmodel = dt_iter4.fit(X_balanced, y_balanced)\nprediction = dt_iter4.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter3.get_params()\nparams['max_leaf_nodes']=200\ndt_iter4=DecisionTreeClassifier(random_state=0)\ndt_iter4.set_params(**params)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = dt_iter4.fit(X_balanced, y_balanced)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprediction = dt_iter4.predict(X_test_balanced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(dt_iter4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Iter 5 Decision tree pruning\n\nsource: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# dt2 = DecisionTreeClassifier(random_state=0)\npath = dt_iter4.cost_complexity_pruning_path(X_balanced, y_balanced)\nmodel = dt_iter4.fit(X_balanced, y_balanced)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_balanced, y_balanced)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1)\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_scores = [clf.score(X_balanced, y_balanced) for clf in clfs]\ntest_scores = [clf.score(X_test_balanced, y_test_balanced) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: ccp_alpha=0.001125 seems to be maximizing the testing accuracy. Lets rerun with this setting."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams=dt_iter4.get_params()\nparams['ccp_alpha']=0.001125\ndt_iter5=DecisionTreeClassifier(random_state=0)\ndt_iter5.set_params(**params)\nmodel = dt_iter5.fit(X_balanced, y_balanced)\nprediction = dt_iter5.predict(X_test_balanced)\nprint(confusion_matrix(y_test_balanced, prediction))\nprint(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_iter5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_learning_curve(dt_iter5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid search for DT\nsource: https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\ngs = GridSearchCV(DecisionTreeClassifier(random_state=0),\n                  param_grid={'min_samples_split': range(2, 403, 10)},\n                  scoring=scoring, refit='AUC', return_train_score=True)\ngs.fit(X_balanced, y_balanced)\nresults = gs.cv_results_   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 13))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n          fontsize=16)\n\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Score\")\n\nax = plt.gca()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_min_samples_split'].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), ['g', 'k']):\n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = results['mean_test_%s' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another grid serach attempt\nsource: https://medium.com/analytics-vidhya/decisiontree-classifier-working-on-moons-dataset-using-gridsearchcv-to-find-best-hyperparameters-ede24a06b489\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# #params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n# params = { \"criterion\" :['gini'],\"max_depth\": range(1,41),\"max_features\":range(1,16),\"min_samples_leaf\":range(1,40),\"max_leaf_nodes\":range(1,40)}\n# grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=0),params ,n_jobs=-1,  verbose=0, cv=3)\n# grid_search_cv.fit(X_balanced, y_balanced)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Below code takes a long time to evaluate in hours!\n# %%time\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import GridSearchCV\n# #params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4], }\n# params = { \"criterion\" :['gini'],\"max_depth\": range(1,27),\"min_samples_split\":range(1,10),\"min_samples_leaf\":range(1,5), 'ccp_alpha': np.arange(0, 1, 0.001).tolist()}\n# grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=0), params, verbose=0, cv=3,n_jobs=-1)\n# grid_search_cv.fit(X_balanced, y_balanced)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_search_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # dt4 = DecisionTreeClassifier(**grid_search_cv.best_estimator_)\n# # model = dt4.fit(X_balanced, y_balanced)\n# model = grid_search_cv.best_estimator_.fit(X_balanced, y_balanced)\n# prediction = grid_search_cv.best_estimator_.predict(X_test_balanced)\n\n# print(confusion_matrix(y_test_balanced, prediction))\n# print(classification_report(y_test_balanced, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_representation = tree.export_text(dt3)\n# print(text_representation)\n# #source: https://mljar.com/blog/visualize-decision-tree/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}