{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Summary**\n\nEvery year, more than 140 million bookings made on the internet and many hotel bookings made through top-visited travel websites like Booking.com, Expedia.com, Hotels.com, etc. According to Google data, hotels are booked in advance of 12 weeks. \n\nThis dataset contains 31 features about booking information such as Average Daily Rate, Arrival Time, Room Type, Special Request, etc. between 2015 and 2017 years.\n\nIn this kernel, I would like to show some booking information details with exploratory data analysis, some feature engineering, reviewing correlations between features, hyperparameter tunning and visualizing most important features and their interesting distribution properties. \nAs a result of all these analyses, I aim to find best model to predict hotel booking cancellations with tree-based algorithms based on rest of the features found in the dataset. \nThe goal of predictive analysis is to avoid overfitting and find the model that has the highest accuracy. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **INTRODUCTION**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Data\n\nhotel_data = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show first 10 rows\n\nhotel_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Data summary\n\nhotel_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis and Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part, I would like to visualize some features and show statistical relationship with target variable. This analysis will help to get overall view and deep familiarity of the data, detect extreme values and identify obvious errors. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First graph is about exploring `hotel` feature which denotes type of the hotels. According to the below graph, approximately 34% of the data was booked for resort hotel and the rest of was booked for City Hotel. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hotel types details\n\nplt.figure(figsize=(10,10))\nsns.countplot(x='hotel', data = hotel_data, palette='gist_earth')\nplt.title('Hotel Types', weight='bold')\nplt.xlabel('Hotel', fontsize=12)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's dive into the target value of data. The numbers are similar with hotel features. While 37% of booking canceled, 63% of booking is not canceled. These numbers also show that there is no balanced problem on the target value.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `is_canceled` graph\n\nplt.figure(figsize=(10,10))\nsns.countplot(y='is_canceled', data= hotel_data, palette='gist_stern', orient = 'v')\nplt.title('Canceled Situation', weight='bold')\nplt.xlabel('Count', fontsize=12)\nplt.ylabel('Canceled or Not Canceled', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below graph shows the relationship of `arrival_date_year` to `lead_time` with booking cancellation status. The graph created by violin plot. Violin plot is a hybrid of box plot and density plot. It shows the distribution of the data. \n\nThree violin plots are correponding to three different years. For canceled booking, means and interquartile ranges are similar in all years. But the shapes of the distributions are quite different from each other. On the other hand distribution of not-canceled booking are almost the same. \nFor all years and every booking situation, the small number of large lead time values are pulling the mean up. It shows that the mean is higher than the median. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `arrival_date_year` vs `lead_time` vs `is_canceled` exploration with violin plot\n\nplt.figure(figsize=(10,10))\nsns.violinplot(x='arrival_date_year', y ='lead_time', hue=\"is_canceled\", data=hotel_data, palette=\"Set3\", bw=.2,\n               cut=2, linewidth=2, iner= 'box', split = True)\nsns.despine(left=True)\nplt.title('Arrival Year vs Lead Time vs Canceled Situation', weight='bold')\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Lead Time', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another exploration is made for the `arrival_date_month` feature. First month names converted to the numbers. It will help easier analysis. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#`arrival_date_month` names converted to the numbers\n\nhotel_data['arrival_date_month'].replace({'January' : '1',\n        'February' : '2',\n        'March' : '3',\n        'April' : '4',\n        'May' : '5',\n        'June' : '6',\n        'July' : '7',\n        'August' : '8',\n        'September' : '9', \n        'October' : '10',\n        'November' : '11',\n        'December' : '12'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#`arrival_date_month` exploration \n\nplt.figure(figsize=(15,10))\nsns.countplot(x='arrival_date_month', data = hotel_data,\n              order=pd.value_counts(hotel_data['arrival_date_month']).index, palette='YlOrBr_r')\nplt.title('Arrival Month', weight='bold')\nplt.xlabel('Month', fontsize=12)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph is showing the number of bookings for each month. According to that, August is the busiest month and January is the most unoccupied month. It is half as busy as August.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another important features which are related to time are `stays_in_week_nights` and `stays_in_weekend_night` features. The below table shows the relationship between these two features. According to that, there is some missing data. 715 values are inputted zero both weekend and weeknights. However, this missing data is small enough to neglect. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Table of `stay_in_weekend` and `stay_in_week_nights` features\n\npd.crosstab(index = hotel_data['stays_in_week_nights'],columns=hotel_data['stays_in_weekend_nights'], margins=True, margins_name = 'Total').iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above table brings an idea about creating a new feature. Which is indicated `just_stay_weekend`, `just_stay_weekday` and `stay_both_weekday_and_weekday`. These 715 values which are not assigned any feature, indicated as undefined_data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating new feature: `Weekday vs Weekend` \n\npd.options.mode.chained_assignment = None\ndef week_function(feature1, feature2, data_source):\n    data_source['weekend_or_weekday'] = 0\n    for i in range(0, len(data_source)):\n        if feature2.iloc[i] == 0 and feature1.iloc[i] > 0:\n            hotel_data['weekend_or_weekday'].iloc[i] = 'stay_just_weekend'\n        if feature2.iloc[i] > 0 and feature1.iloc[i] == 0:\n            hotel_data['weekend_or_weekday'].iloc[i] = 'stay_just_weekday'\n        if feature2.iloc[i] > 0 and feature1.iloc[i] > 0:\n            hotel_data['weekend_or_weekday'].iloc[i] = 'stay_both_weekday_and_weekend'\n        if feature2.iloc[i] == 0 and feature1.iloc[i] == 0:\n            hotel_data['weekend_or_weekday'].iloc[i] = 'undefined_data'\n\n            \nweek_function(hotel_data['stays_in_weekend_nights'],hotel_data['stays_in_week_nights'], hotel_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next graph is about `weekend_or_weekday` feature's relationship with `arrival_date_month`. Below bar graph shows that most bookings were made to stay only for weekdays or both weekdays and weekends. On the other, numbers of staying just the weekend category are quite low compared to other categories. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#`arrival_date_month` vs `weekend_or_weekday` graph \n\nhotel_data['arrival_date_month']= hotel_data['arrival_date_month'].astype('int64')\ngroup_data = hotel_data.groupby([ 'arrival_date_month','weekend_or_weekday']).size().unstack(fill_value=0)\ngroup_data.sort_values('arrival_date_month', ascending = True).plot(kind='bar',stacked=True, cmap='Set2',figsize=(15,10))\nplt.title('Arrival Month vs Staying Weekend or Weekday', weight='bold')\nplt.xlabel('Arrival Month', fontsize=12)\nplt.xticks(rotation=360)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another feature engineering is made for `children` and `babies` features. Since, there is no obvious difference, these features gathered under the one feature which name is `all_children`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature:`all_children` with merge children and baby features\n\nhotel_data['all_children'] = hotel_data['children'] + hotel_data['babies']\npd.crosstab(hotel_data['adults'], hotel_data['all_children'], margins=True, margins_name = 'Total').iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below the donut pie graph shows the meal categories. There is a big difference in the `Bed&Breakfast` category and the others. Almost 80% of bookings reserved for `Bed&Breakfast`. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `Meal` feature donut chart\n\nmeal_labels= ['BB','HB', 'SC', 'Undefined', 'FB']\nsize = hotel_data['meal'].value_counts()\nplt.figure(figsize=(10,10))\ncmap =plt.get_cmap(\"Pastel2\")\ncolors = cmap(np.arange(3)*4)\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(size, labels=meal_labels, colors=colors, wedgeprops = { 'linewidth' : 5, 'edgecolor' : 'white' })\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('Meal Types', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below table shows frequency details about meal types according to the hotel types. Following the results, 67% of `Bed&Breakfast` booking made for `City Hotel` and almost every `Full Board` bookings made in the `Resort Hotel`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby `Meal` and `Hotel` features\n\ngroup_meal_data = hotel_data.groupby(['hotel','meal']).size().unstack(fill_value=0).transform(lambda x: x/x.sum())\ngroup_meal_data.applymap('{:.2f}'.format)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below graph gives information about the location which bookings made in. According to that, there is an apparent difference in booking location between Portugal and the others. Approx. 40% of all bookings made in the same location: Portugal. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Top 10 Country of Origin graph\n\nplt.figure(figsize=(10,10))\nsns.countplot(x='country', data=hotel_data, \n              order=pd.value_counts(hotel_data['country']).iloc[:10].index, palette=\"brg\")\nplt.title('Top 10 Country of Origin', weight='bold')\nplt.xlabel('Country', fontsize=12)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The statistics show that online hotel and airline reservations are increased in recent years. Most people complete their reservation via their smartphones. The below graphs is summarise these statistics. More than 45% of bookings are made via `Online Travel Agents` and around 20% of bookings made via `Offline Travel Agents`. Less than 20% of bookings made directly without any agents. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `Market_segment` feature exploration\n\nplt.figure(figsize=(10,10))\nsns.countplot(hotel_data['market_segment'], palette='spring_r', \n              order=pd.value_counts(hotel_data['market_segment']).index)\nplt.title('Market Segment Types', weight='bold')\nplt.xlabel('Market Segment', fontsize=12)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below crosstable shows if there is any difference between assigned and reserved room types or not. The results are shown as a percentage. Average 84% of bookings keep their reserved room and the rest of the' rooms have been changed. Every row represents the reserved type and distribution over the columns shows what was the assigned room despite of reserved type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reserved vs Assigned room types table\n\npd.crosstab(index = hotel_data['reserved_room_type'], \n            columns = hotel_data['assigned_room_type'],normalize='index').round(2)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another exploratory analysis made for diving deep into the relationship between ADR and arrival month and booking cancellation status. As explained in the previous graph of `arrival month`, August is the most intense month of bookings. Besides the highest `Arrival Daily Rate` has ben occurred in that month too. Except for rush months like August, July, and September, canceled bookings have higher `ADR` than not canceled bookings. Maybe this highest rate could be one of the reasons for canceled bookings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `Arrival Month` vs `ADR` vs `Booking Cancellation Status`\n\nhotel_data['adr'] = hotel_data['adr'].astype(float)\nplt.figure(figsize=(15,10))\nsns.barplot(x='arrival_date_month', y='adr', hue='is_canceled', dodge=True, palette= 'PuBu_r', data=hotel_data)\nplt.title('Arrival Month vs ADR vs Booking Cancellation Status', weight='bold')\nplt.xlabel('Arrival Month', fontsize=12)\nplt.ylabel('ADR', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below graph is about `Total Special Request` numbers. Around 55% of bookings do not have any special requests. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `total_of_special_requests` graph\n\nplt.figure(figsize=(10,10))\nsns.countplot(x='total_of_special_requests', data=hotel_data, palette = 'ocean_r')\nplt.title('Total Special Request', weight='bold')\nplt.xlabel('Number of Special Request', fontsize=12)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last graph is about the relationship between special requests and cancellation booking status. Nearly half bookings without any special requests have been canceled and another half of them have not been canceled. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by `total_of_special_requests` and `is_canceled` features\n\ngroup_adr_request = hotel_data.groupby([ 'total_of_special_requests', 'is_canceled']).size().unstack(fill_value=0)\ngroup_adr_request.plot(kind='bar', stacked=True, cmap='vlag', figsize=(10,10))\nplt.title('Total Special Request vs Booking Cancellation Status', weight='bold')\nplt.xlabel('Number of Special Request', fontsize=12)\nplt.xticks(rotation=360)\nplt.ylabel('Count', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dealing with Missing Data and Correlation Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part, first of all, any missing data will be checked.\nThe below code shows missing data for every feature. Such that, the `company` feature's 94% is missing. Because of that, this feature will be eliminated. On the other hand, the `children` and `all_children` features have only 4 missing data. This missing data will replace with zero. Another missing data has occurred in `country` and `agent` features. \nSince missing data of `country` is less than 1%, these data will replace with most frequent value. However, the `agent` missing features are more than the country. For this feature, missing data will be imputed as `0`. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Display sum of null data\n\nhotel_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing data\n\nhotel_data['children'] =  hotel_data['children'].fillna(0)\nhotel_data['all_children'] =  hotel_data['all_children'].fillna(0)\nhotel_data['country'] = hotel_data['country'].fillna(hotel_data['country'].mode().index[0])\nhotel_data['agent']= hotel_data['agent'].fillna('0')\nhotel_data=hotel_data.drop(['company'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change data structure\n\nhotel_data['agent']= hotel_data['agent'].astype(int)\nhotel_data['country']= hotel_data['country'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another part is analyzing categorical features. Categorical labels converted into numerical form. This will help to be more understandable and implementable into machine learning algorithms. Some features are not ordinal such as `country`. In that case, *One-Hot Encoding* could be chosen. Due to the high number of categories, this method could incur higher computational cost. To help reducing that, Label Encoding method will be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Label Encoder method for categorical features\n\nlabelencoder = LabelEncoder()\nhotel_data['hotel'] = labelencoder.fit_transform(hotel_data['hotel'])\nhotel_data['arrival_date_month'] = labelencoder.fit_transform(hotel_data['arrival_date_month'])\nhotel_data['meal'] = labelencoder.fit_transform(hotel_data['meal'])\nhotel_data['country'] = labelencoder.fit_transform(hotel_data['country'])\nhotel_data['market_segment']= labelencoder.fit_transform(hotel_data['market_segment'])\nhotel_data['distribution_channel']=labelencoder.fit_transform(hotel_data['distribution_channel'])\nhotel_data['is_repeated_guest'] = labelencoder.fit_transform(hotel_data['is_repeated_guest'])\nhotel_data['reserved_room_type'] = labelencoder.fit_transform(hotel_data['reserved_room_type'])\nhotel_data['assigned_room_type'] = labelencoder.fit_transform(hotel_data['assigned_room_type'])\nhotel_data['deposit_type'] = labelencoder.fit_transform(hotel_data['deposit_type'])\nhotel_data['agent'] = labelencoder.fit_transform(hotel_data['agent'])\nhotel_data['customer_type'] = labelencoder.fit_transform(hotel_data['customer_type'])\nhotel_data['reservation_status'] = labelencoder.fit_transform(hotel_data['reservation_status'])\nhotel_data['weekend_or_weekday'] = labelencoder.fit_transform(hotel_data['weekend_or_weekday'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After encoding the categorical data, two data frames will be created. One data frame has only categorical data and another has numerical data. These two different data frames will be used to create a correlation matrix. *Spearman* method will be used for categorical data correlation matrix and *Pearson* method will be used for numerical one. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new dataframe for categorical data\n\nhotel_data_categorical = hotel_data[['hotel','is_canceled','arrival_date_month','meal',\n                                     'country','market_segment','distribution_channel', \n                                     'is_repeated_guest', 'reserved_room_type',\n                                     'assigned_room_type','deposit_type','agent',\n                                     'customer_type','reservation_status', \n                                     'weekend_or_weekday']]\nhotel_data_categorical.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new dataframe for numerical data\n\nhotel_data_numerical= hotel_data.drop(['hotel','is_canceled', 'arrival_date_month','meal',\n                                       'country','market_segment','distribution_channel', \n                                       'is_repeated_guest', 'reserved_room_type', \n                                       'assigned_room_type','deposit_type','agent', \n                                       'customer_type','reservation_status',\n                                       'weekend_or_weekday'], axis = 1)\nhotel_data_numerical.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Correlation Matrix with Spearman method\n\nplt.figure(figsize=(15,15))\ncorr_categorical=hotel_data_categorical.corr(method='spearman')\nmask_categorical = np.triu(np.ones_like(corr_categorical, dtype=np.bool))\nsns.heatmap(corr_categorical, annot=True, fmt=\".2f\", cmap='BrBG', vmin=-1, vmax=1, center= 0,\n            square=True, linewidths=2, cbar_kws={\"shrink\": .5}).set(ylim=(15, 0))\nplt.title(\"Correlation Matrix Spearman Method- Categorical Data \",size=15, weight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix with pearson method\n\nplt.figure(figsize=(15,15))\ncorr_numerical=hotel_data_numerical.corr(method='pearson')\nmask_numerical = np.triu(np.ones_like(corr_numerical, dtype=np.bool))\nsns.heatmap(corr_numerical, annot=True, fmt=\".2f\", cmap='RdBu', mask= mask_numerical, vmin=-1, vmax=1, center= 0,\n            square=True, linewidths=2, cbar_kws={\"shrink\": .5}).set(ylim=(17, 0))\nplt.title(\"Correlation Matrix Pearson Method- Numerical Data \",size=15, weight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding high correlated features\n\ncorr_mask_categorical = corr_categorical.mask(mask_categorical)\ncorr_values_categorical = [c for c in corr_mask_categorical.columns if any (corr_mask_categorical[c] > 0.90)]\ncorr_mask_numerical = corr_numerical.mask(mask_numerical)\ncorr_values_numerical = [c for c in corr_mask_numerical.columns if any (corr_mask_numerical[c] > 0.90)]\nprint(corr_values_categorical, corr_values_numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above correlation matrix shows positive or negative relationships between them. In those two heatmaps, the `reservation_ status` feature is drawn more attention because of its negative correlation with the `is_canceled` feature. The below table shows the relationship with details. This high correlation can cause a wrong prediction or overfitting. Prevent this situation, `reservation_status` feature will be eliminated. \n\nOn the other hand, there is another high relationship between the `children` and the `all_children` features, since the `all_children` feature is constituted with the `children` and the `babies` features. Therefore the `children` feature will be eliminated too.\n\nLast feature is `reservation_status_date`. Since this feature includes date type data and it could not convert another type, this feature will be eliminated. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# `reservation_status` vs `is_canceled` table\n\npd.crosstab(columns = hotel_data['reservation_status'], index = hotel_data['is_canceled'],\n           margins=True, margins_name = 'Total')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping some features from data\n\nhotel_data = hotel_data.drop(['reservation_status', 'children', 'reservation_status_date'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copying data to used next parts\n\nhotel_data_model = hotel_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Hyperparameter Tunning and Feature Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part, optimum hyperparameters for several tree-based machine learning algorithms will be searched with the help of the `Grid Search Algorithm`. Hyperparameter tuning will help to make a prediction in the training part more accurately. Therefore, hyperparameters tunning will be fixed before the training process. \n\nAnother important work is constituted Permutation Feature Importance graph with the `Extreme Gradient Boosting` algorithm. This technique calculates feature importance and performance metric be chosen as the basis of the accuracy score. This graph will help to understand features' contributed to prediction, provide insight into the dataset, and will help to find deemed non-important features if any. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate target variable\n\nhotel_data_tunning = hotel_data\ny = hotel_data_tunning.iloc[:,1]\nX = pd.concat([hotel_data_tunning.iloc[:,0],hotel_data_tunning.iloc[:,2:30]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding parameters for XGBoost model\n\n# model = XGBClassifier()\n# parameters = {\n# 'n_estimators' : [100,250,500],\n# 'learning_rate' : [0.01, 0.1],\n# 'subsample' :[0.5, 1.0],\n# 'max_depth' : [3,5,7],\n# 'criterion' : ['giny','entropy'],\n# 'objective':['binary:logistic'],\n# }\n\n# grid_search = GridSearchCV(estimator=model, param_grid=parameters,\n#                           cv=5, scoring='f1', verbose=True, n_jobs=-1)\n# grid_search.fit(X, y)\n# print(grid_search.best_score_)\n# print(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding parameters for RF model\n\n# model_rfc_gs = RandomForestClassifier()\n# parameters_rfc = {\n# 'n_estimators' : [100,200,500],\n# 'min_samples_split' : [2,4,6,8],\n# 'min_samples_leaf': [1,2,4,6]\n# }\n\n# grid_search_rfc = GridSearchCV(estimator=model_rfc_gs, param_grid=parameters_rfc,\n#                           cv=5, scoring='f1', verbose=True, n_jobs=-1)\n# grid_search_rfc.fit(X, y)\n# grid_search_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding parameters for Extra Tree Classifier\n\n# model_etc_gs = ExtraTreesClassifier()\n# parameters_etc = {\n# 'n_estimators' : [100,250,500],\n# 'min_samples_split' : [2,4,6,8],\n# 'min_samples_leaf': [1,3,5,7]\n# }\n\n# grid_search_etc = GridSearchCV(estimator=model_etc_gs, param_grid=parameters_etc,\n#                           cv=5, scoring='f1', verbose=True, n_jobs=-1)\n# grid_search_etc.fit(X, y)\n# grid_search_etc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding parameters for Decision Tree\n\n# model_dtc_gs = DecisionTreeClassifier()\n# parameters_dtc = {\n# 'criterion' : ['gini', 'entropy'],\n# 'min_samples_split' : [2,4,6,8],\n# 'min_samples_leaf': [1,2,3,4,5],\n# 'max_features' : ['auto', 'sqrt']\n# }\n\n# grid_search_dtc = GridSearchCV(estimator=model_dtc_gs, param_grid=parameters_dtc,\n#                           cv=5, scoring='f1', verbose=True, n_jobs =-1)\n# grid_search_dtc.fit(X, y)\n# grid_search_dtc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Permutation Importance graph with XGB Classifier algorithm.\n\nparams = {\n    'criterion': 'giny', \n    'learning_rate': 0.01, \n    'max_depth': 5,\n    'n_estimators': 100, \n    'objective': 'binary:logistic', \n}\nmodel = XGBClassifier(parameters=params)\n# fit the model\nmodel.fit(X, y)\n# perform permutation importance\nresult = permutation_importance(model, X, y, scoring='accuracy', n_repeats = 5, n_jobs=-1)\nsorted_idx = result.importances_mean.argsort()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Feature scores table\n\nfor i,v in enumerate(sorted_idx):\n    print('Feature: %0d, Score: %.5f' % (i,v))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Permutation Importance graph \n\nfig, ax = plt.subplots(figsize=(20,15))\n\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X.columns[sorted_idx])\nax.set_title(\"Permutation Importance\")\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph shows the feature importance of the features. according to that, 1 out of 29 features are not being important to prediction which is `babies`. It will eliminated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop `baby` feature from data\n\nhotel_data_model = hotel_data_model.drop(['babies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model Building","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part, some tree-based algorithms have been used for model building. These are *Decision Tree*, *Random Forest*, *Extra Trees Classifier*, and *Extreme Gradient Boosting*. *Random Forest* and *Extra Tree Classification* algorithms have been chosen as bagging algorithms, `XGBoost` has been chosen as one of the boosting algorithms. *Decision Tree* algorithm has been chosen as one tree algorithm. \n\nBefore model building, data will be split to train and test respectively 70% and 30% ratio. \n`X_train` and `X_test` data will be standardized with the *Standard Scaler* technique. After that, the *Stratified K-Fold Cross Validation* method will be used for resampling. Cross-validation is an important implementation to avoid overfitting. *Stratified K-Fold Cross Validation* method provides train/test indices to split data into train/test sets.\nModel parameters have been defined in the previous part. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate target variable for model building \n\ny_model = hotel_data_model.iloc[:,1]\nX_model = pd.concat([hotel_data_tunning.iloc[:,0],hotel_data_tunning.iloc[:,2:30]], axis=1)\ny_model.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split to train and test with 70-30 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.3, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement standart scaler method\n\nstandardScalerX = StandardScaler()\nX_train = standardScalerX.fit_transform(X_train)\nX_test = standardScalerX.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified K-Fold Cross Validation Method\n\nkfold_cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in kfold_cv.split(X_model,y_model):\n    X_train, X_test = X_model.iloc[train_index], X_model.iloc[test_index]\n    y_train, y_test = y_model.iloc[train_index], y_model.iloc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Model Building\n\ndtc_model = DecisionTreeClassifier(criterion= 'gini', min_samples_split=8,\n                                  min_samples_leaf = 4, max_features = 'auto')\n# fit the model\ndtc_model.fit(X_train, y_train)\n\n#Predict Model\npredict_dtc = dtc_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Model Building\n\nrf_model = RandomForestClassifier(min_samples_leaf = 6, min_samples_split=6,\n                                  n_estimators = 100)\n\n# fit the model\nestimator= rf_model.fit(X_train, y_train)\n#Predict Model\npredict_rf = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra Treees Classsifier Model Building\n\netc_model = ExtraTreesClassifier(min_samples_leaf = 7, min_samples_split=2,\n                                  n_estimators = 100)\n# fit the model\netc_model.fit(X_train, y_train)\n\n#Predict Model\npredict_etc = etc_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extreme Gradient Boosting Model Building\n\nxgb_model = XGBClassifier(criterion = 'giny', learning_rate = 0.01, max_depth = 5, n_estimators = 100,\n                          objective ='binary:logistic', subsample = 1.0)\n# fit the model\nxgb_model.fit(X_train, y_train)\n#Predict Model\npredict_xgb = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Classification Reports and Classification Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The last part is comparison of classification reports of ML models. \n\n\nFirst comparison the accuracy results. \n* Accuracy is a ratio of correct predictions to the total predictions.  \nIts formula is $(TP+TN) / (TP+FP+FN+TN)$\n\nAccording to that, *Random Forest* have the highest correct prediction with 88%. \nAnother performance metrics explained below:\n\n* Precision: It is the ratio of correctly predicted observation to the total positive predicted observation. \nIts formula is $TP / (TP+FP)$\n* Recall: It is the ratio of correctly predicted positive observations to the actual positive observations.\nIts formula is $TP / (TP+FN)$\n\n*Random Forest*  and the *Extra Tree Classifier* share the highest precision ratios. It means that both models predicted around 88% of all the positive labels correctly.  On the other hand *Random Forest* has the highest recall ratio. It means that this model predicted 79% of actual positive observations correctly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Reports \n\nprint(\"RF\", classification_report(y_test, predict_rf))\nprint(\"DTC\",classification_report(y_test, predict_dtc))\nprint(\"ETC\", classification_report(y_test, predict_etc))\nprint(\"XGB\", classification_report(y_test, predict_xgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix \n\nDTC_matrix = confusion_matrix(y_test, predict_dtc)\nRF_matrix = confusion_matrix(y_test, predict_rf)\nETC_matrix = confusion_matrix(y_test, predict_etc)\nXGB_matrix = confusion_matrix(y_test, predict_xgb) \n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\nsns.heatmap(RF_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel2\",  ax = ax[0]).set_ylim([0,2])\nax[0].set_title(\"Random Forest\", weight='bold')\nax[0].set_xlabel('Predicted Labels')\nax[0].set_ylabel('Actual Labels')\nsns.heatmap(DTC_matrix,annot=True, fmt=\"d\" ,cbar=False, cmap=\"tab20\", ax = ax[1]).set_ylim([0,2])\nax[1].set_title(\"Decision Tree\", weight='bold')\nax[1].set_xlabel('Predicted Labels')\nax[1].set_ylabel('Actual Labels')\n\nfig, axe = plt.subplots(1, 2, figsize=(15, 8))\nsns.heatmap(ETC_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Paired\", ax = axe[0]).set_ylim([0,2])\naxe[0].set_title(\"Extra Tree Classifier\", weight='bold')\naxe[0].set_xlabel('Predicted Labels')\naxe[0].set_ylabel('Actual Labels')\nsns.heatmap(XGB_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel1\", ax = axe[1]).set_ylim([0,2])\naxe[1].set_title(\"Gradient Boosting\", weight='bold')\naxe[1].set_xlabel('Predicted Labels')\naxe[1].set_ylabel('Actual Labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Any feedback is welcome, please be generous to share your feedbacks**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}