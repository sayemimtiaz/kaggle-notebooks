{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Loading the basic Data Science Libraries\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  # for matrix operations\nimport pandas as pd  # for loading CSV Files\nimport matplotlib.pyplot as plt # for Data Visualization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data\nPandas module is used for reading files. Since we have our data in '.csv' format, we will use 'read_csv()' function for loading the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape # This gives the shape of the DataFrame i.e the (no. of rows, no. of columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum() #Checking null values in the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Separating Input and Target Variable\n**Target Variable/ Independent Variable(y):** Our objective is to detect the presence of heart disease. Thus, our target variable will be the column that indicates whether heart disease is present or not. \n\nWe'll store that column in a variable y.\n\n**Input Variables/ Dependent Variables(X):** The remaining columns, without the Target variable will be stored in variable X.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('target', axis=1) #Input variables\n# axis=1 indicates that a column will be dropped\ny = data['target']  # Target variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting into Train and Test Sets\nThe next step will be to divide the data into test and train sets. We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into **train set** which will be used to train the model, and **test set** which will be used to check how accurately the model is predicting outcomes.\n\nThis is achieved using `train_test_split` function provided in the `model_selection class of sklearn` module.\n\nBy passing our X and y variables into the train_test_split method, we are able to capture the splits in data by assigning 4 variables to the result.\n\n* **X_train:** independent/input feature data for training the model\n* **y_train:** dependent/output feature data for training the model\n* **X_test:** independent/input feature data for testing the model; will be used to predict the output values\n* **y_test:** original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n* **test_size = 0.20:** 20% of the data will go for test set and 80% of the data will go for train set\n* **random_state = 42:** this is just for code reproducability. It will fix the split i.e. there will be the same data in train and test sets each time we run the code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of input features\nX_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building\nNow that we have our data fully processed and split into training and testing datasets, we can begin building a neural network to solve this classification problem. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport tensorflow as tf  # Importing the TensorFlow Library\nfrom tensorflow import keras  # Import Keras from TensorFlow\n\nfrom tensorflow.keras import Sequential \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Creation/ Definition\nWe create a Sequential model and add layers one at a time until we are happy with our network architecture.\n\nThe first thing to get right is to ensure the input layer has the right number of input features.\n\nSince this is a binary classification problem, we will use a sigmoid activation function in the final layer of our network.\n\n**Sigmoid is commonly used in the output layer. This is because it helps in giving a probability(value between 0 and 1) which is useful in Binary Classification.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Our model architecture\nFor our model, we'll be considering the following:\n\n* Input = the no. of features in X_train = 13\n* No. of neurons/units in first Dense layer = 32\n* No. of neurons/units in second Dense layer = 16\n* No. of neurons/units in third Dense layer = 8\n* No. of neurons/units in output layer = 1\n\nIf you closely look at it, we're slowly decreasing the number of neurons in each layer. Deciding the no. of hidden layers and no. of neurons is a process of trial and error. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))   \nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above code creates a Neural Network that has 4 layers. \n\n* The **last node** uses the **sigmoid activation function** that will squeeze all the values between 0 and 1.\n\n* The other layers use **ReLU (Rectified Linear Units)** as the activation function. ReLU is a half rectified function; that is, for all the inputs less than 0 (e.g. -120,-6.7, -0.0344, 0) the value is 0 while for anything positive (e.g. 10,15, 34) the value is retained. \n\n* One output unit is used since for each record values in X, a probability will be predicted. If it is high, then the person has a heart disease. If it is less, then the person does not have a heart disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Compilation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the model\nfrom tensorflow.keras.optimizers import RMSprop\noptimizer = RMSprop(0.001)  # Here, we have set our learning rate as 0.001\nmodel.compile(loss='binary_crossentropy', optimizer= optimizer , metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above code compiles the network.\n\n* The **loss function** used is **binary_crossentropy**. For binary classification problems that give output in the form of probability, binary_crossentropy is usually the optimizer of choice.\n\n*  It uses **rmsprop** as an **optimizer**. \n\n* The **learning rate** is taken to be **0.001**. You can even try different values to see which works the best.It is important to find a good value for the learning rate for your model on your training dataset. \n\n  We cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error.\n\n  The range of values to consider for the learning rate is less than 1.0 and greater than $10^{-6}$.\n\n  A traditional default value for the learning rate is 0.1, 0.01 or 0.001, and this may represent a good starting point on your problem.\n\n* **Metrics** used to evaluate the model is **accuracy**. Accuracy calculates how often the predictions calculated by the model are correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the summary of the model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training\nThe model is initially trained for **200 epochs** with a **batch size of 10**. \nBoth epochs and batch size are hyperparameters that can be modified to optimise the model.\n\n**validation_split:** Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \nThe val_loss and val_accuracy that you can see below are calculated with this validation data.\n\nYou'll learn more about the importance of validation data later. \n\n**verbose:** Verbose is just for printing purposes, for making the output more readable.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### History\nNotice that we're saving the trained model to a variable **history**. \n\nWhen running a model, Tensorflow Keras maintains a so-called History object in the background. This object keeps all loss values and other metric values in memory so that they can be used for visualizations.\n\nThe history object is the output of the fit operation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The validation accuracy predicted by the model is around 80%. It can further be increased by trying to optimize the epochs, the number of layers or the number of nodes per layer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\nEvaluating the model requires that we first choose a separate dataset used to evaluate the model. This should be data not used in the training process i.e. the X_test.\n\nNow, let us use the trained model to predict the probability values for the new data set - The test set we had initially created. The below code passes the X_test and y_test to the trained model and gives out the probability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Accuracy\nNow, we'll use the history object created above to plot the Accuracy and Loss throughout the training process.\n\nYou can think of history.history as a Python dictionary from which the values can be obtained by specifying a key within square brackets.\n\nFor eg. `history.history['accuracy']` will give the train accuracy throughout the training process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nIn this example, we developed a working Neural Network for the binary classification problem.\n\nThe model can be optimised further using hyperparameter tuning and other techniques.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}