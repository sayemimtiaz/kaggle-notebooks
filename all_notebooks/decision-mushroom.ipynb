{"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","file_extension":".py","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"_uuid":"da439a0e8db745127b3d1d8a8f277322325c4f8e","_cell_guid":"c861a8e5-363f-4bd4-8d9f-4cfbf9d9c974"},"source":"## Decision Tree Classification of Poisonous Mushrooms","cell_type":"markdown"},{"metadata":{"_uuid":"9d207233d3a7ebd4648cc4e9b3dacc9f5246efad","_cell_guid":"c7064e71-c6ce-4258-8e89-90f0a25082e2","collapsed":true},"execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd\n\nmushdf = pd.read_csv('../input/mushrooms.csv')\n\nmushdf.head()","cell_type":"code"},{"metadata":{"_uuid":"f1deeb33f80503cf700bdea57301cc1f2d8eb3e9","_cell_guid":"a0144c62-bb68-4172-806c-e14f32d99e05"},"source":"We need to create new data frame of  indicator variables because Decision Trees / Random Forests from scikit-learn do not tolerate strings, only numeric values for features. This is known as one-hot encoding or binarization. \n\nNOTE: It is possible to use scikit-learn's LabelEncoder for ordinal categorical features, but the current labels for the data do not appear to have any ordinality:\n\ncap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s  \ncap-surface: fibrous=f, grooves=g, scaly=y, smooth=s  \ncap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y  \nbruises?: bruises=t, no=f  \nodor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s  \ngill-attachment: attached=a, descending=d, free=f, notched=n  \ngill-spacing: close=c, crowded=w, distant=d  \ngill-size: broad=b, narrow=n  \ngill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y  \nstalk-shape: enlarging=e, tapering=t  \nstalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?  \nstalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s  \nstalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s  \nstalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y  \nstalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y  \nveil-type: partial=p, universal=u  \nveil-color: brown=n, orange=o, white=w, yellow=y  \nring-number: none=n, one=o, two=t  \nring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z  \nspore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y  \npopulation: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y  \nhabitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d","cell_type":"markdown"},{"metadata":{"_uuid":"4383cf7a2f1e7459e4df5fd4dac3adaa84c9bbf3","_cell_guid":"fcb290df-234e-4b76-b2e9-a52861496e1e","collapsed":true},"execution_count":null,"outputs":[],"source":"mushdf = pd.get_dummies(mushdf)\n\nmushdf.head()","cell_type":"code"},{"metadata":{"_uuid":"152d29a78aec8279015854c922732b22c89601fd","_cell_guid":"9bdc896c-4928-4681-9353-8a4c2247491b","collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_mush = mushdf.iloc[:,2:]\ny_mush = mushdf.iloc[:,1] # class_p (0=edible, 1=poisonous)\n\nX_train, X_test, y_train, y_test = train_test_split(X_mush, y_mush, random_state=650)","cell_type":"code"},{"metadata":{"_uuid":"65413f2ecc71222f23f30b923ef55693e0d7c6b2","_cell_guid":"0d93e217-c078-4d4f-9f44-1053da678123"},"source":"Let's fit a Decision Tree with default parameters in an attempt to classify the data and to examine feature importances to get a sense for what features are most informative when classifying poisonous mushrooms.","cell_type":"markdown"},{"metadata":{"_uuid":"3b2f92b6697d734d5f23f20e111a2b2d7c675ed7","_cell_guid":"9c56332b-1227-4d66-ab50-69fab5e811c6","collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier().fit(X_train, y_train)\n    \nfimportance = list(zip(X_train.columns, dtc.feature_importances_))\nfimportance.sort(key = lambda x: x[1], reverse=True)\n\nfimportance","cell_type":"code"},{"metadata":{"_uuid":"b651d90bb1a930dd5ebf597d363dc8880c8623c6","_cell_guid":"3e5e5f52-add7-4fc4-90ad-53350152c1e4"},"source":"## **Feature Selection**\n\nLet's examine the dimensionality of our features","cell_type":"markdown"},{"metadata":{"_uuid":"271574c5a900191a62285543f81aae818b88b1d8","_cell_guid":"f9d58e22-5b2d-4a1e-a56c-a213031e7a3d","collapsed":true},"execution_count":null,"outputs":[],"source":"X_mush.shape","cell_type":"code"},{"metadata":{"_uuid":"465fd24ffc85f948c2639dd717440cd6fee5c180","_cell_guid":"99c649be-8103-4926-9751-65bb3e327829"},"source":"Dummy variable parameterization of categorical variables creates *i*-1 indicator variables for each original feature with *i* levels. Our original 22 features ha expanded to 117 features. This has implications when it comes to performance. \n\nUsing sklearn's [SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) we can drop features that are not informative as measured by the estimated feature importance from the Decision Tree classifier.\n\nThe selected features we will use for learning are:","cell_type":"markdown"},{"metadata":{"_uuid":"276a2541f6347edcc96ae5fb0e3b5888dacaef41","_cell_guid":"2edc5fcd-391b-4a6f-b5b1-f4948ef301b4","collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.feature_selection import SelectFromModel\n\nselected = SelectFromModel(dtc, prefit=True, threshold='.01*mean')\n\nfeature_mask = m1.get_support(indices=False)\n\nX_mush_selected = X_mush[ X_mush.columns[feature_mask] ] \nX_mush_selected.columns","cell_type":"code"},{"metadata":{"_uuid":"e8907c28074cc84e29254b612aca55f10f017a76","_cell_guid":"64d1679c-1ce9-482d-bce5-ce01992d4d1e","collapsed":true},"execution_count":null,"outputs":[],"source":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush_selected, y_mush, random_state=650)\n\ndtc2 = DecisionTreeClassifier().fit(X_train2,y_train2)\n\n#feature importances\nfimportance_selected = list(zip(X_train2.columns, dtc2.feature_importances_))\nfimportance_selected.sort(key = lambda x: x[1], reverse=True)\n\nfimportance_selected","cell_type":"code"},{"metadata":{"_uuid":"7e6370af499158933dec00cc0f8c02c4af68f032","_cell_guid":"3eae7b34-b5c7-49da-9abd-a41c0a4ecfd5","collapsed":true},"execution_count":null,"outputs":[],"source":"features = list(list(zip(*fimportance))[0])[:10]\nimportances = list(list(zip(*fimportance))[1])[:10]\n\nplt.figure()\nsns.barplot(x=features, y=importances)\nplt.ylabel('Importance')\nplt.xlabel('Feature')\nplt.title('Selected Feature Importances from Default Decision Tree')\n\nfor item in plt.gca().xaxis.get_ticklabels():\n    item.set_rotation(90)","cell_type":"code"},{"metadata":{"_uuid":"5a4a88f8c5d3fdcd2d3e5cb3e020f542de2fd4e0","_cell_guid":"8d185c6e-90ff-4c7d-8106-fb7206e0a867"},"source":"It appears that the most informative features are odor and the stalk root.\n\nLet's examine how well the Decision Tree with default parameters does at classifying the mushrooms","cell_type":"markdown"},{"metadata":{"_uuid":"c1a9c320524ab32bf8b61b7828e5e68b836c80a8","_cell_guid":"d305bf60-cd4d-4ce7-9674-c34d8525f170","collapsed":true},"execution_count":null,"outputs":[],"source":"print('Mushroom dataset: decision tree')\nprint('Accuracy of DT classifier on training set: {:.2f}'\n     .format(dtc2.score(X_train2, y_train2)))\nprint('Accuracy of DT classifier on test set: {:.2f}'\n     .format(dtc2.score(X_test2, y_test2)))","cell_type":"code"},{"metadata":{"_uuid":"5cee4b3526c76d081b618ff2efb9d19b8860c6b7","_cell_guid":"879aac90-7312-4321-bc98-9ec7ce27dbe9"},"source":"## Visualizing the Tree","cell_type":"markdown"},{"metadata":{"_uuid":"8711df0597fae9c0e5dd1054fce67f3a6b52bbc6","_cell_guid":"63acd865-2232-41bb-96e0-2a030f94c912","collapsed":true},"execution_count":null,"outputs":[],"source":"import graphviz\nfrom sklearn.tree import export_graphviz\n\ndef plot_decision_tree(dtc, feature_names, class_names):\n    \n    export_graphviz(dtc, out_file=\"temp.dot\", feature_names=feature_names, class_names=class_names, filled = True, impurity = False)\n    with open(\"temp.dot\") as f:\n        dot_graph = f.read()\n\n    return graphviz.Source(dot_graph)\n\nplot_decision_tree(dtc2, X_mush_selected.columns, ['poisonous','edible'])","cell_type":"code"},{"metadata":{"_uuid":"4f76c6313aeca6fb95ef513dd037ac975e8f7a79","_cell_guid":"e33b72bf-b5b3-4313-995c-804cdaae52a0","collapsed":true},"source":"## Tuning Hyperparameters with GridSearchCV","cell_type":"markdown"},{"metadata":{"_uuid":"bafcd3d6d6daa743c346d66efa05ad4af2aba6a7","_cell_guid":"c6a20431-6dfc-4862-8f85-6153cd783519"},"source":"can we tune parameters after reduce dimensionality?","cell_type":"markdown"},{"metadata":{"_uuid":"289ede07e5c4dd6e2304d0f31e404e4ba1a8f8ec","_cell_guid":"34ab9767-f9f0-4ebd-9433-b0d5ae878478","collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.pipeline import Pipeline\n\ntune max_depth, min_samples_leaf","cell_type":"code"}]}