{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction\n\nWhen working with time series, the question most often arises - what will happen to us with our indicators in the next day / week / month / etc. - how much will be online, how many actions users will perform, and so on. The forecasting problem can be approached in different ways, depending on what quality should be pronase, for what period we want to build it, and, of course, how long it takes to select and adjust model parameters to obtain it.\n\n> A time series is a sequence of values ​​describing a process proceeding in time, measured at consecutive moments of time, usually at regular intervals\n\nThus, the data are ordered relative to nonrandom points in time, and, therefore, unlike random samples, they may contain additional information that we will try to extract.\n\nIn general, tasks related to time series can be divided into several groups\n- Forecasting - when we want to know what will happen next\n- Search for anomalies - when we want to understand where there were problems in the past\n- Clustering and classification - when time series themselves are signs of objects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom dateutil.relativedelta import relativedelta\nfrom scipy.optimize import minimize\n\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forecast Quality Metrics\n\nLet us consider the main and most widespread metrics for the quality of forecasts, which, by and large, are metrics for the regression problem and are used not only in time series.\n\n- [R squared](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination), $(-\\infty, 1]$\n\n$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$ \n\n```python\nsklearn.metrics.r2_score\n```\n---\n- [Mean Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error), $[0, +\\infty)$\n\n$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$ \n\n```python\nsklearn.metrics.mean_absolute_error\n```\n---\n- [Median Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error), $[0, +\\infty)$\n\n$MedAE = median(|y_1 - \\hat{y}_1|, ... , |y_n - \\hat{y}_n|)$\n\n```python\nsklearn.metrics.median_absolute_error\n```\n---\n- [Mean Squared Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error),  $[0, +\\infty)$\n\n$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n\n```python\nsklearn.metrics.mean_squared_error\n```\n---\n- [Mean Squared Logarithmic Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error),  $[0, +\\infty)$\n\n$MSLE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (log(1+y_i) - log(1+\\hat{y}_i))^2$\n\n```python\nsklearn.metrics.mean_squared_log_error\n```\n---\n-  $[0, +\\infty)$\n\n$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ \n\n```python\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n```","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, now we know how to measure the quality of our forecast, what metrics should be used and how to explain what the customer has done, it’s all left to do - we need to build a forecast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.read_csv('/kaggle/input/time-series-starter-dataset/Month_Value_1.csv', sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = full_df[['Revenue']].dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. We move, smooth and evaluate\n\nWe begin the simulation with the naive assumption - “tomorrow will be like yesterday”, but instead of a model of the form $\\hat{y}_{t} = y_{t-1}$ we assume that the future value of the variable depends on the average of $n$ previous values, which means we will use the moving average.\n\n$$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k-1}_{n=0} y_{t-n}$$","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18, 6))\nplt.plot(df.Revenue)\nplt.title('Revenue (month data)')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(df, 12) # prediction for the last observed day (past 24 hours)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, such a forecast cannot be made long-term; in order to obtain a one-step prediction, the previous value must be an actually observed value. But the moving average has another application - smoothing the initial series for revealing trends, there is a ready-made implementation in the ramp - [`DataFrame.rolling (window) .mean ()`] (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The more we set the interval width, the smoother the trend will be. In case the data is very noisy, which is especially common, for example, in financial indicators, such a procedure can help to see common patterns.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotMovingAverage(df, 4) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotMovingAverage(df, 12) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can also display confidence intervals for our averages.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plotMovingAverage(df, 4, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotMovingAverage(df, 12, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A modification of a simple moving average is a weighted average, inside which different weights are assigned to the observations, which can be equal to one, and usually the last observations are assigned more weight.\n\n\n$$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$$","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"weighted_average(df, [0.6, 0.3, 0.1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exponential smoothing\n\nNow let's see what happens if, instead of weighing the last $n$ values of the series, we start to weigh all available observations, while exponentially reducing weights as we go deeper into historical data. The simple [exponential smoothing] formula will help us with this.(http://www.machinelearning.ru/wiki/index.php?title=Экспоненциальное_сглаживание):\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n\n\nHere, the model value is the weighted average between the current true and previous model values. The weight of $\\alpha$ is called the smoothing factor. It determines how quickly we “forget” the last available true observation. The smaller $\\alpha$, the more influence the previous model values have, and the more smooth the series is.\n\nThe exponentiality lies in the recursiveness of the function - each time we multiply $(1-\\alpha)$ by the previous model value, which, in turn, also contained $(1-\\alpha)$, and so on to the very beginning.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotExponentialSmoothing(df.Revenue, [0.3, 0.05])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Double exponential smoothing\n\nUntil now, we could only get one point forward from our methods at best (and smooth the series nicely), which is great, but not enough, so let's move on to expanding exponential smoothing, which will allow us to build the forecast two points ahead (and nice to smooth a row too).\n\nThis will help us to split the series into two components - the level (level, intercept) $\\ell$ and the trend $b$ (trend, slope). We predicted the level or expected value of the series using the previous methods, and now the same exponential smoothing is applicable to the trend, naively or not really believing that the future direction of the series changes depends on weighted previous changes.\n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\nAs a result, we get a set of functions. The first describes the level - it, as before, depends on the current value of the series, and the second term is now divided into the previous value of the level and trend. The second is responsible for the trend - it depends on the level change at the current step, and on the previous trend value. Here, the coefficient $\\beta$ acts as the weight in exponential smoothing. Finally, the final prediction is the sum of the model values of the level and trend.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotDoubleExponentialSmoothing(df.Revenue, alphas=[0.9, 0.02], betas=[0.9, 0.02])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-validation on time series\n\nBefore constructing a model, we will finally talk about a non-manual estimation of parameters for models.\n\nThere is nothing unusual here, as before, you must first select the loss function suitable for this task, which will monitor the quality of fitting the model to the source data. Then we will evaluate the cross-validation value of the loss function with these parameters of the model, look for the gradient, change the parameters in accordance with it and vigorously fall towards the global minimum of the error.\n\nSince the time series suddenly has a time structure, it is impossible to randomly mix the values of the entire series in folds without preserving this structure, otherwise in the process all the relationships of observations with each other will be lost. Therefore, you will have to use a slightly trickier way to optimize the parameters, the official name of which I did not find, but on the site [CrossValidated] (https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation -for-time-series-model-selection), offer the name \"cross-validation on a rolling basis\", which can be literally translated as cross-validation on a sliding window.\n\nThe essence is quite simple - we begin to train the model on a small segment of the time series, from the beginning to some $ t $, make a forecast for $t+n$ steps forward and consider the error. Next, we expand the training set to $t+n$ values and predict from $t+n$ to $t+2*n$, so we continue to move the test segment of the series until we run into the last available observation. As a result, we get as many folds as $n$ fits in the gap between the initial training segment and the entire row length.\n\n<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stationarity\n\nBefore moving on to modeling, it is worth mentioning such an important property of the time series as [**stationarity**] (https://ru.wikipedia.org/wiki/Stationarity).\nStationarity is understood as the property of a process not to change its statistical characteristics over time, namely, the constancy of expectation, the constancy of dispersion (it is [homoskedasticity] (https://ru.wikipedia.org/wiki/Gomoskedasticity)) and the covariance function is independent of time ( should depend only on the distance between observations). You can visually look at these properties in the pictures taken from the post [Sean Abu] (http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n\n- The time series on the right is not stationary, since its expectation increases with time\n\n<img src = \"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\" />\n\n- There is no luck with dispersion - the range of values of the series varies significantly depending on the period\n\n<img src = \"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\" />\n\n- Finally, the last graph shows that the values of the series suddenly become closer to each other, forming a certain cluster, and as a result we get inconstancy of covariances\n\n<img src = \"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\" />\n\nWhy is stationarity so important? For a stationary series, it is easy to make a forecast, since we believe that its future statistical characteristics will not differ from the observed current ones. Most models of time series in one way or another model and predict these characteristics (for example, expectation or variance), so if the initial series is not stationary, the predictions will turn out to be wrong. Unfortunately, most of the time series that you have to deal with beyond the scope of training materials are not stationary, but you can (and should) deal with this.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get rid of non-stationarity and build SARIMA\n\nNow let's try to build a SARIMA model, having gone through all the ~~circles of hell~~ stage of bringing the series to a stationary form. Details about the model itself can be found here - [Building a SARIMA model using Python + R] (https://habrahabr.ru/post/210530/), [Time series analysis using python] (https://habrahabr.ru/ post / 207160 /)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tsplot(df.Revenue, lags=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_diff = df.Revenue - df.Revenue.shift(12)\ntsplot(df_diff[12:], lags=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, this series is stationary according to the Dickey-Fuller criterion, according to its graph it is also visible that the trend, as such, is absent, i.e., the expectation is constant, the spread around the average is also about the same, which means that the dispersion is also constant. There remains a seasonality that needs to be dealt with before building the model.\nTo do this, we perform the transformation under the cunning name of \"seasonal differentiation\", under which lies a simple subtraction from a series of oneself with a lag equal to the seasonality season.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# To be continued","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}