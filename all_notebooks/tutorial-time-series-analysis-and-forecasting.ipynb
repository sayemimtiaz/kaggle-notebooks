{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Time_Series_Header](https://raw.githubusercontent.com/satishgunjal/images/master/Time_Series_Header_1000x690.png)"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [What is Time Series](#1)\n  - [Time Series Characteristics](#2)\n    + [Trend](#3) \n    + [Seasonality](#4)\n    + [Irregularities](#5)\n    + [Cyclicity](#6)\n* [Time Series Analysis](#7)\n  - [Decomposition of Time Series](#8)\n  - [Stationary Data](#9)\n  - [Test for Stationarity](#10)\n    + [Rolling Statistics](#11)\n    + [Augmented Dickey Fuller Test](#12)\n  - [Convert Non Stationary Data to Stationary Data](#13)\n    + [Differencing](#14)\n    + [Transformation](#15)\n    + [Moving Average](#16)\n      + [Weighted Moving Averages(WMA)](#17)\n      + [Centered Moving Averages(CMS)](#18)\n      + [Trailing Moving Averages(TMA)](#19)\n  - [Correlation](#20)\n    + [ACF: Auto Correlation Function](#21)\n    + [PACF: Partial Auto Correlation Function](#22)\n* [Time Series Forecasting](#23)\n  - [Models Used For Time Series Forecasting](#24)\n  - [ARIMA](#25)\n    + [Auto-Regressive (AR) Model](#26)\n    + [Integration(I)](#27)\n    + [Moving Average (MA) Model](#28)\n* [Python Example](#29)\n  - [Import The Library](#30)\n  - [Understanding The Data](#31)\n  - [Time Series Characteristics](#32)\n  - [Analysis](#33)\n  - [Forecasting](#34)"},{"metadata":{},"cell_type":"markdown","source":"# What is Time Series <a id =\"1\"></a>\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temp reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable. \n\nSales forecasting time series with shampoo sales for every month will look like this, \n\n![Shampoo_Sales](https://raw.githubusercontent.com/satishgunjal/images/master/Shampoo_Sales.png)\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\n\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n\n\n## Time Series Characteristics <a id =\"2\"></a>\nMean, standard deviation and seasonality defines different characteristics of the time series. \n\n![Time_Series_Characteristics](https://raw.githubusercontent.com/satishgunjal/images/master/Time_Series_Characteristics.png)\n\nImportant characteristics of the time series are as below\n\n### Trend <a id =\"3\"></a>\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It's not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n![Trend](https://raw.githubusercontent.com/satishgunjal/images/master/Trend.png)\n\n### Seasonality <a id =\"4\"></a>\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n![Seasonality](https://raw.githubusercontent.com/satishgunjal/images/master/Seasonality.png)\n\n### Irregularities <a id =\"5\"></a>\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n![Irregularities](https://raw.githubusercontent.com/satishgunjal/images/master/Irregularities.png)\n\n### Cyclicity <a id =\"6\"></a>\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n![Cyclicity](https://raw.githubusercontent.com/satishgunjal/images/master/Cyclicity.png)\n\nTime series data which has above characteristics is called as 'Non-Stationary Data'. For any analysis on time series data we must convert it to 'Stationary Data'\n\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n\n\n# Time Series Analysis <a id =\"7\"></a>\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n## Decomposition of Time Series <a id =\"8\"></a>\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n![Decomposition_of_Time_Series](https://raw.githubusercontent.com/satishgunjal/images/master/Decomposition_of_Time_Series.png)\n\n## Stationary Data <a id =\"9\"></a>\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series.\nTime series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n![Stationarity](https://raw.githubusercontent.com/satishgunjal/images/master/Stationarity.png)\n\n## Test for Stationarity <a id =\"10\"></a>\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data. \n\n### Rolling Statistics <a id =\"11\"></a>\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn't change with time.\n\n### Augmented Dickey Fuller (ADF) Test <a id =\"12\"></a>\nI won't go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return 'p-value' and 'Test Statistics' output values.\n* **p-value > 0.05**: non-stationary.\n* **p-value <= 0.05**: stationary.\n* **Test statistics**: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n## Convert Non-Stationary Data to Stationary Data <a id =\"13\"></a>\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\n### Differencing <a id =\"14\"></a>\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\n```\ndifference = previous observation - current observation\n```\nAfter performing the differencing it's recommended to plot the data and  visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n### Transformation <a id =\"15\"></a>\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques.\nMost of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don't work, you may first want to use one of above transformation technique to remove the variation from the series. \n\n![Log_Transformation](https://raw.githubusercontent.com/satishgunjal/images/master/Log_Transformation.png)\n\n### Moving Average <a id =\"16\"></a>\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as 'window width (w)'. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n![Moving_Average](https://raw.githubusercontent.com/satishgunjal/images/master/Moving_Average.png)\n\n#### Weighted Moving Averages(WMA) <a id =\"17\"></a>\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n#### Centered Moving Averages(CMS) <a id =\"18\"></a>\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n  ```\n  CMA(t) = mean(t-1, t, t+1)\n  ```\n  \nCMA is very useful for visualizing the time series data\n  \n#### Trailing Moving Averages(TMA) <a id =\"19\"></a>\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n ```\n TMA(t) = mean(t-2, t-1, t)\n ```\n \n TMA are useful for forecasting.\n\n## Correlation <a id =\"20\"></a>\n* Most important point about values in time series is its dependence on the previous values.\n* We can calculate the correlation for time series observations with previous time steps, called as lags.\n* Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\n* To understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices. \n    - P(t)= Fish price of today\n    - P(t-1) = Fish price of last month\n    - P(t-2) =Fish price of last to last month\n* Time series of fish prices can be represented as P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\n* So if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\n### Auto Correlation Function (ACF) <a id =\"21\"></a>\n* ACF tells you how correlated points are with each other, based on how many time steps they are separated by.\n* Now to understand it better lets consider above example of fish prices. Let's try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today's fish price or it can indirectly affect the fish price through last months price P(t-1)\n* So ACF consider the direct as well indirect effect between the points while determining the correlation\n\n### Partial Auto Correlation Function (PACF) <a id =\"22\"></a>\n* Unlike ACF, PACF only consider the direct effect between the points while determining the correlation\n* In case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n\n# Time Series Forecasting <a id =\"23\"></a>\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\n* Step 1: Understand the time series characteristics like trend, seasonality etc\n* Step 2: Do the analysis and identify the best method to make the time series stationary\n* Step 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\n* Step 4: Based on data analysis choose the appropriate model for time series forecasting\n* Step 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\n* Step 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\n* Step 7: At the end we can do the future forecasting and get the future forecasted values in original scale.\n\n## Models Used For Time Series Forecasting <a id =\"24\"></a>\n* Autoregression (AR)\n* Moving Average (MA)\n* Autoregressive Moving Average (ARMA)\n* Autoregressive Integrated Moving Average (ARIMA)\n* Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n* Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n* Vector Autoregression (VAR)\n* Vector Autoregression Moving-Average (VARMA)\n* Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n* Simple Exponential Smoothing (SES)\n* Holt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n\n## ARIMA <a id =\"25\"></a>\n* ARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model. \n* ARIMA has three parameters 'p' for the order of Auto-Regressive (AR) part, 'q' for the order of Moving Average (MA) part and 'd' for the order of integrated part. \n\n### Auto-Regressive (AR) Model: <a id =\"26\"></a>\n* As the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values. \n* To figure out the order of AR model we will use PACF function\n\n### Integration(I): <a id =\"27\"></a>\n* Uses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values d number of times.\n* Most of the time value of d = 1, means first order of difference.\n\n### Moving Average (MA) Model: <a id =\"28\"></a>\n* Rather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\n* To figure out the order of MA model we will use ACF function"},{"metadata":{},"cell_type":"markdown","source":"# Python Example <a id =\"29\"></a>\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months."},{"metadata":{},"cell_type":"markdown","source":"## Import The Library <a id =\"30\"></a>\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* matplotlib : It’s plotting library, and we are going to use it for data visualization\n* seaborn: It’s a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n* statsmodels: Using statsmodels module classes and functions for time series analysis and forecasting \n   * adfuller: Augmented Dickey-Fuller\n   * ACF: Auto Correlation Function\n   * PACF: Partial Auto Correlation Function\n   * ARIMA: Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n   * sm.tsa.seasonal.seasonal_decompose: For decomposition of time series\n* rcParams: To change the matplotlib properties like figure size"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller,acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set plot size \nrcParams['figure.figsize'] = 10, 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/Time_Series_AirPassengers.csv')\nprint('Shape of the data= ', df.shape)\nprint('Column datatypes= \\n',df.dtypes)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding The Data <a id =\"31\"></a>\n* Dataframe 'df' contains the time series data. There are two columns 'Month' and 'Passengers'. Month column contains the value of month in that year and passenger column contains the number of air passenger for that particular month.\n* As you may have noticed 'Month' column datatype is 'Object', so we are going to convert it to 'datetime'\n* To make plotting easier, we set the index of pandas dataframe 'df' to the 'Month' column so that it will act as x-axis & Passenger column as y-axis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Month'] = pd.to_datetime(df.Month)\ndf = df.set_index(df.Month)\ndf.drop('Month', axis = 1, inplace = True)\nprint('Column datatypes= \\n',df.dtypes)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Characteristics <a id =\"32\"></a>\n\n### Trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10,6))\nplt.plot(df)\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')\nplt.title('Trend of the Time Series')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from above plot there is upward trend of number of passenger for every year. \n\n### Variance\nIn above graph you can clearly see that the variation is also increasing with the level of the series. You will see in the later part of this exercise how we handle the variance to increase the stationarity of the series.\n\n### Seasonality\nWe can also see the graph going up and down at regular interval, that is the sign of seasonality. Let's plot the graph for few months to visualize for seasonality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To plot the seasonality we are going to create a temp dataframe and add columns for Month and Year values\ndf_temp = df.copy()\ndf_temp['Year'] = pd.DatetimeIndex(df_temp.index).year\ndf_temp['Month'] = pd.DatetimeIndex(df_temp.index).month\n# Stacked line plot\nplt.figure(figsize=(10,10))\nplt.title('Seasonality of the Time Series')\nsns.pointplot(x='Month',y='Passengers',hue='Year',data=df_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say that every year in month of July we observe maximum number of passengers and similarly minimum number of passenger in the month of November.\n\n### Decomposition of Time Series\nLet's now use the decomposition technique to deconstruct the time series data into several component like trend and seasonality for visualization of time series characteristics.\n\nHere we are going to use 'additive' model because it is quick to develop, fast to train, and provide interpretable patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(df, model='additive') \nfig = decomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Analysis <a id =\"33\"></a>\nSo our time series has variance, trend and seasonality characteristics. During our analysis we are going to try multiple techniques to make time series stationary and record the stationarity scores for each method. Finally, we will select the method, which is easy for inverse transformation easy and give best stationarity score.\n\n### Check for Stationarity\nWe are going to use rolling statistics and Dickey-Fuller test to check the stationarity of the time series"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stationarity_test(timeseries):\n    # Get rolling statistics for window = 12 i.e. yearly statistics\n    rolling_mean = timeseries.rolling(window = 12).mean()\n    rolling_std = timeseries.rolling(window = 12).std()\n    \n    # Plot rolling statistic\n    plt.figure(figsize= (10,6))\n    plt.xlabel('Years')\n    plt.ylabel('No of Air Passengers')    \n    plt.title('Stationary Test: Rolling Mean and Standard Deviation')\n    plt.plot(timeseries, color= 'blue', label= 'Original')\n    plt.plot(rolling_mean, color= 'green', label= 'Rolling Mean')\n    plt.plot(rolling_std, color= 'red', label= 'Rolling Std')   \n    plt.legend()\n    plt.show()\n    \n    # Dickey-Fuller test\n    print('Results of Dickey-Fuller Test')\n    df_test = adfuller(timeseries)\n    df_output = pd.Series(df_test[0:4], index = ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in df_test[4].items():\n        df_output['Critical Value (%s)' %key] = value\n    print(df_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets test the stationarity score with original series data\nstationarity_test(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though it's clear from visual observation that it's not a stationary series, but still lets have look at the rolling statistics and Duckey Fuller test results\n\n* Rolling statistics: Standard deviation has very less variation but mean is increasing continuously.\n* Duckey Fuller Test: Test statistic is way more than the critical values.\n\n### Convert Non-Stationary Data to Stationary Data\nLet's first use the differencing technique to obtain the stationarity.\n\n#### Differencing\nTo transform the series using 'Differencing' we will use the diff() method of pandas. A benefit of using the Pandas function, in addition to requiring less code, is that it maintains the date-time information for the differenced series"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diff = df.diff(periods = 1) # First order differencing\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Differencing ')\nplt.plot(df_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from above graph its clear that differencing technique removed the trend from the time series, but variance is still there Now lets run the stationarity_test() to check the effectiveness of the 'Differencing' technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diff.dropna(inplace = True)# Data transformation may add na values\nstationarity_test(df_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rolling values appear to be varying slightly, and we can see there is slight upward trend in standard deviation. Also, the test statistic is smaller than the 10% critical but since p-value is greater than 0.05 it is not a stationary series.\n\nNote that variance in the series is also affecting above results, which can be removed using transformation technique.\n\nLet's also check with transformation technique\n\n\n#### Transformation\n\n\nSince variance is proportional to the levels, we are going to use the log transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log = np.log(df)\n\nplt.subplot(211)\nplt.plot(df, label= 'Time Series with Variance')\nplt.legend()\nplt.subplot(212)\nplt.plot(df_log, label='Time Series without Variance (Log Transformation)')\nplt.legend()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since log transformation has removed the variance from series, lets use this transformed data hence forward. \nNote that, Since we are using log transformation, we can use the exponential of the series to get the original scale back.\n```\n    df = exp(df_log)\n```\n\nLet cross-check the differencing method scores with this log transformed data again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_diff = df_log.diff(periods = 1) # First order differencing\n\ndf_log_diff.dropna(inplace = True)# Data transformation may add na values\nstationarity_test(df_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rolling mean and standard deviation values are okay now. The test statistic is smaller than the 10% critical values but since p-value is greater than 0.05 it is not a stationary series.\n\nLet's also check with Moving Average technique…\n\n#### Moving Average\n\nSince we have time series data from 1 Jan 1949 to 1 Dec 1960, we will define a yearly window for moving average. Window size = 12. Note that we are going to use Log transformed data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_moving_avg = df_log.rolling(window = 12).mean()\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Moving Average')\nplt.plot(df_log, color= 'blue', label='Orignal')\nplt.plot(df_log_moving_avg, color= 'red', label='Moving Average')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from above graph that data is more smooth without any variance. If we use the differencing technique with log transformed data and mean average data then we should get better stationarity scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_moving_avg_diff = df_log - df_log_moving_avg\ndf_log_moving_avg_diff.dropna(inplace = True)\nstationarity_test(df_log_moving_avg_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected now we are able to see some real improvements. p-value is less than 0.05 that means our series is stationary, but we can only say this with 95% of confidence, as test statistics is less than 5% critical value.\n\nIn order to increase the stationarity of the series lets try to use 'Weighted Moving Average' technique\n\n#### Weighted Moving Average (WMA)\n\nHere we are going to use exponentially weighted moving average with parameter ‘halflife = 12’. This parameter defines the amount of exponential decay. This is just an assumption here and would depend largely on the business domain."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_weighted_avg = df_log.ewm(halflife = 12).mean()\nplt.plot(df_log)\nplt.plot(df_log_weighted_avg, color = 'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that WMA follow's no of passenger values more closely than a corresponding Simple Moving Average which also results in more accurate trend direction. Now lets check, the effect of this on stationarity scores!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_weighted_avg_diff = df_log - df_log_weighted_avg\nstationarity_test(df_log_weighted_avg_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it’ll work even with no previous values.\n\nThere is one more way to obtain better stationarity is by using the residual data from time series decomposition.\n\n#### Decomposition of Time Series\n\nLet's now use the decomposition technique to deconstruct the log transformed time series data, so that we can check the stationarity using residual data."},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(df_log,period =12)\nfig = decomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the trend and seasonality are separated out from log transformed data, and we can now check the stationarity of the residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_residual = decomposition.resid\ndf_log_residual.dropna(inplace = True)\nstationarity_test(df_log_residual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Dickey-Fuller test statistic is significantly lower than the 1% critical value and p-value is almost 0. So this time series is very close to stationary. This concludes our time series analysis and data transformation to get the stationary series. Now we can start modeling it for forecast.\n\n## Forecasting <a id =\"34\"></a>\n* Though using residual values gives us very good results, but it's relatively difficult to add noise and seasonality back into predicted residuals in this case. \n* So we are going to make model on the time series(df_log_diff), where we have used log transformation and differencing technique. This is one of the most popular and beginner-friendly technique. As per our time series analysis 'df_log_diff' is not a perfectly stationary series, that's why we are going to use statistical models like ARIMA to forecast the data.\n* Remember that ARIMA model uses three parameters, 'p' for the order of Auto-Regressive (AR) part, 'q' for the order of Moving Average (MA) part and 'd' for the order of integrated part. We are going to use d =1 but to find the value for p and q lets plot ACF and PACF.\n* Note that since we are using d=1, first order of differencing will be performed on given series. Since first value of time series don't have any value to subtract from resulting series will have one less value from original series\n\n### ACF and PACF Plots\n* To figure out the order of AR model(p) we will use PACF function. p = the lag value where the PACF chart crosses the upper confidence interval for the first time\n* To figure out the order of MA model(q) we will use ACF function. q = the lag value where the ACF chart crosses the upper confidence interval for the first time"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(df_log_diff, nlags=20)\nlag_pacf = pacf(df_log_diff, nlags=20, method='ols')\n\n# Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph its clear that p=2 and q=2. Now we have the ARIMA parameters values, lets make 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS(Residual Sum of Square) metric for each. Please note that here RSS is for the values of residuals and not actual series.\n\n### AR Model\nSince 'q' is MA model parameter we will keep its value as '0'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# freq = 'MS' > The frequency of the time-series MS = calendar month begin\n# The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use\nmodel = ARIMA(df_log, order=(2, 1, 0), freq = 'MS')  \nresults_AR = model.fit(disp= -1)# If disp < 0 convergence information will not be printed\nplt.plot(df_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('AR Model, RSS: %.4f'% sum((results_AR.fittedvalues - df_log_diff['Passengers'])**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MA Model\nSince 'p' is AR model parameter we will keep its value as '0'."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(df_log, order=(0, 1, 2), freq = 'MS')  \nresults_MA = model.fit(disp=-1)  \nplt.plot(df_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('MA Model, RSS: %.4f'% sum((results_MA.fittedvalues-df_log_diff['Passengers'])**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combined Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(df_log, order=(2, 1, 2), freq = 'MS')  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(df_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('Combined Model, RSS: %.4f'% sum((results_ARIMA.fittedvalues-df_log_diff['Passengers'])**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the AR and MA models have almost the same RSS score but combined is significantly better. So we will go ahead with combined ARIMA model and use it for predictions.\n\n### Prediction and Reverse Transformation\n* We will create a separate series of predicted values using ARIMA model\n* Reverse transform the predicted values to get the original scale back\n* Compare the predicted values with original values and plot them"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a separate series of predicted values\npredictions_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n\nprint('Total no of predictions: ', len(predictions_diff))\npredictions_diff.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are using first order of differencing(d =1), there is no prediction available for first value (1949-02-01) of original series. In order to remove 'differencing transformation' from the prediction values we are going to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. We are going to use pandas cumsum() function for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_diff_cumsum = predictions_diff.cumsum()\npredictions_diff_cumsum.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above values once added to the base number will completely remove the differencing transformation. For this, lets create a series with all values as base number and add the 'predictions_diff_cumsum' to it."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_log = pd.Series(df_log['Passengers'].iloc[0], index=df_log.index) # Series of base number\npredictions_log = predictions_log.add(predictions_diff_cumsum,fill_value=0)\npredictions_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as of now we have removed the differencing transformation, now lets remove the log transformation to get the original scale back."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.exp(predictions_log)\nplt.plot(df)\nplt.plot(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_predictions =pd.DataFrame(predictions, columns=['Predicted Values'])\npd.concat([df,df_predictions],axis =1).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Future Forecasting\n* We have data from 1 Jan 1949 to 1 Dec 1960. 12 years of data with passenger number observation for each month i.e. 144 total observations.\n* If we want to forecast for next 5 years or 60 months then, ‘end’ count will be >  144 + 60 = 204.\n* We are going to use statsmodels plot_predict() method for it"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ARIMA.plot_predict(start = 1, end= 204) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasted values in original scale will be\nforecast_values_log_scale = results_ARIMA.forecast(steps = 60)\nforecast_values_original_scale = np.exp(forecast_values_log_scale[0])\n\nforecast_date_range= pd.date_range(\"1961-01-01\", \"1965-12-01\", freq=\"MS\")\n\ndf_forecast =pd.DataFrame(forecast_values_original_scale, columns=['Forecast'])\ndf_forecast['Month'] = forecast_date_range\n\ndf_forecast[['Month', 'Forecast']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference material is as below. \n* https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n* https://www.kaggle.com/freespirit08/time-series-for-beginners-with-arima/notebook\n* https://maelfabien.github.io/statistics/TimeSeries3/#\n* https://www.geeksforgeeks.org/python-arima-model-for-time-series-forecasting/\n* https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n* https://www.kaggle.com/chirag19/time-series-analysis-with-python-beginner/notebook\n* https://www.kaggle.com/freespirit08/time-series-for-beginners-with-arima/notebook\n* https://www.youtube.com/watch?v=gj4L2isnOf8&t=1167s\n* https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-visualization-with-python-3\n* https://towardsdatascience.com/analyzing-time-series-data-in-pandas-be3887fdd621\n* https://machinelearningmastery.com/power-transform-time-series-forecast-data-python/\n* https://www.geeksforgeeks.org/python-arima-model-for-time-series-forecasting/\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}