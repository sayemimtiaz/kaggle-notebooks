{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Mining Project\n\nHi there, My name is Diego from Colombia and I want to share with you all my work done with this dataset.\nWe use the CRISP methodology when We apply some of strategies found in this. the main features applied are the following:\n> \n1. Business Understanting\n2. Data Understanding \n3. Data Preparation\n4. Modeling\n5. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## 1. Business Understanding\n\nThe dataset is involved into the analysis of depression. The data was consists as a study about the life conditions of people who live in rurales zones.\nBecause all the columns were not explicated in this challenge so We can´t understand them. We proceded to delete them or ignoring. Fhe final features or columns were the following:\n\n* Survey_id \n* Ville_id\n* sex\n* Age\n* Married\n* Number_children\n* education_level\n* total_members (in the family)\n* gained_asset\n* durable_asset\n* save_asset\n* living_expenses\n* other_expenses\n* incoming_salary\n* incoming_own_farm\n* incoming_business\n* incoming_no_business\n* incoming_agricultural\n* farm_expenses\n* labor_primary\n* lasting_investment\n* no_lasting_investmen\n* depressed: [ Zero: No depressed]  or [One: depressed] (Binary for target class)\n\nthe main objective is to show statistic analysis and some data mining techniques.\nThe dataset has 23 columns o dimensiones y un total de 1432 objetos o\nregistros.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2. Data Understanding and Descriptive statistics"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_pd = pd.read_csv(\"../input/b_depressed.csv\")\ndata_pd_original = pd.read_csv(\"../input/b_depressed.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Describing the datatypes:\ndata_pd.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.1 Unbalanced classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd.groupby('depressed').size()\n# People with depression: 1191\n# People with NO depression: 238","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or if you like the percentages as follows: \nfp = data_pd.groupby('depressed').size()/data_pd.shape[0]\nprint(fp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Obvisously, Not for all the columns make sense obtain the mean or median. However, We got the mean for all variables with mean function of pandas Data Frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  2.2. Correlation Matrix\nA very common way to describe the data is throught correlation matrix. this is a visual technique to show the relationship between variables od the dataset.\nThe correlation values use to be between -1 and +1. However, in the practice, on the whole the elements have positive correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.matshow(data_pd.corr())\nplt.show()\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data_pd.corr(), fignum=f.number)\nplt.xticks(range(data_pd.shape[1]), data_pd.columns, fontsize=14, rotation=45)\nplt.yticks(range(data_pd.shape[1]), data_pd.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('C',fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For example, according to correlation matrix, We can see a good relationship between \"labor_primary\" and \"incoming_own_farm\". Other good relationship is the following: \"incoming_salary\" and \"landing_investment\", make sense. Finally, We can analyze a histogram to some variable, for instance, the \"age\" variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = data_pd['Age'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphic before, shows the main ages where the the most frequent value is age = 30 with 400 times."},{"metadata":{"trusted":true},"cell_type":"code","source":"#% matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.countplot(x='sex', data = data_pd) \n# Where woman is 1 and man is 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='depressed', col='Married', kind='count', data= data_pd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some conclusions about that, hurriedly We can say that to be married helps to not be depressed according with the before graphics !. But remember We dealing with unbalanced data, the majory of people (>80%) are no depressed."},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.1. Cleaning data: Missing values\n\nThis dataset is clean, for the missing values to the numerics fields, We opted to fill them computing the mean and median of all values of this variable and putting in each missing cell.\nThe columns where we applied this technique were the following: 'gained_asset', 'durable_asset', 'save_asset', 'living_expenses', 'other_expenses', 'incoming_agricultural','farm_expenses', and 'lasting_investment'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#If we explore the next feature 'no_lasting_investment' there are some rows without values:\ndata_pd.no_lasting_investmen.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We are going to proce to fill the missing values with the Median:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd['no_lasting_investmen'].fillna(data_pd['no_lasting_investmen'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_original['no_lasting_investmen'].fillna(data_pd_original['no_lasting_investmen'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. To comprobe that the data was filled"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd.no_lasting_investmen.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_original.no_lasting_investmen.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd['no_lasting_investmen'] = data_pd['no_lasting_investmen'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_original['no_lasting_investmen'] = data_pd_original['no_lasting_investmen'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Cleaning data: Outliers Detection:\n\nThis dataset is clean, for the outliers values We applied the mean and the variance with the folowing umbral: media +-2 variance.\nThe variable wich had many outliers was the age. More ahead I'll show how to generate new discrete variables to take ranges and not values with a high difference between them. \n"},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling"},{"metadata":{},"cell_type":"markdown","source":"In this chapter, I want to make in three sections: Asociation, Clustering and Classification. \nAsociation Rules are common ways to search dependencies between the variables doing some changes in support and confidence and find out or extract rules as market basket analysis. You can read mre about that [here](https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)\n\nWe propose some methods for clustering as K-Means, Hierachical clustering, K-Neighborhoods and some techniques to validation these algorithms like distance intra and extra cluster, similarity validation also. "},{"metadata":{},"cell_type":"markdown","source":"# 4.1. Asociation Rules\n\nFor this task, it opted to do a normalization of data. The normalization used was z-score (Gaussian normalization). "},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_normalize = [ 'Age', 'Number_children','education_level', 'total_members', 'gained_asset', 'durable_asset',\n       'save_asset', 'living_expenses', 'other_expenses',\n       'incoming_agricultural', 'farm_expenses', \n       'lasting_investment', 'no_lasting_investmen']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use Scipy library zscore to the normalization:\nfrom scipy import stats\n\nfor c in columns_to_normalize:\n    data_pd[c] = stats.zscore(data_pd[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the data normalizaed \ndata_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)We have to remove the 'survey_id' and 'Ville_id' columns because this not represents a valid information to extract."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop By Name:\ndata_pd = data_pd.drop(['Survey_id', 'Ville_id'], axis=1)\ndata_pd.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_original = data_pd_original.drop(['Survey_id', 'Ville_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binning in Python and Pandas: We applied Bining or Discretization in the dimentions:\nThe size of binning for each variable is 5. It creates range of data, as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in columns_to_normalize:\n    data_pd[c] = pd.cut(data_pd[c], 5)\n\ndata_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To discretize the binary data to avoid duplicate the quantity o columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_two_bins = ['sex', 'Married',\n       'incoming_own_farm', 'incoming_business','incoming_no_business', \n       'labor_primary', 'depressed']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for c in columns_two_bins:\n  data_pd[c] = pd.cut(data_pd[c], 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, We continue adding new Dummies variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data = pd.DataFrame()\nfor i in data_pd.columns:\n  data_pd[i] = pd.Categorical(data_pd[i])\n  data_frame_Dummies = pd.get_dummies(data_pd[i], prefix = i)\n  total_data = pd.concat([total_data, data_frame_Dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_normalized = total_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compute the time that the Algorithm takes in calculates the rules for this excercise of Association:\nimport time\nfrom mlxtend.frequent_patterns import apriori\n\nstart_time = time.time()\nfrequent_apriori = apriori(total_data, min_support=0.65, use_colnames=True)\nprint(\"Execution time by Apriori Algorithm  %s\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying a confidence of 90% and a minimum of support 65%, find the following rules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.frequent_patterns import association_rules\n\nreglas_asociacion_dataset=association_rules(frequent_apriori, metric=\"confidence\", min_threshold=0.8)\ndisplay(reglas_asociacion_dataset.sort_values(by = ['support','confidence'], ascending = [False,False]))\nprint(\"Total reglas encontradas %s\" % (reglas_asociacion_dataset.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2 Clustering\n\nSegmentation of people according to personal features and conditions of life. "},{"metadata":{},"cell_type":"markdown","source":"### 4.2.1. K-Means Clustering\nIt executes the k-Means algorithm through a python implementation with the data without normalize. As input parameters, We specify 6 clusters (Number of K)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X = data_pd_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nn_clusters = 1\nkm = KMeans( n_clusters=n_clusters)\nkm.fit(X)\ny = km.predict(X)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It calculates the size of Cluster\npd.Series(y).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List the centroids of the clusters\nkm.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_original.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We decide to select some columns to separate the data and We can print the clusters very well created.\n\nX_filtered = X.filter(['save_asset','durable_asset', 'save_asset',\n                      'living_expenses','other_expenses','incoming_agricultural',\n                       'farm_expenses','lasting_investment','no_lasting_investmen']).values+15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 3\n\nkm = KMeans( n_clusters=n_clusters)\nkm.fit(X_filtered)\ny_filtered = km.predict(X_filtered)\n\nprint(X_filtered.shape)\nprint(y_filtered.shape)\n\n#Print the lists of the centroids\nkm.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the groups of the centroids\n\nimport numpy as np\ncmap = plt.cm.plasma\n\ncmap((y_filtered*255./(n_clusters-1)).astype(int))\nfor i in np.unique(y_filtered):\n    cmap = plt.cm.bwr\n    col = cmap((i*255./(n_clusters-1)).astype(int))\n    Xr = X_filtered[y_filtered==i]\n    plt.scatter(Xr[:,0], Xr[:,1], color=col, label=\"cluster %d\"%i, alpha=.5)\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1],marker=\"x\", lw=5, s=200, color=\"black\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To determine the number of K with SSE (Sum of Square Error)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sum_of_squared_distances = []\n\n#It iterates\nK = range(2,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(X_filtered)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow Graphic shows that the k estimated value for k is 8 aproximalety."},{"metadata":{},"cell_type":"markdown","source":"K-Means with other Dimentions: \"Age, Number_children, education_level, total_members\""},{"metadata":{"trusted":true},"cell_type":"code","source":"X_filtered_2 = X.filter(['Age', 'Number_children', 'education_level','total_members']).values+15\n\n\ndef plot_kmeans(n_clusters):\n    km = KMeans( n_clusters=n_clusters)\n    km.fit(X_filtered_2)\n    y_filtered_2 = km.predict(X_filtered_2)\n\n    # Dibuja los grupos con sus centroides\n    cmap = plt.cm.plasma\n\n    cmap((y_filtered*255./(n_clusters-1)).astype(int))\n    for i in np.unique(y_filtered_2):\n        cmap = plt.cm.bwr\n        col = cmap((i*255./(n_clusters-1)).astype(int))\n        Xr = X_filtered_2[y_filtered_2==i]\n        plt.scatter(Xr[:,0], Xr[:,1], color=col, label=\"cluster %d\"%i, alpha=.5)\n    plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1],marker=\"x\", lw=5, s=200, color=\"black\")\n    plt.legend()\n\nplot_kmeans(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_kmeans(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2.2. Validation Metrics for Clustering\n\n#### 4.2.2.1. Internal Validation Metrics\n\n#### 4.2.2.1. Sum of Squared Errors SSE "},{"metadata":{"trusted":true},"cell_type":"code","source":"Sum_of_squared_distances = []\n\n#Se itera \nK = range(2,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(X_filtered_2)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2.2. Silhouette Coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_samples, silhouette_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2.3.1. External Metrical Validation\n\nThe validation is throuhg the Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\nfrom sklearn.utils.multiclass import unique_labels\n\nX_filtered_3 = X.copy()\n\ny_true = X_filtered_3['depressed']\nclass_names = ['Depressed','No Depressed']\n\nkm = KMeans(n_clusters=2)\nkm = km.fit(X_filtered_2)\ny_pred = km.labels_\n\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some more \"intuitive\" the Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False, cmap=plt.cm.Blues):\n\n    title = 'Confusion Matrix'\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True Label',\n           xlabel='Predicted Label')\n\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"red\" if cm[i, j] > thresh else \"red\")\n    fig.tight_layout()\n    return ax\n\nnp.set_printoptions(precision=2)\nplot_confusion_matrix(y_true, y_pred, classes=class_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.3.1.1. Purity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_purity(confusion_matrix):\n    maximus = []\n    purity_score = 0\n    for i in range(0,confusion_matrix.shape[0]):\n        maximus.append(np.max(cm[i]))\n    purity_score = np.sum(maximus)/np.sum(confusion_matrix)\n    return purity_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"purity_score= compute_purity(cm)\nprint(\"The purity calculated is: \", purity_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, it computes the Precision, Recall and F1 Score:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluar la precisión:\nprecision = precision_score(y_true, y_pred, average='macro') \nrecall = recall_score(y_true, y_pred, average='macro')  \nf1score = f1_score(y_true, y_pred, average='macro')  \n\nprint(\"The Precision calculated was \", precision)\nprint(\"The Recall calculated was: \", recall)\nprint(\"The F1-Score calculated was: \", f1score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3. Classification\nIs this section, We can analyse the supervised learning. Our dataset has a target class, the variable is \"depressed\". Some models have been applied to seek what is the best model to fit to our data. \n\n## 4.3.1. Bayesian Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_copy = X.copy()\ny = X.depressed.values\nX = X.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partition of the dataset using 70% to training and 30% to Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Se parten los datos para usar 70 Entrenamiento y 30 test:\nX_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = train_test_split(X, y,\n                                                    test_size=.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.1.1. Training the Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\nmodel = GaussianNB()\nmodel.fit(X_train_bayes, y_train_bayes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.1.2. To apply the new model to the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, classification_report\ny_pred_bayes = model.predict(X_test_bayes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.1.3. Model Validation with Confusion Matrix, Accuracy, Clasiffication error, Precision, Recall and f1-score"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_bayes, y_pred_bayes)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_bayes, digits=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3.2. Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the libraries required\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = LogisticRegression(class_weight=\"balanced\", random_state=1, max_iter=1000, solver=\"liblinear\")\n\nclassifier.fit(X_train_bayes, y_train_bayes)\ny_pred_logistic = classifier.predict(X_test_bayes)\nscore = classifier.score(X_test_bayes, y_test_bayes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_bayes, y_pred_logistic)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_logistic, digits=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3.3. Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(y_test, y_pred)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3.3. Multi-Layer Perceptron Neural Network Mode\n\nThe following guides We used to implement this MLP neural network:\n\n[1. tutorial-first-neural-network-python-keras](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/) <br>\n[2. build-multi-layer-perceptron-neural-network-models-keras](https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/) <br>\n[3. basic-keras-neural-network-sequential-model](https://www.kdnuggets.com/2018/06/basic-keras-neural-network-sequential-model.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3.3.1. Defining the model in Keras framework\n\n![](https://www.luisllamas.es/wp-content/uploads/2019/02/tensorflowkeras.png)"},{"metadata":{},"cell_type":"markdown","source":"Keras is a framework where you can create the architecture of the network in a very intuitive way, hidding the complex details. The following lines of code shows the Network implementation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential() # Creation of the model\nmodel.add(Dense(units= 128, input_dim = 21, activation = 'relu')) # input_dim = Variables or attributes\nmodel.add(Dense(units = 64, activation='relu')) # First activation Layer\nmodel.add(Dense(units = 8, activation='relu')) # Second activation Layer\nmodel.add(Dense(units = 1, activation='sigmoid')) # Sigmoid activation function for binary classification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define the optimizer as the estocastic gradient descent efficient algorithm (adam). \nCompiling the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the model in Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_MLP = model.fit(X_train_bayes, y_train_bayes, \n                       validation_data=([X_test_bayes],[y_test_bayes]) ,epochs=150, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, accuracy = model.evaluate(X_train_bayes, y_train_bayes)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Showing the graphics the curves of learning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred_MLP.history.keys())\n\n# summarize history for accuracy\nplt.plot(y_pred_MLP.history['accuracy'])\nplt.plot(y_pred_MLP.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(y_pred_MLP.history['loss'])\nplt.plot(y_pred_MLP.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_NN = model.predict(X_test_bayes, batch_size=50, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_NN.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_NN_bool = np.argmax(y_pred_NN, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_bayes, y_pred_NN_bool)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_NN_bool, digits=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are vey bad for our target class (\"No depressed\"). The accuracy of the model is 86%, but this imbalance dataset We have to check the F1-Score for the second class. In this case, We got 0% for this target class. "},{"metadata":{},"cell_type":"markdown","source":"### 4.3.3.2. Normalization of the Dimensions\n\nThe normalization of the data made through z-score (Gaussian normalization)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save depressed variable and after pup again,\ny_target = X_copy['depressed'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(y_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_normalized.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To Apply the Random Forest Model Again:\nX = depression_normalized.values\ny = y_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nindexes = []\nscores = []\ncv = KFold(n_splits=5, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index:\", train_index)\n    print(\"Test  Index:\", test_index)\n    X_train, X_test, y_train, y_test = X[train_index],X[test_index],y[train_index], y[test_index]\n    rf = RandomForestClassifier()\n    rf.fit(X_train,y_train)\n    scores.append(rf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = [0,1,2,3,4,5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Printing the Training for each fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(scores)\nplt.title('K fold - Cross Validation')\nplt.ylabel('Acuracy')\nplt.xlabel('Fold')\nplt.legend(['Training for each fold'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}