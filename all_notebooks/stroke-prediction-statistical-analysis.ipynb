{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NOTE","metadata":{}},{"cell_type":"markdown","source":"This is the 2nd part of the series where I explore and model the stroke prediction dataset. <br>\nHere are the notebooks in the series in the correct order:\n\n1. **[Exploratory Data Analysis (EDA)](https://www.kaggle.com/ansonnn/stroke-prediction-eda)** <br> - Exploring the dataset to derive insights about distributions and relationships between features.<br><br>\n2. **Statistical Analysis** - current notebook <br> - Analyzing the normality of the features and their correlations. <br><br>\n3. **[Feature Engineering and Modelling](https://www.kaggle.com/ansonnn/stroke-prediction-modelling)** <br> - Preprocessing the features and building a model for evaluation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Load necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Enable multiple cell outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'last'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting some visualization parameters\nplt.style.use('seaborn')\nmpl.rcParams['figure.figsize'] = (8, 8)\nplt.rcParams['axes.labelsize'] = 15\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_preprocess_df():\n    df = pd.read_csv('../input/stroke-categorical/stroke_det_cat.csv')\n    cats = list(df.select_dtypes(include=['object', 'category']).columns)\n    nums = list(df.select_dtypes(exclude=['object', 'category']).columns)\n    \n    features_to_conv = ['bmi_range', 'hypertension', 'heart_disease', 'stroke']\n    cats.extend(features_to_conv)\n    for feature in features_to_conv:\n        if feature in nums:\n            nums.remove(feature)\n    print(f'Categorical variables:  {cats}')\n    print(f'Numerical variables:  {nums}')\n    df = df.astype({i: 'category' for i in cats})\n    df = pd.concat([df[cats], df[nums]], axis=1)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = load_preprocess_df()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'bmi_range', 'hypertension', 'heart_disease', 'stroke']\nnum_features = ['age', 'avg_glucose_level', 'bmi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting significance level as 0.05\nALPHA = 0.05","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical Analysis on Categorical Features","metadata":{}},{"cell_type":"markdown","source":"**Hypothesis definition** <br>\nNull hypothesis, $H_0$ : Variable A and Variable B are **independent** <br>\nAlternative hypothesis, $H_1$ : Variable A and Variable B are **dependent** <br>\n <br>\nSignificance level, $\\alpha$ is generally set to 0.05. <br>\n\n- p-value $\\le$  $\\alpha$ : significant result, reject null hypothesis.\n- p-value $\\gt$  $\\alpha$ : not significant result, fail to reject null hypothesis. <br><br>\n- To analyze whether there is a significant association between two categorical features, **Pearson's chi-squared test** analysis is commonly used.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat_label =  pd.concat([df.loc[:, 'gender': 'smoking_status'].apply(lambda x: LabelEncoder().fit_transform(x)),\n                           df.loc[:, 'bmi_range': 'stroke']], axis=1)\n\ndf_cat_label.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi2_res = chi2(df_cat_label, df.stroke)\ndf_chi2 = pd.DataFrame({'target': 'stroke',\n                        'cat_feature': df_cat_label.columns,\n                        'chi2': chi2_res[0],\n                        'p-value': chi2_res[1],\n                        'rounded p': np.around(chi2_res[1], 3),\n                        'alpha': ALPHA})\n# df_chi2 = df_chi2.drop(df['cat_feature'=='stroke'], axis=0).reset_index(drop=True)\ndf_chi2['H0'] = df_chi2['p-value'].apply(lambda x: 'Rejected' if x <= ALPHA else 'Fail to reject')\ndf_chi2['relation'] = df_chi2['H0'].apply(lambda x: 'Dependent' if x=='Rejected' else 'Independent')\ndf_chi2.sort_values(by='chi2', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- When analyzing the independence between the `target` variable (`stroke` in our case) and other cat. features, <br>\nMost of the features are dependent on each other except for `stroke` VS `gender`/`work_type`/`Residence_type`/`smoking_status`\n- Particularly, `stroke` is very dependent on `heart_disease` and `hypertension`, with relatively very large `chi2` values","metadata":{}},{"cell_type":"markdown","source":"# Statistical Analysis on Numerical Features","metadata":{}},{"cell_type":"markdown","source":"## Visual Normality Checks","metadata":{}},{"cell_type":"markdown","source":"- Checking using histograms with kernel density estimate plots","metadata":{}},{"cell_type":"code","source":"num_features = ['age', 'avg_glucose_level', 'bmi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_nums = df[num_features]\ndf_nums.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the overall theme for the notebook\nsns.set_theme(style='darkgrid', palette='tab10', font_scale=1.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(1, (18, 5))\n\nfor i, feature in enumerate(num_features):\n    ax = plt.subplot(1, 3, i+1)\n    # sns.displot(data=df_nums, x=feature, kde=True)\n    sns.histplot(data=df_nums, x=feature, kde=True, bins=50)\n    ax.set_xlabel(None)\n    ax.set_title(f'Distribution of {feature}')\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Already checked in EDA previously. Only `age` is similar to normal distribution.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\nA **log-normal distribution** is a distribution of a random variable whose logarithm is normally distributed.\n\nChecking whether these features are log-normal or not.","metadata":{}},{"cell_type":"code","source":"df_log = np.log2(df_nums)\ndf_log.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(1, (18, 5))\n\nfor i, num_feature in enumerate(num_features):\n    ax = plt.subplot(1, 3, i+1)\n    ax.set_title(f'Distribution of log-transformed {num_feature}', size=15)\n    sns.histplot(data=df_log, x=num_feature, kde=True, bins=50)\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After log transformation, `age` has become significantly left-skewed, `avg_glucose_level` is not log-normal too.\n- Only `bmi` has become visually very similar to a log-normal distribution, note that the higher peak in the middle is likely due to the median imputation for the missing values.","metadata":{}},{"cell_type":"markdown","source":"**Using quantile-quantile plot (Q-Q Plot) to visualize their normality**","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.gofplots import qqplot\n# quantile-quantile plots on original data\nfig = plt.figure(1, (18, 8))\n\nfor i, num_feature in enumerate(df_nums):\n    ax = plt.subplot(2, 3, i+1)\n    qqplot(df_nums[num_feature], line='s', ax=ax, markersize=5)\n    ax.set_title(f'qqplot - {num_feature}')\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at whether the points fall on the expected best fit line (red line, mean=0, std=1), normality can be determined. <br>\n- `age` is half-way to become a normal distribution, with less data at extreme values (points at left side are above the expected best line, points at right side are below)\n- `bmi` also generally seems to be normal distribution, but with some exceptions at extreme values.","metadata":{}},{"cell_type":"markdown","source":"## Hypothesis Tests for Normality","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n# let's contruct a function\ndef shapiro_wilk_test(df, cols, alpha=0.05):\n    # test the null hypothesis for columns given in `cols` of the dataframe `df` under significance level `alpha`.\n    for col in cols:\n        w, p = stats.shapiro(df[col])\n        if p <= alpha:\n            print(f\"Rejected H0 under significance level {alpha}\")\n            print(f\"{col} differs significantly from a normal distribution (W={w:.3f}, p={p:.5f})\\n\")\n        else:\n            print(f\"\\nFail to reject H0 due to lack of evidence under significance level {alpha}\\n\")\n            print(f\"{col} seem to be normally distributed (W={w:.3f}, p={p:.5f})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shapiro_wilk_test(df_nums, num_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Shapiro-Wilk Test has a problem for detecting normality when the sample size is large (>5000), which is typical for most tests","metadata":{}},{"cell_type":"markdown","source":"- Trying to apply `power transform` to make the data more Gaussian-like (normal-like). Default method is `Yeo-Johnson` transform.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer()\ntransformed_df = pt.fit_transform(df_nums)\ntransformed_df = pd.DataFrame(transformed_df, columns=df_nums.columns)\ntransformed_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(1, (18, 5))\n\nfor i, num_feature in enumerate(num_features):\n    ax = plt.subplot(1, 3, i+1)\n    ax.set_title(f'Distribution of power-transformed {num_feature}', size=15)\n    sns.histplot(data=transformed_df, x=num_feature, kde=True, bins=50)\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(1, (18,8))\n\nfor i, num_feature in enumerate(df_nums):\n    ax = plt.subplot(2,3,i+1)\n    qqplot(transformed_df[num_feature], line='s', ax=ax, markersize=5)\n    ax.set_title(f'qqplot - {num_feature}')\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `age` is similar to before transformation, while `avg_glucose_level` has become more normal-like but still far from ideal.\n- `bmi` has become almost entirely normal distribution after the power transformation.","metadata":{}},{"cell_type":"code","source":"shapiro_wilk_test(transformed_df, num_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Even after transforming, all of them are still deemed significantly different from normal distribution through the Shapiro-Wilk Test, <br> \nthis is most likely due to the limitation of the test.\n- However, visually, it is evident to tell that `age` and `bmi` can be considered normal distribution, while `avg_glucose_level` is close to normal distribution after power transformation.\n- Therefore, hypothesis tests of `t-test` or `ANOVA` can still be conducted.\n\n\n- Generally, Shapiro-Wilk Test is not a good way to determine whether they are Gaussian \"enough\" to conduct such parametric hypothesis tests. \n- Quantile-quantile plot visualization is better for this.","metadata":{}},{"cell_type":"markdown","source":"## Hypothesis Test for Correlation","metadata":{}},{"cell_type":"markdown","source":"**Hypothesis definition** <br>\nNull hypothesis, $H_0$ : Variable A and Variable B are **uncorrelated** <br>\nAlternative hypothesis, $H_1$ : Variable A and Variable B are **correlated** <br>\n <br>\nSignificance level, $\\alpha$ is generally set to 0.05. <br>\n\n- p-value $\\le$  $\\alpha$ : significant result, reject null hypothesis.\n- p-value $\\gt$  $\\alpha$ : not significant result, fail to reject null hypothesis. <br><br>\n- To analyze whether there is a significant association between two numerical features, **Pearson's/Spearman's correlation** are commonly used. <br>\n- Such methods are for statistical hypothesis test, therefore they assume that the samples are uncorrelated (fail to reject $H_0$).","metadata":{}},{"cell_type":"markdown","source":"**Assumptions of pearson correlation**:\n1. Both variables should have a Gaussian or Gaussian-like distribution.\n2. Relationship between the variables should be linear.\n3. Homoscedasticity i.e., a sequence of random variables where all its random variables have the same finite variance.\n\nAlso Pearson is quite sensitive to outliers. So in this case, Spearman's correlation is preferred since there are many outliers for `avg_glucose_level` and `bmi`.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import pearsonr, spearmanr, kendalltau\nfrom itertools import combinations\n\nlist(combinations(num_features, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_correlation(x1, x2, method='spearman', alpha=ALPHA):\n    # this function returns correlation, p-value and H0 for `x1` & `x2`\n    \n    ALLOWED_METHODS = ['pearson', 'spearman', 'kendall']\n    if method not in ALLOWED_METHODS:\n        raise ValueError(f'Allowed methods are {ALLOWED_METHODS}')\n        \n    if method =='pearson':\n        corr, p = pearsonr(x1, x2)\n    elif method =='spearman':\n        corr, p = spearmanr(x1, x2)\n    else:\n        corr, p = kendalltau(x1, x2)\n    \n    h0 = 'Rejected' if p <= alpha else 'Fail to reject'\n    \n    return corr, p, h0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_corr = pd.DataFrame(columns=['feature_1', 'feature_2', 'correlation', 'p', 'H0'])\n\nfor x, y in combinations(num_features, r=2):\n    corr, p, h0 = test_correlation(df_nums[x], df_nums[y])\n    \n    df_corr = df_corr.append({'feature_1': x, 'feature_2': y,\n                              'correlation': round(corr, 5), 'p': p, 'H0': h0}, ignore_index=True)\n    \ndf_corr.style.highlight_max(color='navy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All of their null hypothesis are rejected, therefore all of the numerical features are correlated to each other. \n- With `age` and `bmi` being moderately more correlated than the other 2 combinations","metadata":{}},{"cell_type":"markdown","source":"# Statistical Significance Tests","metadata":{}},{"cell_type":"markdown","source":"**Parametric Statistical Significance Tests** <br>\n1. **Studentâ€™s t-test** - It tests whether the two independent normal distributed samples have the same mean or not. <br>\n2. **Analysis of Variance test (ANOVA)** - It tests whether the two or more independent normal distributed samples have the same mean or not. <br>\nANOVA serves the same purpose with t-test but for more than two variables instead of only two variables. So either apply t-test pair-wise or apply ANOVA once.  <br>\nANOVA also only tells whether all the samples have the same mean or not, it doesn't quantify which samples differ or by how much.\n\n**Non-Parametric Statistical Significance Tests** <br>\n1. **Mann-Whitney U test** - Non-parametric equivalent of **Student's t-test**. <br>\n2. **Kruskal-Wallis H test** - Non-parametric equivalent of **ANOVA** (but it compares the median instead). <br>\n\nParametric tests are for normally distributed data, while non-parametric tests are for non-normally distributed data. <br>\nHowever, in this case, the Shapiro-Wilk test is not a good way for determine whether they are normally distributed or not. <br>\nWe have already verified that the 3 numerical features `age`, `avg_glucose_level` and `bmi` are similar to normal distribution in the Q-Q plots. <br>\nTherefore, it is generally good enough to apply parametric tests by inspecting the visualizations.","metadata":{}},{"cell_type":"code","source":"num_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cat_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## t-tests","metadata":{}},{"cell_type":"code","source":"from scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = df[df.gender=='Male'].age, df[df.gender=='Female'].age\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different Age distributions between Male and Female (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same Age distributions between Male and Female (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The p-value is 0.0499 which is ***very close*** to **fail to reject H0**, therefore they can actually be considered as borderline similar distribution.","metadata":{}},{"cell_type":"code","source":"x, y = df[df.gender=='Male'].avg_glucose_level, df[df.gender=='Female'].avg_glucose_level\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different avg_glucose_level distributions between Male and Female (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same avg_glucose_level distributions between Male and Female (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = df[df.gender=='Male'].bmi, df[df.gender=='Female'].bmi\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different BMI distributions between Male and Female (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same BMI distributions between Male and Female (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = df[df.stroke==1].age, df[df.stroke==0].age\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different Age distributions between Stroke and \"No stroke\" (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same Age distributions between Stroke and \"No stroke\" (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = df[df.stroke==1].avg_glucose_level, df[df.stroke==0].avg_glucose_level\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different avg_glucose_level distributions between Stroke and \"No stroke\" (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same avg_glucose_level distributions between Stroke and \"No stroke\" (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = df[df.stroke==1].bmi, df[df.stroke==0].bmi\n\nt, p = stats.ttest_ind(x, y, equal_var=False)\nt, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different BMI distributions between Stroke and \"No stroke\" (t={t}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same BMI distributions between Stroke and \"No stroke\" (t={t}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary Tables for t-test Results","metadata":{}},{"cell_type":"code","source":"t_test_df = pd.DataFrame(columns=['cat_feat', 'num_feat', 'alpha', 't', 'p'])\n\nfor feature in num_features:\n    t, p = stats.ttest_ind(df.loc[df.gender=='Male', feature],\n                            df.loc[df.gender=='Female', feature],\n                            equal_var=False)\n    t, p = round(t, 3), round(p, 5)\n    t_test_df = t_test_df.append({'cat_feat': 'gender', 'num_feat': feature,\n                    'alpha': ALPHA, 't': t, 'p': p}, ignore_index=True)\n\nt_test_df['H0'] = t_test_df.p.apply(lambda x: 'Rejected' if x <= ALPHA else 'Fail to reject')\nt_test_df['relation'] = t_test_df.H0.apply(lambda x: 'Different' if x =='Rejected' else 'Same')\n\ndef color_fail_to_rej(value):\n    if value in ('Fail to reject', 'Same'):\n        return 'color:green'\n    else:\n        return 'cmap: Blues'\n    \nt_test_df.style.background_gradient(cmap = 'Blues').applymap(color_fail_to_rej)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_test_df = pd.DataFrame(columns=['cat_feat', 'num_feat', 'alpha', 't', 'p'])\n\nfor feature in num_features:\n    t, p = stats.ttest_ind(df.loc[df.stroke==0, feature],\n                            df.loc[df.stroke==1, feature],\n                            equal_var=False)\n    t, p = round(t, 3), round(p, 5)\n    t_test_df = t_test_df.append({'cat_feat': 'stroke', 'num_feat': feature,\n                    'alpha': ALPHA, 't': t, 'p': p}, ignore_index=True)\n\nt_test_df['H0'] = t_test_df.p.apply(lambda x: 'Rejected' if x <= ALPHA else 'Fail to reject')\nt_test_df['relation'] = t_test_df.H0.apply(lambda x: 'Different' if x =='Rejected' else 'Same')\nt_test_df.style.background_gradient(cmap = 'Blues').applymap(color_fail_to_rej)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-way ANOVA tests","metadata":{}},{"cell_type":"code","source":"smoking_age_group = df.groupby('smoking_status')['age'].unique()\nsmoking_age_group","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_statistic, p = stats.f_oneway(*smoking_age_group)\nf_statistic, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different Age distributions among different smoking_status (F={f_statistic}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same Age distributions among different smoking_status (F={f_statistic}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bmi_range_group = df.groupby('bmi_range')['age'].unique()\nf_statistic, p = stats.f_oneway(*bmi_range_group)\nf_statistic, p = round(t, 5), round(p, 5)\n\nif p <= ALPHA:\n    print(f'Since p = {p} < {ALPHA}, reject H0, different Age distributions among different bmi_range (F={f_statistic}, p={p})')\nelse:\n    print(f'Since p = {p} > {ALPHA}, fail to reject H0, same Age distributions among different bmi_range (F={f_statistic}, p={p}).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\n- For the important parts, only `Male` and `Female` has similar distribution in terms of the mean values of `age` and `bmi`.\n- With respect to `stroke`, all the numerical features have different distributions.","metadata":{}},{"cell_type":"markdown","source":"# Summary of Relationship between Categorical and Numerical Features","metadata":{}},{"cell_type":"code","source":"from sklearn import feature_selection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, p = feature_selection.f_classif(df.loc[:, 'age': 'bmi'], df.gender)\nf, p = np.around(f, 3), np.around(p, 5)\nanova_test_cat = pd.DataFrame({\n    'cat-attr': 'gender',\n    'cont-attr': df.loc[:, 'age': 'bmi'].columns,\n    'f': f, \n    'p': p,\n    'alpha': ALPHA\n})\nanova_test_cat['H0'] = anova_test_cat.p.apply(lambda x: 'Rejected' if x <= ALPHA else 'Fail to reject')\nanova_test_cat['relation'] = anova_test_cat.H0.apply(lambda x: 'Different' if x =='Rejected' else 'Same')\nanova_test_cat.style.background_gradient(cmap = 'Blues').applymap(color_fail_to_rej)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, p = feature_selection.f_classif(df.loc[:, 'age': 'bmi'], df.stroke)\nf, p = np.around(f, 3), np.around(p, 5)\nanova_test_cat = pd.DataFrame({\n    'cat-attr': 'stroke',\n    'cont-attr': df.loc[:, 'age': 'bmi'].columns,\n    'f': f, \n    'p': p,\n    'alpha': ALPHA\n})\nanova_test_cat['H0'] = anova_test_cat.p.apply(lambda x: 'Rejected' if x <= ALPHA else 'Fail to reject')\nanova_test_cat['relation'] = anova_test_cat.H0.apply(lambda x: 'Different' if x =='Rejected' else 'Same')\nanova_test_cat.style.background_gradient(cmap = 'Blues').applymap(color_fail_to_rej)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\n\n- The ANOVA test results are the same with the t-test results, with little difference between the p-values.\n- But it is summarized clearly in table forms here, where **only** the distributions of `gender` and `bmi` are the same.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}