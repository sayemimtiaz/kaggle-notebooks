{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tic Tac Toe Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tictactoe-endgame-dataset-uci/tic-tac-toe-endgame.csv',',')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['V1'],v1 = pd.factorize(df['V1'], sort=True)\ndf['V2'],v2 = pd.factorize(df['V2'], sort=True)\ndf['V3'],v3 = pd.factorize(df['V3'], sort=True)\ndf['V4'],v4 = pd.factorize(df['V4'], sort=True)\ndf['V5'],v5 = pd.factorize(df['V5'], sort=True)\ndf['V6'],v6 = pd.factorize(df['V6'], sort=True)\ndf['V7'],v7 = pd.factorize(df['V7'], sort=True)\ndf['V8'],v8 = pd.factorize(df['V8'], sort=True)\ndf['V9'],v9 = pd.factorize(df['V9'], sort=True)\ndf['V10'],v10 = pd.factorize(df['V10'], sort=True)\nprint(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = [v10[0], v10[1]]\nclass_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop('V10',axis=1)\ny = df['V10']\n\n# Vamos separar (split) nossos dados em conj. de dados para treinamento e testes..\nx_train, x_test, y_train, y_test = train_test_split(x, y)\n[x_train.shape, x_test.shape, y_train.shape, y_test.shape]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify_mean_accuracy(max_n, max_c):\n    \n    max_range = range(1, max_n)\n    cases = range(0, max_c)\n    funcs = ['relu', 'logistic', 'tanh']\n    \n    best_func = 'relu'\n    best_g_sum = 0\n    best_g = 0\n    best_g_n = 0\n    \n    for func in funcs:\n        \n        best_list = []\n        best_sum = 0\n        best_c = 0\n        best_c_n = 0\n        \n        for case in cases:\n            \n            best = 0\n            best_n = 0\n            accuracy_list = []\n            \n            for n in max_range:\n                mlp = MLPClassifier(solver='lbfgs', activation=func, hidden_layer_sizes=(n))\n                mlp.fit(x_train, y_train)\n                y_pred = mlp.predict(x_test)\n                score = accuracy_score(y_test, y_pred)\n                accuracy_list.append(score)\n                \n                if score > best:\n                    best = score\n                    best_n = n\n            \n            accuracy_sum = sum(accuracy_list)\n            \n            if accuracy_sum > best_sum:\n                best_sum = accuracy_sum\n                best_list = accuracy_list\n                best_c = best\n                best_c_n = best_n\n                \n        if best_sum > best_g_sum:\n            best_g_sum = best_sum\n            best_g = best_c\n            best_g_n = best_c_n\n            best_func = func\n                \n        plt.plot(max_range, best_list, label=func)\n    \n    plt.title('Activation function: relu x logistic x tanh')\n    plt.xlabel('Number of neurons')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return (best_g, best_g_n, best_func)\n\ndef classify_point_accuracy(max_n, max_c):\n      \n    max_range = range(1, max_n)\n    cases = range(0, max_c)\n    funcs = ['relu', 'logistic', 'tanh']\n    colors = ['ro', 'go', 'bo']\n    \n    best = 0\n    best_n = 0\n    best_mlp = None\n    c = 0\n    \n    for func in funcs:\n        \n        color = colors[c]\n        c += 1\n        \n        for case in cases:\n            \n            accuracy_list = []\n            \n            for n in max_range:\n                mlp = MLPClassifier(solver='lbfgs', activation=func, hidden_layer_sizes=(n))\n                mlp.fit(x_train, y_train)\n                y_pred = mlp.predict(x_test)\n                score = accuracy_score(y_test, y_pred)\n                accuracy_list.append(score)\n                \n                if score > best:\n                    best = score\n                    best_n = n\n                    best_mlp = mlp\n            \n            plt.plot(max_range, accuracy_list, color, label='{}{}'.format(func, case))\n    \n    plt.title('Activation function: relu x logistic x tanh')\n    plt.xlabel('Number of neurons')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return (best, best_n, best_mlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_mean_results = classify_mean_accuracy(10, 10) # usually the best amount of neurons is found between the input and output size. Only one hidden layer is required for most feedback neural networks\nbest_mean_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_point_results = classify_point_accuracy(10, 10) # usually the best amount of neurons is found between the input and output size. Only one hidden layer is required for most feedback neural networks\nbest_point_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First try out the best punctual accuracy mlp found. It usually brings results of at least 85%\nmlp = best_point_results[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the model to make predictions with the test data\ny_pred = mlp.predict(x_test)\n# how did our model perform?\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}/{}'.format(count_misclassified, len(y_test)))\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Second try out the best mean accuracy data into a new mlp and check results. They are usually at least 75%\nmlp = MLPClassifier(solver='lbfgs', activation=best_mean_results[2], hidden_layer_sizes=best_mean_results[1])\nmlp.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the model to make predictions with the test data\ny_pred = mlp.predict(x_test)\n# how did our model perform?\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}/{}'.format(count_misclassified, len(y_test)))\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}