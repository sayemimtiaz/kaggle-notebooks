{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install spacy==2.3.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install https://med7.s3.eu-west-2.amazonaws.com/en_core_med7_lg.tar.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Syntax\n\nSyntax is the structure of a language which is governed by grammers. Any ordering of words can not be a sentence. Hence, we need syntactical analysis for natural languages.\n\n## Table of Contents\n\n* [Parts of Speech Tagging](#pos)\n* [Dependency Parsing](#parsing)\n* [Named Entity Recognition](#ner)","metadata":{}},{"cell_type":"markdown","source":"<a id='pos'></a>\n\n# 1. Parts of Speech Tagging\n\nParts of speech (POS) are specific lexical categories to which words are assigned, based on their syntactic context and role. Usually, words can fall into one of the following major categories.\n\n* <strong>Nouns</strong>\n* <strong>Verb</strong>\n* <strong>Adjective</strong>\n* <strong>Adverb</strong>\n\nBesides these four major categories of parts of speech , there are other categories that occur frequently in the English language. These include pronouns, prepositions, interjections, conjunctions, determiners, and many others. The process of classifying and labeling POS tags for words called parts of speech tagging or POS tagging . POS tags are used to annotate words and depict their POS, which is really helpful to perform specific analysis, such as narrowing down upon nouns and seeing which ones are the most prominent, word sense disambiguation, and grammar analysis.\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/nlp-specialization-data/Cleaned_POS_Medical_Notes.csv') #for excel file use read_excel\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport spacy\nimport en_core_med7_lg #en_core_web_sm\nimport re\n\nnlp=en_core_med7_lg.load()\n#nlp = spacy.load('en_core_med7_lg', parse=True, tag=True, entity=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = data.clean_text.iloc[1]\nprint (sample_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tokenized = nlp(sample_text)\n\nfor token in text_tokenized:\n    print (\"{} ---> {}\".format(token,token.pos_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Usually POS tags are used for analysis, feature engineering or, feature selection. In this analysis, let us select only the words that are nouns, verbs, number and adjectives.","metadata":{}},{"cell_type":"code","source":"def get_selected_pos(text):\n    text_tokenized = nlp(text)\n    selected_words = [token.string for token in text_tokenized if token.pos_ in ['NOUN','PROPN','NUM','ADJ','VERB','PUNCT']]\n    processed_text = re.sub(' +',' ', \" \".join(selected_words))\n    return processed_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna(subset=['clean_text'])\ndata.clean_text = data.clean_text.apply(get_selected_pos)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='parsing'></a>\n\n# 2. Dependency Parsing\n\nIn dependency parsing, we try to use dependency-based grammars to analyze and infer both structure and semantic dependencies and relationships between tokens in a sentence. The basic principle behind a dependency grammar is that in any sentence in the language, all words except one, have some relationship or dependency on other words in the sentence. The word that has no dependency is called the root of the sentence. The verb is taken as the root of the sentence in most cases. All the other words are directly or indirectly linked to the root verb using links , which are the dependencies.","metadata":{}},{"cell_type":"code","source":"from spacy import displacy\ntext_tokenized = nlp(data.clean_text.iloc[1])\n\noptions = {\"compact\": True}\ndisplacy.serve(text_tokenized, style=\"dep\", options=options)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ner'></a>\n\n# 3. Named Entity Recognition (NER)\n\nIn any text document, there are particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities , which more specifically refer to terms that represent real-world objects like people, places, organizations, and so on, which are often denoted by proper names. A naive approach could be to find these by looking at the noun phrases in text documents. Named entity recognition (NER) , also known as entity chunking/extraction , is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.","metadata":{}},{"cell_type":"code","source":"text_tokenized_orig = nlp(data.text.iloc[1])\n#for ent in text_tokenized_orig_eng.ents:\n#    print (\"{} ---> {}\".format(ent.text, ent.label_))\n    \ndisplacy.serve(text_tokenized_orig, style=\"ent\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for ent in text_tokenized.ents:\n#    print (\"{} ---> {}\".format(ent.text, ent.label_))\n    \ndisplacy.serve(text_tokenized, style=\"ent\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# English Language Model\nnlp_eng = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tokenized_orig_eng = nlp_eng(data.text.iloc[1])\n#for ent in text_tokenized_orig_eng.ents:\n#    print (\"{} ---> {}\".format(ent.text, ent.label_))\n    \ndisplacy.serve(text_tokenized_orig_eng, style=\"ent\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tokenized_eng = nlp_eng(data.clean_text.iloc[1])\n#for ent in text_tokenized_eng.ents:\n#    print (\"{} ---> {}\".format(ent.text, ent.label_))\n    \ndisplacy.serve(text_tokenized_eng, style=\"ent\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References for further reading\n\n<strong> POS tagging </strong>\n\n* https://www.nltk.org/book/ch05.html\n\n<strong> Medical named entity recognition </strong>\n\n* https://github.com/kormilitzin/med7\n\n* https://github.com/NLPatVCU/medaCy\n\n* https://github.com/text-machine-lab/CliNER\n","metadata":{}}]}