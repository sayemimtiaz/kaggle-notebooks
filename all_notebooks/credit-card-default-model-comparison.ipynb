{"cells":[{"metadata":{"_uuid":"e69cdf71049fb2288e08289116a0449c867e0978"},"cell_type":"markdown","source":"# Classification of Taiwan Credit Card Payment Default Prediction\n"},{"metadata":{"_uuid":"09c2e954471f1cfa29138363be91bda4e073d3f3"},"cell_type":"markdown","source":"\n\nThis notebook attempts to find the best classification algorithm to predict credit card payment defaults.\n\nThe data set is available in UCI repository: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n     \nClassification algorithms used for model estimations are:\n\n1. Perceptron\n2. LogisticRegression\n3. Decision Tree\n4. Random Forest\n5. K Nearest Neighbor\n6. SVM\n        \n        \n#### GOAL: Notebook should be able to conclude which particular model performs best for the given data set with justifications        "},{"metadata":{"_uuid":"9d8eb75d6237ae12ce026ef823554847abb8279c"},"cell_type":"markdown","source":" ##### Data Set Information:\n\nThis research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\n\n\n##### Attribute Information:\n\nThis research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. \n\nThis study reviewed the literature and used the following 23 variables as explanatory variables: \n\nX1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n\nX2: Gender (1 = male; 2 = female). \n\nX3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n\nX4: Marital status (1 = married; 2 = single; 3 = others). \n\nX5: Age (year). \n\nX6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = \nthe repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n\n\nX12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n\n\nX18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. \n\n"},{"metadata":{"trusted":true,"_uuid":"14c654894028bd6f5ec4ed0a8e6fc63d3dac7c90"},"cell_type":"code","source":"import pandas as panda\n\nfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, classification_report, confusion_matrix, f1_score, roc_curve, roc_auc_score\nfrom sklearn.linear_model import Perceptron, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom matplotlib import pyplot as plot\nimport seaborn as sns\n\n\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\n\nimport itertools, time, datetime\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6e727e631dd57c564cc1b71b47963f40d1ed09"},"cell_type":"code","source":"remote_location = \"../input/UCI_Credit_Card.csv\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e65655c3080159d98f8a0ee58727cd199096f30"},"cell_type":"code","source":"def downLoadData():\n    \"\"\"\n    \n    Downloads the excel data from remote location. Reads one particular sheet, converts all columns names \n    to lower case and then returns the data\n    \n    \"\"\"\n\n    data = panda.read_csv(remote_location)\n\n    data.rename(str.lower, inplace = True, axis = 'columns')\n\n    print(data.dtypes)\n\n    return data\n\ndata = downLoadData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ee9d8088289658fbd3fa4f8d83f7fc1f858f4f5"},"cell_type":"code","source":"## check for varied data types. there may be alphabetical data types or numeric data written as string eg \"4\"\".\n## in such cases reformatting may be required\nprint(data.dtypes.value_counts()) ## all values are numeric and no formatting of data types are required in that case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8d05c8550037d566c1ba770c7897af20810307"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e007137f2d4d1d46df7adc20088c0b1b55448d3d"},"cell_type":"code","source":"\n## since all fields are numeric we can get away with normal describe. else we would have required describe(include='all')\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c7871494aae18f503c6eb64d05283305674d5cf"},"cell_type":"markdown","source":"\n\nThere are simply 25 columns , all numeric values. No further feature selection or extraction may be required. We will go ahead and run algorithms deirectly and calculate accuracies.\n\nHowever before we can do that we will check for class bias. Our target attribute is 'default payment next month', we will check for the value count for the same to see if our data sets is skewed towards one particular class. \n\nWe can see there are almost 4times cases of non default versus default cases. Lets proceed with these analysis and check if we would require to account for class bias during model evaluation\n\nAdditionally, we would also drop the id column, since it adds no value other than row count\n"},{"metadata":{"trusted":true,"_uuid":"420eca9281c013a4a5eb21d7d2f572635ed6e793"},"cell_type":"code","source":"print(data['default.payment.next.month'].value_counts())\n\ndata.drop(['id'], inplace=True, axis =1)\n\n'id' not in data.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cb277a656d254fe0b5d760718a0bc8506428f68"},"cell_type":"markdown","source":"We can see that they are almost 4 times as many non defaulters as there are defaulters. Hence there is a clear non uniform division in classes\n\nThis is important , since we now know that using accuracy score it self will not be important. We will have to use F1 score to estimate our models, since F1 score is a better indicator of classification models when there is a bias in target classes\n"},{"metadata":{"trusted":true,"_uuid":"28ab9ed024a81d866ce30c2874ccbbd906c17af4"},"cell_type":"code","source":"## divide up our x and y axis\n\n_y_target = data['default.payment.next.month'].values\n\ncolumns = data.columns.tolist()\ncolumns.remove('default.payment.next.month')\n\n_x_attributes = data[columns].values\n\n\n## meaning of stratify = _y_target. returns test and training data having the same proportions of class label '_y_target'\n_x_train,_x_test,_y_train, _y_test = train_test_split(_x_attributes, _y_target, test_size =0.30, stratify = _y_target, random_state = 1)\n\n## lets check the distribution. we can see 4times the lower value as was the case before as well. train/test set distributed well\nprint(\"label counts in y train %s\" %bincount(_y_train))\nprint(\"label counts in y test %s\" %bincount(_y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d983e712fa6327bcd16bffe18683dfd27a7c902"},"cell_type":"markdown","source":"\n\nWe have completed out exploratory data analysis and here are our conclusions for the same:\n\n1. Total number of data : 30000\n\n2. Total number of features : 25\n\n3. Target class has only two unique values and expects only two unique values. This would be a problem of binary classification\n\n4. All our features are numeric data type and we will not be needed to deal with any non numeric categorical variables\n\n5. There might be numeric categorical data however we are not performing any correlation/ ANOVA testing to decide on feature selections since the number of features is pretty small\n\n6. Similarly considering the size of data we will not be performing any feature extraction as well.\n\n7. Target classes are biased in their division of classes\n\n\n\nThe next steps would be to actually build out the model and compare metrics across all. In order to do so, we will take the following steps:\n\n\n1. split your data into testing and training set\n\n2. create a list of classifiers\n\n3. create parameters for each of these classifiers. We will be using multiple possible parameters and let the alogrithm decide which is the best parameter providing the best score. eg for perceptron model, we will create 5-6 instances of learning rate and epoch values for parameter and model tuning\n\n4. create a pipeline from step 1 and 2. StandardScaling would be done across all.\n\n5. use gridsearch to perform parameter tuning for each model. Best parameters would be evaluated and we would be using the same parameters to predict values.\n\n6. plot learning curve using the above gridsearch instance so that we can compare our mean testing and training accuracy values and check for bias/variance\n\n7. Plot a bar chart for each model depicting f1 score, accuracy, training times, testing times.  P.S: since our scores are within 0 and 1 , we are going to normalize our time metrics as well using min max method. \n\n8. Additionally display confusion matrix for each model\n\n"},{"metadata":{"trusted":true,"_uuid":"40396d46c68a759e057c247defaee017df04886e"},"cell_type":"code","source":"class CodeTimer:\n    \n    \"\"\"\n        Utility custom contextual class for calculating the time \n        taken for a certain code block to execute\n    \n    \"\"\"\n    def __init__(self, name=None):\n        self.name = \" '\"  + name + \"'\" if name else ''\n\n    def __enter__(self):\n        self.start = time.clock()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (time.clock() - self.start) * 1000.0\n        time_taken = datetime.timedelta(milliseconds = self.took)\n        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78ec6236871c418ea4514584483a69e681cfdb3c"},"cell_type":"code","source":"## cv is essentially value of K in k fold cross validation\n    \n## n_jobs = 1 is  non parallel execution    , -1 is all parallel , any other number say 2 means execute in 2 cpu cores\n\ndef plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n    \n    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n                                                                X = _x_train, \\\n                                                                y = _y_train, \\\n                                                                train_sizes = training_sample_sizes, \\\n                                                                cv = k_fold, \\\n                                                                n_jobs = jobsInParallel) \n\n\n    training_mean = mean(training_score, axis = 1)\n    training_std_deviation = std(training_score, axis = 1)\n    testing_std_deviation = std(testing_score, axis = 1)\n    testing_mean = mean(testing_score, axis = 1 )\n\n    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n    ## to see the accuracy results and check for bias vs variance\n\n    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n\n    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n\n    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n\n    plot.title(\"Scoring of our training and testing data vs sample sizes\")\n    plot.xlabel(\"Number of Samples\")\n    plot.ylabel(\"Accuracy\")\n    plot.legend(loc= 'best')\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b98b11962aa0f3e83177a02f4b5f7bcd32fffe03"},"cell_type":"code","source":"def runGridSearchAndPredict(pipeline, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n    \n    response = {}\n    training_timer       = CodeTimer('training')\n    testing_timer        = CodeTimer('testing')\n    learning_curve_timer = CodeTimer('learning_curve')\n    predict_proba_timer  = CodeTimer('predict_proba')\n    \n    with training_timer:\n        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n\n        search = gridsearch.fit(x_train,y_train)\n\n        print(\"Grid Search Best parameters \", search.best_params_)\n        print(\"Grid Search Best score \", search.best_score_)\n            \n    with testing_timer:\n        y_prediction = gridsearch.predict(x_test)\n            \n    print(\"Accuracy score %s\" %accuracy_score(y_test,y_prediction))\n    print(\"F1 score %s\" %f1_score(y_test,y_prediction))\n    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n    \n    with learning_curve_timer:\n        plotLearningCurve(_x_train, _y_train, search.best_estimator_)\n        \n    with predict_proba_timer:\n        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n            \n            y_probability = gridsearch.predict_proba(x_test)\n            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:,1])\n            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n            response['roc_curve'] = (false_positive_rate, true_positive_rate)\n    \n        else: ## eg SVM, Perceptron doesnt have predict_proba method\n            \n            response['roc_auc_score'] = 0\n            response['roc_curve'] = None\n    \n    response['learning_curve_time'] = learning_curve_timer.took\n    response['testing_time'] = testing_timer.took\n    response['_y_prediction'] = y_prediction\n    response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n    response['training_time'] = training_timer.took\n    response['f1_score']  = f1_score(y_test, y_prediction)\n    \n    \n    return response\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0011c1237ba984f1735d6584b288521ea4be9c"},"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plot.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n    plot.title(title)\n    plot.colorbar()\n    tick_marks = arange(len(classes))\n    plot.xticks(tick_marks, classes, rotation=45)\n    plot.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plot.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plot.ylabel('True label')\n    plot.xlabel('Predicted label')\n#     plot.tight_layout()\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7534b59461d72909fa1c8cef3c53e782d0537b07"},"cell_type":"markdown","source":"We will create a list of classifiers and compare performance. The algorithms selected for classification are as below:\n\n1. Perceptron\n2. SVM\n3. Logistic Regression\n4. SVM Kernel RBF\n5. DecisionTrees\n5. RandomForest\n6. Nearest Neighbour\n\n\n"},{"metadata":{"_uuid":"15a6da70116e5275ff179a9642e5453479385367"},"cell_type":"markdown","source":"**P.S : in the below list,  models starting from random forest classifier till SVC take a huge amount of time. If you would like to run the kernel, i would suggest commenting out any one of the contents and check **"},{"metadata":{"trusted":true,"_uuid":"a86ddacc1c51ded5fd16d3e73ef57a4632b1f501"},"cell_type":"code","source":"\n\nclassifiers = [\n    Perceptron(random_state = 1),\n    LogisticRegression(random_state = 1),\n    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),\n    RandomForestClassifier(random_state = 1, criterion = 'gini'),\n    KNeighborsClassifier(metric = 'minkowski'),\n    SVC(random_state = 1, kernel = 'rbf'),    \n]\n\n\nclassifier_names = [\n            'perceptron',\n            'logisticregression',\n            'decisiontreeclassifier',\n            'randomforestclassifier',\n            'kneighborsclassifier',\n            'svc',               \n    \n]\n\nclassifier_param_grid = [\n            \n            {'perceptron__max_iter': [1,5,8,10], 'perceptron__eta0': [0.5,.4, .2, .1]},\n            {'logisticregression__C':[100,200,300,50,20,600]},\n            {'decisiontreeclassifier__max_depth':[6,7,8,9,10,11]},\n            {'randomforestclassifier__n_estimators':[1,2,3,5,6]} ,\n            {'kneighborsclassifier__n_neighbors':[4,6,7,8]},\n            {'svc__C':[1], 'svc__gamma':[0.01]},\n    \n]\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fde3b6981c3d81bccce7512d2e6100d0132646aa"},"cell_type":"code","source":"\ntimer = CodeTimer(name='overalltime')\nmodel_metrics = {}\n\nwith timer:\n    for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n\n        pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                (model_name, model)\n        ])\n\n        result = runGridSearchAndPredict(pipeline, _x_train, _y_train, _x_test, _y_test, model_param_grid , score = 'f1')\n\n        _y_prediction = result['_y_prediction']\n\n        _matrix = confusion_matrix(y_true = _y_test ,y_pred = _y_prediction)\n\n        model_metrics[model_name] = {}\n        model_metrics[model_name]['confusion_matrix'] = _matrix\n        model_metrics[model_name]['training_time'] = result['training_time']\n        model_metrics[model_name]['testing_time'] = result['testing_time']\n        model_metrics[model_name]['learning_curve_time'] = result['learning_curve_time']\n        model_metrics[model_name]['accuracy_score'] = result['accuracy_score']\n        model_metrics[model_name]['f1_score'] = result['f1_score']\n        model_metrics[model_name]['roc_auc_score'] = result['roc_auc_score']\n        model_metrics[model_name]['roc_curve'] = result['roc_curve']\n        \n        \nprint(timer.took)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d016e8658fcae004f0ff1a80e69318634ebd682d"},"cell_type":"code","source":"\n\nmodel_estimates = panda.DataFrame(model_metrics).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb2848ead02de59914b3f6534e61fc0a4718e00"},"cell_type":"code","source":"\n## convert model_metrics into panda data frame\n## print out across model estimations and accuracy score bar chart\n\n\nmodel_estimates['learning_curve_time'] = model_estimates['learning_curve_time'].astype('float64')\nmodel_estimates['testing_time'] = model_estimates['testing_time'].astype('float64')\nmodel_estimates['training_time'] = model_estimates['training_time'].astype('float64')\nmodel_estimates['f1_score'] = model_estimates['f1_score'].astype('float64')\nmodel_estimates['roc_auc_score'] = model_estimates['roc_auc_score'].astype('float64')\n\n#scaling time parameters between 0 and 1\nmodel_estimates['learning_curve_time'] = (model_estimates['learning_curve_time']- model_estimates['learning_curve_time'].min())/(model_estimates['learning_curve_time'].max()- model_estimates['learning_curve_time'].min())\nmodel_estimates['testing_time'] = (model_estimates['testing_time']- model_estimates['testing_time'].min())/(model_estimates['testing_time'].max()- model_estimates['testing_time'].min())\nmodel_estimates['training_time'] = (model_estimates['training_time']- model_estimates['training_time'].min())/(model_estimates['training_time'].max()- model_estimates['training_time'].min())\n\nprint(model_estimates)\nmodel_estimates.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a90647f4150dec5bca187fcdf4af8d16e010992"},"cell_type":"code","source":"def plotROCCurveAcrossModels(positive_rates_sequence, label_sequence):\n    \n\n    for plot_values, label_name in zip(positive_rates_sequence, label_sequence):\n        \n        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n        \n    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n    plot.title('ROC Curve across models')\n    plot.xlabel('False Positive Rate')\n    plot.ylabel('True Positive Rate')\n    plot.legend(loc='best')\n    plot.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfcb230ed15cbab733e7cd419c74e99d3f1baffc"},"cell_type":"code","source":"\nroc_curve_input = {}\nfor i , j in enumerate(model_metrics):\n    \n    _matrix = model_metrics[j]['confusion_matrix']\n    plot_confusion_matrix(_matrix, classes = [0,1], title = 'Confusion Matrix for %s'%j)\n    if model_metrics[j]['roc_curve']:\n        roc_curve_input[j]= model_metrics[j]['roc_curve']\n    \n\nplotROCCurveAcrossModels(list(roc_curve_input.values()), list(roc_curve_input.keys()))\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"122b478eabc336d1de156f7b3fb34913fd72e756"},"cell_type":"markdown","source":"###### Findings so far:\n\n\n1. Perceptron model performed the worst, the accuracy and f1 scores both are very low. We can ignore perceptron model\n\n\n2. Logistic Regression performs better than perceptron with higher f1 score.\n\n\n3. SVM, K Nearest Neighbor, RandomForests all have pretty high f1 score. However their testing and training times are pretty high as well.\n\n\n4. If we look at the training vs testing curve for Nearest Neighbor and RandomForests, we see that there is a high variance. We can simply go ahead and ignore the models.\n\n\n5. SVM learning curve shows that at high number of samples, our training and testing data performs well. However, SVM has a very high training and testing time and F1 score is also pretty much comparable with the scores for DecisionTreeClassifier\n\n\n\n6. Based on our analysis above, we will select DecisionTreeClassifier as our model. The best parameters for the tree classifier is max depth of 6\n"},{"metadata":{"_uuid":"863e657a031b37fc017c00690c727f4cb849dd29"},"cell_type":"markdown","source":"### Conclusion\n\nBased on our datasets, the model selected for best performance would be Decision Tree with a max depth of 7.\nThese are the metrics we get for DecisionTreeClassifier:\n\n1. accuracy score : 0.81\n\n2. f1 score: 0.47\n\n3. roc_auc_score : 0.75\n\n4. training time: 29 secs\n\n5. testing time: miniscule\n\n\n"},{"metadata":{"_uuid":"08b2fef81223b1295715325b90a330d862d36a2d"},"cell_type":"markdown","source":"\n\nIn order to account for the unbalanced target classes, we are going to assign a class weight to our selected DEcisionTreeClassifier using the same best estimate. To check , if we are able to increase our F1 score and roc_auc_score"},{"metadata":{"trusted":true,"_uuid":"1fefa6c063f09236ab1c88e7ae3749f86e1fff96"},"cell_type":"code","source":"classifier = DecisionTreeClassifier(random_state = 1, criterion = 'gini', max_depth = 7, class_weight = 'balanced')\n\npipeline = Pipeline([('scaler', StandardScaler()), ('decisiontreeclassifier', classifier)])\n\npipeline.fit(_x_train,_y_train)\n\ny_prediction = pipeline.predict(_x_test)\n\ny_probability = pipeline.predict_proba(_x_test)\n\nprint(\"Accuracy score accounting for unbalanced classes\", accuracy_score(_y_test, y_prediction))\nprint(\"F1 score accounting for unbalanced classes\", f1_score(_y_test, y_prediction))\nprint(\"ROC AUC score accounting for unbalanced classes\", roc_auc_score(_y_test, y_probability[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c74e98bdd4c7cc3336e4d5aafcbd8a53c3d2f549"},"cell_type":"markdown","source":"Seeing that we have not been able to increase our estimates as much , lets see if we can boost our model using any of the boosting techniques\n"},{"metadata":{"trusted":true,"_uuid":"fd8a1af19c32221874922ceb4767b4bda2ab4aff"},"cell_type":"code","source":"\nfrom sklearn.ensemble import  AdaBoostClassifier\n\nboostClassifier = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state = 1, criterion = 'gini', max_depth = 7, class_weight = 'balanced'), random_state = 1)\n\npipeline = Pipeline([('scaler', StandardScaler()), ('boostClassifier', boostClassifier)])\n\nparam_grid = [{'boostClassifier__n_estimators':[ 500], 'boostClassifier__learning_rate' :[0.1]}]\n\nresult = runGridSearchAndPredict(pipeline, _x_train, _y_train, _x_test, _y_test, param_grid , score = 'f1')\n\nprint(\"Accuracy score accounting for unbalanced classes\", result['accuracy_score'])\nprint(\"F1 score accounting for unbalanced classes\", result['f1_score'])\nprint(\"ROC AUC score accounting for unbalanced classes\", result['roc_auc_score'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}