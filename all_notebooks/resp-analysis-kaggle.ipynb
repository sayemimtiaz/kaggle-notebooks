{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RESPIRATORY ANALYSIS - DIAGNOSE CONDITIONS","metadata":{}},{"cell_type":"markdown","source":"Main reference: Kaggle dataset \"Respiratory Sound Database\"\n\nhttps://www.kaggle.com/vbookshelf/respiratory-sound-database\n    \nThe aim is to identify the condition of a patient from the audio recordings of their breathing.\n\nHigh-level overview of what happens in this notebook:\n* Import data\n   * Read in patient metadata (age, sex, BMI, etc.)\n   * Impute missing values of patient metadata\n   * Visualise distributions of patient metadata\n   * Import annotations of audio files (contain info about number of crackles and wheezes detected, as annotated by humans)\n   * Read in audio data (.wav files) and calculate time-frequency analyses (STFT and IRCC). Because htis is a slow process, we wil save the spectrograms to pickle files for faster retrieval later.\n   \n   \n* Combine data: \n   * patient metadata and audio annotations are merged in one big dataframe (one record per audio file)\n   * split the datasets into a train/test sets. Do this for both the combined metadata+annotation and the STFT matrixes, keeping track of the audio file and keeping the two inputs aligned (if a record in the metadata+annotations data is used for training, do so also for the corresponding STFT)\n   \n   \n* Run classification algorithms\n   * Logistic regression  (inputs are the combined metadata+annotations)\n   * Boosted Decision Tree with XGBoost (inputs are the combined metadata+annotations)\n   * CNN using the STFT  (inputs are the STFT)\n   * Neural Network concatenating a CNN (inputs are the STFT) and a deep Neural Network (inputs are the combined metadata+annotations)\n   * For each test run a standard set of evaluation metrics (focus on precision/recall rather than accuracy given the strong imbalance of the data set)","metadata":{}},{"cell_type":"code","source":"# general purpose libraries\nimport numpy as np\nimport datetime as dt\nimport pandas as pd\nimport os\nimport pickle\nfrom timeit import default_timer as timer\nfrom collections import OrderedDict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plots and visualisation\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as ply_go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DSP libraries\nimport librosa\nimport librosa.display as librosa_display","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ML and data modelling libraries\nfrom sklearn.preprocessing   import MinMaxScaler, OneHotEncoder,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, roc_auc_score,roc_curve, precision_recall_curve,confusion_matrix,precision_score, recall_score,average_precision_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Setup paths and directories\nwork_dir = \"/kaggle/working/\"\ndata_dir = \"/kaggle/input/respiratory-sound-database/\"\naudio_indir = data_dir + \"Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"\naudio_outdir = work_dir + \"audio/\"\ntext_outdir = work_dir + \"annotations/\"\n\ndemo_file = 'demographic_info.txt'\ndiagnosis_file = 'patient_diagnosis.csv'\n\ndef create_dir_if_not_exists(mydir):\n    if not os.path.exists(mydir):\n        os.makedirs(mydir)\n        return -1\n    return 0\n\ncreate_dir_if_not_exists(audio_outdir)\ncreate_dir_if_not_exists(text_outdir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### To be used only the first time: move audio files to different subfolder, \n### just to organise files in a way that I think is cleaner\n# source_files = os.listdir(audio_indir)\n# for f in source_files:\n#     if f.endswith(\".wav\"):\n#         os.rename(audio_indir+f, audio_outdir+f)\n#     elif f.endswith(\".txt\"):\n#         os.rename(audio_indir+f, text_outdir+f)\n#     else:\n#         pass\n###","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collect patient metadata and organise them in a convenient format","metadata":{}},{"cell_type":"code","source":"patient_data=pd.read_csv(data_dir+demo_file,sep=\" \",header=None,names=['PATIENT_ID', 'AGE', 'SEX', 'BMI', 'WEIGHT', 'HEIGHT'])\ndiagnosis_data = pd.read_csv(data_dir+\"Respiratory_Sound_Database/Respiratory_Sound_Database/\"+diagnosis_file,header=None, names=['PATIENT_ID','DIAGNOSIS'])\npatient_data = patient_data.merge(diagnosis_data, on='PATIENT_ID')\n#print(patient_data.head())\nprint(patient_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Impute missing values","metadata":{}},{"cell_type":"code","source":"print(patient_data.isna().sum() )\n\n# only one entry without age, use median of relevant population\npatient_data.loc[patient_data['AGE'].isnull(),'AGE'] = patient_data.loc[patient_data['DIAGNOSIS']=='COPD', 'AGE'].median()\n\n# count how many M/F there are if diagnosis is COPD\n#patient_data.loc[patient_data['DIAGNOSIS']=='COPD', ['SEX','PATIENT_ID'] ].groupby(['SEX']).count() \npatient_data.loc[patient_data['SEX'].isnull(),'SEX'] = 'M' # the only row with nan for SEX has diagnosis COPD; replace with most common outcome\n\n\n# Impute missing BMI values\n# start from the easy case: no BMI but weight and height available\nnull_bmi = patient_data['BMI'].isnull()\npatient_data.loc[ null_bmi,'BMI'] = patient_data.loc[null_bmi, 'WEIGHT'] / (patient_data.loc[null_bmi,'HEIGHT']/100)**2\n\n# for the remaining cases, we use the median BMI for the appropriate stratified group\nage_quantiles = patient_data['AGE'].quantile([0.2,0.4,0.6,0.8]).values\nprint(age_quantiles)\npatient_data['AGE_CAT'] = 'E'\npatient_data.loc[ patient_data['AGE'] < age_quantiles[-1],'AGE_CAT'] = 'D'\npatient_data.loc[ patient_data['AGE'] < age_quantiles[-2],'AGE_CAT'] = 'C'\npatient_data.loc[ patient_data['AGE'] < age_quantiles[-3],'AGE_CAT'] = 'B'\npatient_data.loc[ patient_data['AGE'] < age_quantiles[-4],'AGE_CAT'] = 'A'\n\ntmp_data = (patient_data.loc[~patient_data['BMI'].isnull(),['DIAGNOSIS','AGE_CAT','SEX','BMI']].\n            groupby(['DIAGNOSIS','AGE_CAT','SEX']).agg('median').reset_index().rename(columns={'BMI':'BMI_imputed'}) )\npatient_data_imputed = patient_data.loc[patient_data['BMI'].isnull(),].merge(tmp_data,on=['DIAGNOSIS','AGE_CAT','SEX'],how='left')\npatient_data = patient_data.merge(patient_data_imputed[['PATIENT_ID', 'BMI_imputed']], on=['PATIENT_ID'],how='left')\npatient_data.loc[patient_data['BMI'].isnull(), 'BMI'] = patient_data.loc[patient_data['BMI'].isnull(), 'BMI_imputed']\n\n# these are the last cases with very little information available\ntmp_data = (patient_data.loc[~patient_data['BMI'].isnull(),].\n            groupby(['AGE_CAT']).agg('median').reset_index().rename(columns={'BMI':'BMI_imputed2'}) )\npatient_data_imputed = patient_data.loc[patient_data['BMI'].isnull(),].merge(tmp_data[['AGE_CAT','BMI_imputed2']],on=['AGE_CAT'],how='left')\npatient_data = patient_data.merge(patient_data_imputed[['PATIENT_ID', 'BMI_imputed2']], on=['PATIENT_ID'],how='left')\npatient_data.loc[patient_data['BMI_imputed'].isnull(), 'BMI_imputed'] = patient_data.loc[patient_data['BMI_imputed'].isnull(), 'BMI_imputed2']\npatient_data.loc[patient_data['BMI'].isnull(), 'BMI'] = patient_data.loc[patient_data['BMI'].isnull(), 'BMI_imputed']\npatient_data.drop(['BMI_imputed2'],1,inplace=True)\n\npatient_data['BMI_imputed'] = ~patient_data['BMI_imputed'].isnull()\n\nprint(\"\\n\\nSummary of missing values after imputation procedure:\")\nprint(patient_data.isna().sum() )\npatient_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualise distributions","metadata":{}},{"cell_type":"code","source":"my_title_layout = dict({\"text\":\"my distribution\", 'xanchor':'center', 'x':0.5, 'y':0.9, 'font':{'size':24}})\nmy_xaxis_layout = dict(title=dict(text=\"my x axis\", font={'size':16}))\nmy_layout = dict(title=my_title_layout,\n                xaxis= my_xaxis_layout)\nbin_size_dict = dict(AGE=1, BMI=5.0, DIAGNOSIS=1,SEX=1)\nxaxis_title_dict = dict(AGE='AGE [years]', BMI=\"BMI\", DIAGNOSIS=\"Condition\", SEX=\"Male/Female]\")\n\nfor c in ['AGE','SEX', 'BMI', 'DIAGNOSIS']:\n    hist_data = ply_go.Histogram(x=patient_data[c], name=c, showlegend=False, xbins={'size':bin_size_dict[c]})\n    fig = ply_go.Figure(data=[hist_data], layout=my_layout)\n    fig.update_layout(title={'text': c+\" distribution\"}, xaxis={\"title\":{\"text\":xaxis_title_dict[c]}})\n    fig.show()\n###\n\n\nfig = ply_go.Figure( layout=my_layout)\nfor tmp_diag in patient_data['DIAGNOSIS'].unique():\n    violin_data = ply_go.Violin(x=patient_data.loc[patient_data['DIAGNOSIS']==tmp_diag, 'DIAGNOSIS'],\n                                y=patient_data.loc[patient_data['DIAGNOSIS']==tmp_diag, 'AGE'],\n                                name=tmp_diag,\n                                box_visible=True,\n                                meanline_visible=True)\n    fig.add_trace(violin_data)\n###\nfig.update_layout(title={'text': \"Distribution of AGE by type of DIAGNOSYS\"}, xaxis={\"title\":{\"text\":None}}, \n                  yaxis={\"title\":{\"text\":\"AGE [years]\"}})\nfig.show()\n\nfig = ply_go.Figure( layout=my_layout)\nfor tmp_diag in patient_data['DIAGNOSIS'].unique():\n    violin_data = ply_go.Violin(x=patient_data.loc[patient_data['DIAGNOSIS']==tmp_diag, 'DIAGNOSIS'],\n                                y=patient_data.loc[patient_data['DIAGNOSIS']==tmp_diag, 'BMI'],\n                                name=tmp_diag,\n                                box_visible=True,\n                                meanline_visible=True)\n    fig.add_trace(violin_data)\n###\nfig.update_layout(title={'text': \"Distribution of BMI by type of DIAGNOSYS\"}, xaxis={\"title\":{\"text\":None}}, \n                  yaxis={\"title\":{\"text\":\"BMI\"}})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import and process audio data\n\n1. Import audio annotation data and store them in a dataframe\n1. Import raw audio data: \n  1. extract audio features (signal amplitude) and store them in a dataframe with audio features; \n  1. calculate time-frequency representations (spectrograms, IIRT, recurrence charts) and store them as matrixes (to be used later for image recognition) \n1. merge dataframes with patient data, annotations and audio features\n","metadata":{}},{"cell_type":"code","source":"def import_annotation(filename, indir):\n    d = pd.read_csv(indir+filename, sep=\"\\t\",header=None, names=[\"BEGIN_CYCLE\", \"END_CYCLE\", \"CRACKLE\", \"WHEEZE\"])\n    d[\"CRACKLE\"] = d[\"CRACKLE\"].astype(int)\n    d[\"WHEEZE\"] = d[\"WHEEZE\"].astype(int)\n    totals = np.array([d['CRACKLE'].sum(), d['WHEEZE'].sum()]).reshape(1,-1)\n    tokens = np.array(f.replace(\".txt\",\"\").split(\"_\")).reshape(1,-1)\n    #print(\"{}   {}  {} {}\".format(f1.shape, tokens.shape, totals.shape, x.shape))\n    ann_df = pd.DataFrame(data= np.hstack((np.array([filename[:-4]]).reshape(1,-1), tokens, totals)),\n                         columns=['ANNOTATION_FILE','PATIENT_ID','REC_IDX', 'CHEST_LOC', 'ACQ', 'DEVICE', 'TOT_CRACKLE', 'TOT_WHEEZE' ],\n                         )\n    for i in ['PATIENT_ID','TOT_CRACKLE', 'TOT_WHEEZE']:\n        ann_df[i] = ann_df[i].astype(int)\n    return ann_df\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record_data = pd.DataFrame()\nfor f in os.listdir(audio_indir):\n    if f.endswith(\".txt\"):\n        tmp_df = import_annotation(f, audio_indir)\n        record_data = record_data.append(tmp_df)\n\n\nprint(record_data.shape)\nprint(record_data[['PATIENT_ID', 'REC_IDX']].groupby(['PATIENT_ID']).count().sort_values(by='REC_IDX') )\nprint(record_data[['TOT_CRACKLE', 'TOT_WHEEZE']].sum() )\nrecord_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import audio data","metadata":{}},{"cell_type":"code","source":"\ndef import_raw_audio(filename,indir, sr=None,ret_timefreqrep=False ):\n    t, sr = librosa.load(indir+filename, sr=sr, mono=True)\n    duration = t.shape[0]/sr #in seconds\n    mu_t = t.mean()\n    min_t = t.min()\n    max_t = t.max()\n    #tnorm = (t - mu_t )\n    #tnorm = tnorm / (max_t-mu_t)\n    f_token = np.array([filename[:-4]]).reshape(1, -1)\n    tokens = np.array([sr, duration, mu_t, max_t, min_t]).reshape(1,-1)\n    audio_df = pd.DataFrame(data= np.hstack((f_token, tokens)),\n                         columns=['ANNOTATION_FILE', 'SAMPLING_RATE','DURATION', 'MEAN_SIG', 'MAX_SIG', 'MIN_SIG' ],\n                         )\n    audio_df['SAMPLING_RATE'] = audio_df['SAMPLING_RATE'].astype(float).astype(int)#weird conversion from string to int \n    for i in ['DURATION', 'MEAN_SIG', 'MAX_SIG', 'MIN_SIG' ]:\n        audio_df[i] = audio_df[i].astype(float)\n    \n    return audio_df, t, sr\n    \ndef zero_padding(t, sr, target_duration):\n    \"\"\"do zero-padding to get audio files all of the same duration; \n       this will allow us to have spectrograms all of the same size\"\"\"\n    target_len = target_duration * sr\n    if t.shape[0] > target_len:\n        t = t[0:target_len]\n    elif t.shape[0] < target_len:\n        n_pads = target_len - t.shape[0] \n        t = np.append(t, np.repeat(0,n_pads)  )\n    else:\n        pass\n    return t\n\n\ndef calc_spectral_features(t, n_fft = 512, win_length = None, win_overlap=0.0, rec_width=0):\n    if win_length is None:\n        win_length = n_fft\n        \n    assert (win_overlap>=0)&(win_overlap<1.0), \"Invalid value of win_overlap {} - it must be in range [0.0, 1.0) \".format(win_overlap)\n    hop_length = int(win_length*(1.0-win_overlap))\n    # calculate spectrograms\n    t_stft_db = librosa.amplitude_to_db(  np.abs(librosa.stft(t, n_fft=n_fft, \n                                                              hop_length=hop_length, win_length=win_length )))\n    t_iirt_db = librosa.amplitude_to_db(  np.abs(librosa.iirt(t, hop_length=hop_length, win_length=win_length )) )\n    #t_mfcc_db = librosa.feature.mfcc(t, n_mfcc=40)\n\n    R_stft = librosa.segment.recurrence_matrix(t_stft_db, mode='affinity', self=False, width=rec_width)\n    R_iirt = librosa.segment.recurrence_matrix(t_iirt_db, mode='affinity', self=False, width=rec_width)\n\n    return t_stft_db,t_iirt_db,R_stft,R_iirt\n\ndef append_array(idx,old_a,new_a, axis=0):\n    if idx==0:\n        return new_a\n    else:\n        return np.append(old_a, new_a, axis=axis)\n    \n    assert False, \"Should not have been here\"\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wav_file_list = np.sort( [f for f in os.listdir(audio_indir) if f.endswith(\".wav\") ] )\n\n#set to true after first run, unless you want to change something basic in how the spectrograms are calculated\n#(for example duration of padded audio, or window size of STFT)\nread_from_file = False \nmy_sampling_rate = int(4096*2) # the max frequency in the STFT will be half of this\n                               # looking at the STFT spectrograms, there is very little above 4096 Hz\nmy_n_fft = 512 # number of frequency bins to be calculated in the STFT; \n               # if my_window_size is None, this drives also the time-sampling window\nmy_window_size = None # should not be greater than n_fft\ntarget_duration = 30 # seconds; obtained from an earlier dry run over all data and charting the distribution \n                     # of duration of the raw sound samples; 30 sec corresponds to the 97th percentile and \n                     # represent a significant improvement in terms of computing time (x5 faster) \n                     # respect to more conservative choices like 70 seconds (99th percentile)\n\n# files where data are stored\naudio_metadata_file = audio_outdir+\"RESP_METADATA_ALL.pkl\"\nstft_data_file =  audio_outdir+\"RESP_STFT.pkl\"\niirt_data_file =  audio_outdir+\"RESP_IIRT.pkl\"\nr_stft_data_file =  audio_outdir+\"RESP_RECSTFT.pkl\"\nr_iirt_data_file =  audio_outdir+\"RESP_RECIIRT.pkl\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if read_from_file :\n    print(\"Loading audio metadata from {}\".format(audio_metadata_file))\n    try:\n        audio_metadata = pd.read_pickle(audio_metadata_file)\n    except FileNotFoundError as e_fnf:\n        print(\"Could not find metadata file {}. Please rerun this cell after settign the variableread_from_file to False\\n\\n\\n\".format(audio_metadata_file))\n        raise e_fnf\n    except Exception as  e:\n        raise e #rethrow exception\n    print(\"Loading time-frequency representations\")\n    try:\n        stft_data = pickle.load(open(stft_data_file, \"rb\") )\n        #iirt_data = pickle.load(open(iirt_data_file, \"rb\") ) # decomment if you plan to use these in your model\n        #r_stft_data = pickle.load(open(r_stft_data_file, \"rb\") )\n        #r_iirt_data = pickle.load(open(r_iirt_data_file, \"rb\") )\n    except FileNotFoundError as e_fnf:\n        print(\"Could not find spectrogram file {}. Please rerun this cell after settign the variableread_from_file to False\\n\\n\\n\".format(stft_data_file))\n        raise e_fnf\n    except Exception as e:\n        raise e #rethrow exception\n\nelse:\n    audio_metadata = pd.DataFrame()\n    # choose to use a dict in this way we are sure that we keep track of the image-label association, \n    # without relying on the order of the files that leaves me always a bit uncomfortable\n    stft_data = OrderedDict({key[:-4]:None for key in wav_file_list})\n    iirt_data = OrderedDict({key[:-4]:None for key in wav_file_list})\n    r_stft_data = OrderedDict({key[:-4]:None for key in wav_file_list})\n    r_iirt_data = OrderedDict({key[:-4]:None for key in wav_file_list})\n    t_start = timer()\n    for idxf, f in enumerate( wav_file_list  ):\n        if (idxf % 50 ==0) | (idxf==10)| (idxf==2):\n            print(\"File #{}: {} ({:.1f} seconds elapsed)\".format(idxf, f,  timer()-t_start ))\n\n        tmp_df, tmp_audio, sr = import_raw_audio(f, indir=audio_indir, sr=my_sampling_rate)\n        tmp_audio = zero_padding(tmp_audio, sr=sr, target_duration=target_duration)                                                         \n        audio_metadata = audio_metadata.append(tmp_df)\n        if (idxf % 300 ==0):\n            print(\"File #{}: starting STFT ({:.1f} seconds elapsed)\".format(idxf,   timer()-t_start ))\n        stft, iirt, r_stft, r_iirt = calc_spectral_features(tmp_audio, n_fft=my_n_fft, win_length = my_window_size,\n                                                            win_overlap=0.5, rec_width=16)\n        stft_data[f[:-4]] = stft.astype(np.float32)\n        iirt_data[f[:-4]] = iirt.astype(np.float32)\n        r_stft_data[f[:-4]] = r_stft.astype(np.float32)\n        r_iirt_data[f[:-4]] = r_iirt.astype(np.float32)\n        \n        if (idxf % 400 ==0) & (idxf>0):\n            print(\"File #{}: ended STFT ({:.1f} seconds elapsed)\".format(idxf,   timer()-t_start ))\n            # save metadata and spectrograms at intermediate, just as a safety: in case of issues we will not lose it all\n            print(\"Saving data at file #{}\".format(idxf))\n            pd.to_pickle(audio_metadata, audio_metadata_file)\n            with open(stft_data_file, \"wb\") as fh:\n                pickle.dump(stft_data , fh)\n            with open(iirt_data_file, \"wb\") as fh:\n                pickle.dump(iirt_data , fh)\n            with open(r_stft_data_file, \"wb\") as fh:\n                pickle.dump(r_stft_data , fh)\n            with open(r_iirt_data_file, \"wb\") as fh:\n                pickle.dump(r_iirt_data , fh)\n\n    ### end for loop over raw audio files\n    print(\"\\n{} files processed in {:.1f} seconds\\n\".format(idxf+1, timer()-t_start ))\n\n    # save metadata and spectrograms\n    print(\"Saving data\")\n    pd.to_pickle(audio_metadata, audio_metadata_file)\n    with open(stft_data_file, \"wb\") as fh:\n        pickle.dump(stft_data , fh)\n    with open(iirt_data_file, \"wb\") as fh:\n        pickle.dump(iirt_data , fh)\n    with open(r_stft_data_file, \"wb\") as fh:\n        pickle.dump(r_stft_data , fh)\n    with open(r_iirt_data_file, \"wb\") as fh:\n        pickle.dump(r_iirt_data , fh)\n### end else do not read_from_file    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the shape of the containers (should match the number of wav files) and the shape of each 2D array with spectrogram.\nTake note of the shape because we will need to know it when we create the CNN later","metadata":{}},{"cell_type":"code","source":"print(\"Size of STFT {}: {}\".format(type(stft_data), len(stft_data) ))\nprint(\"Size of IIRT {}: {}\".format(type(iirt_data), len(iirt_data) ))\nprint(\"Size of Recurrence STFT {}: {}\".format(type(r_stft_data), len(r_stft_data) ))\nprint(\"Size of Recurrence IIRT {}: {}\".format(type(r_iirt_data), len(r_iirt_data) ))\n\nprint(\"\\n\\n\")\nprint(\"Shape of first STFT element {}: {}\".format(type(list(stft_data.values())[0]), list(stft_data.values())[0].shape ))\nprint(\"Shape of first IIRT element {}: {}\".format(type(list(iirt_data.values())[0]), list(iirt_data.values())[0].shape ))\nprint(\"Shape of first Recurrence STFT element {}: {}\".format(type(list(r_stft_data.values())[0]), list(r_stft_data.values())[0].shape ))\nprint(\"Shape of first Recurrence IIRT element {}: {}\".format(type(list(r_iirt_data.values())[0]), list(r_iirt_data.values())[0].shape ))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = stft_data[wav_file_list[0][:-4]]\nprint(\"Max of first 20 rows: {}\".format(np.max(x[0:20]) ))\nprint(\"Max of last 20 rows: {}\".format(np.max(x[-20:-1]) ))\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,18)) # tight_layout=False,constrained_layout=True\nfig.tight_layout()\nimg0 = librosa_display.specshow(x, y_axis='log', x_axis='time',\n                               sr=my_sampling_rate, ax=ax)\nax.set_title('Log-Frequency power spectrogram', size=18)\nfig.colorbar(img0, format=\"%+2.f Db\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge all patient data and record data to a big dataframe\n\nPerform feature extraction and data preparation","metadata":{}},{"cell_type":"code","source":"x_features = [ 'CHEST_LOC', 'NORM_CRACKLE', 'NORM_WHEEZE',  'MEAN_SIG','MAX_SIG', 'MIN_SIG', 'AGE', 'SEX', 'BMI']\ny_label = 'DIAGNOSIS'\n\nfull_data = record_data.merge(audio_metadata,on='ANNOTATION_FILE', how='inner')\nfull_data = full_data.merge(patient_data[['PATIENT_ID', 'AGE', 'SEX', 'BMI', 'DIAGNOSIS']], on='PATIENT_ID', how='inner')\nfull_data['NORM_CRACKLE'] = full_data['TOT_CRACKLE'] / full_data['DURATION']\nfull_data['NORM_WHEEZE']  = full_data['TOT_WHEEZE']  / full_data['DURATION']\n\n# Asthma has only one entry, the train/test split procedure will fail. Aggregate it in the COPD class \n# (could also be removed but from quick search online asthma and COPD sound similar to my medically uneducated brain)\nfull_data.loc[full_data['DIAGNOSIS']=='Asthma', 'DIAGNOSIS'] = 'COPD'\nprint(full_data.shape)\nfull_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = full_data[['PATIENT_ID']+x_features+[y_label] ]\nprint(data.shape)\ndata.loc[data['PATIENT_ID']==107,]\n\n# One-hot encode categorical labels\nenc_inputs = ['CHEST_LOC', 'SEX']\nenc = OneHotEncoder(handle_unknown='error', sparse=False)\nenc1_out = enc.fit_transform(data['CHEST_LOC'].values.reshape(-1,1))\nenc1_cat = [x.upper() for x in enc.categories_[0] ]\nenc1_df = pd.DataFrame(data=enc1_out,columns=enc1_cat).astype(int)\nenc2_out = enc.fit_transform(data['SEX'].values.reshape(-1,1))\nenc2_cat = [x.upper() for x in enc.categories_[0] ]\nenc2_df = pd.DataFrame(data=enc2_out,columns=enc2_cat).astype(int)\n\ndata = pd.concat([data, enc1_df, enc2_df],axis=1)\ndata.drop(enc_inputs,1)\nx_features = [x for x in x_features if x not in enc_inputs] +enc1_cat+enc2_cat\n\n\n# exclude some features that are either redundant or should not be predictive of the disease \n# (like the part of the lungs that has been recorded)\nexcluded_x_features = ['AL', 'AR', 'LL', 'LR', 'PL', 'PR', 'TC', 'M']\nif len(excluded_x_features) > 0:\n    train_features = [ xf for xf in x_features if xf not in excluded_x_features ]\nelse:\n    train_features = x_features\n\nprint(\"Training features: {}\".format(train_features) )\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split train-test\n\nUse argument 'stratify' to preserve small classes, due to imbalance in dataset","metadata":{}},{"cell_type":"code","source":"\nfiles = np.array(full_data[\"ANNOTATION_FILE\"].values)\nindices = np.array(data.index)\nidx_train, idx_test = train_test_split( indices,test_size=0.3,random_state=612, stratify=data['DIAGNOSIS'])\n\n# train_test_split shuffles the order; re-order train and test inputs as per original filename list\nidx_train = np.sort(idx_train)\nidx_test = np.sort(idx_test)\nX_train = data.loc[idx_train, train_features] \ny_train = data.loc[idx_train, y_label]\nX_test = data.loc[idx_test, train_features] \ny_test = data.loc[idx_test, y_label]\nfiles_train = files[idx_train]\nfiles_test = files[idx_test]\n\n# rememebr that stft_data and the otehr time-freq matrices are stored as dictionaries, keys are the file names \n# (stripped of file extension). When we split the dataset, we have done it in a way to keep track of which file \n# names go to the train set and which go to the test sets; that extra complication pays back now\nstft_train = np.array( [stft_data[f] for f in files_train] )\nstft_test  = np.array( [stft_data[f] for f in files_test] )\nprint(len(stft_train) )\nprint(len(stft_test))\niirt_train = np.array( [iirt_data[f] for f in files_train] )\niirt_test = np.array( [iirt_data[f] for f in files_test] )\ndel stft_data, iirt_data # free up some memory\n\n# this is needed later for training the CNN, keras wants the labelling one-hot encoded rather than multiclass\nclasses = np.unique(y_train)\nout_le = LabelEncoder()\nout_le.fit(y_train)\nprint(out_le.classes_)\nn_classes = len(out_le.classes_)\nout_enc = OneHotEncoder(handle_unknown='error', sparse=False)\n\ny_train_enc = out_le.transform(y_train)\ny_train_enc = np.array( out_enc.fit_transform(y_train_enc.reshape(-1,1) ) )\ny_test_enc = np.array( out_enc.transform(out_le.transform(y_test).reshape(-1,1) ) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some functions to help with evaluating models","metadata":{}},{"cell_type":"code","source":"def score_eval(ytrue, ypreds, model_name=\"\"):\n    tmp_acc = accuracy_score(ytrue, ypreds)\n    tmp_precision = precision_score(ytrue, ypreds, average='macro')\n    tmp_recall = recall_score(ytrue, ypreds, average='macro')\n    tmp_cm = confusion_matrix(ytrue, ypreds)\n    print(\"{mn} accuracy / precision / recall: {a:.3f} / {p:.3f} / {r:.3f}\".format(a=tmp_acc, p=tmp_precision, r=tmp_recall, mn=model_name) )\n    print(\"\\n\\n\")\n    print(classification_report(ytrue, ypreds) )\n    return tmp_acc,tmp_precision, tmp_recall, tmp_cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TEST #1: Simple take: do not use the audio trace, use just patient and record data. Feed train test to a logit ","metadata":{}},{"cell_type":"code","source":"### SETUP LOGISTIC REGRESSION (MULTICLASS)\nfrom sklearn.linear_model import LogisticRegression\nlogit_params = dict(multi_class='multinomial', penalty='l2', C=0.20, solver='newton-cg', random_state=991)\n\nlogit_class = LogisticRegression(**logit_params)\nlogit_model = logit_class.fit(X_train, y_train)\nlogit_test  = logit_model.predict(X_test )\n\nlogit_acc, logit_precision, logit_recall, logit_cm = score_eval(y_test, logit_test, \"Logit_multi\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TEST #2: do not use the audio trace, use just patient and record data. Feed train test to a BDT ","metadata":{}},{"cell_type":"code","source":"#\n# define XGBoost classification model\nxgb_params = {'max_depth': 5, \n              'n_estimators': 100,\n              'learning_rate': 0.2,   # learning rate; smaller eta make convergence more accurate but slower\n              'reg_lambda':0.0,   # disable L2 reg only if features are all reasonably independent\n              'reg_alpha':0.0,    #  L1 reg,tring to prune unnecessary features\n              'objective': 'multi:softmax',\n              'random_state':9443,\n              }\n\n\nxgb_class = xgb.XGBClassifier(**xgb_params)\n\nxgb_model = xgb_class.fit(X_train, y_train, \n                          eval_metric=['mlogloss'], \n                          eval_set=[(X_train, y_train), (X_test, y_test)])\n\n\n# run scoring\nxgb_test = xgb_model.predict(X_test )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_acc, xgb_precision, xgb_recall , xgb_cm = score_eval(y_test, xgb_test, \"XGBoost\")\n\nprint(\"\\n\\nConfusion matrix:\")\nprint(xgb_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot global feature importance as calculated by xgboost","metadata":{}},{"cell_type":"code","source":"xgb_importance = xgb_class.feature_importances_\nplt.barh(train_features, xgb_importance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST #3: Conv2D on spectrogram images\n\nRun a Convolutional NN on the STFT files; use the same train/test split defined above","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Flatten, Concatenate, Conv2D, Input, MaxPooling2D, Activation, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.activations import relu\nfrom tensorflow.keras.initializers import GlorotNormal\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\nfrom tensorflow.config import list_physical_devices as tf_config_list_physical_devices\ntf_config_list_physical_devices() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN for the STFT data\n\nUse only the STFT. Cascade 3 layers of CNN, drop out to regularise, use max pooling. After the third round of Conv2D, do a Global Avg Pooling and feed into a classification layer. The classification layer is made of two dense hidden layers plus dropout layers to regularise even there. Finally, a dense layer with a softmax activation to produce the scoring for each of the seven classes of respiratory conditions. \n\n**Note: this specific solution was chosen after few trials of different architectures.**","metadata":{}},{"cell_type":"code","source":"stft_input_shape = (257, 961, 1) #this was printed few cells above, when the STFT data were loaded\n                                  # the last '1' indicates that we feed one image at the time\n_stft_input = Input(shape=stft_input_shape, name=\"stft_input\")\n\n# First Conv layer\n_stft = Conv2D(filters=32, kernel_size=(7,7), \n                     padding=\"same\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=41)  , \n                     name=\"stft_conv2d_01\")(_stft_input)\n\n_stft = Activation(relu, name=\"stft_relu_01\")(_stft)\n_stft = Dropout(rate=0.25, name=\"stft_convdropout_01\")(_stft)\n_stft = MaxPooling2D(pool_size=(7,7),padding='same' ,name=\"stft_maxpool_01\")(_stft)\n\n# Second Conv layer\n_stft = Conv2D(filters=64, kernel_size=(5,5), \n                     padding=\"valid\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=42)  , \n                     name=\"stft_conv2d_02\")(_stft)\n\n_stft = Activation(relu, name=\"stft_relu_02\")(_stft)\n_stft = Dropout(rate=0.25, name=\"stft_convdropout_02\")(_stft)\n_stft = MaxPooling2D(pool_size=5,padding='same' ,name=\"stft_maxpool_02\")(_stft)\n\n\n# Third Conv layer\n_stft = Conv2D(filters=128, kernel_size=(3,3), \n                     padding=\"valid\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=42)  , \n                     name=\"stft_conv2d_03\")(_stft)\n\n_stft = Activation(relu, name=\"stft_relu_03\")(_stft)\n_stft = Dropout(rate=0.25, name=\"stft_convdropout_03\")(_stft)\n_stft = MaxPooling2D(pool_size=3,padding='same' ,name=\"stft_maxpool_03\")(_stft)\n\n_stft = GlobalAveragePooling2D()(_stft)\n\n# Classification layer\n_stftclass = Dense(96, activation=\"relu\", name=\"stft_hiddendense_01\")(_stft)\n_stftclass = Dropout(rate=0.25, name=\"stft_hiddendroput_01\")(_stftclass)\n_stftclass = Dense(48, activation=\"relu\", name=\"stft_hiddendense_02\")(_stftclass)\n_stftclass = Dropout(rate=0.25, name=\"stft_hiddendroput_02\")(_stftclass)\n_stftclass = Dense(7, activation=\"softmax\", name=\"stft_outputdense\")(_stftclass)\n\nstft_model = Model(_stft_input,_stftclass, name=\"stft_model\")\nstft_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stft_model.compile(loss='categorical_crossentropy', \n                   optimizer='nadam', \n                   metrics=['accuracy'])\n#keras.metrics.CategoricalCrossentropy()\n#K.set_value(stft_model.optimizer.learning_rate, 0.001)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_callbacks = [ EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5),\n                 ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                  patience=3, min_lr=0.0001,mode='min') ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=stft_model.fit(\n    {\"stft_input\":stft_train}, y_train_enc,\n    validation_data=({\"stft_input\":stft_test},y_test_enc),\n    epochs=20,\n    verbose=2,\n    callbacks=my_callbacks\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.plot()\nplt.gca().set_xlabel(\"Epoch\")\nplt.gca().set_xlim(-0.5,20.5)\nplt.gca().set_ylim(-0.1,1.1)\nplt.grid(True)\nplt.show()\n\nstft_model.evaluate({\"stft_input\":stft_test},y_test_enc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compute predictions. Select as predicted class the one with the highest score. Then convert the class back to the orginal labelling and use it to evaluate the model as per the previous tests.","metadata":{}},{"cell_type":"code","source":"cnnstft01_test_probs = stft_model.predict(stft_test)\ncnnstft01_test = np.argmax(cnnstft01_test_probs, axis=1)\ncnnstft01_test = out_le.inverse_transform(cnnstft01_test)\n\ncnnstft01_acc, cnnstft01_precision, cnnstft01_recall , cnnstft01_cm = score_eval(y_test, cnnstft01_test, \"CNN-STFT\")\n\nprint(\"\\n\\nConfusion matrix:\")\nprint(cnnstft01_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST #4: Conv2D combined with patient data and annotations\n\nCombine the same CNN developed so far for STFT with the metadata used previously","metadata":{}},{"cell_type":"code","source":"stft_input_shape = (257, 961, 1) #this was printed few cells above, when the STFT data were loaded\n                                  # the last '1' indicates that we feed one image at the time\n_stft_input = Input(shape=stft_input_shape, name=\"stft_input\")\n\n# First Conv layer\n_stft = Conv2D(filters=32, kernel_size=(7,7), \n                     padding=\"same\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=41)  , \n                     name=\"stft_conv2d_01\")(_stft_input)\n\n_stft = Activation(relu, name=\"stft_relu_01\")(_stft)\n_stft = Dropout(rate=0.25, name=\"stft_convdropout_01\")(_stft)\n_stft = MaxPooling2D(pool_size=(7,7),padding='same' ,name=\"stft_maxpool_01\")(_stft)\n\n# Second Conv layer\n_stft = Conv2D(filters=64, kernel_size=(5,5), \n                     padding=\"valid\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=42)  , \n                     name=\"stft_conv2d_02\")(_stft)\n\n_stft = Activation(relu, name=\"stft_relu_02\")(_stft)\n_stft = Dropout(rate=0.25, name=\"stft_convdropout_02\")(_stft)\n_stft = MaxPooling2D(pool_size=5,padding='same' ,name=\"stft_maxpool_02\")(_stft)\n\n\n# Third Conv layer\n_stft = Conv2D(filters=128, kernel_size=(3,3), \n                     padding=\"valid\", \n                     activation=None, \n                     kernel_initializer=GlorotNormal(seed=42)  , \n                     name=\"stft_conv2d_03\")(_stft)\n\n_stft = Activation(relu, name=\"stft_relu_03\")(_stft)\n_stft = BatchNormalization(name=\"stft_batchnorm_02\")(_stft)# \n_stft = Dropout(rate=0.25, name=\"stft_convdropout_03\")(_stft)\n_stft = MaxPooling2D(pool_size=3,padding='same' ,name=\"stft_maxpool_03\")(_stft)\n\n_stft = GlobalAveragePooling2D()(_stft)\n\nstft_model = Model(_stft_input,_stft, name=\"stft_model\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_input_shape = (8, )\n_feat_input = Input(shape=feat_input_shape, name=\"feat_input\")\n\n_feat = Dense(48, activation=\"relu\", name=\"feat_hiddendense_01\")(_feat_input)\n_feat = Dropout(rate=0.25, name=\"feat_hiddendroput_01\")(_feat)\n_feat = Dense(48, activation=\"relu\", name=\"feat_hiddendense_02\")(_feat)\nfeat_model = Model(_feat_input,_feat, name=\"feat_model\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(_stft.shape)\nprint(_feat.shape)\nconcat = Concatenate()([_stft,_feat])\nprint(concat.shape)\n_out_class = Dense(96, activation=\"relu\", name=\"out_hiddendense_01\")(concat)\n_out_class = Dropout(rate=0.25, name=\"out_hiddendroput_01\")(_out_class)\n_out_class = Dense(48, activation=\"relu\", name=\"out_hiddendense_02\")(_out_class)\n_out_class = Dropout(rate=0.25, name=\"out_hiddendroput_02\")(_out_class)\n_out_class = Dense(7, activation=\"softmax\", name=\"out_outputdense\")(_out_class)\ncombined_model = Model([_stft_input,_feat_input], _out_class, name=\"Combined_CNN_Metadata\")\ncombined_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_model.compile(loss='categorical_crossentropy', \n                   optimizer='nadam', \n                   metrics=['accuracy'])\n#keras.metrics.CategoricalCrossentropy()\n#K.set_value(stft_model.optimizer.learning_rate, 0.001)\n\nmy_callbacks = [ EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=8),\n                 ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                  patience=3, min_lr=0.0001,mode='min') ]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_model_history=combined_model.fit(\n    {\"stft_input\":stft_train,\"feat_input\":X_train}, y_train_enc,\n    validation_data=({\"stft_input\":stft_test,\"feat_input\":X_test},y_test_enc),\n    epochs=30,\n    verbose=2,\n    callbacks=my_callbacks\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(combined_model_history.history)\nhistory_df.plot()\nplt.gca().set_xlabel(\"Epoch\")\nplt.gca().set_xlim(-0.5,30.5)\nplt.gca().set_ylim(-0.1,1.1)\nplt.grid(True)\nplt.show()\n\n#combined_model.evaluate({\"stft_input\":stft_test,\"feat_input\":X_test},y_test_enc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_test_probs = combined_model.predict([stft_test, X_test])\ncombined_test = np.argmax(combined_test_probs, axis=1)\ncombined_test = out_le.inverse_transform(combined_test)\ncombined_acc, combined_precision, combined_recall , combined_cm = score_eval(y_test, combined_test, \"CNN-COMBINED\")\n\nprint(\"\\n\\nConfusion matrix:\")\nprint(combined_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}