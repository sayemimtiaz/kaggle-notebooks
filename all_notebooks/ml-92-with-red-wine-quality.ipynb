{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"~1. Overview\n\n\n~2. Data Preprocessing\n\n    ~2.1  Defining \"Quality\" Logic\n    \n    ~2.2  Encoding categorical variables\n    \n    ~2.3  Standardisation \n    \n    \n~3. Classification Model\n\n    ~3.1 Sample Model\n    \n    ~3.2 Tuning model\n    \n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Overview"},{"metadata":{},"cell_type":"markdown","source":"*Input Variables:*\n\n**fixed acidity**: most acids involved with wine or fixed or nonvolatile\n\n**volatile acidity**: the amount of acetic acid in wine\n\n**citric acid**: found in small quantities, citric acid can add 'freshness' and flavor to wines\n\n**residual sugar**: the amount of sugar remaining after fermentation stops\n\n**chlorides**: the amount of salt in the wine\n\n**free sulfur dioxide**: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion\n\n**total sulfur dioxide**: amount of free and bound forms of S02\n\n**density**: the density of water is close to that of water depending on the percent alcohol and sugar content\n\n**pH**: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic)\n\n**sulphates**: a wine additive which can contribute to sulfur dioxide gas (S02) levels\n\n**alcohol**: the percent alcohol content of the wine\n\n\n*Output Variable:*\n\nquality: target variable (score between 0 and 10, expect output 'good' / 'bad')"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understanding the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.isna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isin([' ?']).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Defining \"Quality\" Logic"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\nfig = plt.figure(figsize = [15,20])\ncols = ['quality']\ncnt = 1\nfor col in cols :\n    plt.subplot(4,3,cnt)\n    sns.boxplot(data = df, y = col)\n    cnt+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this graph, taking 75th or above for 'Good' definition, so\n\nAssuming:\n\n-quality >= 7.0 is GOOD\n\n-quality < 7.0 is BAD"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'] = ['good' if i>=7 else 'bad' for i in df['quality']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking all data stype again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Encoding categorical variables"},{"metadata":{},"cell_type":"markdown","source":"We need to convert features which contain strings to numerical values. \n\nThis is required by most model algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_df = df.select_dtypes(include=['object'])\ncategorical_df.columns\n\n\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\n\n\ncategorical_df = categorical_df.apply(enc.fit_transform)\ncategorical_df.head()\n\ndf = df.drop(categorical_df.columns, axis=1)\n\n\ndf = pd.concat([df, categorical_df], axis=1)\ndf.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('quality', axis=1)\ny = df['quality']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.  Classification Model"},{"metadata":{},"cell_type":"markdown","source":"# 3.1 Sample Model"},{"metadata":{},"cell_type":"markdown","source":"Split Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using default model:\n\n1. Logistic Regression\n\n2. SVM\n\n3. Decision Tree\n\n4. Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndef_lr= LogisticRegression()\ndef_lr.fit(X_train, y_train)\n\nlr_pred = def_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, lr_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\ndef_svm = svm.SVC()\ndef_svm.fit(X_train, y_train)\n\nsvm_pred = def_svm.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(\"SVM accuracy: \", accuracy_score(y_test, svm_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef_dt= DecisionTreeClassifier()\ndef_dt.fit(X_train, y_train)\n\ndt_pred = def_dt.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Decision Tree accuracy\", accuracy_score(y_test, dt_pred))\n  \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef_rf = RandomForestClassifier()\ndef_rf.fit(X_train, y_train)\n\n\nrf_pred = def_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, rf_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2 Tuning model"},{"metadata":{},"cell_type":"markdown","source":"As the best accuracy is Random Forest, so we will use Random Forest to tune the ML model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV , KFold\n\n\nrf = RandomForestClassifier()\n\ngs_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 6, 8, 10],\n              \"min_samples_split\": [2, 3, 6, 8, 10],\n              \"min_samples_leaf\": [1, 3, 6, 8, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100, 300, 500],\n              \"criterion\": [\"gini\"]}\n\n\nrf_CV = GridSearchCV(estimator = rf, param_grid=gs_grid, cv=kfold ,scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nresult = rf_CV.fit(X_train, y_train)\n\nprint(result.best_params_)\nprint(result.best_score_)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Input the param into model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfinal_rf = RandomForestClassifier(bootstrap=False , criterion='gini', max_features=3, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\nfinal_rf.fit(X_train, y_train)\n\n\nfinal_rf_pred = final_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, final_rf_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 4 Model's accuary scoure:\n\nI will choose **Random Forest** as Final Model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}