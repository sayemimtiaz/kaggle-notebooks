{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Party Outcome Predictions & Group Fairness\n \nThis tutorial examine a docket's dataset from the Federal Judical Center.  We'll focus on Torts from 2010 to 2019.  The objective is to predict if the case's outcome is in the favor of the Plaintiff.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T15:11:30.336124Z","iopub.execute_input":"2021-08-03T15:11:30.336647Z","iopub.status.idle":"2021-08-03T15:11:30.537471Z","shell.execute_reply.started":"2021-08-03T15:11:30.336538Z","shell.execute_reply":"2021-08-03T15:11:30.536547Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\n\ndf = pd.read_csv('/kaggle/input/iltacon-fair-ai-tutorial/preped_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:36:34.583315Z","iopub.execute_input":"2021-08-13T19:36:34.583652Z","iopub.status.idle":"2021-08-13T19:36:36.321622Z","shell.execute_reply.started":"2021-08-13T19:36:34.583622Z","shell.execute_reply":"2021-08-13T19:36:36.320506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install Fair AI packages\n\n1. [Surgio](https://surgeo.readthedocs.io/en/dev/) - Python package to deterime race by proxy variables (i.e. given name, surname, geocode)\n2. [AI Fairness 360](https://aif360.mybluemix.net/) - An API to measure fairness and mitigate bias in machine learning datasets and models.","metadata":{}},{"cell_type":"code","source":"pip install -U -q ipympl /kaggle/input/iltacon-fair-ai-tutorial/surgeo-1.2.0-py3-none-any.whl 'aif360[all]'","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:36:37.739116Z","iopub.execute_input":"2021-08-13T19:36:37.739464Z","iopub.status.idle":"2021-08-13T19:36:58.602451Z","shell.execute_reply.started":"2021-08-13T19:36:37.739434Z","shell.execute_reply":"2021-08-13T19:36:58.601337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Labels\nWas the outcome in favor of the Plaintiff?\n\nFor the demo, we'll mark\n* Judgment in favor of Plaintiff (or Both parties)\n* Disposition is a settlement \n* If the judgment is unknown, then default and consent judgments are considered favorable.\n\n> Note: We're only determining if the plaintiff receives a judgment/settlement and not if the judgment was equitable  ","metadata":{}},{"cell_type":"code","source":"def for_plaintiff(row):\n    judgment = row['JUDGMENT']\n    disp = row['DISP']\n    if judgment in [1,3] or disp == 13 or (judgment != 2 and disp in [4,5]):\n        return 1.0\n    else:\n        return 0.0\n\ndf['for_plaintiff'] = df.apply(for_plaintiff, axis=1)\nprint(df['for_plaintiff'].value_counts())\n\nlabeled_df = df.drop(columns=['NOJ','JUDGMENT','DISP','DEF']).dropna().reset_index()\n\nsns.countplot(labeled_df['for_plaintiff'])\nplt.title(\"favors plaintiff\",color = 'blue',fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:36:58.604188Z","iopub.execute_input":"2021-08-13T19:36:58.604508Z","iopub.status.idle":"2021-08-13T19:37:00.958189Z","shell.execute_reply.started":"2021-08-13T19:36:58.604473Z","shell.execute_reply":"2021-08-13T19:37:00.957075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Privileged Classes\n\nAlthough the federal docket dataset doesn't contain explicit race or gender columns, the dataset includes some income attributes.  These include:\n1. Informa Pauperis (uanble to pay court costs)\n2. Pro Se (not represented by a laywer)","metadata":{}},{"cell_type":"code","source":"sns.countplot(labeled_df['is_ifp'])\nplt.title(\"Informa Pauperis (court fees waived)\",color = 'blue',fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:00.960157Z","iopub.execute_input":"2021-08-13T19:37:00.960452Z","iopub.status.idle":"2021-08-13T19:37:01.085058Z","shell.execute_reply.started":"2021-08-13T19:37:00.960423Z","shell.execute_reply":"2021-08-13T19:37:01.084167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(labeled_df['pro_se_plt'])\nplt.title(\"Pro Se plaintiff (i.e. no lawyer)\",color = 'blue',fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:03.829141Z","iopub.execute_input":"2021-08-13T19:37:03.82969Z","iopub.status.idle":"2021-08-13T19:37:03.946661Z","shell.execute_reply.started":"2021-08-13T19:37:03.829643Z","shell.execute_reply":"2021-08-13T19:37:03.945995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(labeled_df['pro_se_def'])\nplt.title(\"Pro Se defendant (i.e. no lawyer)\",color = 'blue',fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:07.911835Z","iopub.execute_input":"2021-08-13T19:37:07.912337Z","iopub.status.idle":"2021-08-13T19:37:08.034091Z","shell.execute_reply.started":"2021-08-13T19:37:07.912307Z","shell.execute_reply":"2021-08-13T19:37:08.03308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset \n\nUsing the docket's metadata predict if the case's outcome was in the favor of the Plaintiff.\n\nFrom the Panda's dataframe:\n* Select to column's label\n* List the protected attributes\n* List categorical features (i.e. the district the case was filed in)\n* List features to keep (i.e. the number of days the case was opened)\n","metadata":{}},{"cell_type":"code","source":"from aif360.datasets import StandardDataset\ncategorial_features = ['NOS','plaintiff_res','def_res','JURIS','PROCPROG']\nfeatures_to_keep = ['NOS',\n                    'plaintiff_res',\n                    'def_res',\n                    'JURIS',\n                    'PROCPROG',\n                    'county_pop',\n                    'filing_rate_mean',\n                    'filing_rate_stdev',\n                    'dismiss_rate_mean',\n                    'dismiss_rate_stdev',\n                    'no_demand',\n                    'demanded_imp',\n                    'days_open',\n                    'is_joined',\n                    'trial_began']\n\nds = StandardDataset(labeled_df, \n                     label_name='for_plaintiff', \n                     favorable_classes=[1.0],\n                     protected_attribute_names=['is_ifp'], \n                     privileged_classes=[[0.0]],\n                     categorical_features=categorial_features,\n                     features_to_keep=features_to_keep)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:11.495356Z","iopub.execute_input":"2021-08-13T19:37:11.495867Z","iopub.status.idle":"2021-08-13T19:37:21.800427Z","shell.execute_reply.started":"2021-08-13T19:37:11.495835Z","shell.execute_reply":"2021-08-13T19:37:21.799372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fairness Metrics \n\n__Consistency:__\n    \nIndividual fairness metric that measures how similar the labels are for similar instances (0.0  to 1.0).\n    \n$1 - \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}_i -\n\\frac{1}{\\text{n_neighbors}} \\sum_{j\\in\\mathcal{N}_{\\text{n_neighbors}}(x_i)} \\hat{y}_j|$\n\n__Disparate Impact:__   \n\nProbability postive outcomes of unprivileged group over privilaged group (0 to $\\infty$).  If we were looking at employment by sex, race, or ethinic group, then a disparate impact less than 80% is considered discriminatory (29 CFR § 1607.4).\n    \n$\\frac{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{unprivileged})}{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{privileged})}$\n\n__Statistical Parity Difference:__\n\nProbability postive outcomes difference between unprivileged and privilaged groups (1 to -1).\n    \n$Pr(\\hat{Y} = \\text{pos_label} | D = \\text{unprivileged}) - Pr(\\hat{Y} = \\text{pos_label} | D = \\text{privileged})$","metadata":{}},{"cell_type":"code","source":"from aif360.metrics import BinaryLabelDatasetMetric\npriv_group = [{'is_ifp': 0.0}]\nunpriv_group = [{'is_ifp': 1.0}]\nifp_metrics = BinaryLabelDatasetMetric(ds, unpriv_group, priv_group)\nconsistency = round(ifp_metrics.consistency()[0], 4)\ndisparate_impact = round(ifp_metrics.disparate_impact(), 4)\nstatistical_parity_difference = round(ifp_metrics.statistical_parity_difference(), 4)\nprint(f'Consistency: {consistency}')\nprint(f'Disparate Impact: {disparate_impact}')\nprint(f'Statistical Parity Difference: {statistical_parity_difference}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:31.89867Z","iopub.execute_input":"2021-08-13T19:37:31.899066Z","iopub.status.idle":"2021-08-13T19:37:45.714309Z","shell.execute_reply.started":"2021-08-13T19:37:31.899031Z","shell.execute_reply":"2021-08-13T19:37:45.713104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:45.716487Z","iopub.execute_input":"2021-08-13T19:37:45.716884Z","iopub.status.idle":"2021-08-13T19:37:45.724127Z","shell.execute_reply.started":"2021-08-13T19:37:45.71685Z","shell.execute_reply":"2021-08-13T19:37:45.723095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"protected = 'is_ifp'\ntest = ds.subset(labeled_df[labeled_df['TAPEYEAR'] == 2015].index)\ntrain = ds.subset(labeled_df[labeled_df['TAPEYEAR'] < 2015].index)\n\nscaler = MinMaxScaler(copy=False)\ntrain.features = scaler.fit_transform(train.features)\ntest.features = scaler.fit_transform(test.features)\n\nindex = train.feature_names.index(protected)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:45.726251Z","iopub.execute_input":"2021-08-13T19:37:45.726534Z","iopub.status.idle":"2021-08-13T19:37:45.813732Z","shell.execute_reply.started":"2021-08-13T19:37:45.726508Z","shell.execute_reply":"2021-08-13T19:37:45.812756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIs = []\nfor level in tqdm(np.linspace(0., 1., 11)):\n    di = DisparateImpactRemover(repair_level=level)\n    train_repd = di.fit_transform(train)\n    test_repd = di.fit_transform(test)\n    \n    X_tr = np.delete(train_repd.features, index, axis=1)\n    X_te = np.delete(test_repd.features, index, axis=1)\n    y_tr = train_repd.labels.ravel()\n    \n    lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n    lmod.fit(X_tr, y_tr)\n    \n    test_repd_pred = test_repd.copy()\n    test_repd_pred.labels = lmod.predict(X_te)\n\n    p = [{protected: 0.0}]\n    u = [{protected: 1.0}]\n    cm = BinaryLabelDatasetMetric(test_repd_pred, privileged_groups=p, unprivileged_groups=u)\n    DIs.append(cm.disparate_impact())","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:37:50.757512Z","iopub.execute_input":"2021-08-13T19:37:50.75786Z","iopub.status.idle":"2021-08-13T19:42:42.662289Z","shell.execute_reply.started":"2021-08-13T19:37:50.75783Z","shell.execute_reply":"2021-08-13T19:42:42.661048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nplt.plot(np.linspace(0, 1, 11), DIs, marker='o')\nplt.plot([0, 1], [1, 1], 'g')\nplt.plot([0, 1], [0.8, 0.8], 'r')\nplt.ylim([0.0, 1.3])\nplt.ylabel('Disparate Impact (DI)')\nplt.xlabel('repair level')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:42:42.664505Z","iopub.execute_input":"2021-08-13T19:42:42.66494Z","iopub.status.idle":"2021-08-13T19:42:42.879559Z","shell.execute_reply.started":"2021-08-13T19:42:42.664897Z","shell.execute_reply":"2021-08-13T19:42:42.878578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Estimating Race\n\nThe Dockets don’t contain race but may contain fields that are proxies for race.  In order to see the imapct of proxies on an machine learning algorithm, we're going to estimate the plaintiff's race.  We'll use [Bayesian Improved Surname Geocode (BISG)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1797082/) to estimate race based on US Cenus Data.  ","metadata":{}},{"cell_type":"code","source":"import re\n\ndef last_name(party: str):\n    \"\"\"Extract lastname form party.\"\"\"\n    tokens = re.split(r'[.,\\s]+', party)\n    if(len(tokens) == 1):\n        #Assume is party is a person if only one word\n        return tokens[0]\n    elif len(tokens) == 3 and ' '.join(tokens).endswith(' ET AL'):\n        #Assume first name listed is the primary person\n        return tokens[0]\n    else:\n        #Assume longer names are orginzations\n        return None\n\nplt_df = df[df['PLT'].notnull()].copy()\nplt_df['PLT_LASTNAME'] = plt_df['PLT'].apply(last_name)\nperson_df = plt_df[(plt_df['PLT_LASTNAME'].notnull()) & (plt_df['COUNTY'].notnull())].copy().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:42:42.881539Z","iopub.execute_input":"2021-08-13T19:42:42.88182Z","iopub.status.idle":"2021-08-13T19:42:43.292433Z","shell.execute_reply.started":"2021-08-13T19:42:42.881791Z","shell.execute_reply":"2021-08-13T19:42:43.291429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person_df[['PLT','PLT_LASTNAME','COUNTY','county_name']].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:42:43.294259Z","iopub.execute_input":"2021-08-13T19:42:43.294714Z","iopub.status.idle":"2021-08-13T19:42:43.319832Z","shell.execute_reply.started":"2021-08-13T19:42:43.294667Z","shell.execute_reply":"2021-08-13T19:42:43.318838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Surgeo \n\n[Surgeo](https://github.com/theonaunheim/surgeo) is a python package to estimate race base on zip codes, surname, and forename.\n\n> Note: We've modified surgeo to use county codes instead of zip codes.","metadata":{}},{"cell_type":"code","source":"import surgeo\n\n# Instatiate your model\nsurgeo_model = surgeo.SurgeoModel(geo_level=\"FIPSCC\")\nsurgeo_df = surgeo_model.get_probabilities(person_df['PLT_LASTNAME'], person_df['COUNTY'])\n\ndef race_label(row):\n    prob_threshold = 0.5\n    for race in ['black','api','native','hispanic','white']:\n        if row[race] >= prob_threshold:\n            return race    \n    \n    return 'uncertain'\n\nsurgeo_df['race'] = surgeo_df.apply(race_label, axis=1)\n        \nsurgeo_df.sample(n=20, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:42:43.321316Z","iopub.execute_input":"2021-08-13T19:42:43.321723Z","iopub.status.idle":"2021-08-13T19:42:47.30909Z","shell.execute_reply.started":"2021-08-13T19:42:43.321681Z","shell.execute_reply":"2021-08-13T19:42:47.308242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(surgeo_df['race'])\nplt.title(\"Primary Plaintiff by Race (threshold > 0.5)\",color = 'blue',fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:42:47.311944Z","iopub.execute_input":"2021-08-13T19:42:47.312269Z","iopub.status.idle":"2021-08-13T19:42:47.559923Z","shell.execute_reply.started":"2021-08-13T19:42:47.312241Z","shell.execute_reply":"2021-08-13T19:42:47.559021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob_df = pd.concat([person_df,surgeo_df['white']], axis=1)\nproxy_df = prob_df.copy().dropna().reset_index()\nproxy_df['white'] = proxy_df['white'].apply(lambda x: 1.0 if x > 0.5 else 0.0)\nprint(proxy_df['white'].value_counts())\n\nsns.countplot(proxy_df['white'])","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:46:01.341584Z","iopub.execute_input":"2021-08-13T19:46:01.341932Z","iopub.status.idle":"2021-08-13T19:46:01.663141Z","shell.execute_reply.started":"2021-08-13T19:46:01.341901Z","shell.execute_reply":"2021-08-13T19:46:01.662232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from aif360.datasets import StandardDataset\nproxy_ds = StandardDataset(proxy_df, \n                     label_name='for_plaintiff', \n                     favorable_classes=[1.0],\n                     protected_attribute_names=['white'], \n                     privileged_classes=[[1.0]],\n                     categorical_features=categorial_features,\n                     features_to_keep=features_to_keep)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:46:58.361113Z","iopub.execute_input":"2021-08-13T19:46:58.361616Z","iopub.status.idle":"2021-08-13T19:46:58.674507Z","shell.execute_reply.started":"2021-08-13T19:46:58.361585Z","shell.execute_reply":"2021-08-13T19:46:58.673487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ground Truth Metrics (entire dataset)","metadata":{}},{"cell_type":"code","source":"from aif360.metrics import BinaryLabelDatasetMetric\npriv_group = [{'white': 1.0}]\nunpriv_group = [{'white': 0.0}]\nrace_metrics = BinaryLabelDatasetMetric(proxy_ds, unpriv_group, priv_group)\nconsistency = round(race_metrics.consistency()[0],4)\ndisparate_impact = round(race_metrics.disparate_impact(),4)\nstatistical_parity_difference = round(race_metrics.statistical_parity_difference(),4)\n\nprint(f'Consistency: {consistency}')\nprint(f'Disparate Impact: {disparate_impact}')\nprint(f'Statistical Parity Difference: {statistical_parity_difference}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:47:03.519276Z","iopub.execute_input":"2021-08-13T19:47:03.519626Z","iopub.status.idle":"2021-08-13T19:47:12.281138Z","shell.execute_reply.started":"2021-08-13T19:47:03.519596Z","shell.execute_reply":"2021-08-13T19:47:12.280155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plaintiff Outcome Classifier\n\nWe're going to compare fairness metrics between the ground truth labels and a classifier.","metadata":{}},{"cell_type":"code","source":"protected = 'white'\np = [{protected: 1.0}]\nu = [{protected: 0.0}]\ntest = proxy_ds.subset(proxy_df[proxy_df['TAPEYEAR'] == 2015].index)\ntrain = proxy_ds.subset(proxy_df[proxy_df['TAPEYEAR'] < 2015].index)\n\nscaler = MinMaxScaler(copy=False)\ntrain.features = scaler.fit_transform(train.features)\ntest.features = scaler.fit_transform(test.features)\n\nindex = train.feature_names.index(protected)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:47:25.302437Z","iopub.execute_input":"2021-08-13T19:47:25.302796Z","iopub.status.idle":"2021-08-13T19:47:25.354729Z","shell.execute_reply.started":"2021-08-13T19:47:25.302767Z","shell.execute_reply":"2021-08-13T19:47:25.353794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2015 Ground Truth Dataset Metrics\n\n__Disparate Impact:__   \n\nProbability postive outcomes of unprivileged group over privilaged group (0 to $\\infty$).\n    \n$\\frac{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{unprivileged})}\n{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{privileged})}$\n\n__Smoothed Empirical Differential Fairness:__\n\nJ. R. Foulds, R. Islam, K. N. Keya, and S. Pan, “An Intersectional Definition of Fairness,” arXiv preprint arXiv:1807.08362, 2018.","metadata":{}},{"cell_type":"code","source":"ground_truth_ds = BinaryLabelDatasetMetric(test, privileged_groups=p, unprivileged_groups=u)\ndisparate_impact = round(ground_truth_ds.disparate_impact(), 4)\nsmdf = round(ground_truth_ds.smoothed_empirical_differential_fairness(), 4)\nprint(f'Disparate Impact: {disparate_impact}')\nprint(f'Smoothed Empirical Differential Fairness: {smdf}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:47:31.075938Z","iopub.execute_input":"2021-08-13T19:47:31.07645Z","iopub.status.idle":"2021-08-13T19:47:31.090909Z","shell.execute_reply.started":"2021-08-13T19:47:31.076417Z","shell.execute_reply":"2021-08-13T19:47:31.090185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2015 Predicted Dataset Fairness Metric\n\n__Disparate Impact:__   \n    \n$\\frac{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{unprivileged})}\n{Pr(\\hat{Y} = \\text{pos_label} | D = \\text{privileged})}$\n\n__Smoothed Empirical Differential Fairness:__\n\nJ. R. Foulds, R. Islam, K. N. Keya, and S. Pan, “An Intersectional Definition of Fairness,” arXiv preprint arXiv:1807.08362, 2018.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nX_tr = np.delete(train.features, index, axis=1)\nX_te = np.delete(test.features, index, axis=1)\ny_tr = train.labels.ravel()\n\nlmod = LogisticRegression(class_weight='balanced', solver='liblinear')\nlmod.fit(X_tr, y_tr)\n    \ntest_pred = test.copy()\ntest_pred.labels = lmod.predict(X_te)\nclassifier_ds = BinaryLabelDatasetMetric(test_pred, privileged_groups=p, unprivileged_groups=u)\ncl_disparate_impact = round(classifier_ds.disparate_impact(), 4)\ncl_smdf = round(classifier_ds.smoothed_empirical_differential_fairness(), 4)\n\nprint(f'Disparate Impact: {cl_disparate_impact}')\nprint(f'Smoothed Empirical Differential Fairness: {cl_smdf}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:47:40.23007Z","iopub.execute_input":"2021-08-13T19:47:40.230616Z","iopub.status.idle":"2021-08-13T19:47:40.457115Z","shell.execute_reply.started":"2021-08-13T19:47:40.230583Z","shell.execute_reply":"2021-08-13T19:47:40.456036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Fairness\n\nLets look at the different between the ground truth dataset and the predicted dataset.  Is there any bias amplification?\n\n__Average Odds Difference:__\n\nComputed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups.  The ideal value of this metric is 0.  A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\n\n$\\dfrac{(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}}) + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}})}{2}$\n\n__Equal Opportunity Difference:__\n\nThe true positive rate difference (TPR).  The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\n\n$TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}$\n\n\n__Differential Fairness Bias Amplification:__\n\nThe extent to which the classifier increases the unfairness over the original data. \n\nJ. R. Foulds, R. Islam, K. N. Keya, and S. Pan, “An Intersectional Definition of Fairness,” arXiv preprint arXiv:1807.08362, 2018.","metadata":{}},{"cell_type":"code","source":"from aif360.metrics import ClassificationMetric\ncm = ClassificationMetric(test, test_pred, privileged_groups=p, unprivileged_groups=u)\naod = round(cm.average_odds_difference(), 4)\neod = round(cm.equal_opportunity_difference(), 4)\nbias_amplification = round(cm.differential_fairness_bias_amplification(), 4)\n\nprint(f'Average Odds Difference: {aod}')\nprint(f'Equal Oppertunity Difference: {eod}')\nprint(f'Differential Fairness Bias Amplification: {bias_amplification}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T19:54:03.23399Z","iopub.execute_input":"2021-08-13T19:54:03.234339Z","iopub.status.idle":"2021-08-13T19:54:03.267113Z","shell.execute_reply.started":"2021-08-13T19:54:03.234308Z","shell.execute_reply":"2021-08-13T19:54:03.265943Z"},"trusted":true},"execution_count":null,"outputs":[]}]}