{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Data** : **eighty-years-of-canadian-climate-data/Canadian_climate_history.csv**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\ndata_file = \"\"\nMODEL_SELECTED = \"deepant\" # Possible Values ['deepant', 'lstmae']\nLOOKBACK_SIZE = 10\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data_file = os.path.join(dirname, filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_modulate_data(data_file):\n    \"\"\"\n        Data ingestion : Function to read and formulate the data\n    \"\"\"\n    data = pd.read_csv(data_file)\n    data.fillna(data.mean(), inplace=True)\n    df = data.copy()\n    data.set_index(\"LOCAL_DATE\", inplace=True)\n    data.index = pd.to_datetime(data.index)\n    return data, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_pre_processing(df):\n    \"\"\"\n        Data pre-processing : Function to create data for Model\n    \"\"\"\n    try:\n        scaled_data = MinMaxScaler(feature_range = (0, 1))\n        data_scaled_ = scaled_data.fit_transform(df)\n        df.loc[:,:] = data_scaled_\n        _data_ = df.to_numpy(copy=True)\n        X = np.zeros(shape=(df.shape[0]-LOOKBACK_SIZE,LOOKBACK_SIZE,df.shape[1]))\n        Y = np.zeros(shape=(df.shape[0]-LOOKBACK_SIZE,df.shape[1]))\n        timesteps = []\n        for i in range(LOOKBACK_SIZE-1, df.shape[0]-1):\n            timesteps.append(df.index[i])\n            Y[i-LOOKBACK_SIZE+1] = _data_[i+1]\n            for j in range(i-LOOKBACK_SIZE+1, i+1):\n                X[i-LOOKBACK_SIZE+1][LOOKBACK_SIZE-1-i+j] = _data_[j]\n        return X,Y,timesteps\n    except Exception as e:\n        print(\"Error while performing data pre-processing : {0}\".format(e))\n        return None, None, None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DeepAnT(torch.nn.Module):\n    \"\"\"\n        Model : Class for DeepAnT model\n    \"\"\"\n    def __init__(self, LOOKBACK_SIZE, DIMENSION):\n        super(DeepAnT, self).__init__()\n        self.conv1d_1_layer = torch.nn.Conv1d(in_channels=LOOKBACK_SIZE, out_channels=16, kernel_size=3)\n        self.relu_1_layer = torch.nn.ReLU()\n        self.maxpooling_1_layer = torch.nn.MaxPool1d(kernel_size=2)\n        self.conv1d_2_layer = torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3)\n        self.relu_2_layer = torch.nn.ReLU()\n        self.maxpooling_2_layer = torch.nn.MaxPool1d(kernel_size=2)\n        self.flatten_layer = torch.nn.Flatten()\n        self.dense_1_layer = torch.nn.Linear(80, 40)\n        self.relu_3_layer = torch.nn.ReLU()\n        self.dropout_layer = torch.nn.Dropout(p=0.25)\n        self.dense_2_layer = torch.nn.Linear(40, DIMENSION)\n        \n    def forward(self, x):\n        x = self.conv1d_1_layer(x)\n        x = self.relu_1_layer(x)\n        x = self.maxpooling_1_layer(x)\n        x = self.conv1d_2_layer(x)\n        x = self.relu_2_layer(x)\n        x = self.maxpooling_2_layer(x)\n        x = self.flatten_layer(x)\n        x = self.dense_1_layer(x)\n        x = self.relu_3_layer(x)\n        x = self.dropout_layer(x)\n        return self.dense_2_layer(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMAE(torch.nn.Module):\n    \"\"\"\n        Model : Class for LSTMAE model\n    \"\"\"\n    def __init__(self, LOOKBACK_SIZE, DIMENSION):\n        super(LSTMAE, self).__init__()\n        self.lstm_1_layer = torch.nn.LSTM(DIMENSION, 128, 1)\n        self.dropout_1_layer = torch.nn.Dropout(p=0.2)\n        self.lstm_2_layer = torch.nn.LSTM(128, 64, 1)\n        self.dropout_2_layer = torch.nn.Dropout(p=0.2)\n        self.lstm_3_layer = torch.nn.LSTM(64, 64, 1)\n        self.dropout_3_layer = torch.nn.Dropout(p=0.2)\n        self.lstm_4_layer = torch.nn.LSTM(64, 128, 1)\n        self.dropout_4_layer = torch.nn.Dropout(p=0.2)\n        self.linear_layer = torch.nn.Linear(128, DIMENSION)\n        \n    def forward(self, x):\n        x, (_,_) = self.lstm_1_layer(x)\n        x = self.dropout_1_layer(x)\n        x, (_,_) = self.lstm_2_layer(x)\n        x = self.dropout_2_layer(x)\n        x, (_,_) = self.lstm_3_layer(x)\n        x = self.dropout_3_layer(x)\n        x, (_,_) = self.lstm_4_layer(x)\n        x = self.dropout_4_layer(x)\n        return self.linear_layer(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_train_step(model, loss_fn, optimizer):\n    \"\"\"\n        Computation : Function to make batch size data iterator\n    \"\"\"\n    def train_step(x, y):\n        model.train()\n        yhat = model(x)\n        loss = loss_fn(y, yhat)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        return loss.item()\n    return train_step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute(X,Y):\n    \"\"\"\n        Computation : Find Anomaly using model based computation \n    \"\"\"\n    if str(MODEL_SELECTED) == \"lstmae\":\n        model = LSTMAE(10,26)\n        criterion = torch.nn.MSELoss(reduction='mean')\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n        train_data = torch.utils.data.TensorDataset(torch.tensor(X.astype(np.float32)), torch.tensor(X.astype(np.float32)))\n        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=False)\n        train_step = make_train_step(model, criterion, optimizer)\n        for epoch in range(30):\n            loss_sum = 0.0\n            ctr = 0\n            for x_batch, y_batch in train_loader:\n                loss_train = train_step(x_batch, y_batch)\n                loss_sum += loss_train\n                ctr += 1\n            print(\"Training Loss: {0} - Epoch: {1}\".format(float(loss_sum/ctr), epoch+1))\n        hypothesis = model(torch.tensor(X.astype(np.float32))).detach().numpy()\n        loss = np.linalg.norm(hypothesis - X, axis=(1,2))\n        return loss.reshape(len(loss),1)\n    elif str(MODEL_SELECTED) == \"deepant\":\n        model = DeepAnT(10,26)\n        criterion = torch.nn.MSELoss(reduction='mean')\n        optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-5)\n        train_data = torch.utils.data.TensorDataset(torch.tensor(X.astype(np.float32)), torch.tensor(Y.astype(np.float32)))\n        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=False)\n        train_step = make_train_step(model, criterion, optimizer)\n        for epoch in range(30):\n            loss_sum = 0.0\n            ctr = 0\n            for x_batch, y_batch in train_loader:\n                loss_train = train_step(x_batch, y_batch)\n                loss_sum += loss_train\n                ctr += 1\n            print(\"Training Loss: {0} - Epoch: {1}\".format(float(loss_sum/ctr), epoch+1))\n        hypothesis = model(torch.tensor(X.astype(np.float32))).detach().numpy()\n        loss = np.linalg.norm(hypothesis - Y, axis=1)\n        return loss.reshape(len(loss),1)\n    else:\n        print(\"Selection of Model is not in the set\")\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, _data = read_modulate_data(data_file)\nX,Y,T = data_pre_processing(data)\nloss = compute(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = pd.DataFrame(loss, columns = [\"loss\"])\nloss_df.index = T\nloss_df.index = pd.to_datetime(loss_df.index)\nloss_df[\"timestamp\"] = T\nloss_df[\"timestamp\"] = pd.to_datetime(loss_df[\"timestamp\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    Visualization \n\"\"\"\nplt.figure(figsize=(20,10))\nsns.set_style(\"darkgrid\")\nax = sns.distplot(loss_df[\"loss\"], bins=100, label=\"Frequency\")\nax.set_title(\"Frequency Distribution | Kernel Density Estimation\")\nax.set(xlabel='Anomaly Confidence Score', ylabel='Frequency (sample)')\nplt.axvline(1.80, color=\"k\", linestyle=\"--\")\nplt.legend()\n\nplt.figure(figsize=(20,10))\nax = sns.lineplot(x=\"timestamp\", y=\"loss\", data=loss_df, color='g', label=\"Anomaly Score\")\nax.set_title(\"Anomaly Confidence Score vs Timestamp\")\nax.set(ylabel=\"Anomaly Confidence Score\", xlabel=\"Timestamp\")\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}