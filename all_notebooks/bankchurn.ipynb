{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Card Customers - EDA and XGBoost\n\n**Introduction**\n\nThis notebook uses data available at [https://www.kaggle.com/sakshigoyal7/credit-card-customers](http://) and:\n\n* Reads in the relevant data.\n* Performs exploratory data analysis (EDA) to identify trends and estimate feature importance.\n* Tests various different classifier models on the data (spoiler alert - XGBoost wins).\n* Attempt tuning of the winning classifier to improve performance.\n* Once the final model is built, feature importance for the model is evaluated.\n* Finally, insights and potential next steps are discussed.\n\nThe first step gets the data and some basic information, checking column data types assigned by Pandas and establishing whether there is missing data in any columns.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n#whilst not useful for analysis, the CLIENTNUM is taken as the index as, were the model productionised, it can be used as a key to match prediction to customer\n#similarly, the original Naive Bayes classifier columns were included in the data, so these are removed\ndf = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv',index_col='CLIENTNUM').iloc[:,:-2]\n\nprint(df.shape)\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe has 10,127 rows and 20 columns, with *Attrition_Flag* the target variable. The majority of columns are numeric type, but there are also object dtype columns, which relate to several categorical variables. These columns will be re-encoded to new binary columns  before building the model.\n\nFirstly, let's visualize the data in each column, grouping results by *Attrition_Flag*. I'll use boxplots for numeric columns and countplots for categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import is_numeric_dtype\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n\n#we get our numeric columns and produce a 4x4 boxplot grid, removing empty plots\nnumeric_cols= df.select_dtypes('number').columns.tolist()\n\nfig,ax = plt.subplots(4,4,figsize=(20,15))\nax = ax.flatten()\nfor i in range(len(numeric_cols)):\n    boxplot = sns.boxplot(x=df['Attrition_Flag'],y=df[numeric_cols[i]],ax=ax[i],order=['Attrited Customer','Existing Customer'])\n    boxplot.set_title(numeric_cols[i])\n    plt.tight_layout()\n#there are 14 columns, so plots 15 & 16 are empty and can be removed\nfig.delaxes(ax.flatten()[14])\nfig.delaxes(ax.flatten()[15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the numeric columns, we can see that:\n* Existing customers make more transactions (count and amounts) and tend to have more relationships with the bank versus leavers.\n* Existing customers have higher revolving balances and credit utilisation percentages.\n* Leavers ('Attrited_Customer') tend to have been inactive more often and contacted the bank more frequently.\n* Certain columns (e.g. Customer Age) appear to be similar across the two groups.\n\nA key assumption made at this point is that any column relative to a time (e.g. months inactive), for leavers, is reflective of their tenure rather than a fixed point. If the latter is true, differences in these columns may be due to timing (for instance, if months inactive counts the last twelve months, and a customer left 8 months ago,their inactivity would be high by default).\n\nBefore moving on to categorical columns, I'll look at a correlation matrix to understand which numeric features may be correlated with one another. Visualizing this will be useful for any attempts to reduce the dimensions of the model without compromising significantly on accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ncorr_matrix= df.corr()\nmask = np.triu(np.ones_like(corr_matrix))\nplt.figure(figsize=(25,20))\ncolmap = sns.diverging_palette(150, 150, s=100, as_cmap=True)\nsns.heatmap(corr_matrix,cmap=colmap,mask=mask,center=0,annot=True,fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we get the category columns (minus Attrition_Flag) and build a 2x3 stacked countplot grid,removing empty plots\n#we don't want to plot Attrition_Flag, hence slicing the category_cols list in this way\ncategory_cols = df.select_dtypes('object').columns.tolist()[1:]\n\nfig,ax = plt.subplots(2,3,figsize=(20,15))\nax = ax.flatten()\nfor i in range(len(category_cols)):\n    pivot = df.pivot_table(index='Attrition_Flag',columns=str(category_cols[i]),aggfunc='size')\n    pivot = pivot.div(pivot.sum(1),axis=0)\n    graph = pivot.plot(kind='bar',stacked=True,label=str(category_cols[i]),ax=ax[i],sort_columns=True,rot=0)\n    relevant = pivot.iloc[1,:].reset_index(level=0)\n    tickvals = graph.get_yticks().tolist()\n    graph.yaxis.set_major_locator(ticker.FixedLocator(tickvals))\n    graph.set_yticklabels([\"{:,.1%}\".format(y) for y in tickvals])\n    graph.set_ylabel(str(category_cols[i]))\nplt.tight_layout()\n\n                     \n#there are 5 columns, so plot 6 is empty and can be removed\nfig.delaxes(ax.flatten()[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I used a stacked countplot to compare the distribution of categories across the two customer groups. We see that the distribution of the categorical variables across our two customer groups is reasonably close for all columns. This suggests they may be less valuable in building a model (because any differences are less clear-cut).\n\nIn the next step, let's split out the target and predictor columns, and re-encode categorical columns as boolean ready for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"#split into target and dependent variables\n#the target variable is first re-encoded to 0 (stayed) and 1 (left/lost), as some of the models don't support non-numeric labels\ndf['Attrition_Flag'].replace({'Attrited Customer':1,'Existing Customer':0},inplace=True)\ntarget=df['Attrition_Flag']\npredictor = df.drop(['Attrition_Flag'],axis=1)\n\npredictor = pd.get_dummies(predictor,drop_first=True)\nprint(target.shape)\nprint(predictor.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has increased from 20 columns (19 predictor and the target column) to 32 columns by re-encoding the categorical columns.\n\nIn the final steps before creating a model, let's use sci-kit learn's feature selection package to assess the estimated feature importance of the predictor variables: which features does it expect will most and least influence the model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, mutual_info_classif\n\nplt.figure(figsize=(15,8))\nfmodel = SelectKBest(score_func=mutual_info_classif,k='all')\nfmodel.fit(predictor,target)\nfeatureimportance = pd.DataFrame(fmodel.scores_,index=predictor.columns,columns=['score']).sort_values(by=['score'])\nplt.barh(featureimportance.index,featureimportance['score'])\ntickvals = plt.gca().get_xticks().tolist()\nplt.gca().xaxis.set_major_locator(ticker.FixedLocator(tickvals))\nplt.gca().set_xticklabels([\"{:,.1%}\".format(x) for x in tickvals])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this graph we can see that Total Transaction Amount is expected to most influence the model, and the columns with the highest influence are all numeric columns, which supports the graphs seen above.\n\nNote that the two features deemed most important to the predicting churn (Total Transaction Amount / Total Transaction Count) are also highly correlated as seen above. It may be possible to remove either of these features without losing accuracy in the model (though dimension reduction techniques won't be used here)."},{"metadata":{},"cell_type":"markdown","source":"Conversely, categorical columns are largely expected to have little influence on the model, which again supports the results of our countplot grid.\n\nI'll revisit the feature importance graph later with the final model: for now it's time to split the data into training,test and holdout sets. The data description noted a significant class imbalance in the target variable, with leavers accounting for just 16% of records, so this needs to be accounted for when splitting the data.\n\nFor this model, I'll train against ~80% of records, with test and holdouts being 15% and ~5% respectively.\n\nThe predictor columns will also be scaled after splitting the data. This process reduces the absolute values in each column but keeps the relevant distribution and distance between individual data points. This is helpful for model efficiency"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n#build into stratified train/test/holdout splits (to account for target imbalance)\nX_train,X_test,y_train,y_test = train_test_split(predictor,target,test_size=0.15,random_state=1909,stratify=target)\nX_train,X_holdout,y_train,y_holdout = train_test_split(predictor,target,train_size=0.95,random_state=1909,stratify=target)\n    \n#now the data is scaled, placing the scaled results into the datasets above\ndataset_list = [(X_train,'X_train'),(X_test,'X_test'),(X_holdout,'X_holdout')]\nfor df, out in dataset_list:\n      scaled =pd.DataFrame(StandardScaler().fit_transform(df),columns=predictor.columns,index=df.index)\n      globals()[out] = scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now onto model building. Firstly, it's worth trying out several different models with 'cookie cutter' (no hyperparameters tuned, seed set if applicable for reproducible results) setups.\n\nThe next code block:\n* imports the required packages\n* loops over each model fitting it to our training data and predicting the target column for our test data\n* the results are verified using the classification_report function, with the results stored in a DataFrame and plotted"},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we initialise our models, preparing for a loop\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\n\n#we make a list of model names and functions\n#xgboost \nmodellist = [('KNN',KNeighborsClassifier()),\\\n             ('Random Forest',RandomForestClassifier(random_state=1909)),\\\n             ('Logistic Regression',LogisticRegression()),\\\n             ('XGBoost',xgb.XGBClassifier(objective='binary:logistic',verbosity=0,use_label_encoder=False,random_state=1909))]\n\n#create an empty list\nbase = []\n\n#loop through the models\nfor name, model in modellist:\n    model.fit(X_train,y_train)\n    predictions = model.predict(X_test)\n    predictions_holdout = model.predict(X_holdout)\n    linked_datasets = [(y_test,predictions,'test'),(y_holdout,predictions_holdout,'holdout')]\n    for actual,predicted,split in linked_datasets:\n    #We keep the results for our key class (Attrited Customers / 1) only, slicing [1:2]. All models perform reasonably well at predicting customers who stay\n        results_table = pd.DataFrame(classification_report(actual,predicted,output_dict=True)).transpose()[1:2]\n        results_table['model'] = name\n        results_table['split'] = split\n        base.append(results_table)\n\nresults = pd.concat(base).round(decimals=2)\nprint(results)\n\nfig,ax = plt.subplots(nrows=1,ncols=2,sharey=True,figsize=(12,8))\nfor i,split in ((0,'test'),(1,'holdout')):\n    sns.barplot(data=results[results['split']==str(split)],x='model',y='recall',ax=ax[i])\n    tickvals = plt.gca().get_yticks().tolist()\n    ax[i].title.set_text('Recall scores ('+str(split)+')')\nplt.gca().yaxis.set_major_locator(ticker.FixedLocator(tickvals))\nplt.gca().set_yticklabels([\"{:,.1%}\".format(y) for y in tickvals])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the four models used, XGBoost is a clear winner (and K-Nearest Neighbors performed particularly poorly). \n\nThe next step is to see whether the performance can be improved further by tweaking some of the hyperparameters used for the XGBoost model. For this I'll take advantage of XGBoost's sci-kit learn API functionality to perform GridSearch Cross Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\n#initialise our winning model, taking advantage of the GPU to make use of gradinet based sampling\nxgbmodel = xgb.XGBClassifier(random_state=1909,\n                             use_label_encoder=False,\n                             verbosity=0,\n                             objective='binary:logistic',\n                             tree_method='gpu_hist'\n                             )\n\n#set up hyperparameters to tune - focus is on reducing overfitting to improve generalization without a penalty to performance on test/holdout datasets \nparameters = {\"learning_rate\":np.arange(0.1,0.8,0.1),\n              \"max_depth\":np.arange(2,9,1)}\n#perform 5-fold cross validation, using recall as the scoring method\nxgb_crossval = GridSearchCV(estimator=xgbmodel, param_grid=parameters,cv=3,scoring='recall',n_jobs=-1)\nxgb_crossval.fit(X_train,y_train)\nprint(\"The best parameters are: \", xgb_crossval.best_params_)\nprint('The recall score for the best parameters is {0:.3f}'.format(xgb_crossval.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross-validated model is now used to make predictions on the test and holdout sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_predictions = xgb_crossval.predict(X_test)\ntuned_predictions_holdout = xgb_crossval.predict(X_holdout)\n\nprint(classification_report(y_test,tuned_predictions))\nprint(classification_report(y_holdout,tuned_predictions_holdout))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores on both sets are lower than their untuned counterpart: given the high performance of the untuned model, and the focus on cross validation to help model generalization, this is not unexpected. It also points to different patterns within the data across our subsets, as the default parameters used in tuning (max depth = 6 and learning_rate=0.3) were within the range of values used in the tuning process, but were not selected by the cross validated model as the best hyperparameters.\n\nAs both have high performance versus the original Naive Bayes classifier, either could be productionised. Given that fewer steps are involved the final steps will use the original model, which had a score of 96% on the test set and 93% on the holdout set.\n\nFirstly, the graph below shows the final tree and its predictions. Some points to note:\n* The values used to split branches are based on the scaled data, not the actual data\n* The leaf values are the final groups each data point can end up in, and the values are used to determine the prediction.\n* Leaf values are higher for outcomes of the 'positive' class (here 'Attrited Customer')"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the xgbmodel and plot the final tree\nxgbmodel.fit(X_train,y_train)\nfig,ax = plt.subplots(figsize=(25,25))\nfinaltree = xgb.plot_tree(xgbmodel,num_trees=-1,rankdir=\"LR\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the final tree we can see that the model heavily relies on numeric features, with Gender and Education Level being the only categorical fields used in any branch.\n\nWe'll also revisit the feature importance graph from earlier, seeing how the 'winning' model measured importance comparative to the estimates earlier in the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,8))\nresults = pd.DataFrame(xgbmodel.feature_importances_,index=predictor.columns,columns=['Importance']).sort_values(by=['Importance'],ascending=True).plot(kind='barh',ax=ax,legend=False)\ntickvals = results.get_xticks().tolist()\nresults.xaxis.set_major_locator(ticker.FixedLocator(tickvals))\nresults.set_xticklabels([\"{:,.1%}\".format(x) for x in tickvals])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final model's feature importance graph broadly maps back to the original graph, though some of the most important features have been declared as less or more important (e.g. Total Transaction Amount has dropped in importance, though Total Transaction Count remains high).\n\nAt this point, a high performing, production-ready model is available to be deployed. Revisiting the original question, what insight can be drawn from the EDA and model process that can translate into actionable insight for the business stakeholder?\n\n* Some of the most important model features for predicting customer churn aren't directly able to be influenced (e.g. Total Transaction Count) - knowing who will leave doesn't necessarily mean that process can be halted.\n* That said, initiatives could be launched that would reward use of the card, which may increase customer's incentive to use it (which should translate into them being more likely to remain with the bank).\n* The EDA also noted that a higher number of relationships with a customer improves their loyalty: is there scope to offer more products and contact credit card customers?\n\n\nIn terms of the model, the next steps are suggested:\n* If more records are available (it's unknown whether the 10.1k sample is 5%/25%/75% etc. of the bank's total customer count), test model performance on the remaining customers.\n* Productionize the model and setup periodic reviews.\n* Look at options to reduce unnecessary features and/or engineer new features that could support model accuracy. This may involve building a 'challenger' model to compete with the 'champion' model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}