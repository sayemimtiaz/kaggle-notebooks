{"cells":[{"metadata":{"_uuid":"709d9e2d310b8c3330689ceb57c3c414ca1bf635"},"cell_type":"markdown","source":"<h1> MARK5826 Assignment 2 Version 1 </h1>\n\n<h1> You are now using POKEMON data!! Total marks is 20 (tableau = 10, python = 10), but is scaled down to 13. </h1>\n\n\nNote this is VERSION 1. There are 2 VERSIONS.\n\n<h1> THIS IS VERSION ONE </h1>\n\n<h1> THIS IS VERSION ONE </h1>\n\n<h1> Do VERSION 1 if your ZID IS ODD (500201, 500303, 500607, 508085, 513909 etc [if your ZID ENDS in 1, 3, 5, 7, 9] </h1>\n\n<h1> Do VERSION 2 if your ZID [ENDS IN 0, 2, 4, 6, 8] </h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nglobal directory; directory = '../input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1, method = 'count'):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True / False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n       \n       method == count | tfidf\n       Can choose normal bag of words (count) or tfidf (Term Document Frequency)\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] /= counts['tally'].sum()/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n\n        if method == 'count':\n            from sklearn.feature_extraction.text import CountVectorizer\n            counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        else:\n            from sklearn.feature_extraction.text import TfidfVectorizer\n            counter = TfidfVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.float32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        if column.name is None: column.name = 'text'\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)/60+seconds(date)/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)/12+day(date)/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    out = pd.concat(cols, 1)\n    columns = []\n    seen = {}\n    for i,x in enumerate(out.columns):\n        if x not in seen:\n            columns.append(x)\n            seen[x] = 0\n        else:\n            columns.append(x+f'_{i}')\n    out.columns = columns\n    return out\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \n    \ndef match_pattern(x, pattern, mode = 'find'):\n    '''Regex pattern finds in data and returns only match\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \n        Modes =\n            1. find:   True/False if find or not\n            2. keep:   Output original string if match, else NAN\n            3. match:  Output only the matches in the string, else NAN\n        '''\n    pattern = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n\n    regex = re.compile(r'{}'.format(pattern))\n    \n    def patternFind(i):\n        try: j = re.match(regex, i).group(); return True\n        except: return False;\n    def patternKeep(i):\n        try: j = re.match(regex, i).group(); return i\n        except: return np.nan;\n    def patternMatch(i):\n        try: j = re.match(regex, i).group(); return j\n        except: return np.nan;\n    \n    if mode == 'find':        return x.apply(patternFind)\n    elif mode == 'keep':      return x.apply(patternKeep)\n    elif mode == 'match':     return x.apply(patternMatch)\n    \n    \ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \\S = most symbols including spaces but not apostrophes\n        '''\n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)').replace('\\S','[.!, \"\\(\\)\\?\\*\\&\\^%$#@:/\\\\_;\\+\\-\\â€¦]')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    try: return x.str.split(pattern2)\n    except: return x.apply(lambda i: re.split(regex, i))\n    \ndef replace(x, pattern, with_ = None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern: d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n            \n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    try: return x.str.replace(pattern2, with_)\n    except: return x.apply(lambda i: re.sub(regex, with_, i))\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column / dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values / ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p /= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n):\n    return data.iloc[n:n+1]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j\n\n################### -----------------------------------------------------------------#######################\n#Natural Language Processing & Machine Learning\n\ndef multiply(left, right):\n    ''' Multiplies 2 tables or columns together.\n        Will do automatic type casting'''\n\n    if len(left.shape) == 1:\n        try: return left.values.reshape(-1,1)*right\n        except: return left.reshape(-1,1)*right\n    elif len(right.shape) == 1:\n        try: return right.values.reshape(-1,1)*left\n        except: return right.reshape(-1,1)*left\n    else:\n        return left*right\n    \n    \ndef clean(data, missing = 'mean', remove_id = True):\n    '''Cleans entire dataset.\n    1. missing =\n        mean, max, median, min\n        Fills all missing values with column mean/median etc\n\n    2. remove_id = True/False\n        Checks data to see if theres an ID column.\n        Removes it (not perfect)\n    '''\n    x = data[data.columns[dtype(data) != 'object']].copy()\n    for c in x.columns[x.count()!=len(x)]:\n        x[c] = eval('x[c].fillna(x[c].{}())'.format(missing))\n    if remove_id:\n        for c in x.columns[(dtype(x) == 'int')|(dtype(x) == 'uint')]:\n            if x[c].min() >= 0:\n                j = (x[c] - x[c].min()).sort_values().diff().sum()\n                if j <= 1.001*len(x) and j >= len(x)-1: x.pop(c);\n    return x\n\n\ndef scale(data):\n    columns = data.columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler().fit(data)\n    X = pd.DataFrame(scaler.transform(data))\n    X.columns = columns\n    return X\n\nfrom sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n# from keras.models import Sequential, load_model\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import class_weight\n# from keras.layers import Dense, Activation, GaussianNoise, BatchNormalization, Dropout\n# from keras.initializers import glorot_normal\n# from keras.callbacks import *\n# from keras.optimizers import Nadam, SGD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nclass LinearModel(BaseEstimator, RegressorMixin):\n\n    def __init__(self, lasso = False, scale = True, logistic = False, layers = 0, activation = 'tanh', epochs = 50,\n                    time = None, shift = 1, test_size = 0.2, early_stopping = 7, lr = 0.1, fast = False, random_state = 0, impact = 1):\n        self.scale = scale; self.logistic = logistic; self.lasso = lasso; self.layers = layers;\n        assert activation in ['tanh','relu','sigmoid','linear']\n        assert shift > 0;\n        self.activation = activation; self.epochs = epochs; self.time = time; self.shift = shift\n        if logistic: self.model = Sequential()\n        elif lasso: self.model = LassoLarsIC()\n        else: self.model = Ridge()\n        self.mapping = {}; self.test_size = test_size; self.early_stopping = early_stopping\n        self.lr = lr; self.fast = fast; self.random_state = random_state; self.impact = impact\n\n    def fit(self, X, Y):\n        print('Model now fitting...')\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        \n        self.uniques, self.columns = X.apply(self._range_unique), list(X.columns)\n        if self.scale: X, self.means, self.stds = self._scaler(X)\n        else:\n            A = np.array([np.nan for x in range(len(self.columns))])\n            self.means, self.stds = A, A\n        self.uniques_scale = X.apply(self._range_unique)\n        \n        if self.logistic:\n            Y = self._process_Y(Y)\n            if self.fast: self._fit_sklearn(X, Y)\n            else: self._fit_keras(X, Y)\n        else:\n            try: \n                if self.layers == 0: self._fit_sklearn(X, Y)\n                else: \n                    self.out = 1\n                    if Y.min() >= 0:\n                        if Y.max() <= 1: self.activ = 'sigmoid'\n                        else: self.activ = 'relu'\n                    elif Y.min() >= -1 and Y.max() <= 1:\n                        self.activ = 'tanh'\n                    else: self.activ = 'linear'\n                    self.loss = 'mse'\n                    self._fit_keras(X, Y)\n            except: \n                print('Y is not numeric. Choose logistic = True for classification'); return None\n\n        self._store_coefficients()\n        self._df = self._store_analysis(X, self.impact)\n        print('Model finished fitting')\n\n        \n    def predict(self, X):\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        X = self._transform(X)\n        if self.logistic:\n            prob = self.model.predict(X)\n            if self.fast == False:\n                if self.activ == 'sigmoid': prob = prob.round().astype('int').flatten()\n                else: prob = prob.argmax(1).astype('int').flatten()\n                prob = pd.Series(prob).replace(self.mapping)\n            return prob\n        else: return self.model.predict(X).flatten()\n\n\n    def predict_proba(self, X):\n        if self.logistic:\n            X = self._process_X(X.copy())\n            X = self._time_transform(X)\n            X = self._transform(X)\n            if self.fast == False: prob = self.model.predict(X)\n            else: prob = self.model.predict_proba(X)\n            return prob\n        else: print('Predict Probabilities only works for logisitc models.'); return None;\n\n        \n    def coefficients(self, plot = False, top = None):\n        df = self.coef\n        if self.layers == 0:\n            if top is not None: df = df[:top]\n            if plot:\n                df = df.fillna('')\n                if self.fast:\n                    if len(self.model.classes_) > 2: df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                    else: df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n                else:\n                    if len(self.mapping) > 2: df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                    else: df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n        return df\n    \n    \n    def plot(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import confusion_matrix\n            conf = pd.DataFrame(confusion_matrix(real_Y, predictions))\n            try: \n                if self.fast:\n                    conf.index = [f'True({x})' for x in self.model.classes_]\n                    conf.columns = [f'{x}' for x in self.model.classes_]\n                else:\n                    conf.index = [f'True({x}/{i})' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n                    conf.columns = [f'{x}/{i}' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n            except: \n                conf.index = [f'True({x})' for x in range(nunique(real_Y))]\n            conf = conf.divide(conf.sum(1), axis = 'index')*100\n            return sb.heatmap(conf, cmap=\"YlGnBu\", vmin = 0, vmax = 100, annot = True)\n        else:\n            return plot(x = predictions, y = real_Y, style = 'regplot')\n    \n    \n    def score(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import matthews_corrcoef\n            coef = matthews_corrcoef(real_Y, predictions)\n            if np.abs(coef) < 0.3: print('Model is not good. Score is between (-0.3 and 0.3). A score larger than 0.3 is good, or smaller than -0.3 is good.') \n            else: print('Model is good.')\n            return coef\n        else:\n            from sklearn.metrics import mean_squared_error\n            error = np.abs(np.sqrt(mean_squared_error(real_Y, predictions))/np.mean(real_Y))\n            if error > 0.4: print('Model is not good. Score is larger than 40%. Smaller than 40% relative error is good.')\n            else: print('Model is good.')\n            return error\n    \n    \n    def analyse(self, column = None, plot = False, top = 20, impact = 1):\n        if self.layers == 0:\n            df = self._df.round(2)\n            if self.logistic:\n                if column is not None: df = df.loc[column]\n                else: df = df[:top]\n                def color_negative_red(val):\n                    color = 'lightgreen' if 'Add' in val else 'pink'\n                    return 'background-color: %s' % color\n\n                def highlight_max(s):\n                    is_max = s == s.max()\n                    return ['color: lightgreen' if v else '' for v in is_max]\n\n                if plot:\n                    df = df.fillna('')\n                    if self.activ == 'sigmoid':\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\n                    else:\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\\\n                                    .apply(highlight_max, subset = df.columns[2:], axis = 1)\n            else:\n                if column is not None: df = pd.DataFrame(df.loc[[column]])\n                else: df = df[:top]\n                if plot:\n                    cols = list(df.columns); cols.remove('If Stays'); cols.remove('Change if Removed')\n                    df[cols] = df[cols].fillna('')\n                    def color_negative_red(val):\n                        if val == True: color = 'cyan'\n                        elif val == False: color = 'pink'\n                        else: color = ''\n                        return 'background-color: %s' % color\n\n                    df = df.style.bar(subset = ['Coefficient','If Stays','Change if Removed','Best Addon',\n                                               'Worst Reduced'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n                            .applymap(color_negative_red, subset = ['Stay'])\\\n                            .bar(subset = ['Best Contrib','Worst Contrib'], align='mid', color=['pink', 'cyan'])\n            return df\n        else:\n            print(\"Can't analyse since it's a neural network. I can only give you the model layout and loss graphs\")\n            print(self.model.summary()); history = self.history\n            plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])\n            plt.title('model loss'); plt.ylabel('loss');plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()\n        \n        \n    def degrees(self, prediction, real_Y):\n        '''The closer the degree of the fit line to 45*, the better!'''\n        if not self.logistic:\n            from sklearn.linear_model import Ridge as modeller\n            models = modeller().fit(prediction.reshape(-1,1),real_Y)\n            deg = np.round((np.arctan(models.coef_[0]))/np.pi*180, 3)\n            if deg <= 50 and deg > 45: print('Prediction seems good, but probably overpredicting')\n            elif deg > 50: print(\"Prediction doesn't seem good. It's overpredicting\")\n            elif deg == 45: print(\"Prediction looks ideal! It's quite smooth\")\n            elif deg <= 45 and deg > 40: print(\"Prediction seems good, but probably underpredicting\")\n            else: print(\"Prediction doesn't seem good. It's underpredicting\")\n            return deg\n        else: print('Model is not regression. Use score instead'); return None;\n        \n        \n    def _process_X(self, X):\n        try: X.shape[1]\n        except: X = X.reshape(-1,1)\n        if type(X) is not pd.DataFrame: X = pd.DataFrame(X)\n        try: X = X[self.columns]\n        except: pass\n        return X\n\n\n    def _process_Y(self, Y):\n        if type(Y) is not pd.Series: Y = pd.Series(Y)\n        if self.fast == False:\n            n = nunique(Y); Y = Y.astype('category')\n            self.mapping = dict(enumerate(Y.cat.categories))\n            self.reverse_mapping = dict(zip(self.mapping.values(), self.mapping.keys()))\n            Y = Y.cat.codes\n\n            class_weights = class_weight.compute_class_weight('balanced', list(self.mapping.keys()), Y)\n            self.class_weights = dict(enumerate(class_weights))\n\n            if n == 2:\n                self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n            else:\n                self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n                Y = pd.get_dummies(Y); self.out = Y.shape[1]\n        else:\n            if len(np.unique(Y)) > 2: self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n            else: self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n        return Y\n    \n    \n    def _time_transform(self, X):\n        if self.time is not None:\n            X.sort_values(self.time, inplace = True)\n            alls = [X]\n            for s in range(1,self.shift+1):\n                ss = X.shift(s); ss.columns = [x+f'({-s})' for x in ss.columns]\n                alls.append(ss)\n            X = pd.concat(alls, 1)\n            X.fillna(method = 'backfill', inplace = True); X.sort_index(inplace = True)\n        return X\n    \n        \n    def _store_coefficients(self):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.fast: \n                    if len(self.model.classes_) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.model.classes_], self.columns\n                    else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                else: \n                    if len(self.mapping) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                coefs['Abs'] = np.abs(coefs).sum(1)\n                coefs['Mean'], coefs['Std'], coefs['Range'], coefs['Scale'] = self.means, self.stds, self.uniques, self.uniques_scale\n                coefs.sort_values('Abs', inplace = True, ascending = False); coefs.pop('Abs');\n                self.coef = coefs\n            else: self.coef = self.coef_\n        else:\n            if self.layers == 0:\n                df = pd.DataFrame({'Coefficient':self.coef_ , 'Abs' : np.abs(self.coef_),\n                                    'Mean':self.means, 'Std':self.stds, 'Range':self.uniques, 'Scale':self.uniques_scale})\n                df.index = self.columns; df.sort_values('Abs', ascending = False, inplace = True)\n                df.pop('Abs');\n                self.coef = df\n            else: self.coef = self.coef_\n                \n\n    def _store_analysis(self, X, impact = 1):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.activ == 'sigmoid':\n                    if self.fast: col = 'Probability (Y={})'.format(self.model.classes_[1])\n                    else: col = 'Probability (Y={})'.format(max(list(self.mapping.values())))\n                    coefs.columns = [col]\n                    coefs.index = self.columns\n                    exponential = np.exp(impact*coefs + self.bias_)\n                    exponential = exponential.divide(exponential + 1)*100\n                    exponential['Effect'] = f'Add {impact}'\n\n                    neg_exponential = np.exp(-impact*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential + 1)*100\n                    neg_exponential['Effect'] = f'Minus {impact}'\n\n                    coefs = pd.concat([exponential, neg_exponential]).round(2)\n                    coefs.reset_index(inplace = True); coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs.sort_values(col, ascending = False, inplace = True)\n                else:\n                    if self.fast: coefs.columns, coefs.index = [f'Y=({x})' for x in self.model.classes_], self.columns\n                    else: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    exponential = np.exp(impact*coefs + self.bias_)\n                    exponential = exponential.divide(exponential.sum(1), axis = 0)*100\n                    exponential['Effect'] = f'Add {impact}'\n\n                    neg_exponential = np.exp(-impact*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential.sum(1), axis = 0)*100\n                    neg_exponential['Effect'] = f'Minus {impact}'\n\n                    coefs = pd.concat([exponential, neg_exponential])\n                    coefs.reset_index(inplace = True)\n                    coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs = coefs[['Column','Effect']+list(coefs.columns)[1:-1]].round(2)\n\n                    coefs['Max'] = coefs.max(1); coefs.sort_values('Max', ascending = False, inplace = True); del coefs['Max'];\n                return coefs\n            else: return None\n        else:\n            if self.layers == 0:\n                full = X*self.coef_\n                transformed = full.sum(1) + self.bias_\n                selects, unselects, worst, best, W, B, L, original_G, original_B, overall = [],[],[],[],[],[],[],[],[],[]\n\n                for i, (col, mu) in enumerate(zip(self.columns, self.means)):\n                    if np.isnan(mu):\n                        cond = (X[col]!=0)\n                        select = transformed.loc[cond]\n                        unselect = transformed.loc[~cond]\n                        selects.append(select.mean())\n                        unselects.append(unselect.mean())\n\n                        original = X.loc[cond].mean(0)\n                        d = full.loc[cond].mean(0)\n                        dx = full.loc[~cond].mean(0)\n\n                        d = pd.DataFrame({col: d, 'Abs': np.abs(d)}).sort_values('Abs', ascending = False)[col]\n                        s = (d.index == col)\n                        d = d.loc[~s].sort_values(ascending = False)\n                        first = d.index[0]; end = d.index[-1]\n                        best.append(first)\n                        B.append(d[0]-dx.loc[first])\n                        worst.append(d.index[-1])\n                        W.append(d[-1]-dx.loc[end])\n                        L.append(len(select))\n\n                        original_G.append(original.loc[first])\n                        original_B.append(original.loc[end])\n                    else:\n                        selects.append(np.nan); unselects.append(np.nan); L.append(np.nan)\n\n                        gt = (full.gt(full[col], axis = 'index')*full)\n                        gt[gt == 0] = np.nan; gt_means = gt.mean(0).sort_values(ascending = False)\n                        changes = gt.subtract(full[col], axis = 'index').mean(0)\n                        b = gt_means.index[0]; b_add = changes.loc[b]; b_contrib = gt_means.iloc[0]\n                        best.append(b); B.append(b_add); original_G.append(b_contrib)\n\n                        lt = (full.lt(full[col], axis = 'index')*full)\n                        lt[lt == 0] = np.nan; lt_means = lt.mean(0).sort_values(ascending = True)\n                        changes = lt.subtract(full[col], axis = 'index').mean(0)\n                        w = lt_means.index[0]; w_add = changes.loc[w]; w_contrib = lt_means.iloc[0]\n                        worst.append(w); W.append(w_add); original_B.append(w_contrib)\n\n\n                df = pd.DataFrame({'Coefficient':self.coef_, 'N':L,'If Stays':selects, 'Removed':unselects, 'Change if Removed': 0, 'Stay' : 0,\n                                  'Best Combo':best, 'Best Addon':B,'Best Contrib':original_G,'Worst Combo':worst, 'Worst Reduced':W, 'Worst Contrib':original_B})\n\n                df['Change if Removed'] = df['Removed'] - df['If Stays']\n                df['Stay'] = (df['Change if Removed'] < 0); df['Abs'] = np.abs(df['Change if Removed'])\n                df.loc[df['N'].isnull(), 'Stay'] = np.nan\n                df['Abs_Coef'] = np.abs(df['Coefficient'])\n                df.index = self.columns\n                df.sort_values(['Abs','Abs_Coef'], ascending = [False,False], inplace = True)\n                df.pop('Abs'); df.pop('Removed'); df.pop('Abs_Coef');\n                return df\n            else: return None\n\n    def _fit_keras(self, X, Y):\n        self.model.add(GaussianNoise(0.01, input_shape = (X.shape[1],)))\n        \n        for l in range(self.layers):\n            self.model.add(Dense(X.shape[1], kernel_initializer = glorot_normal(seed = 0)))\n            self.model.add(Activation(self.activation))\n            self.model.add(BatchNormalization())\n            self.model.add(Dropout(0.15))\n            self.model.add(GaussianNoise(0.01))\n            \n        self.model.add(Dense(self.out, kernel_initializer = glorot_normal(seed = 0)))\n        self.model.add(Activation(self.activ))\n    \n        earlyStopping = EarlyStopping(monitor = 'val_loss', patience = int(self.early_stopping*(self.layers/2+1)), verbose = 0, mode = 'min')\n        reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = int(1*(self.layers/2+1)), verbose = 0, epsilon = 1e-4, mode = 'min')\n        cycle = CyclicLR(base_lr = 0.0005, max_lr = self.lr, step_size = 2000, mode = 'exp_range')\n        checkpoint = ModelCheckpoint('Best_Model.hdf5', save_best_only = True)\n        \n        self.metrics = ['acc']        \n        if not self.logistic: self.class_weights = None; self.metrics = None\n        \n        self.model.compile(optimizer = Nadam(), loss = self.loss, metrics = self.metrics)\n\n        if len(X) < 100: bs = 10\n        elif len(X) < 200: bs = 20\n        elif len(X) < 300: bs = 30\n        else: bs = 32\n\n        self.history = self.model.fit(X, Y, epochs = self.epochs, batch_size = bs, verbose = 2, validation_split = self.test_size, shuffle = True,\n                    callbacks = [earlyStopping, TerminateOnNaN(), reduce_lr_loss, cycle, checkpoint], \n                   class_weight = self.class_weights)\n        self.model = load_model('Best_Model.hdf5')\n        if self.layers == 0: self.coef_, self.bias_ = self.model.get_weights()\n        else: self.coef_ = self.model.get_weights()\n        self.lr = cycle\n        \n        \n    def _fit_sklearn(self, X, Y):\n        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = self.test_size, random_state = self.random_state)\n\n        if self.logistic: \n            if len(np.unique(Y)) > 2: \n                self.model = LogisticRegression(n_jobs = -1, class_weight = 'balanced', \n                                               multi_class = 'multinomial', solver = 'saga', tol = 0.1,\n                                              max_iter = int((self.epochs)/3))\n            else:\n                self.model = LogisticRegression(n_jobs = -1, class_weight = 'balanced', \n                                               multi_class = 'ovr', solver = 'saga', tol = 0.1,\n                                              max_iter = int((self.epochs)/3))\n            self.scorer_ = accuracy_score\n        else: self.scorer_ = mean_squared_error\n            \n        self.model.fit(x_train, y_train)\n        self.coef_, self.bias_ = self.model.coef_.T, self.model.intercept_.T\n\n        if self.logistic: text = 'accuracy'\n        else: text = 'error'\n        print('Training {} = {}'.format(text, self.scorer_(y_train, self.model.predict(x_train))*100))\n        print('Testing {} = {}'.format(text, self.scorer_(y_test, self.model.predict(x_test))*100))\n        \n\n\n    def _range_unique(self, x):\n        s = x.sort_values(ascending = True).values\n\n        mins, maxs = np.round(s[0], 2), np.round(s[-1], 2)\n        length = len(s)/4\n        qtr1, qtr3 = np.round(s[int(length)], 2), np.round(s[int(3*length)], 2)\n        return sorted(set([mins, qtr1, qtr3, maxs]))\n\n\n    def _scaler(self, X):\n        result = []; means = []; stds = []\n        \n        for col in X.columns:\n            df = X[col]\n            if df.nunique() == 2 and df.min() == 0 and df.max() == 1:\n                result.append(df); means.append(np.nan); stds.append(np.nan)\n            else:\n                mu, std = df.mean(), df.std()\n                means.append(mu); stds.append(std)\n                result.append((df-mu)/std)\n        return pd.concat(result, 1), np.array(means), np.array(stds)\n\n\n    def _transform(self, X):\n        if self.scale:\n            final = []\n            for col, mu, std in zip(self.columns, self.means, self.stds):\n                if np.isnan(mu): final.append(X[col])\n                else: final.append((X[col]-mu)/std)\n            X = pd.concat(final, 1)\n        return X\n\n        \n##----NATURAL LANG PROCESSING\ndef lower(x):\n    if type(x) is pd.Series: return x.str.lower()\n    else: return [y.lower() for y in x]\n\ndef upper(x):\n    if type(x) is pd.Series: return x.str.upper()\n    else: return [y.upper() for y in x]\n    \ndef remove_space(x):\n    '''Removes duplicate spaces'''\n    return x.str.replace('[\\s]{2,}', ' ')\n\ndef keep_length(x, length = 2):\n    '''Removes objects in lists spaces'''\n    return x.apply(lambda x: [y for y in x if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > length])\n\ndef clean_up(x):\n    '''Deletes \\n \\r'''\n    return x.str.replace('\\n',' ').str.replace('\\r',' ').str.lstrip().str.rstrip()\n\ndef clean_text(x, method = 'basic'):\n    '''Cleans a column of text. 2 methods exist:\n    1. basic:\n        1. Lowercase FIRST letter ONLY (Apple == apple) (APPLE == aPPLE --> symbolizes caps)\n        1. Remove all digits (**replace**)\n        2. Clean up some messy strings using (**clean_up**)\n        3. Reduce spaces to just 1 (3 spaces == 1 space) (**remove_space**)\n        4. Split by space OR punctuation OR any symbols (**split**)\n        5. Ignore words that are 2 letters or shorter (a, by, am, I, ...) (**keep_length**)\n    \n    2. complex:\n        1. Does all basic INCLUDING:\n            1. Places :), :(, other smileys as separate entities.\n            2. Removes all digit like, number like, email/url like objects as well and puts them into another variable.\n    '''\n    if method == 'basic':\n        callback = lambda pat: pat.group(0).lower()\n        lowered = x.str.replace(r'([A-Z])([^A-Z\\s\\-!@#$%\\^&\\*\\(\\):\\\"\\,\\.])', callback)\n        cleaned = clean_up(lowered)\n        no_digits = cleaned.str.replace(r'[0-9]+',' ')\n        space_fixed = remove_space(no_digits)\n        splitted = split(space_fixed, '\\S')\n        \n        k = []\n        for i in splitted:\n            j = []\n            try:\n                for y in i:\n                    try:\n                        if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > 2:\n                            j.append(y)\n                    except: pass\n                k.append(j)\n            except: k.append([])\n        x = pd.Series(k)\n    return x\n\nimport statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n\ndef Results(results, columns, y = None):\n    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n    pvals = results.pvalues\n    coeff = results.params\n    significant = {3:'@@@',2:'@@',1:'@',0:'X',4:'@@@@',5:'@@@@@'}\n\n    if y is not None:\n        #conf_lower = results.conf_int()[0]\n        #conf_higher = results.conf_int()[1]\n        names = np.array(sorted(np.unique(y))[1:])\n        if 'float' in str(names.dtype):\n            names = names.astype(int)\n        pvals = pd.DataFrame(pvals)\n        pvals.columns = [f'(Y={x})-P>|z|' for x in names]\n        coeff = pd.DataFrame(coeff)\n        coeff.columns = [f'(Y={x})-coef' for x in names]\n        #conf_lower = pd.DataFrame(conf_lower)\n        #conf_lower.columns = [f'(Y={x})-[0.025' for x in names]\n        #conf_higher = pd.DataFrame(conf_higher)\n        #conf_higher.columns = [f'0.975]-(Y={x})' for x in names]\n        \n        very_sig = ((pvals <= 0.01) * 3).max(1)\n        sig = (((pvals > 0.01) & (pvals <= 0.05)) * 2).max(1)\n        kinda = (((pvals > 0.05) & (pvals <= 0.1)) * 1).max(1)\n        final = very_sig + sig + kinda\n        final.loc[final > 4] = 5\n        final.name = 'imp'\n        codes = final.replace(significant)\n        codes.name = 'sig'\n        \n        results_df = pd.concat([coeff, pvals, final, codes], 1).sort_values('imp', ascending = False)\n        results_df.pop('imp');\n    else:\n        #conf_lower = results.conf_int().T[0]\n        #conf_higher = results.conf_int().T[1]\n        \n        check = pd.Series(pvals)\n        very_sig = (check <= 0.01) * 3\n        sig = ((check > 0.01) & (check <= 0.05)) * 2\n        kinda = ((check > 0.05) & (check <= 0.1)) * 1\n        final = very_sig + sig + kinda\n\n        results_df = pd.DataFrame({\"coef\":coeff,\n                                   \"P>|z|\":pvals,\n                                   \"sig\":final.replace(significant),\n                                   #\"[0.025\":conf_lower,\n                                   #\"0.975]\":conf_higher\n                                    })\n        #results_df['Columns'] = columns\n        results_df = results_df[['coef','P>|z|','sig']].sort_values('P>|z|')\n    return results_df\n\n\n\nclass Inference(BaseEstimator, RegressorMixin, ClassifierMixin):\n\n\tdef __init__(self, logistic = False):\n\t\tself.logistic = logistic\n\t\treturn None\n\n\tdef fit(self, X, y):\n\t\tif type(X) is not pd.DataFrame:\n\t\t\tX = pd.DataFrame(X)\n\n\t\tif self.logistic:\n\t\t\tif len(np.unique(y)) == 2:\n\t\t\t\tmodel = sm.Logit(y, X)\n\t\t\t\tresults = model.fit(maxiter = 200)\n\t\t\t\tself.coef = Results(results, X.columns)\n\t\t\telse:\n\t\t\t\tmodel = sm.MNLogit(y, X)\n\t\t\t\tresults = model.fit(maxiter = 200)\n\t\t\t\tself.coef = Results(results, X.columns, y)\n\t\telse:\n\t\t\tmodel = sm.OLS(y, X)\n\t\t\tresults = model.fit()\n\t\t\tself.coef = Results(results, X.columns)\n\t\tself.model = results\n\t\treturn self\n\n\tdef predict(self, X):\n\t\treturn self.model.predict(X)\n\n\tdef coefficients(self):\n\t\treturn self.coef\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Note this is VERSION 1. There are 2 VERSIONS.\n\n<h1> THIS IS VERSION ONE </h1>\n\n<h1> THIS IS VERSION ONE </h1>\n\n<h1> Do VERSION 1 if your ZID IS ODD (500201, 500303, 500607, 508085, 513909 etc [if your ZID ENDS in 1, 3, 5, 7, 9] </h1>\n\n<h1> Do VERSION 2 if your ZID [ENDS IN 0, 2, 4, 6, 8] </h1>"},{"metadata":{"_uuid":"c29487db93cc18eaf3043a20ef5f68fd5d091f14"},"cell_type":"markdown","source":"<h2> Question 1 [0.5 marks] </h2>\nRead the Pokemon data in.\nEXCLUDE COLUMNS from the data including:\n1. #\n2. name\n3. total.\n\nMake a new variable Y.\n\nY = the column TOTAL in data"},{"metadata":{"trusted":true,"_uuid":"3c6d11d23418beae06e349c8ee8b901c77b2e0f0"},"cell_type":"code","source":"files()\ndata = read('Pokemon.csv')\nY = data['total']\nE = exclude(data, ['#', 'name', 'total'])\nhead(E)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34bd92a816665987f2e6301a18771d22d22d8128"},"cell_type":"markdown","source":"<h2> Question 2 [1 marks] </h2>\n\nUsing the following columns:\n1. Type_1\n2. Type_2\n3. Legendary\n4. Generation\n\nUse TALLY and perform BAG OF WORDS on all 4 columns. Save all 4 to 4 separate variables.\n\nIE:\na = tally(...)\nb = tally(...)\n..."},{"metadata":{"trusted":true,"_uuid":"a713f14a28468e0bed7fd052f1b59437fbc98fc7"},"cell_type":"code","source":"T1 = tally(data['type_1'], multiple = True)\nT2 = tally(data['type_2'], multiple = True)\nL = tally(data['legendary'], multiple = True)\nG = tally(data['generation'], multiple = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22c64b3463d6cd7afb4bb9c51c5e45c711cd59c2"},"cell_type":"code","source":"Q2 = hcat(T1, T2, L, G)\nhead(Q2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44ac2d765522b8f8d0b8f62ae374af1472be2aae"},"cell_type":"markdown","source":"<h2> Question 3 [0.5 marks] </h2>\n\nPLOT a BARPLOT where x = Type_1, y = Y (from Question 1), and COLUMN = Type_2. The data is from Question 1 after you excluded the columns"},{"metadata":{"trusted":true,"_uuid":"e0b920e2aef9bb91a371eb4202624ae5aece36b4"},"cell_type":"code","source":"plot(x = data['type_1'], y = Y, column = data['type_2'], style='barplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d711acb9dab6126af2b090d0b6a8d50f5c372960"},"cell_type":"markdown","source":"<h2> Question 4 [0.5 marks] </h2>\n\nPLOT a REPLOT where x = Attack, y = Defense and COLUMN = TYPE_1.  ALSO POWER = 2. The data is the excluded data from question 1"},{"metadata":{"trusted":true,"_uuid":"4c2131842c53684fcafb7c1f83ab3aad64aad2c7"},"cell_type":"code","source":"plot(data = data, x= 'attack', y = 'defense', column = 'type_1', power = 2, style = 'regplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a73de6c4d83eaf81541eaad36d83d1f95efc03c1"},"cell_type":"markdown","source":"<h2> Question 5 [1 marks] </h2>\n\nUsing TERNARY on the following columns:\n1. SP._DEF\n2. SP._ATK\n3. ATTACK\n\n1. SP._DEF >50, letting TRUE = 1 and FALSE = 0\n2. SP._ATK >50, letting TRUE = -1, FALSE = 1\n3. ATTACK <= 50, letting TRUE = 0 and FALSE = 1\n\nSave all 3 to new variables.\n"},{"metadata":{"trusted":true,"_uuid":"4a5bee417ae8fea5e99e707f87215ae69d2d85b2"},"cell_type":"code","source":"SD = ternary(data['sp._def'], '>50', 1, 0)\nSP = ternary(data['sp._atk'], '>50', -1, 1)\nAT = ternary(data['attack'], '<=50', 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6255dfcdc3e2969956e2a374a1a0f5c4528a9306"},"cell_type":"code","source":"Q5 = hcat(SD, SP, AT)\nhead(Q5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad4ee298f972e4149384a75079a4f003bd158e4e"},"cell_type":"markdown","source":"<h2> Question 6 [1 marks] </h2>\n\nCLEAN the DATA from Question 1.\n\nEXCLUDE the column LEGENDARY\n\nCOMBINE all the datas / new variables into 1 large data (combine all data from Question 1,2,3,4,5)"},{"metadata":{"trusted":true,"_uuid":"ee6a51672e599b46a60337a48792dbace572efa3"},"cell_type":"code","source":"E1 = clean(E)\nE2 = exclude(E1, 'legendary')\nhead(E2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9d54f63f3bd94945a179869bd01865e6875a05"},"cell_type":"code","source":"Ed=hcat(Q2, Q5, E2)\nhead(Ed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3e1fcf04cb39845f54c56e6b6e7eb9244cacf23"},"cell_type":"markdown","source":"<h2> Question 7 [2 marks] </h2>\n\nCreate a LINEARMODEL where SCALE = TRUE.\n\nFIT the model on Y with the combined data.\n\nShow the COEFFICIENTS top 10, and plot = TRUE\n\nANALYSE the MODEL with PLOT = TRUE\n\nMake PREDICTIONS with model, and using PLOT, show whether the model is GOOD or BAD.\n\nSCORE the model."},{"metadata":{"trusted":true,"_uuid":"9c88fe891c57752197749a0f3f7678c777803fe8"},"cell_type":"code","source":"model = LinearModel(scale = True)\nmodel.fit(Ed,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4532424aa490629e69d4ded1d50e8fbc67e7e88d"},"cell_type":"code","source":"model.coefficients(plot=True,top=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d989515076a77ef08af4b4c2709f84de1a4814"},"cell_type":"code","source":"model.analyse(plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00879cf35cf9aa9221cb72936af0fe0cd8de27d2"},"cell_type":"code","source":"predictions=model.predict(Ed)\nmodel.plot(model.predict(Ed), Y)\nmodel.score(model.predict(Ed), Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c26e78c7cd8c9ce9f75bddfe5acf9fe8e5ccf31c"},"cell_type":"markdown","source":"<h2> Question 8 [2 marks] </h2>\n\nLooking at ANALYSE from Question 7, remove all MULTICOLLINEAR columns as shown in WORST COMBO.\n\nYou need to JUSTIFY why you removed the columns. WRITE A SHORT TEXT explaining WHY.\n\nAlso, refit the MODELS each time you remove columns, and JUSTIFY if the REMOVAL of the columns was good / bad"},{"metadata":{"trusted":true,"_uuid":"39b86e7036f960ecd12b5b7203638fb4d4f94417"},"cell_type":"code","source":"model.analyse(plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae55af6dfad2e3c5cafd763b25a9ecedc3c7d4a4"},"cell_type":"code","source":"Ed1 = exclude(Ed,['legendary_(False)'])\nmodel = LinearModel(scale=True)\nmodel.fit(Ed1, Y)\n\nmodel.analyse(plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91b22876d221693ed142cc015da63b479f08bc02"},"cell_type":"code","source":"model.analyse(plot=True, column = 'defense')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5cb5d9cf85ec4bd72ec57053056fdd70e5dc087"},"cell_type":"code","source":"model.score(model.predict(Ed1), Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b982f0de43b7c04cbcacd04dd03adcdff8a93a5"},"cell_type":"markdown","source":"**First of all, the 'legendary(false)' should be removed,  because it has the least  influences in the worst combo. Such an affect can be ignored. The reason why it is small is that most of the Pokemon are not legendary.\n\n**After removing the 'legendary(false), the data changes to be more cleaar, and it is more easily to compare the rest of factors. \n****"},{"metadata":{"_uuid":"c1eb74649035d36e137a2e0bbba86ef97066b299"},"cell_type":"markdown","source":"[](http://)Had to remove LEGENDARY FALSE since it was skewing the results as seen in the first ANALYSE. Now, the BEST COMBO, WORST COMBO is much more clear and seems more RANDOM."},{"metadata":{"trusted":true,"_uuid":"4f69ab78182caf12db9285bee778096acc5f8571"},"cell_type":"markdown","source":"<h2> Question 9 [1.5 marks] </h2>\n\nINTERPRET your MODEL. Write 200 - 300 words EXPLAINING your model's coefficients. Which columns are good, and how do they contribute to the prediction of the column TOTAL (Y)?"},{"metadata":{"trusted":true,"_uuid":"52cf8600ca0aad32ca81f4868ab5fcf92da2872c"},"cell_type":"markdown","source":"This model is used to evaluate a Pokemon by analysing its different attribute data like attack and defence. This model is based on the regression mathmatic method, although there must be some errors exists between the real Pokemon and our model, but at least we can use this model to make sure whether a Pokemon is good or not.\n\nThe coefficient in this model is to describe the relationship between different variables and a Pokemonâ€™s total ability. The bigger the coefficient number, the more tied relationship between this attibute to a Pokemon.\n\nTo make advanced analyzation, we should pay attention to the 'comboâ€™. According to the â€˜Analyzing sheetâ€™ above, I found that whether a Pokemon is legendary or not is the biggest factor to influence its total ability. This is right because the coefficient in this sheet is 0.31(larger than others), and if it stays, it can help to improve a Pokemonâ€™s total ability around 640. And if the 'special attack' was increased also, there will be more positive influences for the total ability.\n\nAnd this conclusion is based on the evaluating method about the â€˜Analyzing sheetâ€™ above, if the coefficient number is greater than 0, this column is good for the total ability. "},{"metadata":{"_uuid":"c6b3a9191df297bf38985775c51fc1159024c03e"},"cell_type":"markdown","source":"<h1> DONT FORGET TO DO THE TABLEAU ASSIGNMENT VERSION 1 </h1>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}