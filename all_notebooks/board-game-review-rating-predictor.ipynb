{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Board Game Review Rating Predictor\n\n\n* Name: Bhavik Samir Shah\n* Email ID: bhaviksamir.shah@mavs.uta.edu\n\n\n## Summary\n\nThe main aim of the project is to predict the rating of a game review based on the review provided as input. The purpose of this project is handling large scale data and understanding the working of classification algorithms. Also creating a web User Interface and hosting it on cloud.\nThe main challenge of this project was handling the large scale data and creating a prediction model that can be deployed on cloud to provide the predicted output in good time. The textual data was cleaned by removing the 'urls', 'emojis', Stopwords, and other special characters that were redundant for the classification model i.e Multinomial Naive Bayes Classifier. for prediction.\n\n## Project Links\n\n### [Project Demo](http://bsshah4216.pythonanywhere.com/)\n\n### [GitHub Repo](https://github.com/shah-bhavik204/BoardGamePrediction)\n\n### [Video link](https://youtu.be/fRhuZLYC2bo) \n\n## Reference\n\n* https://www.kaggle.com/abhishekmshinde3097/datamining-termproject\n\n### Contribution over the reference\n\nIn the above Reference, multiple classifier algorithms are applied to the data like SVM and Multinomial Naive Bayes Classifier. After removing the null values from the dataset,the data is splitted into Training and Test data by ratio of 75:25. Here the whole training data is used for training the model.\n\nThe data is very large and imbalanced even after removing the null values. To combat this problem, Resampling strategies for imbalanced dataset is used for pre-processing the dataset. The aim of this stratergy is to reduce the learning rate for training the model while maintaing the accuracy. Further explaination is given below.\n\n### Challanges\n\n1. Due to large data, the learning time for training the model was very high i.e upto 2.5 hours. Also the data is very imbalanced. \n    **Soultion**: Used Random Undersampling technique. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).The leaarning rate reduced to less than 3 minutes.\n2. While deploying the saved model in \"Pythonanywhere\", the model required transformed Tfid vector for the prediction. While Tfid tranformer requires train vectors to be fitted and transformed for transformation of test vector.\n    **Solution**: Saved the training Tfid vectors as \"vectorizer.pickle\" in localhost and uploaded the file on server for vector transormation.  \n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading Dataset File**\n\nHere, after mounting our drive and importing our essential libraries, we read our dataset from its source using Pandas, as we want to read the data into our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-15m-reviews.csv')\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we know, the scope of our project is to predict the rating, given a review. Therefore, we are only concerned with the columns \"rating\" and \"comment\".\n\nTherefore, we drop the rest of the columns as they would not fit any purpose for our data analysis. Hence, here, we drop the \"ID\",\"user\",\"name\" below.\n\nDropping Column \"ID\", \"user\", \"name\".\n\nRemoving the rows with \"NaN\" values in \"comment\".\n\nRounded the ratings to its closest integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the rows without comments\ndf = df.drop(columns=[\"Unnamed: 0\", 'user', 'ID', 'name'])\ndf = df.dropna(subset=['comment'])\ndf['rating'] = df.rating.apply(lambda x: round(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the .head() function to display the top 5 records of our dataset.\n\nAs you can see below, we have 5 rows of data, and the column names are also available."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n# print(len(df[df.rating == 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here is the sorted list of classes with its length. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rating.value_counts(sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rating.value_counts()\nsns.countplot(x=\"rating\", data=df, palette=\"magma\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Resampling\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).\n\n![1](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\n\nDespite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the DataFrame.sample method to get random samples each class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_0_count,rating_1_count,rating_2_count,rating_3_count,rating_4_count,rating_5_count,rating_6_count,rating_7_count,rating_8_count,rating_9_count,rating_10_count = df.rating.value_counts(sort= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_class_0 = df[df['rating'] == 0]\ndf_class_1 = df[df['rating'] == 1]\ndf_class_2 = df[df['rating'] == 2]\ndf_class_3 = df[df['rating'] == 3]\ndf_class_4 = df[df['rating'] == 4]\ndf_class_5 = df[df['rating'] == 5]\ndf_class_6 = df[df['rating'] == 6]\ndf_class_7 = df[df['rating'] == 7]\ndf_class_8 = df[df['rating'] == 8]\ndf_class_9 = df[df['rating'] == 9]\ndf_class_10 = df[df['rating'] == 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random under-sampling\n\nHere all the classes are sampled into the length of a class with minimum size i.e \"df_class_1\" for balancing the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rating_2_under = df_class_2.sample(rating_1_count)\ndf_rating_3_under = df_class_3.sample(rating_1_count)\ndf_rating_4_under = df_class_4.sample(rating_1_count)\ndf_rating_5_under = df_class_5.sample(rating_1_count)\ndf_rating_6_under = df_class_6.sample(rating_1_count)\ndf_rating_7_under = df_class_7.sample(rating_1_count)\ndf_rating_8_under = df_class_8.sample(rating_1_count)\ndf_rating_9_under = df_class_9.sample(rating_1_count)\ndf_rating_10_under = df_class_10.sample(rating_1_count)\n# len(df_rating_10_under)\ndf_under = pd.concat([ df_class_1, df_rating_2_under, df_rating_3_under, df_rating_4_under, df_rating_5_under, df_rating_6_under, df_rating_7_under, df_rating_8_under, df_rating_9_under, df_rating_10_under], axis=0)\n\nprint('Size after Random under-sampling:',len(df_under))\n\n# print(df_test_under.target.value_counts())\n\n# df_test_under.target.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After Under-sampling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_under.rating.value_counts()\nsns.countplot(x=\"rating\", data=df_under, palette=\"magma\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing of Sample Data\n\nIn the cleaning process for our text data, we:\n\n* Remove all Punctuations that are present in our text data.\n* Convert all text data into a standardized LowerCase Text.\n* Removing all the Stopwords from our text data.\n* Stopwords are a set of commonly used words, irrespective of the language. The main reason for removing Stopwords from our text data is so that if we remove the common words, we will be able to focus on the important words instead.\n\nTo import the list of stopwords which we can use to remove them easily, we need to use nltk and download it to our system one time so that we can perform text cleaning without errors.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All the processed data is then added to column name \"processed_comments\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_under['processed_comments'] = df_under.comment.apply(lambda x: clean_text(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_under.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Splitting the Dataset into Train and Test SubsetsÂ¶\n\nSince the Total Dataset is still huge, we split it into Train And Test Set in a 80:20 ratio.\n\nFor this operation, we use train_test_split library from Sklearn package.\n\nThe train_test_split function is for splitting a single dataset for two different purposes: training and testing. The training subset is for building your model. The testing subset is for using the model on unknown data to evaluate the performance of the model. This function makes random partitions for the two subsets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# df_under\nX = df_under['processed_comments']\ny = df_under['rating']\n# X.head()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multinomial Naive Bayes\nMultinominal Naive Bayes (MNB) algorithm has been widely used in text classification due to its computational advantage and simplicity. MNB maximizes likelihood rather than conditional likelihood or accuracy.The task of text classification can be approached from a Bayesian learning perspective, which assumes that the word distributions in documents are generated by a specific parametric model, and the parameters can be estimated from the training data. Below Equation shows Multinominal Naive Bayes (MNB) model which is one such parametric model commonly used in text classification.\n\n![2](data:image/png;base64,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)\n\nHere fi is the number of occurrences of a word wi in a document d, P(wijc) is the conditional probability that a word wi may happen in a document d given the class value c, and n is the number of unique words appearing in the document d.Conditional probability P(wijc) can be determined using the relative frequency of the word wi in documents belonging to class c.\n\nHere fic is the number of times that a word wi appears in all documents with the class label c, and fc is the total number of words in documents with class label c in T.\n\nOne advantage of the Multinominal Naive Bayes model is that it can make predictions efficiently."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ncount_vectorizer = feature_extraction.text.TfidfVectorizer(stop_words='english')\n\ntrain_vectors = count_vectorizer.fit_transform(X_train)\ntest_vectors = count_vectorizer.transform(X_test)\n\nclf2 = MultinomialNB()\nclf2.fit(train_vectors, y_train)\n\ny_pred_nb = np.round(clf2.predict(test_vectors))\nprint(y_pred_nb)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Range Accuracy on Multinomial Naive Bayes is 25.4% "},{"metadata":{"trusted":true},"cell_type":"code","source":"acc2 = accuracy_score(y_pred_nb, np.round(y_test))\nprint('Range Accuracy on Multinomial Naive Bayes : {} %'.format(acc2*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets have funn"},{"metadata":{"trusted":true},"cell_type":"code","source":"comment = input(\"Enter comment here...\\n\")\n# print(comment)\n#   if comment == 'q':\n#     break\ncomment = count_vectorizer.transform([comment])\n# print('Score of prediction:'comment)\nrating = clf2.predict(comment)[0]\n\nprint(\"The predicted rating is:\",rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}