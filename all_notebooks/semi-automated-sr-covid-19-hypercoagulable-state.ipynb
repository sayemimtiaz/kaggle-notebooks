{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19 Semi-Automated Rapid Review Workflow\n## Task: What is the best method to combat the hypercoagulable state seen in COVID-19?\n\nThis notebook implements an **iterative, semi-automated workflow** for key components of a **rapid systematic review**.\n\n**[Systematic reviews](https://en.wikipedia.org/wiki/Systematic_review)** are a comprehensive and transparent method for synthesizing evidence from the published literature. What primarily distinguishes systematic reviews from other types of literature reviews is the level of detail and thoroughness; researchers involved in systematic reviews take painstaking care into developing a precise review question, inclusion and exclusion criteria, search strategy, assessments of study quality, and synthesis of results (often in the form of a formal meta-analysis).   \n\n**[Rapid reviews](https://guides.temple.edu/c.php?g=78618&p=4156608)** are a timely alternative when a full systematic review is too resource or time intensive to conduct (systematic reviews often take [12-24 months](https://www.heardproject.org/news/rapid-review-vs-systematic-review-what-are-the-differences/) to complete). Rapid reviews attempt to balance the need for systematic and transparent methods with expediency by simplifying the review components.  This may include reducing the number of databases for search, assigning a single reviewer in each step while another reviewer verifies the results, excluding grey literature, or narrowing the scope of the review.\n\nThis submission deals with two fundamental components of a rapid review: (1) [searching the literature for relevant studies](#Filter-papers), and (2) [extracting key information](#Extract-information) from studies relevant to the research question. While the specific task here is to extract various information related to hypercoagulability and COVID-19, this approach could be applied to any rapid systematic review process.\n\n![Process Diagram](https://i.imgur.com/vc2AVsQ.png)\n\n### Table of Contents\n* [Filter Papers](#Filter-papers)\n* [Extract Information](#Extract-information)\n   * [Sample Size](#Sample-Size)\n   * [Study Type](#Study-Type)\n   * [Severity](#Severity)\n   * [Therapeutic Methods](#Therapeutic-Methods)\n   * [Outcome/Conclusion Excerpt](#Outcome/Conclusion-Excerpt)\n   * [Primary Endpoint](#Primary-Endpoint)\n   * [Clinical Improvement](#Clinical-Improvement)\n* [Generate Final Output](#Generate-Final-Output)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, fix versions of all our dependencies for reproducibility:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q pandas==1.0.3 spacy==2.2.1 scispacy==0.2.4 https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz numpy==1.18.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport warnings\nimport shutil\nimport en_core_sci_md\nimport pandas as pd\nfrom enum import Enum\nfrom spacy.tokens import Doc, Token, Span\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Any, Optional, List, Set\nfrom pprint import pprint\n\nCORD19_INPUT_DIR = Path(\"/kaggle/input/CORD-19-research-challenge\")\nANNOTATIONS_INPUT_FILE = Path(\"/kaggle/input/generate-umls-annotations/umls_annotations.zip\")\nANNOTATION_DIR = Path(\"/tmp/umls_annotations\")\nANNOTATION_DIR.mkdir(exist_ok=True, parents=True)\nshutil.unpack_archive(ANNOTATIONS_INPUT_FILE, ANNOTATION_DIR.parent)\n\nOUTPUT_DIR = Path(\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For filtering papers and most of the information extraction tasks, we'll need to identify specific biological concepts in the text and potentially relate them to each other or other words.  Doing this via manual text matching (regexes, string searching) is difficult and error-prone because of the wide variety of synonyms and related terms for any given biological concept.  Entirely data-driven approaches that might address the synonym problem (similarity calculations/searches using word/sentence embeddings) are also not ideal due to the inability to tweak/tune the results using valuable subject matter expertise.\n\nTherefore, our chosen approach for concept annotation is to use scispacy's [EntityLinker](https://github.com/allenai/scispacy#entitylinker) to annotate [UMLS](https://www.nlm.nih.gov/research/umls/index.html) concepts in the text via string matching.  The UMLS vocabulary encodes a wealth of biological knowledge about concepts and how they relate, saving us a lot of time coming up with synonym lists and identifying related terms.  The annotations are imperfect, since there are many cases where string matching can fail to disambiguate between concepts that are named the same but mean different things (e.g., someone named Parkinson vs Parkinson's disease).  However, the results are fairly good overall and provide a decent baseline.  Improved results could potentially be obtained by training and applying a model which uses machine learning to disambiguate entity mentions, such as [MedCAT](https://github.com/CogStack/MedCAT).\n\nIn the [generate-umls-annotations kernel](https://www.kaggle.com/jasonnance/generate-umls-annotations), we applied the EntityLinker to all the articles in the CORD-19 dataset and saved the annotations.  The result is a directory full of JSON objects containing lists of annotated term data.  We'll load some data in below and display an example annotation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for annotation_file in ANNOTATION_DIR.iterdir():\n    with open(annotation_file, \"r\") as f:\n        paper_annotations = json.load(f)\n        \n    if len(paper_annotations) > 0:\n        pprint((annotation_file.name, paper_annotations[:2]))\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we define a function that provides an iterator over all the JSON metadata documents in the competition input dataset and some other functions that return the title/abstract/other data from the JSON format.  We'll use these to cleanly access the data in each paper.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def all_json_iter() -> Tuple[str, Dict[str, Any]]:\n    \"\"\"\n    Iterate over all data files across all text subsets\n    \"\"\"\n    all_files = CORD19_INPUT_DIR.glob(\n        \"document_parses/*/*.json\"\n    )\n\n    for json_file in all_files:\n        if json_file.name.startswith(\".\"):\n            # There are some garbage files in here for some reason\n            continue\n\n        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n            try:\n                article_json = json.load(f)\n            except Exception:\n                raise RuntimeError(f\"Failed to parse json from {json_file}\")\n\n        # PMC XML, PDF, etc\n        text_type = json_file.parent.name\n\n        yield text_type, article_json\n        \n        \ndef get_annotations(sha: str) -> List[Dict[str, Any]]:\n    with open(ANNOTATION_DIR / f\"{sha}.json\", \"r\") as f:\n        return json.load(f)\n        \n        \ndef get_article_json(sha: str) -> Dict[str, Any]:\n    article_files = list(CORD19_INPUT_DIR.glob(f\"document_parses/*/{sha}.json\"))\n    if len(article_files) == 0:\n        raise RuntimeError(f\"No JSON file found for SHA {sha}\")\n    else:\n        # If there are multiple parses for this document, we'll take the last one\n        # This is intended to match up with how the annotations are generated --\n        # If there are multiple papers with the same ID (SHA), the last one will end\n        # up in the annotations\n        article_file = article_files[-1]\n        \n    with open(article_file, \"r\") as f:\n        return json.load(f)\n        \ndef get_paper_id(article_json: Dict[str, Any]) -> str:\n    return article_json[\"paper_id\"]\n\n\ndef get_title(article_json: Dict[str, Any]) -> str:\n    return article_json[\"metadata\"][\"title\"]\n\n\ndef get_abstract(article_json: Dict[str, Any]) -> str:\n    if \"abstract\" not in article_json:\n        return \"\"\n    return \"\\n\\n\".join(a[\"text\"] for a in article_json[\"abstract\"])\n\n\ndef get_full_text(article_json: Dict[str, Any]) -> str:\n    if \"body_text\" not in article_json:\n        return \"\"\n    return \"\\n\\n\".join(a[\"text\"] for a in article_json[\"body_text\"])\n\n\ndef get_all_text(article_json: Dict[str, Any]) -> str:\n    return f\"{get_abstract(article_json)} {get_full_text(article_json)}\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Filter papers\n\nThe first step of our workflow is to use the annotated terms to filter the papers.  We're interested in all papers that discuss both COVID-19 and hypercoagulability.\n\nWe identify hypercoagulability using a set of related concepts from the UMLS vocabulary identified by the Concept Unique Identifiers (CUIs) in the code below.  Unfortunately, the UMLS release available in scispacy's EntityLinker is from 2017, so it doesn't have any of the newer concepts that were added for COVID-19 and related terms.  Instead, we do a text search based on a list of synonyms.\n\nNote this is the first place in the workflow where iteration is necessary.  New relevant terms may be discovered, and terms which were previously thought to be relevant may turn out to be irrelevant.  The filter criteria here should be easy to edit going forward, so iteration can be rapid.  The final state of this notebook submission represents multiple rounds of iterating on filter terms.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filtered_annotations = {}\n\nCOVID19_TERMS = tuple(t.lower() for t in (\n    \"covid\",\n    \"covid-\",\n    \"SARS-CoV-2\",\n    \"HCoV-19\",\n    \"coronavirus 2\",\n))\n\nHYPERCOAG_CUIS = set((\n    \"C0398623\",  # Thrombophilia\n    \"C2984172\",  # F5 Leiden Allele\n    \"C0311370\",  # Lupus anticoagulant disorder\n    \"C1704321\",  # Nephrotic Syndrome, Minimal Change\n    \"C3202971\",  # Non-Infective Endocarditis\n    \"C0040053\",  # Thrombosis\n    \"C2826333\",  # D-Dimer Measurement\n    \"C0060323\",  # Fibrin fragment D\n    \"C3536711\",  # Anti-coagulant [EPC]\n    \"C2346807\",  # Anti-Coagulation Factor Unit\n    \"C0012582\",  # dipyridamole\n))\n\nnum_papers_missing_text = 0\nnum_annotations_found = 0\nnum_annotations_missing = 0\n\nmetadata = pd.read_csv(CORD19_INPUT_DIR / \"metadata.csv\")\nshas = set()\npublish_times = {}\ndois = {}\n# The sha field may have multiple SHAs delimited by semicolons\nfor multi_sha, publish_time, doi in zip(metadata[\"sha\"], metadata[\"publish_time\"], metadata[\"doi\"]):\n    if not pd.isnull(multi_sha):\n        for sha in multi_sha.split(\"; \"):\n            shas.add(sha)\n            publish_times[sha] = publish_time\n            dois[sha] = doi\n\nfor sha in shas:\n    try:\n        paper_json = get_article_json(sha)\n    except RuntimeError:\n        # This paper doesn't have a JSON file for its full text\n        num_papers_missing_text += 1\n        continue\n        \n    paper_annotations = get_annotations(sha)\n    if paper_annotations is None:\n        # Missing annotations here indicates a desync between our generated annotations\n        # and the input data -- keep track but continue\n        num_annotations_missing += 1\n        continue\n    else:\n        num_annotations_found += 1\n    \n    found_covid19 = False\n    found_hypercoag = False\n    \n    search_text = f\"{get_title(paper_json)}\\n{get_abstract(paper_json)}\".lower()\n    \n    for term in COVID19_TERMS:\n        if term in search_text:\n            found_covid19 = True\n            break\n    if not found_covid19:\n        continue\n    \n    paper_concepts = set(concept[\"cui\"] for concept in paper_annotations)\n    for hypercoag_cui in HYPERCOAG_CUIS:\n        if hypercoag_cui in paper_concepts:\n            found_hypercoag = True\n            break\n            \n    if found_covid19 and found_hypercoag:\n        filtered_annotations[sha] = paper_annotations[:]\n\n\nprint(f\"Checked {num_annotations_found} paper parses with annotations.\\n\"\n      f\"Ignored {num_annotations_missing} paper parses without annotations and {num_papers_missing_text} papers without text available.\\n\"\n      f\"Identified {len(filtered_annotations)} papers related to COVID-19 and hypercoagulability.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the final set of papers we're interested in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(OUTPUT_DIR / \"filtered_annotations.json\", \"w\") as f:\n    json.dump(filtered_annotations, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Extract information\n\nOnce we have the set of papers filtered to only those containing our topics of interest, we need to extract the relevant information.  This is a tricky process which is going to require entirely different methods for different pieces of information.  Without a large training set to use data-driven methods (like training a machine learning model for classification), we're limited to heuristics developed using subject matter expertise.  We fully acknowledge they're imperfect, but we can tweak them as we identify gaps, and being able to automatically extract at least some of the required information can save human reviewers a lot of time.\n\nThis is the other part of the workflow which requires a lot of iteration.  The performance of each heursitic can only be improved through back-and-forth with a subject matter expert after reviewing the performance and identifying potential areas of improvement.  The goal of the heuristics should be to err on the side of capturing too much information -- then it's easy for human reviewers to quickly look through and identify the correct info, whereas if info is missing, they have to go back and read the abstract and potentially full text.\n\nWe use the title and abstract for information extraction due to memory limitations generating annotations for the full text of all papers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_sci_md.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/493788\ndef text2int(textnum, numwords={}):\n    \"\"\"\n    Convert anything that matches the spaCy \"like_num\" rule to an integer.\n    \n    https://spacy.io/api/token#attributes\n    \"\"\"\n    try:\n        # Commas trip up the int parser, so remove them if there are any\n        return int(textnum.replace(\",\", \"\"))\n    except ValueError:\n        pass\n    \n    if not numwords:\n        units = [\n          \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n          \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n          \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n        ]\n\n        tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n\n        scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n\n        numwords[\"and\"] = (1, 0)\n        for idx, word in enumerate(units):\n            numwords[word] = (1, idx)\n        for idx, word in enumerate(tens):\n            numwords[word] = (1, idx * 10)\n        for idx, word in enumerate(scales):\n            numwords[word] = (10 ** (idx * 3 or 2), 0)\n\n    current = result = 0\n    for word in textnum.split():\n        if word not in numwords:\n            raise ValueError(\"Illegal word: \" + word)\n\n        scale, increment = numwords[word]\n        current = current * scale + increment\n        if scale > 100:\n            result += current\n            current = 0\n\n    return result + current\n\ndef get_span(annotation: Dict[str, Any], doc: Doc) -> Span:\n    \"\"\"\n    Return the span corresponding to the given annotation.\n        \n    Assumes the start/end indices in the annotation line up correctly with the document\n    (i.e., the document was constructed exactly the same way in the original annotation process\n    as it was in the given parsed document).\n    \"\"\"\n    return doc.char_span(annotation[\"start\"], annotation[\"end\"])\n\ndef get_context(annotation: Dict[str, Any], doc: Doc) -> str:\n    \"\"\"\n    Return the context (sentence) in the document containing the given annotation.\n    \"\"\"\n    return get_span(annotation, doc).sent.text\n\ndef get_root_token(annotation: Dict[str, Any], doc: Doc) -> Token:\n    \"\"\"\n    Return the root token for the given annotation.\n    \"\"\"\n    return get_span(annotation, doc).root","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Size\n\nTo identify the sample size heuristically, we'll use spaCy's dependency parsing to find nouns representing a study participant (\"subject\", \"patient\", etc) and record any numbers that were associated with them.  For each extracted sample size, we return the containing sentence, so a human reviewer can quickly determine whether they agree without reading the full text.\n\nThis produces a lot of false positives which are difficult to disambiguate from the language features alone -- ex.  a study may mention various subsets of its cohort, or a review paper may be describing the sample size of the papers it's reviewing.  We'll do a best-effort disambiguation by taking the maximum number if there are multiple numbers, to attempt to avoid picking up subsets of the sample.\n\nWe note this approach fails in some cases on this dataset because line numbers are embedded in many PDFs which confuse spaCy's dependency parser.  For example:\n\n> This 95 model is first trained on patients from a single hospital and then externally validated on 96 patients from four other hospitals. We achieve strong performance, notably predicting 97 mortality\n\nOur somewhat naive approach doesn't recognize that \"96\" in the above excerpt is a line number and reports it incorrectly.  Implementing a method to clean the line numbers (or parsing the PDFs in a way which doesn't include them in the first place) should improve the performance.\n\nWe noticed other failure cases which aren't easily addressable using this approach:\n\n - Case studies with a single patient\n - Sentences where the spaCy model's part-of-speech tagger fails (ex. in \"83 confirmed patients\", \"confirmed\" could be parsed as a verb rather than an adjective)\n - Papers which don't explicitly list the full sample size (ex. \"100 COVID-19 patients and 200 non-COVID-19 patients\")\n \nHowever, we find that in general, the heuristic performs fairly well, and returning the context of the sentence along with the extracted sample size allows a human reviewer to very quickly confirm whether the extracted value is correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLE_SIZE_NOUNS = set((\"patient\", \"subject\", \"case\", \"birth\"))\n\ndef find_sample_size(doc: Doc) -> Tuple[Optional[int], Optional[str], Optional[str]]:\n    \"\"\"\n    For a parsed spaCy document representing a paper, try to identify the sample size heuristically.\n    If possible, return a tuple containing the sample size, noun describing\n    the sample, and the sentence that inference was generated from.\n    \n    If not, return None for each tuple element.\n    \"\"\"\n    sample_size_candidates = []\n    \n    for tok in doc:\n        if tok.like_num and tok.head.lemma_ in SAMPLE_SIZE_NOUNS and tok.dep_ == \"nummod\":\n            try:\n                sample_size_int = text2int(tok.text.lower())\n            except ValueError:\n                continue\n            sample_size_candidates.append((sample_size_int, tok.head.text, tok.sent.text))\n    \n    if len(sample_size_candidates) == 0:\n        return (None, None, None)\n    chosen_candidate = max(sample_size_candidates, key=lambda c: c[0])\n    return chosen_candidate\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Study Type\n\nThere's a fairly complex set of definitions for study types -- we'll try to pull out some concepts and tokens that might indicate one study type over the other.  If we can't make a reasonable guess, we'll leave it blank.\n\nNote we introduce an additional study type: \"Observational\".  In many cases, distinguishing between retrospective and prospective observational studies is very difficult, so we include a fallback umbrella term which encapsulates both.\n\nWe also didn't find enough instances of the following study types to develop rules for them:\n\n- Cross-sectional study\n- Case series\n- Ecological regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SYS_REV_CUIS = set((\n    \"C1955832\",  # Review, Systematic\n    \"C0282458\",  # Meta-Analysis (publications)\n))\n\nEXP_REV_CUIS = set((\n    \"C0282443\",  # Review [Publication Type]\n))\n\nSIM_CUIS = set((\n    \"C0376284\",  # Machine Learning\n    \"C0683579\",  # scenario\n))\n\nRET_OBS_CUIS = set((\n    \"C0035363\",  # Retrospective Studies\n    \"C2362543\",  # Electronic Health Records\n    \"C2985505\",  # Retrospective Cohort Study\n))\n\nOBS_CUIS = set((\n    \"C0030705\",  # Patients\n))\n\nEDIT_WORDS = set((\n    \"editor\",\n    \"opinion\",\n))\n\nRET_OBS_WORDS = set((\n    \"retrospective\",\n    \"retrospectively\",\n    \"autopsy\",\n))\n\nPROS_OBS_WORDS = set((\n    \"prospective\",\n    \"prospectively\",\n    \"enrolled\",\n))\n\n\nclass StudyType(Enum):\n    PROS_OBS = \"Prospective observational study\"\n    RET_OBS = \"Retrospective observational study\"\n    OBS = \"Observational study\"\n    SYS_REV = \"Systematic review and meta-analysis\"\n    EXP_REV = \"Expert review\"\n    SIM = \"Simulation\"\n    EDIT = \"Editorial\"\n\ndef find_study_type(doc: Doc, annotations: List[Dict[str, Any]]) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Attempt to identify study type using words and concepts in the document.\n    \"\"\"\n    study_type = None\n    study_type_words = set()\n    contexts = []\n    # Fallback classification for studies which are clearly observational, but we can't\n    # tell whether it's prospective or retrospective\n    is_observational = False\n\n    doc_annotations = {a[\"cui\"]: a for a in annotations}\n    \n    def update_study_type(new_study_type: StudyType, word: str, context: str):\n        nonlocal study_type\n        study_type = new_study_type\n        study_type_words.add(word)\n        contexts.append(context)\n        \n    def check_cui_set(cui_set: Set[str], cui_set_study_type: StudyType):\n        for cui in cui_set:\n            if cui in doc_annotations:\n                annotation = doc_annotations[cui]\n                update_study_type(cui_set_study_type,\n                                  annotation[\"canonical_name\"],\n                                  get_context(annotation, doc))\n                \n    def check_word_set(word_set: Set[str], word_set_study_type: StudyType):\n        for sent in doc.sents:\n            for tok in sent:\n                if tok.lemma_.lower() in word_set:\n                    update_study_type(word_set_study_type, tok.text, sent.text)\n        \n    # Check CUIs first, since those should be more reliable\n    for cui_set, cui_set_study_type in (\n        (SYS_REV_CUIS, StudyType.SYS_REV),\n        (EXP_REV_CUIS, StudyType.EXP_REV),\n        (SIM_CUIS, StudyType.SIM),\n        (RET_OBS_CUIS, StudyType.RET_OBS),\n    ):\n        if study_type is None:\n            check_cui_set(cui_set, cui_set_study_type)\n    \n    # Check word sets next\n    for word_set, word_set_study_type in (\n        (EDIT_WORDS, StudyType.EDIT),\n        (RET_OBS_WORDS, StudyType.RET_OBS),\n        (PROS_OBS_WORDS, StudyType.PROS_OBS)\n    ):\n        if study_type is None:\n            check_word_set(word_set, word_set_study_type)\n            \n    # Finally, if we still don't have a study type, check the fallback umbrella study type\n    # for all observational studies\n    if study_type is None:\n        check_cui_set(OBS_CUIS, StudyType.OBS)\n        \n    return (\n        None if study_type is None else study_type.value,\n        \"; \".join(list(study_type_words)),\n        \"\\n\".join(contexts)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Severity\n\nThe severity is tougher to identify, since there are several general words which may be used to describe it.  We'll make a list of terms related to severity, pull them out, report all the unique ones, and report any context that could help a human reviewer make a quick decision.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEVERITY_WORDS = set((\"mild\", \"severe\", \"critical\", \"ICU\", \"intensive care unit\"))\n\ndef find_severity(doc: Doc) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Attempt to find case severity in the document.\n    \"\"\"\n    severity = set()\n    contexts = []\n    \n    for tok in doc:\n        if tok.lemma_ in SEVERITY_WORDS:\n            severity.add(tok.text.lower())\n            contexts.append(tok.sent.text)\n            \n    if len(severity) == 0:\n        return None, None\n\n    return \"; \".join(list(severity)), \"\\n\".join(contexts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Therapeutic Methods\n\nHere, we'll use the annotated UMLS concepts to identify therapeutic methods and return their contexts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"THERAPEUTIC_METHOD_CUIS = set((\n    \"C0012582\",  # dipyridamole\n    \"C1963724\",  # Antiretroviral therapy\n    \"C0770546\",  # heparin, porcine\n))\n\ndef find_therapeutic_methods(doc: Doc, annotations: List[Dict[str, Any]]) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Attempt to find therapeutic methods by UMLS concepts in the document.\n    \"\"\"\n    methods = set()\n    contexts = []\n    \n    for annotation in annotations:\n        if annotation[\"cui\"] in THERAPEUTIC_METHOD_CUIS:\n            methods.add(annotation[\"canonical_name\"])\n            contexts.append(get_context(annotation, doc))\n            \n    return (\n        \"; \".join(list(methods)),\n        \"\\n\".join(contexts),\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outcome/Conclusion Excerpt\n\nHere, we'll search for various words that might indicate a sentence describes an outcome or conclusion and report all such sentences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTCOME_CUIS = set((\n    \"C0332281\",  # Associated with\n    \"C0392756\",  # Reduced\n    \"C1260953\",  # Suppressed\n    \"C0309872\",  # PREVENT (product) [proxy for word \"prevent\"]\n    \"C0278252\",  # Prognosis bad\n    \"C0035648\",  # risk factors\n    \"C0184511\",  # Improved\n    \"C0442805\",  # Increased\n    \"C0205216\",  # Decreased\n))\n\ndef find_outcomes(doc: Doc, annotations: List[Dict[str, Any]]) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Attempt to find outcome excerpts by UMLS concepts in the document.\n    \"\"\"\n    outcome_words = set()\n    contexts = []\n    \n    for annotation in annotations:\n        if annotation[\"cui\"] in OUTCOME_CUIS:\n            outcome_words.add(annotation[\"canonical_name\"])\n            contexts.append(get_context(annotation, doc))\n            \n    return (\n        \"; \".join(list(outcome_words)),\n        \"\\n\".join(contexts),\n    )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Primary Endpoint\n\nWe'll search for words that describe the primary endpoint(s) of the study and report all such sentences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ENDPOINT_CUIS = set((\n    \"C0011065\",  # Cessation of life\n    \"C0026565\",  # Mortality Vital Statistics\n))\n\ndef find_endpoints(doc: Doc, annotations: List[Dict[str, Any]]) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Attempt to find information related to primary endpoints by UMLS concepts in the document.\n    \"\"\"\n    endpoint_words = set()\n    contexts = []\n    \n    for annotation in annotations:\n        if annotation[\"cui\"] in ENDPOINT_CUIS:\n            endpoint_words.add(annotation[\"canonical_name\"])\n            contexts.append(get_context(annotation, doc))\n            \n    return (\n        \"; \".join(list(endpoint_words)),\n        \"\\n\".join(contexts),\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clinical Improvement\n\nThis is a tricky one to identify with heuristics, since it's very general.  We're going to simplify it down to the problem of identifying a set of biological mechanisms/functions and tagging them as \"good\" or \"bad\" (from a clinical health perspective).  We can then identify all instances of these mechanisms in the text and use the spaCy model's dependency parser to identify whether those functions went \"up\" or \"down\".  We make the extremely naive assumption that a \"good\" mechanism going \"up\" or a \"bad\" mechanism going \"down\" leads to clinical improvement and vice versa.\n\nWe'll use CUIs when possible to ensure maximal coverage of synonyms and incorporating the biological knowledge in UMLS, but we'll use exact token searches in some cases where CUIs aren't tagged or otherwise don't exist for certain concepts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GOOD_MECHANISM_CUIS = set((\n    \"C2247948\",  # response to type I interferon\n    \"C0005821\",  # Blood Platelets\n    \"C0200635\",  # Lymphocyte Count measurement\n    \"C0301863\",  # \"U\" lymphocyte\n    \"C1556326\",  # Adverse Event Associated with Coagulation\n    \"C0019010\",  # Hemodynamics\n    \"C1527144\",  # Therapeutic Effect\n    \n))\n\nBAD_MECHANISM_CUIS = set((\n    \"C1883725\",  # Replicate\n    \"C0042774\",  # Virus Replication\n    \"C0677042\",  # Pathology processes\n    \"C0398623\",  # Thrombophilia\n    \"C2826333\",  # D-Dimer Measurement\n    \"C1861172\",  # Venous Thromboembolism\n))\n\nUP_WORDS = set((\n    \"elicit\",\n    \"good\",\n))\n\nUP_CUIS = set((\n    \"C0442805\",  # Increase\n    \"C0205250\",  # High\n    \"C2986411\",  # Improvement\n))\n\nDOWN_WORDS = set((\n    \"ameliorate\",\n))\n\nDOWN_CUIS = set((\n    \"C1260953\",  # Suppressed\n    \"C0205216\",  # Decreased\n    \"C0392756\",  # Reduced\n    \"C1550458\",  # Abnormal\n))\n\ndef find_clinical_improvement(\n    doc: Doc,\n    annotations: List[Dict[str, Any]]\n) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Identify phrases corresponding to clinical improvement (or not).  Return a tuple with 3 strings:\n    \"y/n\" based on whether more evidence for improvement or worsening is found, evidence for improvement, and\n    evidence for worsening.\n    \"\"\"\n    improvement_evidence = set()\n    worsening_evidence = set()\n    \n    # Locate tokens corresponding to all categories (good/bad mechanisms, up/down)\n    bad_spans = set()\n    good_spans = set()\n    up_spans = set()\n    down_spans = set()\n    \n    for tok in doc:\n        for word_set, span_set in (\n            (UP_WORDS, up_spans),\n            (DOWN_WORDS, down_spans),\n        ):\n            if tok.lemma_ in word_set:\n                span_set.add(doc[tok.i:tok.i+1])\n    \n    for annotation in annotations:\n        for cui_set, span_set in (\n            (GOOD_MECHANISM_CUIS, good_spans),\n            (BAD_MECHANISM_CUIS, bad_spans),\n            (UP_CUIS, up_spans),\n            (DOWN_CUIS, down_spans),\n        ):\n            if annotation[\"cui\"] in cui_set:\n                annotation_span = get_span(annotation, doc)\n                span_set.add(annotation_span)\n    \n    # Check dependencies to see if any of the discovered tokens relate in ways that\n    # might provide evidence one for improvement or worsening\n    for mechanism_set, modifier_set, evidence_set in (\n        (bad_spans, up_spans, worsening_evidence),\n        (bad_spans, down_spans, improvement_evidence),\n        (good_spans, up_spans, improvement_evidence),\n        (good_spans, down_spans, worsening_evidence),\n    ):\n        for mechanism_span in mechanism_set:\n            mechanism_tok = mechanism_span.root\n            for modifier_span in modifier_set:\n                modifier_tok = modifier_span.root\n                if modifier_tok in mechanism_tok.children:\n                    evidence_set.add((modifier_span, mechanism_span))\n            \n    improvement = None\n    if len(improvement_evidence) > len(worsening_evidence):\n        improvement = \"y\"\n    elif len(improvement_evidence) < len(worsening_evidence):\n        improvement = \"n\"\n        \n    def format_evidence(evidence_set):\n        return \"; \".join(\n            \" \".join((modifier_span.text, mechanism_span.text)) for modifier_span, mechanism_span in evidence_set\n        )\n        \n    return (\n        improvement,\n        format_evidence(improvement_evidence),\n        format_evidence(worsening_evidence),\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Final Output\n\nWe can apply all of the above heuristics to each document in turn to automatically generate a spreadsheet for human review.  Human reviewers can manually correct items which need correcting and provide feedback to improve the heuristics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_data = []\n\nfor sha, paper_annotations in filtered_annotations.items():\n    try:\n        article_json = get_article_json(sha)\n    except RuntimeError:\n        warnings.warn(f\"No article JSON found for SHA {sha}\")\n        continue\n    \n    doc_text = f\"{get_title(article_json)}\\n\\n{get_abstract(article_json)}\"\n    doc = nlp(doc_text)\n    \n    publish_time = publish_times[sha]\n    doi = dois[sha]\n    sample_size, sample_unit, sample_size_context = find_sample_size(doc)\n    severity, severity_context = find_severity(doc)\n    therapeutic_methods, therapeutic_method_context = find_therapeutic_methods(doc, paper_annotations)\n    study_type, study_type_words, study_type_context = find_study_type(doc, paper_annotations)\n    outcome_words, outcome_context = find_outcomes(doc, paper_annotations)\n    endpoints, endpoint_context = find_endpoints(doc, paper_annotations)\n    improvement, improvement_evidence, worsening_evidence = find_clinical_improvement(doc, paper_annotations)\n\n    auto_data.append({\n        \"Paper ID\": get_paper_id(article_json),\n        \"Title\": get_title(article_json),\n        \"Abstract\": get_abstract(article_json),\n        \"DOI\": doi,\n        \"Date\": publish_time,\n        \"Sample Size\": sample_size,\n        \"Sample Unit\": sample_unit,\n        \"Sample Size Context\": sample_size_context,\n        \"Severity\": severity,\n        \"Severity Context\": severity_context,\n        \"Therapeutic Methods\": therapeutic_methods,\n        \"Therapeutic Method Context\": therapeutic_method_context,\n        \"Study Type\": study_type,\n        \"Study Type Words\": study_type_words,\n        \"Study Type Context\": study_type_context,\n        \"Outcome Words\": outcome_words,\n        \"Outcome Context\": outcome_context,\n        \"Endpoint Words\": endpoints,\n        \"Endpoint Context\": endpoint_context,\n        \"Clinical Improvement\": improvement,\n        \"Clinical Improvement Evidence\": improvement_evidence,\n        \"Clinical Worsening Evidence\": worsening_evidence,\n    })\n    \nauto_df = pd.DataFrame(auto_data).set_index(\"Paper ID\")\nauto_df.to_csv(OUTPUT_DIR / \"What is the efficacy of novel therapeutics being tested currently_.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nWhile we provide a semi-automated workflow to help with components of a rapid review (specifically, \"the best method to combat the hypercoagulable state seen in COVID-19\"), there are several areas for future work.  First, we restricted our submission to the CORD-19 literature set, which only contains articles from PMC, the WHO, bioRxiv, and medRxiv.  Though examining fewer databases and grey literature is a common strategy in rapid reviews, it may bias the findings towards articles that are more convenient to access. Additionally, we do not tackle other important aspects of rapid reviews such as a risk of bias assessment, reliablity checks by multiple reviewers, or a meta-analysis. Given we did not have a targeted [PICO](https://canberra.libguides.com/c.php?g=599346&p=4149722) statement defining our intended population, comparison groups, etc., as part of the challenge, we felt these steps would be better addressed in future work supported by domain experts and systematic review methodologists. \n\nMethodologically, our approaches relies heavily on heuristics, ontologies, and domain input. While data engineering folk wisdom [champions the use of heuristics](https://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning) as a strong baseline, ideally, machine learning algorithms could improve upon these heuristics in a more systematic way. Supervised machine learning was not strongly considered for information extract due to the limited number of articles and labeled data. Future work could explore the use of methods such as [data programming](https://dawn.cs.stanford.edu/pubs/snorkel-nips2016.pdf) to integrate heuristics as noisy labels for supervised learning.   ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}