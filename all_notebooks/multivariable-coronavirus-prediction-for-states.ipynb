{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n<html>\n\t<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.1/css/bootstrap.min.css\" integrity=\"sha384-VCmXjywReHh4PwowAiWNagnWcLhlEJLA5buUprzK8rxFgeH0kww/aWY76TfkUoSX\" crossorigin=\"anonymous\">\n    <body>\n\t\t<div class=\"container-fluid\">\n\t\t\t<h1> Multi-variable Coronavirus Projections for US States <img src='https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftoppng.com%2Fuploads%2Fthumbnail%2Fcoronavirus-covid-19-11582576817hpyucwma6o.png&f=1&nofb=1' width='50', height='50'> </h1>\n\t\t\t<p class=\"text-monospace\"> This notebook aims to predict future coronvirus cases in US States. It will use \n\t\t\tmultiple variables as input. They include population density, testing, as well as mobility data.</p>\n\t\t\t<p class=\"text-monospace\"> Sources:  </p>\n\t\t\t<ul class=\"list-group list-group-horizontal-lg\">\n\t\t\t  <li class=\"list-group-item\">Mobility data: <a href='https://www.apple.com/covid19/mobility'>Apple</a> </li>\n\t\t\t  <li class=\"list-group-item\">COVID-19 tracking API: <a href='https://covidtracking.com/'> The Covid Tracking</a></li>\n\t\t\t  <li class=\"list-group-item\">COVID-19 State Data: <a href='https://www.kaggle.com/nightranger77/covid19-state-data'> Nightranger77's Dataset</a></li>\n                 <li class=\"list-group-item\">COVID-19 Image: <a href='https://toppng.com/uploads/thumbnail/coronavirus-covid-19-11582576817hpyucwma6o.png'> Toppng.com\n                     </a></li>\n\t\t\t</ul>\n\t\t\t<br>\n\t\t\t<b><p class=\"text-monospace\">Feel free to provide me with feedback. I really want to hear about your ideas.</p></b>\n            <p class=\"text-monospace\">Longer outputs are being hidden.</p>\n\t\t\t<p class=\"text-monospace\">Check out my visualization notebook <a href='https://www.kaggle.com/therealcyberlord/coronavirus-covid-19-visualization-prediction'>here</a>.</p>\n\n    \n<html>","execution_count":null},{"metadata":{"id":"umGdYuVqBMlF","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression\nimport datetime\nimport matplotlib.pyplot as plt \nimport warnings\nimport tensorflow as tf \nfrom scipy.stats import exponweib\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the coronavirus dataset from the covid tracking API","execution_count":null},{"metadata":{"id":"l02morHgBdQd","outputId":"df2ea4cb-270a-43dd-d9bf-a6ccac0ad6b9","trusted":true},"cell_type":"code","source":"data = pd.read_csv('https://covidtracking.com/api/v1/states/daily.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking out the data for null values ","execution_count":null},{"metadata":{"id":"JPs-fzdvYIQF","outputId":"25b0cc7e-547c-441d-d40c-198d687615dc","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"Y4BMHcg6B45l","outputId":"fcd30482-7579-4ae7-d9c3-1385160456e3","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sort values by date since we are approaching it in a chronological order","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sort_values('date', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling null values ","execution_count":null},{"metadata":{"id":"VjnVwWHjXe1G","trusted":true},"cell_type":"code","source":"# drop columns with 50% null values \ndef drop_na_50(df):\n    columns = df.columns\n    for col in columns:\n       if data[col].isnull().sum() > 0.5 * len(df):\n         df.drop(col, 1, inplace=True)\n    return df \n         \nnew_data = drop_na_50(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"7Y4R0eftC9tS","outputId":"eccb3305-f434-4060-f371-6e429f5ccc31","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"new_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's look at Connecticut as an example. ","execution_count":null},{"metadata":{"id":"Ot3RqDw77Umg","outputId":"69db2737-c219-4c09-fbc1-659b26c69bb2","trusted":true},"cell_type":"code","source":"new_data[new_data.state=='CT'].hist(bins=10, figsize=(20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop the features we don't need ","execution_count":null},{"metadata":{"id":"Fh26BfKeIhQJ","trusted":true},"cell_type":"code","source":"# drop variables with few/no variations\nnew_data.drop(['commercialScore', 'fips', 'hospitalizedIncrease', 'negativeRegularScore', 'negativeScore', 'positiveScore', 'score'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"tOz1B5wVJU5N","outputId":"dc30a8c3-77c5-4be4-a053-f1e6f1ce8798","trusted":true},"cell_type":"code","source":"new_data[new_data.state=='CT'].hist(bins=10, figsize=(20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"v5uYkDXRJzXD","outputId":"6ef37f7a-4dc2-4e0d-8f38-37498a87ba01","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"new_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need a way to map states to their abbreviations. This will help us later in the next dataset.","execution_count":null},{"metadata":{"id":"QoeoctNvcVKE","trusted":true},"cell_type":"code","source":"# dictonary for mapping state to abbreviation \nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Again, let's use Connecticut as an example. It is an amazing state. ","execution_count":null},{"metadata":{"id":"Lvr2K6VPcd4o","outputId":"efd9a51f-cc38-4b5f-cb1c-f592803ee823","trusted":true},"cell_type":"code","source":"us_state_abbrev['Connecticut']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load state information, we want the population density from each state from this. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this data frame is going to provide static variables\nadditional_data = pd.read_csv('../input/covid19-state-data/COVID19_state.csv')\nadditional_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"additional_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# additional_data.hist(bins=10, figsize=(20, 15))\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use the dictionary we created before to ensure that data are consistent ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make the data consistent with the other data frame \nfor i in additional_data.State.unique():\n    additional_data.replace(i, us_state_abbrev[i], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a look at California's population density. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"additional_data[additional_data.State=='CA']['Pop Density']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# incoporate population density into prediction \npop_dense = [] \nfor i in new_data.state:\n    pop_dense.append(additional_data[additional_data.State==i]['Pop Density'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data['pop_dense'] = pop_dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data['pop_dense'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale the popluation density between 0 and 1. It is always good to normalize your data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scales the population density based on all the states \nmin_max_scaler = MinMaxScaler()\nnew_data['pop_dense'] = min_max_scaler.fit_transform(np.array(new_data['pop_dense']).reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data['pop_dense'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at Apple's mobility data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#this dataset is going to provide mobility information within the US\n\nmobility_data = pd.read_csv('https://covid19-static.cdn-apple.com/covid19-mobility-data/2014HotfixDev17/v3/en-us/applemobilitytrends-2020-08-16.csv')\nmobility_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mobility_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make the data consistent with the other data frame \nfor i in mobility_data['sub-region'].unique():\n    if i in us_state_abbrev.keys():\n        mobility_data.replace(i, us_state_abbrev[i], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mobility_data['transportation_type'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We want to make sure the dates from the mobility dataset matches with others. ","execution_count":null},{"metadata":{"id":"E2Ls7FugEsvH","trusted":true},"cell_type":"code","source":"# get individual states and dates\nunique_states = new_data.state.unique()\nunique_states.sort()\nunique_dates = new_data.date.unique()\n\n# making sure that the dates match between mobility and testing/cases\nmobility_latest_date = datetime.datetime.strptime(mobility_data.columns[-1], '%Y-%m-%d').strftime('%Y%m%d')\nmobility_latest_index = np.where(unique_dates == int(mobility_latest_date))[0][0]\n\n# start from a later date 3/1/2020\nunique_dates = unique_dates[39:mobility_latest_index+1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple function to get mobility of a state at a particular day.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# gets the mobility information of a particular day\ndef get_mobility_by_state(transport_type, state, day):\n    return mobility_data[mobility_data['sub-region']==state][mobility_data['transportation_type']==transport_type].sum()[day]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_mobility_by_state('walking', 'FL', '2020-03-01')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We also want the date format to match with others as well ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# change the date format to match the mobility data \nrevised_unique_dates = [] \nfor i in range(len(unique_dates)):\n    revised_unique_dates.append(datetime.datetime.strptime(str(unique_dates[i]), '%Y%m%d').strftime('%Y-%m-%d'))\nrevised_unique_dates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at Florida's mobility at a random day.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_mobility_by_state('transit', 'FL', revised_unique_dates[9]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We want to convert our dates into numbers. Our machine learning algorithm cannot take text.","execution_count":null},{"metadata":{"id":"vsl1Gm0AbbmC","trusted":true},"cell_type":"code","source":"def convert_date_to_int(d):\n    return [i for i in range(len(revised_unique_dates))]\n\ndays_since_3_1 = convert_date_to_int(revised_unique_dates)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are predicting 10 days. Extrapolation way into the future may not yield the most accurate results. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days_ahead = 10\nfuture_dates = [i for i in range(len(revised_unique_dates)+days_ahead)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This is a SVM implementation I wrote. Feel free to use this instead of bayesian ridge. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def svm_reg(X_train, X_test, y_train, y_test, future_forecast, state):\n        \n    svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01, epsilon=1, degree=4, C=0.1)\n    svm_confirmed.fit(X_train, y_train)\n    test_svm_pred = svm_confirmed.predict(X_test)\n    svm_pred = svm_confirmed.predict(future_forecast)\n    \n    plt.plot(y_test)\n    plt.plot(test_svm_pred)\n    plt.title('Testing Set Evaluation for {}'.format(state))\n    plt.xlabel('Days since 3/1/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.legend(['Actual', 'Predicted'])\n    plt.show()\n    \n    print('MAE:', mean_absolute_error(test_svm_pred, y_test))\n    print('MSE:',mean_squared_error(test_svm_pred, y_test))\n\n    # plot the graph to see compare predictions and actual coronavirus cases\n    plt.plot(positive)\n    plt.plot(svm_pred)\n    plt.title('Coronavirus Cases in {}'.format(state))\n    plt.legend(['Actual cases', 'Predicted cases using support vector regression'])\n    plt.xlabel('Days since 3/1/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.show()\n    print('Completed:', state)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayesian ridge implementation of the prediction model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_ridge(X_train, X_test, y_train, y_test, future_forecast, state):\n        \n    # convert data to be compatible with polynomial regression\n    bayesian_poly = PolynomialFeatures(degree=3)\n    bayesian_poly_X_train = bayesian_poly.fit_transform(X_train)\n    bayesian_poly_X_test = bayesian_poly.fit_transform(X_test)\n    bayesian_poly_future_forecast = bayesian_poly.fit_transform(future_forecast)\n    \n    # polynomial regression model\n    # bayesian ridge polynomial regression\n    tol = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n    alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    normalize = [True, False]\n    fit_intercept = [True,  False]\n    lambda_init = [1e-2, 1e-1, 1, 1e1]\n\n    bayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2, \n                    'normalize' : normalize, 'fit_intercept': fit_intercept, 'lambda_init' : lambda_init}\n\n    bayesian = BayesianRidge()\n    bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_root_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=200, verbose=1)\n    bayesian_search.fit(bayesian_poly_X_train, y_train)\n    \n    # get the best estimator \n    best_params = bayesian_search.best_params_\n    bayesian_confirmed = BayesianRidge(**best_params)\n    bayesian_confirmed.fit(bayesian_poly_X_train, y_train)\n    \n    test_bayesian_pred = bayesian_confirmed.predict(bayesian_poly_X_test)\n    bayesian_pred = bayesian_confirmed.predict(bayesian_poly_future_forecast)\n    \n    plt.plot(y_test)\n    plt.plot(test_bayesian_pred)\n    plt.title('Testing Set Evaluation for {}'.format(state))\n    plt.xlabel('Days since 3/1/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.legend(['Actual', 'Predicted'])\n    plt.show()\n    \n    print('MAE:', mean_absolute_error(test_bayesian_pred, y_test))\n    print('MSE:',mean_squared_error(test_bayesian_pred, y_test))\n    print('Weight:', bayesian_confirmed.coef_)\n\n    # plot the graph to see compare predictions and actual coronavirus cases\n    plt.plot(positive)\n    plt.plot(bayesian_pred)\n    plt.title('Coronavirus Cases in {}'.format(state))\n    plt.legend(['Actual cases', 'Predicted cases using bayesian ridge'])\n    plt.xlabel('Days since 3/1/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.show()\n    print('Completed:', state)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function for finding daily change \ndef daily_change(y2, y1):\n    return (y2-y1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are taking moving averages for coronavirus cases (our label). It gives us a better picture than a single day value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get moving average for positive case \n\ndef moving_positive_cases(data, window_size):\n    moving_positive = []\n    for i in range(len(data)):\n        if i + window_size < len(data):\n            moving_positive.append(np.mean(data[i:i+window_size]))\n        else:\n            moving_positive.append(np.mean(data[i:len(data)]))\n    return moving_positive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We want to extrapolate testing information using a polynomial regression. This will provide us with values for future prediction. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def future_testing_extrapolation(X, y, future_forecast, state):\n    poly = PolynomialFeatures(degree=3)\n    poly_X = poly.fit_transform(X)\n    poly_future_forecast = poly.fit_transform(future_forecast)\n    \n    poly_confirmed = LinearRegression(fit_intercept=True, normalize=True)\n    poly_confirmed.fit(poly_X, y)\n    \n    poly_pred = poly_confirmed.predict(poly_future_forecast)\n    \n    plt.plot(y)\n    plt.plot(poly_pred)\n    plt.title('Coronavirus testing in {}'.format(state))\n    plt.legend(['Actual testing', 'Predicted testing using polynomial regression'])\n    plt.xlabel('Days since 3/1/2020')\n    plt.ylabel('# of testing')\n    plt.show()\n    \n    future_increases = [] \n    \n    # calulate future rates of change \n    for i in range(days_ahead):\n        c = len(X) - 1\n        future_increases.append(daily_change(poly_pred[c+i+1], poly_pred[c+i]))\n    return future_increases","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reshape our arays for sklearn. ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"days_since_3_1 = np.array(days_since_3_1).reshape(-1, 1)\nfuture_dates = np.array(future_dates).reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are taking winodws averages for our mobility data. It will give us a better view of the situation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def window_average(window_size, data, method):\n    avg_data = [] \n    date_length = len(data)\n    \n    for i in range(len(data)):\n        remainder = i % window_size \n        if method == 'median':\n            if i - remainder + window_size - 1 < date_length:\n                avg_data.append(np.median(data[i-remainder:i-remainder+window_size-1]))\n            else:\n                delta = date_length % window_size \n                avg_data.append(np.median(data[date_length-delta-1:date_length-1]))\n        elif method == 'mean':\n             if i - remainder + window_size - 1 < date_length:\n                avg_data.append(np.mean(data[i-remainder:i-remainder+window_size-1]))\n             else:\n                delta = date_length % window_size \n                avg_data.append(np.mean(data[date_length-delta-1:date_length-1]))\n        else:\n            warnings.warn('Methods can only be mean or median')\n            \n    return avg_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Determine if a day is a weekend or weekday. We want to separate weekend mobility from weekday mobility due to statistical differences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# returns true if it is a weekend, and false if it is a weekday \ndef weekday_or_weekend(date):\n    date_obj = datetime.datetime.strptime(str(date), '%Y%m%d')\n    day_of_the_week =  date_obj.weekday()\n    if (day_of_the_week+1) % 6 == 0 or (day_of_the_week+1) % 7 == 0:\n        return True \n    else:\n        return False ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(future_dates[-10:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# implementing this in the prediction in the future \ndef mobility_scenario(mobility, mode):\n    local_min = np.min(mobility)\n    local_max = np.max(mobility)\n    \n    local_min_index = np.where(mobility==local_min)[0]\n    local_max_index = np.where(mobility==local_max)[0]\n    \n    slope = (local_max - local_min) / (local_max_index - local_min_index)\n    \n#     plt.axhline(y=local_min, color='red')\n#     plt.axhline(y=local_max, color='purple')\n    \n    # if mobility increases \n    \n    if mode == 'decrease':\n        # extrapolating mobility \n        m = mobility[-1] + slope \n        if m < local_min:\n            future_mobility = local_min\n        else:\n            future_mobility = m\n    \n    # if mobility decreases \n    if mode == 'increase':\n         # extrapolating mobility \n        m = mobility[-1] + np.abs(slope) \n        if m > local_max:\n            future_mobility = local_max\n        else:\n            future_mobility = m\n            \n    return future_mobility","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the model. ","execution_count":null},{"metadata":{"id":"qWI6oeHlOZEs","outputId":"f18ebdc9-7897-41de-bc8d-2d1c0c4023c3","scrolled":false,"trusted":true},"cell_type":"code","source":"states = ['FL', 'CA', 'GA', 'TX']\n\nfor state in states:\n    positive = []\n    pop_density = [] \n    testing = [] \n    \n    # mobility data\n    walking_weekday = [] \n    walking_weekend = [] \n    walking = []\n    walking_weekday_window = 7\n    walking_weekend_window = 7\n    \n    # adjust window size for mobility\n    \n    date_length = len(revised_unique_dates)\n    \n    # get cases in sequential order for each state\n    for i in range(date_length):\n        positive.append(new_data[new_data.date==unique_dates[i]][new_data.state==state].positive.sum())\n        pop_density.append(new_data[new_data.state==state]['pop_dense'].max())\n        testing.append(new_data[new_data.date==unique_dates[i]][new_data.state==state].totalTestResults.sum())\n        \n        # determines if it is a weekend or weekday \n        if weekday_or_weekend(unique_dates[i]): \n            walking_weekend.append(get_mobility_by_state('walking', state, revised_unique_dates[i]))\n        else:\n            walking_weekday.append(get_mobility_by_state('walking', state, revised_unique_dates[i]))\n        \n#         remainder = i % window_size \n#         if i - remainder + window_size < date_length:\n#             walking.append(get_mobility_by_state('walking', state, revised_unique_dates[i-remainder], revised_unique_dates[i-remainder+window_size-1], 'median'))\n#         else:\n#             # if extrapolating use the mobility average from the last few days based on the window size\n#             delta = date_length % window_size \n#             walking.append(get_mobility_by_state('walking', state, revised_unique_dates[date_length-delta-1], revised_unique_dates[date_length-1], 'median'))\n\n\n    # remove any decreases in cum testing and positive cases\n    for i in range(len(testing)):\n        if i != 0:\n            if testing[i] < testing[i-1]:\n                testing[i] = testing[i-1]\n            if positive[i] < positive[i-1]:\n                positive[i] = positive[i-1]\n    \n    # remove 0 in mobility from both weekday and weekend data (there are few null values from Apple's mobility data)\n    for i in range(len(walking_weekend)):       \n        if walking_weekend[i] == 0 and i != 0:\n            walking_weekend[i] = walking_weekend[i-1]\n            \n    for i in range(len(walking_weekday)):\n        if walking_weekday[i] == 0 and i != 0:\n            walking_weekday[i] = walking_weekday[i-1]\n            \n    \n    # taking window average for mobility \n    walking_weekday_avg = window_average(7, walking_weekday, 'mean')\n    walking_weekend_avg = window_average(7, walking_weekend, 'mean')\n\n    \n    # making sure the shape of the mobility arrays match \n    r_walking_weekday_avg = [] \n    r_walking_weekend_avg = [] \n    \n    k = 0 \n    j = 0 \n    for i in range(date_length):\n        if i % walking_weekday_window == 0 and i != 0:\n            if k + walking_weekday_window < len(walking_weekday_avg):\n                k += walking_weekday_window\n            else:\n                k = len(walking_weekday_avg) - 1 \n                \n            if j + walking_weekend_window < len(walking_weekend_avg):\n                j += walking_weekend_window\n            else:\n                j = len(walking_weekend_avg) - 1\n        \n        r_walking_weekday_avg.append(walking_weekday_avg[k])\n        r_walking_weekend_avg.append(walking_weekend_avg[j])\n        \n\n    # take moving average for positive cases\n    positive = moving_positive_cases(positive, 3)\n\n    # future testing extrapolations from poylnomial prediction \n    future_testing = future_testing_extrapolation(days_since_3_1, testing, future_dates, state)\n    for i in future_testing:\n        testing.append(testing[-1] + i)\n    \n    testing = np.array(testing).reshape(-1, 1)\n    positive = np.array(positive).reshape(-1, 1)\n    r_walking_weekday_avg = np.array(r_walking_weekday_avg).reshape(-1, 1)\n    r_walking_weekend_avg = np.array(r_walking_weekend_avg).reshape(-1, 1)\n    \n    min_max_scaler = MinMaxScaler()\n    testing = min_max_scaler.fit_transform(testing)\n    r_walking_weekday_avg = min_max_scaler.fit_transform(r_walking_weekday_avg)\n    r_walking_weekend_avg = min_max_scaler.fit_transform(r_walking_weekend_avg)\n    \n    # combining the two features\n    X = [] \n    future_forecast = []\n    \n    for i in range(len(days_since_3_1)):\n        X.append([days_since_3_1[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[i][0], r_walking_weekend_avg[i][0]])\n    \n    X = np.array(X, object).reshape(-1, 5)\n    \n    for i in range(len(future_dates)):\n        if i < date_length:\n            future_forecast.append([future_dates[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[i][0], r_walking_weekend_avg[i][0]])\n        else:\n            future_forecast.append([future_dates[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[-1][0], r_walking_weekend_avg[-1][0]])\n            \n    future_forecast = np.array(future_forecast, object).reshape(-1, 5)\n    \n    # splitting into training and testing sets \n    X_train, X_test, y_train, y_test = train_test_split(X, positive, shuffle=False, test_size=0.05)\n    bayesian_ridge(X_train, X_test, y_train, y_test, future_forecast, state)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}