{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PLANET: Understanding the Amazon from Space"},{"metadata":{},"cell_type":"markdown","source":"**IMPORT LIBRARIES**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pathlib\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport cv2\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import SGD\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import fbeta_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DOWNLOAD DATASETS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set path\npath = '../input/planets-dataset/'\nos.chdir(path)   \n\nfor dirname, _, filenames in os.walk('planet'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load train and test datasets\ntrain_data = pd.read_csv(\"planet/planet/train_classes.csv\")\ntest_data = pd.read_csv(\"planet/planet/sample_submission.csv\")\n\n# preview train dataset\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**INSPECT IMAGE LABELS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build list with unique labels\nlabel_list = []\nfor tag_str in train_data.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)\n\n            \n# Display label list and length \nprint(f'There are {len(train_data)} data samples, with {len(label_list)} possible classes.', '\\n' \n      f'The Label list includes {label_list}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the tags column to get the unique labels\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in train_data['tags'].values])))\n\n\n# Create label map\nlabel_map = {l: i for i, l in enumerate(labels)}\n\nprint(f'label_map = {label_map},\\n length = {len(label_map)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add onehot features for every label\ntrain_tag_data = train_data.copy()\nfor label in label_list:\n    train_tag_data[label] = train_tag_data['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n\n# Display head\ntrain_tag_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print all unique tags\nfrom itertools import chain\nlabels_list = list(chain.from_iterable([tags.split(\" \") for tags in train_tag_data['tags'].values]))\nlabels_set = set(labels_list)\nprint(\"There is {} unique labels including {}\".format(len(labels_set), labels_set))\n\n\n# Histogram of label instances\ntag_labels = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=tag_labels, y=tag_labels.index, orient='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**INSPECT COOCCURENCE MATRICES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for cooocurence matrix plotting\ndef make_cooccurence_matrix(labels):\n    numeric_data = train_tag_data[labels]; \n    c_matrix = numeric_data.T.dot(numeric_data)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot weather element cooccurence matrix\nweather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\nmake_cooccurence_matrix(weather_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot land-use element classes cooccurence matrix\nland_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nmake_cooccurence_matrix(land_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PREVIEW IMAGES IN EACH CATEGORY**"},{"metadata":{"trusted":true},"cell_type":"code","source":"images = [train_data[train_data['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg' \n                for i, label in enumerate(labels_set)]\n\nplt.rc('axes', grid=False)\n_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\nfor i, (image_name, label) in enumerate(zip(images, labels_set)):\n    img = mpimg.imread('planet/planet/train-jpg' + '/' + image_name)\n    axs[i].imshow(img)\n    axs[i].set_title('{} - {}'.format(image_name, label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA PRE-PROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"Data Length Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determining if the length of the train and test dataset csv file equals the actual number of images in the folder\n\n# Assign train and the two test dataset paths\n# train path\ntrain_img_dir = pathlib.Path('planet/planet/train-jpg')\ntrain_img_path = sorted(list(train_img_dir.glob('*.jpg')))\n\n# test path\ntest_img_dir = pathlib.Path('planet/planet/test-jpg')\ntest_img_path = sorted(list(test_img_dir.glob('*.jpg')))\n\n# additional test path\ntest_add_img_dir = pathlib.Path('test-jpg-additional')\ntest_add_img_path = sorted(list(test_add_img_dir.glob('*/*.jpg')))\n\n# Length Confirmation\nassert len(train_img_path) == len(train_data)\nprint(len(test_img_path)+len(test_add_img_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Image Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input size\ninput_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating x_train and y_train\nx_train = []\ny_train = []\n\nfor f, tags in tqdm(train_data.values, miniters=1000):\n    img = cv2.imread('planet/planet/train-jpg/{}.jpg'.format(f))\n    img = cv2.resize(img, (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_train.append(img)\n    y_train.append(targets)\n        \nx_train = np.array(x_train, np.float32)\ny_train = np.array(y_train, np.uint8)\n\nprint(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating x_test\nx_test = []\n\ntest_jpg_dir = 'planet/planet/test-jpg'\ntest_image_names = os.listdir(test_jpg_dir)\n\nn_test = len(test_image_names)\ntest_classes = test_data.iloc[:n_test, :]\nadd_classes = test_data.iloc[n_test:, :]\n\ntest_jpg_add_dir = 'test-jpg-additional/test-jpg-additional'\ntest_add_image_names = os.listdir(test_jpg_add_dir)\n\nfor img_name, _ in tqdm(test_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_dir + '/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n    \nfor img_name, _ in tqdm(add_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_add_dir + '/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n\nx_test = np.array(x_test, np.float32)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the train data into train and validation data sets\nX_train = x_train[ :33000]\nY_train = y_train[ :33000]\n\nX_valid = x_train[33000: ]\nY_valid = y_train[33000: ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL TRAINING**"},{"metadata":{},"cell_type":"markdown","source":"Define Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify sizes (batch and model input) and number of input channels\ninput_size = 64\ninput_channels = 3\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add model parameters including dropout, layers and activation function\nbase_model = VGG16(include_top=False,\n                   weights='imagenet',\n                   input_shape=(input_size, input_size, input_channels))\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(17, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npath = '/kaggle/working'\nos.chdir(path)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# define model training optimizer parameters\noptimizer  = SGD(lr=0.01)\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n                ModelCheckpoint(filepath='weights/best_weights',\n                                 save_best_only=True,\n                                 save_weights_only=True)]\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nhistory = model.fit(x=X_train, y=Y_train, validation_data=(X_valid, Y_valid),\n                  batch_size=batch_size,verbose=2, epochs=15,callbacks=callbacks,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_valid = model.predict(X_valid, batch_size = batch_size, verbose=1)\n\nprint(fbeta_score(Y_valid, np.array(p_valid) > 0.18, beta=2, average='samples'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = []\np_test = model.predict(x_test, batch_size=batch_size, verbose=2)\ny_pred.append(p_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels1 = ['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn', 'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming', 'selective_logging', 'blow_down']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = np.array(y_pred[0])\nfor i in range(1, len(y_pred)):\n    result += np.array(y_pred[i])\nresult = pd.DataFrame(result, columns=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translating the probability predictions to the unique labels\npreds = []\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.loc[[i]]\n    a = a.apply(lambda x: x>0.2, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the tags columns with the predicted labels\ntest_data['tags'] = preds\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataframe to a csv file for submission\ntest_data.to_csv('amazon_sample_submission10.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# converting output to label tags for submission.csv file\n\nmySubmission = sample_sub.copy()\nsorted_labels = list(label_map1.keys())\nfor i in tqdm(range(result.shape[0])):\n    tag = \"\"\n    x = result[i]\n    for lbl in sorted_labels:\n        if x[sorted_labels.index(lbl)] == 1:\n            tag += \" \" + lbl\n            mySubmission[\"tags\"][i] = tag[1:]\n\nmySubmission.to_ccsv(\"submission.csv\", index = False)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}