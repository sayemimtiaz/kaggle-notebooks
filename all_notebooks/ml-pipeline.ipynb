{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Great help taken from below resources:**\n        * Reference:\n        1. https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n        2. https://www.datacamp.com/community/tutorials/machine-learning-python\n        3. https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python\n        4. https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features\n        5. https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html\n        6. https://github.com/mdkearns/automated-data-preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## **Pipeline Ruleset:**\n1. **Exploring and Preparing Data**\n        1.1 **Loading files in pandas dataframe**\n        1.2 **Understand the data**\n        1.3 **Explore the data**\n            1.3.1 **Examine shape of the dataframe i.e .shape**\n            1.3.2 **Examine exploratory information of the dataframe i.e .info()**\n            1.3.3 **Examine statistical data within the dataframe i.e .describe()**\n            1.3.4 **Examine columnwise datatypes in the dataframe i.e .dtypes**\n            1.3.5 **Examine number of unique values in a column in the dataframe i.e value_counts**\n            1.3.6 **Examine and visualize distribution of the dataframe based on traget column(s)**\n        1.4 **Finding and visualizing outliers using cross tables and pivot table**\n2. **Find anomalies within the dataframe checking every column**\n        2.1 **Check for anomalous column and data importance**\n        2.2 **Replace anomalous data i.e np.nan, imputation**\n3. **Examine missing values with columnwise statistics in the dataframe** \n4. **Encode categorical variables in the dataframe i.e label encoding, one-hot encoding**\n5. **Find correlations i.e column -> target_column, column -> column, dataframe**\n6. **Align training and testing dataframe** \n "},{"metadata":{},"cell_type":"markdown","source":"## **Automated Data Preprocessing**\n    A command-line utility program for automating the trivial, frequently occurring data preparation tasks: missing value interpolation, outlier removal, and encoding categorical variables.\n\n* Identify missing values in the data set and replace them with the sentinel NaN value.\n* Interpolate missing values using mean for continuous features, mode for discrete features.\n* Remove outliers on the assumption that the distribution of the field values follow a normal distribution.\n* Encode categorical features using a one-hot encoding schema."},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Project Template\n\n1. Prepare Problem\n    * Load libraries [Done]\n    * Load dataset [Done]\n\n2. Summarize Data\n    * Descriptive statistics (summary, varType, corr Matrix, Count of class labels) [Done]\n    * Visualizations (histogram, density, whisker plot, scatter & correlation matrix)\n\n3. Prepare Data\n    * Data Cleaning\n    * Feature selection\n    * Testing the assumptions\n    * Data transforms\n    \n4. Evaluate algorithms\n    * Split out validation dataset\n    * Test options and evaluation matrix\n    * Spot check algorithms\n    * Compare algorithms\n    \n5. Improve accuracy\n    * Algorithm tuning\n    * Ensembles\n\n6. Finalize model\n    * Predictions on validation dataset\n    * Create standalone model on entire training dataset\n    * save model for later use"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## DONE: Imports for any data science project\nimport operator as opt\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\nfrom contextlib import contextmanager\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## DONE: Create files dictionary for any file in the input directory\n\nfiles = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        files[filename] = os.path.join(dirname, filename)\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Loading files in pandas dataframe [1.1]\n\ndef load_file_into_dataframe(file_name):\n    df = pd.read_csv(files[file_name])\n    return df\n\n#     print(f\"Dataframe shape:\", df.shape)\n#     print(f\"\\nDataframe data type(s):\\n\",df.dtypes)\n#     print(f\"\\nDataframe first five rows:\\n\",df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Check on Pipeline [1.1]  \n\ntrain_df = load_file_into_dataframe('Train.csv')\ntest_df = load_file_into_dataframe('Test.csv')\nsubmission_df = load_file_into_dataframe('Submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(train_df.count(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Examine shape of the dataframe [1.3.1], Examine exploratory information of the dataframe [1.3.2]\n# Examine statistical data within the dataframe [1.3.3], Examine columnwise datatypes in the dataframe [1.3.4]\n# Examine number of unique values in a column in the dataframe [1.3.5]\ndef dataframe_shape(df): \n    \"\"\" Examine shape of the dataframe [1.3.1] \"\"\"\n    return df.shape\n\ndef dataframe_split_between_null_and_not_null(df):\n    not_null_df = df.dropna()\n    null_df = df.drop(not_null_df.index)\n    print(\"Actual dataset \", df.shape)\n    print(\"Null dataset \", null_df.shape)\n    print(\"Not null dataset \", not_null_df.shape)\n    \ndef dataframe_info(df):\n    \"\"\" Examine exploratory information of the dataframe [1.3.2] \"\"\"\n    return df.info()\n\ndef dataframe_description(df):\n    \"\"\" Examine statistical data within the dataframe [1.3.3] \"\"\"\n    return df.describe().T\n\ndef dataframe_columnwise_individuals_data_types(df):\n    \"\"\" Examine columnwise datatypes in the dataframe [1.3.4] \"\"\"\n    return df.dtypes\n\ndef check_datatypes_of_col_number(df):\n    \"\"\" Unique data types column number \"\"\"\n    return df.dtypes.value_counts()\n\ndef unique_value_in_specified_dtype_cols(df, dtype='object'):\n    \"\"\" Shows number of unique values of specified data type in several columns \"\"\"\n    return df.select_dtypes(dtype).apply(pd.Series.nunique, axis = 0)\n\ndef number_of_unique_values_in_col(df, col_name='def_col_name'):\n    \"\"\" Examine number of unique values in a column in the dataframe [1.3.5] \"\"\"\n    return df[col_name].value_counts()\n\ndef correlation_matrix_df(df):\n    \"\"\" Return data frame correlation matrix \"\"\"\n    return df.corr()  #.sort_values(ascending=False)\n\ndef check_null_columnwise(df):\n    \"\"\" Return column wise none value \"\"\"\n    new_df = df.isnull().sum()\n    new_df = new_df[new_df > 0]\n    return new_df\n\n## TODO: **Examine and visualize distribution of the dataframe based on traget column(s)**\n\n# def visualize_col_distribution(df, col_name):\n#     return df[col_name].astype(int).plot.hist();\n\n## LOOKUP: Concentrate here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" -\" * 30)\nprint(dataframe_info(train_df)) # info\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint(dataframe_description(train_df)) # describe\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint(dataframe_columnwise_individuals_data_types(train_df)) # columnwise data types\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint(check_datatypes_of_col_number(train_df)) # which datatype has how many columns\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint(correlation_matrix_df(train_df)) # correlation matrix\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint( unique_value_in_specified_dtype_cols(train_df, dtype='object')) # count of class labels of object datatype\nprint(\" -\" * 30)\nprint(\" -\" * 30)\nprint(check_null_columnwise(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Examine distribution of the dataframe based on traget column [2]\n\n# def check_distribution(df, col_name='default_col_name'):\n#     dist_ = df[col_name].value_counts()\n#     return dist_\n\n# def viz_distribution(df, col_name='default_col_name'):\n#     return df[col_name].plot.hist();\n\n# print(check_distribution(train_df, 'Outlet_Identifier'))\n# viz_distribution(train_df, 'Outlet_Identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO: Finding and visualizing outliers using cross tables and pivot table [1.4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO: Find anomalies within the dataframe checking every column [2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Examine missing values with columnwise statistics in the dataframe [3]\n\ndef missing_values_stat_in_columns(df):\n    \"\"\" Showing a dataframe columnwise missing values in number and percentage of total values \"\"\"\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n    return mis_val_table_ren_columns\n\n# def missing_values_dataframe_in_series(df):\n#     \"\"\" Missing values count in the dataframe as series \"\"\"\n#     r_df = df.isnull().sum().sort_values(ascending=False)\n#     df = r_df[r_df.iloc[:] != 0]\n#     return df\n\ndef drop_columns_from_dataframe(df, col_list= ['colA','colB']):\n    \"\"\" Drops a list of columns with provided column names \"\"\"\n    return df.drop(col_list, axis=1, inplace=True)\n\ndef generate_condition(df,col_name='colA', value= 'valA', com_op = opt.lt):\n    \"\"\"Return row indices based on column name, column value and condition operator i.e ['eq', 'ne', 'ge', 'le', 'gt', 'lt']\"\"\"\n    dtype_ = df[col_name].dtype\n    value_ = pd.Series([value], dtype=dtype_)\n    indices = df[com_op(df[col_name], value_[0])].index\n    return indices\n\ndef drop_rows_based_on_indices(df, indices = None):\n    \"\"\" Return dataframe and drop rows based on given row indices \"\"\"\n    return df.drop(indices, inplace = True)\n\ndef missing_value_manipulation(df, missing_percentage = 30.0):\n    \"\"\" Return dataframe and drop columns based on their missing data percentage \"\"\"\n    missing_percentage_df = missing_values_stat_in_columns(df)\n    cols_to_drop = set()\n    \n    for index, row in missing_percentage_df.iterrows():\n        if row['% of Total Values'] >= float(missing_percentage):\n            cols_to_drop.add(index)\n    print(cols_to_drop)\n    return cols_to_drop # list of row index to drop\n\ndef check_correlation_against_target_column(df, col_name, target_col_name):\n    \"\"\" Return correlation value of a column against target column \"\"\"\n    return df[col_name].corr(df[target_col_name])\n\ndef correaltion_dataframe(df, target_col, positive= True):\n    \"\"\" Return correlation dataframe based on positive and negative \"\"\"\n    correlations =  df.corr()[target_col].sort_values(ascending=False)\n#     return correlations\n    if positive == True:\n        return correlations[correlations > 0]\n    else:\n        return correlations[correlations < 0]\n    \n## 1. Columnwise missing value check [done]\n## 2. Missing value percentage check [done]\n## 3. Addition or deletion of column based on missing value threshold i.e percentage [done]\n## 4. Missing value column significance based on correlation() [done]\n## 5. numerical -> (MOD -> checking on highest frequency | if std() is very small in that sense we can impute mean()) | median(), categorical -> (maximum frequency)\n\n## TODO: ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correaltion_dataframe(train_df, 'Item_Outlet_Sales', positive= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df.head()\n# generate_condition(df,col_name='colA', value= 'valA', com_op = opt.lt)\ngenerate_condition(train_df,'Outlet_Establishment_Year','1999')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing_values_stat_in_columns(train_df)\nmissing_value_manipulation(train_df, 10.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Encode categorical variables in the dataframe i.e label encoding, one-hot encoding\n\ndef label_encoding(df_train, df_test): ## TODO: add df_test dataframe\n    le = LabelEncoder()\n    le_count = 0\n    \n    for col in df_train:\n        if df_train[col].dtype == 'object' and len(list(df_train[col].unique())) <= 2:\n            le.fit(df_train[col])\n            df_train[col] = le.transform(df_train[col])\n            df_test[col] = le.transform(df_test[col]) ## TODO: while test dataframe will be provided\n            le_count += 1\n    print('%d columns were label encoded.' % le_count)\n        \ndef one_hot_encoding(df_train, df_test): ## TODO: add df_test dataframe\n    df_train = pd.get_dummies(df_train)\n    df_test = pd.get_dummies(df_test) ## TODO: while test dataframe will be provided\n\n    print('Training Features shape: ', df_train.shape)\n    print('Testing Features shape: ', df_test.shape) ## TODO: while test dataframe will be provided\n    \n    return df_train, df_test\n\n# label_encoding(df_train, df_test)\n# df_train, df_test = one_hot_encoding(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DONE: Align dataframe based on primary and foreign key of two dataframe\n\ndef align_dataframe(df_train, df_test, target_col):\n    target_labels = df_train[target_col]\n    df_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n    df_train[target_col] = target_labels\n    \n    print('Training Features shape: ', df_train.shape)\n    print('Testing Features shape: ', df_test.shape)\n    \n# align_dataframe(df_train, df_test, 'TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}