{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true},"cell_type":"markdown","source":"# Aim\nI have been fascinated by the topic of **time series analysis.** This kernel is prepared to be a container of many broad topics in the field of time series analysis. My motive is to make this the ultimate reference to time series analysis for beginners and experienced people alike.\n\n# Some important things\n1. This kernel **is a work in progress so every time you see on your home feed and open it, you will surely find fresh content.**\n2. I am doing this only after completing various courses in this field. I continue to study more advanced concepts to provide more knowledge and content.\n3. If there is any suggestion or any specific topic you would like me to cover, kindly mention that in the comments.\n4. **If you like my work, be sure to upvote**(press the like button) this kernel so it looks more relevant and meaningful to the community."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n# Above is a special style template for matplotlib, highly useful for visualizing time series data\n%matplotlib inline\nfrom pylab import rcParams\n#from plotly import tools\n#import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport statsmodels.api as sm\nfrom numpy.random import normal, seed\nfrom scipy.stats import norm\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\nfrom sklearn.metrics import mean_squared_error\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d798543e3b1a8df4989f8361f241b8f3ec00b0d"},"cell_type":"markdown","source":"- <a href='#1'>1. Introduction to date and time</a>\n    - <a href='#1.1'>1.1 Importing time series data</a>\n    - <a href='#1.2'>1.2 Cleaning and preparing time series data</a>\n    - <a href='#1.3'>1.3 Visualizing the datasets</a>\n    - <a href='#1.4'>1.4 Timestamps and Periods</a>\n    - <a href='#1.5'>1.5 Using date_range</a>\n    - <a href='#1.6'>1.6 Using to_datetime</a>\n    - <a href='#1.7'>1.7 Shifting and lags</a>\n    - <a href='#1.8'>1.8 Resampling</a>\n- <a href='#2'>2. Finance and Statistics</a>\n    - <a href='#2.1'>2.1 Percent change</a>\n    - <a href='#2.2'>2.2 Stock returns</a>\n    - <a href='#2.3'>2.3 Absolute change in successive rows</a>\n    - <a href='#2.4'>2.4 Comaring two or more time series</a>\n    - <a href='#2.5'>2.5 Window functions</a>\n    - <a href='#2.6'>2.6 OHLC charts</a>\n    - <a href='#2.7'>2.7 Candlestick charts</a>\n    - <a href='#2.8'>2.8 Autocorrelation and Partial Autocorrelation</a>\n- <a href='#3'>3. Time series decomposition and Random Walks</a>\n    - <a href='#3.1'>3.1 Trends, Seasonality and Noise</a>\n    - <a href='#3.2'>3.2 White Noise</a>\n    - <a href='#3.3'>3.3 Random Walk</a>\n    - <a href='#3.4'>3.4 Stationarity</a>\n- <a href='#4'>4. Modelling using statsmodels</a>\n    - <a href='#4.1'>4.1 AR models</a>\n    - <a href='#4.2'>4.2 MA models</a>\n    - <a href='#4.3'>4.3 ARMA models</a>\n    - <a href='#4.4'>4.4 ARIMA models</a>\n    - <a href='#4.5'>4.5 VAR models</a>\n    - <a href='#4.6'>4.6 State space methods</a>\n        - <a href='#4.6.1'>4.6.1 SARIMA models</a>\n        - <a href='#4.6.2'>4.6.2 Unobserved components</a>\n        - <a href='#4.6.3'>4.6.3 Dynamic Factor models</a>"},{"metadata":{"_uuid":"c0dc336a9d4969910d85fbb9462f2dde0bb01dc3"},"cell_type":"markdown","source":"# <a id='1'>1. Introduction to date and time</a>"},{"metadata":{"_uuid":"d8fd2907fd2d4ef86da73d65ba3d228dd56326a8"},"cell_type":"markdown","source":"## <a id='1.1'>1.1 Importing time series data</a>"},{"metadata":{"_uuid":"0a98331f0441b4365873a02f0c2208321ac959b0"},"cell_type":"markdown","source":"### How to import data?\nFirst, we import all the datasets needed for this kernel. The required time series column is imported as a datetime column using **parse_dates** parameter and is also selected as index of the dataframe using **index_col** parameter. \n#### Data being used:-\n1. Stocks Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ded63a0ec9e973d605a44fbea7ec50bb8c85a90","trusted":true},"cell_type":"code","source":"FFC = pd.read_csv('../input/ksedataset/FFC.csv', index_col='Date', parse_dates=['Date'])\nFFC.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc01753c087bdb1968478ea172d3f49ded7d130"},"cell_type":"markdown","source":"## <a id='1.2'>1.2 Cleaning and preparing time series data</a>"},{"metadata":{"_uuid":"f672f5e3ef334a129cd2d730ee1bc827f92b2e62"},"cell_type":"markdown","source":"### How to prepare data?\nStocks data does have any missing values its fair share of missing values. It is cleaned using **fillna()** method with **ffill** parameter which propagates last valid observation to fill gaps"},{"metadata":{"trusted":true},"cell_type":"code","source":"FFC = FFC.iloc[1:]\nFFC = FFC.fillna(method='ffill')\nFFC.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed2ec5326c1771450a60efbdfc246b9f20495406"},"cell_type":"markdown","source":"## <a id='1.3'>1.3 Visualizing the datasets</a>"},{"metadata":{"_uuid":"13599f97548a5b33fa28b016bff5152c17d49296","trusted":true},"cell_type":"code","source":"FFC['2003':'2019'].plot(subplots=True, figsize=(10,12))\nplt.title('FFC stock attributes from 2003 to 2019')\nplt.savefig('FFC stocks.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd73a8237c04b14b3afcf1d9dc3f244753b35a8b"},"cell_type":"markdown","source":"## <a id='1.4'>1.4 Timestamps and Periods</a>"},{"metadata":{"_uuid":"4c2ada0d73d7893f0899a8cfffcdfae75ee7b57b"},"cell_type":"markdown","source":"### What are timestamps and periods and how are they useful?\nTimestamps are used to represent a point in time. Periods represent an interval in time. Periods can used to check if a specific event in the given period. They can also be converted to each other's form."},{"metadata":{"_uuid":"f87c7b3f17071c0ce6ed05ce91fe085642e5377d"},"cell_type":"markdown","source":"## <a id='1.5'>1.5 Using date_range</a>"},{"metadata":{"_uuid":"47cf8637f8c60e0bdbeb457f2af05aeeaaf763f9"},"cell_type":"markdown","source":"### What is date_range and how is it useful?\n**date_range** is a method that returns  a fixed frequency datetimeindex. It is quite useful when creating your own time series attribute for pre-existing data or arranging the whole data around the time series attribute created by you."},{"metadata":{"_uuid":"c26aa5fc419a077276c3b24f52d1721245de94fe"},"cell_type":"markdown","source":"## <a id='1.6'>1.6 Using to_datetime</a> "},{"metadata":{"_uuid":"821003a25d35c41cc9a33134549a1c46d6ca4b99"},"cell_type":"markdown","source":"pandas.to_datetime() is used for converting arguments to datetime. Here, a DataFrame is converted to a datetime series."},{"metadata":{"_uuid":"9acb890fd5a021bc1a10ee19184425031d891f84","collapsed":true},"cell_type":"markdown","source":"## <a id='1.7'>1.7 Shifting and lags</a>"},{"metadata":{"_uuid":"16a55de3afba1b92503ff5484e45b39087e77f38"},"cell_type":"markdown","source":"We can shift index by desired number of periods with an optional time frequency. This is useful when comparing the time series with a past of itself"},{"metadata":{"_uuid":"72d0ea11c176f3731727b81269148188141c7dcc"},"cell_type":"markdown","source":"# <a id='2'>2. Finance and statistics</a>"},{"metadata":{"_uuid":"e99a77337a15dfbcc5ec97358c0d31d0ca04ab0d"},"cell_type":"markdown","source":"## <a id='2.1'>2.1 Percent change</a>"},{"metadata":{"_uuid":"ee26cbd48ae1025b9a072a4385609d6abc0406d9","trusted":true},"cell_type":"code","source":"FFC['Change'] = FFC.High.div(FFC.High.shift())\nFFC['Change'].plot(figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eed7547b522e9630ea2dc833afdee51e61ca41e6"},"cell_type":"markdown","source":"## <a id='2.2'>2.2 Stock returns</a> "},{"metadata":{"_uuid":"09d5af0ce40cfdd1338433c204538fe894004b3f","trusted":true},"cell_type":"code","source":"FFC['Return'] = FFC.Change.sub(1).mul(100)\nFFC['Return'].plot(figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5f7a089b1ae64b4403c588f8095332bc32c0d11","trusted":true},"cell_type":"code","source":"FFC.High.pct_change().mul(100).plot(figsize=(20,6)) # Another way to calculate returns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1daf826b9b25c7f3c69683d9b59a7d2aa285df06"},"cell_type":"markdown","source":"## <a id='2.3'>2.3 Absolute change in successive rows</a>"},{"metadata":{"_uuid":"399bd0a9f469d69dd11523e54f1599302291be44","trusted":true},"cell_type":"code","source":"FFC.High.diff().plot(figsize=(20,6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5ec63ecd09543b21df15aff6432db598ee1ab5","collapsed":true},"cell_type":"markdown","source":"## <a id='2.4'>2.4 Comaring two or more time series</a> "},{"metadata":{"_uuid":"7a7a4a918d1d80d19d33f71965a3ca5583a8aa2e"},"cell_type":"markdown","source":"We will compare 2 time series by normalizing them. This is achieved by dividing each time series element of all time series by the first element. This way both series start at the same point and can be easily compared."},{"metadata":{"_uuid":"91ff99a81a53eecbda4e8c81b231db3dc8f62082","trusted":true},"cell_type":"code","source":"# We choose ENGRO stocks to compare them with FFC\nENGRO = pd.read_csv('../input/ksedataset/ENGRO.csv', index_col='Date', parse_dates=['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ENGRO = ENGRO.iloc[1:]\n#ENGRO = ENGRO.fillna(method='ffill')\n#ENGRO.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751ce8e4ff9bb26f106342c26d9f4893c21a7bfa","trusted":true},"cell_type":"code","source":"# Plotting before normalization\nFFC.High.plot()\nENGRO.High.plot()\nplt.legend(['FFC','ENGRO'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9d610487415c9c65726c187909abbd8ce0f57b0","trusted":true},"cell_type":"code","source":"# Normalizing and comparison\n# Both stocks start from 100\nnormalized_FFC = FFC.High.div(FFC.High.iloc[0]).mul(100)\nnormalized_ENGRO = ENGRO.High.div(ENGRO.High.iloc[0]).mul(100)\nnormalized_FFC.plot()\nnormalized_ENGRO.plot()\nplt.legend(['FFC','ENGRO'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a89d0a945a7cff7d2dbd074d437be83607c17e3"},"cell_type":"markdown","source":"You can clearly see how google outperforms microsoft over time."},{"metadata":{"_uuid":"7de5aa0e16da9790177a2994e843b4bf69c82688","collapsed":true},"cell_type":"markdown","source":"## <a id='2.5'>2.5 Window functions</a>\nWindow functions are used to identify sub periods, calculates sub-metrics of sub-periods.\n\n**Rolling** - Same size and sliding\n\n**Expanding** - Contains all prior values"},{"metadata":{"_uuid":"2759a3894d231e62d1a868d61591a78796b428ac","trusted":true},"cell_type":"code","source":"# Rolling window functions\nrolling_FFC = FFC.High.rolling('90D').mean()\nFFC.High.plot()\nrolling_FFC.plot()\nplt.legend(['High','Rolling Mean'])\n# Plotting a rolling mean of 90 day window with original High attribute of google stocks\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6de30c1c19b15b698c04f6328430d25e0c29e016"},"cell_type":"markdown","source":"Now, observe that rolling mean plot is a smoother version of the original plot."},{"metadata":{"_uuid":"0c00473ae42582d73987233a761e0a22a9c2b9af","trusted":true},"cell_type":"code","source":"# Expanding window functions\nENGRO_mean = ENGRO.High.expanding().mean()\nENGRO_std = ENGRO.High.expanding().std()\nENGRO.High.plot()\nENGRO_mean.plot()\nENGRO_std.plot()\nplt.legend(['High','Expanding Mean','Expanding Standard Deviation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"468318f02e689885dd64c8c4cc30984fb91d8f3b"},"cell_type":"markdown","source":"## <a id='2.6'>2.6 OHLC charts</a>\nAn OHLC chart is any type of price chart that shows the open, high, low and close price of a certain time period. Open-high-low-close Charts (or OHLC Charts) are used as a trading tool to visualise and analyse the price changes over time for securities, currencies, stocks, bonds, commodities, etc. OHLC Charts are useful for interpreting the day-to-day sentiment of the market and forecasting any future price changes through the patterns produced.\n\nThe y-axis on an OHLC Chart is used for the price scale, while the x-axis is the timescale. On each single time period, an OHLC Charts plots a symbol that represents two ranges: the highest and lowest prices traded, and also the opening and closing price on that single time period (for example in a day). On the range symbol, the high and low price ranges are represented by the length of the main vertical line. The open and close prices are represented by the vertical positioning of tick-marks that appear on the left (representing the open price) and on right (representing the close price) sides of the high-low vertical line.\n\nColour can be assigned to each OHLC Chart symbol, to distinguish whether the market is \"bullish\" (the closing price is higher then it opened) or \"bearish\" (the closing price is lower then it opened).\n\n<img src=\"https://datavizcatalogue.com/methods/images/anatomy/SVG/OHLC_chart.svg\">\n\nSource: [Datavizcatalogue](https://datavizcatalogue.com/methods/OHLC_chart.html)"},{"metadata":{"_uuid":"cda7c35e8689a4638f0cfda0cc280bc670733993","trusted":true},"cell_type":"code","source":"# OHLC chart of June 2003\ntrace = go.Ohlc(x=FFC['06-2003'].index,\n                open=FFC['06-2003'].Open,\n                high=FFC['06-2003'].High,\n                low=FFC['06-2003'].Low,\n                close=FFC['06-2003'].Close)\ndata = [trace]\niplot(data, filename='simple_ohlc')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"569adf188c009c34651a9caf832f0834dea20088","trusted":true},"cell_type":"code","source":"# OHLC chart of 2003\ntrace = go.Ohlc(x=FFC['2003'].index,\n                open=FFC['2003'].Open,\n                high=FFC['2003'].High,\n                low=FFC['2003'].Low,\n                close=FFC['2003'].Close)\ndata = [trace]\niplot(data, filename='simple_ohlc')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a3394814b5dab1c9f1104b0058366cc39979db2","trusted":true},"cell_type":"code","source":"# OHLC chart of 2003\ntrace = go.Ohlc(x=FFC.index,\n                open=FFC.Open,\n                high=FFC.High,\n                low=FFC.Low,\n                close=FFC.Close)\ndata = [trace]\niplot(data, filename='simple_ohlc')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93363b9969f9ec48dd95b0a33201ea00d8f6bd0b"},"cell_type":"markdown","source":" ## <a id='2.7'>2.7 Candlestick charts</a>\nThis type of chart is used as a trading tool to visualise and analyse the price movements over time for securities, derivatives, currencies, stocks, bonds, commodities, etc. Although the symbols used in Candlestick Charts resemble a Box Plot, they function differently and therefore, are not to be confused with one another.\n\nCandlestick Charts display multiple bits of price information such as the open price, close price, highest price and lowest price through the use of candlestick-like symbols. Each symbol represents the compressed trading activity for a single time period (a minute, hour, day, month, etc). Each Candlestick symbol is plotted along a time scale on the x-axis, to show the trading activity over time.\n\nThe main rectangle in the symbol is known as the real body, which is used to display the range between the open and close price of that time period. While the lines extending from the bottom and top of the real body is known as the lower and upper shadows (or wick). Each shadow represents the highest or lowest price traded during the time period represented. When the market is Bullish (the closing price is higher than it opened), then the body is coloured typically white or green. But when the market is Bearish (the closing price is lower than it opened), then the body is usually coloured either black or red.\n\n<img src=\"https://datavizcatalogue.com/methods/images/anatomy/SVG/candlestick_chart.svg\">\n\nCandlestick Charts are great for detecting and predicting market trends over time and are useful for interpreting the day-to-day sentiment of the market, through each candlestick symbol's colouring and shape. For example, the longer the body is, the more intense the selling or buying pressure is. While, a very short body, would indicate that there is very little price movement in that time period and represents consolidation.\n\nCandlestick Charts help reveal the market psychology (the fear and greed experienced by sellers and buyers) through the various indicators, such as shape and colour, but also by the many identifiable patterns that can be found in Candlestick Charts. In total, there are 42 recognised patterns that are divided into simple and complex patterns. These patterns found in Candlestick Charts are useful for displaying price relationships and can be used for predicting the possible future movement of the market. You can find a list and description of each pattern here.\n\nPlease bear in mind, that Candlestick Charts don't express the events taking place between the open and close price - only the relationship between the two prices. So you can't tell how volatile trading was within that single time period.\n\nSource: [Datavizcatalogue](https://datavizcatalogue.com/methods/candlestick_chart.html)"},{"metadata":{"_uuid":"ab77825a5b940f2db8795af0871c769bed6a3048","trusted":true},"cell_type":"code","source":"# Candlestick chart of march 2003\ntrace = go.Candlestick(x=FFC['03-2003'].index,\n                open=FFC['03-2003'].Open,\n                high=FFC['03-2003'].High,\n                low=FFC['03-2003'].Low,\n                close=FFC['03-2003'].Close)\ndata = [trace]\niplot(data, filename='simple_candlestick')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d022e0d965c6a65e5d79807e22f81ad38e42ecce","trusted":true},"cell_type":"code","source":"# Candlestick chart of 2003\ntrace = go.Candlestick(x=FFC['2003'].index,\n                open=FFC['2003'].Open,\n                high=FFC['2003'].High,\n                low=FFC['2003'].Low,\n                close=FFC['2003'].Close)\ndata = [trace]\niplot(data, filename='simple_candlestick')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7d995be388df5b93f33818953deb429568341e7","trusted":true},"cell_type":"code","source":"# Candlestick chart of 2003-2019\ntrace = go.Candlestick(x=FFC.index,\n                open=FFC.Open,\n                high=FFC.High,\n                low=FFC.Low,\n                close=FFC.Close)\ndata = [trace]\niplot(data, filename='simple_candlestick')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"554c6559d926c57de9e75da9c908aed9843bb529"},"cell_type":"markdown","source":"## <a id='2.8'>2.8 Autocorrelation and Partial Autocorrelation</a>\n* Autocorrelation - The autocorrelation function (ACF) measures how a series is correlated with itself at different lags.\n* Partial Autocorrelation - The partial autocorrelation function can be interpreted as a regression of the series against its past lags.  The terms can be interpreted the same way as a standard  linear regression, that is the contribution of a change in that particular lag while holding others constant. \n\nSource: [Quora](https://www.quora.com/What-is-the-difference-among-auto-correlation-partial-auto-correlation-and-inverse-auto-correlation-while-modelling-an-ARIMA-series)"},{"metadata":{"_uuid":"90d9e5bb466fc9efbdb4c2cdaea14c4d30704988"},"cell_type":"markdown","source":"## Autocorrelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Autocorrelation of FFC of Close\nplot_acf(FFC[\"Close\"],lags=25,title=\"FFC\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As all lags are either close to 1 or at least greater than the confidence interval, they are statistically significant."},{"metadata":{"_uuid":"31e685329d81b91e23539b3fc589d21bed946ce7"},"cell_type":"markdown","source":"## Partial Autocorrelation"},{"metadata":{"_uuid":"d7174be0162bbb5e24b78fd184703527f19625eb","trusted":true},"cell_type":"code","source":"# Partial Autocorrelation of closing price of microsoft stocks\nplot_pacf(ENGRO[\"Close\"],lags=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82f98f2fc35b61c45f9b65e0477c227aefdaab47"},"cell_type":"markdown","source":"![](http://)Here, only 0th, 1st and 10th lag are statistically significant."},{"metadata":{"_uuid":"61ef0d5be5a9c6ab63c0f3e432deae0166530bd0","collapsed":true},"cell_type":"markdown","source":"# <a id='3'>3. Time series decomposition and Random walks</a>"},{"metadata":{"_uuid":"29a6016f3d75811e47db9d727497f06b5d194113"},"cell_type":"markdown","source":"## <a id='3.1'>3.1. Trends, seasonality and noise</a>\nThese are the components of a time series\n* Trend - Consistent upwards or downwards slope of a time series\n* Seasonality - Clear periodic pattern of a time series(like sine funtion)\n* Noise - Outliers or missing values"},{"metadata":{"_uuid":"55da1ae309826244f5aeeb18d293d2512535619d","trusted":true},"cell_type":"code","source":"# Let's take FFC stocks High for this\nFFC[\"High\"].plot(figsize=(16,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb9a833bf7f58467c00008560509a4ce6905ea92","trusted":true},"cell_type":"code","source":"# Now, for decomposition...\nrcParams['figure.figsize'] = 11, 9\ndecomposed_FFC_volume = sm.tsa.seasonal_decompose(FFC[\"High\"],freq=360) # The frequncy is annual\nfigure = decomposed_FFC_volume.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0f37643e9f41715ec54ecca09b9c10b81fe557d"},"cell_type":"markdown","source":"* There is clearly an upward trend in the above plot.\n* You can also see the uniform seasonal change.\n* Non-uniform noise that represent outliers and missing values"},{"metadata":{"_uuid":"fee84a3db1d59f4706711f078299cdff6b4bf1dc"},"cell_type":"markdown","source":"## <a id='3.2'>3.2. White noise</a>\nWhite noise has...\n* Constant mean\n* Constant variance\n* Zero auto-correlation at all lags"},{"metadata":{"_uuid":"01317cb2d3d8142945d94f751b60b7d59765f3df","trusted":true},"cell_type":"code","source":"# Plotting white noise\nrcParams['figure.figsize'] = 16, 6\nwhite_noise = np.random.normal(loc=0, scale=1, size=1000)\n# loc is mean, scale is variance\nplt.plot(white_noise)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4408976acde81f0281375fac4f6aa8dadd0e0c30","trusted":true},"cell_type":"code","source":"# Plotting autocorrelation of white noise\nplot_acf(white_noise,lags=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f70b17298bc4f7cd11ebb22af3369506e83ce5f8"},"cell_type":"markdown","source":"See how all lags are statistically insigficant as they lie inside the confidence interval(shaded portion)."},{"metadata":{"_uuid":"7b5855eed1497f9aea8ebc70b1f5621a1cd8ab92"},"cell_type":"markdown","source":"## <a id='3.3'>3.3. Random Walk</a>\nA random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers. \n\nIn general if we talk about stocks, Today's Price = Yesterday's Price + Noise\n\n# P<sub>t</sub> = P<sub>t-1</sub> + ε<sub>t</sub> \n\nRandom walks can't be forecasted because well, noise is random.\n\nRandom Walk with Drift(drift(μ) is zero-mean)\n#### P<sub>t</sub> - P<sub>t-1</sub> = μ + ε<sub>t</sub> \n<br>\nRegression test for random walk \n#### P<sub>t</sub> = α + βP<sub>t-1</sub> + ε<sub>t</sub>\n#### Equivalent to  P<sub>t</sub> - P<sub>t-1</sub> = α + βP<sub>t-1</sub> + ε<sub>t</sub>\n<br>\nTest:\n#### H<sub>0</sub>: β = 1 (This is a random walk)\n#### H<sub>1</sub>: β < 1 (This is not a random walk)\n<br>\nDickey-Fuller Test:\n#### H<sub>0</sub>: β = 0 (This is a random walk)\n#### H<sub>1</sub>: β < 0 (This is not a random walk)"},{"metadata":{"_uuid":"da19a9d1d683224c227654f28c1f53432953781c"},"cell_type":"markdown","source":"### Augmented Dickey-Fuller test\nAn augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. It is basically Dickey-Fuller test with more lagged changes on RHS."},{"metadata":{"_uuid":"c43ff44679e19f726cbca75a99d3b0c8e828cdca","trusted":true},"cell_type":"code","source":"# Augmented Dickey-Fuller test on volume of FFC and ENGRO stocks \nadf = adfuller(ENGRO[\"Volume\"])\nprint(\"p-value of ENGRO: {}\".format(float(adf[1])))\nadf = adfuller(FFC[\"Volume\"])\nprint(\"p-value of FFC: {}\".format(float(adf[1])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d81b28d7326841f6056eac3eb1e6e26e59b3809c"},"cell_type":"markdown","source":"##### As ENGRO has p-value 0.0003201525 which is less than 0.05, null hypothesis is rejected and this is not a random walk.\n##### Now FFC has p-value 0.0000006510 which is more than 0.05, null hypothesis is rejected and this is not a  random walk."},{"metadata":{"_uuid":"cc765a2077924a474a404b35d895d53e2f235ece"},"cell_type":"markdown","source":"### Generating a random walk"},{"metadata":{"_uuid":"a92da067abdb8f712e11e80b774bf326bf423def","trusted":true},"cell_type":"code","source":"seed(42)\nrcParams['figure.figsize'] = 16, 6\nrandom_walk = normal(loc=0, scale=0.01, size=1000)\nplt.plot(random_walk)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b65345199e9356ac6fc0c1f91013cfd2cfab0067","trusted":true},"cell_type":"code","source":"fig = ff.create_distplot([random_walk],['Random Walk'],bin_size=0.001)\niplot(fig, filename='Basic Distplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3503b3468028a38ac1345e62ab59996a333465b9"},"cell_type":"markdown","source":"## <a id='3.4'>3.4 Stationarity</a>\nA stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.\n* Strong stationarity:  is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n* Weak stationarity: is a process where mean, variance, autocorrelation are constant throughout the time\n\nStationarity is important as  non-stationary series that depend on time have too many parameters to account for when modelling the time series. diff() method can easily convert a non-stationary series to a stationary series.\n\nWe will try to decompose seasonal component of the above decomposed time series."},{"metadata":{"_uuid":"aa6661f017536cfc6c69d8b1a15316a6e37ca200","trusted":true},"cell_type":"code","source":"# The original non-stationary plot\ndecomposed_FFC_volume.trend.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"348f09fd4cb0c9ba057930c502786aaeda80452d","trusted":true},"cell_type":"code","source":"# The new stationary plot\ndecomposed_FFC_volume.trend.diff().plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bf108b3638b0aaa06b4022993b99fd5a57cecd2"},"cell_type":"markdown","source":"# <a id='4'>4. Modelling using statstools</a>"},{"metadata":{"_uuid":"7901cd86bf946ff83314ead9ee074f45e1ff76f8"},"cell_type":"markdown","source":"## <a id='4.1'>4.1 AR models</a>\n An autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation.\n #### AR(1) model\n R<sub>t</sub> = μ + ϕR<sub>t-1</sub> + ε<sub>t</sub>\n ##### As RHS has only one lagged value(R<sub>t-1</sub>)this is called AR model of order 1 where μ is mean and ε is noise at time t\n If ϕ = 1, it is random walk. Else if ϕ = 0, it is white noise. Else if -1 < ϕ < 1, it is stationary. If ϕ is -ve, there is men reversion. If ϕ is +ve, there is momentum.\n #### AR(2) model\n R<sub>t</sub> = μ + ϕ<sub>1</sub>R<sub>t-1</sub> + ϕ<sub>2</sub>R<sub>t-2</sub> + ε<sub>t</sub>\n #### AR(3) model\n R<sub>t</sub> = μ + ϕ<sub>1</sub>R<sub>t-1</sub> + ϕ<sub>2</sub>R<sub>t-2</sub> + ϕ<sub>3</sub>R<sub>t-3</sub> + ε<sub>t</sub>"},{"metadata":{"_uuid":"ead96dc18024b191be5d515516e69e3009cd209b"},"cell_type":"markdown","source":"## Simulating AR(1) model"},{"metadata":{"_uuid":"ff03dd2d04772c9da9cc1bbbe44cecbeff4386a3","trusted":true},"cell_type":"code","source":"# AR(1) MA(1) model:AR parameter = +0.9\nrcParams['figure.figsize'] = 16, 12\nplt.subplot(4,1,1)\nar1 = np.array([1, -0.9]) # We choose -0.9 as AR parameter is +0.9\nma1 = np.array([1])\nAR1 = ArmaProcess(ar1, ma1)\nsim1 = AR1.generate_sample(nsample=1000)\nplt.title('AR(1) model: AR parameter = +0.9')\nplt.plot(sim1)\n# We will take care of MA model later\n# AR(1) MA(1) AR parameter = -0.9\nplt.subplot(4,1,2)\nar2 = np.array([1, 0.9]) # We choose +0.9 as AR parameter is -0.9\nma2 = np.array([1])\nAR2 = ArmaProcess(ar2, ma2)\nsim2 = AR2.generate_sample(nsample=1000)\nplt.title('AR(1) model: AR parameter = -0.9')\nplt.plot(sim2)\n# AR(2) MA(1) AR parameter = 0.9\nplt.subplot(4,1,3)\nar3 = np.array([2, -0.9]) # We choose -0.9 as AR parameter is +0.9\nma3 = np.array([1])\nAR3 = ArmaProcess(ar3, ma3)\nsim3 = AR3.generate_sample(nsample=1000)\nplt.title('AR(2) model: AR parameter = +0.9')\nplt.plot(sim3)\n# AR(2) MA(1) AR parameter = -0.9\nplt.subplot(4,1,4)\nar4 = np.array([2, 0.9]) # We choose +0.9 as AR parameter is -0.9\nma4 = np.array([1])\nAR4 = ArmaProcess(ar4, ma4)\nsim4 = AR4.generate_sample(nsample=1000)\nplt.title('AR(2) model: AR parameter = -0.9')\nplt.plot(sim4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a33e37bf59ccf5d312e918b3f700b43672fa8f3"},"cell_type":"markdown","source":"## Forecasting a simulated model"},{"metadata":{"_uuid":"feba20bff98203d6862a547eb5919e3e6c809c8e","trusted":true},"cell_type":"code","source":"model = ARMA(sim1, order=(1,0))\nresult = model.fit()\nprint(result.summary())\nprint(\"μ={} ,ϕ={}\".format(result.params[0],result.params[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20039473ec2863639d8913e1ecde4f9803ac17bf"},"cell_type":"markdown","source":"ϕ is around 0.9 which is what we chose as AR parameter in our first simulated model."},{"metadata":{"_uuid":"f50dc3fc29a2618da26ae11d9127716233ed67cb"},"cell_type":"markdown","source":"## Predicting the models"},{"metadata":{"_uuid":"a9af97bfcdf0fd1fb5cf626a8bba0b4560fc4358","trusted":true},"cell_type":"code","source":"# Predicting simulated AR(1) model \nresult.plot_predict(start=900, end=1010)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1ec123d6ab0780304160f82b37f5f01fd007f84","trusted":true},"cell_type":"code","source":"rmse = math.sqrt(mean_squared_error(sim1[900:1011], result.predict(start=900,end=999)))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ecbeedf969c4972f05b9624a847aaffcb7862a9"},"cell_type":"markdown","source":"y is predicted plot. Quite neat!"},{"metadata":{"_uuid":"b24234f9ca3f0de65a3f75aa23caa204b21833b3","collapsed":true},"cell_type":"markdown","source":"# Predicting humidity level of Montreal\nhumid = ARMA(humidity[\"Montreal\"].diff().iloc[1:].values, order=(1,0))\nres = humid.fit()\nres.plot_predict(start=1000, end=1100)\nplt.show()"},{"metadata":{"_uuid":"d96b6ea96dacf1ee6fe41abd902b9184d9c9edbc","collapsed":true},"cell_type":"markdown","source":"rmse = math.sqrt(mean_squared_error(humidity[\"Montreal\"].diff().iloc[900:1000].values, result.predict(start=900,end=999)))\nprint(\"The root mean squared error is {}.\".format(rmse))"},{"metadata":{"_uuid":"8987a49063cea031745bd87ec71221db473069ab"},"cell_type":"markdown","source":"1.  Not quite impressive. But let's try google stocks. "},{"metadata":{"_uuid":"72ef1135eee34841500c55718bda48310386dceb","trusted":true},"cell_type":"code","source":"# Predicting closing prices of google\nhumid = ARMA(FFC[\"Close\"].diff().iloc[1:].values, order=(1,0))\nres = humid.fit()\nres.plot_predict(start=900, end=1010)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a01a4ff2973bd9f1a0d61d4099529d2b7852a52"},"cell_type":"markdown","source":"There are always better models."},{"metadata":{"_uuid":"00c24abb4b8f694234352b17bf4bfafc2ebe56f6"},"cell_type":"markdown","source":"## <a id='4.2'>4.2 MA models</a>\nThe moving-average (MA) model is a common approach for modeling univariate time series. The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.\n#### MA(1) model\nR<sub>t</sub> = μ  +  ϵ<sub>t</sub>1 + θϵ<sub>t-1</sub>\n\nIt translates to Today's returns = mean + today's noise + yesterday's noise\n##### As there is only 1 lagged value in RHS, it is an MA model of order 1"},{"metadata":{"_uuid":"cc7f14391e83d0b7ea17af030d91fc10b6aaf1d1"},"cell_type":"markdown","source":"## Simulating MA(1) model"},{"metadata":{"_uuid":"43853e28272d3949e8cb6a3d8bac0198d3de2457","trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 16, 6\nar1 = np.array([1])\nma1 = np.array([1, -0.5])\nMA1 = ArmaProcess(ar1, ma1)\nsim1 = MA1.generate_sample(nsample=1000)\nplt.plot(sim1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe571f9460704eedb7f2d13a2f08cf1332cb2b8f"},"cell_type":"markdown","source":"## Forecasting the simulated MA model"},{"metadata":{"_uuid":"bb579590bd40b3b31ece519dfe82f04cb85e1b09","trusted":true},"cell_type":"code","source":"model = ARMA(sim1, order=(0,1))\nresult = model.fit()\nprint(result.summary())\nprint(\"μ={} ,θ={}\".format(result.params[0],result.params[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2573e04db1c1a8830830122f879e6f855bad292b"},"cell_type":"markdown","source":"## Prediction using MA models"},{"metadata":{"_uuid":"62c83f6a97bee5f0dff89511fbf6910f83417b0a","collapsed":true},"cell_type":"markdown","source":"# Forecasting and predicting montreal humidity\nmodel = ARMA(humidity[\"Montreal\"].diff().iloc[1:].values, order=(0,3))\nresult = model.fit()\nprint(result.summary())\nprint(\"μ={} ,θ={}\".format(result.params[0],result.params[1]))\nresult.plot_predict(start=1000, end=1100)\nplt.show()"},{"metadata":{"_uuid":"0b72fb8004bfbdcee81960089502b4c892f40458","collapsed":true},"cell_type":"markdown","source":"rmse = math.sqrt(mean_squared_error(humidity[\"Montreal\"].diff().iloc[1000:1101].values, result.predict(start=1000,end=1100)))\nprint(\"The root mean squared error is {}.\".format(rmse))"},{"metadata":{"_uuid":"b2a42dfc2f0f4d38a95e85b6e2531314b1b2865c"},"cell_type":"markdown","source":"Now, for ARMA models.\n\n## <a id='4.3'>4.3 ARMA models</a>\nAutoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression and the second for the moving average. It's the fusion of AR and MA models.\n### ARMA(1,1) model\n### R<sub>t</sub> = μ + ϕR<sub>t-1</sub> + ϵ<sub>t</sub> + θϵ<sub>t-1</sub> \nBasically, Today's return = mean + Yesterday's return + noise + yesterday's noise."},{"metadata":{"_uuid":"0b163bbf7c903aa7f82d0dae381f019b572e27f5"},"cell_type":"markdown","source":"## Prediction using ARMA models\nI am not simulating any model because it's quite similar to AR and MA models. Just  forecasting and predictions for this one."},{"metadata":{"_uuid":"6b9f4f218af096d41cbeafc00bf15e1d6581c6c3","trusted":true},"cell_type":"code","source":"# Forecasting and predicting ENGRO stocks volume\nmodel = ARMA(ENGRO[\"Volume\"].diff().iloc[1:].values, order=(3,3))\nresult = model.fit()\nprint(result.summary())\nprint(\"μ={}, ϕ={}, θ={}\".format(result.params[0],result.params[1],result.params[2]))\nresult.plot_predict(start=1000, end=1100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a1b9a4635ee4d6ad382010f066d229daebbf2c","trusted":true},"cell_type":"code","source":"rmse = math.sqrt(mean_squared_error(ENGRO[\"Volume\"].diff().iloc[1000:1101].values, result.predict(start=1000,end=1100)))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c337a7fb003b98285a0c3157c0f48742d3a1b9e"},"cell_type":"markdown","source":"ARMA model shows much better results than AR and MA models."},{"metadata":{"_uuid":"952fcfd1498c44e4541f93dc07f32c34c37cede1"},"cell_type":"markdown","source":"## <a id='4.4'>4.4 ARIMA models</a>\nAn autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model. Both of these models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the \"integrated\" part of the model) can be applied one or more times to eliminate the non-stationarity.\nARIMA model is of the form: ARIMA(p,d,q): p is AR parameter, d is differential parameter, q is MA parameter\n### ARIMA(1,0,0)\n**y<sub>t</sub> = a<sub>1</sub>y<sub>t-1</sub> + ϵ<sub>t</sub>**\n### ARIMA(1,0,1)\n**y<sub>t</sub> = a<sub>1</sub>y<sub>t-1</sub> + ϵ<sub>t</sub> + b<sub>1</sub>ϵ<sub>t-1</sub>**\n### ARIMA(1,1,1)\n**Δy<sub>t</sub> = a<sub>1</sub>Δy<sub>t-1</sub> + ϵ<sub>t</sub> + b<sub>1</sub>ϵ<sub>t-1</sub>** where **Δy<sub>t</sub> = y<sub>t</sub> - y<sub>t-1</sub>**"},{"metadata":{"_uuid":"1f87feaedef7e790890e271e86c4fa2e71c05367"},"cell_type":"markdown","source":"## Prediction using ARIMA model"},{"metadata":{"_uuid":"d80a45b8c911f346891534fe2f1c6d519907a86c","trusted":true},"cell_type":"code","source":"# Predicting the ENGRO stocks volume\nrcParams['figure.figsize'] = 16, 6\nmodel = ARIMA(ENGRO[\"Volume\"].diff().iloc[1:].values, order=(2,1,0))\nresult = model.fit()\nprint(result.summary())\nresult.plot_predict(start=700, end=1000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feef92dc9eabf930b947f64d46dc90b91afc49a4","trusted":true},"cell_type":"code","source":"rmse = math.sqrt(mean_squared_error(ENGRO[\"Volume\"].diff().iloc[700:1001].values, result.predict(start=700,end=1000)))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99fe66a0b2efcc111ff3c42ef48fac0b08af4c8a"},"cell_type":"markdown","source":"Taking the slight lag into account, this is a fine model."},{"metadata":{"_uuid":"1c3b1f6da7f472fcc8275a23d9f9fde20a89fc3e"},"cell_type":"markdown","source":"## <a id='4.5'>4.5 VAR models</a>\nVector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations: The only prior knowledge required is a list of variables which can be hypothesized to affect each other intertemporally.\n\n<img src=\"http://gdurl.com/VvRm\">"},{"metadata":{"_uuid":"c7ee4df651be1857f5eb08666ddd30a64953e20a","scrolled":false,"trusted":true},"cell_type":"code","source":"# Predicting closing price of FFC and ENGRO\ntrain_sample = pd.concat([FFC[\"Close\"].diff().iloc[1:],ENGRO[\"Close\"].diff().iloc[1:]],axis=1)\nmodel = sm.tsa.VARMAX(train_sample,order=(2,1),trend='c')\nresult = model.fit(maxiter=1000,disp=False)\nprint(result.summary())\npredicted_result = result.predict(start=0, end=1000)\nresult.plot_diagnostics()\n# calculating error\nrmse = math.sqrt(mean_squared_error(train_sample.iloc[1:1002].values, predicted_result.values))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a20e3db98f52bb92143793d651bc280884ecd068"},"cell_type":"markdown","source":"## <a id='4.6'>4.6 State Space methods</a>\n\nA general state space model is of the form\n\ny<sub>t</sub>=Z<sub>t</sub>α<sub>t</sub>+d<sub>t</sub>+ε<sub>t</sub>\n\nα<sub>t</sub>=T<sub>t</sub>α<sub>t</sub>−1+c<sub>t</sub>+R<sub>t</sub>η<sub>t</sub>\n\nwhere y<sub>t</sub> refers to the observation vector at time t, α<sub>t</sub> refers to the (unobserved) state vector at time t, and where the irregular components are defined as\n\nε<sub>t</sub>∼N(0,H<sub>t</sub>)\n\nη<sub>t</sub>∼N(0,Q<sub>t</sub>)\n\nThe remaining variables (Z<sub>t</sub>,d<sub>t</sub>,H<sub>t</sub>,T<sub>t</sub>,c<sub>t</sub>,R<sub>t</sub>,Q<sub>t</sub>) in the equations are matrices describing the process. Their variable names and dimensions are as follows\n\nZ : design (k_endog×k_states×nobs)\n\nd : obs_intercept (k_endog×nobs)\n\nH : obs_cov (k_endog×k_endog×nobs)\n\nT : transition (k_states×k_states×nobs)\n\nc : state_intercept (k_states×nobs)\n\nR : selection (k_states×k_posdef×nobs)\n\nQ : state_cov (k_posdef×k_posdef×nobs)\n\nIn the case that one of the matrices is time-invariant (so that, for example, Z<sub>t</sub>=Z<sub>t</sub>+1 ∀ <sub>t</sub>), its last dimension may be of size 1 rather than size nobs.\n\nThis generic form encapsulates many of the most popular linear time series models (see below) and is very flexible, allowing estimation with missing observations, forecasting, impulse response functions, and much more.\n\nSource: [statsmodels](https://www.statsmodels.org/dev/statespace.html)"},{"metadata":{"_uuid":"aa056e8b4f0c457639f5cfb01cfa6f9d6b1fb486"},"cell_type":"markdown","source":"## <a id='4.6.1'>4.6.1 SARIMA models</a>\nSARIMA models are useful for modeling seasonal time series, in which the mean and other statistics for a given season are not stationary across the years. The SARIMA model defined constitutes a straightforward extension of the nonseasonal autoregressive-moving average (ARMA) and autoregressive integrated moving average (ARIMA) models presented"},{"metadata":{"_uuid":"8441c5af41b070d19fd711dee43b118855676862","trusted":true},"cell_type":"code","source":"# Predicting closing price of FFC'\ntrain_sample = FFC[\"Close\"].diff().iloc[1:].values\nmodel = sm.tsa.SARIMAX(train_sample,order=(4,0,4),trend='c')\nresult = model.fit(maxiter=1000,disp=False)\nprint(result.summary())\npredicted_result = result.predict(start=0, end=500)\nresult.plot_diagnostics()\n# calculating error\nrmse = math.sqrt(mean_squared_error(train_sample[1:502], predicted_result))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9a2a2fa5762719f770406f9f37fb04af4cb384","trusted":true},"cell_type":"code","source":"plt.plot(train_sample[1:502],color='red')\nplt.plot(predicted_result,color='blue')\nplt.legend(['Actual','Predicted'])\nplt.title('FFC Closing prices')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba27f063b177faca83f659c91981ebbcf3d32513"},"cell_type":"markdown","source":"## <a id='4.6.2'>4.6.2 Unobserved components</a>\nA UCM decomposes the response series into components such as trend, seasons, cycles, and the regression effects due to predictor series. The following model shows a possible scenario:\n<img src=\"http://support.sas.com/documentation/cdl/en/etsug/66840/HTML/default/images/etsug_ucm0134.png\">\nSource: [http://support.sas.com/documentation/cdl/en/etsug/66840/HTML/default/viewer.htm#etsug_ucm_details01.htm](http://support.sas.com/documentation/cdl/en/etsug/66840/HTML/default/viewer.htm#etsug_ucm_details01.htm)"},{"metadata":{"_uuid":"eccfc1ab3a1ffec0f739f09c3c52e9a4e40e93c0","trusted":true},"cell_type":"code","source":"# Predicting closing price of FFC'\ntrain_sample = FFC[\"Close\"].diff().iloc[1:].values\nmodel = sm.tsa.UnobservedComponents(train_sample,'local level')\nresult = model.fit(maxiter=1000,disp=False)\nprint(result.summary())\npredicted_result = result.predict(start=0, end=500)\nresult.plot_diagnostics()\n# calculating error\nrmse = math.sqrt(mean_squared_error(train_sample[1:502], predicted_result))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef27e360e92c6603a80ab4072c8101416de04a3","trusted":true},"cell_type":"code","source":"plt.plot(train_sample[1:502],color='red')\nplt.plot(predicted_result,color='blue')\nplt.legend(['Actual','Predicted'])\nplt.title('FFC Closing prices')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d041bf48f3f835d8b9a397f066c21577c0cd235"},"cell_type":"markdown","source":"## <a id='4.6.3'>4.6.3 Dynamic Factor models</a>\nDynamic-factor models are flexible models for multivariate time series in which the observed endogenous variables are linear functions of exogenous covariates and unobserved factors, which have a vector autoregressive structure. The unobserved factors may also be a function of exogenous covariates. The disturbances in the equations for the dependent variables may be autocorrelated."},{"metadata":{"_uuid":"d6d0a1d158db53132b9b0406ca971da7820af338","trusted":true},"cell_type":"code","source":"# Predicting closing price of FFC and ENGRO\ntrain_sample = pd.concat([FFC[\"Close\"].diff().iloc[1:],ENGRO[\"Close\"].diff().iloc[1:]],axis=1)\nmodel = sm.tsa.DynamicFactor(train_sample, k_factors=1, factor_order=2)\nresult = model.fit(maxiter=1000,disp=False)\nprint(result.summary())\npredicted_result = result.predict(start=0, end=1000)\nresult.plot_diagnostics()\n# calculating error\nrmse = math.sqrt(mean_squared_error(train_sample.iloc[1:1002].values, predicted_result.values))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41fef604d6c49239a262468401fb548fed77ad7b"},"cell_type":"markdown","source":"I may add more regression models soon and there is much more stuff to cover.\nStay tuned for more! And don't forget to upvote and comment."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}