{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a beginner friendly for predicting Heart disease using superivised machine learning.\nHere we are given certain values of like age, gender, blood pressure, and we have to predict whether the patient would suffer from heart disease or has heart disease. Along with the information about patient, we are also given the label. The label tells us whether the patient suffers from disease or not.","metadata":{}},{"cell_type":"markdown","source":"So Lets get to it. For any supervised machine learning tasks, the most popular set of steps followed are:\n1. Exploratory Data Analysis.\n2. Data preprocessing.\n3. Model training.\n4. Model evaluation and testing.","metadata":{}},{"cell_type":"markdown","source":"# Load the Data.","metadata":{}},{"cell_type":"markdown","source":"**Lets Start...**\nFirst lets load the data, so that we can manipulate it using python. We do this using *pandas* library in python.","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt# for plotting\nimport seaborn as sns # for plotting again... for advanced plots.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we load the data in data frame of python.\ndf=pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\") #loading the data\ndf.head(5)#lets see the first 5 rows of the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(5)#lets see the last 5 rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the shape of the dataset\nprint(\"Number of rows:\",df.shape[0])\nprint(\"Number of columns:\",df.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA).","metadata":{}},{"cell_type":"markdown","source":"**So what is this term EDA? Why is this said as the most crucial step in data science?**\n\nWell here we actually try to understand the data, plot multiple graphs which might give us some insights or intresting patterns. Understanding the data before going for model building is very important. \n\n**Why is it important?**\n\nIt is important as only after understanding the data, you can apply the machine learning algorithms in an effective way.\n\nSo lets start with the types of attributes. There are 2 broad types of attributes *Numeric* attributes like age, or marks of a test. The second type of attribute is *Categorical* attributes which have categories like gender or grade obtained from a test. ","metadata":{}},{"cell_type":"markdown","source":"We would first find the meaning of each attribute.\n\n**Numeric Attributes:**\n\n* age: Age in years\n* trestbps: Resting Blood Pressure, individual values displayed in mmHg unit.\n* chol: Serum cholesterol value displayed in mg/dl.\n* thalach: Maximum heart rate achieved.\n* oldpeak: ST depression induced by exercise related to rest.\n\n**Categorical attributes:**\n\n* sex: Displays the gender (Male(1), Female(0)).\n* cp: Chest pain type (Typical Angina(0), Atypical Angina(1), Non-angina pain(2), asymptomatic(3)).\n* fbs: Fasting blood sugar, given as a comparison. If value > 120 mg/dl then true(1) else false(0).\n* restecg:  Resting electrocardiographic result (Normal(0), ST-T wave abnormality(1), Left ventricular hypertrophy (2)).\n* exang: Exercise include angina (No(0), Yes(1)).\n* slope: Slope of the peak exercise ST segment (Upsloping(0), Flat(1), Downsloping(2)).\n* thal: Displays the thalassemia (Unknown(0), Normal(1), Fixed defect(2), Reversible defect(3)).\n* ca: Number of major vessels colored by fluoroscopy.(Numbers 0, 1, 2, 3, 4).\n","metadata":{}},{"cell_type":"code","source":"#ok lets start with some simple analysis:\n#lets get the mean, mode and other information about the dataset\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Its just numbers...\n#lets start plots...\n#lets get the count of male and females in the dataset.\ndf['sex'].hist()\n#looks like we have more male patients than females.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets have such of an histogram for every attribute:\nfig, axis = plt.subplots(7,2,figsize=(10, 15))\ndf.hist(ax=axis)\nplt.show()\n#after having a closer look at the graphs below you can see the difference in the histograms of  numeric and categorical attributes.\n#The categorical ones have distinct bars while those of numeric attributes are connected.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have heard that male are more prone to heart disease than female. \n#Lets see if its true for our dataset:\n#Visualizing data with liver disease along with Gender\nplt.figure(figsize=(5,5))\n\n#here we use count plot from seaborn which can plot the frequencies\nax = sns.countplot(x = df['target'].apply(lambda x:'Heart Disease' if x == 1 else 'Normal'), hue=df['sex'])\nax.set_xlabel('Patient Condition')\nplt.show()\n#turns out it is true. Men are more prone to heart disease than women.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets have a look at correlation plot.\n\n**Whats correlation??**\n* This value is calculated between two attributes.\n* Range from -1 to 1.\n* Positive correlation (value: 0 to 1):If value of first attribute increases, the value of second attribute also increases. for eg. Grade and marks: the grade of the person is much dependent on the test marks. So the value of correlation will be high.\n* Negative correlation (value: 0 to -1):If value of first attribute increases, the value of second attribute decreases. for eg. ID and Grade: The grade of the person is not at all dependent on the email id. So the value of correlation will be less.\n* The higher the magnitude of the value, more the attributes are related to each other.","metadata":{}},{"cell_type":"code","source":"#Lets plot this\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap='Greens',annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above graph we can get following colclusions regarding the Target column:\n1. Thalach and target: if the maximum heartrate increases, the possiblity of heart disease increases.\n2.  Exang and Target: if patient doesn't have exercise include angina, the possiblity of heart disease increases (negative correlation).\n3. Ca and target: if the patient has less number of major vessels colored by fluoroscopy,  the possiblity of heart disease increases. \nIn similar way we can extract other correlations as well.","metadata":{}},{"cell_type":"code","source":"#Lets plot pair plots.\n# This is mostly done for numerical columns where you can see all the points for the two columns.\n# Using this aswell we can get some intresting patterns.\ncolumns=['age','trestbps','chol','thalach','oldpeak','target']#selecting columns\nsns.pairplot(df[columns],hue='target',corner=True,diag_kind='hist')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Data preprocessing has 3 sub steps namely Data cleaning, Data Transformation, Data Reduction.\n![t*](https://media.geeksforgeeks.org/wp-content/uploads/20190312184006/Data-Preprocessing.png)\n\nSource: [Geeks for geeks](https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/)","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"code","source":"#Checking for missing values\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great there are no values missing in the dataset...\nlets go and have a look for Noisy data.\n\n**What is noisy data?**\n\nNoisy data is meaningless data that computers cannot perceive. It can be generated because of errors in data collection, input errors, etc. Such noisy data rows are also called as outliers. Mostly outliers occur for numeric data. For eg. consider in our data, the age of a person is 29 but by mistake it got registered as 129. That point is beyond the normal range of age. Hence it is an outlier. For categorical data, consider the target column where 1 signifies patitent has heart disease and 0 signifies patient doesnot have heart disease, but we got value as 2 which is undefined. However in our dataset there arent \n\n**How to detect Outliers?**\n\nWell there are several ways to do that like using Box plots, using z-score techniques.\n\n**How to deal with Outliers?**\n\nWe can either apply binning or delete the outliers. In this notebook, we would delete them. However, in most of the cases, it depends on the nature of the outliers.\n\nLets go for outliers","metadata":{}},{"cell_type":"code","source":"#Checking for unique values of categorical values.\n#If number of unique values dont change from the given number of unique values,then we do not have categorical outliers in the data.\nnumeric_cols=['age','trestbps','chol','thalach','oldpeak']#numeric attributes\ncat_cols=['sex','cp','fbs','restecg','exang','slope','ca','thal']#categorical attributes\n\ndf[cat_cols].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The results show that there are no outliers. \n# There are two values for the sex attribute 1-0 and the code above also indicates the same value. \n#This applies to other categorical attributes as well.\n\n#lets go for box plots of numeric attributes.\nplt.figure(figsize=(10,19))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df[numeric_cols]))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#It turns out there are some outliers in trestbps, chol, and thalach.\n#lets apply zscore technique aswell to detect outliers.\n#if value of zscore is greater than 3 and less than -3 are treated as outliers \n#However, this threshold of 3  and -3 is set by us and we can change it.\nfrom scipy import stats\nz = np.abs(stats.zscore(df))\nprint(z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok here we cant see any outliers :( \n# we cant make out which ones are outliers from such a big table....\n#let the computer do it for us...\ndf_outliers= df[(z >= 3).any(axis=1)] # it says that any of the column with value of z above 3 \ndf_outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_outliers.shape)\n#16 columns!! thats too many as we have a small dataset of only 303 rows.\n#we would increase the threshold silghtly\ndf_outliers= df[(z >= 3.5).any(axis=1)] \ndf_outliers\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We would remove these 6 rows with outlier values now\ndf_clean=df[(z <3.5).all(axis=1)] # pay attention to the condition.\ndf_clean.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets check for duplicate samples.\n# When present in large quantities, duplicate samples may reduce the effectiveness of the models.\nduplicate = df_clean[df_clean.duplicated()]\nduplicate\n#There is only one duplicate value so there is no need to delete it.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Transformation.","metadata":{}},{"cell_type":"markdown","source":"Here we would transform the data in suitable forms to avoid biases.\n* Categorical attribute: apply one hot encoding.\n* Numeric attribute: scale them in range of -1 to 1.","metadata":{}},{"cell_type":"code","source":"#applying One Hot encoding to categorical columns \ndata = pd.get_dummies(df_clean,columns =['cp','restecg','slope','ca','thal'])\n#we only apply one hot encoding to categorical columns with more than 2 unique value.\n#hence the above set doesnot contain the columns sex, fbs and exang which have only 2 unique values 0 and 1.\ndata.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets scale the numeric columns\n# This scaling is done so as the model wont get stuck in local opitmal value and converge fast.\nnumeric_cols=['age','trestbps','chol','thalach','oldpeak']#numeric attributes\nfrom sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\n# standard scaler scales the columns in range of -1 to 1 based on the value of their mean, and standard deviation.\ndata[numeric_cols] = standardScaler.fit_transform(data[numeric_cols])\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Reduction","metadata":{}},{"cell_type":"markdown","source":"Here,we select important features. This is done to reduce dataset size.\nAlso by selecting important features we improve the performance of the model.\nFor this, we would apply *Selection of features from the model* technique by sklearn.\n\nIn this technique, we apply the Machine learning model and the model defines an importance value for each feature.\nFeatures with importance value above certain threshold are decided as important features.","metadata":{}},{"cell_type":"code","source":"# for this to work we should first divide the dataset in features and labels.\n# features are normally denoted by \"x\" and labels by \"y\".\ny=data['target']\ny=np.array(y)\nx=data.drop(columns=['target']) # removed the label column from dataframe and passed it to x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets work on feature selection\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier(n_estimators=500).fit(x, y)\nselector = SelectFromModel(clf, prefit=True)\nx_columns=x.columns #all columns of x\ncolumns=selector.get_support() #selected list of columns with true and false values\nselected_columns=list([x_columns[i] for i in range(len(columns)) if columns[i]]) #creating list of selected columns\nprint(selected_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#selecting these important columns\nx_reduced=selector.transform(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thats great, now our data is ready to be given to a machine learning model. ","metadata":{}},{"cell_type":"markdown","source":"# Model Training and Evaluation","metadata":{}},{"cell_type":"markdown","source":"We wont be training one mocel but 4 models and compare there performance.\nThose 4 models are Logistic regression, Random Forest, SVM, and KNN.\n\nAlso we will be evaluating our models performance on various common used metric:\n* Accuracy: The percentage of correct predictions for the test data.\n\n\nWe use *5 fold cross validation* to get reliable results","metadata":{}},{"cell_type":"code","source":"#splitting the dataset for Training and testing and using 5-fold Cross validation.\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nkf.get_n_splits(x_reduced)\n\n#metrics for SVM\nSVM_accuracy=[]\n\n#metrics for Random Forest\nRF_accuracy=[]\n\n#metrics for KNN\nKNN_accuracy=[]\n\n#metrics for Logistic Regression\nLR_accuracy=[]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initializing the models\n#importing libraries of the selected algorithms\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n#importing libraries of performance Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n#Making the classifier Objects\nclf_svm=SVC() #SVM object\nclf_rf=RandomForestClassifier(max_depth=100, random_state=0)#Random Forest Object\nclf_knn = KNeighborsClassifier(n_neighbors=30)#KNN object\nclf_lr=LogisticRegression(C= 1, class_weight= None, penalty= 'l2', solver= 'newton-cg')#Logistic regression model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=1# count the number of folds\n#starting the 5 fold cross valivation\nfor train_index, test_index in kf.split(x_reduced):\n    print(\"\\nNumber of fold: %d\"%i)\n    i+=1\n    #Splitting the data\n    X_train, X_test = x_reduced[train_index], x_reduced[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    #Training and Evaluating SVM\n    model=clf_svm.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    SVM_accuracy.append(accuracy_score(y_test,y_pred))\n    print(\"Working on SVM\")\n    \n    #Training and Evaluating Random Forest\n    model=clf_rf.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    RF_accuracy.append(accuracy_score(y_test,y_pred))\n    print(\"Working on Random Forest\")\n    \n    #Training and Evaluating KNN\n    model=clf_knn.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    KNN_accuracy.append(accuracy_score(y_test,y_pred))\n    print(\"Working on KNN\")\n    \n    #Training and Evaluating LR\n    model=clf_lr.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    LR_accuracy.append(accuracy_score(y_test,y_pred))\n    print(\"Working on Logistic Regression\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing average results:\nSVM=[\"SVM \", (sum(SVM_accuracy)/len(SVM_accuracy))]\n\nRF=[\"RF \", (sum(RF_accuracy)/len(RF_accuracy)) ]\n\nKNN=[\"KNN \", (sum(KNN_accuracy)/len(KNN_accuracy))]\n\nLR=[\"LR \", (sum(LR_accuracy)/len(LR_accuracy))]\ndata=[]\ndata.append(SVM)\ndata.append(RF)\ndata.append(KNN)\ndata.append(LR)\n#converting results to dataframe\nresults=pd.DataFrame(data,columns=[\"Algorithms\",\"Accuracy\"])\nresults\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets plot this accuracy\nresults_new=results.set_index('Algorithms')\nresults_new['Accuracy'].plot(kind='bar')\nplt.ylabel(\"Accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above comparison we get that Logistic Regression gives the best accuracy of 78%. There are still many things which can be done like developing new features, tuning hyper parameters and also improving feature selection techniques.  ","metadata":{}}]}