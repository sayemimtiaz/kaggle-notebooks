{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div>\n    <h1 style=\"text-align:left;\">Non Pharmaceutical Intervention During Pandemic </h1>\n    <p style = \"line-height:2;text-align:left;font-size:80%\">This is research about the effectivity of Non Pharmaceutical Intervention (NPI) during pandemic. As we know, NPI focused on <strong>rules and policy</strong> during pandemic and we aim to find the effectivity of the NPIs based on the research that has done before. The main idea is to create <strong> question-answering (QA) by BERT - SQuAD from the abstract of the papers </strong>(which is inspired by this <a href = \"https://www.kaggle.com/dirktheeng/anserini-bert-squad-for-context-corpus-search\">kernel from Dirk</a> and please upvote him). But to make the process faster and could cover up as many paper related to our question as we could, we must add similar <strong> keyword finding</strong> and topic clustering to our overall scheme.<br>\n    For example we want to search for the answer to the question \"how is the lockdown effectivity to reduce transmission rate during pandemic?\" with the keyword \"lockdown\". But then we will find that \"quarantine\" in this case has almost similar meaning to the word \"lockdown\" and if we only focusing on \"lockdown\", we may miss important information and we can't cover up as many paper related to our question as we could. So the first step is <strong>generating keyword, convert it to embeddings, and finding similar words to our keyword by Euclidian Distance</strong>.<br>\n    The second step is from those keywords, we <strong>find papers related to the keywords by BM25 algorithm</strong> which is inspired from <a href = \"https://www.kaggle.com/dgunning/building-a-cord19-research-engine-with-bm25\">this kernel from DwightGunning</a> and please upvote him.<br>\n    But then a problem arise : how if the quarantine's meaning in the paper is for some bacteria quarantine? The topic is not the same with the original keyword. The solution is we <strong>only choose the papers with the same cluster of topic from the selected papers from the additional keyword</strong>. That means : we must also do topic clustering for the abstracts of papers and the topic clustering code is inspired from <a href = \"https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\"> this article</a> by Shashank Kapadia.<br>\n    Illustration of the workflow can be seen below.</p> \n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing it this way because have been trying to post it on image hoster and unable to open it from here\n\nImage(\"../input/scheme-covid/scheme.png\", width = 800, height = 800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Preparation Step : Loading Module, Helper Function, and Preparing Data</h3>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Loading modules\n\n# general module\nimport os\nimport collections\nimport pandas as pd\nimport pickle\npd.options.display.max_colwidth = 500\nimport numpy as np\nimport re\nimport json\nfrom tqdm.notebook import tqdm as tqdm\nfrom pprint import pprint\nfrom copy import deepcopy\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nimport logging\nimport math\n\n# visualization module for topic modelling\nimport pyLDAvis.gensim\nimport pickle \nimport pyLDAvis\n\n# modelling module\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary\nimport spacy\nfrom sklearn.decomposition import PCA\npca = PCA(2)  # setting PCA to 2 axis\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n\n# search engine module bm25\n!pip install rank_bm25 nltk\nfrom rank_bm25 import BM25L","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Helper function\n\n# function to structurize the dataframe from json format\n# the function is taken from this kernel https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv\n# but with a little editing (we only need the author, title, and abstract)\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        name_ls.append(name)  \n    return \", \".join(name_ls)\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"  \n    return body\n# function to process the json file based on structurizing function above\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file) \n    return raw_files\ndef generate_clean_df(all_files):\n    cleaned_files = []   \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_body(file['abstract'])\n        ]\n        cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors', 'abstract']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head() \n    return clean_df\n\n# function for pre-processing text  \ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n# initialize stopwords\nenglish_stopwords = stopwords.words('english')\n# initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n# define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in english_stopwords] for doc in texts]\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n# tokenizing the text\nlist_stopwords = list(set(english_stopwords))\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words if word.isalnum() \n                                             and not word in list_stopwords\n                                             and not (word.isnumeric() and len(word) < 4)]))\ndef preprocess(string):\n    return tokenize(string.lower())\n\n# class for creating \"search engine\"\nclass Paper:\n    def __init__(self, item):\n        self.paper = item.to_frame().fillna('')\n        self.paper.columns = ['Value']\n    def abstract(self):\n        return self.paper.loc['abstract'].values[0]\n    def title(self):\n        return self.paper.loc['title'].values[0]\n\nclass SearchResults:\n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]        \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    def __len__(self):\n        return len(self.results)    \n    def _repr_html_(self):\n        return self.results._repr_html_()\n\nSEARCH_DISPLAY_COLUMNS = ['title','abstract']\n\nclass WordTokenIndex:\n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])\n\nclass RankBM25Index(WordTokenIndex):\n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25L(self.index.terms.tolist())  \n    def search(self, search_string, n=10):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating data frame\n\nprepared_dataframe = 1 # change this to zero if you dont have any prepared dataframe\n\nif not prepared_dataframe :\n    # data from biorxiv\n    biorxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\n    biorxiv_files = load_files(biorxiv_dir)\n    biorxiv_df = generate_clean_df(biorxiv_files)\n\n    # data from commercial use\n    comm_dir = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'\n    comm_files = load_files(comm_dir)\n    comm_df = generate_clean_df(comm_files)\n\n\n    # data from non commercial use\n    noncomm_dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/'\n    noncomm_files = load_files(noncomm_dir)\n    noncomm_df = generate_clean_df(noncomm_files)\n\n    # data from custom use\n    custom_dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/'\n    custom_files = load_files(custom_dir)\n    custom_df = generate_clean_df(custom_files)\n\n    df = pd.concat([biorxiv_df, comm_df, noncomm_df, custom_df], axis  = 0)\n\n    # deleting duplicated abstract because we focused on abstract\n    df.drop_duplicates(subset = 'abstract', keep = 'first', inplace = True)\n    df.to_csv('data.csv', index = False)\n    \nelse :\n    df = pd.read_csv('../input/data-covid-paper/data.csv')\n\nbm25 = RankBM25Index(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Modelling Step : Word Embedding by Word2Vec and Topic Modelling by LDA</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessed the abstract\n\npre_trained = 1 # change this to 0 if you dont have the embeddings\n\ndf['abstract'] = df['abstract'].fillna('').apply(str)\n# remove punctuation\ndf['abstract_processed'] = df['abstract'].map(lambda x: re.sub('[,\\.!?]', '', x))\n# convert to lowercase\ndf['abstract_processed'] = df['abstract_processed'].map(lambda x: x.lower())\ndata_words = list(sent_to_words(df.abstract_processed.values.tolist()))\ndel df['abstract_processed']\n# build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n# faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n# remove stop words\ndata_words_nostops = remove_stopwords(data_words)\n# form bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dictionary of W2V embeddings\n\nif not pre_trained :\n    # do lemmatization for unbigram data (keeping only noun, adj, vb, adv)\n    data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n    # making the pre trained w2v_embeddings's vocabulary and embeddings to dictionary\n    w2v_embeddings = Word2Vec(data_lemmatized, size=200, sg=1, min_count=1, window=8, hs=0, negative=15, workers=1)\n    ordered_vocab = [(term, voc.index, voc.count) for term, voc in w2v_embeddings.wv.vocab.items()]\n    ordered_vocab = sorted(ordered_vocab, key=lambda k: k[2])\n    ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n    word_vectors = pd.DataFrame(w2v_embeddings.wv.syn0[term_indices, :], index=ordered_terms)\n    word_vectors = word_vectors.T.to_dict()\n    f = open(\"word_vectors\",\"wb\")\n    pickle.dump(word_vectors,f)\n    f.close()\nelse :\n    with open('../input/word-embeddings-covid-paper/word_vectors', 'rb') as embedding:\n        word_vectors = pickle.load(embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create LDA model\n# code taken from https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n\n# do lemmatization for bigram data (keeping only noun, adj, vb, adv)\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\nid2word = Dictionary(data_lemmatized)\nid2word.filter_extremes(no_below=20, no_above=0.5)\n# create corpus of bag of words\ncorpus = [id2word.doc2bow(text) for text in data_lemmatized]\nif not pre_trained :\n    # create LDA model\n    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=17, \n                                           random_state=100,chunksize=100,passes=10,\n                                           per_word_topics=True)\n    lda_model.save('lda_model')\nelse :\n    lda_model = LdaModel.load('../input/lda-model-covid-paper/ldamodel')\n# compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n# print the topic model\npprint(lda_model.print_topics())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the topics\npyLDAvis.enable_notebook()\nLDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nLDAvis_prepared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the coherence score is good enough and from the visualization some of the topic clusters are well separated"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Listing topics of each paper\n\n# listing the topic's probability for each paper\nt = []\nfor text in data_lemmatized:\n    bow = id2word.doc2bow(text)\n    t.append(lda_model.get_document_topics(bow))\n    \nparsing_t = []\nfor topics in t :\n    topic = [a[0] for a in topics]\n    parsing_t.append(topic)\ndf['topic'] = parsing_t  \n\n# creating dictionary of topics for each title\nlist_title = list(df['title'])\nlist_topic = list(df['topic'])\ntopic_dict = {}\nfor i in range(len(list_title)) :\n    topic_dict.update(dict({list_title[i] : list_topic[i]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the similar word by comparing Euclidian Distance between transformed word embeddings to PCA\n\nkeywords = [\"coordination and communication during pandemic\",\"resource distribution strategies\",\"infrastructure for pandemic\",\"institutional collaboration\",\"government strategy\",\"facemask distribution\",\"hospital utilization\", \n            \"npi assessment\",\"intervention effectiveness during outbreak\",\"execution of planning\",\"isolation implementation\",\"school closure\",\"travel ban\",\"mass gathering effect\",\"social distancing result\",\"lockdown effectivity\",\"emigration ban\",\"testing of mass scale\",\"facemask wearing\", \"nursing education\",\"city quarantine\",\"mathematical modelling\",\n            \"web based surveillance\",\"tracking of community source\",\"digital technology\",\"media effectivity on reporting\",\"campaign strategy\",\"influencing in social media\",\"policy during pandemic\", \"public knowledge\", \"spread controlling\",\n            \"economic strategy during pandemic\",\"cost analysis\",\"housing price\",\"occupation rate\",\"household price\",\n            \"homeless compliance\", \"quarantine of city\",\"mathematical modelling\"]\n\ntitle_dict = {}\n\nfor keyword in keywords:\n    \n    keyword_init = keyword.split()[0]\n    print(\"Most similar words to the keyword '\" + keyword_init + \"' :\")\n    print(\"\")\n    \n    # getting keyword's embeddings\n    vec1 = []\n    for val in word_vectors[keyword_init].values() :\n        vec1.append(val)\n    vec1 = np.array(tuple(vec1))\n        \n    distances = []\n    words = []\n    \n    for word in word_vectors.keys() :\n        if word != keyword_init :\n            # getting compared word's embeddings\n            vec2 = []\n            for val in word_vectors[word].values() :\n                vec2.append(val)\n            vec2 = np.array(tuple(vec2))\n        # calculating Euclidian Distance between two vector\n        distances.append(np.linalg.norm(vec1 - vec2))\n        words.append(word)\n    \n    # sorting the distance\n    indices = np.argsort(distances)\n    top_words = [words[j] for j in indices[0:7]]\n    \n    for idx, word in enumerate(top_words):\n        print(str(idx+1) + \". \" + word)\n        print(\"\")\n    \n    # making list of the paper related to the keywords\n    \n    top_words.insert(0,keyword)\n    title_list = []\n    abstract_dict = {}\n    for word in top_words :\n        paper_result = bm25.search(word)\n        if word != keyword : \n            for i in range(len(paper_result)) :\n                try : \n                    if (paper_result[i].title() not in title_list) & (len(set(topic_dict[paper_result[i].title()]) \\\n                                                                           & set(topic_keyword)) >= 2) :   # ensuring the topic is similar\n                        title_list.append(paper_result[i].title())\n                except :\n                    abstract = paper_result[i].abstract()\n                    topic = list(df[df['abstract'] == abstract]['topic'])[0]\n                    if (paper_result[i].title() not in title_list) & (len(set(topic) \\\n                                                                           & set(topic_keyword)) >= 2) :   # ensuring the topic is similar\n                        title_list.append(paper_result[i].title())\n        else :\n            topic_keyword = []\n            for i in range(len(paper_result)) :\n                try : \n                    title = paper_result[i].title()\n                    topic = topic_dict[title]\n                    for j in topic :\n                        if j not in topic_keyword :\n                            topic_keyword.append(j)\n                    title_list.append(paper_result[i].title())\n                except :\n                    abstract = paper_result[i].abstract()\n                    topic = list(df[df['abstract'] == abstract]['topic'])[0]\n                    for j in topic :\n                        if j not in topic_keyword :\n                            topic_keyword.append(j)\n                    title_list.append(paper_result[i].title())\n                    \n    title_list = title_list[:30]\n                    \n    # updating list of paper for every keyword\n    \n    title_dict.update(dict({keyword : title_list}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>QA Step</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n                                  BertForQuestionAnswering, BertTokenizer)\nfrom pytorch_transformers.tokenization_bert import (BasicTokenizer,\n                                                    whitespace_tokenize)\n\nclass SquadExample(object):\n    \"\"\"\n    A single training/test example for the Squad dataset.\n    For examples without an answer, the start and end position are -1.\n    \"\"\"\n\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"qas_id: %s\" % (self.qas_id)\n        s += \", question_text: %s\" % (\n            self.question_text)\n        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n        if self.start_position:\n            s += \", start_position: %d\" % (self.start_position)\n        if self.end_position:\n            s += \", end_position: %d\" % (self.end_position)\n        return s\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 paragraph_len,\n                 start_position=None,\n                 end_position=None,):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.paragraph_len = paragraph_len\n        self.start_position = start_position\n        self.end_position = end_position\n\ndef input_to_squad_example(passage, question):\n    \"\"\"Convert input passage and question into a SquadExample.\"\"\"\n\n    def is_whitespace(c):\n        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n            return True\n        return False\n\n    paragraph_text = passage\n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n    for c in paragraph_text:\n        if is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                doc_tokens.append(c)\n            else:\n                doc_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(doc_tokens) - 1)\n\n    qas_id = 0\n    question_text = question\n    start_position = None\n    end_position = None\n    orig_answer_text = None\n\n    example = SquadExample(\n        qas_id=qas_id,\n        question_text=question_text,\n        doc_tokens=doc_tokens,\n        orig_answer_text=orig_answer_text,\n        start_position=start_position,\n        end_position=end_position)\n                \n    return example\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word 'bought' will have two scores from spans B and C. We only\n    # want to consider the score with \"maximum context\", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for 'bought' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\ndef squad_examples_to_features(example, tokenizer, max_seq_length,\n                                 doc_stride, max_query_length,cls_token_at_end=False,\n                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n                                 cls_token_segment_id=0, pad_token_segment_id=0,\n                                 mask_padding_with_zero=True):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    unique_id = 1000000000\n    example_index = 0\n    features = []\n    query_tokens = tokenizer.tokenize(example.question_text)\n    if len(query_tokens) > max_query_length:\n        query_tokens = query_tokens[0:max_query_length]\n\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    for (i, token) in enumerate(example.doc_tokens):\n        orig_to_tok_index.append(len(all_doc_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        \"DocSpan\", [\"start\", \"length\"])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n        length = len(all_doc_tokens) - start_offset\n        if length > max_tokens_for_doc:\n            length = max_tokens_for_doc\n        doc_spans.append(_DocSpan(start=start_offset, length=length))\n        if start_offset + length == len(all_doc_tokens):\n            break\n        start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n        tokens = []\n        token_to_orig_map = {}\n        token_is_max_context = {}\n        segment_ids = []\n\n        # CLS token at the beginning\n        if not cls_token_at_end:\n            tokens.append(cls_token)\n            segment_ids.append(cls_token_segment_id)\n\n        # Query\n        for token in query_tokens:\n            tokens.append(token)\n            segment_ids.append(sequence_a_segment_id)\n\n        # SEP token\n        tokens.append(sep_token)\n        segment_ids.append(sequence_a_segment_id)\n\n        # Paragraph\n        for i in range(doc_span.length):\n            split_token_index = doc_span.start + i\n            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                    split_token_index)\n            token_is_max_context[len(tokens)] = is_max_context\n            tokens.append(all_doc_tokens[split_token_index])\n            segment_ids.append(sequence_b_segment_id)\n        paragraph_len = doc_span.length\n\n        # SEP token\n        tokens.append(sep_token)\n        segment_ids.append(sequence_b_segment_id)\n\n        # CLS token at the end\n        if cls_token_at_end:\n            tokens.append(cls_token)\n            segment_ids.append(cls_token_segment_id)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_length:\n            input_ids.append(pad_token)\n            input_mask.append(0 if mask_padding_with_zero else 1)\n            segment_ids.append(pad_token_segment_id)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        start_position = None\n        end_position = None\n\n        features.append(\n            InputFeatures(\n                unique_id=unique_id,\n                example_index=example_index,\n                doc_span_index=doc_span_index,\n                tokens=tokens,\n                token_to_orig_map=token_to_orig_map,\n                token_is_max_context=token_is_max_context,\n                input_ids=input_ids,\n                input_mask=input_mask,\n                segment_ids=segment_ids,\n                paragraph_len=paragraph_len,\n                start_position=start_position,\n                end_position=end_position))\n        unique_id += 1\n\n    return features\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\ndef _get_best_indexes(logits, n_best_size):\n    \"\"\"Get the n-best logits from a list.\"\"\"\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\nRawResult = collections.namedtuple(\"RawResult\",[\"unique_id\", \"start_logits\", \"end_logits\"])\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don't want in\n    # our prediction.\n    #\n    # For example, let's say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith's\n    #\n    # We don't want to return `orig_text` because it contains the extra \"'s\".\n    #\n    # We don't want to return `pred_text` because it's already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is \"Steve Smith\".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == \" \":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = \"\".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text\n\ndef _compute_softmax(scores):\n    \"\"\"Compute softmax probability over raw logits.\"\"\"\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\ndef get_answer(example, features, all_results, n_best_size,\n                max_answer_length, do_lower_case):\n    example_index_to_features = collections.defaultdict(list)\n    for feature in features:\n        example_index_to_features[feature.example_index].append(feature)\n    \n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n    \n    _PrelimPrediction = collections.namedtuple( \"PrelimPrediction\",[\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n\n    example_index = 0\n    features = example_index_to_features[example_index]\n\n    prelim_predictions = []\n\n    for (feature_index, feature) in enumerate(features):\n        result = unique_id_to_result[feature.unique_id]\n        start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n        end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                # We could hypothetically create invalid predictions, e.g., predict\n                # that the start of the span is in the question. We throw out all\n                # invalid predictions.\n                if start_index >= len(feature.tokens):\n                    continue\n                if end_index >= len(feature.tokens):\n                    continue\n                if start_index not in feature.token_to_orig_map:\n                    continue\n                if end_index not in feature.token_to_orig_map:\n                    continue\n                if not feature.token_is_max_context.get(start_index, False):\n                    continue\n                if end_index < start_index:\n                    continue\n                length = end_index - start_index + 1\n                if length > max_answer_length:\n                    continue\n                prelim_predictions.append(\n                    _PrelimPrediction(\n                        feature_index=feature_index,\n                        start_index=start_index,\n                        end_index=end_index,\n                        start_logit=result.start_logits[start_index],\n                        end_logit=result.end_logits[end_index]))\n    prelim_predictions = sorted(prelim_predictions,key=lambda x: (x.start_logit + x.end_logit),reverse=True)\n    _NbestPrediction = collections.namedtuple(\"NbestPrediction\",\n                        [\"text\", \"start_logit\", \"end_logit\",\"start_index\",\"end_index\"])\n    seen_predictions = {}\n    nbest = []\n    for pred in prelim_predictions:\n        if len(nbest) >= n_best_size:\n            break\n        feature = features[pred.feature_index]\n        orig_doc_start = -1\n        orig_doc_end = -1\n        if pred.start_index > 0:  # this is a non-null prediction\n            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n            tok_text = \" \".join(tok_tokens)\n\n            # De-tokenize WordPieces that have been split off.\n            tok_text = tok_text.replace(\" ##\", \"\")\n            tok_text = tok_text.replace(\"##\", \"\")\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = \" \".join(tok_text.split())\n            orig_text = \" \".join(orig_tokens)\n\n            final_text = get_final_text(tok_text, orig_text,do_lower_case)\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n        else:\n            final_text = \"\"\n            seen_predictions[final_text] = True\n\n        nbest.append(\n            _NbestPrediction(\n                text=final_text,\n                start_logit=pred.start_logit,\n                end_logit=pred.end_logit,\n                start_index=orig_doc_start,\n                end_index=orig_doc_end))\n\n    if not nbest:\n        nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0,start_index=-1,\n                end_index=-1))\n\n    assert len(nbest) >= 1\n\n    total_scores = []\n    for entry in nbest:\n        total_scores.append(entry.start_logit + entry.end_logit)\n\n    probs = _compute_softmax(total_scores)\n    \n    answer = {\"answer\" : nbest[0].text,\n               \"start\" : nbest[0].start_index,\n               \"end\" : nbest[0].end_index,\n               \"confidence\" : probs[0],\n               \"document\" : example.doc_tokens\n             }\n    return answer\n\nRawResult = collections.namedtuple(\"RawResult\",\n                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n\ndef makeBERTSQuADPrediction(model, document, question):\n    input_ids = tokenizer.encode(question, document)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    num_seg_b = len(input_ids) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    assert len(segment_ids) == len(input_ids)\n    n_ids = len(segment_ids)\n    if n_ids < 512:\n        start_scores, end_scores = model(torch.tensor([input_ids]), \n                                 token_type_ids=torch.tensor([segment_ids]))\n    else:\n        #this cuts off the text if its more than 512 words so it fits in model space\n        #need run multiple inferences for longer text. add to the todo\n        start_scores, end_scores = model(torch.tensor([input_ids[:512]]), \n                                 token_type_ids=torch.tensor([segment_ids[:512]]))\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    answer = tokens[answer_start]\n    \n\n    for i in range(answer_start + 1, answer_end + 1):\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        else:\n            answer += ' ' + tokens[i]\n            \n    full_txt = ''\n    \n    for t in tokens:\n        if t[0:2] == '##':\n            full_txt += t[2:]\n        else:\n            full_txt += ' ' + t\n            \n    abs_returned = full_txt.split('[SEP] ')[1]\n            \n    ans={}\n    ans['answer'] = answer\n    #print(answer)\n    if answer.startswith('[CLS]') or answer.endswith('[SEP]'):\n        ans['confidence'] = -1.0\n    else:\n        confidence = torch.max(start_scores) + torch.max(end_scores)\n        confidence = np.log(confidence.item())\n        ans['confidence'] = confidence/(1.0+confidence)\n    ans['start'] = answer_start\n    ans['end'] = answer_end\n    ans['abstract_bert'] = abs_returned\n    return ans\n\ndef searchAbstracts(data, model, question):\n    abstractResults = {}\n    for i in tqdm(range(len(df_choosen))):  \n        abstract = data.iloc[i]['abstract']\n        emptyToken = -1\n        if abstract:\n            ans = makeBERTSQuADPrediction(model, abstract, question)\n            confidence = ans['confidence']\n            abstractResults[confidence]={}\n            abstractResults[confidence]['answer'] = ans['answer']\n            abstractResults[confidence]['start'] = ans['start']\n            abstractResults[confidence]['end'] = ans['end']\n            abstractResults[confidence]['idx'] = i\n        else:\n            abstractResults[emptyToken]={}\n            abstractResults[emptyToken]['answer'] = []\n            abstractResults[emptyToken]['start'] = []\n            abstractResults[emptyToken]['end'] = []\n            abstractResults[confidence]['idx'] = i\n            emptyToken -= 1\n    return abstractResults\n\nfrom IPython.core.display import display, HTML\n\ndef displayResults(df_choosen, answers, question):\n    display(HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:7px ; line-height: 1.5\"><b>Question</b>: '+question+'</div>')) \n    display(HTML('<div style=\"font-family: Times New Roman; font-size: 16px; padding-bottom:7px ;  line-height: 1.5\"><b>Answer</b>:</div>'))\n    \n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    \n    HTML_list_text = '<div style=\"font-family: Times New Roman; font-size: 14px; padding-bottom:7px ;  line-height:1.5\"> <ul style=\"list-style-type:disc;\">' \n    \n    list_id = []\n    for i,c in enumerate(confidence):\n        if (i < 8) & (c > 0):\n            if 'idx' not in  answers[c]:\n                continue\n            list_id.append(i)\n            bert_ans = answers[c]['answer']\n            HTML_list_text += '<li>'+bert_ans+'</li>'\n    HTML_list_text += '</ul> </div>'\n            \n    display(HTML(HTML_list_text))\n            \n    for i,c in enumerate(confidence):\n        if (i in list_id):\n            idx = answers[c]['idx']\n            title = df_choosen.iloc[idx]['title']\n            authors = df_choosen.iloc[idx]['authors']\n\n            display(HTML('<div style=\"font-family: Times New Roman; font-size: 14px; padding-bottom:7px ; line-height: 1.5\"><b>Author</b>: '+\n                         F'{authors}' + '<br>' + '<br>' + '<b>Title</b>: ' + '<br>' + F'{title}. ' + '</div>'))\n\n            full_abs = df_choosen.iloc[idx]['abstract']\n            bert_ans = answers[c]['answer']\n            display(HTML('<div style=\"font-family: Times New Roman; font-size: 13px; padding-bottom:7px ; line-height: 1.5\"><b>Answer</b>: '\n                         +\" <font color='red'>\"+bert_ans+\"</font> \"+'</div>'))\n            display(HTML('<div style=\"font-family: Times New Roman; font-size: 13px; padding-bottom:7px ; line-height: 1.5\"><b>Abstract</b>: '\n                         +full_abs+'</div>'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the best option of coordination and communication during pandemic strategy?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is better and optimum resource distribution strategies during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the infrastructure for pandemic preparation?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the optimum institutional collaboration to prepare for pandemics?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is effective government strategy during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the better strategy to optimize facemask distribution during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the suggestion to manage hospital utilization during outbreak?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the method to do npi assessment?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to optmize intervention effectiveness during outbreak?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the best execution of planning during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"Is isolation implementation effective to prevent outbreak?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How effective is school closure during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How effective is travel ban during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How significant is mass gathering effect during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is social distancing result in reducing transmission during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the lockdown effectivity in reducing transmission rate during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the emigration ban effectivity in reducing transmission rate during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \" How effective is testing of mass scale during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How effective is proper facemask wearing during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How effective is web based surveillance to reduce transmission during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the effectivity of self nursing education during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to do tracking of community source during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the role of digital technology during pandamic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is media effectivity on reporting during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to optimize campaign strategy during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the effect of influencing in social media during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to optimize policy during pandemic to reduce transmission?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to optimize public knowledge during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the optimum method to do spread controlling during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How to optimize the economic strategy during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How much cost during outbreak from cost analysis?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the impact of pandemic on housing price?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What is the effect of pandemic on household price?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"What causes citizen fail to comply the law during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the homeless compliance during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How is the effectivity of quarantine of city during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"How mathematical modelling helps during pandemic?\"\nfor i in keywords : \n    if i in query :\n        title_choosen = title_dict[i]\n        df_choosen = df[df['title'].isin(title_choosen)]\n        break\nanswers = searchAbstracts(df_choosen, model, query)\ndisplayResults(df_choosen, answers, query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some weaknesses of these workflow are :\n* There is a possibility to find better number of topic clustering\n* There also a possibility to find better clustering beside using the bigram lemmatized bigram\n* There is also a possibility to find better word embedding beside Word2Vec (maybe from BERT itself)\n* Sentence of the question must be adjusted to find better result of answer\n* Some threshold (like number of similar topic) must be investigate further to produce the best result\n* Takes time\n* Still need manual checking whether the answer is really answering the question or not"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}