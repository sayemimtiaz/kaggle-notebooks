{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Challenge START&GO Data Science 2020 \"Botando pra Quebrar\" \n# Sentiment Analysis in IMDB Reviews and Tweets with Natural Language Processing (NLP) \n\n\n\nHello! We are Andreis Purim and Eduarda Agostini, we're two brazilian students in currently doing a double-degree program at the Ã‰cole Centrale de Lille, in France. For our challenge in Data Science, we chose to make a mix of NLP algorithms in multiple intersting datasets.\n\nAll documentation in this notebook will be in english because we believe it may be intersting to a wider audience. I hope you like it."},{"metadata":{},"cell_type":"markdown","source":"# 1. IMDB\n\nOur first challenge will be to make a Machine Learning classifier for the 50k movie reviews IMDB Dataset. The dataset itself is very simple: the review and the sentiment (positive or negative).\n\n## 1.1. Visualizing our Data\n\nLet's start by plotting some graphs and of course, seeing how our data works."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport numpy\nimport pandas\n%matplotlib inline\n\nReviews = pandas.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nprint(Reviews.shape)\nprint(Reviews.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use the [spaCy library](https://spacy.io/models) to take a look at how our phrases are constructed. spaCy is a beautifully constructed library for NLP that has a pretrained statistical models in various languages. We could use the starter models to make a transfer learning, but for now let's just use the complete model to see how it fares.\n\nBy loading the English core pretrained models, we can use it to deconstruct the phrase and see every part of it (with explanations!). Let's choose our second review, in this case, I don't want to print all words (because it'd be too huge), so I made a zip with range(20), in case you want to observe all words, just make a for in Chosen_Sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport torch\nimport spacy\nimport nltk\nimport re\n\nfrom IPython.display import clear_output\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom spacy import displacy\n\nnltk.download('stopwords')\nstopwords_dict = Counter(stopwords.words(\"english\"))\nclear_output()\n\ndef clear_sentence(sentence: str) -> str:\n    sentence = sentence.replace('<br />', ' ')\n    sentence = sentence.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n    sentence = sentence.lower()\n    return sentence\n\ndef remove_stopwords(sentence: str) -> list:\n    return [word for word in sentence.split() if not word in stopwords_dict]\n\ndef tokenize(sentence:str) -> list:\n    sentence = sentence.replace('<br />', ' ')\n    sentence = sentence.lower()\n    return word_tokenize(sentence)\n\n# And let's clean our reviews\nprint(\"Cleaning sentences...\")\nReviews['review'] = Reviews['review'].apply(clear_sentence)\nprint(\"Removing stopwords...\")\nReviews['no_stopwords'] = Reviews['review'].apply(remove_stopwords)\nclear_output()\n\ndef render_spacy(chosen_sentence: str):\n    spaCy = spacy.load('en_core_web_sm')\n    spacy_sentence = spaCy(chosen_sentence)\n    print('-'*15,'Review text','-'*15)\n    for i,word in zip(range(20),spacy_sentence):\n        print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n    print('\\n','-'*15,'Entities','-'*15)\n    for entity in spacy_sentence.ents:\n        print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n\n    displacy.render(spacy_sentence, style='ent', jupyter=True)\n    displacy.render(spacy_sentence, style='dep', jupyter=True, options={'distance': 50})\n\n\nclear_output()\nrender_spacy(Reviews['review'][1])\ntorch.cuda.empty_cache()\nmemory_clear = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note how our data still needs some cleaning. For example, there are HTML tags (like ```<br/>```) which spaCy classifies as \"superfluous punctuation\". We'll clean the text before starting our models.\n\nAnother beautiful thing about spaCy is displaCy, displaCy is a visualizer which not only makes visualizing dependencies in NLP fun but also very helpful for us. "},{"metadata":{},"cell_type":"markdown","source":"Another powerful thing spaCy can do is to identify entities in the text, like organizations, people, nationalities, etc... you can use displaCy again to visualize the entities in the text."},{"metadata":{},"cell_type":"markdown","source":"Ok, now that we had enough fun with spaCy, let's start to visualize our dataset as a whole. The first step is to clean the reviews of ponctuation, tags and then put everything in lowercase. Our objective in this cell is to know the number of positive and negatives (spoiler: it should be 25000 and 25000) and see how the number of words is related to the sentiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as matplotlib\nimport seaborn\n\ndef EDA_function():\n    # Let's take a small look on how many posivies and negatives our reviews have\n    # for completeness sake, let's look at both sentences with stopwords and without them.\n    EDA = Reviews.copy()\n    EDA['#_no_stopwords'] = EDA['no_stopwords'].apply(len)\n    EDA['#_with_stopwords'] = EDA['review'].apply(str.split).apply(len)\n    \n    # Let's prepare our plot\n    matplotlib.figure(figsize=(8,8))\n\n    # Let's transform those columns in a dictionary that contains the lists of the numbers of words (with and without stopwords)\n    words_EDA = {sentiment: [] for sentiment in ['total','positive','negative']}\n    for sentiment in words_EDA:\n        words_EDA[sentiment].append(('Without',EDA[EDA['sentiment'] == sentiment][\"#_no_stopwords\"] if sentiment !='total' else EDA[\"#_no_stopwords\"]))\n        words_EDA[sentiment].append(('With',EDA[EDA['sentiment'] == sentiment][\"#_with_stopwords\"] if sentiment !='total' else EDA[\"#_with_stopwords\"]))\n        print(len(words_EDA[sentiment][1][1]),sentiment,'reviews:')\n        for choice in words_EDA[sentiment]:\n            print(' '*2,choice[0],'stopwords:\\trange', '[' + str(choice[1].min()),'-', str(choice[1].max()) + ']','\\tmean:',round(choice[1].mean()), '- standart:',round(choice[1].std()))\n        if sentiment != 'total':\n            seaborn.distplot(words_EDA[sentiment][1][1],kde=False, label=sentiment.capitalize())\n        \n    matplotlib.xlabel(\"Number of Words per Phrase\")\n    matplotlib.ylabel(\"Number of Phrases\")\n    matplotlib.legend()\n    matplotlib.show()\n\ndef EDA_common_words() -> pandas.DataFrame:\n    # Use counter to get the most common words\n    dataframes = [pandas.DataFrame(Counter([item for sublist in Reviews['no_stopwords'] for item in sublist]).most_common(20))]\n    for sentiment in ['negative','positive']:\n        dataframes.append(pandas.DataFrame(Counter([item for sublist in Reviews[Reviews['sentiment'] == sentiment]['no_stopwords'] for item in sublist]).most_common(20)))\n    dataframe = pandas.concat([dataframes[0],dataframes[1],dataframes[2]], axis=1)\n    dataframe.columns = ['Common Words','#C','Negative Words','#N','Positive Words','#P']\n    return dataframe\n    \nEDA_function()\nEDA_common_words().style.background_gradient(cmap='Greys', subset='#C').background_gradient(cmap='Reds', subset='#N').background_gradient(cmap='Greens', subset='#P')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, as you can probably see, just getting the most common words don't tell us a lot - even after we remove stop words, because considering only numerical values detract from the meaning of the text. That's why in negative reviews, for example, we have \"good\" appearing multiple times, (we don't see bad appearing in positive reviews, on the other hand). \n\nSo let's jump ahead to something that can help us extract relation between words: Vectorized words! The idea is that every word will have a unique vector in an N Dimensional space. Therefore, if two vectors are close in the space, they are related words, if they have completely opposite numbers, they are opposite words, and so on...\n\nHere's a demonstration of how these words look like."},{"metadata":{},"cell_type":"markdown","source":"Ok, so let's go ahead and use word2vec, the most famous Word Vectorizer\\Tokenizer and see how it looks like in a 2D graphic with the labels on. Let's also take a look at the w2vec similar_words finder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as plotpygo\nimport plotly.offline as plotplyoff\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\n\n# Clear memory from last cell \ntorch.cuda.empty_cache()\nmemory_clear = gc.collect()\n\ndef EDA_word2vec():\n    tokenized = Reviews['no_stopwords'][0:1000].tolist()\n\n    # Now let's use Word2Vec\n    w2v = word2vec.Word2Vec(tokenized, size=100, window=30, min_count=1, sample=1e-3, iter=50)\n\n    # This is a list of terms we want to find, we are going to search for the 7 most related words (and the pad it to 5, to remove any repetition)\n    terms = ['movie','bad','good','sucks','awesome','action','documentary','benedict']\n    related_words = {search_term: [search_term]+[item[0] for item in w2v.wv.most_similar([search_term], topn=7) if item[0] not in terms][:5] for search_term in terms}\n    for word in related_words:\n        print(word,\"is related to\",related_words[word][1:])\n\n    # Now, we make a list of our dictionary values and get the word2vec vectors of each one.\n    # And then we use TSNE to make them 2D or 3D\n    words = sum(related_words.values(), [])\n    word_vectors_ND = w2v.wv[words]\n    word_vectors_2D = TSNE(n_components=2, random_state=0, n_iter=500, perplexity=1).fit_transform(word_vectors_ND)\n    word_vectors_3D = TSNE(n_components=3, random_state=0, n_iter=500, method='barnes_hut', angle=0.5, perplexity=3).fit_transform(word_vectors_ND)\n\n    # Plot in 2D\n    matplotlib.figure(figsize=(14, 8))\n    seaborn.color_palette(\"tab10\")\n    seaborn.scatterplot(word_vectors_2D[:, 0], word_vectors_2D[:, 1], s=200)\n    for word, x, y in zip(words, word_vectors_2D[:, 0], word_vectors_2D[:, 1]):\n        matplotlib.annotate(word, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n    matplotlib.show()\n    \n    # Plot in 3D\n    data = [plotpygo.Scatter3d(x=word_vectors_3D[:,0],y=word_vectors_3D[:,1], z=word_vectors_3D[:,2], mode='markers')]\n    layout = dict(height=800, width=800, title='(this is a 3D graphic, take a look around with your mouse)')\n    plotplyoff.iplot(dict(data=data, layout=layout), filename='3DBubble')\n    \nEDA_word2vec()\ntorch.cuda.empty_cache()\nmemory_clear = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool right? You can probably see that when we vectorize our words, we have a much easier time finding words that are somewhat related to one another."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Choosing algorithms for our IMDB\n\nOk, now that we know how our data looks like, let's choose some Machine Learning algorithms to work with our data. The first thing we need to do is to reduce our dataset (because some of these algorithms can take quite a while), so we'll be using only the first 5000 reviews.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom matplotlib.lines import Line2D\nfrom xgboost import XGBClassifier\nimport time\n\ndef compare_ML():\n    # Let's get a slightly bigger part to test a few Machine Learning Algorithms.\n    # Makes two datasets, x and y, x will be the clear reviews and y will be the sentiment\n    x = Reviews['review'][0:5000].tolist()\n    y = Reviews['sentiment'][0:5000].tolist()\n    # Split the dataset in a 80%/20% fashion\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\n    # I'm making two dictionaries, one for models to transform our words in vectors and the other of models to work on these vectors\n    Vectorizer_Models = {\n        'Count': CountVectorizer(),\n        'Hash': HashingVectorizer(),\n        'Tfidf': TfidfVectorizer(ngram_range=(1, 2))\n    }\n\n    ML_Models = {\n        'LinearSVC': LinearSVC(),\n        'SVC': SVC(),\n        'NuSVC': NuSVC(),\n        'DecisionTree': DecisionTreeClassifier(),\n        'XGBClassifier': XGBClassifier(),\n        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=0),\n        'SGDC': SGDClassifier(),\n        'MultiNB': MultinomialNB(),    \n    }\n\n    # I'll make a new list for a future dataset to see the accuracy and time of each algorithm.\n    plotting_data = []\n    \n    for j,vector in enumerate(Vectorizer_Models):\n\n        # Let's first vectorize, because the vectorized words will be used in common by all MLs. Also, starts counting the time to vectorize.\n        time_vector_start = time.time()\n        X_train_vectorized = Vectorizer_Models[vector].fit_transform(X_train) \n        X_test_vectorized= Vectorizer_Models[vector].transform(X_test)\n        time_vector_end = time.time()\n\n        for i,ml in enumerate(ML_Models):\n\n            # Small detail: Multinomial Naive-Baise does not work with negative numbers, so we can just use him with Count\n            if (ml == 'MultiNB' and vector != 'Count') == False:\n               \n                # Ok, let's start the time and put our models to fit the data.\n                starting_time = time.time()\n                model = ML_Models[ml]\n                model.fit(X_train_vectorized, y_train)\n\n                # Predict the data and try to find the accuracy\n                y_predicted = model.predict(X_test_vectorized)\n                accuracy = accuracy_score(y_test, y_predicted)\n                ending_time = time.time()\n\n                # Now, get the times and append everything in our plotting data.\n                cut_time = round(time_vector_end - time_vector_start,2)\n                ml_time = round(ending_time - starting_time,2)\n                plotting_data.append([ml,vector,accuracy,ml_time,cut_time,cut_time+ml_time])\n\n\n    # Makes a pandas dataset for our data (for better visualization)\n    plot_times = pandas.DataFrame(plotting_data, columns=['ML','Vectorizer','Accuracy','ML_Time','Cut_time','Total_time'])\n\n    # Now, let's make a Seaborn scatterplot\n    seaborn.set(color_codes=True)\n    matplotlib.figure(figsize=(12, 8))\n    matplotlib.title(\"Best vectorization and Accuracy Algorithms (Reduced Dataset)\")\n\n    ax = seaborn.scatterplot(data=plot_times, x='Total_time', y='Accuracy', hue='ML', style='Vectorizer')\n    matplotlib.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    ax.set(xlabel=\"Time (s)\", ylabel=\"Accuracy\")\n    matplotlib.show()\n    return plot_times\n\ndef complete_ML_chosen():\n    # Do everything again\n    starting_time = time.time()\n    x = Reviews['review'].tolist()\n    y = Reviews['sentiment'].tolist()\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n    vector = TfidfVectorizer(ngram_range=(1, 2))\n    X_train_vectorized = vector.fit_transform(X_train) \n    X_test_vectorized= vector.transform(X_test)\n    model = LinearSVC()\n    model.fit(X_train_vectorized, y_train)\n    y_predicted = model.predict(X_test_vectorized)\n    accuracy = accuracy_score(y_test, y_predicted)\n    ending_time = time.time()\n    print('COMPLETE DATASET WITH LinearSVC\\nAccuracy:',\"{:.2f}\".format(accuracy*100),\"in\",\"{:.2f}s\".format(ending_time-starting_time))\n    print(confusion_matrix(y_test, y_predicted))\n\ncomplete_ML_chosen()\ncompare_ML()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you have probably seen in the graph, the best algorithm is probably LinearSVC using Tfidf or Hash, with a small difference in accuracy and time, while things like Hash with XGB or RandomForest probably fared pretty bad in time.\n\nI won't explain in great detail why (if you google you'll probably find better answers) but this is because LinearSVC are Support Vector Machine, that is, machine learning algorithms made to use vectors as inputs, while RandomForest, while a very good algorithm, just can't handle vectors with hundreds of dimensions in a good time. In this case, you can see SGDClassifier rates a little higher than LinearSVC because SGD (Stochastic Gradient Descent) is a good approach of fitting linear classifiers in a manner similar to SVM.\n\nIn fact, our SGDClassifier is LinearSVM with some better training, as the docs in scikit state: \"Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. Often, an instance of SGDClassifier or SGDRegressor will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique.\"\n\nAnd for the vectorizes, you can see Count is way faster, Hash is a mix of fast and accurate, and Tfidf is accurate(r). \n\nBut, when we scale the data back to its 50.000 original size, you might notice LinearSVC might outscore SDGC."},{"metadata":{},"cell_type":"markdown","source":"So, let's stick to LinearSVC a little more and do one final thing: fine-tuning. Scikit comes with a nice tool called GridSearchCV that allows us to fine tune our model a little further.\n\nIdeas:\n```python\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n```\nand fine tune with\n```python\nfrom sklearn.model_selection import GridSearchCV\nGridSearchCV()\n```\n\nWe are not going to do it now because of time restrictions."},{"metadata":{},"cell_type":"markdown","source":"# 1.3 Deep Learning\n\nDeep learning sometimes is overkill, but hey, maybe we do want to achieve that 99% accuracy, right? So let's explore a little the Deep Learning models available in Keras. Let's start using the X and Y already separated before"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense , Input , LSTM , Embedding, Dropout, Conv1D, MaxPooling1D, Activation, GRU, Flatten\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.layers import Dense, Flatten, Convolution1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Model, Sequential\nfrom numpy import asarray\nfrom numpy import array\nfrom numpy import zeros\nimport keras\n \ndef compare_DL():\n    # So, for our IMDB deep learning, we need to tokenize the words in a different manner from our ML.\n    to_binary = lambda phrase: 1 if phrase=='positive' else 0\n    x = Reviews['review'].tolist()\n    y_binary = numpy.array(Reviews['sentiment'].apply(to_binary))\n    tokenizer = Tokenizer(num_words=6000)\n    tokenizer.fit_on_texts(x)\n    x_tokenized = tokenizer.texts_to_sequences(x)\n\n    # Now, in average, our reviews have 128 words, so let's pad the maximum size to 130. We can make it longer but it will also make it slower \n    x_padded = pad_sequences(x_tokenized, maxlen=130)\n\n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(x_padded, y_binary, test_size=0.2, random_state=0)\n\n    # Make our model\n    def Model1():\n        name = 'LSTM'\n        model = Sequential()\n        model.add(Embedding(6000, 128))\n        model.add(Bidirectional(LSTM(32, return_sequences = True)))\n        model.add(GlobalMaxPool1D())\n        model.add(Dense(20, activation=\"relu\"))\n        model.add(Dropout(0.05))\n        model.add(Dense(1, activation=\"sigmoid\"))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return (name,model)\n    def Model2():\n        name = 'CNN (ver. 1)'\n        model = Sequential()\n        model.add(Embedding(6000, 15, input_length=130))\n        model.add(Dropout(0.50))\n        model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n        model.add(Dropout(0.50))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dropout(0.50))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return (name,model)\n    def Model3():\n        name = 'CNN (ver. 2)'\n        model = Sequential()\n        model.add(Embedding(6000, 32, input_length=130))\n        model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(250, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        return (name,model)\n    def Model4():\n        name = 'LSTM with GloVe'\n        embeddings_dictionary = dict()\n        with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', encoding=\"utf8\") as glove_file:\n            for line in glove_file:\n                records = line.split()\n                word = records[0]\n                vector_dimensions = asarray(records[1:], dtype='float32')\n                embeddings_dictionary[word] = vector_dimensions\n        vocab_size = len(tokenizer.word_index) + 1\n        embedding_matrix = zeros((vocab_size, 100))\n        for word, index in tokenizer.word_index.items():\n            embedding_vector = embeddings_dictionary.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[index] = embedding_vector\n        model = Sequential()\n        embedding_layer = Embedding(vocab_size, 100,  weights=[embedding_matrix], input_length=130 , trainable=False)\n        model.add(embedding_layer)\n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return (name,model)\n    def Model5():\n        name = 'Multi-Layer Perceptron'\n        model = Sequential()\n        model.add(Embedding(6000, 32, input_length=130))\n        model.add(Flatten())\n        model.add(Dense(250, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        return (name,model)\n\n    # I'm not the biggest fan of plotting Epoch data (specially considering we are using just 3 epochs), but here's a function if you want to do it anyway.\n    # I'm not calling it because I added an EarlyStopping, so there's not much sense to it.\n    def plot_scores(history,name) :\n        matplotlib.plot(history.history['accuracy'])\n        matplotlib.plot(history.history['val_accuracy'])\n        matplotlib.title(name + ' accuracy')\n        matplotlib.ylabel('accuracy')\n        matplotlib.xlabel('epoch')\n        matplotlib.legend(['train','test'], loc = 'upper left')\n        return matplotlib.show()\n\n    Models_List = [Model1(),Model2(),Model3(),Model4(),Model5()]\n\n    results = []\n    for name,model in Models_List:\n        starting_time = time.time()\n        es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, verbose=False, callbacks=[es_callback])\n        scores = model.evaluate(X_test, y_test, verbose=0)\n        ending_time = time.time()\n        #plot_scores(history,name)\n        results.append(name + \": %.2f%%\" % (scores[1]*100) + \" in {:.2f}s\".format(ending_time-starting_time))\n\n    for i in results:\n        print(i)\ncompare_DL()\ntorch.cuda.empty_cache()\nmemory_clear = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT NOTE:** Ok, so this is the end of the bulk of our study of the IMDB dataset - which was our primary objective - but we also wanted to implement BERT and study the Australian election tweets. Since BERT takes a lot of memory, he will be in a separate notebook, that can be acessed here. The rest will be here:"},{"metadata":{},"cell_type":"markdown","source":"# 2. Let's start looking at tweets!\n\nAs promised, our final objective in this challenge is to learn enough about sentiment analysis to read tweets about the australian election. Before we do that, let's play with some other datasets to see how far our models are faring.\n\n# 2.1 Airline Tweets\n\nOur first dataset will be the Airline Tweets Sentiment. It is a smaller dataset and it gives us a precious insight in Twitter Analysis. The result is probably way lower than you expected. Why is that? The main reasons are:\n\n- Tweets are a lot smaller than reviews.\n- Reviews tend to be clear and informative (even though there might be sarcasm), tweets do not necessarily are accessible, clear or informative - in fact, most are not.\n- Tweets are charged with sarcasm and abbreviations.\n- The dataset we are using is smaller, as well.\n\nLet's see how ML and Deep Learning compare"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.layers import GlobalMaxPooling1D \nfrom keras.layers import BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\n\ndef Airline_Analysis():\n    # Load the data and take a look at how tweet datasets usually look like\n    Airlines_Total = pandas.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\n\n    # Now, we won't be using any other data other than the text and the sentiment. We could use location or reason for a in-depth analysis but we'll reserve that to the Australian Tweets.\n    Airlines = Airlines_Total[['airline_sentiment','text']]\n    print('Dataset shape',Airlines_Total.shape)\n\n    # Use our clear_sentence function made at the very beggining\n    x = Airlines['text'].apply(clear_sentence).tolist()\n    y = Airlines['airline_sentiment'].tolist()\n\n    # Let's use the SVC model we used before.\n    starting_time = time.time()   \n    vector = TfidfVectorizer(ngram_range=(1, 2))\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n    X_training = vector.fit_transform(X_train) \n    X_testing = vector.transform(X_test)\n    model = LinearSVC()\n    model.fit(X_training, y_train)\n    y_prediction = model.predict(X_testing)\n    accuracy = accuracy_score(y_test, y_prediction)\n    ending_time = time.time()\n    print('LinearSVC:',\"{:.2f}\".format(accuracy*100) + \" in {:.2f}s\".format(ending_time-starting_time))\n\n\n    # And the CNN (ver. 3) Model \n    starting_time = time.time() \n    tokenizer = Tokenizer(num_words=6000)\n    tokenizer.fit_on_texts(x)\n    x_tokenized = tokenizer.texts_to_sequences(x)\n    x_padded = pad_sequences(x_tokenized, maxlen=130)\n    label_encoder = LabelEncoder()\n    y_binary = label_encoder.fit_transform(y)\n    y_binary = to_categorical(y_binary)\n    model = Sequential()\n    model.add(Embedding(6000, 100, input_length=130))\n    model.add(Conv1D(1024, 3, padding='valid', activation='relu', strides=1))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(2048, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    y_binary = numpy.array(y_binary)\n    X_train, X_test, y_train, y_test = train_test_split(x_padded, y_binary, test_size=0.2, random_state=0)\n    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128, verbose=False)\n    scores = model.evaluate(X_test, y_test, verbose=0)\n    ending_time = time.time()\n    print(\"CNN (ver. 3): %.2f%%\" % (scores[1]*100) + \" in {:.2f}s\".format(ending_time-starting_time))\n    # I'm adding the head after because the print obscures it.\n    return Airlines.head()\n\nAirline_Analysis()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. The Final Showdown: Let's try to predict the Australian Elections"},{"metadata":{},"cell_type":"markdown","source":"By this point, we now know how to make some pretty good models to classify sentiments. So let's try to predict something very real: elections. In this case, let's try to use the Australia 2019 Election Tweets and see how's getting love or hate, and then compare it to the real results of the elections.\n\nThis dataset has 183.379 tweets, 146.485 of those have locations filled, in the end, we will have 111.137 tweets in Australia, so we can use these tweets to map our analysis in Australia!\n\nThe first step is to train our LinearSVC with the sentiment140 tweet database. It is a  database with 1,600,000 tweets (0 = negative, 2 = neutral, 4 = positive)."},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\nmemory_clear = gc.collect()\n\ndef train_our_model_in_tweets():\n    # Load the data and take a look at how tweet datasets usually look like\n    Sentiments = pandas.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n\n    # Now, we won't be using any other data other than the text and the sentiment. \n    Sentiments = Sentiments[['target','text']]\n\n    # Make the sentiments strings\n    sentiment_value = {0: \"negative\", 2: \"neutral\", 4: \"positive\"}\n    decode = lambda label: sentiment_value[int(label)]\n    x = Sentiments['text'].apply(clear_sentence).tolist()\n    y = Sentiments['target'].apply(decode).tolist()\n\n    # Let's use the SVC model we used before.\n    starting_time = time.time()   \n    vector = TfidfVectorizer(ngram_range=(1, 2))\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n    X_training = vector.fit_transform(X_train) \n    X_testing = vector.transform(X_test)\n    model = LinearSVC()\n    model.fit(X_training, y_train)\n    y_prediction = model.predict(X_testing)\n    accuracy = accuracy_score(y_test, y_prediction)\n    ending_time = time.time()\n    print('Trained our model in',len(Sentiments.index),'tweets')\n    print('Accuracy:',\"{:.2f}\".format(accuracy*100) + \" in {:.2f}s\".format(ending_time-starting_time))\n    return model\n\ntrained_model = train_our_model_in_tweets()\ntorch.cuda.empty_cache()\nmemory_clear = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we fitted our model to understand tweets, let's take a look at the dataset of australian elections"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.basemap import Basemap\n\ndef Predict_Australia(model):\n    # Get the dataset, take a look\n    Australia_Tweets = pandas.read_csv('../input/australian-election-2019-tweets/auspol2019.csv')\n\n    # There's an aditional CSV with the geolocation, this is nice.\n    Australia_geocode = pandas.read_csv('../input/australian-election-2019-tweets/location_geocode.csv')\n    Australia_geocode.columns = ['user_location','lat','long']\n    Australia_Tweets = Australia_Tweets[['full_text','user_location']]\n    Australia_Tweets = pandas.merge(Australia_Tweets, Australia_geocode)\n    del Australia_geocode\n\n    # A function to get only the data inside australia (there are many tweets from abroad)\n    def get_region(data, bot_lat, top_lat, left_lon, right_lon):\n        top = data.lat <= top_lat\n        bot = data.lat >= bot_lat\n        left = data.long >= left_lon\n        right = data.long <= right_lon\n        index = top&bot&left&right \n        return data[index]\n\n    Australia_Tweets = get_region(Australia_Tweets,-44,-10,109,156)\n    Australia_Tweets = Australia_Tweets.drop(['user_location'],axis=1)\n\n    Australia_Tweets['full_text'] = Australia_Tweets['full_text'].apply(clear_sentence)\n    sentiment = pandas.DataFrame(model.predict(vector.transform(Australia_Tweets['full_text'].tolist())),columns=['sentiment'])\n    Australia_Tweets = pandas.concat([Australia_Tweets, sentiment], axis=1).dropna()\n    \n    Australia_map = Basemap(llcrnrlat=-44,urcrnrlat=-10,llcrnrlon=109,urcrnrlon=156)\n    matplotlib.figure(figsize=(12,10))\n    Australia_map.bluemarble(alpha=0.9)\n    \n    labor_positive = Australia_Tweets[(Australia_Tweets['full_text'].str.contains('labor')) & (Australia_Tweets['sentiment'] == 'positive')]\n    liberal_positive = Australia_Tweets[(Australia_Tweets['full_text'].str.contains('liberal')) & (Australia_Tweets['sentiment'] == 'positive')]\n    \n    seaborn.scatterplot(x='long', y='lat', data=liberal_positive, alpha=1, s=200, label='Support for Liberals')\n    seaborn.scatterplot(x='long', y='lat', data=labor_positive, alpha=0.07, s=200, label='Support for Labor')\n    \n    matplotlib.gca().get_legend().legendHandles[1].set_alpha(1)\n    matplotlib.title(\"Tweets supporting political parties in Australia, 2019\")\n    matplotlib.show()\n\nPredict_Australia(trained_model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}