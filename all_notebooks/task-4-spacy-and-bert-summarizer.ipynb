{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **TASK - 4 Spacy and BERT summarizer**\n\n# this is version 4.0 (at least) of tesitng, which will now be combined with other knowledged learned to enhance performance for round 2.\n\n![](https://sportslogohistory.com/wp-content/uploads/2018/09/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**Executive Summary:** Unsupervised scientific literature understanding system that accepts natural language questions and returns specific answers from the CORD19 scientific paper corpus. The answers are wholly generated by the system from the publicatons cited below the answer.  There is also a link with the question pre-loaded to the CORD19 web-based corpus (QA) search.\n\n**PROBLEM:** When a new virus is discovered and causes a pandemic, it is important for scientists to get information coming from all scientific sources that may help them combat the pandemic.  The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this mass of data.\n\n**SOLUTION:** Unsupervised scientific literature understanding system that accepts natural language quesitons (with a focusing keyword) and returns specific answers from the CORD19 scientific paper corpus.\n\n**APPROACH:**\n- meta.csv is loaded - system currently only utilizes the abstracts\n- the natural language questions for the task are contained in a list\n- there is a list of focusing keywords to focus on documents that specifically relate to the topic\n- the natural language questions have the stop words removed for passing to spacy for sentence comparison\n- the abtracts are parsed at sentence level and compared to the question\n- the most relevant sentences are returned in a dataframe and sent to BERT summarizer\n- https://pypi.org/project/bert-extractive-summarizer/\n- The question,summary answers and table of relevant scientific papers are returned in HTML format.\n\n**Pros:** The system uses quesitons from the task with some very slight alterations and returns very responsive and summarized answers to specific questions.  There is a focusing keyword used simply to reduce the size fo the dataframe to search through at sentence level.\n\n**Cons:** The system currently only uses the abstracts of the papers so it may not get the most relevant text for crafting responses. Next steps will be combinming versions of different methods to provide even better results.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"import spacy\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnlp = spacy.load('en_core_web_lg')\nimport numpy as np\nimport pandas as pd\n!pip install bert-extractive-summarizer\nfrom summarizer import Summarizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint (df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\n\ndef remove_stopwords(text,stopwords):\n    text = \"\".join(c for c in text if c not in ('!','.',',','?','(',')','-'))\n    text_tokens = word_tokenize(text)\n    #remove stopwords\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    str1=''\n    str1=' '.join(word for word in tokens_without_sw)\n    return str1\n\ndef score_sentence(search,sentence):\n        main_doc=nlp(sentence)\n        search_doc=nlp(search)\n        sent_score=main_doc.similarity(search_doc)\n        return sent_score\n\ndef process_question(df,search,focus):\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"title\",\"excerpt\",\"rel_score\"])\n    df1 = df[df['abstract'].str.contains(focus)]\n    search=remove_stopwords(search,stopwords)\n    for index, row in df1.iterrows():\n        sentences = row['abstract'].split('. ')\n        pub_sentence=''\n        hi_score=0\n        for sentence in sentences:\n            if len(sentence)>75:\n                rel_score=score_sentence(search,sentence)\n                if rel_score>.82:\n                    sentence=sentence.capitalize()\n                    if sentence[len(sentence)-1]!='.':\n                        sentence=sentence+'.'\n                    pub_sentence=pub_sentence+' '+sentence\n                    if rel_score>hi_score:\n                        hi_score=rel_score\n        if pub_sentence!='':\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            score=hi_score\n            linka='https://doi.org/'+link\n            linkb=title\n            final_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,linkb)\n            #author_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,authors[0]+' et al.')\n            #sentence=pub_sentence+' '+author_link\n            sentence=pub_sentence\n            #sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'</p>'\n            to_append = [row['publish_time'],final_link,sentence,score]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    df_table=df_table.sort_values(by=['rel_score'], ascending=False)\n    return df_table\n\ndef prepare_summary_answer(text,model):\n    #model = pipeline(task=\"summarization\")\n    return model(text)\n\n###### MAIN PROGRAM ######\nmodel = Summarizer()\n# questions\nsearch=[\n'What is the effectiveness of drugs being developed and tried to treat COVID-19 patients?',\n'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen clarithromycin, and minocyclinethat that may exert effects on viral replication',\n'How are potential complications of Antibody-Dependent Enhancement ADE in vaccine recipients being researched?',\n'Exploration of use of best animal models and their predictive value for a human vaccine',\n'Capabilities to discover a therapeutic not vaccine for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up and identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need',\n'What research and work is being done to develop a universal vaccine for coronavirus',\n'What work and research has been done to develop animal models and standardize challenge studies',\n'What work and research has been done to develop prophylaxis clinical studies and prioritize in healthcare workers',\n'Approaches to evaluate risk for enhanced disease after vaccination',\n'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics'\n]\n# main focus keywords\nfocus=['drugs','drugs','antibodies','animal model','therapeutic','models','vaccine','model','drugs','vaccine','animal']\n\nz=0\n\nfor question in search:\n    # process with spacy model and return df\n    df_table=process_question(df,question,focus[z])\n    \n    #use only the top 20 papers excerpts for summarizaiton\n    df_answers=df_table.head(20)\n    \n    text=''\n    \n    #loop through df to assemble sentences\n    for index, row in df_answers.iterrows():\n        text=text+' '+row['excerpt']\n    \n    display(HTML('<h2>'+question+'</h2>'))\n    \n    #summarize questions\n    summary_answer=prepare_summary_answer(text,model)\n    \n    #summary_answer=summary_answer[0]['summary_text']\n    display(HTML('<h4> Summarized Answer: </h4><i>'+summary_answer+'</i>'))\n    display(HTML('<h5>results limited to 5 for ease of scanning</h5>'))\n    \n    #limit the size of the df for the html table\n    df_table=df_table.head(5)\n    \n    #convert df_table to html and display\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    display(df_table)\n    \n    z=z+1\n\nprint ('done') ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}