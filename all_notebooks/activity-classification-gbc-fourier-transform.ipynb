{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook as a short experiment with fourier transformation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, plot_confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier as gbc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # I like big figures!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the dataset as recommended in the [github repository's](https://github.com/mmalekzadeh/motion-sense) README:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ds_infos():\n    \"\"\"\n    Read the file includes data subject information.\n    \n    Data Columns:\n    0: code [1-24]\n    1: weight [kg]\n    2: height [cm]\n    3: age [years]\n    4: gender [0:Female, 1:Male]\n    \n    Returns:\n        A pandas DataFrame that contains inforamtion about data subjects' attributes \n    \"\"\" \n\n    dss = pd.read_csv(\"data_subjects_info.csv\")\n    print(\"[INFO] -- Data subjects' information is imported.\")\n    \n    return dss\n\ndef set_data_types(data_types=[\"userAcceleration\"]):\n    \"\"\"\n    Select the sensors and the mode to shape the final dataset.\n    \n    Args:\n        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n\n    Returns:\n        It returns a list of columns to use for creating time-series from files.\n    \"\"\"\n    dt_list = []\n    for t in data_types:\n        if t != \"attitude\":\n            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n        else:\n            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n\n    return dt_list\n\n\ndef creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n    \"\"\"\n    Args:\n        dt_list: A list of columns that shows the type of data we want.\n        act_labels: list of activites\n        trial_codes: list of trials\n        mode: It can be \"raw\" which means you want raw data\n        for every dimention of each data type,\n        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n\n    Returns:\n        It returns a time-series of sensor data.\n    \n    \"\"\"\n    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n\n    if labeled:\n        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n    else:\n        dataset = np.zeros((0,num_data_cols))\n        \n    ds_list = get_ds_infos()\n    \n    print(\"[INFO] -- Creating Time-Series\")\n    for sub_id in ds_list[\"code\"]:\n        for act_id, act in enumerate(act_labels):\n            for trial in trial_codes[act_id]:\n                fname = 'A_DeviceMotion_data/A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n                raw_data = pd.read_csv(fname)\n                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n                vals = np.zeros((len(raw_data), num_data_cols))\n                for x_id, axes in enumerate(dt_list):\n                    if mode == \"mag\":\n                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n                    else:\n                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n                    vals = vals[:,:num_data_cols]\n                if labeled:\n                    lbls = np.array([[act_id,\n                            sub_id-1,\n                            ds_list[\"weight\"][sub_id-1],\n                            ds_list[\"height\"][sub_id-1],\n                            ds_list[\"age\"][sub_id-1],\n                            ds_list[\"gender\"][sub_id-1],\n                            trial          \n                           ]]*len(raw_data))\n                    vals = np.concatenate((vals, lbls), axis=1)\n                dataset = np.append(dataset,vals, axis=0)\n    cols = []\n    for axes in dt_list:\n        if mode == \"raw\":\n            cols += axes\n        else:\n            cols += [str(axes[0][:-2])]\n            \n    if labeled:\n        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n    \n    dataset = pd.DataFrame(data=dataset, columns=cols)\n    return dataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\nTRIAL_CODES = {\n    ACT_LABELS[0]:[1,2,11],\n    ACT_LABELS[1]:[3,4,12],\n    ACT_LABELS[2]:[7,8,15],\n    ACT_LABELS[3]:[9,16],\n    ACT_LABELS[4]:[6,14],\n    ACT_LABELS[5]:[5,13]\n}\n\n## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\nsdt = [\"attitude\", \"userAcceleration\"]\nprint(\"[INFO] -- Selected sensor data types: \"+str(sdt))    \nact_labels = ACT_LABELS [0:6]\nprint(\"[INFO] -- Selected activites: \"+str(act_labels))    \ntrial_codes = [TRIAL_CODES[act] for act in act_labels]\ndt_list = set_data_types(sdt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/motionsense-dataset/')\ndataset = creat_time_series(dt_list, act_labels, trial_codes, mode=\"raw\", labeled=True)\nprint(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create labels so we don't get confused."},{"metadata":{"trusted":true},"cell_type":"code","source":"act_dict = {0: 'dws',\n            1: 'ups',\n            2: 'wlk',\n            3: 'jog',\n            4: 'std',\n            5: 'sit'}\ndataset['label'] =  dataset.act.apply(lambda act: act_dict[act])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## General statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like ups, jog & dws are underrepresented but I think we have enough data to get a decent prediction on all labels."},{"metadata":{},"cell_type":"markdown","source":"## Let's take a deeper look at our data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = ['attitude.roll', 'attitude.pitch', 'attitude.yaw', 'userAcceleration.x', 'userAcceleration.y', \n            'userAcceleration.z']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To get a better feel for the data we'll take a deeper look and try to guess which metrics will be helpful to us."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def plot_data(start_position, number_of_frames, metrics=metrics, fourier=False):\n    print(f\"Looking at {metrics} for {number_of_frames} from {start_position}\")\n    for label in dataset.label.unique():\n        mini_df = dataset[dataset['label']==label].iloc[start_position:start_position+number_of_frames].reset_index()\n        for metric in metrics:\n            mini_df[metric].plot(title=label, legend=True)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I ran a bunch of these at random points just to see what we are dealing with."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot_data(np.random.randint(1,130000), 300)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot_data(np.random.randint(1,130000), 300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some quick conclusions:\n - 300 frames should be enough to get a frequency and amplitude.\n - Acceleration data is noisy!\n - As we would expect, sitting & standing are differet because of their low acceleration amplitude and can be told apart based on pitch.\n - As expected, jogging would be easy to differentiate using frequency."},{"metadata":{},"cell_type":"markdown","source":"### Denoising acceleration data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"acceleration_metrics = [metric for metric in metrics if metric.startswith('userAcceler')]\nacceleration_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for metric in acceleration_metrics:\n    dataset[f'{metric}_ra'] = dataset[metric].rolling(20).median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_acc_metrics = [metric for metric in dataset.columns if metric.endswith('_ra')]\navg_acc_metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Egineering\n### Each metric will get 5 features:\n - ### median\n - ### std\n - ### amplitude\n - ### frequency\n - ### phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset.fillna(0, axis=0) # the rolling window made a bunch of nans and it makes our fft unhappy.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The next block uses loops inside a dataframe which, generally, is a big nono as it is very inefficicent. <br> I chose to not refactor it into something faster (using groupby for instance) because I think it makes things much clearer and more readable and that is our priority in this pedagogical notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = ['userAcceleration.x_ra', 'userAcceleration.y_ra', 'userAcceleration.z_ra', \n           'attitude.roll', 'attitude.pitch', 'attitude.yaw']\n\ndef build_features(data=df, number_of_frames=300, metrics=metrics):\n    instances = pd.DataFrame() # this is where our features and labels will end up.\n    instance = 0\n    data.fillna(method='bfill')\n    for label in data.label.unique():\n        print(f\"Building features for {label}...\")\n        start_position=0\n        label_df = data[data['label']==label]\n        while len(label_df) > start_position+number_of_frames:\n            for metric in metrics:\n                instance_df = label_df.iloc[start_position:start_position+number_of_frames].reset_index()\n                instances.loc[instance, 'label'] = label\n                instances.loc[instance, f'median_{metric}'] = instance_df[metric].median()\n                instances.loc[instance, f'std_{metric}'] = instance_df[metric].std()\n                fourier = np.fft.rfft(instance_df[metric])[1:]\n                amplitude = max(np.abs(fourier))\n                frequency = np.where(np.abs(fourier)==amplitude)[0][0]\n                instances.loc[instance, f'amplitude_{metric}'] = amplitude\n                instances.loc[instance, f'frequency_{metric}'] = frequency\n                instances.loc[instance, f'phase_{metric}'] = np.angle(fourier)[frequency]\n            instance = instance + 1\n            start_position = start_position + number_of_frames\n    return instances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instances = build_features()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity checks:"},{"metadata":{"trusted":true},"cell_type":"code","source":"instances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(instances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instances.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We expect median pitch to be a way of differentiating sitting:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for label in instances.label.unique():\n    for feature in ['median_attitude.pitch']:\n        instances[instances.label==label][feature].hist(alpha=0.3, label=label, bins=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Yay!"},{"metadata":{},"cell_type":"markdown","source":"#### We also expect frequency to be a way of setting the joggers apart:"},{"metadata":{"trusted":true},"cell_type":"code","source":"frequencies = [feature for feature in instances.columns if feature.startswith('frequency')]\nfrequencies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instances['med_freq'] = instances[frequencies].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in instances.label.unique():\n    instances[instances.label==label]['med_freq'].hist(alpha=0.3, label=label, bins=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tada!"},{"metadata":{},"cell_type":"markdown","source":"## Let's just shove it in a decision tree and see what happens:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(model, feature_names):\n    feature_importances = model.feature_importances_\n    idxSorted = np.argsort(feature_importances)[-10:]\n    barPos = np.arange(idxSorted.shape[0]) + .5\n    plt.barh(barPos, feature_importances[idxSorted], align='center')\n    plt.yticks(barPos, feature_names[idxSorted])\n    plt.xlabel('Feature Importance')\n    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = instances.label\nX = instances.drop('label', axis=1)\nkf = KFold(n_splits=5, shuffle=True)\ngbc_model = gbc()\nMMC = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    gbc_model.fit(X_train, y_train)\n    y_pred = gbc_model.predict(X_test)\n    print(f\"MMC: {matthews_corrcoef(y_test, y_pred):.3f}\")\n    MMC.append(matthews_corrcoef(y_test, y_pred))\nprint(f\"Mean MMC: {np.mean(MMC):.3f}\")\nprint(f\"Std of MMCs: {np.std(MMC):.4f}\")\nprint(\"These are the plots of the last test so we could get an idea of what it looks like:\")\nplot_confusion_matrix(gbc_model, X=X_test, y_true=y_test, labels=gbc_model.classes_, cmap='Blues')\nplt.show()\nplot_feature_importance(gbc_model, X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bam!\nResults are pretty good, very stable and comparable with different notebooks here using DL techniques."},{"metadata":{},"cell_type":"markdown","source":"### I spent quite a bit of time debugging that fourier function up there.. so I wonder how much did it help for the overall score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_fft_features = [feature for feature in instances.columns  if feature.startswith('median') or feature.startswith('std')]\nno_fft_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = instances.label\nfeatures = no_fft_features\nX = instances[features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\ngbc_model = gbc()\ngbc_model.fit(X_train, y_train)\ny_pred = gbc_model.predict(X_test)\nprint(f\"MMC: {matthews_corrcoef(y_test, y_pred)}\")\nplot_confusion_matrix(gbc_model, X=X_test, y_true=y_test, labels=gbc_model.classes_, cmap='Blues')\nplt.show()\nplot_feature_importance(gbc_model, X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the Fourier transform made a difference but even without it the algorithm works surprisingly well..."},{"metadata":{},"cell_type":"markdown","source":"## What can be done to improve predictions:\nLots! <br>\n- Hyperparameter optimization - I just took the default values in the first tree classifier I could think of...\n- Different prediction model - Another tree or a different classifier.\n- More features - for instance - Phase!\n- More preprocessing so a better moving avg or aligning the data somehow.\n- Cut the data into smaller pieces (less frames per instance) might improve prediction but, we would need to think hard about how to measure the improvement."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}