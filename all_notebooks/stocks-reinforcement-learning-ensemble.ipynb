{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <img src=\"https://storage.googleapis.com/kaggle-datasets-images/1248254/2082083/f390143a751110b468bb441ad038e048/dataset-cover.jpeg\"/>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/trading/\")\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom env.EnvMultipleStock_train import StockEnvTrain\nfrom env.EnvMultipleStock_validation import StockEnvValidation\nfrom env.EnvMultipleStock_trade import StockEnvTrade\n\n!pip uninstall -y tensorflow-probability\n!pip uninstall -y tensorflow-cloud\n!pip uninstall -y pytorch-lightning\n!pip uninstall -y tensorflow\n!pip uninstall -y gast\n\n!pip install -qq 'tensorflow==1.15.0'\nimport tensorflow as tf\n\n!apt-get update > /dev/null\n!apt-get install -qq -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install -qq \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines import GAIL, SAC\nfrom stable_baselines import ACER\nfrom stable_baselines import PPO2\nfrom stable_baselines import A2C\nfrom stable_baselines import DDPG\nfrom stable_baselines import TD3\n\nfrom stable_baselines.ddpg.policies import DDPGPolicy\nfrom stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\nfrom stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\nfrom stable_baselines.common.vec_env import DummyVecEnv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"dataset\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/trading/trading.csv'\ndf = pd.read_csv(path)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rebalance_window = 63\nvalidation_window = 63","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_trade_date = df[(df.datadate > 20151001)&(df.datadate <= 20200707)].datadate.unique()\nprint(unique_trade_date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"stable\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>Stable Baseline\n        <a class=\"anchor-link\" href=\"#stable\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"def train_A2C(env_train, model_name, timesteps=25000):\n    start = time.time()\n    model = A2C('MlpPolicy', env_train, verbose=0)\n    model.learn(total_timesteps=timesteps)\n    end = time.time()\n\n    model.save(f\"/kaggle/working/{model_name}\")\n    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n    return model\n\ndef train_ACER(env_train, model_name, timesteps=25000):\n    start = time.time()\n    model = ACER('MlpPolicy', env_train, verbose=0)\n    model.learn(total_timesteps=timesteps)\n    end = time.time()\n\n    model.save(f\"/kaggle/working/{model_name}\")\n    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n    return model\n\ndef train_DDPG(env_train, model_name, timesteps=10000):\n    # add the noise objects for DDPG\n    n_actions = env_train.action_space.shape[-1]\n    param_noise = None\n    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n\n    start = time.time()\n    model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n    model.learn(total_timesteps=timesteps)\n    end = time.time()\n\n    model.save(f\"/kaggle/working/{model_name}\")\n    print(' - Training time (DDPG): ', (end-start)/60,' minutes')\n    return model\n\ndef train_PPO(env_train, model_name, timesteps=50000):\n    start = time.time()\n    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\n    \n    model.learn(total_timesteps=timesteps)\n    end = time.time()\n\n    model.save(f\"/kaggle/working/{model_name}\")\n    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n    return model\n\ndef train_GAIL(env_train, model_name, timesteps=1000):\n    start = time.time()\n    # generate expert trajectories\n    model = SAC('MLpPolicy', env_train, verbose=1)\n    generate_expert_traj(model, 'expert_model_gail', n_timesteps=100, n_episodes=10)\n\n    # Load dataset\n    dataset = ExpertDataset(expert_path='expert_model_gail.npz', traj_limitation=10, verbose=1)\n    model = GAIL('MLpPolicy', env_train, dataset, verbose=1)\n\n    model.learn(total_timesteps=1000)\n    end = time.time()\n\n    model.save(f\"/kaggle/working/{model_name}\")\n    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"additional\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>Additional Functions\n        <a class=\"anchor-link\" href=\"#additional\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"def data_split(df,start,end):\n    data = df[(df.datadate >= start) & (df.datadate < end)]\n    data=data.sort_values(['datadate','tic'],ignore_index=True)\n    data.index = data.datadate.factorize()[0]\n    return data\n\ndef get_validation_sharpe(iteration):\n    df_total_value = pd.read_csv('/kaggle/working/account_value_validation_{}.csv'.format(iteration), index_col=0)\n    df_total_value.columns = ['account_value_train']\n    df_total_value['daily_return'] = df_total_value.pct_change(1)\n    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n             df_total_value['daily_return'].std()\n    return sharpe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"predvalid\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>Predict-Validate\n        <a class=\"anchor-link\" href=\"#predvalid\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"def DRL_prediction(df,\n                   model,\n                   name,\n                   last_state,\n                   iter_num,\n                   unique_trade_date,\n                   rebalance_window,\n                   turbulence_threshold,\n                   initial):\n\n    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n    env_trade = DummyVecEnv([lambda: StockEnvTrade(trade_data,\n                                                   turbulence_threshold=turbulence_threshold,\n                                                   initial=initial,\n                                                   previous_state=last_state,\n                                                   model_name=name,\n                                                   iteration=iter_num)])\n    obs_trade = env_trade.reset()\n\n    for i in range(len(trade_data.index.unique())):\n        action, _states = model.predict(obs_trade)\n        obs_trade, rewards, dones, info = env_trade.step(action)\n        if i == (len(trade_data.index.unique()) - 2):\n            last_state = env_trade.render()\n\n    df_last_state = pd.DataFrame({'last_state': last_state})\n    df_last_state.to_csv('/kaggle/working/last_state_{}_{}.csv'.format(name, i), index=False)\n    return last_state\n\ndef DRL_validation(model, test_data, test_env, test_obs) -> None:\n    for i in range(len(test_data.index.unique())):\n        action, _states = model.predict(test_obs)\n        test_obs, rewards, dones, info = test_env.step(action)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"ensemble\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>Ensemble\n        <a class=\"anchor-link\" href=\"#ensemble\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n    last_state_ensemble = []\n    ppo_sharpe_list = []\n    ddpg_sharpe_list = []\n    a2c_sharpe_list = []\n\n    model_use = []\n\n    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n\n    start = time.time()\n    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n        if i - rebalance_window - validation_window == 0:\n            # inital state\n            initial = True\n        else:\n            # previous state\n            initial = False\n\n        # Tuning trubulence index based on historical data\n        # Turbulence lookback window is one quarter\n        end_date_index = df.index[df[\"datadate\"] == unique_trade_date[i - rebalance_window - validation_window]].to_list()[-1]\n        start_date_index = end_date_index - validation_window*30 + 1\n\n        historical_turbulence = df.iloc[start_date_index:(end_date_index + 1), :]\n        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n\n        if historical_turbulence_mean > insample_turbulence_threshold:\n            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n            # then we assume that the current market is volatile,\n            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold\n            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n            turbulence_threshold = insample_turbulence_threshold\n        else:\n            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n            # then we tune up the turbulence_threshold, meaning we lower the risk\n            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n            \n        print(\"-\" * 50)\n        print(\" - Turbulence_threshold: \", turbulence_threshold)\n\n        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n        env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n\n        ## validation env\n        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n                                end=unique_trade_date[i - rebalance_window])\n        env_val = DummyVecEnv([lambda: StockEnvValidation(validation,\n                                                          turbulence_threshold=turbulence_threshold,\n                                                          iteration=i)])\n        obs_val = env_val.reset()\n        \n        print(\" - Model training from: \", 20090000, \"to \",\n              unique_trade_date[i - rebalance_window - validation_window])\n        print(\" - A2C Training\")\n        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n        print(\" - A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n              unique_trade_date[i - rebalance_window])\n        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n        sharpe_a2c = get_validation_sharpe(i)\n        print(\" - A2C Sharpe Ratio: \", sharpe_a2c)\n\n        print(\" - PPO Training\")\n        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n        print(\" - PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n              unique_trade_date[i - rebalance_window])\n        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n        sharpe_ppo = get_validation_sharpe(i)\n        print(\" - PPO Sharpe Ratio: \", sharpe_ppo)\n\n        print(\" - DDPG Training\")\n        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n        print(\" - DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n              unique_trade_date[i - rebalance_window])\n        \n        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n        sharpe_ddpg = get_validation_sharpe(i)\n\n        ppo_sharpe_list.append(sharpe_ppo)\n        a2c_sharpe_list.append(sharpe_a2c)\n        ddpg_sharpe_list.append(sharpe_ddpg)\n\n        # Model Selection based on sharpe ratio\n        if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n            model_ensemble = model_ppo\n            model_use.append('PPO')\n        elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n            model_ensemble = model_a2c\n            model_use.append('A2C')\n        else:\n            model_ensemble = model_ddpg\n            model_use.append('DDPG')\n\n        print(\" - Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n        print(\"-\" * 50)\n        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n                                             last_state=last_state_ensemble, iter_num=i,\n                                             unique_trade_date=unique_trade_date,\n                                             rebalance_window=rebalance_window,\n                                             turbulence_threshold=turbulence_threshold,\n                                             initial=initial)\n        \n    end = time.time()\n    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    run_ensemble_strategy(df=df, \n                          unique_trade_date= unique_trade_date,\n                          rebalance_window = rebalance_window,\n                          validation_window=validation_window)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"references\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n    <center>References\n        <a class=\"anchor-link\" href=\"#references\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy.<br>\nIn ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA.","metadata":{}}]}