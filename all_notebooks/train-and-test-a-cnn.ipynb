{"cells":[{"metadata":{},"cell_type":"markdown","source":"CNN algorithm described in the paper \"Picture What you Read\",  presented at the [DICTA2019](http://dicta2019.dictaconference.org/) international conference in Perth, Australia.  \nThe goal is to use a model that reads textual descriptions in input and outputs the generated images described in the text.\n\n\n### Acknowledgements\n\nIf you use the this script, please cite the following [paper](http://artelab.dista.uninsubria.it/res/research/papers/2019/2019-DICTA-Gallo-Visualization.pdf)  \n```\n@INPROCEEDINGS{Gallo:2019:DICTA,   \n  author={I. Gallo and S. Nawaz, A. Calefati, R. La Grassa, N. Landro},   \n  booktitle={2019 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},   \n  title={Picture What you Read},   \n  year={2019},   \n  month={Dec},  \n}  \n```\n\n"},{"metadata":{},"cell_type":"markdown","source":"## DataHelper class\nUsed to read the multimodal dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\nimport os\n\nimport numpy as np\n\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.utils import shuffle\n\nfrom tqdm import tqdm\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nUNKNOWN = 1000000\n\nclass DataHelper:\n\n    def __init__(self, num_words_to_keep):\n        self.label_encoder = LabelEncoder()\n        self.one_hot_encoder = OneHotEncoder(sparse=False)\n        self.tokenizer = Tokenizer(num_words_to_keep)\n\n    def train_tokenizer(self, text):\n        self.tokenizer.fit_on_texts(text)\n\n    def pickle_everything_to_disk(self, dir):\n        with open(os.path.join(dir, 'label_encoder.pickle'), 'wb') as handle:\n            pickle.dump(self.label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(os.path.join(dir, 'one_hot_encoder.pickle'), 'wb') as handle:\n            pickle.dump(self.one_hot_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(os.path.join(dir, 'tokenizer.pickle'), 'wb') as handle:\n            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def load_all_pickles(dir):\n        with open(os.path.join(dir, 'label_encoder.pickle'), 'rb') as handle:\n            label_encoder = pickle.load(handle)\n        with open(os.path.join(dir, 'one_hot_encoder.pickle'), 'rb') as handle:\n            one_hot_encoder = pickle.load(handle)\n        with open(os.path.join(dir, 'tokenizer.pickle'), 'rb') as handle:\n            tokenizer = pickle.load(handle)\n        return label_encoder, one_hot_encoder, tokenizer\n\n    def convert_to_indices(self, text):\n        return self.tokenizer.texts_to_sequences(text)\n\n    def train_one_hot_encoder(self, train_labels):\n        integer_encoded = self.label_encoder.fit_transform(train_labels)\n        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n        integer_encoded = integer_encoded.tolist()\n        integer_encoded.append([UNKNOWN])\n        self.one_hot_encoder.fit_transform(integer_encoded)\n        return self.one_hot_encoder, self.label_encoder\n\n    def encode_to_one_hot(self, labels_list):\n        # sklearn.LabelEncoder with never seen before values\n        le_dict = dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))\n        integer_encoded = [le_dict.get(i, UNKNOWN) for i in labels_list]\n        integer_encoded = np.array(integer_encoded)\n        #integer_encoded = self.label_encoder.transform(labels_list)\n        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n\n        return self.one_hot_encoder.transform(integer_encoded)\n\n    def prepare_for_tensorflow(self, text_list, labels_list, path_list, num_words_x_doc):\n        text_list = [element.decode('UTF-8') for element in text_list]\n        labels_list = [element.decode('UTF-8') for element in labels_list]\n        path_list = [element.decode('UTF-8') for element in path_list]\n\n        text_indices = self.tokenizer.texts_to_sequences(text_list)\n\n        text_indices = pad_sequences(text_indices, maxlen=num_words_x_doc, value=0.)\n        integer_encoded = self.label_encoder.transform(labels_list)\n        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n        y_one_hot = self.one_hot_encoder.transform(integer_encoded)\n\n        return text_indices, y_one_hot, path_list\n\n    def load_shuffle_data(self, train_path, val_path):\n        print(\"Loading data...\")\n\n        text_train, label_train, img_train = self.load_data(train_path)\n        text_val, label_val, img_val = self.load_data(val_path)\n\n        text_train, label_train, img_train = shuffle(text_train, label_train, img_train, random_state=10)\n\n        print(\"Train/Dev split: {:d}/{:d}\".format(len(text_train), len(text_val)))\n        return text_train, label_train, img_train, text_val, label_val, img_val\n\n    @staticmethod\n    def load_data(train_file):\n        text = []\n        label = []\n        img = []\n        with open(train_file) as tr:\n            for line in tqdm(tr.readlines()):\n                line = line.replace(\"\\n\", \"\")\n                line = line.split('|')\n                text.append(line[0])\n                label.append(line[1])\n                img.append(line[2])\n        return text, label, img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data in RAM"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"num_words_to_keep = 30000\nnum_words_x_doc = 100\ntrain_path=\"/kaggle/input/rectangle-ellipse-multimodal/rectangle_ellipse_multimodal/train.csv\" # csv file containing text|class|image_path\nval_path=\"/kaggle/input/rectangle-ellipse-multimodal/rectangle_ellipse_multimodal/val.csv\" # csv file containing text|class|image_path\nsave_model_dir_name=\"model\" # dir used to save the model\nabs_dirname = os.path.dirname(os.path.abspath(train_path))\nprint(\"Absolute dir name:\", abs_dirname)\n        \ndata_helper = DataHelper(num_words_to_keep)\n\ntrain_x, train_y, train_img_paths, test_x, test_y, test_imgs_paths = data_helper.load_shuffle_data(train_path, val_path)\n\ndata_helper.train_one_hot_encoder(train_y)\ntrain_y = data_helper.encode_to_one_hot(train_y)\ntest_y = data_helper.encode_to_one_hot(test_y)\n\ndata_helper.train_tokenizer(train_x)\n\ntrain_x = data_helper.convert_to_indices(train_x)\ntest_x = data_helper.convert_to_indices(test_x)\n\ntrain_x = pad_sequences(train_x, maxlen=num_words_x_doc, value=0.)\ntest_x = pad_sequences(test_x, maxlen=num_words_x_doc, value=0.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the pickle files in the model dir.  \nThese files are useful when we need to load the trained model and run a test"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(save_model_dir_name):\n    os.makedirs(save_model_dir_name)\n    \ndata_helper.pickle_everything_to_disk(save_model_dir_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('.'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the neural model\n\nRESUED CODE FROM [github](https://github.com/carpedm20/DCGAN-tensorflow/blob/master/ops.py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nclass batch_norm(object):\n    \"\"\"Code modification of http://stackoverflow.com/a/33950177\"\"\"\n\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n\n            self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n            self.name = name\n\n    def __call__(self, x, train=True):\n        shape = x.get_shape().as_list()\n\n        if train:\n            with tf.variable_scope(self.name) as scope:\n                self.beta = tf.get_variable(\"beta\", [shape[-1]],\n                                            initializer=tf.constant_initializer(0.))\n                self.gamma = tf.get_variable(\"gamma\", [shape[-1]],\n                                             initializer=tf.random_normal_initializer(1., 0.02))\n\n                try:\n                    batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n                except:\n                    batch_mean, batch_var = tf.nn.moments(x, [0, 1], name='moments')\n\n                ema_apply_op = self.ema.apply([batch_mean, batch_var])\n                self.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n\n                with tf.control_dependencies([ema_apply_op]):\n                    mean, var = tf.identity(batch_mean), tf.identity(batch_var)\n        else:\n            mean, var = self.ema_mean, self.ema_var\n\n        normed = tf.nn.batch_norm_with_global_normalization(\n            x, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n\n        return normed\n\n    \ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n    shape = input_.get_shape().as_list()\n\n    with tf.variable_scope(scope or \"Linear\"):\n        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        bias = tf.get_variable(\"bias\", [output_size],\n                               initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\n        \ndef conv2d(input_, output_dim,\n           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n           name=\"conv2d\"):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n\n        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n        conv = tf.nn.bias_add(conv, biases)\n        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n        return conv\n\ndef deconv2d(input_, output_shape,\n             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n             name=\"deconv2d\", with_w=False):\n    with tf.variable_scope(name):\n        # tmp_tensor = tf.placeholder(dtype=tf.float32, shape=output_shape)\n        # deconv_shape = tf.stack(output_shape)\n\n        # filter : [height, width, output_channels, in_channels]\n        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n\n        try:\n            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n            # deconv = tf.nn.conv2d_transpose(input_, w, output_shape=deconv_shape, strides=[1, d_h, d_w, 1])\n\n        # Support for verisons of TensorFlow before 0.7.0\n        except AttributeError:\n            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n            # deconv = tf.nn.deconv2d(input_, w, output_shape=deconv_shape, strides=[1, d_h, d_w, 1])\n\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        # deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases),  tf.shape(deconv))\n\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv\n        \ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n    return tf.maximum(x, leak * x)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom math import ceil\n\n\nclass TextImgCNN(object):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    \"\"\"\n\n    def __init__(self, sequence_length, num_classes, vocab_size,\n                 embedding_size, filter_sizes, num_filters, output_image_width, encoding_height,\n                 l2_reg_lambda=0.5, batch_size=32):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n        self.input_image = tf.placeholder(tf.float32, [None, output_image_width, output_image_width, 3], name=\"input_mask\")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n        self.g_bn0 = batch_norm(name='generator_bn0')\n        self.g_bn1 = batch_norm(name='generator_bn1')\n        self.g_bn2 = batch_norm(name='generator_bn2')\n        self.g_bn3 = batch_norm(name='genrerator_bn3')\n        self.d_bn1 = batch_norm(name='discriminator_bn1')\n        self.d_bn2 = batch_norm(name='discriminator_bn2')\n        self.d_bn3 = batch_norm(name='discriminator_bn3')\n        self.d_bn4 = batch_norm(name='discriminator_bn4')\n\n        # Embedding layer\n        with tf.name_scope(\"generator_embedding\"):\n            self.W = tf.Variable(\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                name=\"W\")\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(\"generator_conv-maxpool-%s\" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=\"VALID\",\n                    name=\"conv\")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b))\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n        # Deconv from https://github.com/paarthneekhara/text-to-image/blob/master/model.py\n        gf_dim = 64 # Number of conv\n        s = output_image_width # src image size\n        s2, s4, s8, s16 = int(ceil(s / 2)), int(ceil(s / 4)), int(ceil(s / 8)), int(ceil(s / 16))\n\n        fc0 = linear(self.h_pool_flat, gf_dim * 8 * s16 * s16, 'generator_h0_lin')\n\n        h0 = tf.reshape(fc0, [-1, s16, s16, gf_dim * 8])\n        h0 = tf.nn.relu(self.g_bn0(h0))\n\n        h1 = deconv2d(h0, [tf.shape(h0)[0], s8, s8, gf_dim * 4], name='generator_h1')\n        h1 = tf.nn.relu(self.g_bn1(h1))\n\n        h2 = deconv2d(h1, [tf.shape(h0)[0], s4, s4, gf_dim * 2], name='generator_h2')\n        h2 = tf.nn.relu(self.g_bn2(h2))\n\n        h3 = deconv2d(h2, [tf.shape(h0)[0], s2, s2, gf_dim * 1], name='generator_h3')\n        h3 = tf.nn.relu(self.g_bn3(h3))\n\n        self.reshaped_text_features = deconv2d(h3, [tf.shape(h0)[0], s, s, 3], name='generator_encoded_text')\n        # self.reshaped_text_features = tf.tanh(self.reshaped_text_features) / 2. + 0.5\n        self.reshaped_text_features = tf.sigmoid(self.reshaped_text_features, name='generator_encoded_text_sigmoid')\n\n        sh0, sh1, sh2, sh3 = int(ceil(s / 2)), int(ceil(s / 4)), int(ceil(s / 8)), int(ceil(s / 16))\n\n        hh0 = lrelu(conv2d(self.reshaped_text_features, gf_dim, name='discriminator_h0_conv'))\n        hh1 = lrelu(self.d_bn1(conv2d(hh0, gf_dim / 2, name='discriminator_h1_conv')))\n        hh2 = lrelu(self.d_bn2(conv2d(hh1, gf_dim / 4, name='discriminator_h2_conv')))\n        hh3 = lrelu(self.d_bn3(conv2d(hh2, gf_dim / 8, name='discriminator_h3_conv')))\n\n        hh3_flat_size = int(sh3*sh3*(gf_dim / 8)) \n        self.h_pool_flat_2 = tf.reshape(hh3, [-1, hh3_flat_size])\n\n        fc1_size = 1024 # hh3_flat_size/2\n        fc1 = linear(self.h_pool_flat_2, fc1_size, 'discriminator_f1_lin')\n\n        fc2_size = 512  # hh3_flat_size/2\n        fc2 = linear(fc1, fc2_size, 'discriminator_f2_lin')\n\n        # Add dropout\n        with tf.name_scope(\"dropout\"):\n            self.h_drop = tf.nn.dropout(fc2, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"discriminator_output\"):\n            W = tf.get_variable(\n                \"W\",\n                shape=[fc2_size, num_classes],\n                initializer=tf.glorot_uniform_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss_d = tf.reduce_mean(losses)\n            self.loss_g = tf.reduce_sum(tf.square(tf.subtract(self.reshaped_text_features, self.input_image)))\n            self.loss_full = self.loss_d + l2_reg_lambda * self.loss_g\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size_test = 32\nembedding_dim = 128 # Dimensionality of character embedding\nfilter_sizes = \"3,4,5\" # Comma-separated filter sizes (default: '3,4,5')\nnum_filters = 128 # Number of filters per filter size\ndropout_keep_prob = 0.5 # Dropout keep probability\nl2_reg_lambda = 1.0 # L2 regularization lambda\nbatch_size = 32 # Batch Size\nnum_epochs=3 # Number of training epochs \nevaluate_every=1000 # Evaluate model on dev set after this many steps \nnum_checkpoints=1 # Number of checkpoints to store \npatience=5 # Stop criteria \noutput_image_width=100 # Size of output Image plus embedding \n\nencoding_height=10 # Height of the output embedding ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n#import os\nimport time\nimport cv2\n\ndef train(x_train, y_train, img_train, x_test, y_test, img_test, words_to_keep, output_image_width, encoding_height,\n          patience_init_val):\n    def train_step(x_batch, y_batch, images_batch):\n        \"\"\"\n        A single training step\n        \"\"\"\n        feed_dict = {\n            cnn.input_x: x_batch,\n            cnn.input_y: y_batch,\n            cnn.input_image: images_batch,\n            cnn.dropout_keep_prob: dropout_keep_prob\n        }\n\n        _, step, summaries, loss_g, loss_d, loss_full, accuracy = sess.run(\n            [train_op_full, global_step, train_summary_op, cnn.loss_g, cnn.loss_d, cnn.loss_full, cnn.accuracy], feed_dict)\n\n        time_str = datetime.datetime.now().isoformat()\n        #print(\"TRAIN: {}: step {}, loss_d {:g}, loss_g {:g}, loss_full {:g}, acc {:g}\".format(time_str, step, loss_d, loss_g, loss_full, accuracy))\n        train_summary_writer.add_summary(summaries, step)\n\n    def dev_step_only_accuracy(x_batch, y_batch, images_batch):\n        \"\"\"\n        Evaluates model on a dev set\n        \"\"\"\n        feed_dict = {\n            cnn.input_x: x_batch,\n            cnn.input_y: y_batch,\n            cnn.input_image: images_batch,\n            cnn.dropout_keep_prob: 1.0\n        }\n        step, summaries, loss, accuracy = sess.run([global_step, dev_summary_op, cnn.loss_d, cnn.accuracy], feed_dict)\n        return accuracy\n\n    best_accuracy = 0\n    patience = patience_init_val\n\n    with tf.Graph().as_default():\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n        with sess.as_default():\n\n            cnn = TextImgCNN(\n                sequence_length=x_train.shape[1],\n                num_classes=y_train.shape[1],\n                vocab_size=words_to_keep,\n                embedding_size=embedding_dim,\n                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n                num_filters=num_filters,\n                output_image_width=output_image_width,\n                encoding_height=encoding_height,\n                l2_reg_lambda=l2_reg_lambda,\n                batch_size=batch_size)\n\n            dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, img_train))\n            dataset = dataset.batch(batch_size)\n            train_iterator = dataset.make_initializable_iterator()\n            sess.run(train_iterator.initializer)\n            next_element = train_iterator.get_next()\n\n            test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test, img_test))\n            test_dataset = test_dataset.batch(batch_size_test)\n            test_iterator = test_dataset.make_initializable_iterator()\n            next_test_element = test_iterator.get_next()\n\n            t_vars = tf.trainable_variables()\n            d_vars = [var for var in t_vars if 'discriminator' in var.name]\n            g_vars = [var for var in t_vars if 'generator' in var.name]\n\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n            train_op_full = tf.train.AdamOptimizer(1e-2).minimize(cnn.loss_full, global_step=global_step)\n\n            # Output directory for models and summaries\n            timestamp = str(int(time.time()))\n            out_dir = save_model_dir_name\n            if not os.path.exists(out_dir):\n                os.makedirs(out_dir)\n\n            print(\"Writing to {}\\n\".format(out_dir))\n\n            # Summaries for loss and test_accuracy\n            loss_summary = tf.summary.scalar(\"loss\", cnn.loss_d)\n            acc_summary = tf.summary.scalar(\"test_accuracy\", cnn.accuracy)\n\n            # Train Summaries\n            train_summary_op = tf.summary.merge([loss_summary, acc_summary]) #, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Dev summaries\n            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n\n            # initialize all of the variables in the session\n            sess.run(tf.global_variables_initializer())\n\n            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n            checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n                print(\"Creating new checkpoint dir\", checkpoint_dir)\n                saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n            else:\n                # Load the saved meta graph and restore variables\n                tf.initialize_all_variables().run()\n                saver = tf.train.Saver(tf.all_variables())\n                # restore gen_model\n                saver.restore(sess, checkpoint_file)\n                print(\"Restoring model from\", checkpoint_dir)\n\n            with open(os.path.join(out_dir, \"results.txt\"), \"a\") as resfile:\n                resfile.write(\"Model dir: {}\\n\".format(out_dir))\n                resfile.write(\"Dataset: {}\\n\".format(img_train[0]))\n\n\n            train_length = len(x_train)\n            val_length = len(x_test)\n\n            for ep in range(num_epochs):\n                print(\"***** Epoch \" + str(ep) + \" *****\")\n                sess.run(train_iterator.initializer)\n\n                for b in range((train_length // batch_size) + 1):\n                    images_batch = []\n                    element = sess.run(next_element)\n\n                    path_list = [el.decode('UTF-8') for el in element[2]]\n\n                    for path in path_list:\n                        path = os.path.join(abs_dirname, path)\n                        #print(\"image path: \" + path)\n                        img = cv2.imread(path)\n                        img = cv2.resize(img, (output_image_width, output_image_width))\n                        img = img / 255\n                        images_batch.append(img)\n\n                    train_step(element[0], element[1], images_batch)\n\n                    current_step = tf.train.global_step(sess, global_step)\n                    if current_step % evaluate_every == 0:\n                        sess.run(test_iterator.initializer)\n\n                    if current_step % evaluate_every == 0:\n                        print(\"\\nEvaluation:\")\n                        # Run one pass over the validation dataset.\n                        sess.run(test_iterator.initializer)\n                        correct = 0\n                        for b in range((val_length // batch_size_test) + 1):\n                            test_img_batch = []\n                            test_element = sess.run(next_test_element)\n\n                            test_path_list = [el.decode('UTF-8') for el in test_element[2]]\n\n                            for path in test_path_list:\n                                path = os.path.join(abs_dirname, path)\n                                img = cv2.imread(path)\n                                img = cv2.resize(img, (output_image_width, output_image_width))\n                                img = img / 255\n                                test_img_batch.append(img)\n\n                            acc = dev_step_only_accuracy(test_element[0], test_element[1], test_img_batch)\n                            correct += acc * len(test_path_list)\n                            \n                        test_accuracy = correct / val_length\n\n                        if test_accuracy > best_accuracy:\n                            best_accuracy = test_accuracy\n                            patience = patience_init_val\n                            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                            print(\"Saved model checkpoint to {}\\n\".format(path))\n                        else:\n                            patience -= 1\n\n                        print(\"TEST: epoch: %d, step: %d, test acc: %f, best acc: %f, patience: %d\\n\" % (\n                                ep, current_step, test_accuracy, best_accuracy, patience))\n\n                        #with open(os.path.join(out_dir, \"results.txt\"), \"a\") as resfile:\n                        #    resfile.write(\"epoch: %d, step: %d, test acc: %f, best acc: %f, patience: %d\\n\" % (\n                        #        ep, current_step, test_accuracy, best_accuracy, patience))\n\n                        if patience == 0:\n                            return cnn\n            return cnn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run the training process"},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_cnn = train(train_x, train_y, train_img_paths, test_x, test_y, test_imgs_paths, num_words_to_keep, output_image_width, encoding_height, patience)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_length = len(test_x)\n\nwith tf.Graph().as_default():\n    sess = tf.Session()\n    with sess.as_default():\n        val_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y, test_imgs_paths))\n        val_dataset = val_dataset.batch(batch_size)\n        val_iterator = val_dataset.make_initializable_iterator()\n        val_next_element = val_iterator.get_next()\n\n        sess.run(val_iterator.initializer)\n\n        print(\"Creating VAL images from CNN features...\")\n        correct = 0\n        for b in tqdm(range((val_length // batch_size) + 1)):\n            images = []\n            element = sess.run(val_next_element)\n            path_list = [el.decode('UTF-8') for el in element[2]]\n\n            feed_dict = {\n                trained_cnn.input_x: element[0],\n                trained_cnn.input_y: element[1],\n                trained_cnn.dropout_keep_prob: 1.0\n            }\n            acc, encoded_text_features, preds = sess.run([trained_cnn.accuracy, trained_cnn.reshaped_text_features, trained_cnn.predictions], feed_dict)\n            pred_labels = list(label_encoder.inverse_transform(preds))\n            correct += acc * len(path_list)  # batch_size\n\n            # for features, src_img in zip(encoded_text_features, images):\n            #     cv2.imshow(\"image_features\", features)\n            #     print(\"min:\", features.min(), \"max:\", features.max())\n            #     cv2.imshow(\"image_src\", src_img)\n            #     cv2.waitKey(0)\n\n        test_accuracy = correct / val_length\n        print(\"Test accuracy: {}/{}={}\".format(correct, val_length, test_accuracy))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}