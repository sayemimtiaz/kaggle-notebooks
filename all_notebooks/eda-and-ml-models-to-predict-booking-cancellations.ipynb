{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thank you for checking out my work on the Hotel Booking Demand dataset!\n\n# Context\n\nIn this exercise, I try to simulate a real-life business problem about Hotels and bookings.\n\nIs it possible to predict in advance whether a booking will be cancelled by knowing some details about it (time, cost, special requests ...) and the future-guests (nationality, group-size, first time/repeated ...)?\nYes!\n\nBy looking at past bookings, it's possible to create accurate predictive models that will tell us the probability of a new reservation being cancelled. \n\n# About the data\n\nThis dataset contains information of reservations spanning from July 2015 to August 2017 included, of two large hotels in Portugal: one is a Resort by the beach, the other is in a major City. \n\nData is specific about these two hotels, but customers come from all over the world (even though a good majority is from Portugal). For this reason, we can assume that customers tend to behave similarly when it comes to making hotel reservations. \n\nPretending to be an hotel manager that currently doesn't exploit data at best, by looking at this report it's possible to (1) use Exploratory Data Analysis to understand whether these bookings and customers data are similar to their customers' profiles and (2), in the affirmative case, these predictive models can be used as a proxy for their own situation."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n\ndata = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')\ndata.head()\ndata.shape\ndata.dtypes\ndata = data.rename({'is_canceled':'y'}, axis=1)\npd.set_option('display.max_columns', None)\ndata.describe()\nnans = pd.DataFrame(data.isna().sum(), columns=['count'])    #check nans\nnans.reset_index(inplace=True)\nnans.loc[nans['count'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data = data.drop(columns=['company', \n                'agent', 'reservation_status', 'reservation_status_date']) #company, agent for too many nans. Reservation to prevent leakage\ndata['country'] = data['country'].fillna(value = 'no info')\ndata['children'] = data['children'].fillna(0)\nnans = pd.DataFrame(data.isna().sum(), columns=['count'])                  #check nans\nnans.reset_index(inplace=True)\nnans.loc[nans['count'] != 0]                                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#modify variable hotel\ndata.replace(['Resort Hotel', 'City Hotel'], [1, 0], inplace=True)\ndata.rename(columns={'hotel':'resort'}, inplace=True)\n\n#delete outliers and errors (adults: it's a series of 12 mistakes, all with common characteristics)\ndata = data.loc[data['adults'] <= 10]\ndata = data.loc[data['babies'] <= 3]\ndata = data.loc[data['adr'] <= 510]\ndata.reset_index(drop=True, inplace=True)\n\n#remove negative 'adr' (must be a mistake)\ndata = data.loc[data['adr'] >= 0 ]\n\n#remove obs with 0 adults and 0 children (180 observations)\ndata = data.loc[(data['adults'] != 0) | (data['children'] != 0)]\ndata.reset_index(drop=True, inplace=True)\n\n#separate numerical non-bin, numerical and bin vars, and categorical variables (count arrival dates as categorical)\ndata.arrival_date_year = data.arrival_date_year.astype('object')\ndata.arrival_date_week_number = data.arrival_date_week_number.astype('object')\ndata.arrival_date_day_of_month = data.arrival_date_day_of_month.astype('object')\ncont_var = data.drop(columns=['y', 'resort', 'is_repeated_guest']).select_dtypes(include='number').columns\nnum_var = data.drop(columns=['y']).select_dtypes(include='number').columns\ncat_var = data.drop(columns=['y']).select_dtypes(include='object').columns\n\ndata = data[['y', 'resort', 'lead_time', 'arrival_date_year', 'arrival_date_month',\n       'arrival_date_week_number', 'arrival_date_day_of_month',\n       'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children',\n       'babies', 'meal', 'country', 'market_segment', 'distribution_channel',\n       'is_repeated_guest', 'previous_cancellations',\n       'previous_bookings_not_canceled', 'booking_changes', 'deposit_type',\n       'days_in_waiting_list', 'customer_type', 'adr',\n       'required_car_parking_spaces', 'total_of_special_requests',\n       'reserved_room_type']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis**\n\nSeveral visualizations are made to better undestand the business context.\n\nSince the goal is to try and create models (ideally) usable by any hotel, only visualizations with information relevant to understand customers' profiles and behaviours are provided."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# EXPLORATORY DATA ANALYSIS #\n#create a function that labels objects with some specific information\ndef labeller(kind, values, axis, tot=None, only_minmax=False): \n    if kind == 'bar':\n        ax=axis\n        all_heights=[]\n        for rect in values:\n            all_heights.append(rect.get_height())\n        all_heights = pd.Series(all_heights)\n        for rect in values:\n            height = rect.get_height()\n            perc = height / all_heights.sum() *100\n            per = str(perc.round(1))\n            if only_minmax == True:\n                if height == all_heights.max():\n                    ax.annotate('max:\\n' + str(per) + '%', \n                                xy=(rect.get_x() + rect.get_width() / 2, height), \n                                xytext=(0, 5),\n                                textcoords=\"offset points\", \n                                ha='center', \n                                va='bottom', \n                                size=11, \n                                bbox=dict(boxstyle=\"round4\", fc=\"w\"))\n                elif height == all_heights.min():\n                    ax.annotate('min:\\n' + str(per) + '%', \n                                xy=(rect.get_x() + rect.get_width() / 2, height), \n                                xytext=(0, 5),\n                                textcoords=\"offset points\", \n                                ha='center', \n                                va='bottom', \n                                size=11, \n                                bbox=dict(boxstyle=\"round4\", fc=\"w\"))\n            else:    \n                ax.annotate(str(per) + '%', \n                            xy=(rect.get_x() + rect.get_width() / 2, height), \n                            xytext=(0, 3),\n                            textcoords=\"offset points\", \n                            ha='center', \n                            va='bottom', \n                            size=11)\n    elif kind == 'scatter':\n        for i, txt in enumerate(values):\n            ax=axis\n            if only_minmax == True:\n                values = pd.Series(values)\n                values.reset_index(drop=True, inplace=True)\n                if txt == values.max():\n                    ax.annotate('high: ' + str(txt) + '€', \n                                (x[i], values[i]), \n                                size=11, \n                                xytext=(-85, -3), \n                                textcoords=\"offset points\", \n                                bbox=dict(boxstyle=\"rarrow\", fc=\"w\"))\n                elif txt == values.min():\n                    ax.annotate('low: ' + str(txt) + '€', \n                                (x[i], values[i]), \n                                size=11, \n                                xytext=(17, -3), \n                                textcoords=\"offset points\", \n                                bbox=dict(boxstyle=\"larrow\", fc=\"w\"))\n            else: ax.annotate(str(txt), \n                              (x[i], values[i]), \n                              size=9, \n                              xytext=(-10, 7), \n                              textcoords=\"offset points\", \n                              bbox=dict(boxstyle=\"round4\",fc=\"w\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many cancellations were made?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#How many cancellations??\n#separate resort data & count bookings\ndf = data.copy()\ndr = df[['resort', 'y']].loc[df['resort'] == 1]                      \ndr = pd.DataFrame(dr['y'].value_counts())                            \ndr.reset_index(inplace=True)\ndr.rename({'index':'y', 'y':'y_count'},axis=1,inplace=True)\n#add rows to identify it's Resort data, separate city data\ndr['hotel'] = ['res', 'res']                                             \ndc = df[['resort', 'y']].loc[df['resort'] == 0]                       \ndc.reset_index(drop=True, inplace=True)                               \ndc = pd.DataFrame(dc['y'].value_counts())\ndc.reset_index(inplace=True)\ndc.rename({'index':'y', 'y':'y_count'},axis=1,inplace=True)\n#add rows to identify it's city data\ndc['hotel'] = ['city','city']                                         \nd = dr.append(dc)       #merge again\n\nlabels = ['Beach Resort', 'City Hotel']\ncancellations = d['y_count'].loc[d['y'] == 1]             \nnon_cancellations = d['y_count'].loc[d['y'] == 0]    \ncity_count = dc['y_count'].sum()                     \nresort_count = dr['y_count'].sum()                   \n\nwidth = 0.35                    #width of each rectangle\nx = np.arange(len(labels))      #list for positions\n\nfig, ax = plt.subplots(figsize=(6,6))\n\nrect1 = ax.bar(x - width/2, cancellations, width)         \nrect2 = ax.bar(x + width/2, non_cancellations, width)     \n\n#create a function to put labels. It's different from the other\ndef labeller2(rects, city_tot, res_tot):                  \n    for rect in rects:                                    \n        height = rect.get_height()                        \n        if height == rects[0].get_height():             \n            tot = res_tot\n        else:\n            tot = city_tot\n        percentage = height / tot * 100                   \n        percentage = str(percentage.round(2)) + '%'\n        ax.annotate(percentage, \n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', size=13)\n\nlabeller2(rect1, city_count, resort_count)              \nlabeller2(rect2, city_count, resort_count)  \n\nax.set_xticks(x)\nax.set_xticklabels(labels, fontsize=13)\nax.set_ylim([0,52000])\nax.set_ylabel('Bookings')\nax.set_title('Number of bookings cancelled per Hotel')\nax.legend(['Cancelled', 'Not Cancelled'], fontsize= 12)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many guests had already visited the hotel in the past?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# repeated guests\nd1 = data.copy()\nd1 = data[['resort', 'y', 'is_repeated_guest']]\nd1r = d1[['is_repeated_guest', 'y']].loc[data['resort'] == 1].groupby('is_repeated_guest').count() \nd1c = d1[['is_repeated_guest', 'y']].loc[data['resort'] == 0].groupby('is_repeated_guest').count()\nd1r = d1r['y'] / d1r['y'].sum() * 100\nd1r = d1r.round(2)\nd1c = d1c['y'] / d1c['y'].sum() * 100\nd1c = d1c.round(2)\nprint('City Hotel:', d1c, '\\nBeach Resort:', d1r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is guests' nationality?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Where do guests come from?? \nd2 = data.copy()\nd2 = d2.loc[d2['y'] == 0]  #only actual guests\n\n#City Hotel data\nd2c = d2.loc[d2['resort'] == 0]                  \nd2c = d2c.country.value_counts()\nd2c_top = d2c[0:12]\nd2c_bottom = d2c[13:].sum()\nd2c_bottom = pd.Series({'Others': d2c_bottom})\nd2c = d2c_top.append(d2c_bottom)\n#Beach resort data\nd2r = d2.loc[d2['resort'] == 1]                   \nd2r = d2r.country.value_counts()\nd2r_top = d2r[0:12]\nd2r_bottom = d2r[13:].sum()\nd2r_bottom = pd.Series({'Others': d2r_bottom})\nd2r = d2r_top.append(d2r_bottom)\n\nwidth = 0.6\nx1=np.arange(len(d2r))\nx2=np.arange(len(d2c))\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n\nrect1 = ax1.bar(x1, d2r.values, width)\nrect2 = ax2.bar(x2, d2c.values, width)\n\nlabeller(kind='bar', values=rect1, axis=ax1)\nlabeller(kind='bar', values=rect2, axis=ax2)\n        \nax1.set_title('Beach Resort', fontsize=14)\nax1.set_xticks(x1)\nax1.set_xticklabels(d2r.index, rotation=30, size=13)\nax1.spines[\"top\"].set_visible(False)\nax1.spines[\"right\"].set_visible(False)\nax1.yaxis.grid(True)\nax2.set_title('City Hotel', fontsize= 14)\nax2.set_xticks(x2)\nax2.set_xticklabels(d2c.index, rotation=30, size= 13)\nax2.spines[\"top\"].set_visible(False)\nax2.spines[\"right\"].set_visible(False)\nax2.yaxis.grid(True)\nfig.suptitle('Where do guests come from?', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the average rate per person?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dco = data.copy()\ndco = dco.loc[dco['y'] == 0]    #only actual guests\n#remove adr = 0 (must be guests that got a free stay)\ndco = dco.loc[dco['adr'] != 0]              \ndco.reset_index(drop=True, inplace=True)\ndco['adr_pp'] = dco['adr'] / (dco['adults'] + dco['children'])\ndco_r = dco[['adr_pp', 'arrival_date_month']].loc[dco['resort'] == 1].groupby('arrival_date_month').mean()        \ndco_c = dco[['adr_pp', 'arrival_date_month']].loc[dco['resort'] == 0].groupby('arrival_date_month').mean()\ndco_r = dco_r.round(2)\ndco_c = dco_c.round(2)\n\nmonths_ordered = ['January', 'February', 'March', 'April', 'May', 'June', \n                  'July', 'August', 'September', 'October', 'November', 'December'] \ndco_r = dco_r.reindex(months_ordered)\ndco_c = dco_c.reindex(months_ordered)\ndco_r = dco_r.reset_index()\ndco_r = list(dco_r['adr_pp'])\ndco_c = dco_c.reset_index()\ndco_c = list(dco_c['adr_pp'])\n\nx=np.arange(len(months_ordered))\n\nfig, ax = plt.subplots(figsize=(9,6))\npoints_r = ax.scatter(x=x, y=dco_r, s=100) #create scatters\npoints_c = ax.scatter(x=x, y=dco_c, s=100)\nax.plot(x, dco_r)                          #add lines\nax.plot(x, dco_c)\n\nlabeller(kind='scatter', values=dco_r, axis=ax, only_minmax=True)\nlabeller(kind='scatter', values=dco_c, axis=ax, only_minmax=True)\n    \nax.set_xticks(x)\nax.set_xticklabels(months_ordered, rotation=35)\nax.set_ylim([10,105])\nax.legend(['Beach Resort', 'City Hotel'], fontsize= 12)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.yaxis.grid(True)\nax.set_title('Average Rate per person', fontsize= 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For how long do guests stay?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#how long do guests stay at the hotels?\ndata1= data.copy()\ndata1 = data1.loc[data1['y'] == 0]\ndata1['total_nights'] = data1['stays_in_weekend_nights'] + data1['stays_in_week_nights']\ndata1 = data1.loc[(data1['total_nights'] != 0) & (data1['total_nights'] < 14)]\ndata1 = data1[['resort', 'total_nights']]\nres = data1['total_nights'].loc[data['resort'] == 1 ]\ncit = data1['total_nights'].loc[data['resort'] == 0]\nres.reset_index(drop=True, inplace=True)\ncit.reset_index(drop=True, inplace=True)\nres = list(res)\ncit = list(cit)\n\nfig, ax = plt.subplots(figsize=(6,6))\nax.boxplot(res, positions=[1], widths=0.5, patch_artist = True)\nax.boxplot(cit, positions=[2], widths=0.5, patch_artist = True)\nax.set_xticks([1,2])\nax.set_xticklabels(['Beach Resort', 'City Hotel'], fontsize=13)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.yaxis.grid(True)\nplt.title('Staying nights per booking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How was the booking made? Chart of the market segment"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Bookings by market segment\ndata2 = data.copy()\ndata2 = data2[['resort', 'market_segment']]\ndata2c = data2['market_segment'].loc[data2['resort'] == 0].value_counts()\ndata2r = data2['market_segment'].loc[data2['resort'] == 1].value_counts()\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\nfig.suptitle('How was the booking made?', fontsize=14)\nax1.pie(data2r[0:5].values, startangle=90, autopct='%1.1f%%', pctdistance=1.15)\nax1.set_xlabel('Beach Resort', fontsize=13)\nax2.pie(data2c[0:5].values, startangle=90, autopct='%1.1f%%', pctdistance=1.15)\nax2.set_xlabel('City Hotel', fontsize=13)\n\nplt.legend(data2r.index, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What months get the most cancellations?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#What months get the most cancellations? \n#(divide 2 datasets and plot the difference between Resort and City)\nmonths_ordered = ['January', 'February', 'March', 'April', 'May', 'June', \n                  'July', 'August', 'September', 'October', 'November', 'December']\ndata3 = data.copy()\ndata3 = data3.loc[data3['y'] == 1]\ndata3 = data3[['y','resort', 'arrival_date_month']]\n\ndata3res = data3.loc[data['resort'] == 1]\ndata3res = data3res.drop(columns={'resort'})\ndata3res = data3res.groupby('arrival_date_month').count()\ndata3res.reset_index(inplace=True)\ndata3res.loc[((data3res[\"arrival_date_month\"] == \"July\") | (data3res[\"arrival_date_month\"] == \"August\")),\n                    \"y\"] /= 3\ndata3res.loc[~((data3res[\"arrival_date_month\"] == \"July\") | (data3res[\"arrival_date_month\"] == \"August\")),\n                    \"y\"] /= 2\ndata3res.set_index('arrival_date_month', inplace=True)\ndata3res = data3res.reindex(months_ordered)\n\ndata3cit = data3.loc[data['resort'] == 0]\ndata3cit = data3cit.drop(columns={'resort'})\ndata3cit = data3cit.groupby('arrival_date_month').count()\ndata3cit.reset_index(inplace=True)\ndata3cit.loc[((data3cit[\"arrival_date_month\"] == \"July\") | (data3cit[\"arrival_date_month\"] == \"August\")),\n                    \"y\"] /= 3\ndata3cit.loc[~((data3cit[\"arrival_date_month\"] == \"July\") | (data3cit[\"arrival_date_month\"] == \"August\")),\n                    \"y\"] /= 2\ndata3cit.set_index('arrival_date_month', inplace=True)\ndata3cit = data3cit.reindex(months_ordered)\n\nd3c=data3cit['y'].astype(int)\nd3r=data3res['y'].astype(int)\n\nwidth = 0.5\nx=np.arange(len(months_ordered))\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n\nrect3 = ax1.bar(x, d3r.values, width)\nrect4 = ax2.bar(x, d3c.values, width)\n\nlabeller(kind='bar', values=rect3, axis=ax1, only_minmax=True)\nlabeller(kind='bar', values=rect4, axis=ax2, only_minmax=True)\n\nax1.set_xticks(x)\nax1.set_xticklabels(months_ordered, rotation=30)\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.yaxis.grid(True)\nax1.set_title('Beach Resort')\nax1.set_ylim([0,650])\nax2.set_xticks(x)\nax2.set_xticklabels(months_ordered, rotation=30)\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.yaxis.grid(True)\nax2.set_title('City Hotel')\nax2.set_ylim([0,2100])\n\nplt.suptitle('Cancellations per month (%)', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now move to **Predictive models**. \n\n\nBefore implementation, data is transformed as follows:\n\n- Categorical data is transformed into dummies\n- Continuous data is normalized: Xi = (Xi - mean)/variance\n- Multicollinearity check: variables with Variance Inflator Factor > 5 are removed.\n- Train Test split\n\n\n\nPlease note: Multicollinearity is relevant mainly in linear models. Moreover, when analysis goal is mainly prediction rather then interpretation, multicollinearity can be ignored since it influences coefficients' values but not accuracy. For simplicity and computational limits, in this exercise we delete collinear variables."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#create dummies and standardize\ndata_dummy=pd.get_dummies(data.copy(), dummy_na=False, drop_first=True)\ndata_dummy[cont_var]=pd.DataFrame(preprocessing.StandardScaler().fit_transform(data[cont_var].values),columns = cont_var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What predictors are highly correlated to our y?"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Correlation of top 20 variables\ncancel_corr = data_dummy.corr()\ncancel_corr['y'].abs().sort_values(ascending=False)[1:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nVIF_set = data_dummy.copy().drop(columns=['y'])\ncols=VIF_set.columns\nVIF_set = add_constant(VIF_set.values)\nVIF_series = pd.Series([\"{0:.2f}\".format(variance_inflation_factor(VIF_set, i)) for i in range(VIF_set.shape[1])], \n                       index=['constant'] + list(cols))\n#Keep only variables with VIF < 5\nVIF_df = pd.DataFrame(VIF_series.rename('vif'))\nVIF_df['vif'] = VIF_df['vif'].astype(float)\ncols_to_keep = VIF_df.loc[VIF_df['vif'] < 5].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#import all modules necessary for CV, fitting and measurements\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.linear_model import RidgeClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom statistics import mean\n\n#Divide training and testing\nx = data_dummy[cols_to_keep]\ny = data_dummy['y'].values \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# CV and models: Logistic regression, Decision Tree and Random Forest. For Log. Reg., we have to specify penalty='none', otherwise a penalty is automatically added\nacc_LogReg = mean(cross_val_score(LogisticRegression(solver='newton-cg', penalty='none'), X_train, y_train, cv=10))\nacc_DecTree = mean(cross_val_score(DecisionTreeClassifier(), X_train, y_train, cv=10))\nacc_RanFor = mean(cross_val_score(RandomForestClassifier(), X_train, y_train, cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#RidgeRegression\nalphas = 10**np.linspace(10,-2,100)*0.5\nridge_cv = RidgeClassifierCV(alphas = alphas)    #select the best lambda with CV\nridge_cv.fit(X_train, y_train)\nridge_final = RidgeClassifier(alpha=ridge_cv.alpha_)\nridge_final.fit(X_train, y_train)\n\n#LassoRegression\nlasso_cv = LogisticRegressionCV(solver='liblinear', penalty='l1', cv=5)    #select the best lamba with CV. Penalty = l1 men\nlasso_cv.fit(X_train, y_train)\nlasso_final = LogisticRegression(solver='liblinear', penalty='l1', C=lasso_cv.C_[0])\nlasso_final.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Five different classifiers are tried: Logistic Regression, Ridge, Lasso, Decision Tree and Random Forest.\n\nCross Validation is perfomed to assess what model is best for this data.\n"},{"metadata":{},"cell_type":"markdown","source":"Mean accuracy score for the five fitted models:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Mean Accuracy for Logistic Regression:       ', acc_LogReg)\nprint('Mean Accuracy for Ridge Regression:          ', ridge_final.score(X_test, y_test))\nprint('Mean Accuracy for Lasso Regression:          ', accuracy_score(y_test, lasso_final.predict(X_test)))\nprint('Mean Accuracy for Decision Tree:             ', acc_DecTree)\nprint('Mean Accuracy for Random Forest:             ', acc_RanFor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best performing model is Random Forest!\n\nHyperparameter tuning: what are the hyperparameters with which the model performs at its best?"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Best performing model is Random Forest, even without Hyperparameter tuning\n#Let's discover what are the best hyperparameters\n\nparam_grid = {\n    'max_features': ['sqrt', 'auto', 'log2'],\n    'min_samples_leaf': [1, 3, 5],\n    'n_estimators': [250, 500, 750, 1000, 1500]\n}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the model performs with the hyperparameters we found previously:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"final_model = RandomForestClassifier(n_estimators=750, max_features='sqrt', min_samples_leaf=1)\nfinal_model.fit(X_train, y_train)\ny_pred_final_model = final_model.predict(X_test)\nprint(classification_report(y_test, y_pred_final_model))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall accuracy is not significantly different from the one of basic Random Forest model. \n"},{"metadata":{},"cell_type":"markdown","source":"The above statistics are sensible to the choosen cutoff (*i.e.*, the threshold at which the model classifies an observation as 0 or 1).\n\nConversely, the Area Under the Curve of the Receiver Operatoring Characteristic curve is independent of that."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"roc_auc = roc_auc_score(y_test, y_pred_final_model)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_final_model)\n\nplt.figure()\nplt.plot(fpr, tpr, label='Random Forest (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's check **feature importance**. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Feature importance: create a list ('feature_importances') where each element is a tuple containing feature name and relative importance\n\nimportances = list(final_model.feature_importances_)\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list(data_dummy[cols_to_keep].columns), importances)]\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\nfeature_importances[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lead_time is the most importance feature!\nThis means that the sooner the reservation is made compared to arrival time, the more likely it will be cancelled. \n\nOther relevant predictors are Average Daily Rate, deposit type, special requests, previous cancellations and booking changes. This information is not hotel-specific! \n\nWe can therefore conclude that this analysis & ML models can be of great use to all hotel managers."},{"metadata":{},"cell_type":"markdown","source":"Note: out of the final 120 predictors included, only the top fifteen features are showed. We can notice that after the few firsts many start to have a very low weight. This means the unlikeliness of the features eliminated with VIF to have a significant impact on predictions.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}