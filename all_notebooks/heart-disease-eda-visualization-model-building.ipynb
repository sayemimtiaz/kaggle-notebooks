{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/heart-disease-combined/Comb_heart_data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimension of the dataset is 929 X 14, i.e. 929 rows and 14 features i.e. columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check continuous columns\ncon=df._get_numeric_data().columns\nprint(\"No of Continuous columns:\",len(df._get_numeric_data().columns),\"\\n\\n\",con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 4 columns are shown as contiuous columns, when in actuality all columns must be numeric, so there are certain values which make them categorical. To make sure what the values are in each column we would perform value_counts() function on each column and determine what exactly is making the columns categorical.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### value_counts on each column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name=list(df.columns)\ncol_name \n#These are all the columns in the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in col_name:\n    print(df[i].value_counts(),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking at the Value counts of each column we can see that there are ? in places where there are no values. So replace these ? special character with NaN's.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.replace(\"?\",np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=\"all\") #All features included, categorical and continuous ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # Only continuous values\n# In this case it would be same as above since there are only continuous columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing and Filling Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msn.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msn.heatmap(df)\n#missingno.heatmap visualizes the correlation matrix about the locations of missing values in columns.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msn.bar(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Nan's have to be filled, for filling the NaN valujes there are many methods to do so, but I am going to use mean (interpolate) and mode to fill the NaN values.\n\nAfter looking at the values of columns having NaN values, I have decided for the columns who have wide range of values I would use interpolate mean method to fill the NaN's and for columns who have very few range of values like here for the Fast blood sugar column we have 2 values 0 i.e. no high blood sugar and 1 i.e. has higher than normal range blood sugar, for such columns I would use the mode method to fill the NaN values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_col=['Fast_bld_sugar','Rest_Ecg','Ex_Angina','Slope','Colored_Vessels','Thalassemia']\nfor col in mode_col:\n    df[col].fillna(df[col].mode()[0],inplace=True)\n    \ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above cell is for thode columns which are to be filled by mode. Now for the rest of the columns I will use the interpolate mean method to fill the NaN values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(df.mean()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list = df.columns\nfor i in column_list:\n    print(\"Values of\",i,\"column\\n\",df[i].unique())\n    print(\"--------------\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change the values of the object i.e. the categorical columns.\n\nThe values will be assigned as given in the UCI site for each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sex'][df['Sex'] == 0] = 'female'\ndf['Sex'][df['Sex'] == 1] = 'male'\n\ndf['ChestPain'][df['ChestPain'] == 1] = 'typical angina'\ndf['ChestPain'][df['ChestPain'] == 2] = 'atypical angina'\ndf['ChestPain'][df['ChestPain'] == 3] = 'non-anginal pain'\ndf['ChestPain'][df['ChestPain'] == 4] = 'asymptomatic'\n\ndf['Fast_bld_sugar'][df['Fast_bld_sugar'] == '0'] = 'lower than 120mg/ml'\ndf['Fast_bld_sugar'][df['Fast_bld_sugar'] == '1'] = 'greater than 120mg/ml'\n\ndf['Rest_Ecg'][df['Rest_Ecg'] == '0'] = 'normal'\ndf['Rest_Ecg'][df['Rest_Ecg'] == '1'] = 'ST-T wave abnormality'\ndf['Rest_Ecg'][df['Rest_Ecg'] == '2'] = 'left ventricular hypertrophy'\n\ndf['Ex_Angina'][df['Ex_Angina'] == '0'] = 'no'\ndf['Ex_Angina'][df['Ex_Angina'] == '1'] = 'yes'\n\ndf['Slope'][df['Slope'] == '1'] = 'upsloping'\ndf['Slope'][df['Slope'] == '2'] = 'flat'\ndf['Slope'][df['Slope'] == '3'] = 'downsloping'\n\n# Values of Colored_Vessels column\n#  ['0' '3' '2' '1']\n# ca: number of major vessels (0-3) colored by flourosopy\n\ndf['Thalassemia'][df['Thalassemia'] == '3'] = 'normal'\ndf['Thalassemia'][df['Thalassemia'] == '6'] = 'fixed defect'\ndf['Thalassemia'][df['Thalassemia'] == '7'] = 'reversable defect'\n\ndf['Target'][df['Target'] == 2] = 1\ndf['Target'][df['Target'] == 3] = 1\ndf['Target'][df['Target'] == 4] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.replace(-0.9,df.mean())\n# In the UCI data repository it is mentioned that missing \n# values are distinguised with -0.9, so we replace this \n# value also with the mean value.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check for missing values once again\nmsn.bar(df,color='Purple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values, now visualization of the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nax = sns.countplot(y=\"Target\", data=df, palette=\"hls\")\nax.set_title('Total percentage of people with and without Heart Disease')\nplt.xlabel('No of people')\n\ntotal = len(df['Target'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage of people with heart disease is 58.3% \n\nPercentage of people without heart disease is 41.7%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"husl\")\nax = sns.countplot(x=\"ChestPain\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with chest pain having heart disease\")\nax.set(xlabel=\"ChestPain\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"bright\")\nax = sns.countplot(x=\"Fast_bld_sugar\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with high to low blood sugar having heart disease\")\nax.set(xlabel=\"Fast_bld_sugar\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"colorblind\")\nax = sns.countplot(x=\"Rest_Ecg\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with normal to high ecg rates having heart disease\")\nax.set(xlabel=\"Rest_Ecg\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"PiYG\")\nax = sns.countplot(x=\"Slope\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with slope level having heart disease\")\nax.set(xlabel=\"Slope\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"RdGy\")\nax = sns.countplot(x=\"Thalassemia\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with fixed, normal, reversable thalassemia rates having heart disease\")\nax.set(xlabel=\"Thalassemia\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Sex'])\nax = sns.countplot(y=\"Sex\", data=df, palette=\"Blues\")\nax.set_title('Percentage of Male and Female')\nplt.xlabel('No of people')\n\ntotal = len(df['Sex'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage of Female Patients: 22.6%\n\nPercentage of Male Patients: 77.4%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"mako_r\")\nax = sns.countplot(x=\"Sex\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People having heart disease according to gender\")\nax.set(xlabel=\"Sex\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(df['Target'])\nsns.set(style=\"darkgrid\",palette=\"PRGn\")\nax = sns.countplot(x=\"Ex_Angina\", hue=\"Target\", data=df) \nax.set_title(\"Percentage of People with Angina having heart disease\")\nax.set(xlabel=\"Ex_Angina\",ylabel=\"Target\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:.1%}'.format(height/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.Age,df.Target).plot(kind=\"bar\",figsize=(20,6),cmap='copper')\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariant Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Rest_bp']=df['Rest_bp'].astype(int)\ndf['Cholestrol']=df['Cholestrol'].astype(int)\ndf['St_Depr']=df['St_Depr'].astype(float)\ndf['Max_Rt']=df['Max_Rt'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Age'],color=\"sienna\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Rest_bp'],color=\"cadetblue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Cholestrol'],color=\"black\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['St_Depr'],color=\"darkred\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Max_Rt'],color=\"coral\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['Sex'])], axis=1)\ndf.drop('Sex',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['ChestPain'])], axis=1)\ndf.drop('ChestPain',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Fast_bld_sugar'])], axis=1)\ndf.drop('Fast_bld_sugar',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Rest_Ecg'])], axis=1)\ndf.drop('Rest_Ecg',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Ex_Angina'])], axis=1)\ndf.drop('Ex_Angina',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Slope'])], axis=1)\ndf.drop('Slope',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Thalassemia'])], axis=1)\ndf.drop('Thalassemia',axis=1,inplace=True)\ndf = pd.concat([df, pd.get_dummies(df['Colored_Vessels'])], axis=1)\ndf.drop('Colored_Vessels',axis=1,inplace=True)\ndf.columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Without using smote","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Target',axis=1)\nY = df['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data in train and test\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.30, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc= StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_train=pd.DataFrame(X_train,columns=X.columns)\nX_valid=sc.transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing the results\n\nfrom yellowbrick.target import ClassBalance\nvisualizer = ClassBalance(labels=[0, 1])\n\nvisualizer.fit(y_train,y_valid)\nvisualizer.ax.set_xlabel(\"Classes\")\nvisualizer.ax.set_ylabel(\"Amount of Occurrences of Class\")\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report\nfrom sklearn.metrics import roc_curve , auc\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_classifier= SVC(probability=True, kernel='rbf')\nsvm_classifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ny_pred_svm= svm_classifier.predict(X_valid)\n\n#Classification Report\nprint(classification_report(y_valid,y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mat_svm = confusion_matrix(y_valid, y_pred_svm, labels = [1,0])\nsns.heatmap(mat_svm.T,  annot=True, fmt='d', cbar=False,\n          xticklabels=['Yes','No'],\n          yticklabels=['Yes','No'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mat_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP=mat_svm[0,0]\nFN=mat_svm[0,1]\nFP=mat_svm[1,0]\nTN=mat_svm[1,1]\nRecall=TP/(TP+FN)\nprint(\"Recall: \",Recall)\nPrecision=TP/(TP+FP)\nprint(\"Precision: \",Precision)\nFM=(2*Recall*Precision)/(Recall+Precision)\nprint(\"F-Measure: \",FM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The receiver operating characteristic (ROC) - SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_proba=svm_classifier.predict_proba(X_valid)[:,1]\nfpr_svm, tpr_svm, _svm = roc_curve(y_valid, y_pred_svm_proba)\nroc_auc=auc(fpr_svm,tpr_svm)\n\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\n\nplt.plot(fpr_svm,tpr_svm,label='ROC curve (area = %0.2f)' %roc_auc)\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\n\nplt.title('SVM ROC curve')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_classifier.score(X_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_classifier.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\nsvccr = ClassificationReport(SVC(probability=True))\nsvccr.fit(X_train, y_train)\nsvccr.score(X_valid, y_valid)\nsvccr.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# initiating the classifier and training the model\n\nclassifier = LogisticRegression(max_iter=1000)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results and calculating the accuracy\n\ny_pred_logit = classifier.predict(X_valid)\nclassifier.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nmatrix_logit = confusion_matrix(y_valid, y_pred_logit)\nprint(matrix_logit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_valid, y_pred_logit))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(matrix_logit,  annot=True, fmt='d', cbar=False,\n          xticklabels=['Yes','No'],\n          yticklabels=['Yes','No'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute precision, recall, F-measure and support\n\nTP=matrix_logit[0,0]\nFN=matrix_logit[0,1]\nFP=matrix_logit[1,0]\nTN=matrix_logit[1,1]\n\nPrecision=TP/(TP+FP)\nprint(\"Precision: \",Precision)\n\nRecall=TP/(TP+FN)\nprint(\"Recall: \",Recall)\n\nFM=(2*Recall*Precision)/(Recall+Precision)\nprint(\"F-Measure: \",FM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc=classifier.predict_proba(X_valid)[:,1]\nfpr,tpr,threshold=roc_curve(y_valid,logit_roc_auc)\nroc_auc=auc(fpr,tpr)\nplt.figure()\n\n# ROC\nplt.plot(fpr,tpr,'g',label='Logistic Regression (AUC = %0.2f)'% roc_auc)\n\n# random FPR and TPR\nplt.plot([0,1],[0,1],'r--')\n\n# title and label\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression-Receiver operating characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('The AUC:', auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\nlrcr = ClassificationReport(LogisticRegression(max_iter=1000))\nlrcr.fit(X_train, y_train)\nlrcr.score(X_valid, y_valid)\nlrcr.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Nearest Neighbours (KNN)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ny_pred_knn = classifier.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nmat_knn=confusion_matrix(y_valid, y_pred_knn)\nprint(mat_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_valid, y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(mat_knn,  annot=True, fmt='d', cbar=False,\n          xticklabels=['Yes','No'],\n          yticklabels=['Yes','No'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP=mat_knn[0,0]\nFN=mat_knn[0,1]\nFP=mat_knn[1,0]\nTN=mat_knn[1,1]\n\nknn_precision=TP/(TP+FP)\nprint(\"KNN Precision: \",knn_precision)\n\nknn_recall=TP/(TP+FN)\nprint(\"KNN Recall: \",knn_recall)\n\nknn_FM=(2*knn_recall*knn_precision)/(knn_recall+knn_precision)\nprint(\"KNN F-Measure: \",knn_FM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##computing fpr and tpr we plot tpr vs fpr\n\nknn_roc_auc=classifier.predict_proba(X_valid)[:,1]\nfpr,tpr,threshold_smote=roc_curve(y_valid,knn_roc_auc)\nroc_auc=auc(fpr,tpr)\nplt.figure()\n\n# ROC\nplt.plot(fpr,tpr,'blue',label='KNN (AUC = %0.2f)'% roc_auc)\n\n# random FPR and TPR\nplt.plot([0,1],[0,1],'r--')\n\n# title and label\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('The AUC (smote):',auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\nknncr = ClassificationReport(KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2))\nknncr.fit(X_train, y_train)\nknncr.score(X_valid, y_valid)\nknncr.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above models SVM, Logistic and KNN have been observed and from the metrics used for classification i.e. the confusion matrix, accuracy score, precision, recall, f-measure and the roc-auc curve we can conclude that SVM works better than the other two models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Using SMOTE\n\n### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.drop(['Target'],axis=1)\narray = df1.values \narrayt=df['Target'].values\nX = array\nY = arrayt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data in train and test\nseed=600\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.30, random_state = seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.target import ClassBalance\nvisualizer = ClassBalance(labels=[0, 1])\n\nvisualizer.fit(y_train,y_valid)\nvisualizer.ax.set_xlabel(\"Classes\")\nvisualizer.ax.set_ylabel(\"Amount of Occurrences of Class\")\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Synthetic Minority Oversampling Technique\n\n### [SMOTE]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import set_printoptions\nfrom sklearn.metrics import confusion_matrix, classification_report\nsmt = SMOTE(random_state=seed)\nX_train_SMOTE, Y_train_SMOTE = smt.fit_sample(X_train, y_train.ravel()) \nprint(X_train_SMOTE.shape)\nprint(Y_train_SMOTE.shape)\nset_printoptions(precision=3)\nprint('\\n Oversampled input: \\n %s' % X_train_SMOTE[0:5,:])\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train_SMOTE == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train_SMOTE == 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_features': [2, 3, 4, 5],\n    'n_estimators': [200, 300, 400, 500]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid,\n                           cv = 3, n_jobs = -1, verbose = 2)\ngrid_result= grid_search.fit(X_train_SMOTE, Y_train_SMOTE)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Pipeline to random forest for 300 trees\nfrom sklearn.pipeline import Pipeline \nnum_trees=300\nmax_features=4\nestimator=[]                                           \nestimator.append(('standardize',StandardScaler()))\nestimator.append(('RF',RandomForestClassifier(\n    n_estimators=num_trees,max_features=max_features))) \nmodel=Pipeline(estimator)      \nkfold=KFold(n_splits=10,random_state=seed)                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1=cross_val_score(model,X_train_SMOTE,Y_train_SMOTE,cv=kfold)  \nprint(result1.mean()*100.0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smt1 = SMOTE(random_state=seed)\nX_train_SMOT1, Y_train_SMOT1 = smt1.fit_sample(X_train, y_train) \nestimator1=[]                                           \nestimator1.append(('standardize',StandardScaler()))\nestimator1.append(('RF',RandomForestClassifier(\n    n_estimators=num_trees,max_features=max_features))) \nmodel1=Pipeline(estimator1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X_train_SMOT1, Y_train_SMOT1)\nresult = model1.score(X_valid, y_valid)\nprint((result)*100.0)\npredictions = model1.predict(X_valid)\n# print classification report \nprint(classification_report(y_valid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nclassifier = RandomForestClassifier(n_estimators=num_trees,max_features=max_features)\nclassifier.fit(X_train, y_train)\ny_pred_rf = classifier.predict(X_valid)\nmat_rf=confusion_matrix(y_valid, y_pred_rf)\nprint(mat_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute precision, recall, F-measure and support\n\nTP=mat_rf[0,0]\nFN=mat_rf[0,1]\nFP=mat_rf[1,0]\nTN=mat_rf[1,1]\n\nPrecision=TP/(TP+FP)\nprint(\"Precision: \",Precision)\n\nRecall=TP/(TP+FN)\nprint(\"Recall: \",Recall)\n\nFM=(2*Recall*Precision)/(Recall+Precision)\nprint(\"F-Measure: \",FM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(mat_rf,  annot=True, fmt='d', cbar=False,\n          xticklabels=['Yes','No'],\n          yticklabels=['Yes','No'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf_roc_auc_smote=classifier.predict_proba(X_valid)[:,1]\nfpr,tpr,threshold_smote=roc_curve(y_valid,rf_roc_auc_smote)\nroc_auc_smote=auc(fpr,tpr)\nplt.figure()\n\n# ROC\nplt.plot(fpr,tpr,'blue',label='Random Forest (AUC = %0.2f)'% roc_auc_smote)\n\n# random FPR and TPR\nplt.plot([0,1],[0,1],'r--')\n\n# title and label\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('The AUC (smote):',auc(fpr,tpr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\nrfcr = ClassificationReport(classifier)\nrfcr.fit(X_train, y_train)\nrfcr.score(X_valid, y_valid)\nrfcr.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"| Algorithms | Train Accuracy | ValidationTest Accuracy | Precision | Recall | F-Measure | AUC Score |\n| --- | --- | --- | --- | --- | --- | --- |\n| KNN (Without SMOTE) | 85.0% | 82.0%  | 0.8039 | 0.7321 | 0.7663 | 0.88 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Logistic Regression (Without SMOTE) | 83.6% | 79.9%  | 0.7641 | 0.7232 | 0.7431 | 0.89 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Support Vector Machine (Without SMOTE) | 90.9% | 86.7%  | 0.9281 | 0.8611 | 0.8933 | 0.91 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Random Forest (With SMOTE) | 90.6% | 88.8%  | 0.9183 | 0.7692 | 0.8372 | 0.95 |","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the above table we can infer that the Random Forest with smote works well at the time of calculation. But for the algorithms without using smote Support Vector Machine works well for the data at the time of calculation. \n\nThe calculated values might differ as seed value was not passed for the non smote analysis data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}