{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from subprocess import check_output\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\ndf1=pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_credits.csv')\ndf2=pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_movies.csv')\ndf1.columns = ['id','tittle','cast','crew']\ndf2= df2.merge(df1,on='id')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that homepage and tagline have nulls values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df2.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the most important variable are budge, genres, homepage, id, keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_matrix(df):\n    from matplotlib import pyplot as plt\n    from matplotlib import cm as cm\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    cmap = cm.get_cmap('jet', 50)\n    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n    ax1.grid(True)\n    plt.title('Movies Features Correlation')\n   # print(df.head(0))\n    labels = df.head()\n   # labels =[\"index\", \"budget\", \"genres\", \"homepage\", \"id\", \"keywords\", \"original_language\", \"original_title\", \"overview\", \"popularity\", \"production_companies\", \"production_countries\", \"release_date\", \"revenue\", \"runtime\", \"spoken_languages\", \"status\", \"tagline\", \"title\", \"vote_average\", \"vote_count\", \"tittle\", \"cast\", \"crew\", \"director\", \"soup\"]\n    ax1.set_xticklabels(labels,fontsize=10, rotation=40)\n    ax1.set_yticklabels(labels,fontsize=10)\n    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n    fig.colorbar(cax)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix(df2 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sperman_correlation(data,cols,col1,col2):\n    plt.rcParams['figure.figsize'] = [16, 6]\n    fig, ax = plt.subplots(nrows=1, ncols=3)\n    ax=ax.flatten()  \n    colors=['#415952', '#f35134', '#243AB5', '#243AB5']\n    j=0\n\n    for i in ax:\n        if j==0:\n            i.set_ylabel(ylabel)\n        i.scatter(data[cols[j]], data['popularity'],  alpha=0.5, color=colors[j])\n        i.set_xlabel(cols[j])\n        i.set_title('Pearson: %s'%data.corr().loc[cols[j]][col1].round(2)+' Spearman: %s'%data.corr(method='spearman').loc[cols[j]][col2].round(2))\n        j+=1\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['vote_average', 'popularity', 'vote_count']\nylabel = 'popularity'\nget_sperman_correlation(df2[cols],cols, 'vote_average', 'vote_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scatter(data):\n    import matplotlib.pyplot as plt\n    import pandas\n    from pandas.plotting import scatter_matrix\n    names = data.head()\n    scatter_matrix(data)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scatter(df2[['popularity', 'vote_average', 'vote_count']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot('vote_average', 'popularity', data=df2, kind=\"hex\")\nsns.jointplot('vote_average', 'popularity', data=df2, kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_plot(df):\n    import matplotlib.pyplot as plt\n    df2.hist(bins=25, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_plot(df2[['budget','id','popularity','vote_average']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inputting and  Clean Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace NaN with an empty string\ndf2['overview'] = df2['overview'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need a mechanism to identify the index of a movie in our metadata DataFrame, given its title."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(df2.index, index=df2['title']).drop_duplicates()\nindices.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clear Data for Genres and Keywords of Movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"#parse json and separate columns with genres of movie\n\ndef endcode_genre(df_movies_genre):\n    import json\n    import ast\n    countGener = df_movies_genre[\"genres\"]\n    print(len(countGener))\n    for index in range(len(countGener)):\n        item = ast.literal_eval(countGener[index])\n        for j in item:\n            j = str(j).replace(\"'\", '\"')\n            json_data = json.loads(j)\n            name = \"genres_\" + str(json_data[\"id\"]) + \"_\" + str(json_data[\"name\"])\n            #print(name)\n            if {name}.issubset(df_movies_genre.columns):\n                df_movies_genre.at[index,name] = 1\n            else:\n                df_movies_genre[name] = 0\n                df_movies_genre.at[index,name] = 1\n    return df_movies_genre\n\n#parse json and separate columns with keywords of movie\n\ndef endcode_keywords(df2):\n    import json\n    import ast\n    df_movies_keyword = df2[['id','genres','keywords']]\n    count = df_movies_keyword[\"keywords\"]\n    \n    for index in range(len(count)):\n       for item in count[index]:\n        name = \"kw_\" + item\n        if {name}.issubset(df_movies_keyword.columns):\n            df_movies_keyword.at[index,name] = 1\n        else:\n            df_movies_keyword[name] = 0\n            df_movies_keyword.at[index,name] = 1\n    return df_movies_keyword\n\n# Returns the list top 3 elements or entire list; whichever is more.\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing/malformed data\n    return []\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Demographic Filtering"},{"metadata":{},"cell_type":"markdown","source":"Demographic Filtering- They offer generalized recommendations to every user, based on movie popularity and/or genre. The System recommends the same movies to users with similar demographic features.The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience."},{"metadata":{},"cell_type":"markdown","source":"We'll be using IMDB's weighted rating (wr) which is given as : Weighted Rating(WR)=((v/(v+m)).R)+((m/(v+m)).C)"},{"metadata":{},"cell_type":"markdown","source":"* C is the mean vote across the whole report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C= df2['vote_average'].mean()\nC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean rating for all the movies is approx 6 on a scale of 10.The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. We will use 90th percentile as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 90% of the movies in the list."},{"metadata":{},"cell_type":"markdown","source":"* m is the minimum votes required."},{"metadata":{"trusted":true},"cell_type":"code","source":"m= df2['vote_count'].quantile(0.9)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_movies = df2.copy().loc[df2['vote_count'] >= m]\nq_movies.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are 481 movies which qualify to be in this list. Now, we need to calculate our metric for each qualified movie. To do this, we will define a function, weighted_rating() and define a new feature score, of which we'll calculate the value by applying this function to our DataFrame of qualified movies:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # Calculation based on the IMDB formula\n    return (v/(v+m) * R) + (m/(m+v) * C)\n\n# Define a new feature 'score' and calculate its value with `weighted_rating()`\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's sort the DataFrame based on the score feature and output the title, vote count, vote average and weighted rating or score of the top 10 movies."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sort movies based on score calculated above\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#Print the top 15 movies\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These systems we find movies that are very popular and they can just be obtained by sorting the dataset by the popularity column."},{"metadata":{"trusted":true},"cell_type":"code","source":"pop= df2.sort_values('popularity', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(10),pop['popularity'].head(10), align='center',\n        color='skyblue')\nplt.gca().invert_yaxis()\nplt.xlabel(\"Popularity\")\nplt.title(\"Popular Movies\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Content based (item-item similarity): overview"},{"metadata":{},"cell_type":"markdown","source":"We need to convert the word vector of each overview. Now we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview.\n\nTerm Frequency , it is the relative frequency of a word in a document and is given as (term instances/total instances). \nInverse Document Frequency is the relative count of documents containing the term is given as log(number of documents/documents with term) The overall importance of each word to the documents in which they appear is equal to TF * IDF\n\nThis will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each column represents a movie, as before.This is done to reduce the importance of words that occur frequently, their significance in computing the final similarity score."},{"metadata":{},"cell_type":"markdown","source":"\n* TF-IDF matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape\n#print(tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cosine similarity"},{"metadata":{},"cell_type":"markdown","source":"We will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate.\nWe have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\ncosine_sim[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Get the index of the movie given its title.\n* Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n* Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n* Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n* Return the titles corresponding to the indices of the top elements."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return df2['title'].iloc[movie_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('This Thing of Ours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Content based (item-item similarity): cast, crew, keywords, genres"},{"metadata":{},"cell_type":"markdown","source":"We are going to build a recommender based on the following metadata: the 3 top actors, the director, related genres and the movie plot keywords.\nFrom the cast, crew and keywords features, we need to extract the three most important actors, the director and the keywords associated with that movie. Right now, our data is present in the form of \"stringified\" lists , we need to convert it into a safe and usable structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parse the stringified features into their corresponding python objects\nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\nfor feature in features:\n    df2[feature] = df2[feature].apply(literal_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[feature].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the director's name from the crew feature. If director is not listed, return NaN\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns the list top 3 elements or entire list; whichever is more.\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing/malformed data\n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define new director, cast, genres and keywords features that are in a suitable form.\ndf2['director'] = df2['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    df2[feature] = df2[feature].apply(get_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the new features of the first 3 films\ndf2[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that our vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to convert all strings to lower case and strip names of spaces\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now in a position to create a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\ndf2['soup'] = df2.apply(create_soup, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the CountVectorizer() instead of TF-IDF. This is because we do not want to down-weight the presence of an actor/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(df2['soup'])\n\n# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n\n# Reset index of our main DataFrame and construct reverse mapping as before\ndf2 = df2.reset_index()\nindices = pd.Series(df2.index, index=df2['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_sim2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now reuse our get_recommendations() function by passing in the new cosine_sim2 matrix as your second argument."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Collaborative filtering: given a data set of user-movie ratings (feel free to change your data set if you need to), predict a movie rating for a given user and movie. ( use SVD or any matrix factorization technique you prefer)"},{"metadata":{},"cell_type":"markdown","source":"# Content based (item-item similarity) - pearson: keywords, genres"},{"metadata":{},"cell_type":"markdown","source":"Based on prior analysis, content-based filtering and collaborative filtering is currently used for general recommender system, however given a metric of correlation, specifically Pearson Correlation works great with finding the correlation between two items â€“ or in this case, the correlation between movies, the correlation between users, and a combination of both significantly\nincreased the performance of a recommender system."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom matplotlib import pyplot\nimport warnings\n#\nwarnings.filterwarnings('ignore')\n#\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean(u):\n    # may use specified_rating_indices but use more time\n    #print(specified_rating_indices(u))\n    specified_ratings = u[specified_rating_indices(u)]\n    #u[np.isfinite(u)]\n    m = sum(specified_ratings)/np.shape(specified_ratings)[0]\n    return m\n\ndef pearson(u, v):\n    mean_u = mean(u)\n    mean_v = mean(v)\n    \n    specified_rating_indices_u = set(specified_rating_indices(u)[0])\n    specified_rating_indices_v = set(specified_rating_indices(v)[0])\n    \n    mutually_specified_ratings_indices = specified_rating_indices_u.intersection(specified_rating_indices_v)\n    mutually_specified_ratings_indices = list(mutually_specified_ratings_indices)\n    \n    u_mutually = u[mutually_specified_ratings_indices]\n    v_mutually = v[mutually_specified_ratings_indices]\n    \n    centralized_mutually_u = u_mutually - mean_u\n    centralized_mutually_v = v_mutually - mean_v\n\n    result = np.sum(np.multiply(centralized_mutually_u, centralized_mutually_v)) \n    result = result / (np.sqrt(np.sum(np.square(centralized_mutually_u))) * np.sqrt(np.sum(np.square(centralized_mutually_v))))\n\n    return result\n\n\n# indices for vector\ndef specified_rating_indices(u):\n    return list(map(tuple, np.where(np.isfinite(u))))\n\n#get similar movies\ndef get_movie_similarity_value_for(movie_index, movie_matrix):\n   # print(movies_matrix.loc[movies_matrix.id == 862])\n   \n    movie_item = np.array(movie_matrix.loc[movies_matrix.id == 862])\n    movie_item = np.delete(movie_item, 0)\n    #print(movie_item)\n    #print(np.array(movie_matrix.iloc[0, 1:]))\n    #print(pearson(movie_matrix.iloc[0, 1:], movie_item))\n    similarity_value = np.array([pearson(np.array(movie_matrix.iloc[i, 1:]), movie_item) for i in range(movie_matrix.shape[0])])\n    return similarity_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSimilarMovieKeywords(movie_id):\n    import ast\n    df_movie_meta=pd.read_csv('../input/the-movies-dataset/movies_metadata.csv')\n    df_keyword =pd.read_csv('../input/the-movies-dataset/keywords.csv')\n    cols = [\"id\",\"genres\", \"title\", \"overview\", \"vote_average\", \"vote_count\"]\n    df_movie_meta = df_movie_meta[cols]\n    df_movie_meta['id'] = df_movie_meta['id'].str.replace('-','')\n    df_movie_meta.dropna(subset=[\"id\"], axis = 0 , inplace= True)\n    df_movie_meta[\"id\"] = df_movie_meta[\"id\"].astype(str).astype(int)\n    df_movie_meta= df_movie_meta.merge(df_keyword,on='id')\n    df_movie_meta.set_index('id')\n\n    # Parse the stringified features into their corresponding python objects\n    #from ast import literal_eval\n    df_movie_meta['keywords'] = df_movie_meta['keywords'].apply(ast.literal_eval)\n    df_movie_meta['keywords'] = df_movie_meta['keywords'].apply(get_list)\n    #print(df_movie_meta.shape())\n    movie_genres_keyword_score = endcode_keywords(df_movie_meta)\n    movie_genres_keyword_score = movie_genres_keyword_score.drop(['keywords'], axis=1)\n    movie_genres_keyword_score = endcode_genre(movie_genres_keyword_score)\n    movie_genres_keyword_score = movie_genres_keyword_score.drop(['genres'], axis=1)\n    movie_genres_keyword_score[\"id\"] = movie_genres_keyword_score[\"id\"].astype(str).astype(int)\n  \n    movie_item = np.array(movie_genres_keyword_score.loc[movie_genres_keyword_score.id == movie_id])\n    movie_item = np.delete(movie_item, 0)\n    similarity_value = np.array([pearson(np.array(movie_genres_keyword_score.iloc[i, 1:]), movie_item) for i in range(movie_genres_keyword_score.shape[0])])\n   \n   # print(similarity_value.count())\n   # print(df_movie_meta.count())\n    df_movie_meta[\"score\"] = similarity_value\n    return df_movie_meta, movie_genres_keyword_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List 10 movies which are similar with the inputed movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"similarMovieList_Keyword_genre, movie_genres_keyword_score =  getSimilarMovieKeywords(28656)\nsimilarMovieList_Keyword_genre = similarMovieList_Keyword_genre[similarMovieList_Keyword_genre.score.notnull()]\nsimilarMovieList_Keyword_genre = similarMovieList_Keyword_genre.sort_values(by='score', ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarMovieList_Keyword_genre","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Draw chart (relationship between genres and movies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n              \ndf = movie_genres_keyword_score[movie_genres_keyword_score.columns[pd.Series(movie_genres_keyword_score.columns).str.startswith('genres')]]\ndf = df.transpose().reset_index().rename(columns={'index':'Genres'})\nmovie_genres_keyword_score.head()\ndf['sum'] = df.sum(axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df.plot(x=\"Genres\", y=\"sum\", kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collaborative filtering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import Reader, Dataset, SVD, evaluate\nreader = Reader()\nratings=pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\ndata.split(n_folds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = SVD()\nevaluate(svd, data, measures=['RMSE', 'MAE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Root Mean Sqaure Error of 0.89 approx. Let us now train on our dataset and arrive at predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data.build_full_trainset()\nsvd.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rating user with user Id 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings[ratings['userId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd.predict(1, 302, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For movie with ID 302, we get an estimated prediction of 2.698. "},{"metadata":{},"cell_type":"markdown","source":"It works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have predicted the movie."},{"metadata":{},"cell_type":"markdown","source":"# Recommendation Movie List for User"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef getMovieListRecommendation(userId):\n    import pandas as pd\n    import numpy as np\n\n    ratings=pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\n    movies_list=pd.read_csv('../input/the-movies-dataset/movies_metadata.csv')\n\n    ratings_df = pd.DataFrame(ratings, columns = ['userId', 'movieId', 'rating', 'timestamp'], dtype = int)\n    movies_df = pd.DataFrame(movies_list, columns = ['id', 'title', 'genres'])\n    movies_df['id'] = movies_df['id']\n    movies_df['id'] = movies_df['id'].str.replace('-','')\n    movies_df.dropna(subset=[\"id\"], axis = 0 , inplace= True)\n    movies_df[\"id\"] = movies_df[\"id\"].astype(str).astype(int)\n    R_df = ratings_df.pivot(index = 'userId', columns ='movieId', values = 'rating')\n    R_df=R_df.fillna(0) \n\n    #R_df = R_df.fillna(R_df.mean()) # Replace the na with column mean (Movie mean)\n\n    R = R_df.values\n    user_ratings_mean = np.mean(R, axis = 1)\n    R_demeaned = R - user_ratings_mean.reshape(-1, 1)\n\n    \n    from scipy.sparse.linalg import svds\n    U, sigma, Vt = svds(R_demeaned, k = 50)\n\n    sigma = np.diag(sigma)\n\n    all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n    preds_df = pd.DataFrame(all_user_predicted_ratings, columns = R_df.columns)\n\n\n\n    def recommend_movies(predictions_df, userID, movies_df, original_ratings_df, num_recommendations=5):\n\n        # Get and sort the user's predictions\n        user_row_number = userID\n        sorted_user_predictions = predictions_df.iloc[userID].sort_values(ascending=False)\n        print (list(pd.DataFrame(sorted_user_predictions).columns))\n\n        # Get the user's data and merge in the movie information.\n        user_data = original_ratings_df[original_ratings_df.userId == (userID)]\n        user_full = (user_data.merge(movies_df, how = 'left', left_on = 'movieId', right_on = 'id').\n                         sort_values(['rating'], ascending=False))\n\n        print ('User {0} has already rated {1} movies.'.format(userID, user_full.shape[0]))\n        print ('Recommending the highest {0} predicted ratings movies not already rated.'.format(num_recommendations))\n\n        # Recommend the highest predicted rating movies that the user hasn't seen yet.\n        recommendations = (movies_df[~movies_df['id'].isin(user_full['movieId'])].\n             merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n                   left_on = 'id',\n                   right_on = 'movieId').\n             rename(columns = {user_row_number: 'Predictions'}).\n             sort_values('Predictions', ascending = False).\n                           iloc[:num_recommendations, :-1]\n                          )\n\n        return user_full, recommendations\n\n\n    already_rated, predictions = recommend_movies(preds_df, userId, movies_df, ratings_df, 10)\n    return predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = getMovieListRecommendation(2)\npredictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use GridSearchCV to find out the best value of paramters for SVD and KNNBasic**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import Dataset, evaluate\nfrom surprise import KNNBasic, Reader, Dataset, SVD, accuracy, Dataset\nimport pandas as pd\nfrom surprise.model_selection import GridSearchCV\n\nfrom surprise.model_selection import train_test_split\n\nreader = Reader()\nratings=pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings.head()\n\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n\n# sample random trainset and testset\n# test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data, test_size=.25)\n\n\n#Grid Search CV with SVD\n\nparam_grid = {'n_epochs': [15, 30], 'lr_all': [0.002, 0.05],\n              'reg_all': [0.1, 0.8]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid Search CV with KNNBasic\n\nparam_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n              'reg_all': [0.4, 0.6]}\ngs = GridSearchCV(KNNBasic, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system/notebook#Content-Based-Filtering\n\nSource code:\nhttps://github.com/nhatpk/Movie-Recommendation-System"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}