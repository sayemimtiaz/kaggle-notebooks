{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Practical Guide on Data Preprocessing using Scikit Learn\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bank-loan2/madfhantr.csv')\ntest = pd.read_csv('../input/bank-loan2/madhante.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Start Data Prepossing Journery</h1>\nIn this chapter you'll learn exactly what it means to preprocess data. You'll take the first steps in any preprocessing journey, including exploring data types and dealing with missing data. "},{"metadata":{},"cell_type":"markdown","source":"### Whats Fundamental  approach data prepossing\n\n* clean data set \n* Prepping data for modeling\n* Modeling in Python requires numerical input\n"},{"metadata":{},"cell_type":"markdown","source":"# Clean Data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#see my data set\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# visulize missing value\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import missingno\nimport missingno as msno\n\n# Plot missingness dendrogram of diabetes\nmsno.dendrogram(df)\n\n# Show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Solve Missing value  problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=df.columns[df.isnull().any()]\n\nprint(df[df.isnull().any(axis=1)][null_columns].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### solve missing value problem\n\n* Gender\n* self_employed\n* loanamount\n* Loan_Amount_Term \n* Credit_History  \n\n### these col avalaible Nan value,\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Fill nan value using mode method"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing value use mode method\ndf['Gender'].fillna((df['Gender'].mode()[0]),inplace=True)\n#fill Self_Employed  \ndf['Self_Employed'].fillna((df['Self_Employed'].mode()[0]),inplace=True)\n\ndf['LoanAmount'].fillna((df['LoanAmount'].mode()[0]),inplace=True)\n\n# Loan_Amount_Term  \ndf['Loan_Amount_Term'].fillna((df['Loan_Amount_Term'].mode()[0]),inplace=True)\n\n# Credit_History  \n\ndf['Credit_History'].fillna((df['Credit_History'].mode()[0]),inplace=True)\n\n\ndf['Dependents'].fillna((df['Dependents'].mode()[0]),inplace=True)\n\n# replacing '+' from Dependent column\ndf['Dependents']=df['Dependents'].apply(lambda x:str(x).replace('+','')if '+' in str(x) else str(x))\ndf['Dependents']=df['Dependents'].apply(lambda x:int(x))\n\n#fill marrird col\ndf['Married'].fillna((df['Married'].mode()[0]),inplace=True)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conform your data is not any null value \n#lets check it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Challange -2 :- understand scalling dataset\n\n\nFeature scaling is the method to limit the range of variables so that they can be compared on common grounds. It is performed on continuous variables. Lets plot the distribution of all the continuous variables in  the data set."},{"metadata":{},"cell_type":"markdown","source":"\n### Lets plot-\ndistribution of all the continuous variables \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.dtypes[(df.dtypes==\"float64\")|(df.dtypes==\"int64\")]\n                        .index.values].hist(figsize=[11,11])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN on non-scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let creat x and y value\nx_with_scale = df[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']]\n\ny_with_scale = df['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Initializing and Fitting a k-NN model\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_with_scale,y_with_scale.values.ravel())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_scale,knn.predict(x_with_scale)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN on scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmin_max=MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_with_scale_minmax=min_max.fit_transform(x_with_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting k-NN on our scaled data set\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_with_scale_minmax,y_with_scale)\n# Checking the model's accuracy\naccuracy_score(y_with_scale,knn.predict(x_with_scale_minmax))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### woww\n\nGreat !! Our accuracy has increased from 72% to 83%. This means that some of the features with larger range were dominating the prediction outcome in the domain of distance based methods(kNN).\n\nIt should be kept in mind while performing distance based methods we must attempt to scale the data, so that the feature with lesser significance might not end up dominating the objective function due to its larger range. In addition, features having different unit should also be scaled thus providing each feature equal initial weightage and at the end we will have a better prediction model."},{"metadata":{},"cell_type":"markdown","source":"### - Apply  LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='liblinear', penalty='l1')\nlog_reg.fit(x_with_scale,y_with_scale.values.ravel())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_scale,log_reg.predict(x_with_scale)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Apply  LogisticRegression with  scalling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg_1 = LogisticRegression(solver='liblinear', penalty='l1')\nlog_reg_1.fit(x_with_scale_minmax,y_with_scale.values.ravel())\n\n# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_scale,log_reg_1.predict(x_with_scale_minmax)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Challange no-2 - Feature Standardization\n\nNow, I’ll be introducing a new concept here called standardization. Many machine learning algorithms in sklearn requires standardized data which means having zero mean and unit variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import library\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the scaler\nss = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let creat x and y value\nx_with_stand = df[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']]\n\ny_with_stand = df['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the scaler to the DataFrame subset\nx_with_stand_scaled = ss.fit_transform(x_with_stand)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## -Apply Support vector machine with Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import SVC classifier\nfrom sklearn.svm import SVC\n\n\n# import metrics to compute accuracy\nfrom sklearn.metrics import accuracy_score\n\n\n# instantiate classifier with default hyperparameters\nsvc_scale=SVC() \n\n# fit classifier to training set\nsvc_scale.fit(x_with_stand_scaled,y_with_stand)\n\n# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_stand,svc_scale.predict(x_with_stand_scaled)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Again use logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_stand=LogisticRegression(penalty='l2',C=.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_stand.fit(x_with_stand_scaled,y_with_stand)\n\n# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_stand,log_stand.predict(x_with_stand_scaled)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting k-NN on our scaled data set\nknn_stand=KNeighborsClassifier(n_neighbors=5)\nknn_stand.fit(x_with_stand_scaled,y_with_stand)\n# Checking the model's accuracy\nprint(\"\\nAccuracy score on test set :\", accuracy_score(y_with_stand,knn_stand.predict(x_with_stand_scaled)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challange -3-Feature Engineering\n\nIn this section you'll learn about feature engineering. You'll explore different ways to create new, more useful, features from the ones already in your dataset. You'll see how to encode, aggregate, and extract information from both numerical and textual features"},{"metadata":{},"cell_type":"markdown","source":"### Identifying areas for feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interputation\n our data set has other features too such as Gender, Married, Dependents, Self_Employed and Education. All these categorical features have string values. For example, Gender has two levels either Male or Female. Lets feed the features in our logistic regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#set target and feture value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1 = df.drop(['Loan_Status'], axis=1)\n\ny_1 = df['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Initializing and Fitting a k-NN model\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_1,y_1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We got an error saying that it cannot convert string to float. So, what’s actually happening here is learners like logistic regression, distance based methods such as kNN, support vector machines, tree based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners.\n\n\n# Encoding categorical variables - binary\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# for each column\nfor c in list(df.columns):\n    \n    # get a list of unique values\n    n = df[c].unique()\n    \n    # if number of unique values is less than 30, print the values. Otherwise print the number of unique values\n    if len(n)<5:\n        print(c)\n        print(n)\n    else:\n        print(c + ': ' +str(len(n)) + ' unique values')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# interputaion"},{"metadata":{},"cell_type":"markdown","source":"### Encoding binary features-step-1\n\n**Recasting data types is an important part of data preprocessing.**\n* In this exercise you will assign the values 1 to 'y' and 0 to 'N' to the '**Loan_status**' columns\n* and '**Married**'columns assign the values 1 to 'yes' and 0 to 'no' respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace'no' with 0 and 'yes' with 1 in 'Vmail_Plan'\ndf['Loan_Status'] = df['Loan_Status'].replace({'N': 0 , 'Y': 1})\n\n# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\ndf['Married'] = df['Married'].replace({'No': 0 , 'Yes': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace 'no' with 0 and 'yes' with 1 in 'Churn'\ndf['Self_Employed'] = df['Self_Employed'].replace({'No': 0 , 'Yes': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding binary features-step-2-Label_enconder\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing LabelEncoder and initializing it\nfrom sklearn.preprocessing import LabelEncoder\n\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the \"Accessible\" column\ndf[\"Education_enc\"] = enc.fit_transform(df[\"Education\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare the two columns\nprint(df[[\"Education_enc\", \"Education\"]].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nex endconder\n\n# Importing LabelEncoder and initializing it\nfrom sklearn.preprocessing import LabelEncoder\n\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the \"Accessible\" column\ndf[\"Gender_enc\"] = enc.fit_transform(df[\"Gender\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Droup unnecessary column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save dataset\n#filtered_loans.to_csv(\"processed_data/cleaned_loans_2007.csv\",index=False)\n\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the unnecessary features\ndf = df.drop(df[['Loan_ID','Education','Gender',]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One Hot coding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get dummies and save them inside a new DataFrame\nProperty_Area = pd.get_dummies(df.Property_Area)\n\n# Take a quick look to the first 5 rows of the new DataFrame called departments\nprint(Property_Area.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy trap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the old column \"department\" as you don't need it anymore\ndf = df.drop(\"Property_Area\", axis=1)\n\n# Join the new dataframe \"Property_Area\" to your  dataset: \ndf = df.join(Property_Area)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply Logistic Regression****"},{"metadata":{},"cell_type":"markdown","source":"### Separating Target and Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the target and features\n\n# Choose the dependent variable column (Loan_Status) and set it as target\ntarget = df.Loan_Status\n\n# Drop column Loan_Status and set everything else as features\nfeatures = df.drop(\"Loan_Status\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nse = pd.DataFrame(scale.fit_transform(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features= pd.concat([features, se], axis=1)\nfeatures.drop(['Married',\n         'Dependents',\n         'Self_Employed',\n         'ApplicantIncome',\n         'CoapplicantIncome',\n         'LoanAmount',\n         'Loan_Amount_Term',\n         'Credit_History',\n         'Education_enc',\n         'Gender_enc',\n         'Rural',\n         'Semiurban',\n         'Urban'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.rename(columns={0:'Married',\n                          1:'Dependents',\n                          2:'Self_Employed',\n                          3:'ApplicantIncome',\n                          4:'CoapplicantIncome',\n                          5:'LoanAmount',\n                          6:'Loan_Amount_Term',\n                          7:'Credit_History',\n                          8:'Education_enc',\n                          9:'Gender_enc',\n                          10:'Rural',\n                          11:'Semiurban',\n                          12:'Urban'}, inplace=True)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliting employee data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the function for splitting dataset into train and test\nfrom sklearn.model_selection import train_test_split\n\n# Use that function to create the splits both for target and for features\n# Set the test sample to be 25% of your observations\ntarget_train, target_test, features_train, features_test = train_test_split(target,features,test_size=0.25,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Apply DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Initialize it and call model by specifying the random_state parameter\nmodel = DecisionTreeClassifier(random_state=42)\n\n# Apply a decision tree model to fit features to the target\nmodel.fit(features_train, target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply a decision tree model to fit features to the target in the training set\nmodel.fit(features_train,target_train)\n\n# Check the accuracy score of the prediction for the training set\nmodel.score(features_train,target_train)*100\n\n# Check the accuracy score of the prediction for the test set\nmodel.score(features_test,target_test)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Apply  KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting k-NN on our scaled data set\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(features_train,target_train)\n# Checking the model's accuracy\naccuracy_score(target_train,knn.predict(features_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## -Apply LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg_all = LogisticRegression(solver='liblinear', penalty='l1')\nlog_reg_all.fit(features_train,target_train)\n\n\n# Checking the performance of our model on the testing data set\nprint(\"\\nAccuracy score on test set :\", accuracy_score(target_train,log_reg_all.predict(features_train)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nSo, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}