{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n## Features\n- Date - The date of the observation\n- AveragePrice - the average price of a single avocado\n- type - conventional or organic\n- year - the year\n- Region - the city or region of the observation\n- Total Volume - Total number of avocados sold\n- 4046 - Total number of avocados with PLU 4046 sold\n- 4225 - Total number of avocados with PLU 4225 sold\n- 4770 - Total number of avocados with PLU 4770 sold\n\n## Purpose\nThe purpose of this notebook is to perform a robust analysis of the data. It will include: data cleaning, descriptive analysis, epxloratory data analysis, data analysis with data wranging, classification and regression models.\n\n## Table of Contents\n1.  [Data Loading and Data Cleaning](#1.-Data-Loading-and-Data-Cleaning)\n2. [Descriptive Analysis](#2.-Descriptive-Analysis)\n3. [EDA](#3.-EDA)\n4. [Organic vs Conventional](#4.-which-type-sells-better?-which-one-is-expensier?)\n5. [Are organic avos gaining popularity?](#5.-Are-organic-avos-gaining-popularity?)\n6. [Seasonality. When can I find more avos? and cheaper?](#6.-Seasonality.-When-can-I-find-more-avos?-and-cheaper?)\n7. [Regions. Where can I find more avos? and cheaper?](#7.-Regions.-Where-can-I-find-more-avos?-and-cheaper?)\n8. [Classification models. Predicting the type of avocado](#8.-Classification-models.-Predicting-the-type-of-avocado)\n9. [Regression models](#9.-Regression-models)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data manipulation\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import DivergingNorm\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n# preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\n\n# Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Regression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nnp.warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Loading and Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo = pd.read_csv(\"../input/avocado-prices/avocado.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#avo.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(avo.head(5))\nprint(avo.info())\nprint(avo.describe())\nprint(\"\\n\", avo.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first column gives reduntant index data, so lets drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.drop('Unnamed: 0', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's begin by taking a look at Null's values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(avo.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfuly, we don't have any null value. Lets continue with the descriptive analysis and, as we take a closer look to the data, see if we find strange values that we can drop."},{"metadata":{},"cell_type":"markdown","source":"# 2. Descriptive Analysis\nIn this section we will take a closer look at the data, make distributions, further clean the data, calculate initial basic stats and start analysing the dataset.\nFor this, we are going to take a look at each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.info()\navo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Strings\n\nnow let's take a look at the columns with data type 'object'"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.select_dtypes('object').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(avo['type'].value_counts())\nsns.countplot('type', data=avo, palette='Set3')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have two classes which are almost perfectly distributed. The data is balanced and could be used as a classifier in a machine learning algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(avo['region'].value_counts())\nprint('\\n', 'There are:', len(avo['region'].unique()), 'unique values in the feature')\nsns.countplot('region', data=avo, palette='Set3')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The Features have 54 unique values which are perfectly distributed through the dataset. \n- For machine learning purposes, the data could be transformed with the OneHotEncoder formula to have a larger variaty of features to build a machine learning model\n- The data can be used as well to analyze the price behaviour and quantity sold in each region"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers = list(avo.select_dtypes(['float64', 'int64']).keys())\n\n# removing years\nnumbers.remove('year')\n\navo[numbers].hist(figsize=(20,10), color='green', edgecolor='white')\n\nplt.show()\n\ndisplay(avo[numbers].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AveragePrice.** \n- Is the most normal distribution. Mean and median are really closed, which means the distribution is not severly influenced by outliers. Still, it is a bit skewed to the right, the mean being bigger than the median reflects that.\n**Remaining features**\n- The remaining features are severely influenced by outliers, most of the values are located in the first bin of the histograms and the meean is way bigger than the median. \n- These features seem to follow the same distribution, which makes sense since the information (quantity sold) is similar\n\nLets take the outliers out of the quantities to see if we can find a more normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo_o = avo[avo['Total Volume']<50000]\navo_o[numbers].hist(figsize=(20,10), color='green', edgecolor='white')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These kind of distributions, where most of the values are located in lower values and then descends, is really common and could be represented in a different way through log formulas to make it more 'normal' and useful for a model, like regression models, without getting rid of outliers.\n\nA example below with Total Volume.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"TotalLog = np.log(avo['Total Volume'] + 1)\nTotalLog.hist(color='green', edgecolor='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2.3 Dates\n\nWe have two columns which are 'Date' and 'year', being year the extracted year of date. To make the analysis easier, let's extract day and month out of 'Date' and see each value separately. That way, we are also going to have two more potentially usefull columns: day and month"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avo['Date'] = avo['Date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\navo['month'] = avo['Date'].dt.month\navo['day'] = avo['Date'].dt.day\n# monday = 0\navo['day of week'] = avo['Date'].dt.dayofweek\ndates = ['year', 'month', 'day', 'day of week']\navo[dates]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2, figsize=(20,10))\n\nsns.countplot('year', data=avo, ax=ax[0,0], palette='BuGn_r')\nsns.countplot('month', data=avo, ax=ax[0,1], palette='BuGn_r')\nsns.countplot('day', data=avo, ax=ax[1,0], palette='BuGn_r')\nsns.countplot('day of week', data=avo, ax=ax[1,1], palette='BuGn')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**year**\n- 2015, 2016, 2017 have almost the same values\n- 2018 is the lowest, the should have ended in the beggining of 2018\n\n**month**\n- Shows a descending pattern, This could be because of the same reason as year: 2018 ended in the begging of the year and, therefore, the first months have more entries\n\n**day & day of week**\n- We can see that the day chart has a repeating trend, and this is because of the day that the data was always recorded: day 6 (Sunday). \n- The data was, therefore, recorded weekly, 'day of week' becomes redundant and we can eliminate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.drop('day of week', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Descriptive analysis conclusions\n- 'type' has to categories and is balanced, could be used as a classifier in model building\n- 'region' has 54 unique values and is perfectly balanced, could be hot encoded for model building\n- 'avg' price shows and pretty normal distribution and looks tentative for target variable for regression model\n- units sold columns show similar data which is similarly distributed, log formulas could be used to increase model performance\n- 'dates' is evenly distributed till 2018 and shows that the data was recorded on a weekly basis every Sunday"},{"metadata":{},"cell_type":"markdown","source":"Let's begin analising and exploring the data to get insights out of data wrangling and a more clear idea of how the model is going to be"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. EDA\n## 3.1 Correlations\n- Let's begging by looking at correlation so we can represent our data in a scatterplot along with the type of avocado\n- We are going to take a look at both the dataset with outliers and without outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20,8))\n\navo_o = avo[avo['Total Volume']<50000]\n\nsns.heatmap(avo.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, ax=ax[0])\nax[0].set_title('With outliers', fontsize=20)\n\nsns.heatmap(avo_o.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, ax=ax[1])\nax[1].set_title('Without outliers', fontsize=20)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We are going to take the strongest relationship out of the volume variable and the strongest out of a date variable\n- We are going to take the relationships with AveragePrice, out of both heatmaps, since is our target variable for the regression model,\n- We are going to color the scatterplot with the type of avo since is our target variable for our classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(20,10))\n\nsns.scatterplot(x='4046', y='AveragePrice', data=avo, hue='type', ax=ax[0,0])\nsns.scatterplot(x='Large Bags', y='AveragePrice', data=avo_o, hue='type', ax=ax[0,1])\nsns.scatterplot(x='month', y='AveragePrice', data=avo, hue='type', ax=ax[1,0])\nsns.scatterplot(x='month', y='AveragePrice', data=avo_o, hue='type', ax=ax[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- An important insight here is that we can't take the outliers out since all of them correspond to the conventional type, which means that conventional avocados sell way more than organic avocados\n- There doesn't seem to be a relationship between month and AveragePrice, what we can see in this graph is that the average price of conventional avocados is way smaller that the organic. **We are going to take a closer look at this in the further sections**\n- There is an expected decreasing trend for both types: the more units were sold, the less the average price is, **we are going to take a closer look at this later as well**.\n- Perhaps a better way of representing the data is not by taking out the outliers but by normilizing the data, let's try that now with AveragePrice and 4046"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = Normalizer()\nscaler.fit(avo[['4046', 'AveragePrice']].values)\navo['4046_scaled'] = scaler.transform(avo[['4046', 'AveragePrice']].values)[:,0]\navo['AveragePrice_scaled'] = scaler.transform(avo[['4046', 'AveragePrice']].values)[:,1]\n\nsns.regplot(x='4046_scaled', y='AveragePrice_scaled', data=avo, color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beautiful :) We now know that both the regression and classification is possible since there is a clear tendency"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Dates\nCan we predict price or volume doing a time seriess analysis?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize=(23,10))\n\navo['year_month'] = avo['Date'].dt.to_period('M')\ngrouped = avo.groupby('year_month')[['AveragePrice', 'Total Volume']].mean()\n\nax[0].plot(grouped.index.astype(str), grouped['AveragePrice'])\nax[0].tick_params(labelrotation=90)\nax[0].set_ylabel('AveragePrice')\n\n\nax[1].plot(grouped.index.astype(str), grouped['Total Volume'])\nax[1].tick_params(labelrotation=90)\nax[1].set_ylabel('Total Volume')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the graphic we can tell that the, first of all, average price and total volume move in different direction\n- Total volume has a pike at the beggining of the year. On the other hand, average price drops at the beggining of the year\n- These drops and pikes are a sign of seasonality and that could help in forecasting\n- We will dig deeper into these seasonality in further sections"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 EDA Conclusions\n- Conventional avocados sell way more than organic avocados and cost less. Therefore, Total volume, along with other volume variables, and average price, will work well to predict our target variable, type, in our classification model\n- Average price and total volume move in different directions, this will come in handy when doing a regression analysis over our target variable, which is average price\n- In the time series exploration, we see that there is a pike in total volume and a drop in prices at the beggining of the month, hinting for seasonality and forecasting possibilities"},{"metadata":{},"cell_type":"markdown","source":"# 4. which type sells better? which one is expensier?\nWe now from previous sections that organic is expensier and sell less, but let's get into the numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14,5))\n\nsns.barplot(x='type', y='AveragePrice', data=avo, palette='Set3', ax=ax[0])\nsns.barplot(x='type', y='Total Volume', data=avo, palette='Set3', ax=ax[1], estimator=sum, ci=None)\nplt.show()\n\ndisplay(avo.groupby('type')['AveragePrice'].mean())\ndisplay(avo.groupby('type')['Total Volume'].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So we see that convential is cheaper than organic, but momore shockingly, conventional destroyed organic sells\n\nSo conventional avos are performing quite well and organic are being left behind, but is organic at least geaining popularity?"},{"metadata":{},"cell_type":"markdown","source":"# 5. Are organic avos gaining popularity?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize=(23,12))\nfig.tight_layout(pad=8)\n\n\ngroup = avo.groupby(['type', 'year_month'])['Total Volume'].sum()\n\norganic = group['organic']\norganic = pd.DataFrame(organic)\norganic['Total Volume % change'] = np.round(organic['Total Volume'].pct_change() * 100, 2)\n\nconventional = group['conventional']\nconventional = pd.DataFrame(conventional)\nconventional['Total Volume % change'] = np.round(conventional['Total Volume'].pct_change() * 100, 2)\n\nnorm = DivergingNorm(vmin=organic['Total Volume % change'].min(), vcenter=0, vmax=organic['Total Volume % change'].max())\ncolors = [plt.cm.RdYlGn(norm(c)) for c in organic['Total Volume % change']]\nsns.barplot(x=organic.index, y=organic['Total Volume % change'], data=organic, ax=ax[0], palette=colors)\n\nnorm = DivergingNorm(vmin=conventional['Total Volume % change'].min(), vcenter=0, vmax=conventional['Total Volume % change'].max())\ncolors = [plt.cm.RdYlGn(norm(c)) for c in conventional['Total Volume % change']]\nsns.barplot(x=conventional.index, y=conventional['Total Volume % change'], data=conventional, ax=ax[1], palette=colors)\n\n\nax[0].tick_params(labelrotation=90)\nax[0].set_title('Organic Percentage Change in Sales', fontsize=15)\n\nax[1].tick_params(labelrotation=90)\nax[1].set_title('Conventional Percentage Change in Sales', fontsize=15)\n\nplt.show()\n\nconventional['Total Volume % change'].mean()\nprint(\"The sum of percentage change of Organic is: {}\".format(np.around(organic['Total Volume % change'].sum(), 2)))\nprint(\"The sum of percentage change of Conventional is: {}\".format(np.around(conventional['Total Volume % change'].sum(), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- Is hard to tell from the graphic alone but if we sum every percentage change we find that organic has a bigger growth overall with 200.48 against 137.02 of conventional.\n\n Let's add some business strategy concepts to refine strategy and conclusions here.\n \n The BCG matix is a model that evaluates how a business is performing according its growth and market share. It has for dimensions:\n 1. Dogs: These are products with low growth or market share.\n 2. Question marks or Problem Child: Products in high growth markets with low market share.\n 3. Stars: Products in high growth markets with high market share.\n 4. Cash cows: Products in low growth markets with high market share.\n \n- Organic might be having way smaller sales than conventional, but its growing rate (higher than conventional) is a good sign to keep producing the organic avos and it already has a market. This is a healthy indicator for businesses.Then, **organic is a Star in the BCG matrix**. A suggestion would then be to have a business growth strategy with them: technologies and methods that produce more and cheaper, promotion and importations.\n- Conventional avos are too succesfull and have an already stablished business infrastructure. Therefore, **conventional are Cash cows in the BCG matrix**, and businesses should keep producing them at the same or higher rate."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 6. Seasonality. When can I find more avos? and cheaper?\nSection 3.2 gave us a seasonality clue: more avos are being produce at the beggining of the year.\nLet's take a closer look and confirm this.\n\nThe approach will be getting the quarters and the average Total Volume and price of each quarter"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(12,5))\n\navo['quarter'] = avo['Date'].dt.quarter\n\n\nsns.barplot(x='quarter', y='Total Volume', data=avo, palette='Greens_r', ci=None, ax=ax[0])\nsns.barplot(x='quarter', y='AveragePrice', data=avo, palette='Greens_r', ci=None, ax=ax[1])\n\n\nplt.show()\n\nquarter = avo.groupby('quarter')[['Total Volume', 'AveragePrice']].mean()\ndisplay(quarter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- So we see that in the first quarter of the year sales are better than in other quarters and prices are the lowest.\n- After the first quarter, sales decrease and prices grow. Given the popularity of avos, businesses should be considering importing more avos when they are not produced in the country, a big oportunity for business-men from both countries."},{"metadata":{},"cell_type":"markdown","source":"# 7. Regions. Where can I find more avos? and cheaper?"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.1 Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,5))\n\nregionP = avo.groupby('region')['AveragePrice'].mean()\n\nexpensive = regionP.sort_values(ascending = False).iloc[:10]\ncheap = regionP.sort_values().iloc[:10]\n\nsns.barplot(x='AveragePrice', y='region', data = avo, order=expensive.index, ci=None, palette='Greens_r', ax=ax[0])\nsns.barplot(x='AveragePrice', y='region', data = avo, order=cheap.index, ci=None, palette='Greens_r', ax=ax[1])\n\nplt.show()\n\ncheap = pd.DataFrame(cheap).reset_index()\nexpensive = pd.DataFrame(expensive).reset_index()\n\nprint('the most expensive avocados can be found in {} '.format(list(expensive.iloc[:5,0])))\nprint('the cheapest avocados can be found in {} '.format(list(cheap.iloc[:5,0])))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.2 quantity"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,5))\n\navoStates = avo[avo['region'] !='TotalUS']\n\nregionV = avoStates.groupby('region')['Total Volume'].sum()\n\nmost = regionV.sort_values(ascending = False).iloc[:10]\nleast = regionV.sort_values().iloc[:10]\n\nsns.barplot(x='Total Volume', y='region', data = avoStates, order=most.index, ci=None, palette='Greens_r', ax=ax[0])\nsns.barplot(x='Total Volume', y='region', data = avoStates, order=least.index, ci=None, palette='Greens_r', ax=ax[1])\n\nplt.show()\n\nmost = pd.DataFrame(most).reset_index()\nleast = pd.DataFrame(least).reset_index()\n\nprint('States with the the biggest demand are {} '.format(list(most.iloc[:5,0])))\nprint('States with the least demand are {} '.format(list(least.iloc[:5,0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Classification models. Predicting the type of avocado"},{"metadata":{},"cell_type":"markdown","source":"## 8.1 Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"avoTree = avo.drop(['Date', 'region', '4046_scaled', 'AveragePrice_scaled', 'year_month'], axis=1)\n\ntarget = avoTree['type']\nfeatures = avoTree.drop(['type'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features.values, target.values, random_state=0)\n\ntree = DecisionTreeClassifier(max_depth=7, random_state=0).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(tree.score(X_test, y_test)))\n\nprint(\"feature importances:\")\nfeature_importance = pd.DataFrame(features.keys(), tree.feature_importances_)\nprint(feature_importance)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.2 Knn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# knn\n\navo_model = avo.drop(['Date', 'region', '4046_scaled', 'AveragePrice_scaled', 'year_month'], axis=1)\n\ntarget = avo_model['type']\nfeatures = avo_model.drop(['type'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features.values, target.values, random_state=0)\n\n\nclf = KNeighborsClassifier(n_neighbors=12)\nclf.fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(clf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.3 SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC\n\nsvc = LinearSVC(C=211).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(svc.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The best model is Knn with a training score of 0.97 and a test score of 0.97"},{"metadata":{},"cell_type":"markdown","source":"# 9. Regression models\nIn this section we are going to try to predict the price of the avos.\n\n## 9.1 Decision Tree Regressor\nThe tree models usually don't require preprocessing, so we are going to beging with this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo_model = avo.drop(['Date', '4046_scaled', 'AveragePrice_scaled', 'year_month', 'type', 'region'], axis=1)\ntarget = avo_model['AveragePrice']\nfeatures = avo_model.drop(['AveragePrice'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features.values, target.values, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(max_depth=14, random_state=0).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(tree.score(X_test, y_test)))\n\nprint(\"\\n\", \"feature importances:\")\nfeature_importance = pd.DataFrame(list(features.keys()), tree.feature_importances_)\nprint(feature_importance.sort_index(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.2 Linear models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression().fit(X_train, y_train)\nprint('Linear Regression')\nprint(\"training set score : {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n\nprint('Ridge')\nridge = Ridge().fit(X_train, y_train)\nprint(\"\\n\", \"training set score : {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n\nprint('Lasso')\nlasso = Lasso().fit(X_train, y_train)\nprint(\"\\n\", \"training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used:\", np.sum(lasso.coef_ != 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model performs quite bad with the given features. From previous sections, we know that a better way to represent the volume data is applying a log formula or a normalizer. let's try both methods and see if we can get a better prediciton"},{"metadata":{},"cell_type":"markdown","source":"### log"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo_model = avo.drop(['Date', '4046_scaled', 'AveragePrice_scaled', 'year_month', 'type', 'region'], axis=1)\ntarget = avo_model['AveragePrice']\nfeatures = avo_model.drop(['AveragePrice'], axis=1)\n\nfeatures.iloc[:,0:7] = np.log(features.iloc[:,0:7] + 1)\nX_train, X_test, y_train, y_test = train_test_split(features.values, target.values, random_state=0)\n\nlr = LinearRegression().fit(X_train, y_train)\nprint('Linear Regression')\nprint(\"training set score : {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n\nprint('Ridge')\nridge = Ridge().fit(X_train, y_train)\nprint(\"\\n\", \"training set score : {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n\nprint('Lasso')\nlasso = Lasso().fit(X_train, y_train)\nprint(\"\\n\", \"training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used:\", np.sum(lasso.coef_ != 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"way better, let's add dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"avo_model = avo.drop(['Date', '4046_scaled', 'AveragePrice_scaled', 'year_month'], axis=1)\navo_model = pd.get_dummies(avo_model)\n\ntarget = avo_model['AveragePrice']\nfeatures = avo_model.drop(['AveragePrice'], axis=1)\n\nfeatures.iloc[:,0:7] = np.log(features.iloc[:,0:7] + 1)\nX_train, X_test, y_train, y_test = train_test_split(features.values, target.values, random_state=0)\n\nlr = LinearRegression().fit(X_train, y_train)\nprint('Linear Regression')\nprint(\"training set score : {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n\nprint('Ridge')\nridge = Ridge().fit(X_train, y_train)\nprint(\"\\n\", \"training set score : {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n\nprint('Lasso')\nlasso = Lasso().fit(X_train, y_train)\nprint(\"\\n\", \"training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used:\", np.sum(lasso.coef_ != 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We just got a 0.1 better test score than the descision tree with a way lower training score, which is a good sign that we are not over fitting.\n- With some feature engineering we were able to make the linear models better than the decision tree, except for Lasso where we never were able to increase the test score more than 1"},{"metadata":{},"cell_type":"markdown","source":"The notebook is a bit long but i tried to be as concise as possible considering that I wanted to deliver the most robust analysis i could do.\nThank you ver much for reading my kernel and please upvote if you find it useful :) a vote to a beginner never hurts and motivates me to keep learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}