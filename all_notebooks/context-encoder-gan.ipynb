{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Подключение к tensorboared","metadata":{}},{"cell_type":"code","source":"# using the hard way \n# run tensorboard in kaggle server. and operate using public url\n\n# download the files for ngrok\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n\n# Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\nimport os\nimport multiprocessing\n\n\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir ./runs/ --host 0.0.0.0 --port 6006 &\",\n                        \"./ngrok http 6006 &\"\n                        ]]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:41.151207Z","iopub.execute_input":"2021-07-12T09:42:41.151536Z","iopub.status.idle":"2021-07-12T09:42:43.374191Z","shell.execute_reply.started":"2021-07-12T09:42:41.151509Z","shell.execute_reply":"2021-07-12T09:42:43.372425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:43.382821Z","iopub.execute_input":"2021-07-12T09:42:43.388691Z","iopub.status.idle":"2021-07-12T09:42:44.657601Z","shell.execute_reply.started":"2021-07-12T09:42:43.388641Z","shell.execute_reply":"2021-07-12T09:42:44.655897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom torchvision.utils import save_image, make_grid\n\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:45.576364Z","iopub.execute_input":"2021-07-12T09:42:45.576726Z","iopub.status.idle":"2021-07-12T09:42:47.016844Z","shell.execute_reply.started":"2021-07-12T09:42:45.576693Z","shell.execute_reply":"2021-07-12T09:42:47.015889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of epochs of training\nn_epochs = 50\n# size of the batches\nbatch_size = 64\n# name of the dataset\ndataset_name = \"../input/val-256/val_256\"\n# adam: learning rate\nlr = 10**-4\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# number of cpu threads to use during batch generation\nn_cpu = 8\n# size of each image dimension\nimg_size = 128\n# size of random mask\nmask_size = 16\n# number of image channels\nchannels = 3\n\n# Calculate output dims of image discriminator (PatchGAN)\npatch_h, patch_w = int(mask_size / 2 ** 3), int(mask_size / 2 ** 3)\npatch = (1, patch_h, patch_w)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:47.021849Z","iopub.execute_input":"2021-07-12T09:42:47.024015Z","iopub.status.idle":"2021-07-12T09:42:47.033783Z","shell.execute_reply.started":"2021-07-12T09:42:47.023973Z","shell.execute_reply":"2021-07-12T09:42:47.032831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Class","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, img_size=128, mask_size=16, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        # self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n\n    def apply_random_mask(self, img):\n        \"\"\"Randomly masks image\"\"\"\n        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if int(transforms.ToTensor()(img).shape[0]) == 1:\n            img = transforms.Grayscale(num_output_channels=3)(img)\n        \n        img = transforms.Compose([transforms.ToTensor(),\n                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])(img)\n\n        masked_img, aux = self.apply_random_mask(img)\n\n        return img, masked_img, aux\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:50.31939Z","iopub.execute_input":"2021-07-12T09:42:50.319809Z","iopub.status.idle":"2021-07-12T09:42:50.340342Z","shell.execute_reply.started":"2021-07-12T09:42:50.319761Z","shell.execute_reply":"2021-07-12T09:42:50.339345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = [\n     transforms.Resize((128, 128)),\n     transforms.ColorJitter(hue=.50, saturation=.50),\n     transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     ]\nos.makedirs(\"aug_data\", exist_ok=True)\naugmentation_data = ImageDataset(dataset_name, transforms_=transform)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:50.716968Z","iopub.execute_input":"2021-07-12T09:42:50.717537Z","iopub.status.idle":"2021-07-12T09:42:52.116752Z","shell.execute_reply.started":"2021-07-12T09:42:50.717493Z","shell.execute_reply":"2021-07-12T09:42:52.115804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = [\n     transforms.Resize((128, 128)),\n     transforms.ColorJitter(hue=.20, saturation=.20),\n     transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     ]\nos.makedirs(\"aug_data\", exist_ok=True)\naugmentation_data_1 = ImageDataset(dataset_name, transforms_=transform)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.119361Z","iopub.execute_input":"2021-07-12T09:42:52.119911Z","iopub.status.idle":"2021-07-12T09:42:52.257927Z","shell.execute_reply.started":"2021-07-12T09:42:52.119872Z","shell.execute_reply":"2021-07-12T09:42:52.257231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Train/Test Dataloaders","metadata":{}},{"cell_type":"code","source":"transforms_ = [\n    transforms.Resize((img_size, img_size), Image.BICUBIC),\n    #transforms.ToTensor(),\n    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ConcatDataset([ImageDataset(dataset_name, transforms_=transforms_),augmentation_data_1, augmentation_data]),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.259409Z","iopub.execute_input":"2021-07-12T09:42:52.259901Z","iopub.status.idle":"2021-07-12T09:42:52.397421Z","shell.execute_reply.started":"2021-07-12T09:42:52.259863Z","shell.execute_reply":"2021-07-12T09:42:52.396696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/700/1*fJpamgw0yBZZRNEuex07hw.png\" width=\"1000\" height=\"1000\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/abs/1609.04802\">Context Encoders: Feature Learning by Inpainting [Deepak Pathak et al.]</a></center></h4>","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def encoder(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def decoder(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *encoder(channels, 64, normalize=False),\n            *encoder(64, 64),\n            *encoder(64, 128),\n            *encoder(128, 256),\n            *encoder(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *decoder(4000, 512),\n            *decoder(512, 256),\n            *decoder(256, 128),\n            *decoder(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \n\nclass Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(*discriminator_block(channels, 64, 2, False),\n                                   *discriminator_block(64, 128, 2, True),\n                                   *discriminator_block(128, 256, 2, True),\n                                   *discriminator_block(256, 512, 1, True),\n                                   nn.Conv2d(512, 1, 3, 1, 1))\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.538824Z","iopub.execute_input":"2021-07-12T09:42:52.539116Z","iopub.status.idle":"2021-07-12T09:42:52.561119Z","shell.execute_reply.started":"2021-07-12T09:42:52.539086Z","shell.execute_reply":"2021-07-12T09:42:52.560354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.931447Z","iopub.execute_input":"2021-07-12T09:42:52.931772Z","iopub.status.idle":"2021-07-12T09:42:52.939263Z","shell.execute_reply.started":"2021-07-12T09:42:52.93174Z","shell.execute_reply":"2021-07-12T09:42:52.93823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Context-Encoder GAN","metadata":{}},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n    \n# Loss function\nadversarial_loss = torch.nn.MSELoss()\npixelwise_loss = torch.nn.L1Loss()\n\n# Initialize generator and discriminator\ngenerator = Generator(channels=channels)\ndiscriminator = Discriminator(channels=channels)\n\n# Load pretrained models\n# generator.load_state_dict(torch.load(\"../input/context-encoder-gan-for-image-inpainting-pytorch/saved_models/generator.pth\"))\n# discriminator.load_state_dict(torch.load(\"../input/context-encoder-gan-for-image-inpainting-pytorch/saved_models/discriminator.pth\"))\n# print(\"Using pre-trained Context-Encoder GAN model!\")\n\n\ngenerator.model[27] = nn.Conv2d(64, 32, 4, stride=2, padding=1)\ngenerator.model[28] = nn.BatchNorm2d(32, 0.8)\n\ngenerator = nn.Sequential(generator,\n                          nn.ReLU(),\n                          nn.Conv2d(32, 16, 4, stride=2, padding=1),\n                          nn.BatchNorm2d(16, 0.8),\n                          nn.Conv2d(16, 3, 3, 1, 1),\n                          nn.Tanh()\n        )\n\n\ngenerator.cuda()\ndiscriminator.cuda()\nadversarial_loss.cuda()\npixelwise_loss.cuda()\n\n# Initialize weights\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:53.971646Z","iopub.execute_input":"2021-07-12T09:42:53.972186Z","iopub.status.idle":"2021-07-12T09:42:58.124504Z","shell.execute_reply.started":"2021-07-12T09:42:53.972124Z","shell.execute_reply":"2021-07-12T09:42:58.123703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('runs/context_encoder')\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor epoch in range(n_epochs):\n    \n    ### Training ###\n    gen_adv_loss, gen_pixel_loss, disc_loss, gen_loss = 0, 0, 0, 0\n    tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n    for i, (imgs, masked_imgs, masked_parts) in enumerate(tqdm_bar):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        imgs = Variable(imgs.type(Tensor))\n        masked_imgs = Variable(masked_imgs.type(Tensor))\n        masked_parts = Variable(masked_parts.type(Tensor))\n\n        ## Train Generator ##\n        optimizer_G.zero_grad()\n\n        # Generate a batch of images\n        gen_parts = generator(masked_imgs)\n\n\n        # Adversarial and pixelwise loss\n        g_adv = adversarial_loss(discriminator(gen_parts), valid)\n        g_pixel = pixelwise_loss(gen_parts, masked_parts)\n        # Total loss\n        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        ## Train Discriminator ##\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(masked_parts), valid)\n        fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n        d_loss = 0.5 * (real_loss + fake_loss)\n\n        d_loss.backward()\n        optimizer_D.step()\n        \n\n        \n        gen_adv_loss += g_adv.item()\n        gen_pixel_loss += g_pixel.item()\n        disc_loss += d_loss.item()\n        gen_loss += g_loss.item()\n        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n        if i % 50 == 49:\n            writer.add_scalar('generator loss',\n                            gen_loss/(i+1),\n                            epoch * len(dataloader) + i)\n        \n            writer.add_scalar('descremenator loss',\n                            disc_loss/(i+1),\n                            epoch * len(dataloader) + i)\n        \n         \n    torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n    torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\nwriter.close()  ","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:58.12612Z","iopub.execute_input":"2021-07-12T09:42:58.126437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}