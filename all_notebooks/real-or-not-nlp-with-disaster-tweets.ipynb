{"cells":[{"metadata":{},"cell_type":"markdown","source":"References:\n1. https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n2. https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n3. https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Other Libraries","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Training Set","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Data Loading\ntrain_set_path = '/kaggle/input/nlp-getting-started/train.csv'\ntrain_set = pd.read_csv(train_set_path) # returns pd.DataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\nAnalyze column by column and figure out what to do for each column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Data Preview\n\nInput data has 7613 rows and 5 columns.\n\nColumn headers are ['id', 'keyword, 'location', 'text', 'target']. \n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Data Preview\nprint('---Input Analysis---')\nprint('Number of rows in Input:', train_set.shape[0]) \nprint('Number of columns in Input:', train_set.shape[1])# (7613,5)\nprint('First 5 rows: \\n')\nprint(train_set.head()) # id, keyword, location, text, target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Column 1: ID\n\nThe 'id' column contains unique identifiers for each tweet.\n\nIn the training set, each tweet came from a distinct user.\n\nHowever, there might be a user who always post disaster tweets (target == 1).\n\n    * ID might be helpful for identifying targets.     ","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# ID\nprint('---ID Analysis---')\nprint('Unique IDs')\nprint(train_set.id.unique())\nprint('ID Counts')\nprint(train_set.id.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Column 2: Keyword\n\nThe 'keyword' column contains a tag for each tweet.\n\nTop five most frequent tags are:\n    1. fatalities               45\n    2. armageddon               42\n    3. deluge                   42\n    4. harm                     41\n    5. body%20bags              41\n\n    * Investigate correlation of tags and targets.\n    * Blank is filled by '%20', needs preprocessing.\n    * There are NaNs/empty. ","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Keyword\nprint('---Keyword Analysis---')\nprint('Unique Keywords')\nprint(train_set.keyword.unique())\nprint('Keyword Counts')\nprint(train_set.keyword.value_counts())\ntrain_set.keyword.value_counts().plot.barh(figsize=(5,100), title='Keyword Counts', )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Column 3: Text\nThe 'text' column contains the tweets for classification.\n\nSo this is definitely the ***INPUT/X*** for the model.\n\nAll tweets are printed to check what preprocessing is needed. (Output is suppressed for now).\n\n    * Hashtags\n    * Punctuations\n    * Numbers\n    * Signs (=>)\n    * URLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text\nprint('---Text Analysis---')\nprint('Text Review')\nfor text in train_set.text:\n    print(text)\n\n# Character Counts\ntrain_set['char_counts'] = train_set['text'].str.len()\ntrain_set.hist(column='char_counts')\n\n# Word Counts\ntrain_set['word_counts'] = train_set['text'].str.split().map(lambda x: len(x))\ntrain_set.hist(column='word_counts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Column 4: Target\nThe 'target' column contains the ***TARGET/Y*** for the task.\n\nThere are two targets 0 and 1, not disaster and disaster.\n\nThe classes are 57% to 43%, not too imbalanced.","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Target Distribution\nprint('---Target Analysis---')\nprint('Unique Targets')\nprint(train_set.target.unique())\nprint('Target Counts')\nprint(train_set.target.value_counts())\nprint(train_set.target.value_counts(normalize=True))\ntrain_set.target.value_counts().plot.bar()\nplt.title('Class Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keyword v.s. Targets\n\nThere are some keywords that is exclusively used for disaster and vice versa.\n\nDisaster Exclusive Keywords: derailment, wreckage, debris.\n\nNot Disaster Exclusive Keywords: aftershock.\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Keyword counts per class\nis_disaster = train_set['target'] == 1\ndisaster_keyword_counts = train_set[is_disaster].keyword.value_counts()\nnot_disaster_keyword_counts = train_set[~is_disaster].keyword.value_counts()\njoint_keyword_counts = pd.concat([disaster_keyword_counts, not_disaster_keyword_counts], axis=1, join='outer',\n                                 keys=['disaster', 'not_disaster'])\nprint(joint_keyword_counts)\njoint_keyword_counts.plot.barh(subplots=False, figsize=(5,100))\n\nbool_disaster_exclusive = pd.isnull(joint_keyword_counts.not_disaster)\ndisaster_exclusive_counts = joint_keyword_counts[bool_disaster_exclusive]\n\nprint('Disaster Exclusive Keywords:')\nprint(disaster_exclusive_counts)\n\nbool_not_disaster_exclusive = pd.isnull(joint_keyword_counts.disaster)\nnot_disaster_exclusive_counts = joint_keyword_counts[bool_not_disaster_exclusive]\n\nprint('Not Disaster Exclusive Keywords:')\nprint(not_disaster_exclusive_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text v.s. Targets (Before Processing)\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Character Counts per Class\ntrain_set.hist(column='char_counts', by='target')\n\n# Word Counts per Class\ntrain_set.hist(column='word_counts', by='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n\nIn text analysis section, every sample was reviewed to identify several special characters that should be removed to provide a cleaner input.\n\nCommon sources are punctuations, urls, and tags.\n\nIn this section, helper functions are defined for cleaning the inputs.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Load Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove_punctiations:\n# URLs\ndef remove_urls(text): \n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Punctuations\ndef remove_punctuations(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ntrain_set['clean_text'] = train_set['text']\ntrain_set['clean_text'] = train_set['clean_text'].apply(lambda x : remove_urls(x))\ntrain_set['clean_text'] = train_set['clean_text'].apply(lambda x : remove_punctuations(x))\n\nfor text in train_set.clean_text:\n    print(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Inputs and Targets\n\nThe inputs is a list containing each tweet as a string.\n\nThe targets is a list with the provided labels (0/1). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Shuffle Dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_set.sample(frac=1)\ntrain_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataframe to List","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = train_set.clean_text.tolist()\ntargets = train_set.target.tolist()\n    \nprint(inputs)\nprint(targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = None\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\ndef remove_stopwords(inputs, stop_words):\n    if not stop_words:\n        return corpus\n    \n    new_inputs = []\n    for sentence in inputs:\n        for word in stop_words:\n            word_with_space = \" \" + word + \" \" # Assuming words leads and trails with space. If not, subwords might be replaced.\n            sentence = sentence.replace(word_with_space, \" \")\n        new_inputs.append(sentence)\n            \n    return new_inputs\n\ninputs = remove_stopwords(inputs, stop_words)\nprint(inputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Token and Padding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_length = 50\npadding_type = 'post'\ntrunc_type = 'post'\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(inputs)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nsequences = tokenizer.texts_to_sequences(inputs)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint('Vocabulary Size: ', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of Inputs:', len(inputs))\nprint('Number of Targets:', len(targets))\n\ntrain_size = 7000\ntrain_inputs = padded[:train_size]\ntrain_targets = targets[:train_size]\ntrain_targets = np.array(train_targets)\n\nprint(train_inputs)\nprint(train_targets)\n\ndev_inputs = padded[train_size:]\ndev_targets = targets[train_size:]\ndev_targets = np.array(dev_targets)\n\nprint(dev_inputs)\nprint(dev_targets)\n\nplt.hist(train_targets)\nplt.hist(dev_targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding with GloVe (Global Vectors for Word Representation)\nhttps://nlp.stanford.edu/projects/glove/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_path = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\n\nembeddings_index = {}\nwith open(embedding_path) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint(len(embeddings_index))\n\nvocab_size = len(word_index)\nembedding_dim = 100\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector\n\nprint(len(embeddings_matrix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Compile Options","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\ncompile_opts = {}\ncompile_opts['loss'] = 'binary_crossentropy'\ncompile_opts['optimizer'] = Adam(learning_rate=1e-3)\ncompile_opts['metrics'] = ['accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit Options","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_options = {}\nfit_options['x'] = train_inputs\nfit_options['y'] = train_targets\nfit_options['validation_data'] = (dev_inputs, dev_targets)\nfit_options['batch_size'] = 128\nfit_options['epochs'] = 100\nfit_options['verbose'] = 2\nfit_options['callbacks'] = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n    \nfrom tensorflow.keras.layers import Dense, Input\nimport tensorflow_hub as hub\nimport tokenization\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input = bert_encode(train_set.text.values, tokenizer, max_len=50)\n#test_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train_set.target.values\n\nbert_model = build_model(bert_layer, max_len=50)\nbert_model.summary()\nbert_history = bert_model.fit(train_input, train_labels, validation_split=0.2, epochs=5, batch_size=16, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.5)), # recurrent_dropout too slow\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(**compile_opts)\nlstm_model.summary()\nlstm_history = lstm_model.fit(**fit_options)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GRU Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gru_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, dropout=0.5)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\ngru_model.compile(**compile_opts)\ngru_model.summary()\ngru_history = gru_model.fit(**fit_options)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1D Convolution Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nconv_model.compile(**compile_opts)\nconv_model.summary()\nconv_history = conv_model.fit(**fit_options)\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize Accuracy and Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(history):\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epochs')\n    plt.ylabel('accuracy')\n    plt.legend(['accuracy', 'val_accuracy'])\n    plt.show()\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('loss')\n    plt.legend(['loss', 'val_loss'])\n    plt.show()\n\nplot_graphs(lstm_history)\nplot_graphs(gru_history)\nplot_graphs(conv_history)\nplot_graphs(bert_history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize Embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\ne = lstm_model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)\n\n# Expected output\n# (1000, 16)\n\nout_v = io.open('/kaggle/working/vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('/kaggle/working/meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Misclassified Samples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_pred = lstm_model.predict(dev_inputs)\nlstm_pred = [x[0] for x in lstm_pred]\ngru_pred = gru_model.predict(dev_inputs)\nconv_pred = conv_model.predict(dev_inputs)\n\ndev_sentences = inputs[train_size:]\ndev_df = pd.DataFrame(dev_sentences, columns=['Sentence'])\ndev_df['Truth'] = dev_targets == 1\ndev_df['LSTM'] = lstm_pred\ndev_df['LSTM'] = dev_df['LSTM'] >= 0.5\ndev_df['GRU'] = gru_pred\ndev_df['LSTM'] = dev_df['GRU'] >= 0.5\ndev_df['CONV'] = conv_pred\ndev_df['LSTM'] = dev_df['CONV'] >= 0.5\nprint(dev_df)\nFN = []\nfor truth, pred in zip(dev_targets, lstm_pred):\n    if truth == 1 and pred < 0.5:\n        FN.append(True)\n    else:\n        FN.append(False)\n        \nprint(dev_df[FN]['Sentence'])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_set['clean_text'] = test_set['text']\ntest_set['clean_text'] = test_set['clean_text'].apply(lambda x : remove_urls(x))\ntest_set['clean_text'] = test_set['clean_text'].apply(lambda x : remove_punctuations(x))\n\nfor text in test_set.clean_text:\n    print(text)\n    \ntest_inputs = test_set.clean_text.tolist()\ntest_inputs = remove_stopwords(test_inputs, stop_words)\ntest_sequences = tokenizer.texts_to_sequences(test_inputs)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_pred = lstm_model.predict(test_padded)\nprint(lstm_pred)\nnum_test_samples = len(lstm_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\npred = np.round(lstm_pred).astype(int).reshape(3263)\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':pred})\nsub.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}