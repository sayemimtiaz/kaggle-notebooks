{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set(style='whitegrid')\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nline_colors = [\"#7CEA9C\", '#50B2C0', \"rgb(114, 78, 145)\", \"hsv(348, 66%, 90%)\", \"hsl(45, 93%, 58%)\"]\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nheval = True # load heavier load cells visualisation\nheval2 = True # load model cells ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/deelezu-4612bda5-9711-419f-8a13-f9ef7127198d.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOiIsImlzcyI6InVybjphcHA6Iiwib2JqIjpbW3sicGF0aCI6IlwvZlwvOGNjMWVlYWEtNDA0Ni00YzRhLWFlOTMtOTNkNjU2ZjY4Njg4XC9kZWVsZXp1LTQ2MTJiZGE1LTk3MTEtNDE5Zi04YTEzLWY5ZWY3MTI3MTk4ZC5qcGcifV1dLCJhdWQiOlsidXJuOnNlcnZpY2U6ZmlsZS5kb3dubG9hZCJdfQ.N4zM3kLB9YXHN_tBadKXv-2Gkyg6kVABLzIbrEFJEqc)\n<span>Photo by <a href=\"https://unsplash.com/@fadder8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Fadzai Saungweme</a> on <a href=\"https://unsplash.com/s/photos/perth?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>\n\n# 1. <span style='color:rgb(205, 0, 153)'> Introduction</span>\n\n- The aim of this notebook is to build some models that can predict [Perth](https://www.australia.com/en/places/perth-and-surrounds/guide-to-perth.html) (located in Western Australia) housing prices based on a set of scrapped features made available in the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices). \n- The current dataset has recently been updated, so it's interesting to explore the significance of new features, and their impact on model accuracy.\n- The dataset is very interesting to explore since __Perth__ is not a city that is commonly used in data analysis by people outside Australia. When it comes to Australian cities, __Melbourne__ & __Sydney__ are the big two that one might think about. It's also interesting to explore the Plotly library capability & create interactive choropleth maps, similar to the notebook I wrote about [Australian Geographic Data Plots](https://www.kaggle.com/shtrausslearning/australian-geographic-data-plots).\n- Any recommendations to improve the notebook, such as ideas or areas of improvement are more than welcome.\n\n# 2. <span style='color:rgb(205, 0, 153)'> Perth Housing Dataset</span>\n\nHaving been updated recently version (all_perth_310121.csv), let's review some of the features that are available in the __Perth Housing Dataset__.\n- <code>ADDRESS</code> : Physical address of the property ( we will set to index )\n- <code>SUBURB</code> : Specific locality in Perth; a list of all Perth suburb can be found [here](https://www.homely.com.au/find-suburb-by-region/perth-greater-western-australia)\n- <code>PRICE</code> : Price at which a property was sold (AUD)\n- <code>BEDROOMS</code> : Number of bedrooms\n- <code>BATHROOMS</code> : Number of bathrooms\n- <code>GARAGE</code> : Number of garage places\n- <code>LAND_AREA</code> : Total land area (m^2)\n- <code>FLOOR_AREA</code> : Internal floor area (m^2)\n- <code>BUILD_YEAR</code> : Year in which the property was built\n- <code>CBD_DIST</code> : Distance from the centre of Perth (m)\n- <code>NEAREST_STN</code> : The nearest public transport station from the property\n- <code>NEAREST_STN_DIST</code> : The nearest station distance (m)\n- <code>DATE_SOLD</code> : Month & year in which the property was sold\n- <code>POSTCODE</code> : Local Area Identifier\n- <code>LATITUDE</code> : Geographic Location (lat) of <code>ADDRESS</code>\n- <code>LONGITIDE</code> : Geographic Location (long) of <code>ADDRESS</code>\n- <code>NEAREST_SCH</code> : Location of the nearest School\n- <code>NEAREST_SCH_DIST</code> : Distance to the nearest school\n- <code>NEAREST_SCH_RANK</code> : Ranking of the nearest school \n\n## 2.1. <span style='color:rgb(97, 47, 205)'> New Dataset Additions</span>\n\n- As opposed to a [previous notebook](https://www.kaggle.com/shtrausslearning/perth-housing-price-prediction-eda), the locations of individual addresses is available to us (<code>LONGITUDE</code>,<code>LATITUDE</code>), which is much more handly than what we had before, and allows more leeway to experiment with property relations/tune models, and is a welcome addition, since we have the exact locations.\n- Postcodes (<code>POSTCODES</code>) are interesting additions, allowing us to not only link any useful outside data but there is also a likely relation to property prices due to the ordering. It would be iteresting to visualise the distribution of these zip codes as well.\n- Nearest School Information (__name__,__distance__,__rank__) are interesting additions as well, good schools are often located in expensive suburbs, and visa versa. Proximity may also have an effect on <code>PRICE</code>, however on first impression, <code>NEAREST_SCH_RANK</code>, could change from year to year, during which the properties were sold <code>DATE_SOLD</code>, nevertheless it would be interesting to explore these school related features. As indicated in the dataset, there is some data missing in <code>NEAREST_SCH_RANK</code>, which we have to deal with."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_perth0 = pd.read_csv('/kaggle/input/perth-house-prices/all_perth_310121.csv')\ndf_perth0.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport shap\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBClassifier,XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\n\n# Notebook Helper Class \nclass transformer(BaseEstimator,TransformerMixin):\n    \n    def __init__(self,drop_nan=False,select_dtype=False,show_nan=False,title='Title',show_counts=False,\n                 figsize=(None,None), feature_importance = False, target = 'PRICE'):\n        self.drop_nan = drop_nan\n        self.select_dtype = select_dtype\n        self.show_nan = show_nan\n        self.title = title\n        self.show_counts = show_counts\n        self.figsize = figsize\n        self.feature_importance = feature_importance\n        self.target = target  # target variable\n        \n    # Apply Fit\n    def fit(self,X,y=None):\n        return self\n        \n    # Apply Some Transformation to the Feature Matrix\n    def transform(self,X):\n        \n        '''show NaN % in DataFrame'''\n        if(self.show_nan):\n            \n            fig, ax = plt.subplots(figsize = self.figsize)\n            nan_val = (X.isnull().sum()/len(X)*100).sort_values(ascending = False)\n            cmap = sns.color_palette(\"plasma\")\n            for i in ['top', 'right', 'bottom', 'left']:\n                ax.spines[i].set_color('black')\n            ax.spines['top'].set_visible(True);ax.spines['right'].set_visible(False)\n            ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n            sns.barplot(x=nan_val,y=nan_val.index, edgecolor='k',palette = 'rainbow')\n            plt.title(self.title);ax.grid(ls='--',alpha = 0.9);plt.show()\n            return\n        \n        ''' Plot df.value_counts '''\n        if(self.show_counts):\n        \n            tdf = X.value_counts()\n            cmap = sns.color_palette(\"plasma\")\n            fig, ax = plt.subplots(figsize = self.figsize)\n            for i in ['top', 'right', 'bottom', 'left']:\n                ax.spines[i].set_color('black')\n            ax.spines['top'].set_visible(True);ax.spines['right'].set_visible(False)\n            ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n            sns.barplot(tdf.index,tdf.values,edgecolor='k',palette = 'rainbow',ax=ax);\n            plt.title(self.title);ax.grid(ls='--',alpha = 0.9);plt.show()\n        \n        ''' Drop All NAN values in DataFrame'''\n        if(self.drop_nan):\n            X = X.dropna();\n            return X\n            \n        ''' Split DataFrame into Numerical/Object features'''\n        if(self.select_dtype):\n            X1 = X.select_dtypes(include=['float64','int64','uint8'])     # return only numerical features from df\n            X2 = X.select_dtypes(exclude=['float64','int64','uint8'])\n            return X1,X2\n        \n        ''' Plot Feature Importance '''\n        if(self.feature_importance):\n            \n             # Plot Correlation to Target Variable only\n            def corrMat2(df,target=self.target,figsize=(9,0.5),ret_id=False):\n\n                corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n                corr_mat = corr_mat.transpose()\n                corr = corr_mat.loc[:, df.columns == self.target].transpose().copy()\n\n                if(ret_id is False):\n                    f, ax = plt.subplots(figsize=figsize)\n                    sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                                cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n                    plt.title(f'Feature Correlation to {self.target}')\n\n                if(ret_id):\n                    return corr\n\n            ''' Plot Relative Feature Importance '''\n            def feature_importance(tldf,feature=self.target,n_est=500):\n\n                # X : Numerical / Object DataFrame\n                ldf0,_ = transformer(select_dtype=True).transform(X=tldf)\n                ldf = transformer(drop_nan=True).transform(X=ldf0)  \n\n                # Input dataframe containing feature & target variable\n                X = ldf.copy()\n                y = ldf[feature].copy()\n                del X[feature]\n\n            #   CORRELATION\n                imp = corrMat2(ldf,feature,figsize=(15,0.5),ret_id=True)\n                del imp[feature]\n                s1 = imp.squeeze(axis=0);s1 = abs(s1)\n                s1.name = 'Correlation'\n\n            #   SHAP\n                model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n                explainer = shap.TreeExplainer(model)\n                shap_values = explainer.shap_values(X)\n                shap_sum = np.abs(shap_values).mean(axis=0)\n                s2 = pd.Series(shap_sum,index=X.columns,name='Cat_SHAP').T\n\n            #   RANDOMFOREST\n                model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)\n                fit = model.fit(X,y)\n                rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                                    columns=['RandForest']).sort_values('RandForest',ascending=False)\n                s3 = rf_fi.T.squeeze(axis=0)\n\n            #   XGB \n                model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)\n                model.fit(X,y)\n                data = model.feature_importances_\n                s4 = pd.Series(data,index=X.columns,name='XGB').T\n\n            #   KBEST\n                model = SelectKBest(k=X.shape[1], score_func=f_regression)\n                fit = model.fit(X,y)\n                data = fit.scores_\n                s5 = pd.Series(data,index=X.columns,name='K_best')\n\n                # Combine Scores\n                df0 = pd.concat([s1,s2,s3,s4,s5],axis=1)\n                df0.rename(columns={'target':'lin corr'})\n\n                x = df0.values \n                min_max_scaler = preprocessing.MinMaxScaler()\n                x_scaled = min_max_scaler.fit_transform(x)\n                df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)\n                df = df.rename_axis('Feature Importance via', axis=1)\n                df = df.rename_axis('Feature', axis=0)\n                df['total'] = df.sum(axis=1)\n                df = df.sort_values(by='total',ascending=True)\n                del df['total']\n                fig = px.bar(df,orientation='h',barmode='stack',color_discrete_sequence=line_colors)\n                fig.update_layout(template='plotly_white',height=self.figsize[1],width=self.figsize[0],margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0});\n                for data in fig.data:\n                    data[\"width\"] = 0.6 #Change this value for bar widths\n                fig.show()\n                \n            feature_importance(X)\n            \n# Class to Visualise Things Only\nclass visualise(BaseEstimator,TransformerMixin):\n    \n    def __init__(self,target=None,option=False):\n\n        self.target = target             # target varable [str]\n        self.option = option\n\n    @staticmethod \n    def corrMat2(df,target='demand',figsize=(9,0.5),ret_id=False):\n\n        corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n        corr_mat = corr_mat.transpose()\n        corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n\n        if(ret_id is False):\n            f, ax = plt.subplots(figsize=figsize)\n            sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                         cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n            plt.title(f'Feature Correlation to {target}')\n\n        if(ret_id):\n            return corr\n        \n    def fit(self):\n        return self\n    \n    # X -> Numerical (feature matrix + target variable)\n    def transform(self,X):\n        \n        # Pandas Static Histogram\n        if(self.option is 'histogram'):\n            vdf_perth1_num,_ = transformer(select_dtype=True).transform(X=X)\n            vdf_perth1_num.hist(bins=30, figsize=(20,15));plt.show()    \n        \n        # Seaborn Static Boxplot\n        if(self.option is 'boxplot'):\n            \n            lX,_ = transformer(select_dtype=True).transform(X=X)\n            fig,axs = plt.subplots(ncols=5,nrows=4,figsize=(900,400))\n            index = 0\n            axs = axs.flatten()\n            for k,v in lX.items():\n                flierprops = dict(marker='o',mfc='k',ls='none',mec='k')\n                ax = sns.boxplot(x=k,data=lX,orient='h',flierprops=flierprops,\n                                ax=axs[index],width=0.5)\n                index += 1\n            plt.tight_layout();plt.show()\n            \n        # Outlier Quantile Information\n        if(self.option is 'outliers'):\n            \n    #       2. Define Outliers\n            lX,_ = transformer(select_dtype=True).transform(X=X)\n            for k, v in lX.items():\n                q1 = v.quantile(0.25); q3 = v.quantile(0.75); irq = q3 - q1\n                v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n                perc = np.shape(v_col)[0] * 100.0 / np.shape(lX)[0]\n                print(\"Column %s outliers = %.2f%%\" % (k, perc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are interested in property prices, so let's change the index to <code>ADDRESS</code> and remove any duplicates if they exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some Data Cleaning \ndf_perth0.drop_duplicates(subset=['ADDRESS'],inplace=True) # Some addresses actually have multiple entries\ndf_perth0.index = df_perth0['ADDRESS'] # set dataframe index, since it's not really a useful feature \ndel df_perth0['ADDRESS'] # let's also delete the column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. <span style='color:rgb(97, 47, 205)'>  Categrocal & Ordinal Features </span>\n- We have a few __categorical features__ which can be handy for EDA, as well as for model features, such as One-Hot Encoding/ GetDummies.\n- <code>SOLD_MONTH</code> &<code>SOLD_YEAR</code> can be extracted from <code>DATE_SOLD</code>.\n- <code>SUBURB</code> probably doesn't tell us any more than the <code>POSTCODE</code> does, but useful for __EDA__.\n- Together with <code>NEAREST_STN</code> & <code>NEAREST_SCH</code>, it is possible to create some form of ranking based on the names. Road names definitely could be something we can tie to ranking. At this stage let's just hang on to these dataframes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num,df_cat = transformer(select_dtype=True).transform(X=df_perth0.copy())\ndf_num[['SOLD_MONTH', 'SOLD_YEAR']] = df_cat['DATE_SOLD'].str.split('-', 1, expand=True).astype('float64')\ndf_cat.drop(['DATE_SOLD'],axis=1,inplace=True)\ndf_EDA = pd.concat([df_num,df_cat],axis=1) # combine \ndf_cat.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\n# Plot Histogram, Boxplot using Plotly\ndef px_stats(df, n_cols=4, to_plot='box',height=800,w=None):\n    \n    ldf,_ = transformer(select_dtype=True).transform(X=df)\n    numeric_cols = ldf.columns\n    n_rows = -(-len(numeric_cols) // n_cols)  # math.ceil in a fast way, without import\n    row_pos, col_pos = 1, 0\n    fig = make_subplots(rows=n_rows, cols=n_cols,subplot_titles=numeric_cols.to_list())\n    \n    for col in numeric_cols:\n        if(to_plot is 'histogram'):\n            trace = go.Histogram(x=ldf[col],showlegend=False,autobinx=True,\n                                 marker = dict(color = 'rgb(27, 79, 114)',\n                                 line=dict(color='white',width=0)))\n        else:\n            trace = getattr(px, to_plot)(ldf[col],x=ldf[col])[\"data\"][0]\n            \n        if col_pos == n_cols: \n            row_pos += 1\n        col_pos = col_pos + 1 if (col_pos < n_cols) else 1\n        fig.add_trace(trace, row=row_pos, col=col_pos)\n\n    fig.update_traces(marker = dict(color = 'rgb(27, 79, 114)',\n                     line=dict(color='white',width=0)))\n    fig.update_layout(template='plotly_white');fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0})\n    fig.update_layout(height=height,width=w);fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. <span style='color:rgb(205, 0, 153)'>  Quick Dataset Analysis</span>\n\nWithout digging too deep, we can make very general observations based on a quick preliminary dataset investigation & come to some conclusions to better understand what data we are dealing with & try to note some thing we can try since there is no particular task set in the dataset, other than price prediction.\n\n## 3.1. <span style='color:rgb(97, 47, 205)'> Data Distributions Histograms</span>\n\n- The most common price range of a property; 400-500k AUD, which makes up about 10,000 properties.\n- We can note how uncommon __1 bedroom appartment__ properties are in Perth, most common being a __4 bedroom property__, typically having __1 or 2 bathrooms__ & __garage__ with __two car slots__.\n- We can see a very steady __increase in property sales__ & number of __properties built__ in the last 6 years or so.  I'm guessing real estate agents in Perth are kept busy. \n- Some of the properties have a very large number of garages slots, so it would make sense to just remove them but lets just keep them anyway. Quite a large number of features have __skewed distributions__. Let's note to do soem   \n- There seems to exist an interesting grouping of zipcodes; four groups are visible with larger bincounts."},{"metadata":{"trusted":true},"cell_type":"code","source":"if(heval):\n    px_stats(df_EDA, to_plot='histogram') # interactive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. <span style='color:rgb(97, 47, 205)'> Data Distributions Boxplots\n- Complementary to histograms, boxplots, indicate outliers a little more clearly, as well as useful statistics about __min__, __max__, __median__ & __q1/q3__ values.\n- We will lean towards using tree based methods; It is often stated that, ensemble approaches such as RF are not sensitive to outliers, such as this article on [medium](https://arsrinevetha.medium.com/ml-algorithms-sensitivity-towards-outliers-f3862a13c94d), however there are counter arguments that state the complete opposite as shown on [stackexchange](https://stats.stackexchange.com/questions/187200/how-are-random-forests-not-sensitive-to-outliers). \n- That said our data contains quite a lot of outliers, which is to be expected from a non consistent selling standard/rules for properties, allowing certain properties to be prices above/below values of similar properties depending on specific circumstaces.\n- It is interesting to look into __creating models for a specific subset__ of our data ( eg. similar suburbs, low cost suburbs, presold properties and so on ), in an attempt to get around these outliers. One model for the entire dataset seems like a huge stretch, and most definitely will have accuracy limits."},{"metadata":{"trusted":true},"cell_type":"code","source":"if(heval):\n    px_stats(df_EDA, to_plot='box',height=550)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. <span style='color:rgb(97, 47, 205)'> Target Model Feature Importance Evaluation\n- We can use multiple approaches, even early on, to quickly evaluate __which features have most weight__ in a model evaluation to get a better understanding of not only their imporance but also how different models use these features in their evaluation. An interesting article about the relevance of feature importance, [machinelearningmastery](https://machinelearningmastery.com/calculate-feature-importance-with-python/).\n- In addition to previously evaluated features, <code>NEAREST_SCH_RANK</code> & <code>POSTCODE</code> are quite impactful features. <code>LONGITUDE</code> & <code>LATITUDE</code> are quite similar to that what it was previously, so even with the addition of more accurate address geotagging, there is little difference. <code>NEAREST_SCH_DIST</code> was found to be one of the less impactful features.\n- Although there are a number of features that have little to no impact, the only irrelevant feature seems to be <code>SOLD_MONTH</code>, which we ought to drop, a little later."},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,400),target='PRICE').transform(X=df_EDA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import geopandas as gpd\n\n''' Plotly Geography Choropeth Plots /w Menu Layout '''\ndef plot_geo_menu(ldf,feature):\n    \n    print(ldf.info())\n    \n    # Load Geometry File\n    wa_gdf = gpd.read_file('/kaggle/input/wa-gda2020/WA_LOCALITY_POLYGON_SHP-GDA2020.shp')    # Load the data using \n    wa_gdf.drop(['POSTCODE','PRIM_PCODE','LOCCL_CODE','DT_GAZETD','STATE_PID','DT_RETIRE','DT_CREATE','LOC_PID'],axis=1,inplace=True)\n\n    # Display Values\n    wa_gdf.index = wa_gdf['NAME']\n    median_price = ldf.groupby(['SUBURB']).median()      # Suburb Median Groupby\n    median_price.index = median_price.index.str.upper()\n    df_merged = wa_gdf.join(median_price).dropna() \n#     df_merged = wa_gdf.join(median_price)\n\n    # Convert geometry to GeoJSON\n    df_merged = df_merged.to_crs(epsg=4327)\n    lga_json = df_merged.__geo_interface__\n\n    # Unique Token ID\n    MAPBOX_ACCESSTOKEN = 'pk.eyJ1Ijoic2h0cmF1c3NhcnQiLCJhIjoiY2tqcDU2dW56MDVkNjJ6angydDF3NXVvbyJ9.nx2c5XzUH9MwIv4KcWVGLA'\n#     lst_val = [df_merged[feature].min(),df_merged[feature].max()]\n\n    trace = []    \n    # Set the data for the map\n    for i in feature:\n        trace.append(go.Choroplethmapbox(geojson = lga_json,locations = df_merged.index,    \n                               z = df_merged[i].values,                     \n                               text = df_merged.index,\n                               hovertemplate = \"<b>%{text}</b><br>\" +\n                                                \"%{z}<br>\" +\n                                                \"<extra></extra>\",\n                               colorbar=dict(thickness=10, ticklen=3,outlinewidth=0),\n                               marker_line_width=1, marker_opacity=0.8, colorscale=\"turbo\",\n                               visible=False)\n                        )\n    trace[0]['visible'] = True\n\n    layout = go.Layout(mapbox1 = dict(domain = {'x': [0, 1],'y': [0, 1]},\n                                      center = dict(lat=-31.95, lon=115.8),\n                       accesstoken = MAPBOX_ACCESSTOKEN,zoom = 8),\n                       autosize=True,height=500)\n    \n    lst = [];ii=-1\n    for i in feature:\n        ii+=1\n        tlist = [False for z in range(len(feature))]\n        tlist[ii] = True\n        temp = dict(args=['visible',tlist],label=i,method='restyle') \n        lst.append(temp)\n\n    # add a dropdown menu in the layout\n    layout.update(updatemenus=list([dict(x=0.8,y=1.1,xanchor='left',yanchor='middle',buttons=lst)]))\n    fig=go.Figure(data=trace, layout=layout)\n    fig.update_layout(title_text='Suburb Mean Values', title_x=0.01)\n    fig.update_layout(margin={\"r\":0,\"t\":80,\"l\":0,\"b\":80},mapbox_style=\"light\")\n    fig.show()\n    \n''' Plotly Geography Choropeth Plots '''\ndef plot_geo(ldf,feature,title=None,lst_val=None):\n    \n    # Load Geometry File\n    wa_gdf = gpd.read_file('/kaggle/input/wa-gda2020/WA_LOCALITY_POLYGON_SHP-GDA2020.shp')    # Load the data using \n    wa_gdf.drop(['POSTCODE','PRIM_PCODE','LOCCL_CODE','DT_GAZETD','STATE_PID','DT_RETIRE','DT_CREATE','LOC_PID'],axis=1,inplace=True)\n\n    wa_gdf.index = wa_gdf['NAME']\n    median_price = ldf.groupby(['SUBURB']).median()\n    median_price.index = median_price.index.str.upper()\n    df_merged = wa_gdf.join(median_price).dropna() # some perth suburbs don't have data & drop other WA region suburbs to speed up map load\n\n    # Convert geometry to GeoJSON\n    df_merged = df_merged.to_crs(epsg=4327)\n    lga_json = df_merged.__geo_interface__\n\n    MAPBOX_ACCESSTOKEN = 'pk.eyJ1Ijoic2h0cmF1c3NhcnQiLCJhIjoiY2tqcDU2dW56MDVkNjJ6angydDF3NXVvbyJ9.nx2c5XzUH9MwIv4KcWVGLA'\n\n    if(lst_val is None):\n        lst_val = [df_merged[feature].min(),df_merged[feature].max()]\n\n    # Set the data for the map\n    data = go.Choroplethmapbox(geojson = lga_json,\n                               locations = df_merged.index,    \n                               z = df_merged[feature], \n                               text = title,\n                               colorbar=dict(thickness=20, ticklen=3,outlinewidth=0),\n                               marker_line_width=1, marker_opacity=0.8, colorscale=\"viridis\",\n                               zmin=lst_val[0], zmax=lst_val[1])\n\n    layout = go.Layout(mapbox1 = dict(domain = {'x': [0, 1],'y': [0, 1]},center = dict(lat=-31.95, lon=115.8),\n                       accesstoken = MAPBOX_ACCESSTOKEN,zoom = 9),\n                       autosize=True,height=650)\n\n    # Generate the map\n    fig=go.Figure(data=data, layout=layout)\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4. <span style='color:rgb(97, 47, 205)'> Target Variable &  Other Features  (Suburb Median)</span>\n\n### Boundary Data\n\n- We'll use the __General District Area__ (GDA) datafile for <b>Western Australia</b>, where Perth is located. \n- The geographic data is available in this source [data.gov](https://data.gov.au/dataset/ds-dga-6a0ec945-c880-4882-8a81-4dbcb85e74e5/distribution/dist-dga-9fff5439-7af5-42f4-9102-42c4199c5c1c/details?q=).\n\n### Median Values\n\n- Properties closer to the __Indian Ocean__ tend to have a higher median price than those next to the Perth __CBD__.\n- South-East & Easterns suburbs being mostly associated with the dataset mean value.\n- When it comes to beachside properties, almost all suburbs bordering the __Indian Ocean__, west and northwest & southweast of Perth are quite expensive, however suburbs south of __Kwinana Beach__ are more affordable, especially considering the close proximity to the ocean.\n- Some notable __suburbs__; __Kwinana Beach & Town__ have quite low median price values, however the number of crimes commited doesn't quite justify the price range, for example; there were only a handful of drug related crimes in __Kwinana Town__ in the last year, in fact th number of offenses commited has dropped quite significantly since 2016 in this suburb. There are quite a few suburbs with higher crime rates that the mentioned two. You can find the data at [WA Police](https://www.police.wa.gov.au/Crime/CrimeStatistics#/). An interesting report on crime data is aso available on [ABC](https://www.abc.net.au/news/2018-02-17/crime-data-for-every-perth-suburb-revealed-by-wa-police/9447642?nw=0).\n- Other median feature values can also be investigated using the geographic data using the data below."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp,_ = transformer(select_dtype=True).transform(X=df_EDA)\ntlist = temp.columns.to_list()\ntlist.remove('LONGITUDE');tlist.remove('LATITUDE');\nif(heval):\n    plot_geo_menu(ldf=df_EDA,feature=tlist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. <span style='color:rgb(205, 0, 153)'> Missing Data & Cleaning </span>\n\nWe have three features with missing data, <code>GARAGE</code>, <code>BUILD_YEAR</code> & <code>NEAREST_SCH_RANK</code>."},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(show_nan=True,figsize=(9,5),title='Feature (NaN) %').transform(X=df_EDA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__BUILD_YEAR Imputation__\n- <code>garage</code> seems quite straightforward & probably makes sense to set to zero, it was likely left out because there was no garage present."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA['GARAGE'] = df_EDA['GARAGE'].fillna(0)  # fill missing data with 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:rgb(27, 79, 114)'> Imputation Overview</span>\n\n- In this notebook, let's use a __model based imputation__ approach using the current dataset features, this is one of our main assumptions in this notebook that can affect our model accuracy, so __let's flag the imputed data indicies__, so we don't forget which properties had the missing data. The flagging feature simply is a numerical value of the order in which it was input into the inpute function.\n\n__BUILD_YEAR Imputation__\n\n- Having multiple columns associated with property features, <code>BUILD_YEAR</code> seems like a feature for which we can use a model to predict these values with at least some confidence, the values will be somewhere in the right ballpark, there is also less than 10% missing, so the effects hopefully will be minimal.\n\n__NEAREST_SCH_RANK Imputation__\n\n- On the contrary, quite a large portion of <code>NEAREST_SCH_RANK</code> are missing, however that is not a reason in itself to drop properties. The lack of __Better Education Rank__ for some schools is probably the result of them having fewer than 20 students with an __ATAR__ value as seen on the [website](https://bettereducation.com.au/Results/WA/wace.aspx), and other factors as well. \n- There are logistical issue associated with predicting missing data to rank schools using __property__ related features, nevertheless, what we can do is, flag properties which has missing data, and investigate later the effects that this has on the accuracy of __price__ prediction models. \n- If that introduces significant errors, we can use an alternative distance based search or perhaps a model (if there are several options nearby) that will __find a property__ that is in close proximity to the __property with missing school rank data__, that would be more logical.\n\n### <span style='color:rgb(27, 79, 114)'> Build Year Model Feature Importance</span>\n\n__Model based imputation__ will build a model to estimate the missing data, so we can check what features would be most relevant if we wanted to predict <code>BUILD_YEAR</code>:"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,400),target='BUILD_YEAR').transform(X=df_EDA) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:rgb(27, 79, 114)'> NEAREST_SCH_RANK Model Feature Importance</span>\n\n- Let's take a look at <code>NEAREST_SCH_RANK</code> as well; both __feature_importance__ & __bivariate relations__.\n- The top 5 features give a good indication that <code>NEAREST_SCH_RANK</code> is tried to some form of suburb ranking/ordering, with <code>CBD_DIST</code> being a key feature. It's quite possible that missing values are very small schools that are much further away from the CBD, and don't have sufficient statistics to meet the specific ranking criteria.\n- Whilst there is a tendency for the <code>NEAREST_SCH_RANK</code> value to reduce (better ranking) with increasing <code>PRICE</code> up to about a rank of 30. Below this value there tends to be a very widespread relation, "},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,400),target='NEAREST_SCH_RANK').transform(X=df_EDA)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import display_html\n\n# Display Multiple Dataframe in HTML format\ndef pd_html(dfs, names=[]):\n    html_str = ''\n    for i in dfs:\n        i.style.background_gradient(cmap='viridis') \n    \n    if names:\n        html_str += ('<tr>' + \n                     ''.join(f'<td style=\"text-align:center\">{name}</td>' for name in names) + \n                     '</tr>')\n    html_str += ('<tr>' + \n                 ''.join(f'<td style=\"vertical-align:top\"> {df.to_html(index=True)}</td>' \n                         for df in dfs) + \n                 '</tr>')\n    html_str = f'<table>{html_str}</table>'\n    html_str = html_str.replace('table','table style=\"display:inline\"')\n    display_html(html_str, raw=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext Cython","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%cython -a \nimport numpy as np\ncimport numpy as np\n\n# Regularised Model\ncdef class Rtg:\n\n    cdef public np.float64_t lamda, gamma, gain\n    cdef public int bfeat_id, min_size, max_depth\n    cdef public np.float64_t bfeat_val, value\n    cpdef public Rtg lhs\n    cpdef public Rtg rhs\n    \n    def __init__(self, int max_depth=3, np.float64_t lamda=1.0, np.float64_t gamma=0.1, min_size=5):\n        self.max_depth = max_depth\n        self.gamma = gamma; self.lamda = lamda\n        self.lhs = None; self.rhs = None\n        self.bfeat_id = -1 \n        self.bfeat_val = 0 \n        self.value = -7e10\n        self.min_size = min_size\n        \n        return\n    \n    def fit(self, np.ndarray[np.float64_t, ndim=2] X, np.ndarray[np.float64_t, ndim=1] y):\n        \n        cpdef long ntot = X.shape[0]\n        cpdef long SL = X.shape[0]\n        cpdef long SR = 0\n        cpdef long idx = 0\n        cpdef long thres = 0\n        cpdef np.float64_t GL, GR, gain\n        cpdef np.ndarray[long, ndim=1] idxs\n        cpdef np.float64_t x = 0.0\n        cpdef np.float64_t best_gain = -self.gamma\n        \n        if self.value == -7e10:\n            self.value = y.mean()\n        if(self.max_depth <= 1):\n            return\n        \n        error0 = ((y - self.value) ** 2).sum()\n        error = error0; fid = 0\n        n_feat = X.shape[1]\n        left_value = 0; right_value = 0\n        \n        for feat in range(n_feat):\n            \n            idxs = np.argsort(X[:,feat])\n            GL,GR = y.sum(),0.0\n            SL,SR, thres = ntot, 0, 0\n            \n            while thres < ntot - 1:\n                \n                SL = SL - 1; SR = SR + 1\n                idx = idxs[thres]\n                x = X[idx, feat]\n                \n                GL = GL - y[idx]; GR = GR + y[idx]\n                gain1 = (GL**2) / (SL + self.lamda)  + (GR**2) / (SR + self.lamda)\n                gain2 = - ((GL + GR)**2) / (SL + SR + self.lamda) + self.gamma\n                gain = gain1+gain2\n                \n                if thres < ntot - 1 and x == X[idxs[thres + 1], feat]:\n                    thres += 1\n                    continue\n                \n                if (gain > best_gain) and (min(SL,SR) > self.min_size):\n                    \n                    fid = 1\n                    best_gain = gain\n                    left_value = -GL / (SL + self.lamda)\n                    right_value = -GR / (SR + self.lamda)\n                    \n                    self.bfeat_id = feat\n                    self.bfeat_val = x\n\n                thres += 1\n        \n        self.gain = best_gain\n        if self.bfeat_id == -1:\n            return\n                \n        self.lhs = Rtg(max_depth=self.max_depth - 1, gamma=self.gamma, lamda=self.lamda)\n        self.rhs = Rtg(max_depth=self.max_depth - 1, gamma=self.gamma, lamda=self.lamda)\n        self.lhs.value = left_value\n        self.rhs.value = right_value\n\n        idxs_l = (X[:, self.bfeat_id] > self.bfeat_val)\n        idxs_r = (X[:, self.bfeat_id] <= self.bfeat_val)\n        self.lhs.fit(X[idxs_l, :], y[idxs_l])\n        self.rhs.fit(X[idxs_r, :], y[idxs_r])\n        \n        if (self.lhs.lhs == None or self.rhs.lhs == None):\n            if self.gain < 0.0:\n                self.lhs = None; self.rhs = None; self.bfeat_id = -1\n\n    def ppredict(self, np.ndarray[np.float64_t, ndim=1] x):\n        if self.bfeat_id == -1:\n            return self.value\n        if x[self.bfeat_id] > self.bfeat_val:\n             return self.lhs.ppredict(x)\n        else:\n            return self.rhs.ppredict(x)\n        \n    def predict(self, np.ndarray[np.float64_t, ndim=2] X):\n        y = np.zeros(X.shape[0])\n        \n        for i in range(X.shape[0]):\n            y[i] = self.ppredict(X[i])\n            \n        return y\n    \n# Bagging Model Regularised Model\nclass RtgBag():\n    \n    def __init__(self,min_size=5,max_depth=3,n_samples=10):\n            \n        self.max_depth = max_depth\n        self.min_size = min_size\n        self.n_samples = n_samples\n        self.subsample_size = None\n        self.lst_tree = [Rtg(min_size=self.min_size,max_depth=self.max_depth) for _ in range(self.n_samples)]\n    \n    def get_samples(self,X,y):\n\n        i = np.random.randint(0, len(X), (self.n_samples, self.subsample_size))\n        sampX = X[i]; sampy = y[i]\n        return sampX, sampy\n    \n    def fit(self,X,y):\n        \n        ntot = X.shape[0]\n        self.subsample_size = int(ntot)\n        sampX, sampy = self.get_samples(X,y)\n        for i in range(self.n_samples):\n            self.lst_tree[i].fit(sampX[i], sampy[i].reshape(-1))\n        return self\n        \n    def predict(self,X):\n        \n        mtot = X.shape[0]; pred = []\n        for i in range(self.n_samples):\n            pred.append(self.lst_tree[i].predict(X))\n        pred = np.array(pred).T\n\n        return np.array([np.mean(pred[i]) for i in range(mtot)])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import mean_squared_error as mse\n\n# Gradient Boosting Model (XGB/XGB+Bagging)\nclass GBoost(BaseEstimator,RegressorMixin):\n    \n    def __init__(self, n_estimators=10, learning_rate=0.5, max_depth=3, \n                 n_samples = 15, min_size = 5, tree_id='xgb_bagging'):\n            \n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.min_size = min_size\n        self.treem_id = []\n        self.n_samples = n_samples\n        self.tree_id = tree_id\n        self.dm_depth = 1 \n        self.mse_cond = 1.5 \n    \n    def fit(self, X, y):\n        \n        if(type(X) is not np.ndarray):\n            X = X.values;\n            y = y.values\n        self.X = X; self.y = y\n        \n        ntot = X.shape[0]\n        y0 = np.mean(y) * np.ones([ntot])\n        prediction = y0.copy()\n        prm1 = 1.1; prm2 = 1.5\n        \n        for t in range(self.n_estimators):\n                        \n            if t == 0:\n                resid = y\n            else:\n                resid = (y - prediction)\n                if (mse(temp_resid,resid) < self.mse_cond):\n                    self.learning_rate = self.learning_rate/prm1\n                    self.mse_cond = self.mse_cond/prm2\n                    self.dm_depth = self.dm_depth+1\n            \n            d0 = self.min_size; d1 = self.max_depth+self.dm_depth\n            if self.tree_id == 'xgb':\n                submodel = Rtg(min_size=d0,max_depth=d1)\n            if self.tree_id == 'xgb_bagging':\n                submodel = RtgBag(min_size=d0,max_depth=d1,n_samples=self.n_samples)\n                \n            submodel.fit(X,-resid.astype('float64'))\n            y0 = submodel.predict(X).reshape([ntot])\n            self.treem_id.append(submodel)\n            prediction += self.learning_rate * y0\n            temp_resid = -resid\n\n        return self\n    \n    def predict(self,X):\n        \n        if(type(X) is not np.ndarray):\n            X = X.values;\n        \n        mtot = X.shape[0]\n        y_pred_gb = np.mean(self.y)*np.ones([mtot])\n        for t in range(self.n_estimators):\n            y_pred_gb += self.learning_rate * self.treem_id[t].predict(X).reshape([mtot])\n            \n        return y_pred_gb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:rgb(27, 79, 114)'> Imputation Method</span>\n\nLet's use a __two model based ensemble average__, one prediction which is based on __unsupervised__ and the other based on __supervised__ learning regression & simply average the two, __Bagging XGB__ is used in an attempt to not overfit the data, [previous tests](https://www.kaggle.com/shtrausslearning/xgb-bagging-regressor-tests) have shown such an approach consistently outperforms the based model."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\n# function that imputes a dataframe \ndef impute_model(df,cols=None):\n\n    # separate dataframe into numerical/categorical\n    ldf = df.select_dtypes(include=[np.number])           # select numerical columns in df\n    ldf_putaside = df.select_dtypes(exclude=[np.number])  # select categorical columns in df\n    # define columns w/ and w/o missing data\n    cols_nan = ldf.columns[ldf.isna().any()].tolist()         # list of features w/ missing data \n    cols_no_nan = ldf.columns.difference(cols_nan).values     # get all colun data w/o missing data\n    \n    if(cols is not None):\n        cols_nan = cols\n        df1 = ldf[cols_nan].describe()\n    \n    fill_id = -1\n    for col in cols_nan:    \n        fill_id+=1\n        imp_test = ldf[ldf[col].isna()]   # indicies which have missing data will become our test set\n        imp_train = ldf.dropna()          # all indicies which which have no missing data \n        model0 = GBoost(n_estimators=10,tree_id='xgb_bagging')\n        model1 = KNeighborsRegressor(n_neighbors=15)  # KNR Unsupervised Approach\n        knr = model0.fit(imp_train[cols_no_nan], imp_train[col])\n        xgb = model1.fit(imp_train[cols_no_nan], imp_train[col])\n        knrP = knr.predict(imp_test[cols_no_nan])\n        xgbP = xgb.predict(imp_test[cols_no_nan])\n        pred = (knrP + xgbP)*0.5\n        ldf.loc[df[col].isna(), col] = pred\n        ldf.loc[df[col].isna(),'fill_id'] = fill_id\n        \n    df2 = ldf[cols_nan].describe()\n    pd_html([df1,df2],['before imputation','after imputation'])\n        \n    return pd.concat([ldf,ldf_putaside],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA2 = impute_model(df_EDA,cols=['BUILD_YEAR','NEAREST_SCH_RANK'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We've added  about __10,000 properties__ without affecting the data statistics too much, that said it it won't guarantee we have correctly labeled them, so if these properties are associated with large errors in our __price__ prediction models, we can always reimpute them with some alterntive method.\n- It useful to flag these points as well, so they can be removed easily if we need to, this is done via <code>fill_id</code>, 2003 imputed for <code>BUILD_YEAR</code> &  10938 for <code>NEAREST_SCH_RANK</code>."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA2['fill_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. <span style='color:rgb(205, 0, 153)'> Model Evaluation </span>\n- The main goal of all of these models is to predict the target variable <code>PRICE</code>.\n- For our __evaluation metric__, lets use a more commonly used __RMSLE__ metric, some example applications can be found on [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1877050920316318) & why it is used over RMSE can be found on [medium](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a). \n- __Cross Validation__ is an important strategy to get an idea of how much the model varies across the dataset, here we'll use a 6 fold cross validation approach, together with a 70/30 split for training/evaluation model prediction as well, whilst we do have access to __test results__, we are not going to be tuning or using it as as an indicator to improve our model, since we don't want to overfit to test data. \n- __eval class__ is used as the main evaluation class. The class <code>.transform()</code> output stores the orignal feature together with the predicted training/test models results & and their indivual group_id, so we can identfy which data was used as __training data__ & which was used for __evaluation prediction__, group_id for __post model data exploration__.\n\n__Baseline Model__"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nimport seaborn as sns; import time\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom tqdm import tqdm_notebook\n\n# Class to Build Model\nclass eval(BaseEstimator,TransformerMixin):\n    \n    ''' Input: DataFrame Feature matrix + Target Variable '''\n    \n    def __init__(self,split_id=None,shuffle=False,verbose=False,time=False,\n                 target=None,n_cv=6,cv_yrange=None,hm_vvals=[0.5,1.0,0.75]):\n        self.split_id = split_id    # test side [float]\n        self.shuffle = shuffle      # shuffle option in train/test split [T/F]\n        self.target = target        # target name [str]\n        self.models = models        # list of models used in evaluation [list]\n        self.n_cv = n_cv          \n        self.cv_yrange=cv_yrange # May need to adjust yrane value of cv plot\n        self.hm_vvals = hm_vvals # Heatmap min,max,mid display values\n        self.verbose = verbose \n        self.time = time \n        \n    def fit(self,X,y=None):\n        return self\n        \n    def transform(self,X):\n            \n        if(self.time):\n            t0 = time.time()\n            \n        ''' Split input dataframe into Numerical & Categorical Features '''\n        lX,lXo = transformer(select_dtype=True).transform(X=X)\n\n        ''' Split Data into Training & Evaluation Datasets '''\n        train_df,eval_df = train_test_split(lX,test_size=self.split_id,shuffle=self.shuffle,random_state=32)\n        y_train = train_df[self.target]\n        X_train = train_df.loc[:, train_df.columns != self.target]\n        y_eval = eval_df[self.target]\n        X_eval = eval_df.loc[:, eval_df.columns != self.target]\n              \n        ''' Print Info '''\n        print('features:')\n        print(f'X_train {X_train.columns}')\n        print(f'X_train shape: {X_train.shape}')\n        print(f'X_eval shape: {X_eval.shape}')\n        \n        ''' Use Numpy instead of DataFrame '''\n        if(type(X) is not np.ndarray):\n            X_train = X_train.values;\n            y_train = y_train.values\n            X_eval = X_eval.values\n            y_eval = y_eval.values\n        \n        ''' Main Evaluation Loop, cycle though models in global list '''\n        lst_res = []; names = []; lst_train = []; lst_eval = []; lst_res_mean = []\n        for name, model in self.models:  # cycle through models & evaluate either cv or train/test\n            \n            if(self.verbose):\n                print('')\n                print(f'Running Model: {name}')\n            names.append(name)\n        \n            ''' Pipeline Train/Test Split Prediction '''\n            model.fit(X_train,y_train)\n\n            ''' I. Standard Cross Validation '''\n            \n            # RMSLE score\n            def rmsle(y, y0):\n                assert len(y) == len(y0)\n                return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n            rmsle_score = make_scorer(rmsle, greater_is_better=True)\n            \n#             cv_score = np.sqrt(-cross_val_score(pipe,X_train,y_train,cv=self.n_cv,scoring='neg_mean_squared_error'))\n            cv_score = cross_val_score(model,X_train,y_train,cv=self.n_cv,scoring=rmsle_score)\n    \n            # Print & Store in List CV results\n            if(self.verbose):\n                print(\"Scores:\",cv_score);print(\"Mean:\", cv_score.mean().round(3));print(\"std:\", cv_score.std().round(3)) \n            lst_res.append(cv_score)              # store cross validation scores in list\n            lst_res_mean.append(cv_score.mean())  # store mean cross validation score in list\n\n            ''' II. Train/Test Split Evaluation '''\n            y_model1 = model.predict(X_train)     # predict on training data\n            y_model2 = model.predict(X_eval);     # predict on test data\n            \n            if(self.time):\n                t1 = time.time()\n                print(f'{name} - time: {round((t1-t0),4)}')\n            \n            # Print & Store in List Train/Eval Prediction Scores\n            if(self.verbose):\n                print(f'Train/Test Scores: {rmsle(y_train,y_model1).round(3)} : {rmsle(y_eval,y_model2).round(3)}') \n            lst_train.append(rmsle(y_train,y_model1).round(3))\n            lst_eval.append(rmsle(y_eval,y_model2).round(3))\n            \n            ''' Store Results '''\n            train_df[f'{name}_error'] = abs(y_model1-y_train) # store abs of difference between model & true value (train)\n            eval_df[f'{name}_error'] = abs(y_model2-y_eval) # store abs of difference between model & true value (test)\n            train_df['group_id'] = 0; eval_df['group_id'] = 1   # define training(0)/test(1) group data identifier (useful for plot)\n            train_df[f'{name}'] = y_model1 # store training model prediction\n            eval_df[f'{name}'] = y_model2  # store evaluation model prediction\n            ldf_out = pd.concat([train_df,eval_df],axis=0) \n            \n        ''' Regroup Numerical & Categorical Features '''\n        ldf_out_all = pd.concat([ldf_out,lXo],axis=1) # add non numerical features to output\n        \n        ''' Plot Cross Validation Bar Plots & Heatmap of median CV + Train/Test Results '''\n        \n        # For Heatmap Output\n        s0 = pd.Series(np.array(lst_res_mean),index=names)\n        s1 = pd.Series(np.array(lst_train),index=names)\n        s2 = pd.Series(np.array(lst_eval),index=names)\n        pdf = pd.concat([s0,s1,s2],axis=1)\n        pdf.columns = ['cv_average','train','test']\n        \n        # Plot Results\n        sns.set(style=\"whitegrid\")\n        fig,ax = plt.subplots(1,2,figsize=(15,4))\n        ax[0].set_title(f'{self.n_cv} Cross Validation Results')\n        sns.boxplot(data=lst_res, ax=ax[0], orient=\"v\",width=0.2)\n        ax[0].set_xticklabels(names)\n        sns.stripplot(data=lst_res,ax=ax[0], orient='v',color=\".3\",linewidth=1)\n        ax[0].set_xticklabels(names)\n        ax[0].xaxis.grid(True)\n        ax[0].set(xlabel=\"\")\n        if(self.cv_yrange is not None):\n            ax[0].set_ylim(self.cv_yrange)\n        sns.despine(trim=True, left=True)\n    \n        sns.heatmap(pdf,vmin=self.hm_vvals[0],vmax=self.hm_vvals[1],center=self.hm_vvals[2],\n                    ax=ax[1],square=False,lw=2,annot=True,fmt='.4f',cmap='jet')\n#         ax[1].set_title(f'{scoring} scores')\n        plt.show()\n        \n            \n        return ldf_out_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('All Current Dataset Features')\nprint(df_EDA2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1. <span style='color:rgb(97, 47, 205)'> Baseline Models</span>\n### Model Exploration\n- Let's start by creating a __baseline model__, which will use the features that were [available before the dataset update](https://www.kaggle.com/shtrausslearning/perth-housing-price-prediction-eda).\n- The only variation that exist between the current set of features and the older dataset is that <code>LONGITUDE</code> & <code>LATITUDE</code> are more precise in this dataset, which actually improves the accuracy ( not shown here ).\n- Let's try out different models along the way & __pick out some promising ones__ as suggested in the [ML checklist](https://www.kaggle.com/shtrausslearning/machine-learning-project-checklist), the idea is to get an overall picture of how different models tend to perform on this dataset.\n- We will output the __cross validation__ results in the form of a barplot & cross validation mean, training & test RMSLE results in a seaborn heatmap.\n- Blank spots in the __heatmap__ indicates the model results have all or partial NaN results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# base column feature list\nprint('Baseline Feature List')\ntdf_base = df_EDA2.drop(['LONGITUDE','LATITUDE','SOLD_MONTH','SOLD_YEAR','fill_id','NEAREST_SCH_DIST','NEAREST_SCH_RANK','POSTCODE'],axis=1)\nprint(tdf_base.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import ConstantKernel, RBF\nfrom sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,\n                              ExtraTreesRegressor,AdaBoostRegressor)\nfrom sklearn.linear_model import LinearRegression,Lasso,ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\n''' Evaluate some promising models '''\n# let's look at how models perform overall, using default settings\n\nmodels = [] \nmodels.append(('LR',  LinearRegression()))\nmodels.append(('LASSO',Lasso()))\nmodels.append(('EN',ElasticNet()))  \nmodels.append(('KNN',KNeighborsRegressor()))        \nmodels.append(('CART',DecisionTreeRegressor()))     \nmodels.append(('SVR',SVR()))                        \nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\nmodels.append(('XGB', XGBRegressor(verbose_eval=False)))\nmodels.append(('CAT', CatBoostRegressor(silent=True))) \n\nif(heval2):\n    # Evaluate Base Model & Return Input DataFrame w/ Results \n    out_1 = eval(target='PRICE',split_id=0.3,shuffle=False,verbose=False,time=False,\n                 cv_yrange=(None,None),hm_vvals=[0.0,1.0,0.325]).transform(X=tdf_base)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Model Performance__\n- Whilst we have evaluation of __test data__, we will be making our observations based on __CV & training__ only, it is nice to know the __test results__ as well though.\n- Among the better performing models, we have <code>RandomForest</code>,<code>ExtraTreesRegressor</code>,<code>XGBRegressor</code> & <code>CatboostRegressor</code>, all of which are powerful ensemble approaches.\n- <code>CatboostRegressor</code> as the owners suggested is quite good right out of the box, performing quite well in the cross validation.\n- <code>RandomForest</code> performed quite well as well, having a good training score as well, which to some extent however is likely an overfit model.\n- <code>XGBoost Regressor</code> performed quite well on the trianing data (relative to other models), perhaps we need to tune its hyperparameters a little more. The default XGB doesn't always quite perform very well using the base class instantiation hyperparameters, as shown in [this notebook](https://www.kaggle.com/shtrausslearning/xgb-class-bagging-regressor-tests), let's also use the class shown in the same notebook, over the XGBoost library in the next test.\n- It was interesting to note that similar to other linear models, on ocassion, <code>XGBoost Regressor</code> had a __NaN__ result on the test set, other ensemble models did not have this issue.\n\n__Models that didn't work__\n\n<code>Gaussian Process Regressor</code>, was also attempted, however the code used in [this notebook](https://www.kaggle.com/shtrausslearning/airfoil-noise-prediction-modeling-using-gpr) unfortunatelly is not very efficient to evaluate the likelihood <code>objective function</code> quick enough for it to be viable, the same can be said for sklearn's version, which makes you wonder what you could do to make the approach more viable (aside from just using gridsearch). There are some libraries that increase the efficiency of <code>scipy's optimise.</code>"},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGB + Bagging Model\nLet's check out how our baseline XGB class model fairs, using two variants, __The Standard XGB Model__ &  __The Bagging Ensemble XGB Model__, using the same hyperparameter settings and compare it to the XGBRegressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [] \nmodels.append(('XGB-Library', XGBRegressor(n_estimators=25,learning_rate=0.3,max_depth=6,verbose_eval=False)))\nmodels.append(('BAG-XGB',GBoost(n_estimators=25,tree_id='xgb_bagging',learning_rate=0.3,max_depth=6)))\nmodels.append(('XGB',GBoost(n_estimators=25,tree_id='xgb',learning_rate=0.3,max_depth=6)))\n\n# Evaluate Base Model & Return Input DataFrame w/ Results \nif(heval2):\n    out_2 = eval(target='PRICE',split_id=0.3,shuffle=False,verbose=False,time=True,\n                 cv_yrange=(None,None),hm_vvals=[0.0,1.0,0.325]).transform(X=tdf_base)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Performance of <code>cross validation</code> is very similar to the XGB library, a little worse for the identical models. The current code is definitely not quite on par with the XGB library when it comes to efficiency, so some things to think about.\n- Like [previous tests](https://www.kaggle.com/shtrausslearning/xgb-class-bagging-regressor-tests), the __bagging model performs slightly better than the default model__, even for a more realistic problem, so that was interesting.\n- The current custom XGB class is also not quite as efficient, despite its slightly better cross validation for the __bagging model__, let's stick to the standard XGB library for now, since the efficiency of the library is much better.\n\n### Baseline Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,300),target='PRICE').transform(X=tdf_base) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It was interesting to note that <code>BEDROOMS</code> was not one of the more important features, desipite its high correlation to <code>PRICE</code>.\n- <code>FLOOR_AREA</code>, <code>CBD_DIST</code> & <code>BATHROOMS</code> are on the otherhand one of them more important features in the model.\n- Despite its low correlation to <code>PRICE</code>, in models like <code>CatBoost</code>, <code>LAND_AREA</code> is a relatively important feature. "},{"metadata":{},"cell_type":"markdown","source":"## 5.2. <span style='color:rgb(97, 47, 205)'> All Numerical Feature Models</span>\n- The next models use __all current numerical features available__ (<code>EDA2 DataFrame</code> w/o <code>fill_id</code>).\n- We'll also compare the feature importance and compare with the baseline feature evaluation, and see if there are any variations.\n- <code>RandomForest</code> & <code>CatBoost</code> were quite promising models as well, so let's compare the accuracy as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# base column feature list\ntdf_num = df_EDA2.drop(['fill_id'],axis=1)\n# tdf_num = df_EDA2.copy()\ntdf_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [] \nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\nmodels.append(('XGB', XGBRegressor(verbose_eval=False)))\nmodels.append(('CAT', CatBoostRegressor(silent=True))) \n\n# Evaluate Base Model & Return Input DataFrame w/ Results\nif(heval2):\n    out_3 = eval(target='PRICE',split_id=0.3,shuffle=False,verbose=False,\n                 cv_yrange=(None,None),hm_vvals=[0.0,1.0,0.325]).transform(X=tdf_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- The results are __significantly lower__ than before which is quite promising since we need to improve the model as much as possible.\n- <code>XGB</code> & <code>CAT</code> models interestingly enough failed on some __inner cross validation__, training & test sets, <code>RandomForest</code> on the other hand have no such issues with the set of features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,350),target='PRICE').transform(X=tdf_num) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- <code>NEAREST_SCH_RANK</code> & <code>ZIPCODE</code> are __quite important additions__ to the dataset, as shown in the <code>feature importance</code> table.\n- Looking at <code>XGBRegressor</code> & <code>CatboostRegressor</code>, it was interesting to note that even for this set of features, <code>bedrooms</code> & <code>bathrooms</code> aren't among the top of most important features, usually they are quite significant factors when house properties are presented to customers.\n- <code>SOLD_MONTH</code> is definitely a feature worth dropping. <code>GARAGE</code> also seems to be among the less important features, we definitely need to do more investigation into __feature transformation__. "},{"metadata":{},"cell_type":"markdown","source":"## 5.3. <span style='color:rgb(97, 47, 205)'> Numerical Feature XGB Bagging & Ensemble Models : Reduced Feature Update </span>\n- <code>NEAREST_SCH_DIST</code> and <code>SOLD_MONTH</code> are among the least important features as shown in the previous section, let's get rid of these features and see if there is any impact. \n- A reduction of features that have little contribution is quite desirable since they affect the training times."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical Features w/o SOLD_MONTH & NEAREST_SCH_DIST\ntdf_num_red1 = tdf_num.drop(['SOLD_MONTH','NEAREST_SCH_DIST'],axis=1)\ntdf_num_red1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [] \nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\nmodels.append(('XGB', XGBRegressor(verbose_eval=False)))\nmodels.append(('CAT', CatBoostRegressor(silent=True))) \n\n# Evaluate Base Model & Return Input DataFrame w/ Results \n# if(heval2):\nout_4 = eval(target='PRICE',split_id=0.3,shuffle=False,verbose=False,\n             cv_yrange=(None,None),hm_vvals=[0.0,1.0,0.325]).transform(X=tdf_num_red1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. <span style='color:rgb(205, 0, 153)'> EDA: Presold Housing Properties</span>\n- An interesting concept often encountered in housing are presold properties, not just in Australia, but worldwide in general. A __presold property__ is one which sold before it is physically built. \n- Interest in singling out __presold properties__ stems from the fact that it is a slightly different way of selling a property and could well be acting as outliers, we have yet to investigate in detail, what are the outliers in this problem.\n- The underlying commonality between such a lot of such properties is that __they generally tend to be cheaper__ than properties that are sold after they are built & one possible reason being that there are a certain risk associated with investing a large amount of savings into something that doesn't yet exist, which opens up buyers to the possibility of fraud from the constructors side.\n\n__Some things that can interest us:__\n\n- It would be of interest to build a model specifically to predict prebuilt property <code>PRICE</code>.\n- And investigate some potentially interesting statistics about these properties."},{"metadata":{},"cell_type":"markdown","source":"## 6.1. <span style='color:rgb(97, 47, 205)'> The Numbers </span>\n- It's probably interesting to understand the scale we're dealing with, how many properties are actually presold every year.\n- __568 out of 32k properties__ isn't quite a lot, let's check the numbers by <code>SOLD_YEAR</code> as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA_presold = tdf_num.copy()\ndf_EDA_presold['PRESOLD'] = df_EDA_presold['SOLD_YEAR'].astype('int') < df_EDA_presold['BUILD_YEAR']\ndf_EDA_presold['PRESOLD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dfx = df_EDA_presold.groupby(['SOLD_YEAR']).sum()\ndfx2 = df_EDA_presold['BUILD_YEAR'].value_counts()\n\n# Plot\nfig = go.Figure()\nfig.add_trace(go.Bar(x=dfx.index, y=dfx['PRESOLD'],width=1,name='Presold Properties by Year'))\nfig.add_trace(go.Bar(x=dfx2.index, y=dfx2.values,width=1,name='Properties Built by Year'))\n\n# Figure Aesthetics\n# fig.update_traces(marker_color='rgb(158,202,225)',marker_line_width=1, marker_line_color='rgb(8,48,107)',\nfig.update_traces(opacity=0.9)\nfig.update_layout(barmode='stack',template='plotly_white',height=300,title='Properties Built by Year')\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0});fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can note that the __number of presold properties is quite small__ compared to the total number of properties built every year.\n- __2013 & 2014__ saw a very high number of prebuilt properties being built, __exceeding 100 properties__ for the first time.\n\n## 6.2. <span style='color:rgb(97, 47, 205)'> Bivariate Scatter Feature Relation</span>\n- Let's take a look a scattermatrix relation, __differentiating between presold & non presold properties__, \n- Outlining some of the key features as per <code>feature importance</code> table seen in previous sections, since we have too many features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot Scatter Matrix using Plotly Express\ndef scat_mat(ldf,dim=None,colour=None,hov_name=None,title=None):\n    \n    fig = px.scatter_matrix(ldf,dimensions=dim,opacity=0.5,color=colour,hover_name=hov_name,height=1000)\n    fig.update_traces(marker=dict(size=5,line=dict(width=0.5,color='black')))\n    fig.update_layout(template='plotly_white',title=title) # stack/overlay/group\n    fig.update_traces(diagonal_visible=False)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tlist = ['PRICE','NEAREST_SCH_RANK','BATHROOMS','CBD_DIST','FLOOR_AREA']\nscat_mat(ldf=df_EDA_presold,dim=tlist,colour='PRESOLD',hov_name=df_EDA_presold.index,title='Presold Property Scatter Matrix Relations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- First things first, as mentioned at the start of this section, __presold properties__ have tended to be much cheaper than properties that are sold after completion.\n- Although less common, there were a number of __presold properties__ with schools that are within the top 50-60 & were quite affordable, however like __non presold properties__, properties with the nearest school being even higher ranked, the <code>PRICE</code> tended to go up quite steeply.\n- The number of <code>BATHROOMS</code> & <code>FLOOR_AREA</code> don't tend be very different to non prebuilt properties.\n- Presold properties tend to be slightly further away from the CBD, in the region of 20k-40k (m).\n\nLet's use geospatial maps to visualse their locations as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"if(heval):\n    plot_geo(ldf=df_EDA_presold[df_EDA_presold['PRESOLD']==True],feature='PRICE',title='Presold Properties Median House Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3. <span style='color:rgb(97, 47, 205)'> Build Year & Sold Year </span>\n- From __scattered data__, we already can notice that presold properties tend to be on the lower side compared to non-presold properties, however this doesn't shed any information about different moments in time.\n- Using pivot tables, let's find out __what kind of trends have been occuring every year__, looking at the __minimum__ & __median__ values.\n- We'll take a look at two features, <code>BUILD_YEAR</code> & <code>SOLD_YEAR</code>. <code>BUILD_YEAR</code> can oscillate quite a bit, given that it represents the entire property market, during which owners of properties could have already changed hands, even for <code>prebuilt</code> properties. Even in such a case, presold properties tend to be on the cheaper lower end of <code>PRICE</code>."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA2['PRESOLD'] = df_EDA2['SOLD_YEAR'].astype('int') < df_EDA2['BUILD_YEAR']\n\n# Subset of Interest\npresold = df_EDA2[(df_EDA2['PRESOLD'] == True) & (df_EDA2['fill_id'] != 0)]  # presold properties w/o some build year data\nnot_presold = df_EDA2[(df_EDA2['PRESOLD'] == False) & (df_EDA2['fill_id'] != 0)] # non presold properties w/o some build year data\n\n# Presold Properties \npre_buildyear_min = presold.pivot_table('PRICE',index='BUILD_YEAR',columns='SUBURB').min(axis=1)\npre_soldyear_min = presold.pivot_table('PRICE',index='SOLD_YEAR',columns='SUBURB').min(axis=1)\npre_buildyear_med = presold.pivot_table('PRICE',index='BUILD_YEAR',columns='SUBURB').median(axis=1)\npre_soldyear_med = presold.pivot_table('PRICE',index='SOLD_YEAR',columns='SUBURB').median(axis=1)\n\n# Non Presold Properties\nbuildyear_min = not_presold.pivot_table('PRICE',index='BUILD_YEAR',columns='SUBURB').min(axis=1)\nsoldyear_min = not_presold.pivot_table('PRICE',index='SOLD_YEAR',columns='SUBURB').min(axis=1)\nbuildyear_med = not_presold.pivot_table('PRICE',index='BUILD_YEAR',columns='SUBURB').median(axis=1)\nsoldyear_med = not_presold.pivot_table('PRICE',index='SOLD_YEAR',columns='SUBURB').median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nfig = make_subplots(rows=1,cols=2,subplot_titles=['BUILD_YEAR','SOLD_YEAR'])\n\n# Build Year Plots\n\nfig.add_trace(go.Scatter(x=pre_buildyear_min.index, y=pre_buildyear_min.values,name='Presold: Min Price by Build Year'),row=1, col=1)\nfig.add_trace(go.Scatter(x=buildyear_min.index, y=buildyear_min.values,name='Non-Presold: Min Price by Build Year'),row=1, col=1)\n\n# Sold Year Plots\nfig.add_trace(go.Scatter(x=pre_soldyear_min.index, y=pre_soldyear_min.values,name='Presold: Min Price by Sold Year'),row=1, col=2)\nfig.add_trace(go.Scatter(x=soldyear_min.index, y=soldyear_min.values,name='Non-Presold: Min Price by Sold Year'),row=1, col=2)\n\n# Plot Aesthetics\nfig.update_layout(template='plotly_white',title='Minimum Property PRICE')\nfig.update_yaxes(range=[0,500000]);fig.update_xaxes(range=[1990,2020])\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=['BUILD_YEAR','SOLD_YEAR'])\n\n# Build Year Plots\nfig.add_trace(go.Scatter(x=pre_buildyear_med.index, y=pre_buildyear_med.values,name='Presold: Median Price by Build Year'),row=1, col=1)\nfig.add_trace(go.Scatter(x=buildyear_med.index, y=buildyear_med.values,name='Non-Presold: Median Price by Build Year'),row=1, col=1)\n\n# Sold Year Plots\nfig.add_trace(go.Scatter(x=pre_soldyear_med.index, y=pre_soldyear_med.values,name='Presold: Median Price by Sold Year'),row=1, col=2)\nfig.add_trace(go.Scatter(x=soldyear_med.index, y=soldyear_med.values,name='Non-Presold: Median Price by Sold Year'),row=1, col=2)\n\n# Plot Aesthetics\nfig.update_layout(template='plotly_white',title='Median Property Price')\nfig.update_yaxes(range=[0,800000])\nfig.update_xaxes(range=[1990,2020])\nfig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Build Year__\n\n- For a non-presold property, __min__ & __median__ values of <code>PRICE</code> can fluctuate a bit, due to the variability of market & properties having gone through multiple sales by the time the latest <code>PRICE</code> value was recorded.\n- Presold properties on the other hand tend to be more on the lower end for both __minimum__ & __median__ values, when looking at data from 1990.\n\n__Sold Year__\n\n- <code>SOLD_YEAR</code> in contrast tells us how the market has tended to evolve, since the value represents the last sale year. The __minimum value__ for both has tended to be very similar up to about 2000, both __minimum__ & __median__ <code>PRICE</code> values. \n- In the recent decade there has tended to be quite a large gap between the __minimum__ & __median__ <code>PRICE</code> values between pre-sold & non-presold properties built during the same year.\n\n## 6.4. <span style='color:rgb(97, 47, 205)'> Presold Property Model </span>\n- Having gained some insight into out data, we noted that __presold__ properties do exist in our data, and that they tend to be on the lower end of the <code>PRICE</code> range. \n- It would be interesting to figure out what kind of model works for this subset of data. The reason is motivated by the fact that __higher error__ tend to be associated with higher <code>PRICE</code> values, so if we can narrow down our data and create specific subgroups without simply using <code>PRICE</code>, we might be able to improve our model.\n- Very simple thought, let's create One-Hot-Encoding of a <code>PRESOLD</code> logistical feature to start off."},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf_num_red1['PRESOLD'] = tdf_num_red1['SOLD_YEAR'].astype('int') < tdf_num_red1['BUILD_YEAR']\npresold_dummy = pd.get_dummies(tdf_num_red1['PRESOLD'],prefix='PRESOLD',drop_first=True)\ntdf_num_red2 = pd.concat([tdf_num_red1,presold_dummy],axis=1)\ntdf_num_red2.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Feature Importance__\n\nAside from the __XGB Model__, One-Hot-Encoding doesn't seem to have any serious influence on feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer(feature_importance=True,figsize=(800,350),target='PRICE').transform(X=tdf_num_red2) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Model Evaluation__\n\n- We can note that the addition has had a positive effect, the overall score has gone down in cross validation.\n- The __CAT model__ most notably seems to perform quite well, however there still is a subset of data that tends to cause the model to output NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"out_5 = eval(target='PRICE',split_id=0.3,shuffle=False,verbose=False,\n             cv_yrange=(None,None),hm_vvals=[0.0,1.0,0.325]).transform(X=tdf_num_red2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}