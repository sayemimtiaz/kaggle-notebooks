{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing Required Library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#SMOTE to balance the Imbalance Data\nfrom imblearn.over_sampling import SMOTE\n\n#for Spliting Data and Hyperparameter Tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Importing Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom catboost import CatBoostClassifier\n    \n#Bagging Algo\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.neural_network import MLPClassifier\n\n#To tranform data\nfrom sklearn import preprocessing\n\n#statistical Tools\nfrom sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\n#Setting Format\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/lt-vehicle-loan-default-prediction/train.csv\")\ntest = pd.read_csv(\"../input/lt-vehicle-loan-default-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape,train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loan_default.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets looks at data description\ninfo = pd.read_csv(\"../input/lt-vehicle-loan-default-prediction/data_dictionary.csv\")\ninfo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing all the Spaces with '_'\ntrain.columns = train.columns.str.replace('.','_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()\n\n#So only Employment Type data is missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(train.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Digging Few Columns for Insight"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets Look at few columns\n\ncolumns_unique = ['UniqueID','MobileNo_Avl_Flag',\n         'Current_pincode_ID','Employee_code_ID',\n         'NO_OF_INQUIRIES','State_ID',\n         'branch_id','manufacturer_id','supplier_id']\n\n\nunique_col = train[columns_unique]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_col.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at all unique values\nfor i in unique_col.columns:\n    print(i,\" : distinct_value\")\n    print(unique_col[i].nunique(),\" : No. of unique Items\")\n    #print(unique_col[i].unique())\n    print(\"-\"*30)\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_col.hist(bins=5, figsize=(16,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"UniqueID = It is provided to every customer so its Unique and will always be different\n\nMobileNo_Avl_Flag = Whether person provided Mobile No. Doesn't tell us if loan will default\n\nCurrent_pincode_ID = It is Customers address we don't need that for Prediction\n\nEmployee_code_ID = Employee ID is not required as it doesn't related with Loan_defualt\n\nNO_OF_INQUIRIES = No. of Inquiries to loan doesn't help us to determine wheather loan will default or not\n\nState_ID = It is where loan is availed and doesn't add much to prediction to loan default\n\nbranch_id = Branch ID isn't relevent to Data Processing\n\nmanufacturer_id = Manufacturer ID doesn't add much too data\n\nsupplier_id = Supplier ID doesn't add much too data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def columns_drop(data):\n    data.drop(unique_col,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_drop(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we have 2 Columns named \"AVERAGE_ACCT_AGE\" & \"CREDIT_HISTORY_LENGTH\".\n#They have AplhNumeric Values Lets change them to Months\n\ndef change_col_month(col):\n    year = int(col.split()[0].replace('yrs',''))\n    month = int(col.split()[1].replace('mon',''))\n    return year*12+month\n\ndef months_transformation(data):\n    data['CREDIT_HISTORY_LENGTH'] = data['CREDIT_HISTORY_LENGTH'].apply(change_col_month)\n    data['AVERAGE_ACCT_AGE'] = data['AVERAGE_ACCT_AGE'].apply(change_col_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months_transformation(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot = data.iloc[:test.shape[0]]\nplot = train[train['AVERAGE_ACCT_AGE']<175]\nsns.lineplot(x=train['AVERAGE_ACCT_AGE'],y=train['loan_default'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot = data.iloc[:test.shape[0]]\nplot = train[train['CREDIT_HISTORY_LENGTH']<200]\nsns.lineplot(x=train['CREDIT_HISTORY_LENGTH'],y=train['loan_default'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform CNS Score And Create New Columns"},{"metadata":{},"cell_type":"markdown","source":"Now lets look at CNS Score Description\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.PERFORM_CNS_SCORE_DESCRIPTION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_not_scored(n):\n    #here we are spliting letters before '-'.\n    score=n.split(\"-\")\n    \n    if len(score)!=1:\n        return score[0]\n    else:\n        return 'N'\n\ndef transform_CNS_Description(data):\n    data['CNS_SCORE_DESCRIPTION']=data['PERFORM_CNS_SCORE_DESCRIPTION'].apply(replace_not_scored).astype(np.object)\n    \n    #Now Transform CNS Score Description data into Numbers\n\n    sub_risk = {'N':-1, 'K':0, 'J':1, 'I':2, 'H':3, 'G':4, 'E':5,'F':6, 'L':7, 'M':8, 'B':9, 'D':10, 'A':11, 'C':12}\n\n    data['CNS_SCORE_DESCRIPTION'] = data['CNS_SCORE_DESCRIPTION'].apply(lambda x: sub_risk[x])\n    \ntransform_CNS_Description(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_PERFORM_CNS_SCORE_DESCRIPTION(data):\n    #Replacing all the values into Common Group\n\n    data['PERFORM_CNS_SCORE_DESCRIPTION'].replace({'C-Very Low Risk':'Very Low Risk',\n                                                 'A-Very Low Risk':'Very Low Risk',\n                                                 'D-Very Low Risk':'Very Low Risk',\n                                                 'B-Very Low Risk':'Very Low Risk',\n                                                 'M-Very High Risk':'Very High Risk',\n                                                 'L-Very High Risk':'Very High Risk',\n                                                 'F-Low Risk':'Low Risk',\n                                                 'E-Low Risk':'Low Risk',\n                                                 'G-Low Risk':'Low Risk',\n                                                 'H-Medium Risk':'Medium Risk',\n                                                 'I-Medium Risk':'Medium Risk',\n                                                 'J-High Risk':'High Risk',\n                                                 'K-High Risk':'High Risk'},\n                                                  inplace=True)\n\n    #Transformin them into Numeric Features\n\n    risk_map = {'No Bureau History Available':-1, \n                  'Not Scored: No Activity seen on the customer (Inactive)':-1,\n                  'Not Scored: Sufficient History Not Available':-1,\n                  'Not Scored: No Updates available in last 36 months':-1,\n                  'Not Scored: Only a Guarantor':-1,\n                  'Not Scored: More than 50 active Accounts found':-1,\n                  'Not Scored: Not Enough Info available on the customer':-1,\n                  'Very Low Risk':4,\n                  'Low Risk':3,\n                  'Medium Risk':2, \n                  'High Risk':1,\n                  'Very High Risk':0}\n\n    data['PERFORM_CNS_SCORE_DESCRIPTION'] = data['PERFORM_CNS_SCORE_DESCRIPTION'].map(risk_map)\n\ntransform_PERFORM_CNS_SCORE_DESCRIPTION(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = train['PERFORM_CNS_SCORE_DESCRIPTION'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treating Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Employment_Type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"defa = pd.crosstab(train['Employment_Type'], train['loan_default'])\nprint(defa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_employment_type(data):\n    data['Employment_Type'] = data['Employment_Type'].fillna('Salaried')\n    employment_map = {'Self employed':0, 'Salaried':1, 'Not_employed':-1}\n\n    data['Employment_Type'] = data['Employment_Type'].apply(lambda x: employment_map[x])\nfill_employment_type(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\nchi_sq, p_value, deg_freedom, exp_freq = stats.chi2_contingency(defa)\nprint('Chi Square Statistics',chi_sq)\nprint('p-value',p_value)\nprint('Degree of freedom',deg_freedom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = train['Employment_Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming Primary and Secondary Accounts"},{"metadata":{"trusted":true},"cell_type":"code","source":"pri_columns = ['PRI_NO_OF_ACCTS','SEC_NO_OF_ACCTS',\n           'PRI_ACTIVE_ACCTS','SEC_ACTIVE_ACCTS',\n           'PRI_OVERDUE_ACCTS','SEC_OVERDUE_ACCTS',\n           'PRI_CURRENT_BALANCE','SEC_CURRENT_BALANCE',\n           'PRI_SANCTIONED_AMOUNT','SEC_SANCTIONED_AMOUNT',\n           'PRI_DISBURSED_AMOUNT','SEC_DISBURSED_AMOUNT',\n           'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT']\n\npri_df = train[pri_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_col(data):\n    #Creating and Sorting Columns\n\n    data['NO_OF_ACCTS'] = data['PRI_NO_OF_ACCTS'] + data['SEC_NO_OF_ACCTS']\n\n    data['ACTIVE_ACCTS'] = data['PRI_ACTIVE_ACCTS'] + data['SEC_ACTIVE_ACCTS']\n\n    data['OVERDUE_ACCTS'] = data['PRI_OVERDUE_ACCTS'] + data['SEC_OVERDUE_ACCTS']\n\n    data['CURRENT_BALANCE'] = data['PRI_CURRENT_BALANCE'] + data['SEC_CURRENT_BALANCE']\n\n    data['SANCTIONED_AMOUNT'] = data['PRI_SANCTIONED_AMOUNT'] + data['SEC_SANCTIONED_AMOUNT']\n\n    data['DISBURSED_AMOUNT'] = data['PRI_DISBURSED_AMOUNT'] + data['SEC_DISBURSED_AMOUNT']\n\n    data['INSTAL_AMT'] = data['PRIMARY_INSTAL_AMT'] + data['SEC_SANCTIONED_AMOUNT']\n    \n    data.drop(pri_columns, axis=1, inplace=True)\n\nnew_col(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_columns = ['NO_OF_ACCTS', 'ACTIVE_ACCTS', 'OVERDUE_ACCTS', 'CURRENT_BALANCE',\n       'SANCTIONED_AMOUNT', 'DISBURSED_AMOUNT', 'INSTAL_AMT']\n\nfor i in new_columns:\n    print(i,\" : distinct_value\")\n    print(train[i].nunique(),\" : No. of unique Items\")\n    #print(data[i].unique())\n    print(\"-\"*30)\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and Treating Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=train['ACTIVE_ACCTS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=train['NO_OF_ACCTS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=train['OVERDUE_ACCTS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = train['CURRENT_BALANCE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mode_impute_outlier(data):\n    li = list(data['ACTIVE_ACCTS'].sort_values()[-3:].index)\n    data['ACTIVE_ACCTS'][li] = int(data.drop(li)['ACTIVE_ACCTS'].mode())\n    li = list(data['NO_OF_ACCTS'].sort_values()[-4:].index)\n    data['NO_OF_ACCTS'][li] = int(data.drop(li)['NO_OF_ACCTS'].mode())\n    li = list(data['OVERDUE_ACCTS'].sort_values()[-10:].index)\n    data['OVERDUE_ACCTS'][li] = int(data.drop(li)['OVERDUE_ACCTS'].mode())\n    li = list(data['CURRENT_BALANCE'].sort_values()[-15:].index)\n    data['CURRENT_BALANCE'][li] = int(data.drop(li)['CURRENT_BALANCE'].mode())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_impute_outlier(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets take a look at Date of Birth Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Date_of_Birth.min(), train.Date_of_Birth.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_age = train[['disbursed_amount', 'asset_cost', 'ltv', 'Date_of_Birth','DisbursalDate','loan_default']]\ndf_age.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Date_of_Birth = Date of birth of the customer\t\n\nDisbursal_Date = Date of disbursement\n\nDisbursement means the payment of money from a fund."},{"metadata":{"trusted":true},"cell_type":"code","source":"def age(dob):\n    yr = int(dob[-2:])\n    if yr >=0 and yr < 20:\n        return yr + 2000\n    else:\n         return yr + 1900\n        \ndf_age['Date_of_Birth'] = df_age['Date_of_Birth'].apply(age)\ndf_age['DisbursalDate'] = df_age['DisbursalDate'].apply(age)\ndf_age['Age']=df_age['DisbursalDate']-df_age['Date_of_Birth']\ndf_age=df_age.drop(['DisbursalDate','Date_of_Birth'],axis=1)\n\ndf_age.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_age(data):\n    data['Date_of_Birth'] = data['Date_of_Birth'].apply(age)\n    data['DisbursalDate'] = data['DisbursalDate'].apply(age)\n    # Age of applicant when he/she applied for Loan\n    data['Age'] = data['DisbursalDate'] - data['Date_of_Birth']\n    data = data.drop( ['DisbursalDate', 'Date_of_Birth'], axis=1)\n    \ncalculate_age(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed = []\ntransformed_with_one = []\nnot_transformed = []\n\ndef column_to_transform(data):\n\n    num_col = ['disbursed_amount', 'asset_cost', 'ltv', 'PERFORM_CNS_SCORE',\n            'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',\n           'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'AVERAGE_ACCT_AGE',\n           'NO_OF_ACCTS', 'ACTIVE_ACCTS', 'OVERDUE_ACCTS', 'CURRENT_BALANCE',\n           'SANCTIONED_AMOUNT', 'DISBURSED_AMOUNT', 'INSTAL_AMT', 'Age']\n    \n    num_col_data = data[num_col]\n    \n    def transformation_boxcox(num_col_data):\n    \n        from scipy.stats import boxcox\n\n        for i in num_col:\n            if num_col_data[i].min() > 0:\n                num_col_data[i] = boxcox(num_col_data[i])[0]\n                transformed.append(i)\n            elif num_col_data[i].min() == 0:\n                num_col_data[i] = boxcox(num_col_data[i]+1)[0]\n                transformed_with_one.append(i)\n            else:\n                num_col_data[i] = num_col_data[i]\n                not_transformed.append(i)\n        print(\"Successful\")\n    \n    transformation_boxcox(data)\n\ncolumn_to_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_processing(data):\n    test.columns = test.columns.str.replace('.','_')\n    columns_drop(data)\n    months_transformation(data)\n    transform_CNS_Description(data)\n    transform_PERFORM_CNS_SCORE_DESCRIPTION(data)\n    fill_employment_type(data)\n    new_col(data)\n    mode_impute_outlier(data)\n    calculate_age(data)\n    column_to_transform(data)\n    preprocessing.RobustScaler()\n    scaler.transform(data)\n    return data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['loan_default'], axis=1)\ny = train['loan_default']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balance Data using SMOTE"},{"metadata":{},"cell_type":"markdown","source":"SMOTE is python library which is used when the data is imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE()\nX_tf,y_tf = smote.fit_resample(X,y)\nX_tf.shape, y_tf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.RobustScaler()\nX_tf = scaler.fit_transform(X_tf)\n\n# Split the data into training and testing sets \nx_train,x_test,y_train,y_test = train_test_split(X_tf,y_tf,test_size = .1, random_state = 3300)\n\nprint(x_train.shape[0], x_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traning Our Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = {}\nroc_r = {}\n\ndef train_model(model, model_name):\n    print(model_name)\n    \n    # Fitting model\n    model = model.fit(x_train, y_train)\n    pred = model.predict(x_test)\n    \n    #Model accuracy\n    acc = accuracy_score(y_test, pred)*100\n    accuracy[model_name] = acc\n    print('accuracy_score',acc)\n    print('precision_score',precision_score(y_test, pred)*100)\n    print('recall_score',recall_score(y_test, pred)*100)\n    print('f1_score',f1_score(y_test, pred)*100)\n    \n    \n    #ROC Score\n    roc_score = roc_auc_score(y_test, pred)*100\n    roc_r[model_name] = roc_score\n    print('roc_auc_score',roc_score)\n    \n    # Confusion matrix\n    print('confusion_matrix')\n    print(pd.DataFrame(confusion_matrix(y_test, pred)))\n    \n    #ROC Score\n    fpr, tpr, threshold = roc_curve(y_test, pred)\n    roc_auc = auc(fpr, tpr)*100\n    \n    #ROC Plot\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(weights='distance', algorithm='auto', n_neighbors=15, n_jobs=4)\n\ntrain_model(knn, 'K Nearest Neighbour')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=5.0, solver='saga')\n\ntrain_model(lr, 'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='gini', splitter='random', max_depth=25, min_samples_split=4,\n                            min_samples_leaf=2)\n\ntrain_model(dtc, 'Decision Tree Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb = BernoulliNB()\n\ntrain_model(bnb, 'Bernolli Naive Bayes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 1500, n_jobs=-1, max_depth=15, \n                             min_samples_split=5, min_samples_leaf=3)\n\ntrain_model(rfc, 'Random Forest Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(n_estimators=720, n_jobs=-1, max_depth=15, min_child_weight=5, \n                      min_child_samples=5, num_leaves=10, learning_rate=0.15)\n\ntrain_model(lgbm, 'LGBMClassifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = CatBoostClassifier(verbose = 0)\n\ntrain_model(cat, \"Cat Boost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(hidden_layer_sizes = (200,3), activation = 'relu', solver = 'adam', learning_rate = 'adaptive',\n                   max_iter = 1000)\n\ntrain_model(mlp, 'Multi-layer Perceptron Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(n_estimators = 1500, nthread  = 4, max_depth = 15, min_child_weight = 5, learning_rate=0.1)\n\ntrain_model(xgb, 'XGBClassifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr = XGBRFClassifier(n_estimators = 2000, nthread  = 4, max_depth = 10, min_child_weight = 4, learning_rate=0.1)\n\ntrain_model(xgbr, 'XGBRFClassifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc = GradientBoostingClassifier(n_estimators=1000, min_samples_split=5, max_depth=15)\n\ntrain_model(gbc, 'GradientBoostingClassifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1)\n\ntrain_model(ada, 'AdaBoostClassifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.ensemble import StackingClassifier\n\nestimator = [('Lgbr', lgbm), ('xgb', xgb), ('gbc', gbc), ('mlp', mlp)]\n\nsc = StackingClassifier(estimators = estimator, final_estimator = lgbm, n_jobs=-1)\n\ntrain_model(sc, 'StackingClassifier')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted values\ny_head_lr = lr.predict(x_test)\ny_head_knn = knn.predict(x_test)\ny_head_xgb = xgb.predict(x_test)\ny_head_nb = bnb.predict(x_test)\ny_head_dtc = dtc.predict(x_test)\ny_head_rfc = rfc.predict(x_test)\ny_head_lgbm = lgbm.predict(x_test)\ny_head_ada = ada.predict(x_test)\ny_head_gbc = gbc.predict(x_test)\ny_head_mlp = mlp.predict(x_test)\ny_head_cat = cat.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_xgb = confusion_matrix(y_test,y_head_xgb)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rfc = confusion_matrix(y_test,y_head_rfc)\ncm_lgbm = confusion_matrix(y_test,y_head_lgbm)\ncm_ada = confusion_matrix(y_test,y_head_ada)\ncm_gbc = confusion_matrix(y_test,y_head_gbc)\ncm_mlp = confusion_matrix(y_test,y_head_mlp)\ncm_cat = confusion_matrix(y_test,y_head_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(4,3,5)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\nplt.subplot(4,3,6)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,2)\nplt.title(\"XGB Confusion Matrix\")\nsns.heatmap(cm_xgb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,3)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,1)\nplt.title(\"Random Forest Gini Confusion Matrix\")\nsns.heatmap(cm_rfc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,7)\nplt.title(\"LightGB Confusion Matrix\")\nsns.heatmap(cm_lgbm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,8)\nplt.title(\"Ada Boost Confusion Matrix\")\nsns.heatmap(cm_ada,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,9)\nplt.title(\"Gradient boost Classifier Confusion Matrix\")\nsns.heatmap(cm_gbc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,10)\nplt.title(\"Multi-layer Perceptron Classifier Confusion Matrix\")\nsns.heatmap(cm_mlp,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,11)\nplt.title(\"Cat boost Classifier Confusion Matrix\")\nsns.heatmap(cm_cat,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,5))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nplt.xticks(rotation = 90)\nsns.barplot(x=list(accuracy.keys()), y=list(accuracy.values()), palette=\"cubehelix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now train it with whole Traning dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat.fit(X_tf, y_tf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we will predict on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['UniqueID'] = test['UniqueID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processing(test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['loan_default'] = cat.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}