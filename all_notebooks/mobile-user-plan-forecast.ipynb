{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"**It's common to use mobile user information (such as personal basic information, consumption habits and preferences, etc.) to match and recommend the most suitable plan for users which is quite useful for follow-up personalized service.**\n\n**This scenario is derived from mobile operators.**\n\n**Goal is to predict users' plan (current_service) accurately.**"},{"metadata":{},"cell_type":"markdown","source":"# Data files\nFirst round train set and test set：train_all.csv,test_1.csv  <br />\nSecond round train set and test set：train_2.csv,test_2.csv <br />\n**Here we just combine two together and treat as one. <br />**"},{"metadata":{},"cell_type":"markdown","source":"# Data attributes\n| Attribute     | Meaning| Type|  Comment |\n|:-------:|:-------:|:-------:|:-------:|\n|USERID|\tUser ID|\tVARCHAR2(50)|\tUser code，primary key|\n|current_service|\tPlan\t|VARCHAR2(500)\t|plan code|\n|service_type\t|Plan type\t|VARCHAR2(10)\t|0：2G 3Gmix，1：2I2C，2：2G，3：3G，4：4G|\n|is_mix_service\t|If mix service|\tVARCHAR2(10)|\t1.True 0.False|\n|online_time\t|Online time|\tVARCHAR2(50)\t|/|\n|1_total_fee|\tTotal billing amount current month\t|NUMBER|\t￥|\n|2_total_fee\t|Total billing amount last month|\tNUMBER\t|￥|\n|3_total_fee|\tTotal billing amount last last month|\tNUMBER\t￥|\n|4_total_fee\t|Total billing amount last last last month\t|NUMBER|\t￥|\n|month_traffic\t|Month traffic\t|NUMBER|\tMB|\n|many_over_bill|\tSuccessive over bill\t|VARCHAR2(500)|\t1.True 0.False|\n|contract_type|\tContract type|\tVARCHAR2(500)\t|ZBG_DIM.DIM_CBSS_ACTIVITY_TYPE|\n|contract_time|\tContract time|\tVARCHAR2(500)|\t/|\n|is_promise_low_consume\t|If promise low consumer|\tVARCHAR2(500)\t|1.True 0.False|\n|net_service\t|Net service|\tVARCHAR2(500)\t|20AAAAAA-2G|\n|pay_times\t|Pay times\t|NUMBER\t|Time|\n|pay_num\t|Pay number\t|NUMBER\t|￥|\n|last_month_traffic\t|Last month traffic rest|\tNUMBER|\tMB|\n|local_trafffic_month|\tLocal cumulative trafffic month\t|NUMBER\t|MB|\n|local_caller_time|\tLocal cumulative caller time|\tNUMBER|\tMinute|\n|service1_caller_time\t|Service1_caller_time|\tNUMBER\t|Minute|\n|service2_caller_time\t|Service2_caller_time|\tNUMBER\t|Minute|\n|gender|\tGender\t|varchar2(100)\t|01.male 02.femle|\n|age|\tAge|\tvarchar2(100)|\t/|\n|complaint_level\t|Complaint level|\tVARCHAR2(1000)\t|1：normal，2：important，3：significant|\n|former_complaint_num|Former complaint times|\tNUMBER\t|Time|\n|former_complaint_fee|\tFormer complaint fee\t|NUMBER\t|￥|"},{"metadata":{},"cell_type":"markdown","source":"# Environment dependency\n- python3\n- pandas\n- scikit-learn\n- gensim\n- xgboost/LightGBM/Catboost"},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndata = pd.read_csv(\"/kaggle/input/user-package-information-of-mobile-operators/train_all.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Feature engineering1: time series data vectorization by word2vec\n\nHere we convert total_fee for four consecutive months into four 10-dimension vectors, because **word2vec is good at representing sequence features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nfrom gensim.corpora import WikiCorpus\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nimport multiprocessing\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_write='./'\n\npath = '../input/user-package-information-of-mobile-operators'\n# word2vec path\nsave_path = path_write + '/w2v'\nif not os.path.exists(save_path):\n    print(save_path)\n    os.makedirs(save_path)\n\n# train,test set path\ntrain1 = pd.read_csv(path + '/train_all.csv')\ntrain = pd.read_csv(path + '/train_2.csv')\ntest = pd.read_csv(path + '/test_2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat\ndata = pd.concat([train, test, train1]).reset_index(drop=True).sample(frac=1, random_state=2020).fillna(0)\ndata = data.replace('\\\\N', 999)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word2vec\nsentence = []\nfor line in list(data[['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']].values):\n    sentence.append([str(float(l)) for idx, l in enumerate(line)])\n# set vector dimension\nL = 10\n\n# word2vec training\nprint('word2vec start training...')\nmodel = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(), iter=10)\nprint('embedding vecters saved...')\n\n# word2vec extracting\nfor fea in ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']:\n    # extract total_fee by columns\n    values = []\n    values = list(data[fea].values)\n    values = set(values)\n    \n    print(len(values))\n    \n    # get vectors and form DataFrame\n    w2v = []\n    for i in values:\n        a = [i]\n        a.extend(model[str(float(i))])\n        w2v.append(a)\n    out_df = pd.DataFrame(w2v)\n    \n    # rename DataFrame columns\n    name = [fea]\n    for i in range(L):\n        name.append(name[0] + 'W' + str(i))\n    out_df.columns = name\n    print(out_df.columns)\n    out_df.to_csv(save_path + '/' + fea + '_w2v.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## word2vec result visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport multiprocessing\nimport numpy as np\nimport random\nimport sys\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(save_path+'/1_total_fee_w2v.csv')\nl=list(df['1_total_fee'].astype('str'))\nname=list(df)\n\n# visualization\ndef plot_with_labels(low_dim_embs, labels, filename = './tsne.png'):\n    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n    plt.figure(figsize= (10, 18))\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(label, xy = (x, y), textcoords = 'offset points', ha = 'right', va = 'bottom')\n    plt.savefig(filename) \n\n# t-sne dimensionality reduction (10 to 2)\ntsne = TSNE(perplexity = 30, n_components = 2, init = 'pca', n_iter = 5000)\nplot_only = 300\nlow_dim_embs = tsne.fit_transform(df.iloc[:plot_only][name[1:]])\nlabels = [l[i] for i in range(plot_only)]\n# plot\nplot_with_labels(low_dim_embs, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Feature engineering2: orginial,cumulative, time series, combination features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file path\n\npath = '../input/user-package-information-of-mobile-operators'\nw2v_path = './w2v'\ntrain = pd.read_csv(path + '/train_2.csv')\ntest = pd.read_csv(path + '/test_2.csv')\ntrain_first = pd.read_csv(path + '/train_all.csv')\n\ntrain['data_type'] = 0\ntest['data_type'] = 0\ntrain_first['data_type'] = 1\n\n# data concat (3 tables above)\ndata = pd.concat([train, test, train_first], ignore_index=True).fillna(0)\n\n# set label: current_service\ndata['label'] = data.current_service.astype(int)\ndata = data.replace('\\\\N', 999)\n\n# transform gender type to int\ndata['gender'] = data.gender.astype(int)\n\n# orginial category features\norigin_cate_feature = ['service_type', 'complaint_level', 'contract_type', 'gender', 'is_mix_service',\n                       'is_promise_low_consume',\n                       'many_over_bill', 'net_service']\n\n# orginial number features\norigin_num_feature = ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee',\n                      'age', 'contract_time',\n                      'former_complaint_fee', 'former_complaint_num',\n                      'last_month_traffic', 'local_caller_time', 'local_trafffic_month', 'month_traffic',\n                      'online_time', 'pay_num', 'pay_times', 'service1_caller_time', 'service2_caller_time']\n\n# number features to float\nfor i in origin_num_feature:\n    data[i] = data[i].astype(float)\n\n# import saved word2vec features\nw2v_features = []\nfor col in ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']:\n    df = pd.read_csv(w2v_path + '/' + col + '_w2v.csv')\n    df = df.drop_duplicates([col])\n    fs = list(df)\n    fs.remove(col)\n    w2v_features += fs\n    data = pd.merge(data, df, on=col, how='left')\ncount_feature_list = []\n\n# cumulative features\n# function for cumulative features, 2 parameters required: orginial dataframe and features(list) to be counted\ndef feature_count(data, features=[]):\n    if len(set(features)) != len(features):\n        print('equal feature !!!!')\n        return data\n    new_feature = 'count'\n    for i in features:\n        new_feature += '_' + i.replace('add_', '')\n    try:\n        del data[new_feature]\n    except:\n        pass\n    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n\n    data = data.merge(temp, 'left', on=features)\n    count_feature_list.append(new_feature)\n#     print('temp: ',temp)\n#     print('data: ',data)\n    return data\n\n# call function for counting features （groupby one feature）\ndata = feature_count(data, ['1_total_fee'])\ndata = feature_count(data, ['2_total_fee'])\ndata = feature_count(data, ['3_total_fee'])\ndata = feature_count(data, ['4_total_fee'])\ndata = feature_count(data, ['former_complaint_fee'])\ndata = feature_count(data, ['pay_num'])\ndata = feature_count(data, ['contract_time'])\ndata = feature_count(data, ['last_month_traffic'])\ndata = feature_count(data, ['online_time'])\n\n# function for counting features（groupby two features）\nfor i in ['service_type', 'contract_type']:\n    data = feature_count(data, [i, '1_total_fee'])\n    data = feature_count(data, [i, '2_total_fee'])\n    data = feature_count(data, [i, '3_total_fee'])\n    data = feature_count(data, [i, '4_total_fee'])\n    data = feature_count(data, [i, 'former_complaint_fee'])\n    data = feature_count(data, [i, 'pay_num'])\n    data = feature_count(data, [i, 'contract_time'])\n    data = feature_count(data, [i, 'last_month_traffic'])\n    data = feature_count(data, [i, 'online_time'])\n\n# Time series features（reflect trend）\ndiff_feature_list = ['diff_total_fee_1', 'diff_total_fee_2', 'diff_total_fee_3', 'last_month_traffic_rest',\n                     'rest_traffic_ratio',\n                     'total_fee_mean', 'total_fee_max', 'total_fee_min', 'total_caller_time', 'service2_caller_ratio',\n                     'local_caller_ratio',\n                     'total_month_traffic', 'month_traffic_ratio', 'last_month_traffic_ratio', 'pay_num_1_total_fee',\n                     '1_total_fee_call_fee', '1_total_fee_call2_fee', '1_total_fee_trfc_fee']\n# Time series features: difference\ndata['diff_total_fee_1'] = data['1_total_fee'] - data['2_total_fee']\ndata['diff_total_fee_2'] = data['2_total_fee'] - data['3_total_fee']\ndata['diff_total_fee_3'] = data['3_total_fee'] - data['4_total_fee']\ndata['pay_num_1_total_fee'] = data['pay_num'] - data['1_total_fee']\ndata['last_month_traffic_rest'] = data['month_traffic'] - data['last_month_traffic']\ndata['last_month_traffic_rest'][data['last_month_traffic_rest'] < 0] = 0\n\n# Time series features: ratio\ndata['rest_traffic_ratio'] = (data['last_month_traffic_rest'] * 15 / 1024) / data['1_total_fee']\n\n# Time series features: mean,max,min\ntotal_fee = []\nfor i in range(1, 5):\n    total_fee.append(str(i) + '_total_fee')\ndata['total_fee_mean'] = data[total_fee].mean(1)\ndata['total_fee_max'] = data[total_fee].max(1)\ndata['total_fee_min'] = data[total_fee].min(1)\n\n# Combination features: others\ndata['total_caller_time'] = data['service2_caller_time'] + data['service1_caller_time']\ndata['service2_caller_ratio'] = data['service2_caller_time'] / data['total_caller_time']\ndata['local_caller_ratio'] = data['local_caller_time'] / data['total_caller_time']\n\ndata['total_month_traffic'] = data['local_trafffic_month'] + data['month_traffic']\ndata['month_traffic_ratio'] = data['month_traffic'] / data['total_month_traffic']\ndata['last_month_traffic_ratio'] = data['last_month_traffic'] / data['total_month_traffic']\n\ndata['1_total_fee_call_fee'] = data['1_total_fee'] - data['service1_caller_time'] * 0.15\ndata['1_total_fee_call2_fee'] = data['1_total_fee'] - data['service2_caller_time'] * 0.15\ndata['1_total_fee_trfc_fee'] = data['1_total_fee'] - (data['month_traffic'] - 2 * data['last_month_traffic']) * 0.3\n\ndata.loc[data.service_type == 1, '1_total_fee_trfc_fee'] = None\n\n# merge features\n# category features\ncate_feature = origin_cate_feature\n# number features\nnum_feature = origin_num_feature + count_feature_list + diff_feature_list + w2v_features\n# category feature astype category（For LightGBM）\nfor i in cate_feature:\n    data[i] = data[i].astype('category')\n# number features astype float\nfor i in num_feature:\n    data[i] = data[i].astype(float)\n    \n# final features (name list)\nfeature = cate_feature + num_feature\n\n# features numbers and name\nprint(\"##final features length##: \"+str(len(feature)), '\\n','##final features names##: ',feature)\n\ndata = data[data.label != 999999]\n\n# features(X) and label(y) for training and testing\nX = data[(data.label != 0) & (data.label != 999999)][feature].reset_index(drop=True)\ny = data[(data.label != 0) & (data.label != 999999)].label.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label2current_service = dict(\n    zip(range(0, len(set(y))), sorted(list(set(y)))))\ncurrent_service2label = dict(\n    zip(sorted(list(set(y))), range(0, len(set(y)))))\ncurrent_service2label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and testing with Lightgbm (StratifiedKFold)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# StratifiedKFold + CrossValidation\nfrom sklearn.model_selection import StratifiedKFold\ncv_pred = []\nskf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\nfor index, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print(index)\n    lgb_model = lgb.LGBMClassifier(\n        boosting_type=\"gbdt\", num_leaves=10,\n        max_depth=8, n_estimators=2, objective='multiclass', class_weight='balanced',\n        subsample=0.65, colsample_bytree=0.65, subsample_freq=1,\n        learning_rate=0.05, random_state=2020 + index, n_jobs=-1, metric=\"None\", importance_type='gain'\n    )\n    train_x, test_x, train_y, test_y = X.loc[train_index], X.loc[test_index], y.loc[train_index], y.loc[test_index]\n    \n    # only train by those service_type == 4 this time\n    train_x = train_x[train_x.service_type == 4]\n    train_y = train_y[(train_x.service_type == 4).index]\n    test_x = test_x[test_x.service_type == 4]\n    test_y = test_y[(test_x.service_type == 4).index]\n    print(test_y.unique())\n\n    #eval_set = [(test_x, test_y)]\n    lgb_model.fit(train_x, train_y, categorical_feature=cate_feature)\n    \n    # predict on the same dataset:y_test\n    y_test = lgb_model.predict(data[(data.label == 0) & (data.service_type != 1)][feature])\n    y_test = pd.Series(y_test).map(current_service2label)\n\n    \n    #save prediction to cv_pred (5 times 5 cols)\n    if index == 0:\n        cv_pred = np.array(y_test).reshape(-1, 1)\n    else:\n        cv_pred = np.hstack((cv_pred, np.array(y_test).reshape(-1, 1)))\n    #print(cv_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing \nf1_score(y_true=test_y, y_pred=lgb_model.predict(test_x), average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forecast\n\nsubmit = []\nfor line in cv_pred:\n#     print(line)\n#     print(np.bincount(line))\n#     print(np.argmax(np.bincount(line)))\n    submit.append(np.argmax(np.bincount(line))) #select highest frequency prediction from 5 \n#print(len(submit))    \nresult = pd.DataFrame()\nresult['user_id'] = data[(data.label == 0) & (data.service_type != 1)]['user_id']\nresult['predict'] = submit\nresult['predict'] = result['predict'].map(label2current_service)\nresult.loc[result['user_id'] == '4VNcD6kE0sjnAvFX', 'predict'] = 999999\n\n#print(len(result), result.predict.value_counts())\nprint(result.sort_values('user_id'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}