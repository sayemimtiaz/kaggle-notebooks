{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import required packages\n\nThis session is to import basic packages required for upcoming data manipulation, visualization and analysis.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # for pattern matching, like grep in r\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To read data\n\n## Data_Entry_2017.csv","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_entry = pd.read_csv('../input/nihdata/Data_Entry_2017.csv')\ndata_entry.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** We first look at the first five rows of the *Data_Entry_2017.csv* file and there is 12 columns consisting patient IDs, findings on disease condition and other information. For the upcoming classification, we only need the patient IDs and corresponding findings. Hence, a subset with interested columns is created.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_entry_subset = data_entry.loc[:, 'Image Index':'Finding Labels']\ndata_entry_subset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** From `Finding Labels`, we can tell this is a multi-labelled classification instead of binary (Yes/No). Spliting the column and getting the unique values are required.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"split = pd.DataFrame(data_entry_subset['Finding Labels'].str.split('|').tolist())\n\ntemp = []\nfor i in split:\n    temp.append(split[i].unique())\n\nflatten = pd.DataFrame(temp).values.flatten()\n\nunique = []\nfor x in flatten:\n    if x not in unique:\n        unique.append(x)\n\nlabels = list(filter(None, unique))\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** There are 15 disease condition including *No Finding*. Note that *Cardiomegaly* etc are not our sole interested point of prediction. Hence, it's necessasry to manipulate `Finding Labels` to split all the different disease tags.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_entry_subset[\"Finding Labels\"] = data_entry_subset[\"Finding Labels\"].apply(lambda x:x.split(\"|\"))\ndata_entry_subset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** To understand the data structure, lets take *Image Index* 00000001_001.png as example, that particular patient has two disease conditions that are Cardiomegaly and Emphysema. The classification later on will need to be managed to predict these two conditions for that patient instead of one condition (either Cardiomegaly or Emphysema) only.\n\nNow, we look at the count distribution of each disease.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nlabels_count = Counter(label for lbs in data_entry_subset[\"Finding Labels\"] for label in lbs)\n\nlabels_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:**\n\nFrom the frequency table, we found out that there are two types of imbalances:\n1. Imbalance across different classes, i.e. some disease like *Effusion* and *Infiltration* have high frequencies wheares others have extreme low frquencies.\n2. Imbalance between positive and negative in some classes, i.e. if we look at each disease one by one, more than 90% of patients don't have that condition and there will a risk of giving us a model with high false negative.\n\nWe will have to derive a `class_weights` to adjust the imbalance distribution later which will be used to fit into the `fit` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_count = sum(labels_count.values())\nclass_weights = {cls: total_count / count for cls, count in labels_count.items()}\n\nclass_weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Images\n\n**Programing Note: ** `f, l` are dummy variables representing two columns of *data_entry_subset*.\n\nThe following section will show the images of first 9 patients with their labelled conditions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\nnew_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 20))\ni = 0\nfor f, l in data_entry_subset[:9].values:\n    img = cv2.imread('../input/nihdata/images_001/images/{}'.format(f))\n    ax[i // 3, i % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i // 3, i % 3].set_title('{} - {}'.format(f, l))\n    i += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data frame pre-processing\n\nThis session is to process the images (unstructed data) to machine learnable format (to let the computer understands the images in its way).\n\nWe will also split the entire dataset into training set and validation set for model developement and model validation respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,  \n    zoom_range=0.2,        \n    horizontal_flip=True,\n    validation_split=0.2) \n\ndef get_flow_from_dataframe(generator, \n                            dataframe, \n                            subset,\n                            image_shape=(150, 150),\n                            batch_size=32):\n    \n    train_generator_1 = generator.flow_from_dataframe(dataframe, target_size=image_shape,\n                                                      x_col='Image Index',\n                                                      y_col='Finding Labels',\n                                                      class_mode='categorical',\n                                                      directory = '../input/nihdata/images_001/images',\n                                                      batch_size=batch_size,\n                                                      classes = labels,\n                                                      subset=subset)\n\n    train_generator_2 = generator.flow_from_dataframe(dataframe, target_size=image_shape,\n                                                      x_col='Image Index',\n                                                      y_col='Finding Labels',\n                                                      class_mode='categorical',\n                                                      directory = '../input/nihdata/images_002/images',\n                                                      batch_size=batch_size,\n                                                      classes = labels,\n                                                      subset=subset)\n    \n    train_generator_3 = generator.flow_from_dataframe(dataframe, target_size=image_shape,\n                                                      x_col='Image Index',\n                                                      y_col='Finding Labels',\n                                                      class_mode='categorical',\n                                                      directory = '../input/nihdata/images_003/images',\n                                                      batch_size=batch_size,\n                                                      classes = labels,\n                                                      subset=subset)\n    \n    train_generator_4 = generator.flow_from_dataframe(dataframe, target_size=image_shape,\n                                                      x_col='Image Index',\n                                                      y_col='Finding Labels',\n                                                      class_mode='categorical',\n                                                      directory = '../input/nihdata/images_004/images',\n                                                      batch_size=batch_size,\n                                                      classes = labels,\n                                                      subset=subset)\n    \n    while True:\n        x_1 = train_generator_1.next()\n        x_2 = train_generator_2.next()\n        x_3 = train_generator_3.next()\n        x_4 = train_generator_4.next()\n\n        yield np.concatenate((x_1[0], x_2[0], x_3[0], x_4[0]), axis = 0), np.concatenate((x_1[1], x_2[1], x_3[1], x_4[1]), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = get_flow_from_dataframe(generator=datagen, \n                                    dataframe=data_entry_subset, \n                                    subset = 'training',\n                                    image_shape=(150, 150),\n                                    batch_size=32)\n\nval_gen = get_flow_from_dataframe(generator=datagen, \n                                    dataframe=data_entry_subset, \n                                    subset = 'validation',\n                                    image_shape=(150, 150),\n                                    batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Derive class weights\nAs per the previous remarks, due to the imbalance class, we will define `class_weights_index` based on `class_weights` derived. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = datagen.flow_from_dataframe(data_entry_subset, target_size=(150,150),\n                                                      x_col='Image Index',\n                                                      y_col='Finding Labels',\n                                                      class_mode='categorical',\n                                                      directory = '../input/nihdata/images_001/images',\n                                                      batch_size=32,\n                                                      classes = labels)\n\ngenerator.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights_index = {\n 1: 50.985951008645536,\n 5: 56.25476947535771,\n 4: 10.628294660959675,\n 10: 2.3448418680936367,\n 7: 623.5110132158591,\n 8: 7.114557152910425,\n 9: 24.478900034590108,\n 11: 22.356183857210553,\n 0: 12.244744355048015,\n 14: 26.695020746887966,\n 12: 41.81299852289513,\n 13: 98.9077568134172,\n 6: 83.94839857651246,\n 3: 61.45766391663048,\n 2: 30.327190914934647\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Model Development","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Trial #1 : 1 Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers\nfrom keras import models\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.python.keras.regularizers import l2\n\n#Trying just 1 layer\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(64, (4,4), padding = 'same',\n                        activation='relu', input_shape=(150, 150, 3)))\n#model.add(layers.MaxPooling2D((2, 2)))\n\n#model.add(layers.Conv2D(128, (4,4), activation='relu'))##\n\n#model.add(layers.Flatten()) \n#model.add(layers.Dense(1024, activation='relu')) ##\nmodel.add(layers.Dense(15, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_per_train = 28000//32\nstep_per_val = 6999//32\n\nhistory = model.fit_generator(\n    train_gen, \n    steps_per_epoch  = step_per_train, \n    validation_data  = val_gen,\n    validation_steps = step_per_val,\n    class_weight = class_weights_index,\n    use_multiprocessing = True,\n    epochs = 2)#was 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trial #2 : 2 Layers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Trying 2 layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(64, (4,4), padding = 'same',\n                        activation='relu', input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (4,4), activation='relu'))##\n\nmodel.add(layers.Flatten()) \n#model.add(layers.Dense(1024, activation='relu')) ##\nmodel.add(layers.Dense(15, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_per_train = 28000//32\nstep_per_val = 6999//32\n\nhistory = model.fit_generator(\n    train_gen, \n    steps_per_epoch  = step_per_train, \n    validation_data  = val_gen,\n    validation_steps = step_per_val,\n    class_weight = class_weights_index,\n    use_multiprocessing = True,\n    epochs = 2)#was 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"875/875 [==============================] - 3518s 4s/step - loss: 3.7030 - acc: 0.9195 - val_loss: 0.2621 - val_acc: 0.9156\nEpoch 2/2\n875/875 [==============================] - 3475s 4s/step - loss: 3.6339 - acc: 0.9201 - val_loss: 0.2531 - val_acc: 0.9156","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(16,9))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(64, (4,4), padding = 'same',\n                        activation='relu', input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (4,4), activation='relu'))##\nmodel.add(layers.MaxPooling2D((2, 2)))##\n\nmodel.add(layers.Conv2D(128, (4, 4), activation='relu',kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1)))##\ntf.keras.layers.Dropout(.5, input_shape=(150, 150, 3))##\n\nmodel.add(layers.Conv2D(128, (4,4), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (4,4), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (4,4), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten()) \nmodel.add(layers.Dense(1024, activation='relu')) ##\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(15, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from keras import layers\nfrom keras import models\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.python.keras.regularizers import l2\n\nfrom keras import backend as K\nK.tensorflow_backend.set_image_dim_ordering('th')\n\n\n\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), padding = 'same',\n                        activation='relu',input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1)))\n#model.add(Conv2D(32, (3,3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\ntf.keras.layers.Dropout(.5, input_shape=(150, 150, 3))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))#added\nmodel.add(layers.MaxPooling2D((2, 2)))#added\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(15, activation='sigmoid'))''';","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Optimization\nThis is to instruct the model to improve its performance by learning from its own mistake.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Training\n#This session is to fit the data into the model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add more dropouts\n#improving weight decay\n#adding more neurons?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_per_train = 28000//32\nstep_per_val = 6999//32\n\nhistory = model.fit_generator(\n    train_gen, \n    steps_per_epoch  = step_per_train, \n    validation_data  = val_gen,\n    validation_steps = step_per_val,\n    class_weight = class_weights_index,\n    use_multiprocessing = True,\n    epochs = 2)#was 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(16,9))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}