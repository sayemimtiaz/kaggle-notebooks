{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <h1><center> Customer Churn Prediction:  </center></h1>\n# <h2><center> Leveraging XGBoost and SMOTE to Inform Data-driven Customer Retention Programs </center></h2>\n"},{"metadata":{},"cell_type":"markdown","source":"# 1.0) Introduction:\n\n\n## 1.1) Importance:\n\nIt is often recognized in business that acquiring new customers is significantly more expensive than retaining existing ones. \n\nIn this project, we propose an **XGBoost model** to classify whether a customer will churn (class=1) or not churn (class=0).  By identifying those customers likely to leave, the company would be well-placed to offer individuals incentives to stay, leading to improved retention and higher returns for every dollar spent on customer acquisition.\n\n\n## 1.2) Methods:\n\nTo accomplish this goal, we will:\n1. Perform **exploratory data analysis** (**EDA**) to derive insights regarding our data's features.\n2. Perform **data cleaning**\n3. Perform **feature engineering**\n4. Train **Logistic Regression** and **XGBoost** models to predict customer churn\n5. Perform hyperparameter tuning using the novel **Hyperband** algorithm\n6. Interpret our results using **SHAP** values\n\n\n## 1.3) Findings:\n\n1. Customers who have been with the company longer are less likely to churn (and vice-versa)\n    - 1.5 years with the company marks the \"turning point\" at which a customer is more likely to stay as opposed to churn.\n    - most customers on 2-year contracts have been with the company for at least 3 years, and these customers are more likely to stay.\n2. Customers who experience higher monthly charges are more likely to churn (and vice-versa)\n    - customers with monthly charges below \\\\$70 are less likely to churn, but once a customer's monthly charges go above \\\\$80, then they become more likely to churn.\n    - 2-year contracts help mitigate the risk of a customer churning even if their monthly charges are past the threshold of \\\\$80 and are in the range of \\\\$80-\\\\$100, but once a customer's monthly charges run past \\\\$100, then contract length no longer mitigates the greater risk of them churning.\n3. Customers with poor internet quality, no phone service, no online security, no tech support, or no online backup were those who were more likely to churn (and vice versa)\n    - Likely a proxy for socio-economic status where customers with lower incomes are more likely to churn.\n4. Customers who pay via electronic check were more likely to churn, and those that opted for paper billing were less likely to churn"},{"metadata":{},"cell_type":"markdown","source":"Anyway, to further understand how I arrived at the aformentioned conclusions, let me show you my methodology.  \n\nLet's get started!"},{"metadata":{},"cell_type":"markdown","source":"# 2.0)  Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Install libraries\nimport pandas as pd\nimport numpy as np\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom joblib import dump, load\n\n# Load the data\nX_full = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv', index_col='customerID')\n\nprint('The table has ', X_full.shape[0], ' unique customers')\nprint('The table has ', X_full.shape[1], ' features')\n\n# Allow us to see all columns of our dataframe\npd.set_option('max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a sense of our data\nX_full.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1) Summarize our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize our data\nX_full.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. Most of our data is categorical\n    - A good opportunity try our hand at encoding\n2. Our data contains no null values\n    - This simplifies our task, as there will be no need to impute missing values\n3. Our numeric data is stored as either int64 or float64\n    - Depending on the magnitude of the stored values, this could represent a good opportunity to optimize memory and system storage.\n\n\nConfusingly, `TotalCharges` seems to be an \"oject\" datatype even though `MonthlyCharges` is represented as a \"float\"\nThis likely means that although values in this column seem to be integers, they are actually represented as strings (or a mixture of different datatypes).  We should investigate this before visualizing our numeric data.  But more on that later.\n\n\nFor now, let's take a look at the distribution of our target variable:  `Churn`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at how many customers churned and how many stayed\ntarget_dist = X_full['Churn'].value_counts()\nprint('Customers who stayed:  ', target_dist[0])\nprint('Customers who churned:  ', target_dist[1])\n\n# Visualize the customer churn distribution\nsns.set_style('whitegrid')\nsns.barplot(x=target_dist.index, y=target_dist.values)\nplt.title('Customer churned?', pad=20, fontsize=15, fontweight='bold')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that there exists class imbalance in our dataset because 26.5% of customers churned while 73.5% of customers stayed.\n\nSince decision-tree based models--such as **XGBoost**--are susceptible to frequency bias, we must address the class imbalance in one of two ways in order to make the distribution more equal:\n1. Undersample the majority class (customer stayed)\n2. Oversample the minority class (customer churned)\n\nSince our dataset is relatively small at 7,043 rows, the better choice would be to pick option #2 and oversample the minority class.\nThis can be accomplished by the famous and very effective <u>S</u>ynthetic <u>M</u>inority-<u>O</u>versampling <u>T</u>echnique (**SMOTE** ). \n\nLet's continue our exploration, this time focusing in on our numerical data."},{"metadata":{},"cell_type":"markdown","source":"## 2.2) Explore our numerical data"},{"metadata":{},"cell_type":"markdown","source":"In the previous section, we saw that `TotalCharges` is represented as an 'object' datatype even though it seems to represent numerical data. We investigate as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check which datatypes are contained within 'TotalCharges'\nX_full['TotalCharges'].apply(type).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it seems that `TotalCharges` consists only of strings.  Let's investigate to see if we'll be able to convert it to a float."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to check if a string can be converted to a float\ndef is_convertible(value):\n    \"\"\"\n    Checks to see whether a string can be converted to a float.\n    Input:\n        - A string\n    Output:\n        - A boolean indicating whether or not the string can be converted to a float\n    \"\"\"\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n    \n    \n# Initialize list of strings that can't be converted to a float\nunconvertibles = []\n\n# Iterate through an array of strings, and append unconvertible strings to a list\nstr_array = X_full['TotalCharges'].to_numpy()\nfor element in str_array:\n    if is_convertible(element) == False:\n        unconvertibles.append(element)\n\n# See which strings (if any) are unconvertible\nprint(\"Unconvertibles:  \", unconvertibles)\nprint(\"Count of unconvertibles:  \", len(unconvertibles))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently there are a eleven empty strings which can't be converted to a float.  Though we would normally handle this under the \"data cleaning\" stage, let's handle these values now so we can convert our data to floats in order to better visualize it later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert from 'object' to 'float' while replacing strings containing whitespace with NaN\nX_full['TotalCharges'] = pd.to_numeric(X_full['TotalCharges'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've converted `TotalCharges` to the float datatype, we can analyze its characteristics along with the rest of our numerical data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Get a sense of numerical data\nX_full.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. `SeniorCitizen` seems to be a categorical representation of whether or not a customer is a senior citizen (class=1) or not (class=0)\n    - This column has already been numerically encoded, so there is nothing for us to do here when it comes time to feature engineering.\n2. `tenure` is the number of months a customer has stayed at the company.  It looks like the company has some loyal customers!\n    - Our intuition tells us that this could be a useful predictor of customer churn.  Loyal customers might be unlikely to churn. \n    - There don't seem to be any outliers here, so this isn't something we have to worry aobut.\n3. `MonthlyCharges` is the total amount a customer pays per month.\n    - There don't seem to be any outliers here, so this isn't something we have to worry aobut.\n4. `TotalCharges` contains 11 NaN values.\n    - We will have to handle this later on using imputation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set style\nsns.set_style('whitegrid')\n\n# Set up for graphs to be side-by-side\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,5), sharex=False, sharey=False)\nplt.suptitle('Probability Density Functions of Our Numerical Features', \n             fontsize=30, fontweight='bold', y=1.155)\n\n# Create graph showing distribution of 'tenure'\ntenure_distPlot = sns.kdeplot(data=X_full['tenure'], shade=True, ax=axes[0])\naxes[0].set_title(\"'tenure'\", fontsize=25, pad=15)\naxes[0].set_xlabel(\"Number of months as a customer\", fontsize=20, labelpad=15)\n\n# Create graph showing distribution of 'MonthlyCharges'\nmonthlyCharge_distPlot = sns.kdeplot(data=X_full['MonthlyCharges'], shade=True, ax=axes[1])\naxes[1].set_title(\"'MonthlyCharges'\", fontsize=25, pad=15)\naxes[1].set_xlabel(\"Monthly payment amount\", fontsize=20, labelpad=15)\n\n# Create graph showing distribution of 'TotalCharges'\ntotalCharge_distPlot = sns.kdeplot(data=X_full['TotalCharges'], shade=True, ax=axes[2])\naxes[2].set_title(\"'TotalCharges'\", fontsize=25, pad=15)\naxes[2].set_xlabel(\"Total payment amount\", fontsize=20, labelpad=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. The distribution for `tenure` seems to be bimodal, that is, customers either tend to be relatively new (0-9 months) or tend to have been with the company for an extended period time (5+ years)\n    - The fact that customers are heavily weighted towards being relatively new indicates that there is a legitimate need to improve customer retention rates.  But that's why we're here!\n    - If we can successfully predict which customers are in danger of churning and provide them incentives to stay, these \"flight-risk\" customers could actually end up staying with us for many years to come!\n2. `MonthlyCharges` also seems to be bimodal, so customers either spend on the lower end of the spectrum (18-35 dollars) or--even more likely--on the higher end of the spectrum (70+ dollars).\n3. Looking at`TotalCharges`, we can see that it is more likely for a customer to have spent a relatively small total amount (0-1,000 dollars) than it is to have spent a relatively high amount (4,000-5,000 dollars, for instance)\n\n\nNow that we have some sense of how our numerical features are distributed overall, let's break things down a bit more and see how they're distributed when grouped by our target variable: `Churn`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up for graphs to be side-by-side\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,5), sharex=False, sharey=False)\nplt.suptitle('Distribution of Our Numerical Features by Churn Class', \n             fontsize=30, fontweight='bold', y=1.155)\n\n# Create graph showing distribution of 'tenure'\ntenure_boxPlot = sns.boxplot(x=X_full['Churn'], y=X_full['tenure'], \n                             ax=axes[0], palette='Blues_r')\naxes[0].set_title(\"'tenure'\", fontsize=25, pad=15)\naxes[0].set_xlabel(\"Churn?\", fontsize=20)\naxes[0].set_ylabel(\"Months as a customer\", fontsize=15)\n\n\n# Create graph showing distribution of 'MonthlyCharges'\nmonthlyCharges_boxPlot = sns.boxplot(x=X_full['Churn'], y=X_full['MonthlyCharges'], \n                                     ax=axes[1], palette='Blues_r')\naxes[1].set_title(\"'MonthlyCharges'\", fontsize=25, pad=15)\naxes[1].set_xlabel(\"Churn?\", fontsize=20)\naxes[1].set_ylabel(\"Monthly payment amount\", fontsize=15)\n\n# Create graph showing distribution of 'TotalCharges'\ntotalCharges_boxPlot = sns.boxplot(x=X_full['Churn'], y=X_full['TotalCharges'], \n                                   ax=axes[2], palette='Blues_r')\naxes[2].set_title(\"'TotalCharges'\", fontsize=25, pad=15)\naxes[2].set_xlabel(\"Churn?\", fontsize=20)\naxes[2].set_ylabel(\"Total payment amount\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. `Tenure`\n    - Unsurprisingly, customers who churn are usually the ones who have spent less time with the company\n2. `MonthlyCharges`\n    - As suspected, customers who churn tend to have higher monthly charges than customers who don't churn\n        - Whether or not a customer can still afford to pay is likely a factor affecting churn rate\n3. `TotalCharges`\n    - Customers who churn tend to have lower total charges than customers who don't churn\n        - Likely reflecting the fact that customers who churn are usually not with the company for very long\n    \nInterestingly, by looking at the `TotalCharges` column, there are a decent number of 'outlierly' customers who have spent a great deal of money over the years and still churn.  \n\nLet's investigate these customers further to determine whether the outliers occur due to reporting error or whether their values are within the realm of possibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the 25th and 75th percentile values of 'TotalCharge' for customers who churned\nlower_percentile = X_full[X_full['Churn']=='Yes'].TotalCharges.quantile(.25)\nupper_percentile = X_full[X_full['Churn']=='Yes'].TotalCharges.quantile(.75)\n\n# Calculate the inter-quartile range\niqr = upper_percentile - lower_percentile\n\n# Calculate the cut-off point after which a datapoint becomes an outlier\noutlier_limit = upper_percentile + (1.5 * iqr)\n\n# Dataframe consisting only rows of outlierly, abnormally high 'TotalCharge' customers who churned\noutlier_tot_charge_churn = X_full[ (X_full['Churn']=='Yes') & (X_full['TotalCharges'] > outlier_limit) ]\n\n# Interesting statistics of abnormally high 'TotalCharge' customers who churned\nnum_totCharge_outliers = len(outlier_tot_charge_churn.index)\navg_spent_outliers = outlier_tot_charge_churn.TotalCharges.mean()\ntot_spent_outliers = num_totCharge_outliers * avg_spent_outliers\n\nprint('Number of outlierly customers:  ', num_totCharge_outliers)\nprint(\"Average total amount spent per outlierly customer:  \", avg_spent_outliers)\nprint('Total amount spent by outlierly customers:  ', tot_spent_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 109 such best-paying customers who ended up churning.  On average, each person spent \\\\$6,670 over their entire customer lifetime, and as a whole, the entire group of outliers ended up contributing approximately \\\\$727,060 to the company's bottom line. \n\nThese individuals are our most valuable customers, and we would like to retain them as much as possible.  By being able to predict whether or not such customers are at risk of leaving, we will be better placed to offer these inviduals incentives to stay."},{"metadata":{},"cell_type":"markdown","source":"As our EDA is wrapping up, let's do a final check to see the correlations between our numerical features.  \n\nIf any two features are overly correlated, it would be valuable to remove one of them to solve the problem of multicollinearity."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate the correlation matrix of our numerical features\ncorrMatrix = X_full[['tenure', 'MonthlyCharges', 'TotalCharges']].corr()\n\n# Visualize the correlation matrix\nplt.title('Correlation Matrix of Our Numerical Features', pad=20, fontweight='bold')\nsns.heatmap(corrMatrix, cmap='Blues', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. There's a high correlation between `tenure` and `TotalCharges`\n    - This fits our intuition that the longer a customer has been with the company, the more they will have paid.\n    - Despite the high correlation between the two features, it is not enough to justify removing one of them because they are not excessively correlated.\n2. There's a moderate correlation between `MonthlyCharges` and `TotalCharges`\n    - It makes sense that the more you pay on a month-to-month basis, the more you will have spent overall.\n    - The reason why `MonthlyCharges` is not as correlated to `TotalCharges` as `tenure` is likely due to the company being subscription based\n3. There's a low correlation between `tenure` and `MonthlyCharges`\n\nGiven the fact that `TotalCharges` is more correlated to `tenure` than it is to `MonthlyCharges`, then this could inform the company business model.  In other words, it seems that the trope \"slow and steady wins the race\" holds true, as customer value is more determined by time spent with the company than by their monthly payments.  If this is indeed the case, then it would be interesting to see if high monthly payments lead to increased churn rates among customers.\n\nSince we've completed our EDA of the numerical columns, let's turn our attention to our categorical data."},{"metadata":{},"cell_type":"markdown","source":"## 2.3) Explore our categorical data"},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Visualizing Target Distribution by Gender </center></h3>\n\nIt would be interesting to see whether gender plays a role in predicting customer churn.  We visualize the churn data as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the style of our visualizations\nsns.set_style('whitegrid')\nsns.set(font_scale = 1.2)\nplt.figure(figsize=(7,4))\n\n# Create the barplot (in blue) depicting customer count--by gender\ngender_dist = X_full['gender'].value_counts()\nsns.barplot(x=gender_dist.index, y=gender_dist.values, color='lightskyblue')\n\n# Create the barplot (in orange) depicting customer churn count--by gender\ngender_churn_dist = X_full[X_full.Churn=='Yes']['gender'].value_counts()\nsns.barplot(x=gender_churn_dist.index, y=gender_churn_dist.values, color='khaki')\n\n# Label the graph\nplt.title('Total customers (blue) and churn count (orange) by gender', \n          pad=20, fontsize=14, fontweight='bold')\nplt.ylabel('Count', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen by the equal distribution, gender does not seem to play a role in determining whether a customer will churn or not.  \nNow that we know this, `gender` would be a good column to drop in order to simplify our model and reduce the *curse of dimensionality*\n\nLet's visualize the rest of our attributes to see which columns are droppable."},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Visualizing Target Distribution of Our Categorical Variables </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find and replace categorical values to make visualization more readable\nX_full['SeniorCitizen'].astype('object')\nX_full.replace(to_replace={'SeniorCitizen': {0:'No', 1:'Yes'}}, inplace=True)\nX_full.replace(to_replace={'MultipleLines': {'No phone service':'N/A'}}, inplace=True)\n\n# Get a list of attributes to visualize\ncat_cols_viz = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', \n                'PaperlessBilling', 'PhoneService', 'MultipleLines', 'InternetService',\n                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n                'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n\n\n# Set style\nsns.set_style('whitegrid')\n\n# Set up subplot to display graphs\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20,20), sharex=False, sharey=False)\nplt.suptitle('Distribution of Our Categorical Features by Churn Class', \n             fontsize=30, fontweight='bold', y=1.032)\n\n# Initialize row and column index iterators in preparation for filling in the subplot\nrow_iterator = 0\ncol_iterator = 0\n\n# Fill in the subplot\nfor col in cat_cols_viz:\n    # Adjust indices once we reach the end of a row (moving from left to right)\n    if col_iterator == 4:\n        col_iterator = 0\n        row_iterator = row_iterator + 1\n    \n    \n    # Initialize value count series\n    valCount_series = X_full[col].value_counts()\n    churn_valCount_series = X_full[X_full.Churn=='Yes'][col].value_counts()\n    \n    \n    # Create the barplot (in blue) depicting customer count--by column\n    sns.barplot(x=valCount_series.index, y=valCount_series.values, color='lightskyblue', ax=axes[row_iterator][col_iterator])\n    \n    # Create the barplot (in orange) depicting customer churn count--by column\n    sns.barplot(x=churn_valCount_series.index, y=churn_valCount_series.values, color='khaki', ax=axes[row_iterator][col_iterator])\n\n    # Label the graph\n    axes[row_iterator][col_iterator].set_title('%s' % col, fontsize=20)\n        \n    # Rotate xlabels\n    plt.sca(axes[row_iterator, col_iterator])\n    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')    \n    \n    # Increment row and column indices\n    col_iterator = col_iterator + 1\n\n    \n# Adjust spacing of subplot\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. Seemingly important features:\n    - `Contract`: the longer the contract, the less likely a customer is to churn.\n    - `PaymentMethod`: customers paying via electronic check are more likely to churn. \n    - `OnlineSecurity`: customers with no online security are more likely to churn.\n    - `TechSupport`: customers receiving no tech support are more likely to churn.\n2. Seemingly unimportant features:\n    - `PhoneService` and `MultipleLines`: both have similar distributions across their different categories.  \n        - The distributions aren't as balanced as the one we see with `gender`, so while the two features may not contribute much to the model, it might be worth keeping them in the end. "},{"metadata":{},"cell_type":"markdown","source":"Now that we've explored our data, it's time to perform data cleaning in order to prepare our data for modeling."},{"metadata":{},"cell_type":"markdown","source":"# 3.0)  Data Cleaning\n\nIn general, the dataset is very clean because after checking these things, we found that:\n1. Our target column is complete and doesn't contain any NaN\n    - No rows need to be dropped for this reason\n2. Most columns are complete and don't contain any NaN\n    - No columns need to be dropped for this reason\n3. Our features don't leak information from the future (after the customer churned)\n    - No columns need to be dropped for this reason\n4. No features are redundant or excessively correlated\n    - No columns need to be dropped for this reason\n5. Though there are outlierly customers in terms of `TotalCharge`, the payment totals are not outside the realm of possibility\n    - No rows need to be dropped for this reason"},{"metadata":{},"cell_type":"markdown","source":"Our dataset isn't perfect, however, so we had to do the following to clean our data:\n1. Convert elements of `TotalCharges` from 'strings' to 'floats'\n2. Find and replace empty strings in `TotalCharges` with NaN"},{"metadata":{},"cell_type":"markdown","source":"Now what's left to do is:\n1. Drop the `gender` column since it doesn't contribute to our model. \n2. Remove white space from our value classes to make things more uniform.\n3. Make binary value classes more descriptive\n    - This will be imporant when we begin combining features during feature engineering so that we know what represents what.\n4. Remove any duplicate entries in our data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the 'gender' column from our dataset since it doesn't contribute to our model\nX = X_full.drop('gender', axis=1)\n\n# Remove whitespace from value classes and replace with '_'\nwhitespace_cols = cat_cols_viz[7:]\nX[whitespace_cols] = X[whitespace_cols].stack().str.replace(' ', '_').unstack()\n\n# Make value classes more descriptive\nX.replace(to_replace={'SeniorCitizen': {'No':'Not_SenCit', 'Yes':'SeniorCitizen'}}, inplace=True)\nX.replace(to_replace={'Partner': {'No':'No_Partner', 'Yes':'Partner'}}, inplace=True)\nX.replace(to_replace={'Dependents': {'No':'No_Dependents', 'Yes':'Dependents'}}, inplace=True)\nX.replace(to_replace={'PaperlessBilling': {'No':'No_PaperlessBill', 'Yes':'PaperlessBill'}}, inplace=True)\nX.replace(to_replace={'PhoneService': {'No':'No_PhoneService', 'Yes':'PhoneService'}}, inplace=True)\nX.replace(to_replace={'MultipleLines': {'No':'No_MultiLines', 'Yes':'MultiLines', 'N/A': 'No_PhoneService'}}, inplace=True)\nX.replace(to_replace={'InternetService': {'No':'No_internet_service'}}, inplace=True)\nX.replace(to_replace={'OnlineSecurity': {'No':'No_OnlineSecurity', 'Yes':'OnlineSecurity'}}, inplace=True)\nX.replace(to_replace={'OnlineBackup': {'No':'No_OnlineBackup', 'Yes':'OnlineBackup'}}, inplace=True)\nX.replace(to_replace={'DeviceProtection': {'No':'No_DeviceProtection', 'Yes':'DeviceProtection'}}, inplace=True)\nX.replace(to_replace={'TechSupport': {'No':'No_TechSupport', 'Yes':'TechSupport'}}, inplace=True)\nX.replace(to_replace={'StreamingTV': {'No':'No_StreamingTV', 'Yes':'StreamingTV'}}, inplace=True)\nX.replace(to_replace={'StreamingMovies': {'No':'No_StreamingMov', 'Yes':'StreamingMov'}}, inplace=True)\n\n# Using 'customerID', check to see if there are any duplicate entries\nprint('Number of duplicate entries:  ', X.index.duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there aren't any duplicate entries, so our data set looks to be fully cleaned!  Now it's time to do some feature engineering."},{"metadata":{},"cell_type":"markdown","source":"# 4.0)  Feature Engineering\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will:\n1. Investigate and impute missing values found in `TotalCharges`\n2. Generate novel features\n3. Encode our categorical features\n    - Map ordinal values to appropriate integers\n    - One-hot encode nominal values\n\nBut before we do, let's separate the target from our predictors."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate target from predictors\ny = X['Churn']\nX.drop('Churn', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1) Impute missing values"},{"metadata":{},"cell_type":"markdown","source":"Let's first start off by deciding how to impute missing values found in `TotalCharges`.  Though we *could* impute these values using either the mean or median of the training data (actually the median would be preferable since `TotalCharges` has a skewed distribution), we suspect that there might be some hidden meaning behind these NaN's which might be masked if we imputed in this manner.\n\n\nRecall that these missing values were originally strings of empty text, which may suggest that the value for `TotalCharge` should have been 0.  However, if there are any entries where `TotalCharges` is 0, then this theory would likely be incorrect because strings representing zero total charge would then be represented as '0' instead of the empty string.\n\nLet's test this theory out by checking whether there are any entries where `TotalCharges` is 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many 0's are found within 'TotalCharges'\ntotCharges_zeroes = X[X['TotalCharges'] == 0].shape[0]\nprint('There are  ', totCharges_zeroes, \"  representations of '0' found within 'TotalCharges'\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently there are zero representations of '0' found within `TotalCharges`.  Thus, it's beginning to look more and more likely that the empty string originally found within `TotalCharges` is intented to represent zero total charge.\n\nTo solidify our theory, let's look at `tenure` and `MonthlyCharges` to see if it makes sense that the empty string under `TotalCharges` does indeed represent 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame of customers whose 'TotalCharges' was the empty string\ntotCharges_nan = X[X['TotalCharges'].isnull()]\n\n# Find how many customers there are whose 'TotalCharges' was the empty string\nprint(\"There are \", totCharges_nan.shape[0], \" customers whose 'TotalCharges' was the empty string\")\n\n# Get average 'tenure' of customers whose 'TotalCharges' was the empty string\nprint(\"The average TENURE of this subset is:  \", \n      totCharges_nan['tenure'].mean())\n\n# Get average 'MonthlyCharges' for customers whose 'TotalCharges' was the empty string\nprint(\"The average MONTHLY CHARGE of this subset is:  \", \n      totCharges_nan['MonthlyCharges'].mean())\n\n# Check if any customers whose 'TotalCharges' was the empty string had a monthly charge of 0\nprint('Did any such customers have a monthly charge of 0?    ', 0 in totCharges_nan['MonthlyCharges'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <h3><center> Interesting Observations: </center></h3>\n1. Individuals whose `TotalCharges` was the empty string are all brand new customers\n2. All such customers have a non-zero value associated with `MonthlyCharges`\n\n\nFrom everything we have just learned, I hypothesize that customers whose `TotalCharges` was the empty string are brand new customers who signed up for a monthly subscription but haven't yet made a payment.  Thus, the empty string for `TotalCharges` should really represent '0'.\n    \n\nNow that we know how to impute the NaN's in `TotalCharges`, let's find and replace NaN with 0.00."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find and replace NaN with 0 in 'TotalCharges'\nX.fillna({'TotalCharges': 0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the NaN's in `TotalCharges` likely hold predictive power, we *could* add a missing value indicator (as a feature) in order to further highlight these missing values.  \n\nHowever, we decided against doing this for the following reasons:\n1. There are no other 0's in `TotalCharges` besides the ones we generated, so a value of 0 already stands out\n2. Missing value indicator feature would be a sparse series of ~7,000 zeroes and 11 one's.  Overly complicating our model without providing much valuable information is not worth the tradeoff of efficiency and memory space.\n\n\nBut now that we finished imputing all values and we're just starting to think about generating new features, let's begin with some feature engineering!!"},{"metadata":{},"cell_type":"markdown","source":"## 4.2) Generate novel features"},{"metadata":{},"cell_type":"markdown","source":"In this section we will:\n1. Combine some of our categorical features\n2. Perform grouping operations\n3. Encode our categorical features"},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Combining Categorical Features </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate new features by combining existing ones\nX['SenCit_Dependents'] = X['SeniorCitizen'] + '_' + X['Dependents']\nX['Partner_Dependents'] = X['Partner'] + '_' + X['Dependents']\nX['SenCit_Partner'] = X['SeniorCitizen'] + '_' + X['Partner']\nX['SenCit_Contract'] = X['SeniorCitizen'] + '_' + X['Contract']\nX['SenCit_TechSupport'] = X['SeniorCitizen'] + '_' + X['TechSupport']\nX['SenCit_PayMeth'] = X['SeniorCitizen'] + '_' + X['PaymentMethod']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Aggregations / Group Statistics </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create column giving the average of 'TotalCharges' by contract length\ntemp1 = X.groupby('Contract')['TotalCharges'].agg(['mean']).rename({'mean':'Contract_mean_totCharges'},axis=1)\nX = pd.merge(X, temp1, on='Contract', how='left')\n\n# Create column giving the difference in 'TotalCharges' and the average of 'TotalCharges' by contract length\nX['Contract_totCharges_diff'] = X['TotalCharges'] - X['Contract_mean_totCharges']\n\n\n# Create column giving the average of 'MonthlyCharges' by payment method\ntemp2 = X.groupby('PaymentMethod')['MonthlyCharges'].agg(['mean']).rename({'mean':'PayMeth_mean_monthCharges'},axis=1)\nX = pd.merge(X, temp2, on='PaymentMethod', how='left')\n\n# Create column giving the difference in 'MonthlyCharges' and the average of 'MonthlyCharges' by payment method\nX['PayMeth_monthCharges_diff'] = X['MonthlyCharges'] - X['PayMeth_mean_monthCharges']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Round values to two decimal places\nX = X.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Encode Categorical Features </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Ordinal Encoding\n\n# Ordinal encoding of 'MultipleLines'\nmultiLines_dict = {'No_PhoneService':0, 'No_MultiLines':1, 'MultiLines':2}\nX['MultipleLines_Ordinal'] = X['MultipleLines'].map(multiLines_dict)\n\n# Ordinal encoding of 'InternetService'\nintServ_dict = {'No_internet_service':0, 'DSL':1, 'Fiber_optic':2}\nX['InternetService_Ordinal'] = X['InternetService'].map(intServ_dict)\n\n# Ordinal encoding of 'Contract'\ncontract_dict = {'Month-to-month':0, 'One_year':1, 'Two_year':2}\nX['Contract_Ordinal'] = X['Contract'].map(contract_dict)\n\n# Drop unnecessary columns that have been encoded\nordinal_drop_cols = ['MultipleLines', 'InternetService', 'Contract']\nX.drop(ordinal_drop_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### One-hot Encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to the relevant columns\nOH_col_names = ['SeniorCitizen', 'Partner', 'Dependents', \n           'PaperlessBilling', 'PhoneService', 'OnlineSecurity', \n           'OnlineBackup', 'DeviceProtection', 'TechSupport',\n           'StreamingTV', 'StreamingMovies', 'PaymentMethod',\n           'SenCit_Dependents', 'Partner_Dependents', 'SenCit_Partner',\n           'SenCit_Contract', 'SenCit_TechSupport', 'SenCit_PayMeth']\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(X[OH_col_names]))\n\n# Replace default column names with more descriptive ones\nOH_cols.columns = OH_encoder.get_feature_names(OH_col_names)\n\n# One-hot encoding removed index; put it back\nOH_cols.index = X.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nX.drop(OH_col_names, axis=1, inplace=True)\n\n# Add one-hot encoded columns to numerical features\nX = pd.concat([X, OH_cols], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3) Final Preparation of the Data"},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Oversample the Minority Class </center></h3>\n\nOversampling via **SMOTE** will help combat class imbalance and lead to more accurate predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n# Oversample our dataset using SMOTE to deal with class imbalance\noversample = SMOTE(sampling_strategy=0.5, random_state=42)\nX, y = oversample.fit_resample(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Perform Feature Scaling </center></h3>\nOur goal is to test an entire suite of classification algorithms on our data in order to pick out the candidates that are best suited to the structure of our problem.  In order to make the comparison a fair one, we must scale our features because some of the algorithms to be tested (Logistic Regression, SVM, Linear Discriminant Analysis, and KNN) are based on the assumption that data is standardized and centered around 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the columns we wish to transform\nscale_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', \n              'Contract_mean_totCharges', 'Contract_totCharges_diff', \n              'PayMeth_mean_monthCharges', 'PayMeth_monthCharges_diff',]\n\n# Scale the relevant columns\ntransformer = ColumnTransformer([('scaler', StandardScaler(), scale_cols)], \n                                remainder='passthrough')\nscaled_X = pd.DataFrame(transformer.fit_transform(X))\n\n# Transformation removed column names; put them back\nscaled_X.columns = X.columns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.0)  Analysis of Model Candidates"},{"metadata":{},"cell_type":"markdown","source":"In order to choose our final model, we will first evaluate an entire suite of algorithms to determine which candidates are best suited for the structure of our project.  \n\nLet's start comparing!"},{"metadata":{},"cell_type":"markdown","source":"## 5.1) Reduce the Size of Our Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the DF size\ndef df_mem_reducer(df):\n    \"\"\"\n    Reduces the memory usage of a given dataframe via conversion of numeric attribute datatypes\n    Input:\n        - df: a Pandas dataframe\n    Output: \n        - the same Pandas dataframe that uses less memory\n    \"\"\"\n    \n    # Original memory usage of dataframe in MB\n    start_mem = df.memory_usage().sum() / 1024**2    \n    \n    # Conversion of numerical datatypes\n    num_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns:\n        col_dtype = df[col].dtypes            \n        if col_dtype in num_dtypes:\n            col_min = df[col].min()\n            col_max = df[col].max()\n            if str(col_dtype)[:3] == 'int':\n                if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else: \n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Original memory usage:  ', start_mem, '  MB')\n    print('Final memory usage:  ', end_mem, '  MB')\n    print('Memory of DataFrame reduced by:  ', ((start_mem - end_mem) / start_mem) * 100, '%')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce memory usage of scaled_X\nscaled_X = df_mem_reducer(scaled_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce memory usage of X\nX = df_mem_reducer(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2) Run Our Testing Suite and Analyze the Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\n# Import linear ML algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Import non-linear ML algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# Initialize our testing suite\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\nresults = []\nmodel_names = []\nall_models = []\nall_models.append(('LR', LogisticRegression(max_iter=1000)))\nall_models.append(('LDA', LinearDiscriminantAnalysis()))\nall_models.append(('KNN', KNeighborsClassifier()))\nall_models.append(('NB', GaussianNB()))\nall_models.append(('SVM', SVC()))\nall_models.append(('XGB', XGBClassifier()))\n\n# Run the tests\nfor name, model in all_models:\n    scores = cross_val_score(model, scaled_X, y, cv=cv, scoring='roc_auc')\n    results.append(scores)\n    model_names.append(name)\n    print(name, ':  ', 'Mean =  ', scores.mean(), '  ', 'STD =   ', scores.std())\n    \n# Visualize the results\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nsns.boxplot(x=model_names, y=results, palette='Blues_r')\nplt.title('Comparison of Algorithms', fontsize=20, pad=23, fontweight='bold')\nplt.ylabel('AUC - ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, our **XGBoost** and **Logistic Regression** models performed the best with Linear Discriminant Analysis and Support Vector Machines not far behind.\n\nFor this project, we will select XGBoost and Logistic Regression for further hyperparameter tuning due to their high predictive potential and because they cover both linear and non-linear classes of machine-learning algorithms.  "},{"metadata":{},"cell_type":"markdown","source":"# 6.0)  Training of Final Models\n"},{"metadata":{},"cell_type":"markdown","source":"In order to tune our Logistic Regression and XGBoost models, we will implement a novel hyperparameter tuning algorithm known as **Hyperband** [1][2]. (Li et al., 2018).  \n\nWhereas classic algorithms such as Random Search are \"result-blind\" in the sense that they don't \"learn\" from poor combinations of hyperparameters (choice is random), Hyperband is \"smart\" in the sense that it tests random combinations of hyperparameters, learns from its mistakes, and pursues the hyperparameter configurations that are most likely to minimize its error.  Not only that, Hyperband is more economical in its use of resources by testing bad combinations of hyperparameters only briefly while deeply exploring only the most promising options. As such, Hyperband offers a competitive advantage over other optimization algorithms in terms of speed and efficiency.\n\nWe implement it as follows:"},{"metadata":{},"cell_type":"markdown","source":"## 6.1) Logistic Regression - Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install Hyperband\n!cp -r ../input/hyperbandcv/scikit-hyperband-master/* ./\n\n# Import packages\nfrom hyperband import HyperbandSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_randfloat \nfrom sklearn import metrics\n\n\n# Define our model\nfixed_params = {'dual': False, \n                'random_state': 42,\n                'n_jobs': 1\n               }\nparam_dist = {'max_iter': sp_randint(200,500),\n              'solver': ['lbfgs', 'sag', 'newton-cg'],\n              'penalty': ['l2'],\n              'C': sp_randfloat(0.01, 10),\n             }\nlr = LogisticRegression(**fixed_params)\n\n\n\n# Perform Hyperparameter Tuning\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=0)\nsearch_lr = HyperbandSearchCV(estimator = lr,\n                              param_distributions = param_dist, \n                              resource_param='max_iter',\n                              min_iter = 200,\n                              max_iter = 1000,\n                              cv = cv, \n                              scoring='roc_auc',\n                              refit=True,\n                              verbose = 0,\n                              random_state = 42\n                          )\nsearch_lr.fit(scaled_X, y)\n\n# Print ROC Curve\nplt.figure(figsize=(8,5))\nmetrics.plot_roc_curve(search_lr.best_estimator_, scaled_X, y)\nplt.title('ROC Curve of Our Logistic Regression Model', pad=20, fontweight='bold')\nplt.xlabel('False Positive Rate', labelpad=15, fontsize=15)\nplt.ylabel('True Positive Rate', labelpad=15, fontsize=15)\nplt.show()\n\n# Print results of hyperparameter tuning\nprint('Best parameters:  ', search_lr.best_params_)\nprint('AUC - ROC score:  ', search_lr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2) XGBoost - Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define our model\nfixed_params = {'objective': 'binary:logistic', \n                'random_state': 42,\n                'n_jobs': 1\n               }\nparam_dist = {'max_depth': sp_randint(3,6),\n              'learning_rate': sp_randfloat(0.01, 0.1),\n              'n_estimators': sp_randint(100, 1000),\n              'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n              'reg_alpha': sp_randfloat(0.0, 1.0),\n              'reg_lambda': sp_randfloat(0.0, 1.0),\n             }\nclf = XGBClassifier(**fixed_params)\n\n\n\n# Perform Hyperparameter Tuning\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=0)\nsearch = HyperbandSearchCV(estimator = clf,\n                           param_distributions = param_dist, \n                           resource_param='n_estimators',\n                           min_iter = 100,\n                           max_iter = 1000,\n                           cv = cv, \n                           scoring='roc_auc',\n                           refit=True,\n                           verbose = 0,\n                           random_state = 42\n                          )\nsearch.fit(X, y)\n\n\n# Print ROC Curve\nplt.figure(figsize=(8,5))\nmetrics.plot_roc_curve(search.best_estimator_, X, y)\nplt.title('ROC Curve of Our XGBoost Model', pad=20, fontweight='bold')\nplt.xlabel('False Positive Rate', labelpad=15, fontsize=15)\nplt.ylabel('True Positive Rate', labelpad=15, fontsize=15)\nplt.show()\n\n# Print results of hyperparameter tuning\nprint('Best parameters:  ', search.best_params_)\nprint('AUC - ROC score:  ', search.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.0)  Interpretation of Results\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load our saved models\nloaded_modelLR = load('../input/lrtelcochurnclassifier/lrTelcoChurnClassifier.joblib.dat')\nloaded_modelXGB = load(\"../input/xgbtelcochurnclassifier/xgbTelcoChurnClassifier.joblib.dat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.1)  Logistic Regression - Feature Importance\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the feature weights of our Logistic Regression model\nlr_coefficients = pd.DataFrame(loaded_modelLR.coef_)\n\n# DataFrame lacks column names; put them back\nlr_coefficients.columns = X.columns\n\n# Reshape (wide --> long format) and \n# filter the data to get features our Logistic Regression model found important\nlr_coefficients = pd.melt(lr_coefficients, var_name='Features', value_name='Coefficients')\nlr_coefficients.sort_values('Coefficients', ascending=False, inplace=True)\nlr_important_features = lr_coefficients[np.abs(lr_coefficients['Coefficients']) > 0.9]\nlr_important_features\n\n# Reshape (long --> wide format) in order to get our data in a form \n# such that Seaborn will be able to create a horizontal barplot\nlr_important_features = lr_important_features.pivot_table(columns='Features', values='Coefficients')\n\n# Manual ordering of columns to create visualization\ncol_order = ['MonthlyCharges', 'PayMeth_monthCharges_diff', 'PhoneService_No_PhoneService',\n             'OnlineSecurity_OnlineSecurity', 'MultipleLines_Ordinal', 'StreamingMovies_StreamingMov',\n             'StreamingTV_StreamingTV', 'tenure', 'PhoneService_PhoneService', 'InternetService_Ordinal' ]\nlr_important_features = lr_important_features[col_order]\n\n# Rename columns for enhanced readability\nrename_mapping = {'PayMeth_monthCharges_diff': 'deviatFromPaymentMethodAvgMonthlyCharge',\n                  'PhoneService_No_PhoneService': 'noPhoneService',\n                  'OnlineSecurity_OnlineSecurity': 'hasOnlineSecurity',\n                  'MultipleLines_Ordinal': 'numTelephoneLines',\n                  'StreamingMovies_StreamingMov': 'streamsMovies',\n                  'StreamingTV_StreamingTV': 'streamsTV',\n                  'tenure': 'monthsWithCompany',\n                  'PhoneService_PhoneService': 'hasPhoneService',\n                  'InternetService_Ordinal': 'internetServiceQuality'}\nlr_important_features.rename(columns=rename_mapping, inplace=True)\n\n\n# Visualize the results\nsns.set_style('whitegrid')\nplt.figure(figsize=(7,7))\nsns.barplot(data=lr_important_features, palette='Blues_r', orient='h')\nplt.xlabel('Coefficient Values', labelpad=20)\nplt.ylabel('')\nplt.title('Feature Importance of Our Logistic Regression Model', \n          pad=25, fontsize=20, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <h3><center> Interesting Observations: </center></h3>\n1. `MonthlyCharges` is positively correlated to customer churn\n    - The more a customer pays per month, the more likely they are to churn.\n2. `deviatFromPaymentMethodAvgMonthlyCharge` is positively correlated to customer churn\n    - The more a customer pays per month (relative to the average monthly charge of all customers with the same payment method), the more likely they are to churn.\n3. `noPhoneService` is positively correlated to customer churn\n    - Customers with no phone service are more likely to churn.\n4. `hasOnlineSecurity` is negatively correlated to customer churn\n    - Customers with no online security are more likely to churn.\n5. `numTelephoneLines` is negatively correlated to customer churn\n    - Customers with fewer associated telephone lines are more likely to churn.\n6. `streamsMovies` is negatively correlated to customer churn\n    - Customers who don't stream movies are more likely to churn.\n7. `streamsTV` is negatively correlated to customer churn\n    - Customers who don't stream TV are more likely to churn.\n8. `monthsWithCompany` is negatively correlated to customer churn\n    - Customers who are relatively new are more likely to churn.\n9. `hasPhoneService` is negatively correlated to customer churn\n    - Customers without phone service are more likely to churn.\n10. `internetServiceQuality` is negatively correlated to customer churn\n    - Customers with poor quality internet are more likely to churn."},{"metadata":{},"cell_type":"markdown","source":"## 7.2)  XGBoost - Feature Importance"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import shap\n\n# Code to fix a bug that prevents SHAP from interacting with the XGBoost library [3]\nmybooster = loaded_modelXGB.get_booster()    \nmodel_bytearray = mybooster.save_raw()[4:]\ndef myfun(self=None):\n    return model_bytearray\nmybooster.save_raw = myfun\n\n# Get SHAP values\nexplainer = shap.TreeExplainer(mybooster)\nshap_values = explainer.shap_values(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot feature importance of our XGBoost Model\nplt.figure()\nplt.title('Feature Importance of Our XGBoost Model', pad=25, fontsize=20, fontweight='bold')\nshap.summary_plot(shap_values, X, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n\nLooking at this graph, we can identify a few common features that both our XGBoost and Logistic Regression model found to be important:\n1. `monthsWithCompany`\n2. `hasOnlineSecurity`\n3. `MonthlyCharges`\n4. `internetServiceQuality`\n\nHowever, our XGBoost model (unlike our Logistic Regression model) additionally found the following features to be important:\n1. `Contract_mean_totCharges`\n2. `PaymentMethod_Electronic_check`\n3. `TechSupport_No_TechSupport`\n4. `PaperlessBilling_No_PaperlessBill`\n\n\nThe first finding is valuable to us because if two models independently identify a feature as important, then it is very likely that the feature is, in fact, important.  \n\nHowever, the second finding is also important to compare the results of a linear model to a non-linear model.  Since our Logistic Regression model didn't give these features much weight, it may be the case that our XGBoost model's tree-based structure allowed it to pick up on novel feature interactions that our other model was blind to. These features are interesting, and we'll explore their interactions in the next few cells.\n\nBut first, let's take a look at a more nuanced version of the above graph in order to visualize different groupings in our data as well as spot correlations between features and our target variable.  The following guide should be helpful as we decipher the upcoming summary plot."},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Quick-and-dirty Guide to Reading the Summary Plot: </center></h3>\n\n1. Features are ranked from most important to least important\n2. Each 'dot' reprepresents a customer\n3. Each 'dot' is color coded to represent the relative value assigned to that customer's feature\n    - Red is a high value\n        - if the feature is categorical and has been one-hot encoded, then Red is referencing the value '1' or 'Yes'\n    - Blue is a low value\n        - if the feature is categorical and has been one-hot encoded, then Blue is referencing the value '0' or 'No'\n4. The x-axis represents SHAP value and reflects correlation between a feature's value and model output\n    - Positive values are correlated to a customer churning\n    - Negative values are correlated to a customer staying\n    - The higher the value's magnitude, the stronger the correlation and vice versa.\n5. The segregation of blue and red 'dots' indicates how well the feature was able to map similar values of that feature (red or blue dots) to a churn class (positive or negative SHAP values)\n    - Well-segregated colors indicate high, standalone predictive potential\n    - Poorly-segregated colors point to the presence of underlying feature interactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a summary plot showing groupings of data and correlations between features and the target variable.\nplt.figure()\nplt.title('Summary Plot of Our XGBoost Model', pad=25, fontsize=20, fontweight='bold')\nshap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n\n1. Customers with a higher `Contract_mean_totCharges` are less likely to churn (and vice versa)\n    - Feature is a proxy for contract length in which high values of average total-charges reflect a long-term contract and a customer being 'locked in' to the system.\n2. Customers with a higher `tenure` are less likely to churn (and vice versa)\n    - Customers who have been with the company for longer are more loyal and less likely to churn.\n3. Customers with no online security, no tech support, or no online backup were those who were more likely to churn (and vice versa)\n    - Likely a proxy for socio-economic status where customers with lower incomes are more likely to churn.\n4. Customers who pay via electronic check were more likely to churn, and those that opted for paper billing were less likely to churn.\n\n\nSince we currently have a broad sense of how our predictive variables correlate to model outcome, let's now get a more nuanced understanding of our feature-feature interactions by looking at some dependency plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up graphs to be displayed side-by-side\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,5), sharex=False, sharey=False)\nfig.suptitle('Effect of Tenure, Monthly Charge, and Their Interactions on Model Output', \n             y=1.1, fontsize=23, fontweight='bold')\n\n# Create graph showing effect of 'tenure' on model output\ntenure_dependencePlot = shap.dependence_plot('tenure', shap_values, X, \n                                             ax=axes[0], show=False)\n\n# Create graph showing effect of 'MonthlyCharges' on model output\nmonthlyCharges_dependencePlot = shap.dependence_plot('MonthlyCharges', shap_values, X, \n                                                     ax=axes[1], show=False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> Interesting Observations: </center></h3>\n1. `tenure` is negatively correlated to whether a customer will churn\n    - customers who have been with the company longer are less likely to churn (and vice-versa)\n        - 1.5 years with the company marks the \"turning point\" at which a customer is more likely to stay as opposed to churn.\n    - the feature interacts with `Contract_mean_totCharges` (a proxy feature for contract length)\n        - most customers on 2-year contracts (red dots) have been with the company for at least 3 years, and these customers are likely to stay.\n2. `MonthlyCharges` is positively correlated to whether a customer will churn\n    - customers who experience higher monthly charges are more likely to churn (and vice-versa)\n        - customers with monthly charges below \\\\$70 are less likely to churn, but once a customer's monthly charges go above \\\\$80, then they become more likely to churn.\n    - the feature interacts with `Contract_mean_totCharges` (a proxy feature for contract length)\n        - long-term, 2-year contracts help mitigate the risk of a customer churning even if a customer's monthly charges are past the threshold of \\\\$80 and are in the range of \\\\$80-\\\\$100, but once a customer's monthly charges run past \\\\$100, then contract length no longer mitigates the greater risk of them churning."},{"metadata":{},"cell_type":"markdown","source":"# 8.0)  Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"In this project, we analyzed customer data to predict whether customers would churn or not churn.\nOur trained models had the following predictive potential:\n\n1. **XGBoost**:              AUC-ROC = 0.89\n2. **Logistic Regression**:  AUC-ROC = 0.86\n\nAs for our next steps, we will pass over the XGBoost model to the company's Customer Retention group in order to better inform their business strategy. By targeting those customers who are most likely to churn, the company would be able to cut costs via more efficient allocation of resources towards customer incentive programs. \n\nFurthermore, the company would enjoy increased revenue via its improved retention strategy.  By using our model to help convert 'high-flight-risk' customers to a more loyal following, the company would be looking at maintaing a valuable, mutually-beneficial relationship with its customer base for many years to come."},{"metadata":{},"cell_type":"markdown","source":"### <h3><center> References: </center></h3>\n\n[1] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2017. Hyperband: a novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res. 18, 1 (January 2017), 67656816.\n\n[2] Thomas Huijskens. 2019. Hyperband. https://github.com/thuijskens/scikit-hyperband. (2020).\n\n[3] user3668129 and Luis Curbelo. 2020. Getting UnicodeDecodeError when using shap on xgboost. (May 2020). Retrieved September 7, 2020 from https://stackoverflow.com/a/62924801"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}