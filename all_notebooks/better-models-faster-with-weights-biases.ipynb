{"cells":[{"metadata":{"id":"QJc0KMZlzdtO","colab_type":"text"},"cell_type":"markdown","source":"# Why Weights & Biases Gives You An Edge On Kaggle Competitions\n\nExcited to announce that [Weights & Biases](https://www.wandb.com/) now comes baked into your Kaggle kernels! Used by the likes of [OpenAI](http://openai.com/) and [Github](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/), W&B is part of the new standard of best practices for machine learning.\n\nI thought I'd write a quick post on why my fellow Kagglers might find W&B useful, and how you can integrate it into your projects with just a few lines of code. You can find the [full blog post here](https://www.wandb.com/articles/better-models-faster-with-weights-biases) – I highly encourage you check it out to run a more efficient ML process than your peers.\n\n**Tl;dr –** W&B helps you visualize your model performance and predictions, find the best models fast and share insights learned from your models.\n\nHere are a few use cases in which W&B is specially useful for Kagglers:\n1. **Track and compare models:** I want to test out my hypotheses fast and iterate quickly to find the best model\n2. **Visualize model performance for debugging:** I want to see how my models are performing in real time and debug them\n3. **Efficient hyperparameter search:** I want to find the best model faster than everyone else\n4. **Resource efficient model training:** I want to be efficient with my model training and not spend more money than I need to\n5. **Show off my model:** I want to share my models and key insights with my teammates and the Kaggle community\n\nThe blog post goes into detail about how W&B helps address all of these needs. In this Kernel, I want to show you how to integrate W&B in your models in just a few lines of code.\n\nFirst, here's a quick overview of some of the visualizations W&B automatically creates for your models:\n\n### Visualize model performance and training metrics\n<img src=\"https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574108018540_Screenshot+2019-11-18+12.13.29.png\" style=\"width: 80%\"/>\n\n### Visualize the affect of hyperparameters on model performance\n<img src=\"https://paper-attachments.dropbox.com/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297697453_image.png\" style=\"width: 80%\"/>\n\n### Visualize model predictions\n<img src=\"https://paper-attachments.dropbox.com/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297173261_vzye9ei.png\" style=\"width: 80%\"/>\n\n### Visualize gradients to help deal with vanishing and exploding gradients\n<img src=\"https://paper-attachments.dropbox.com/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578298883597_image.png\" style=\"width: 80%\"/>\n\n### Visualize system metrics, including GPU usage\n<img src=\"https://paper-attachments.dropbox.com/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297329745_image.png\" style=\"width: 80%\"/>\n\n### Visualize images, videos, html, audio, 3D objects, plots, tables, point clouds\n<img src=\"https://paper-attachments.dropbox.com/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297638486_image.png\" style=\"width: 80%\"/>\n\nAs you can see W&B structures your metrics & predictions, and presents them in a way that provides actionable insights about your training process.\n\nAlright, now that you know how W&B can be useful, let's see how you can integrate it into your project and get all these visualizations, with just a few lines of code.\n\nIf you have an existing kernel, you just need to switch your Docker image to 'Latest Available'. All new kernels should work out of the box."},{"metadata":{},"cell_type":"markdown","source":"# 1. Log any metric with Weights and Biases\n*   **wandb.init()** – Initialize a new W&B run. Each run is single execution of the training script.\n- **wandb.log()** – Logs custom objects like images, videos, audio files, HTML, plots, point clouds etc.\n-   **%%wandb** – Add this at the top of a cell to show model metrics live below the cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get Apple stock price data from https://www.macrotrends.net/stocks/charts/AAPL/apple/stock-price-history\nimport pandas as pd\nimport wandb\n\n# Read in dataset\napple = pd.read_csv(\"../input/kernel-files/apple.csv\")\napple = apple[-1000:]\nwandb.init(project=\"visualize-models\", name=\"a_metric\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%wandb\n# Log the metric\nfor price in apple['close']:\n    wandb.log({\"Stock Price\": price})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Monitor boosting model performance\n\nStart out by importing the experiment tracking library and setting up your free W&B account:\n\n*   **import wandb** – Import the wandb library\n*   **callbacks=[wandb.xgboost.wandb_callback()]** – Add the wandb [XGBoost callback](https://docs.wandb.com/library/frameworks/xgboost), or\n*   **callbacks=[wandb.lightgbm.wandb_callback()]** – Add the wandb LightGBM callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the dataset from UCI\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data -qq\n\n# modified from https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py\n# Import wandb\nimport wandb\nimport numpy as np\nimport xgboost as xgb\n\nwandb.init(project=\"visualize-models\", name=\"xgboost\")\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['silent'] = 1\nparam['nthread'] = 4\nparam['num_class'] = 6\nwandb.config.update(param)\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%wandb\n# Add the wandb xgboost callback\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[wandb.xgboost.wandb_callback()])\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\nwandb.summary['Error Rate'] = error_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Monitor scikit learn performance\n\nLogging sklearn plots with Weights & Biases is simple.\n\n### Step 1: First import wandb and initialize a new run.\n```\nimport wandb\nwandb.init(project=\"visualize-sklearn\")\n\n# load and preprocess dataset\n# train a model\n```\n\n### Step 2: Visualize individual plots.\n```\n# Visualize single plot\nwandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)\n```\n\n### Or visualize all plots at once:\n```python\n# Visualize all classifier plots\nwandb.sklearn.plot_classifier(clf, X_train, X_test, y_train, y_test, y_pred, y_probas, labels, model_name='SVC', feature_names=None)\n\n# All regression plots\nwandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test,  model_name='Ridge')\n\n# All clustering plots\nwandb.sklearn.plot_clusterer(kmeans, X_train, cluster_labels, labels=None, model_name='KMeans')\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport wandb\n# wandb.init(anonymous='allow', project=\"sklearn\")\nwandb.init(project=\"visualize-models\", name=\"sklearn\")\n\n# Load data\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train model, get predictions\nreg = Ridge()\nreg.fit(X, y)\ny_pred = reg.predict(X_test)\n\n# Visualize model performance\nwandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, 'Ridge')","execution_count":null,"outputs":[]},{"metadata":{"id":"fVXNBu8Q68r5","colab_type":"text"},"cell_type":"markdown","source":"# 4. Monitor neural network performance\n\nStart out by installing the experiment tracking library and setting up your free W&B account:\n\n*   **import wandb** – Import the wandb library\n*   **from wandb.keras import WandbCallback** – Import the wandb [keras callback](https://docs.wandb.com/library/frameworks/keras)"},{"metadata":{"id":"Lxjw5Qckzg5W","colab_type":"code","outputId":"d404da8f-7497-438d-8fb7-786e225064fa","colab":{"base_uri":"https://localhost:8080/","height":301},"trusted":true},"cell_type":"code","source":"# WandB – Import the W&B library\nimport wandb\nfrom wandb.keras import WandbCallback\n\nfrom keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\nfrom keras.callbacks import TensorBoard","execution_count":null,"outputs":[]},{"metadata":{"id":"FT4dKp1xVeeT","colab_type":"text"},"cell_type":"markdown","source":"*   **wandb.init()** – Initialize a new W&B run. Each run is single execution of the training script.\n*   **wandb.config** – Save all your hyperparameters in a config object. This lets you use W&B app to sort and compare your runs by hyperparameter values.\n*   **callbacks=[WandbCallback()]** – Fetch all layer dimensions, model parameters and log them automatically to your W&B dashboard."},{"metadata":{"id":"ue4Xg1b65Vr4","colab_type":"code","outputId":"5c5f2b37-f71d-4228-81fd-88105b9462d4","colab":{"base_uri":"https://localhost:8080/","height":102},"trusted":true},"cell_type":"code","source":"# Default values for hyper-parameters\ndefaults=dict(\n    dropout = 0.2,\n    hidden_layer_size = 32,\n    layer_1_size = 32,\n    learn_rate = 0.01,\n    decay = 1e-6,\n    momentum = 0.9,\n    epochs = 5,\n    )\n\n# Initialize a new wandb run and pass in the config object\n# wandb.init(anonymous='allow', project=\"kaggle\", config=defaults)\nwandb.init(project=\"visualize-models\", config=defaults, name=\"neural_network\")\nconfig = wandb.config\n\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nlabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\n        \"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n\nimg_width=28\nimg_height=28\n\nX_train = X_train.astype('float32')\nX_train /= 255.\nX_test = X_test.astype('float32')\nX_test /= 255.\n\n#reshape input data\nX_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)[:10000]\nX_test = X_test.reshape(X_test.shape[0], img_width, img_height, 1)[:10000]\n\n# one hot encode outputs\ny_train = np_utils.to_categorical(y_train)[:10000]\ny_test = np_utils.to_categorical(y_test)[:10000]\nnum_classes = y_test.shape[1]\n\n# build model\nmodel = Sequential()\nmodel.add(Conv2D(config.layer_1_size, (5, 5), activation='relu',\n                            input_shape=(img_width, img_height,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(config.dropout))\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n\nsgd = SGD(lr=config.learn_rate, decay=config.decay, momentum=config.momentum, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%wandb\n# Add WandbCallback() to the fit function\nmodel.fit(X_train, y_train,  validation_data=(X_test, y_test), epochs=config.epochs,\n    callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find the best neural network model with hyperparameter sweeps\n\nSearching through high dimensional hyperparameter spaces to find the winning model can get unwieldy very fast. W&B's hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the best one.\n\nRunning a hyperparameter sweep with Weights & Biases is very easy. You can use [this guide](https://docs.wandb.com/sweeps/overview/getting-started) to get started.\n\n![](https://paper-attachments.dropbox.com/s_A8A9577ACEF2EF9A66A68CAA0D798FE3970C9A78CA8BF44A10FA307611490E90_1572034183402_Screenshot+2019-10-25+13.09.37.png)"},{"metadata":{},"cell_type":"markdown","source":"## More Resources\n\nCheck out some other cool things you can do with Weights & Biases:\n* [Visualize sklearn models](https://app.wandb.ai/lavanyashukla/visualize-sklearn/reports/Visualize-Sklearn-Model-Performance--Vmlldzo0ODIzNg)\n* [Visualize model predictions](https://app.wandb.ai/lavanyashukla/visualize-predictions/reports/Visualize-Model-Predictions--Vmlldzo1NjM4OA/)\n* [Track model performance](https://app.wandb.ai/lavanyashukla/visualize-models/reports/Visualize-Model-Performance--Vmlldzo1NTk2MA)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Intro to Keras with W&B.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}