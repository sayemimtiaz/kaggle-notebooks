{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Android Malware Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Android\n\n\nAndroid is one of the most used mobile operating systems worldwide. Due to its technological impact, its open-source code and the possibility of installing applications from third parties without any central control, Android has recently become a malware target. Even if it includes security mechanisms, the last news about malicious activities and Android麓s vulnerabilities point to the importance of continuing the development of methods and frameworks to improve its security.\n\nTo prevent malware attacks, researches and developers have proposed different security solutions, applying static analysis, dynamic analysis, and artificial intelligence. Indeed, data science has become a promising area in cybersecurity, since analytical models based on data allow for the discovery of insights that can help to predict malicious activities.\n\nWe can analyze cyber threats using two techniques, static analysis, and dynamic analysis, the most important thing is that these are the approaches to get the features that we are going to use in data science.\n\n+ **Static analysis**: it includes the methods that allow us to get information about the software that we want to analyze without executing it, one example of them is the study of the code, their callings, resources, etc.\n+ **Dynamic analysis**: it is another approach where the idea is to analyze the cyber threat during its execution, in other words, get information about its behavior, some of their features are the netflows.\n"},{"metadata":{},"cell_type":"markdown","source":"# State of the Art\n\nIn 2016 we published an article [2] about the state of the art of frameworks and results about Android malware detection. This work reflects different static analysis tools (TaintDroid, Stowaway, Crowdroid y Airmid), dynamic analysis systems (Paranoid and DroidMOSS), frameworks (MobSafe, SAAF, and ASEF) and some research results about using machine learning. From this article we concluded that the idea is using both static and dynamic analysis in order to get spectra of features, moreover, some works have been working to use virtual devices in the cloud.\n"},{"metadata":{},"cell_type":"markdown","source":"# Datasets\n\nIn 2016 we explored [3] Android Genome Project (MalGenome), it is a dataset which was active from 2012 until the end of the year 2015, this set of malware has a size of 1260 applications, grouped into a total of 49 families. Today, we can find other jobs such as: Drebin, a research project offering a total of 5560 applications consisting of 179 malware families; AndrooZoo, which includes a collection of 5669661 applications Android from different sources (including Google Play); VirusShare, another repository that provides samples of malware for cybersecurity researchers; and DroidCollector, this is another set which provides around 8000 benign applications and 5560 malware samples, moreover, it facilitates us samples of network traffic as pcap files."},{"metadata":{},"cell_type":"markdown","source":"# Static Analysis \n\nIn this first step, I'm going to analyze some features in order to answer the next hypothesis, *exist a differential of the permissions used between a set of malware and benign samples*, in other wordsβ\n\n<img src=\"https://pics.me.me/when-the-flashlight-app-wants-access-to-your-call-history-32812256.png\" height=\"250\" width=\"250\">\n\nFor this approach, I developed a code that consisted to extract and make a CSV file which has information about permissions of applications, through this script you can map each APK (Android Application Package) against a list of permissions. You can find more information about the proposed framework at [3]\n\nhttps://github.com/urcuqui/WhiteHat"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import preprocessing\nimport torch\nfrom sklearn import svm\nfrom sklearn import tree\nimport pandas as pd\nfrom sklearn.externals import joblib\nimport pickle\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import cross_val_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,Dropout\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory "},{"metadata":{},"cell_type":"markdown","source":"For the next analysis, I'm going to explore the Malgenome dataset, as I said nowadays we can find other sources with a lot of examples and malware families which would be important for future works, the idea of the next experiment and results is to show our first approached.\n\n<img src=\"https://ieee-dataport.org/sites/default/files/styles/large/public/Sin%20t%C3%ADtulo.png?itok=4YoQim00\" /> "},{"metadata":{},"cell_type":"markdown","source":"*It is not neccesary to apply a data cleaning and transformation process*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/datasetandroidpermissions/train.csv\", sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.astype(\"int64\")\ndf.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Type is the label that represents if an application is a malware or not, as we can see this dataset is balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Let's get the top 10 of permissions that are used for our malware samples*"},{"metadata":{},"cell_type":"markdown","source":"*Malicious*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series.sort_values(df[df.type==1].sum(axis=0), ascending=False)[1:11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Benign*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series.sort_values(df[df.type==0].sum(axis=0), ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs =  plt.subplots(nrows=2, sharex=True)\n\npd.Series.sort_values(df[df.type==0].sum(axis=0), ascending=False)[:10].plot.bar(ax=axs[0])\npd.Series.sort_values(df[df.type==1].sum(axis=0), ascending=False)[1:11].plot.bar(ax=axs[1], color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last outputs allow us to get insights about a difference between the permissions used by the malware and the benign applications."},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:330], df['type'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Naive Bayes algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation\nX_train=df.iloc[:, 1:330]\ny_train=df['type']\ngnb = GaussianNB()\n\ndef custom_cross_validation1(model, X_train, y_train, cv):\n    my_pipeline = make_pipeline(Imputer(), model)\n    scores = cross_val_score(my_pipeline, X_train, y_train, cv=cv)\n    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    return scores\n\n#sc=custom_cross_validation(gnb, X_train, y_train, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#R square\ndef r2(y_true,y_pred):\n    m_t_v=np.mean(y_true)\n    numerator=0\n    denominator=0\n    for yt,yp in zip(y_true,y_pred):\n        numerator +=(yt-yp)**2\n        denominator +=(yt-m_t_v)**2\n    r=numerator/denominator\n    return 1-r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MCC\ndef mcc(y_true,y_pred):\n    #tp=true_positive(y_true,y_pred)\n    #tn=true_negative(y_true,y_pred)\n    #fp=false_positive(y_true,y_pred)\n    #fn=false_negative(y_true,y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\n    n=(tp*tn)-(fp*fn)\n    d=((tp+fp)*(fn+tn)*(fp+tn)*(tp+fn))\n    d=d**0.5\n    r=n/d\n    return r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoreset(y_true,y_pred):\n    score_set=[]\n    pred=y_pred\n    y_test=y_true\n    print(\"cohen kappa score\",cohen_kappa_score(y_test, pred))\n    score_set.append(cohen_kappa_score(y_test, pred))\n    print(\"cohen kappa score quadratic\",cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n    score_set.append(cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n    print(\"R square score\", r2(y_test,pred))\n    score_set.append(r2(y_test,pred))\n    print(\"MCC score\", mcc(y_test,pred))\n    score_set.append(mcc(y_test,pred))\n    print(\"Brier Score Loss\",brier_score_loss(y_test, pred))\n    score_set.append(brier_score_loss(y_test, pred))\n    print(\"AUC ROC Score\",roc_auc_score(y_test, pred))\n    score_set.append(roc_auc_score(y_test, pred))\n    print(\"\")\n    print(\"accuracy:\",accuracy_score(y_test,pred))\n    score_set.append(accuracy_score(y_test,pred))\n    print(\"F1 Score:\",f1_score(y_test,pred))\n    score_set.append(f1_score(y_test,pred))\n    print(\"Precision:\",precision_score(y_test,pred))\n    score_set.append(precision_score(y_test,pred))\n    print(\"Recall:\",recall_score(y_test,pred))\n    score_set.append(recall_score(y_test,pred))\n    return score_set\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes algorithm\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# pred\npred = gnb.predict(X_test)\n\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(\"naive_bayes\")\nprint(classification_report(pred, y_test, labels=None))\n\n\n#print(\"cohen kappa score\",cohen_kappa_score(y_test, pred))\n#print(\"cohen kappa score quadratic\",cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n#print(\"R square score\", r2(y_test,pred))\n#print(\"MCC score\", mcc(y_test,pred))\n#print(\"Brier Score Loss\",brier_score_loss(y_test, pred))\n#print(\"AUC ROC Score\",roc_auc_score(y_test, pred))\n#print(\"\")\n#print(\"accuracy:\",accuracy)\n#print(\"F1 Score:\",f1_score(pred, y_test))\n#print(\"Precision:\",precision_score(pred, y_test))\n#print(\"Recall:\",recall_score(pred, y_test))\n\ns=scoreset(y_test,pred)\n#print(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*kneighbors algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# kneighbors algorithm\n\nfor i in range(3,15,3):\n    \n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    pred = neigh.predict(X_test)\n    # accuracy\n    accuracy = accuracy_score(pred, y_test)\n    print(\"kneighbors {}\".format(i))\n    print(accuracy)\n    print(classification_report(pred, y_test, labels=None))\n    s=scoreset(y_test,pred)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Decision Tree*"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\nclf.fit(X_train, y_train)\n\n# Read the csv test file\n\npred = clf.predict(X_test)\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(clf)\nprint(accuracy)\nprint(classification_report(pred, y_test, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearSVM\nfrom sklearn import svm\n\nlin_clf = svm.LinearSVC()\nlin_clf.fit(X_train, y_train)\npred = lin_clf.predict(X_test)\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(accuracy)\nprint(classification_report(pred, y_test, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train, y_train)\npred=xgb_clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\npred=clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50,\n                         learning_rate=1.3)#vary the learning rate from 0 to 1.5\n# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n\n#Predict the response for test dataset\npred = model.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdF=RandomForestClassifier(n_estimators=250, max_depth=50,random_state=45)\nrdF.fit(X_train,y_train)\npred=rdF.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(rdF)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 300) \nX_kbest_features = chi2_features.fit_transform(X_train.astype(), y_train) \n  \n# Reduced features \nprint('Original feature number:', X_train.shape[1]) \nprint('Reduced feature number:', X_kbest_features.shape[1]) \nprint(X_kbest_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through the last results we can see how we trained different classifiers to detect malware using its permissions, but as I said this is only a first approximation, I didn't analyze the hyperparameters and others things to improve the results."},{"metadata":{},"cell_type":"markdown","source":"# Dynamic Analysis"},{"metadata":{},"cell_type":"markdown","source":"For this approach, we used a set of pcap files from the DroidCollector project integrated by 4705 benign and 7846 malicious applications. All of the files were processed by our feature extractor script (a result from [4]), the idea of this analysis is to answer the next question, according to the static analysis previously seen a lot of applications use a network connection, in other words, they are trying to communicate or transmit information, so.. is it possible to distinguish between malware and benign application using network traffic?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/network-traffic-android-malware/android_traffic.csv\", sep=\";\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing null with 0\ndata.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##encoding\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(data['type'])\ndata['type'] = le.transform(data['type'])\n#print(data['type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, we have an unbalanced dataset, so another model evaluation will be used.(4704,3141)"},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning and Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we processed each pcap we had some problems getting three features (duration, avg remote package rate, avg local package rate) this why got during the feature processing script, we don't have this issue nowadays. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['duracion','avg_local_pkt_rate','avg_remote_pkt_rate'], axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the idea is to see the outliers in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data.tcp_urg_packet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data.tcp_urg_packet > 0].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That column will be no used for the analysis, only two rows are different to zero, maybe they are interesting for future analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data = data.drop(['duracion','avg_local_pkt_rate','avg_remote_pkt_rate'], axis=1).copy()\nX,y=data.iloc[:,1:17].astype(\"int\"), data.type.astype(\"int\")\nchi_scores = chi2(X,y)\np_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=[\"tcp_urg_packet\"], axis=1).copy()\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have many outliers in some features, I will omit the depth analysis and only get the set of the data without the noise. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data[data.tcp_packets<20000].copy()\ndata=data[data.dist_port_tcp<1400].copy()\ndata=data[data.external_ips<35].copy()\ndata=data[data.vulume_bytes<2000000].copy()\ndata=data[data.udp_packets<40].copy()\ndata=data[data.remote_app_packets<15000].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.duplicated()].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('source_app_packets.1',axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.RobustScaler()\nscaledData = scaler.fit_transform(data.iloc[:,1:11])\nscaledData = pd.DataFrame(scaledData, columns=['tcp_packets','dist_port_tcp','external_ips','vulume_bytes','udp_packets','source_app_packets','remote_app_packets',' source_app_bytes','remote_app_bytes','dns_query_times'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From [6] we concluded that the best network features are:\n\n+ (R1): TCP packets, it has the number of packets TCP sent and got during communication.\n+ (R2): Different TCP packets, it is the total number of packets different from TCP.\n+ (R3): External IP, represents the number the external addresses (IPs) where the application tried to communicated\n+ (R4): Volume of bytes, it is the number of bytes that was sent from the application to the external sites\n+ (R5) UDP packets, the total number of packets UDP transmitted in a communication.\n+ (R6) Packets of the source application, it is the number of packets that were sent from the application to a remote server.\n+ (R7) Remote application packages, number of packages received from external sources.\n+ (R8) Bytes of the application source, this is the volume (in Bytes) of the communication between the application and server.\n+ (R9) Bytes of the application remote, this is the volume (in Bytes) of the data from the server to the emulator.\n+ (R10) DNS queries, number of DNS queries.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2\n\nX_train,y_train=scaledData.iloc[:,0:10].astype(\"int\"), data.type.astype(\"int\")\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 8) \nX_kbest_features = chi2_features.fit_transform(X_train, y_train) \n  \n# Reduced features \nprint('Original feature number:', X_train.shape[1], X_train.columns) \nprint('Reduced feature number:', X_kbest_features.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX,y=scaledData.iloc[:,0:10].astype(\"int\"), data.type.astype(\"int\")\nchi_scores = chi2(X,y)\np_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since dns_query_times has higher the p-value, it says that this variables is independent of the reponse and can not be considered for model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#optional\nscaledData.shape\nscaledData = scaledData.drop(columns=[\"dns_query_times\"], axis=1).copy()\nscaledData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaledData.iloc[:,0:10], data.type.astype(\"str\"), test_size=0.25, random_state=45)\n#X_train, X_test, y_train, y_test = train_test_split(scaledData.iloc[:,0:9], data.type.astype(\"str\"), test_size=0.25, random_state=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_test=y_test.astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearSVM\nfrom sklearn import svm\n\nlin_clf = svm.LinearSVC()\nlin_clf.fit(X_train, y_train)\npred = lin_clf.predict(X_test)\npred=pred.astype(\"int64\")\n## accuracy\ns=scoreset(y1_test,pred)\n#print(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\npred = gnb.predict(X_test)\n\npred=pred.astype(\"int64\")\n## accuracy\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kneighbors algorithm\n\nfor i in range(3,15,3):\n    \n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    pred = neigh.predict(X_test)\n    pred=pred.astype(\"int64\")\n    # accuracy\n    accuracy = accuracy_score(pred, y1_test)\n    print(\"kneighbors {}\".format(i))\n    s=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdF=RandomForestClassifier(n_estimators=250, max_depth=50,random_state=45)\nrdF.fit(X_train,y_train)\npred=rdF.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(rdF)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train, y_train)\npred=xgb_clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50,\n                         learning_rate=1.3)#vary the learning rate from 0 to 1.5\n# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n\n#Predict the response for test dataset\npred = model.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\npred=clf.predict(X_test)\n#cm=confusion_matrix(y1_test, pred)\n\n#accuracy = accuracy_score(y1_test,pred)\n#print(accuracy)\n#print(classification_report(y1_test,pred, labels=None))\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ny_train=le.fit_transform(y_train)\ny_test=le.fit_transform(y_test)\nprint(len(y_test))\nprint(y_test[0:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neural network module\nfrom keras.models import Sequential \nfrom keras.layers import Dense,Activation,Dropout \nfrom keras.layers.normalization import BatchNormalization \nfrom keras.utils import np_utils\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(12, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######best one with 87% ACC and good graphs\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\nimport matplotlib.pyplot as plt\nimport numpy\nmodel=Sequential()\nmodel.add(Dense(1000,input_dim=10,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(300,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n#model.add(Dense(3,activation='softmax'))\nmodel.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######best one with 87% ACC and good graphs for 500 epochs and 0.10 split\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\nimport matplotlib.pyplot as plt\nimport numpy\nmodel=Sequential()\nmodel.add(Dense(1000,input_dim=10,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(700,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(300,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n# Fit the model\nprint('training')\nhistory = model.fit(X_train, y_train, validation_split=0.10, epochs=500, batch_size=100, verbose=1)\nprint('over')\n\n##test dataset\ny_pred = model.predict(X_test)\n#y_pred=np.argmax(y_pred,axis=-1)\ny_pred=np.where(y_pred> 0.5, 1, 0)\n#print(y_pred[0:5])\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\n#tp, fp, fn, tn = confusion_matrix(y_true,y_pred)\nacc=accuracy_score(y_true, y_pred)\nprecision=precision_score(y_true, y_pred, average='weighted')\nf1=f1_score(y_true, y_pred, average='weighted')\nprint('Accuracy=',acc)\nprint('Precision=',precision)\nprint('F1Score=',f1)\nprint('tn, fp, fn, tp' )\n\nfrom sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=['0','1'])\nprint(report)\nprint(confusion_matrix(y_true,y_pred))\n\ns=scoreset(y_true,y_pred)\nplot_graph_train_val(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(32, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nprint('training')\nhistory = model.fit(X_train, y_train, validation_split=0.10, epochs=500, batch_size=100, verbose=1)\nprint('over')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_graph(history)\nplot_graph_train_val(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_6(dim):\n    model=Sequential()\n    model.add(Dense(1000,input_dim=dim,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(500,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(300,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_5(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(256,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(128,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_4(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(256,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_3(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_2(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_1(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_model(dim):\n    model=Sequential()\n    model.add(Dense(1000,input_dim=dim,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(700,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(500,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(300,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(100,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Smote_imbalance():\n    X,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n    print(Counter(y))\n    sm = SMOTE(random_state=42)\n    X1, y1 = sm.fit_resample(X, y)\n    X= pd.DataFrame(data = X1, columns=X.columns) \n    y=pd.DataFrame.from_records(y1)\n    print(Counter(y1))\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph_train(history):\n    # list all data in history\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.title('model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph_train_val(history):\n    # list all data in history\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_scores(y_true,y_pred):\n    acc=accuracy_score(y_true, y_pred)\n    precision=precision_score(y_true, y_pred, average='weighted')\n    f1=f1_score(y_true, y_pred, average='weighted')\n    print('Accuracy=',acc)\n    print('Precision=',precision)\n    print('F1Score=',f1)\n    return acc,precision,f1\ndef calculate_scores2(y_true,y_pred):\n    acc=accuracy_score(y_true, y_pred)\n    precision=precision_score(y_true, y_pred, average='weighted')\n    f1=f1_score(y_true, y_pred, average='weighted')\n    #print('Accuracy=',acc)\n    #print('Precision=',precision)\n    #print('F1Score=',f1)\n    return acc,precision,f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\n# K-fold Cross Validation model evaluation\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=5, random_state=42, shuffle=False)\nskf = StratifiedKFold(n_splits=5,shuffle=True, random_state=1)\nfold_no = 1 \n\naccuracy_scores=[]\nf1_scores=[]\nprecision_scores=[]\nrecall_scores=[]\nr2_score=[]\nmcc_scores=[]\nbrier_scores=[]\nckappa_scores=[]\n\n\nX,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n#X,y=scaledData.iloc[:,0:9], data.type.astype(\"str\")\n#X,y=Smote_imbalance()\n#X,y=X_train,y_train\nfor train_index, test_index in skf.split(X,y):\n  X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] \n  #X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index] \n  #print(X_train.head())\n  #X_train,y_train=X.iloc[train_index],y.iloc[train_index]\n  print(Counter(y_train),Counter(y_test))  \n  # Define the model architecture  \n  model=final_model(10)\n  #model = Sequential()\n  #model.add(Dense(1000,input_dim=10,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(700,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(500,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(300,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(100,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(1,activation='sigmoid'))\n  model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n  # Generate a print\n  print('------------------------------------------------------------------------')\n  print(f'Training for fold {fold_no} ...') \n  sm = SMOTE(random_state=42)\n  #X_train, y_train = sm.fit_resample(X_train, y_train)\n  #X_test, y_test = sm.fit_resample(X_test, y_test)  \n  print(Counter(y_train),Counter(y_test))  \n  # Fit data to model\n  history = model.fit(X_train, y_train,batch_size=100,epochs=500,verbose=0)\n  \n  y_pred = model.predict(X_test)\n  #y_pred=np.argmax(y_pred,axis=-1)\n  y_pred=np.where(y_pred> 0.5, 1, 0)\n  #print(y_pred[0:5])\n  y_true=y_test.astype('int64')\n  #s=scoreset(y_true,y_pred)\n  acc,precision,f1=calculate_scores(y_true, y_pred)\n  accuracy_scores.append(acc)\n  f1_scores.append(f1)\n  precision_scores.append(precision)  \n  #plot_graph_train(history)  \n  # Increase fold number\n  fold_no = fold_no + 1\n\nprint(\"The mean value of accuracy is\",np.mean(accuracy_scores)) \nprint(\"The mean value of precision is\",np.mean(f1_scores))\nprint(\"The mean value of F1 score is\",np.mean(precision_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\nfrom sklearn.model_selection import StratifiedKFold, KFold\n# K-fold Cross Validation model evaluation\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=5, random_state=42, shuffle=False)\nskf = StratifiedKFold(n_splits=5,shuffle=True, random_state=1)\nfold_no = 1 \n\naccuracy_scores=[]\nf1_scores=[]\nprecision_scores=[]\nrecall_scores=[]\nr2_score=[]\nmcc_scores=[]\nbrier_scores=[]\nckappa_scores=[]\n\n\nX,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n#X,y=scaledData.iloc[:,0:9], data.type.astype(\"str\")\n#X,y=Smote_imbalance()\n#X,y=X_train,y_train\nfor i in range(5,8):\n    for train_index, test_index in skf.split(X,y):\n      X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] \n      #print(Counter(y_train),Counter(y_test))  \n      \n      #model=final_model(10)\n      #model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n      # Generate a print\n      #print('------------------------------------------------------------------------')\n      #print(f'Training for fold {fold_no} ...')   \n      #print(Counter(y_train),Counter(y_test))  \n      # Fit data to model\n      if(i==1):\n        model=model_1(10)\n      elif (i==2):\n        model=model_2(10)\n      elif (i==3):\n        model=model_3(10)\n      elif (i==4):\n        model=model_4(10)\n      elif (i==5):\n        model=model_5(10)\n      elif (i==6):\n        model=model_6(10)\n      elif (i==7):\n        model=final_model(10)\n        \n      model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n      history = model.fit(X_train, y_train,batch_size=100,epochs=500,verbose=0)\n  \n      y_pred = model.predict(X_test)\n      #y_pred=np.argmax(y_pred,axis=-1)\n      y_pred=np.where(y_pred> 0.5, 1, 0)\n      #print(y_pred[0:5])\n      y_true=y_test.astype('int64')\n      #s=scoreset(y_true,y_pred)\n      acc,precision,f1=calculate_scores2(y_true, y_pred)\n      accuracy_scores.append(acc)\n      f1_scores.append(f1)\n      precision_scores.append(precision)  \n      #plot_graph_train(history)  \n      # Increase fold number\n      fold_no = fold_no + 1\n    print(\"For model\",i)\n    print(\"The mean value of accuracy is\",np.mean(accuracy_scores)) \n    print(\"The mean value of precision is\",np.mean(f1_scores))\n    print(\"The mean value of F1 score is\",np.mean(precision_scores))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##test dataset\ny_pred = model.predict(X_test)\n#y_pred=np.argmax(y_pred,axis=-1)\ny_pred=np.where(y_pred> 0.5, 1, 0)\n#print(y_pred[0:5])\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\n#tn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\nacc=accuracy_score(y_true, y_pred)\nprecision=precision_score(y_true, y_pred, average='weighted')\nf1=f1_score(y_true, y_pred, average='weighted')\nprint('Accuracy=',acc)\nprint('Precision=',precision)\nprint('F1Score=',f1)\nprint('tn, fp, fn, tp' )\n\nfrom sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=['0','1'])\nprint(report)\nprint(confusion_matrix(y_true,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"**Great !!**, now we have seen the two approaches to analyze a cyber threat. Of course we can use a lot of variables that in this case we didn't use them, for example netflows, methods callings, graph analysis, and many others, but the idea behind this work is to understand that we need to pay attention of all of the environments because when we are working in **cybersecurity** we face with a **complex problem**. "},{"metadata":{},"cell_type":"markdown","source":"# References\n\n+ [1] L贸pez, U., Camilo, C., Garc铆a Pe帽a, M., Osorio Quintero, J. L., & Navarro Cadavid, A. (2018). Ciberseguridad: un enfoque desde la ciencia de datos-Primera edici贸n.\n+ [2] Navarro Cadavid, A., Londo帽o, S., Urcuqui L贸pez, C. C., & Gomez, J. (2014, June). An谩lisis y caracterizaci贸n de frameworks para detecci贸n de aplicaciones maliciosas en Android. In Conference: XIV Jornada Internacional de Seguridad Inform谩tica ACIS-2014 (Vol. 14). ResearchGate.\n+ [3] Urcuqui-L贸pez, C., & Cadavid, A. N. (2016). Framework for malware analysis in Android.\n+ [4] Urcuqui,  C.,  Navarro,  A.,  Osorio,  J.,  &  Garc谋a,  M.  (2017). Machine Learning  Classifiers  to  Detect  Malicious  Websites. CEUR  Workshop Proceedings. Vol 1950, 14-17.\n+ [5] L贸pez, C. C. U., Villarreal, J. S. D., Belalcazar, A. F. P., Cadavid, A. N., & Cely, J. G. D. (2018, May). Features to Detect Android Malware. In 2018 IEEE Colombian Conference on Communications and Computing (COLCOM) (pp. 1-6). IEEE."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}