{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Country Profiling Using PCA and Clustering\n## Introduction\n\nThe data from this analysis is from [kaggle Unsupervised Learning on Country Data](https://www.kaggle.com/rohan0301/unsupervised-learning-on-country-data), which contains socio-economic and health related factors of 167 countries over the world. The goal of the project is to categorise the countries using socio-economic and health factors that determine the overall development of the country.\n\nThe variables provided in the dataset include socio-economic factors such as export, import and GDP of a country, as well as heath-related factors such as child mortality rate, life expectancy and health spend % of a country. The data is relatively well formatted and unlablled. Due to the nature of the dataset, the analysis will use a combinatino of **Unsupervised Machine Learning techniques** such as kmeans clustering and **Dimensionality Reduction techniques** such as Principal Component Analysis. We'll also perform outlier analysis and scalling the data to provide a more accurate clustering result.\n\n## Loading data and libraries\n\nThe analysis starts with loading data and necessary library.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load necessary library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\n%matplotlib inline\n\n# set default plot size\nplt.rcParams[\"figure.figsize\"] = (15,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and preview data \ncountry = pd.read_csv('../input/unsupervised-learning-on-country-data/Country-data.csv')\n\ncountry.head()\n# print(country.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nAfter the data being loaded, we can see that there are a total of 167 countries and 9 features/factors in the dataset. Using `describe()` to provide a descriptive statistics, we can see that some of the variables such as **GDP** and **income** have some extreme values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary Statistics\ncountry.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check each column for nas\ncountry.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By plotting a spirplot, we can get a better understanding of how the data are distributed. We can also see that some of the variables have strong linear or non-linear correlations between each other. To further explore them, we'll use `corr()` function to generate a correlation matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(country.drop('country',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_cor = country.drop('country',axis=1).corr()\ncountry_cor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(country_cor, dtype=np.bool))\n\n# Set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(15, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(country_cor, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heat map above shows that some of the variables are strongly correlated. For example:\n\n- income and GDP per capita\n- child mortality rate and life expectancy\n- imports and exports\n\nTo address and minimize the extreme value issues, we need to perform some kind of scalling to the dataset. One of the most commen method is the `MinMaxScaler()`. We'll store the scalled dataset to a new dataframe called country_scale_df","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale the data\nmin_max_scaler = MinMaxScaler()\ncountry_scale = min_max_scaler.fit_transform(country.drop('country',axis=1))\ncountry_scale_df = pd.DataFrame(data = country_scale,\n                               columns=country.columns[1:])\ncountry_scale_df['country'] = country['country']\ncountry_scale_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Principal Component Analysis\n\n**Pricipal Component Analysis (PCA)** is a Dimensionality Reduction technique usually used in large datasets with multiple dimensions, by transforming a large set of variables into a smaller one that still contains most of the information in the original dataset.\n\nWe start the PCA by starting an instance of it using `PCA()` and fit it with the scaled country data. The optimal number of Principal compenent are chosen by picking the minimum number of components that demonstrates the highest amount of variance.\n\nAfter plotting the the cumulative summation of the **explained variance** with the **number of Principal components**, we can see that the optimal number of Principal Components are 5. Compared with the original dataset which have 9 dimensions, PCA have reduced the dimension to 5 and still able to explain over 95% of the variance of the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# pass through the scaled data set into our PCA class object\npca = PCA().fit(country_scale)\n\n# plot the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n# define the labels & title\nplt.xlabel('Number of Components', fontsize = 15)\nplt.ylabel('Variance (%)', fontsize = 15) \nplt.title('Explained Variance', fontsize = 20)\n\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then use 5 as `n_component` parameter and save the PCA dataset into a new object called country_pca, this dataset will be used to preform the final clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will choose 5 pca components and create a new dataset\n\ncountry_pca = PCA(n_components=5).fit(country_scale).transform(country_scale)\n\n# store it in a new data frame\ncountry_pca= pd.DataFrame(data = country_pca, columns = ['principal component 1', 'principal component 2',\n                                                        'principal component 3','principal component 4',\n                                                        'principal component 5'])\n# country_pca['country'] = country['country']\n\ncountry_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_pca_cor = country_pca.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(country_pca_cor, dtype=np.bool))\n\n# Set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(15, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(country_pca_cor, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kmeans Clustering\n\nKmeans clustering is one of the most commenly used clustering algorithm due to its easy inplementation. The only parameter we need to define for Kmeans clustering is the number of K.\n\nThere are many different methods to define the optimal number of k, the most commenly used one is called the elbow method. Below is how to read the elbow plot from geeksforgeeks.com.\n![Elbow Method for defining the optimal K](https://media.geeksforgeeks.org/wp-content/uploads/20190606105550/distortion1.png)\n>To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. Thus for the given data, we conclude that the optimal number of clusters for the data is 3.\n\nThe metric for sklearn Kmeans distortion/inertia is stored in `inertia_` variable and we'd like to try multiple k values to see how the inertia changes for different k value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a dictionary that contains all of our relevant info.\nresults = []\n\n# define how many clusters we want to test up to.\nnum_of_clusters = 10\n\n# run through each instance of K\nfor k in range(2, num_of_clusters):\n    \n    print(\"-\"*100)\n    \n    # create an instance of the model, and fit the training data to it.\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(country_pca)\n    \n    \n    # store the different metrics\n#     results_dict_pca[k]['silhouette_score'] = sil_score\n#     results_dict_pca[k]['inertia'] = kmeans.inertia_\n#     results_dict_pca[k]['score'] = kmeans.score\n#     results_dict_pca[k]['model'] = kmeans\n    \n    results.append(kmeans.inertia_)\n    \n    # print the results    \n    print(\"Number of Clusters: {}\".format(k),kmeans.inertia_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we plot the Elbow Method for optimal k, we can see that the optimal k value will be 3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(2, num_of_clusters), results, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another good way to determine the optimal k is called Silhouette analysis, It can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like the number of clusters visually. This measure has a range of (-1, 1).\n\nSilhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n\nWith the help of `yellowbrick` package, we can use `SilhouetteVisualizer()` to visualized the Silhouette score for different k. What we are looking for is that each cluster exceeds the red line or the average silhouette score and that the clusters are as evenly distributed as possible. We are only going to focus on k = 2, k = 3 and k = 4 since based on the elbow method, these are the possible choices.\n\nWe can see that although for all k, each cluster are above the red line, but only when k = 3 provides a more evenly distributed cluters, so both methods indicate that we should choose k = 3 for out Kmeans clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the graph above, it indicates that we should choose k = 3\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nclusters = [2,3,4]\n\nfor cluster in clusters:\n    \n    print('-'*100)\n\n    # define the model for K\n    kmeans = KMeans(n_clusters = cluster, random_state=0)\n\n    # pass the model through the visualizer\n    visualizer = SilhouetteVisualizer(kmeans)\n\n    # fit the data\n    visualizer.fit(country_pca)\n\n    # show the chart\n    visualizer.poof()\n\n    \n# the silhouette plot also shows that the optimal k is 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last step is to apply the kmeans clustering to the data and get the lables of which cluster each country falls into.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, random_state=0).fit(country_pca)\ncountry['cluster'] = kmeans.labels_\ncountry.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print out the country in each cluster\n\n**Cluster 1**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country[country['cluster'] == 0][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cluster 2**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country[country['cluster'] == 1][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cluster 3**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country[country['cluster'] == 2][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the output of the countries in each cluster, combined with our knowledge on some of the countries. We can see that:\n\n- **Cluster 1** are those less developed countries, most of which are in Africa\n- **Cluster 2** are those developed countries, most of which are in Europe, North America and some part of Asia\n- **Cluster 3** are thoe developing countries, most of whic are in South America and Asia","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}