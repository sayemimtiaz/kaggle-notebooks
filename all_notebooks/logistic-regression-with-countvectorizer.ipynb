{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\nimport os\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\n\n# STOPWORDS = set(stopwords.words('english'))\npd.set_option(\"display.max_colwidth\", 400)\npool = multiprocessing.Pool()\n\ndef lemmatize_sentence(sentence):\n    lem_sent = []\n    wordnet_lemmatizer = WordNetLemmatizer()\n    sentence_words = nltk.word_tokenize(sentence)\n    punctuations = \"?:!.,;\"\n    for word in sentence_words:\n        if word in punctuations:\n            sentence_words.remove(word)\n\n    for word in sentence_words:\n        lem_sent.append(wordnet_lemmatizer.lemmatize(word))\n    return \" \".join(lem_sent)\n\ndef stem_sentence(sentence):\n    stem_sent=[]\n    porter = PorterStemmer()\n    sentence_words = nltk.word_tokenize(sentence)\n    for word in sentence_words:\n        stem_sent.append(porter.stem(word))\n    return \" \".join(stem_sent)\n\n\n\ndef remove_unwanted_text(text):\n    \n    if text.startswith(\"Re: \"):\n        text = text.replace(\"Re:\", \"\")\n    new_text = text.replace(\"\\n\", \". \")\n    new_text = remove_mentions(new_text)\n    new_text = new_text.lower()\n    new_text = remove_html(new_text)\n    new_text = remove_link(new_text)\n    new_text = re.sub(r'[^\\w\\s]+', ' ', new_text)\n    return new_text\n\ndef remove_link(text):\n    new_text = re.sub(r\"http\\S+\", \" \", text)\n    new_text = re.sub(r\"www.\\S+\", \" \", new_text)\n    return new_text\n\ndef remove_mentions(text):\n    new_text = re.sub(r\"@\\S+\", \" \", text)\n    new_text = re.sub(r'RT[\\s]+', \" \", new_text)\n    return new_text\n\ndef remove_html(text):\n    return re.sub(r\"<[^<>]+>\", \" \", text)\n\ndef clean_text(data):\n    pool = multiprocessing.Pool(processes=4)\n    data['clean_text'] = pool.map(remove_unwanted_text, data['text'])\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_train_df = pd.read_csv(\"/kaggle/input/generic-sentiment-multidomain-sentiment-dataset/generic_sentiment_dataset_50k.csv\")\nsentiment_test_df = pd.read_csv(\"/kaggle/input/generic-sentiment-multidomain-sentiment-dataset/generic_sentiment_dataset_10k.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_train_df = clean_text(sentiment_train_df)\nsentiment_test_df = clean_text(sentiment_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_train_df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = sentiment_train_df.text\ntest_text = sentiment_test_df.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = pd.concat([train_text, test_text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 6),\n    max_features=50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = sentiment_train_df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = LogisticRegression(C=0.1, solver='sag')\ncv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=5, scoring='roc_auc'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = LogisticRegression(C=0.1, solver='sag')\nclassifier.fit(train_features, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\npredict_test = classifier.predict(test_features)\nprint(\"f1 score: {}\".format(f1_score(predict_test, sentiment_test_df.label, average='micro')))\nprint(\"Accuracy score: {}\".format(accuracy_score(predict_test, sentiment_test_df.label)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiprocessing.Pool?\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}