{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\n* [Fares Sayah - Natural Language Processing (NLP) üßæ for Beginners](https://www.kaggle.com/faressayah/natural-language-processing-nlp-for-beginners)\n* [Ravi Chaubey - Natural Language Processing with Python](https://www.kaggle.com/ravichaubey1506/natural-language-processing-with-python)\n* [Madz2000 - Simple EDA with Data Cleaning & GloVe(98%Accuracy)](https://www.kaggle.com/madz2000/simple-eda-with-data-cleaning-glove-98-accuracy)\n* [adityapatil - Spam detector using NLP and Random Forest](https://www.kaggle.com/adityapatil673/spam-detector-using-nlp-and-random-forest)\n* [Shekhar - Spam Detection using NLP and Random Forest](https://www.kaggle.com/shekhart47/spam-detection-using-nlp-and-random-forest)"},{"metadata":{},"cell_type":"markdown","source":"# Dataset overview    \n* Êé¢Á¥¢ÂºèË≥áÊñôÂàÜÊûê (EDA)\n    * EDA (Exploratory Data Analysis) uses **visualization** and **basic statistics** to get an overview of the data we have, in order to do more complicated and thorough analysis to it.\n    * EDA should let us be able to achieve the following three main things:\n        1. To Know the Data - what information does the data provide, the structure of the data, etc.\n        2. Check the Data - if there‚Äôs any outliers or unusual value.\n        3. Correlation between Data - find out important variables.\n    * We can also check if the data meet our assumption of it and figure out latent errors before actually building the model, so as to do adjusts for the further analysis."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading a text-based dataset into pandas\ndef readData_rawSMS(filepath):\n    data_rawSMS = pd.read_csv(filepath, header=0, usecols=[0,1], encoding='latin-1')\n    data_rawSMS.columns = ['label', 'content']\n    return data_rawSMS\n\ndata_rawSMS = readData_rawSMS(os.path.join(dirname, filename))\ndata_rawSMS.head()\n\n# ÂûÉÂúæË®äÊÅØ(spam) / ÊúâÊïàË®äÊÅØ(ham)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ÂÆöÁæ© `readData_rawSMS` function\n    * header\n        * `0`: Á¨¨‰∏ÄÂàó(Ê©´)ÁÇ∫Ê¨Ñ‰ΩçÂêçÁ®±\n            > Âç≥ v1, v2\n        * `1`: Á¨¨‰∫åÂàó(Ê©´)ÁÇ∫Ê¨Ñ‰ΩçÂêçÁ®±\n            > Âç≥ ham, Go until jurong point, crazy.. Available only ...\n        * `None`: Êú¨Ë≥áÊñô(spam.csv)Ê≤íÊúâÊ¨Ñ‰ΩçÂêçÁ®±\n    * usecols\n        * `[0,1]`: ÂÉÖ‰ΩøÁî®Á¨¨‰∏ÄË°å(Áõ¥)ÂíåÁ¨¨‰∫åË°å(Áõ¥)ÁöÑË≥áÊñôÔºåÂÖ∂‰ªñË°å(Áõ¥)Áï•ÈÅé‰∏çËÆÄÂèñ‰∏î‰∏ç‰ΩøÁî®„ÄÇ\n    * data_rawSMS.columns = ['label', 'content']\n        > ÈáçÊñ∞ÂëΩÂêçÊ¨Ñ‰ΩçÂêçÁ®±ÔºöÁî± `v1, v2` ÊîπÁÇ∫ `label, content`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate descriptive statistics\ndata_rawSMS.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group large amounts of data and compute operations on these groups\ndata_rawSMS.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new column `length` to detect how long the content are.\ndata_rawSMS['length'] = data_rawSMS['content'].apply(len)\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `data_rawSMS['length'] = data_rawSMS['content'].apply(len)`\n    * ‰ΩøÁî® len ÂáΩÊï∏ Ë®àÁÆó `data_rawSMS` DataFrame ÁöÑ `content`Ôºå‰∏¶Â∞áÁµêÊûúÁµ¶‰∫àÂè¶Â¢ûÁöÑÊ¨Ñ‰Ωç `length`„ÄÇ\n* `DataFrame.apply(func, axis=0)`\n    * Apply a function along an axis of the DataFrame.\n    * Example: `df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])`\n        ```\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n        ```\n        * df.apply(np.sum, axis=0)\n            ```\n            A    12\n            B    27\n            dtype: int64\n            ```\n        * df.apply(np.sum, axis=1)\n            ```\n            0    13\n            1    13\n            2    13\n            dtype: int64\n            ```"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS.label=='ham']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(6, 4))\n\ndata_rawSMS[data_rawSMS.label=='ham'].length.plot(\n    bins=35, kind='hist', color='blue', \n    label='Ham messages', alpha=0.6)\ndata_rawSMS[data_rawSMS.label=='spam'].length.plot(\n    kind='hist', color='red', \n    label='Spam messages', alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"Message Length\")\n#Through just basic EDA we've been able to discover a trend that `spam messages` tend to have `more characters`.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nDataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, \n    use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, \n    xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, \n    yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds)\n```\n* bins: int or sequence or str, default: rcParams[\"hist.bins\"] (default: 10)\n    * ÊääÂàÜ‰Ωà(Èï∑Ê¢ù)ÂàáÊàê N Á≠âÂàÜÔºåN È†êË®≠ÁÇ∫ 10„ÄÇ\n        > Âç≥Èï∑Ê¢ùÁ∏ΩÂÖ±Êúâ 10 ÂÄã„ÄÇ\n* legend: Place legend on axis subplots\n    * ÊîæÁΩÆÂúñ‰æã"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.length.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS.label=='ham'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS['length'] == 910]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS['length'] == 910]['content'].iloc[0]\n# data_rawSMS[data_rawSMS.length == 910].content.iloc[0]     #same result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* https://ithelp.ithome.com.tw/articles/10194006\n* `pandas.DataFrame.iloc`\n    * Purely integer-location based indexing for selection by position. (Áî® index ‰ΩçÁΩÆ‰æÜÂèñÊàëÂÄëË¶ÅÁöÑË≥áÊñô)\n* `pandas.DataFrame.loc`\n    * Access a group of rows and columns by label(s) or a boolean array. (Áî® Ê®ôÁ±§ ‰æÜÂèñÂá∫Ë≥áÊñô)"},{"metadata":{},"cell_type":"markdown","source":"# Text Pre-processing\n* The simplest is the `Ë©ûË¢ãÊ®°Âûã (Bag-of-words Model, BoW)` approach, where each unique word in a text will be represented by one number.\n    ![](https://miro.medium.com/max/875/1*ujkZ3JrQ6ubSuEpepHE4Aw.png)\n    > [NLP ÂÖ•ÈñÄ (1) ‚Äî Text Classification (Sentiment Analysis) ‚Äî Ê•µÁ∞°ÊòìÊÉÖÊÑüÂàÜÈ°ûÂô® Bag of words + Naive Bayes](https://sfhsu29.medium.com/nlp-%E5%85%A5%E9%96%80-1-text-classification-sentiment-analysis-%E6%A5%B5%E7%B0%A1%E6%98%93%E6%83%85%E6%84%9F%E5%88%86%E9%A1%9E%E5%99%A8-bag-of-words-naive-bayes-e40d61de9a7f)\n    \n    * Â¶Ç‰ΩïÂà©Áî® bag-of-words ÊñπÊ≥ïÂ∞áÊñáÂ≠óËΩâÊèõÊàêÊï∏Â≠óÔºü\n        > [NLPÁöÑÂü∫Êú¨Âü∑Ë°åÊ≠•È©ü(II) ‚Äî Bag of Words Ë©ûË¢ãË™ûË®ÄÊ®°Âûã](https://medium.com/@derekliao_62575/nlp%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%9F%B7%E8%A1%8C%E6%AD%A5%E9%A9%9F-ii-bag-of-words-%E8%A9%9E%E8%A2%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-3b670a0c7009)\n        * Example: \n            * a. ÁúãÂà∞‰ªñÊàëÂ∞±‰∏çÁàΩ„ÄÇ(ÂÖàÊñ∑Ë©û) => ÁúãÂà∞/‰ªñ/Êàë/Â∞±/‰∏çÁàΩ\n            * b. ÁúãÂà∞‰ªñÊàëÂ∞±ÁÅ´Â§ß„ÄÇ(ÂÖàÊñ∑Ë©û) => ÁúãÂà∞/‰ªñ/Êàë/Â∞±/ÁÅ´Â§ß\n        * ÈÄôÂÄãË™ûÊñôÂ∫´ÁöÑË©ûË¢ãÈï∑ÈÄôÊ®£Ôºö(Â∞± ,‰ªñ , ÁúãÂà∞, Êàë , ÁÅ´Â§ß, ‰∏çÁàΩ)\n            * Notice! \n                * Ë©ûË¢ãË£°ÁöÑË©û„ÄåÊ≤íÊúâÁµïÂ∞çÁöÑÈ†ÜÂ∫èÈóú‰øÇ„ÄçÔºåÊ≠§ÁÇ∫Èö®Ê©üÊéíÊ≥ï„ÄÇ\n        * Êé•ËëóÔºåÂ¶ÇÊûú‰∏ÄÂÄãË©ûÂú®Âè•Â≠ê‰∏≠ÊúâÂá∫ÁèæÔºåÊàëÂÄëÂ∞±Âπ´‰ªñÂÅö‰∏ÄÂÄãË®òËôüÔºåÂ¶Ç‰∏ãÔºö\n            * a. Ë°®Á§∫ÊàêÔºö[1, 1, 1, 1, 0, 1] (a Âè•Ê≤íÊúâ„ÄåÁÅ´Â§ß„ÄçÔºåÊ®ôÁ§∫ÁÇ∫ 0)\n            * b. Ë°®Á§∫ÊàêÔºö[1, 1, 1, 1, 1, 0] (b Âè•Ê≤íÊúâ„Äå‰∏çÁàΩ„ÄçÔºåÊ®ôÁ§∫ÁÇ∫ 0)\n        * ÂÉèÈÄôÊ®£Áî® 1 Ë∑ü 0 ‰æÜË°®Á§∫Âè•Â≠ê‰∏≠Ë©ûË™ûÊúâÊ≤íÊúâÂá∫ÁèæÁöÑÊñπÂºèÔºåÂÆÉÊúâÂÄãÈÖ∑ÁÇ´ÂêçÂ≠óÔºö„ÄéÁç®ÁÜ±Á∑®Á¢º (One-hot encoding)„Äè\n            * Âà©Áî®Áç®ÁÜ±Á∑®Á¢ºÁöÑÊñπÂºèÔºå‰∏ÄÂÄãÂè•Â≠êÂèØ‰ª•Ë¢´ËΩâÊèõÊàê‰∏ÄÂÄãÂêëÈáèÁöÑÂΩ¢ÂºèË°®ÈÅî(ÂêëÈáè \"vector\" Â∞±ÊòØ‰∏ÄÂàóÊï∏Â≠óËÄåÂ∑≤)ÔºåÂ∞±ÂèØ‰ª•ÈÅîÊàêÁ∞°ÂñÆÁöÑÊñáÂ≠óËΩâÊèõÊàêÊï∏Â≠ó„ÄÇ\n    * BoW ÁöÑË°çÁîüÊ®°ÂûãÔºö\n        * TF-IDF\n            * Áç≤Âèñ‰∏ÄÂÄãËÉΩ‰ª£Ë°®‰∏ÄÂÄãË©ûÂú®Êñá‰ª∂‰∏≠ÈáçË¶ÅÁ®ãÂ∫¶ÁöÑÊï∏ÂÄº„ÄÇ\n        * CBoW (Continuous Bag of Words, ÈÄ£Á∫åË©ûË¢ãÊ®°Âûã)\n            * ÈÄôÂÄãÊ®°ÂûãÊòØ‰∏ÄÂÄãÊ∑∫Â±§ÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇ\n            * Áõ∏ËºÉÊñºÂÇ≥Áµ±Ë©ûË¢ãÊ®°ÂûãÔºåCBoW ÁöÑËº∏ÂÖ•‰∏ÄÊ®£ÊòØÁç®ÁÜ±ÁöÑÂΩ¢ÂºèÔºå‰ΩÜ‰∏çÂêåÁöÑÈªûÊòØÔºåCBoW Ê®°ÂûãÊúÉÂ∞á‰∏ÄÈñãÂßãÊØèÂÄãË©ûÈÉΩÈÄèÈÅé‰∏≠ÈñìÁöÑÈö±ËóèÂ±§‰ΩúËΩâÊèõÔºåËÆìÊØèÂÄãË©ûÁöÑË©ûÂêëÈáè‰∏≠‰∏çÂÜçÂè™ÂåÖÂê´0Ëàá1ÔºåËÄåÊòØÊúâÊÑèÁæ©ÁöÑÊï∏ÂÄº„ÄÇ\n                * ‰∏≠ÈñìÁöÑÈö±ËóèÂ±§ËΩâÊèõÊòØÊÄéÈ∫ºÈÄ≤Ë°åÁöÑÔºü\n                    * CBoW ÊúÉÂêåÊôÇÂèÉËÄÉ**‰∏ÄÂÄãË©ûÂâçÂæåÁöÑË™ûÂ¢É**‰æÜÊ±∫ÂÆöÈÇ£ÂÄãË©ûÊâÄ‰ª£Ë°®ÁöÑË©ûÂêëÈáèÊòØ‰ªÄÈ∫º„ÄÇ\n                        > Ex:„Äå‰∏çÁàΩ„ÄçÂíå„ÄåÁÅ´Â§ß„ÄçÂâçÈù¢ÊâÄÊé•ÁöÑË©ûÈÉΩÊòØ„ÄåÁúãÂà∞„Äç„ÄÅ„Äå‰ªñ„Äç„ÄÅ„ÄåÊàë„Äç„ÄÅ„ÄåÂ∞±„Äç\n                        > Âõ†Ê≠§Ê®°ÂûãÂ∞±ÊúÉÂà§Êñ∑Ôºö„Äå‰∏çÁàΩ„ÄçÂíå„ÄåÁÅ´Â§ß„ÄçÂèØËÉΩË°®ÁèæÂá∫Áõ∏‰ººÁöÑË™ûÊÑèÂíåÂè•Ê≥ïÁµêÊßãÔºåÈÄôÂÄãÂÖ©ÂÄãË©ûÂ∞±ÊúÉË¢´Ë≥¶‰∫àÈùûÂ∏∏Êé•ËøëÁöÑË©ûÂêëÈáè„ÄÇ\n        * Word2vec\n            * Áî±„ÄåÈÄ£Á∫åË©ûË¢ãÊ®°Âûã CBoW„ÄçÂíå„ÄåË∑≥Â≠óÊ®°Âûã skip-gram„ÄçÊßãÊàê word2vec Ê®°Âûã„ÄÇ\n            * Áî± Google ÁöÑ Mikolov Á≠â‰∫∫Âú® 2013 Âπ¥ÊèêÂá∫„ÄÇ"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nprint(stopwords.words('english'))\n\nimport string\n\nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', '√º', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    \n    # Check characters to see if they are in punctuation\n    # ÈùûÊ®ôÈªûÁ¨¶ËôüÁöÑ charactersÔºåÂ∞±Â≠òÈÄ≤ list\n    nopunc = [char for char in mess if char not in string.punctuation] \n    \n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    # ÈùûÂÅúÁî®Â≠óÁöÑ wordÔºåËΩâÊàêÂ∞èÂØ´Â≠óÊØçÂæåÂ∞±Â≠òÈÄ≤ list\n    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in STOPWORDS]) \n\ndata_rawSMS['clean_msg'] = data_rawSMS.content.apply(text_process)\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigram Analysis\nfrom collections import Counter\n\ndef get_words(content):\n    words = []\n    for row in content:\n        for j in row.split():\n            words.append(j.strip())\n    return words\n\ncounter = Counter(get_words(data_rawSMS['clean_msg']))\nmost_common = dict(counter.most_common(20))\nprint(most_common)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigram Analysis\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.barplot(x=list(most_common.values()), y=list(most_common.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='ham'].clean_msg.apply(lambda x: [word for word in x.split()])\nham_words = Counter()\n\nfor msg in words:\n    ham_words.update(msg)\n    \nprint(ham_words.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='spam'].clean_msg.apply(lambda x: [word for word in x.split()])\nspam_words = Counter()\n\nfor msg in words:\n    spam_words.update(msg)\n    \nprint(spam_words.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Example] Vectorization\n* We'll do that in 3 steps using the `bag-of-words model`:\n  1. Count how many times does a word occur in each message (**term frequency**)\n  2. Weigh the counts, so that frequent tokens get lower weight (**inverse document frequency**)\n  3. Normalize the vectors to unit length, to abstract from the original text length (**L2 norm**)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use `CountVectorizer` to \"convert text into a matrix of token counts\"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n# The raw data, a sequence of symbols cannot be fed directly to the machine learning algorithms.\n# They require numerical feature vectors with a fixed size.\nsimple_train = ['call you tonight call', 'Call me a cab', 'Please call me... PLEASE!']\n\n# 1. import and instantiate CountVectorizer (with the default parameters)\nvect = CountVectorizer()\n\n# 2. learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)\n\n# 3. examine the fitted vocabulary\nprint(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nprint(simple_train_dtm)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> 'call you tonight'\n->   0    5     4\n\n(docID, wordID)  word count\n  (0, 1)\t1\n  (0, 4)\t1\n  (0, 5)\t1\n  \nindex  0 1 2 3 4 5\n====> [0 1 0 0 1 1]\n\"\"\"\nprint()\nprint(simple_train_dtm.toarray())\n\n# 5. examine the vocabulary and document-term matrix together\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. example text for model testing\nsimple_test = [\"please don't call me\"]\n\n# 7. transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test_dtm = vect.transform(simple_test)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> \"please don't call me\"\n->     3    x     1   2\nvector -> [[0, 1, 1, 1, 0 , 0]]\n\"\"\"\n\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning workflow with Vectorization\n## 1. Divided into training set and testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert label to a numerical variable\ndata_rawSMS['label_num'] = data_rawSMS.label.map({'ham':0, 'spam':1})\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0, train_size=0.8)\n\nprint('„ÄêTraining set„Äë')\nprint('Row_count: {}\\n\\nData content:\\n{}\\n'.format(X_train.shape, X_train))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_train.shape, y_train))\nprint('------------')\nprint('„ÄêTesting set„Äë')\nprint('Row_count: {}\\n\\nData:\\n{}\\n'.format(X_test.shape, X_test))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_test.shape, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `train_test_split()`\n    1. `random_state`\n        * This ensures that if I have to rerun my code, I‚Äôll get the exact same train-test split, so my results won‚Äôt change.\n    2. `stratify=y`\n        * This tells train_test_split to make sure that the training and test datasets contain examples of each class **in the same proportions as in the original dataset**. \n        * Áî±Êñº classes ÁöÑ‰∏çÂπ≥Ë°°ÊÄß(imbalanced)ÔºåÂõ†Ê≠§ÁâπÂà•ÈáçË¶ÅÔºÅ\n        * Ëã•ÂÆåÂÖ®Èö®Ê©üÂú∞ÊãÜÊàê Train Âíå TestÔºåÂÆπÊòìÈÄ†ÊàêÊüêÂÄãÂ∞èÈ°ûÂà•Âú® Test Êúâ„ÄÅ‰ΩÜÂú® Train Ê≤íÊúâÁöÑÊÉÖÊ≥ÅÁôºÁîüÔºå‰ΩøÂæó model ÁÑ°Ê≥ïËæ®Ë≠òÈÇ£ÂÄãÂ∞èÈ°ûÂà•(Âõ†ÁÇ∫ Train Ê≤íÊúâÔºåÊâÄ‰ª•Ê≤íËæ¶Ê≥ïÂ≠∏Áøí)ÔºÅ\n            > A random split could easily end up with all examples of the smallest class in the test set and none in the training set, and then the model would be unable to identify that class."},{"metadata":{},"cell_type":"markdown","source":"## 2. Vectorization (+ Feature Engineering)\n### Training data\n* CountVectorizer\n* TfidfTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Method1 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nvect.fit(X_train)\n# print(len(vect.get_feature_names()))\n# print(vect.get_feature_names())\n\n\n# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n\n# word's TF\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Method2 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\n# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:,.10f}\".format\n\n# word's TF-IDF weight\nTFIDF = tfidf_transformer.transform(X_train_dtm).toarray()\npd.DataFrame(TFIDF, columns=vect.get_feature_names()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()) #.iloc[4170]     #TF-IDF (L2 norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF Explain"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()) #['√¨√Ø'].iloc[4170]\ndf.loc[df['√¨√Ø'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TF = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['√¨√Ø'].iloc[4170]\nTF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['√¨√Ø'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:,.4f}\".format\n\n# word's IDF weight\npd.DataFrame([tfidf_transformer.idf_], columns=vect.get_feature_names()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(vect.get_feature_names())):\n    if '√¨√Ø' == vect.get_feature_names()[i]:\n        print(i)\n        # '√¨√Ø' => index 8147","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()).iloc[4170]\n\n\n\n### TF-IDF (L2 norm) ###\n# √¨√Ø             0.4197\n\n### TF ###\n# \"√¨√Ø\" Âú®Á¨¨ 4170 ÁØáÔºåÂÉÖÂá∫Áèæ‰∏ÄÊ¨°„ÄÇ\n\n### IDF ###\n# import math\n# TF = 1\n# DF = 36\n# total_Doc = 4179\n# math.log((4179 + 1)/(36 + 1)) + 1 = 5.727148612874577","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing data\n* TfidfTransformer\n    * transform testing data (using fitted vocabulary) into a document-term matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# --- start --- Use Method2 --- #\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)\n# --- end --- Use Method2 --- #\n\n\nX_test_dtm = vect.transform(X_test)\nprint(X_test_dtm.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Building and Evaluating a model (‰æùÊìöÁâπÂæµË≥áÊñôË®ìÁ∑¥ÂàÜÈ°ûÂô®)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=20, random_state=0)\n\n# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time clf = rf.fit(X_train_dtm, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# Extract single tree\nestimator = rf.estimators_[5]\n\nn_nodes = rf.estimators_[4].tree_.node_count\nprint(estimator, n_nodes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf = RandomForestClassifier(n_estimators=20, oob_score=True, random_state=0)\n\n# # train the model using X_train_dtm (timing it with an IPython \"magic command\")\n# %time clf = rf.fit(X_train_dtm, y_train)\n\n# ### OOB ###\n# print(rf.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf.feature_importances_.shape\n# rf.feature_importances_\n\n\ndf = pd.DataFrame([rf.feature_importances_], columns=vect.get_feature_names())\ndf.nlargest(20, '√¨√Ø')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate (Testing)\n* accuracy_score\n* precision_score\n* recall_score\n* f1_score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"„Äê Testing „Äë\")\nprint('Accuracy score: {}'.format(accuracy_score(y_test, y_predTest_class)))\nprint('Precision score: {}'.format(precision_score(y_test, y_predTest_class)))\nprint('Recall score: {}'.format(recall_score(y_test, y_predTest_class)))\nprint('F1 score: {}'.format(f1_score(y_test, y_predTest_class)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Ë£úÂÖÖÔºöcalculate AUC (Ê©üÂô®Â≠∏ÁøíÁöÑÊïàËÉΩË°°ÈáèÊåáÊ®ô) ###\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, clf.predict_proba(X_test_dtm)[:,1])\n\n\n# Áï∂AUC = 1ÊôÇÔºå‰ª£Ë°®ÂàÜÈ°ûÂô®ÈùûÂ∏∏ÂÆåÁæéÔºå‰ΩÜÈÄôÁï¢Á´üÊòØÁêÜÊÉ≥ÁãÄÊ≥Å„ÄÇ\n# Áï∂AUC > 0.5ÊôÇÔºå‰ª£Ë°®ÂàÜÈ°ûÂô®ÂàÜÈ°ûÊïàÊûúÂÑ™ÊñºÈö®Ê©üÁåúÊ∏¨ÔºåÊ®°ÂûãÊúâÈ†êÊ∏¨ÂÉπÂÄº„ÄÇ\n# https://ithelp.ithome.com.tw/articles/10229049\n# https://medium.com/marketingdatascience/%E5%88%86%E9%A1%9E%E5%99%A8%E8%A9%95%E4%BC%B0%E6%96%B9%E6%B3%95-roc%E6%9B%B2%E7%B7%9A-auc-accuracy-pr%E6%9B%B2%E7%B7%9A-d3a39977022c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, y_predTest_class))\nprint()\nprint(confusion_matrix(y_test, y_predTest_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_predTest_class), index=['Ham','Spam'] , columns=['Ham','Spam'])\nplt.figure(figsize=(3,3))\nsns.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=['Ham','Spam'], yticklabels=['Ham','Spam'])\n\n# x Ëª∏ÔºöÈ†êÊ∏¨(Predict)\n# y Ëª∏ÔºöÂØ¶Èöõ(Actual)\n\n### Type I error  (Âö¥Èáç) ###    Predict: Ham (0) & Actual: Spam (1)\nprint(len(X_test[y_predTest_class < y_test]), 'ÂÄã\\t=> false negatives (spam incorrectly classifier)\\n') # X_test[y_predTest_class < y_test]\n\n### Type II error  (ËºïÂæÆ) ###   Predict: Spam (1) & Actual: Ham (0)\nprint(len(X_test[y_predTest_class > y_test]), 'ÂÄã\\t=> false positives (ham incorrectly classifier)\\n') # X_test[y_predTest_class > y_test]\nprint(X_test[(y_predTest_class==1) & (y_test==0)]) # same result\nprint()\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=0, average='binary')\nprint('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()/len(y_test),3)))\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=1, average='binary')\nprint('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()/len(y_test),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Input SMS and Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# vect = CountVectorizer()\n# X_train_dtm = vect.fit_transform(X_train)\n\n# tfidf_transformer = TfidfTransformer()\n# tfidf_transformer.fit(X_train_dtm)\n# tfidf_transformer.transform(X_train_dtm)\n    \n# rf = RandomForestClassifier() #n_estimators=20\n# rf.fit(X_train_dtm, y_train)\n\nSMS = 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.'\nclean_text = text_process(SMS)\n# print(clean_text)\nsimple_test_dtm = vect.transform([clean_text])\nprint(simple_test_dtm.toarray(), simple_test_dtm.reshape(1,-1).shape)\n\ny_predSimpleTest_class = rf.predict(simple_test_dtm.reshape(1,-1))\nif int(y_predSimpleTest_class) == 1:\n    print ('SPAM: {}'.format(SMS))\nelse:\n    print ('ham: {}'.format(SMS))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Pipeline (Ë£úÂÖÖ) ###\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics\n\nX = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n\npipe = Pipeline([('bow', CountVectorizer()),\n                 ('tfid', TfidfTransformer()),  \n                 ('model', RandomForestClassifier(n_estimators=20, bootstrap=True, oob_score=False, random_state=1))])\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)\n\ny_pred = pipe.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"„Äê Testing „Äë\")\nprint('Accuracy score: {}'.format(metrics.accuracy_score(y_test, y_pred)))\nprint('Precision score: {}'.format(metrics.precision_score(y_test, y_pred)))\nprint('Recall score: {}'.format(metrics.recall_score(y_test, y_pred)))\nprint('F1 score: {}'.format(metrics.f1_score(y_test, y_pred)))\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}