{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom urllib.request import urlopen\nimport json\n%matplotlib inline \n\nlst_col = ['#B1D784','#2E8486','#004379','#032B52','#EAEA8A']\n\n# Map Tokens\nf= open(\"mapbox_token\",\"w+\")\nf.write('pk.eyJ1Ijoic2h0cmF1c3NhcnQiLCJhIjoiY2tqcDU2dW56MDVkNjJ6angydDF3NXVvbyJ9.nx2c5XzUH9MwIv4KcWVGLA')\nf.close()\nmap_token = 'pk.eyJ1Ijoic2h0cmF1c3NhcnQiLCJhIjoiY2tqcDU2dW56MDVkNjJ6angydDF3NXVvbyJ9.nx2c5XzUH9MwIv4KcWVGLA'\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nplot_choropleth = True\nplot_hex = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>GEOSPATIAL DATA VISUALISATION</b>\n\n## <b>1 | <span style='color:#023e8a'> INTRODUCTION</span></b>\n\n## <b><span style='color:#2779d2'>NOTEBOOK AIM</span></b>\n- The aim of this notebook is to provide a brief overview on what type of geospatial library tools we can use to visualise & analyse map geospatial data.\n- As the title suggests, the notebook is aimed at exploring Australian based maps & visualisation data, subsequent data sources for map geometry & visualisation data is specified.\n\n## <b><span style='color:#2779d2'>MAP ANALYSES TOOLS</span></b>\nThere are a number of useful geospatial tools we can use to visualise & analyse data:\n\n#### **EXPLORED TOOLS INCLUDE:**\n- <code>Choropleth Maps</code> ( uniqueness lies in the requirement of __boundary data__ information ) Requires Boundary Locations (unique boundary identifier)\n- <code>Hexbin Maps</code> ( uniqueness lies in its __geospatial point counting__ within the viscinity of __hexagonal shapes__ ), Requires Point Locations (long,lat)\n- <code>Cluster Maps</code> ( uniqueness lies in its ability to display precise point location data) Requires Point Locations (long,lat)\n- <code>Density Heatmaps</code> ( uniqueness lies in its ability to provide point density approximations ) Requires Point Locations (long,lat)\n\nChoropleth Maps | Hexbin Maps | Cluster Maps | Density Heatmaps\n- | - | - | -\n![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/dej86r3-f0c76864-53c0-4da7-ba0b-c41a71ad460f.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVqODZyMy1mMGM3Njg2NC01M2MwLTRkYTctYmEwYi1jNDFhNzFhZDQ2MGYucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.55OByAkrKHLcuv_lwB-Otbd-5Aox5u2-xb-mIEiGMyk) | ![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/dej86sf-fa3028c9-d463-4e94-a38a-dd2e87513a7f.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVqODZzZi1mYTMwMjhjOS1kNDYzLTRlOTQtYTM4YS1kZDJlODc1MTNhN2YucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.lgxdQcdWgZFG3-xIsmTa71NmLm-UP8baiVK-GDOTKsw) | ![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/dej86to-9784af09-d1b2-4e8b-b744-7bf3312a82c2.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVqODZ0by05Nzg0YWYwOS1kMWIyLTRlOGItYjc0NC03YmYzMzEyYTgyYzIucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.JMl1DGdlEO05YwkffPRH9roFM2pNjbi6J911po4UBRI) | ![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/dej86vb-05cebb97-6d62-45f3-82be-dcf1b7106e62.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVqODZ2Yi0wNWNlYmI5Ny02ZDYyLTQ1ZjMtODJiZS1kY2YxYjcxMDZlNjIucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.CAzEC3tIfKWEP2gq3Hv5hnJo2_Q2ybk8DRjezR8Gwd0)\n\n#### **INTERPOLATION METHODS:**\n\n- We will also look at an <code>interpolation method</code> commonly used for geospatial analysis called __kriging__, it is useful to interpolate/model values which we don't have in an attempt to estimate the value. \n- <code>Interpolation methods</code> and <code>Machine Learning Models</code> are pretty much the exact same thing, the goal is to model a value which we do not have, the only major difference between the two being that interpolation methods must use coordinate locations.","metadata":{}},{"cell_type":"markdown","source":"## <b>2 |<span style='color:#023e8a'> CHOROPLETH MAPS</span></b>\n#### **SNIPPLET FROM: [WIKIPEDIA](https://en.wikipedia.org/wiki/Choropleth_map)**\n\n> A choropleth map is a type of thematic map in which a set of pre-defined areas is colored or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population density or per-capita income.\n> \n> *Choropleth maps* **provide an easy way to visualize how a variable varies across a geographic area** or show the level of variability within a region. A heat map or isarithmic map is similar but uses regions drawn according to the pattern of the variable, **rather than the a priori geographic areas of choropleth maps**. The Choropleth is likely the most common type of thematic map because published statistical data (from government or other sources) is generally aggregated into well-known geographic units, such as countries, states, provinces, and counties, and thus they are relatively easy to create using GIS, spreadsheets, or other software tools.\n\n- As described in the article, these maps are quite useful when we are interested in plotting some variable data in a __particular geographic area__, separated by some kind of boundary.\n- Usually countries have some predefined internal regions; a common internal region data type for Australia is <code>state</code>, similar to the US. \n- To make a <code>choropleth</code> map we need two pieces of data; <b>geographic segment/boundary data</b> & some <b>plot value</b> data.\n- We will look at two particular libraries that are very useful to plot <code>choropeth</code> data; <b>Plotly</b> & <b>GeoPandas</b>.\n\n","metadata":{}},{"cell_type":"code","source":"import geopandas as gpd\nimport geoplot\n\nboroughs = gpd.read_file(\n    geoplot.datasets.get_path('nyc_boroughs')\n)\ncollisions = gpd.read_file(\n    geoplot.datasets.get_path('nyc_injurious_collisions')\n)\n\nax = geoplot.kdeplot(\n    collisions.head(100), clip=boroughs.geometry,\n    shade=True, cmap='Reds',\n    projection=geoplot.crs.AlbersEqualArea())\n# geoplot.polyplot(boroughs, ax=ax, zorder=1)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><span style='color:#2779d2'>STATIC & INTERATIVE MAPS</span></b>\n\nDepending on what is intended to be shown in the geospatial map, we should think about which type of map best is best for the job.\n\n#### **STATIC GEOSPATIAL CHOROPLETH MAPS**\n\n- We have the option to create __static geospacial maps__ using for example <code>Geopandas</code>,<code>Matplotlib</code>. We can also make them in <code>plotly</code>, which is why plotly is pretty much a one goto library for such plots. \n- Such maps are usually more than sufficient for maps with small number of geometry segments/boundaries (such as state boundaries). \n- Not too long ago I wrote some example code showing how it can be done in the notebook: [House Price Prediction using Bayesian Regression](https://www.kaggle.com/shtrausslearning/house-price-prediction-using-bayesian-regression). As long as the data tells the viewer what was intended, static maps are more than sufficient to get the point accross, so we'll look at them as well.\n- Static geospatial plots are quite useful in the situation when we want to plot multiple subplots & want to outline key differences between the maps. An example will be shown further below, in which we look at the difference between Male & Female demographic for a specific statistic.\n\n#### **INTERACTIVE GEOSPATIAL CHOROPLETH MAPS**\n\n- We have the option to create __interactive geospacial maps__, From an <b>Exploratory Data Analysis (EDA)</b> point of view, it certainly makes a lot of sense to use interactive maps over static ones, such as the case in the example below:\n- <code>plotly</code> is especially useful due to the integrated ability to use __Map Box__, which allows geospatial names to be displayed on top of choropeth plots, we will look at this later.","metadata":{}},{"cell_type":"markdown","source":"## <b><span style='color:#2779d2'>EXAMPLE 1 | UNEMPLOYMENT RATE IN VICTORIA FOR SPECIFIC M/F DEMOGRAPHI</span></b>\n\n#### **BRIEF OVERVIEW**\n\n__We should probably have a goal in mind:__\n\n- We are interested in the __unemployment statistics__ of a specific demographic (Both Male & Female) in the state of __Victoria__ on a __Local Government Area__ level. \n- The user wants the ability to explore the exact unemployment value for their own purposes (if possible) and understand any overall differences between male and female unemployment.\n\n__Main issue arising with Australian maps:__\n\n- Unfortunately Australian internal boundary maps are not integrated into __Plotly__ or __Geopandas__, so we be requried to look for this __boundary segment__ data & combine it with our visualisation data we wish to display. \n- Fortunatelly its not hard to use __pandas'__ join command. Both our downloaded __boundary data__ & __visualisation data__ should simply have an index corresponding to the unique boundary identifier, its not really relevant what it is, it could be names (sometimes you'd have to clean the name column a little), or more often it is a __unique code__.\n\n#### **WHAT WE WANT TO DISPLAY**\n\nWe probably should know what type of geospatial boundaries we are interested in:\n\n- We might be interested in __State Bondaries__ (eg. Victoria...) ([State Boundaries AUG2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-ee6c0f18-3f4b-4275-932e-0b6e434e316f/details?q=))\n- We might be interested in __Local Government Boundaries__ (eg. Boroondara ... ) ([Local Government Areas NOV2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=))\n- We might be interested in __Suburb Boundaries__ (eg. Carlton ... ) ([All State Suburbs (November 2020)](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-2d59ddfc-1c0f-41a3-8de8-06fa5d11e72f/?q=) )\n- Others might include Electoral Boundaries, or you might even have your own.\n\n#### **MAIN SOURCES OF DATA**\n\n- [Data.gov](https://www.data.gov.au) , has a very wide range of Geographical datasets; including [Geoscape Administrative Boundaries](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/details?q=), in this example we are interested in __LGA__ boundaries, which we can find in the above link or a direct link [here](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=) ( Just note that the data is already split by state )\n- [Australian Bureau of Statistics](https://www.abs.gov.au), which contains two <code>.shp</code> files (for Local Government & State Electoral Divisions), found specifically [here](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003June%202020?OpenDocument)\n- In this example, we will use __Local Government Areas ASGS Ed 2020 Digital Boundaries in ESRI Shapefile Format__ from __ABS.gov__ & select our specific state (Victoria)\n\n#### **BOUNDARY FORMAT**\n\n- I think it's much more straightforward to use <code>.shp</code> & simply convert them with <code>geopandas</code>, purely because <b>GeoPandas</b> is very straightforward to use and visualise the geopandas dataframe, like normal pandas dataframes.\n- <code>json</code> files on the other hand, may need a little bit of tweaking to get the read format correct, especially if you are using <code>.read_json</code> from the Pandas library in order to understand what your data contains.\n- The above mentioned sources all contain <code>.shp</code> formats, which is an indicator that it is quite popular, neverthelesss __json__ variants are also available, mainly __Data.gov__\n- Let's load & read the <code>.shp</code> file, noting that all the other files that come with the <code>.shp</code> are needed. The current file contains __LGA__ of all states (as it might be obvious by the <code>STE_NAME16</code>, so we need to select only a subset of the data.","metadata":{}},{"cell_type":"code","source":"import geopandas as gpd\n\n# Read SHP file\nlga_gdf = gpd.read_file('/kaggle/input/LGA2020AUST/LGA_2020_AUST.shp')    # Load the data using Geopandas\nlga_gdf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lga_gdf.STE_NAME16.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lga_gdf = lga_gdf[lga_gdf['STE_NAME16']=='Queensland']         # Let's look at only VIC as was intended \nlga_gdf['LGA_CODE20'] = lga_gdf['LGA_CODE20'].astype('str')  # Geo & Vis data will join on this axis (need consistent dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can note that for each boundary segment, <code>LGA_CODE20</code>, we have a unique <code>geometry</code>, which outlines the specific region of interest on a map using GPS coordinates.\n- As for the boundary code identifier (eg. <code>LGA_CODE16</code>,<code>LGA_CODE20</code>,...), they often tend to be updated and hense their numbers may change, but most already assigned regions will not change even if you don't use the exact same identifier for your boundary & visualisation data.\n\n#### **GEOPANDAS FORMAT COMPATIBILITY & FORMAT CONVERSION**\n\n- As shown in [this reference](https://geopandas.org/io.html), GeoPandas can utilise both, <code>.shp</code> & <code>.geojson</code>formats, so it is a very useful library for not only geographical plotting, but also data conversion.\n- __Plotly interactive maps__ require __.json__ format, so having loaded a __.shp__ file, we need to convert, which is quite simple with __geopandas__.\n- If you have a __.json__ file already ready, you could simply load that in as well via the geopandas loaded, in the same way the __.shp__ file was loaded.\n- Before we convert the combined geodataframe, we need to make sure our visualisation data is indexed by the unique identifier. \n\n#### **PREPARING DATA FOR VISUALISATION**\n\nThe user wants to know multiple are demographic statistics:\n- We are interested in the __uneployment percentage__ of __females__ and __males__ in the __age group: 20-24__ years of age & we are interested in the data on a LGA level in __Victoria__, so we need to make sure that the data source contains __this identification (LGA)__.\n- We also want to __compare the statistics for females with males as well__, so we will need to make separe plots. \n- We need to obtain the base subset data, which is contained within overall statistics data & create a specific feature __unemployment percentage__. \n- Our data is extractable from the <b>Census 2016, G43 Labour force status by age by sex (LGA)</b>, available on [stat.data](http://stat.data.abs.gov.au/) with a bit of __Data Wrangling__","metadata":{}},{"cell_type":"code","source":"# Loading the visualisation dataframe\ndf = pd.read_csv('/kaggle/input/ABSC16/ABS_C16_G43_LGA_09012021123023131.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have a column specifically used for region identification <b>LGA</b> identification (similar to the one we saw above but despite them being different numbers and slightly differently labeled <code>LGA_2016</code> & <code>LGA_CODE20</code> are compatible, with perhaps a few exceptions), which we'll need in order to group our data with the GeoPandas dataframe.\n- The column <b>Labour force status</b> tells us the various <b>demographic</b> groups are contained in the dataframe. The dataframe also contains data for different age groups & different genders, which is exactly what we need, we simply need to select a subset of this data & pivot the table.","metadata":{}},{"cell_type":"code","source":"df['Labour force status'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Age'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sex'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define the subset DataFrame that interests us & which we want to plot, we need to compare both Male & Females, so let's select both subsets.","metadata":{}},{"cell_type":"code","source":"emp_df = df[(df['Age'] == '20 - 24') & (df['Sex'] == 'Females')]\nemp_df2 = df[(df['Age'] == '20 - 24') & (df['Sex'] == 'Males')]\nemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data we need is contained within the the two columns; <code>Labour force status</code> & <code>Value</code>","metadata":{}},{"cell_type":"code","source":"emp_df = emp_df[['LGA_2016','Labour force status', 'Region', 'Value']]\nemp_df2 = emp_df2[['LGA_2016','Labour force status', 'Region', 'Value']]\nemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **SOME DATA REARRANGEMENT**\nWe can use the useful Pandas [Pivot Function](https://pandas-docs.github.io/pandas-docs-travis/user_guide/reshaping.html) to <b>restructure</b> the DataFrame:\n- Defining the <b>index as LGA_2016</b> (unique geographic id here corresponds to <b>Local Government Areas</b>; eg. <b>Boroondara</b>).\n- Columns being the shown value_counts() unique data shown above.\n- And the corresponding value for that combination.","metadata":{}},{"cell_type":"code","source":"emp_df['LGA_2016'] = emp_df['LGA_2016'].astype('str') \nemp_df2['LGA_2016'] = emp_df2['LGA_2016'].astype('str') \nemp_df = emp_df.pivot(index='LGA_2016', columns='Labour force status', values='Value').reset_index().rename_axis(None, axis=1)\nemp_df2 = emp_df2.pivot(index='LGA_2016', columns='Labour force status', values='Value').reset_index().rename_axis(None, axis=1)\nemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now that we have our pivoted column data, we can define additional variables that interest us ( & we wish to show on the map)\n- Let's look at the <b>percentage unemployed</b> in for our current demographic in all <b>Local Government Areas</b> of interest.","metadata":{}},{"cell_type":"code","source":"emp_df['Percent_Unemployed'] = emp_df['Total Unemployed']/(emp_df['Total Unemployed']+emp_df['Total Employed']) \nemp_df2['Percent_Unemployed'] = emp_df2['Total Unemployed']/(emp_df2['Total Unemployed']+emp_df2['Total Employed']) \nemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge dataframes\ndf_merged0 = pd.merge(lga_gdf[['LGA_CODE20', 'geometry', 'LGA_NAME20']], \n                      emp_df[['LGA_2016', 'Percent_Unemployed']], \n                      left_on='LGA_CODE20', right_on='LGA_2016', how='outer')\ndf_merged2 = pd.merge(lga_gdf[['LGA_CODE20', 'geometry', 'LGA_NAME20']], \n                      emp_df2[['LGA_2016', 'Percent_Unemployed']], \n                      left_on='LGA_CODE20', right_on='LGA_2016', how='outer')\ndf_merged0 = df_merged0.dropna(subset=['Percent_Unemployed', 'LGA_CODE20', 'geometry'])\ndf_merged2 = df_merged2.dropna(subset=['Percent_Unemployed', 'LGA_CODE20', 'geometry'])\ndf_merged0.index = df_merged0.LGA_CODE20\ndf_merged2.index = df_merged2.LGA_CODE20\ndf_merged0.head()\n\nprint(type(df_merged0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **USING GEOPANDAS TO CONVERT FORMAT**\n- We'll use <b>Geopandas</b> to convert the DataFrame containing <b>geometry</b> column for each <b>index</b> (here the LGA) into the format Plotly uses; <b>GEOJSON</b>\n- Plotly requires <b>GeoJSON</b> as stated above & Geopandas can help us with the conversion.","metadata":{}},{"cell_type":"code","source":"# Convert geometry to GeoJSON\ndf_merged0 = df_merged0.to_crs(epsg=4327)\ndf_merged2 = df_merged2.to_crs(epsg=4327)\nlga_json = df_merged0.__geo_interface__\nlga_json2 = df_merged2.__geo_interface__\n\n# The format is idenical to the previous examle visualisation\n# list(lga_json.keys())[1] # Get the first key\n# lga_json['features'][0] # print first key value only","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **INTERACTIVE CHOROPLETH MAPS**\n- Victoria contains a variety of boundary segment sizes, this makes it difficult to visualise smaller region values.\n- Interactive maps allow us to explore the region and outline any notable key difference.\n- Let's plot the __Female Demographic in the Plotly Go__ plot & __Male Demographic in the Plotly Express__ plot.\n\n#### **INTERACTIVE MAP: PLOTLY GO w/ MAPBOX ACCESS TOKEN**\n- We can use <b>Plotly Go</b> with or/without a MapBox Access token.\n- MapBox tokens are useful to __outline key area names__ on top of the choropeth map plot, we can do this by modifying the <b>layout</b>.\n- Let's plot the female demographic on the plot with the __mapbox layout__.","metadata":{}},{"cell_type":"code","source":"if(plot_choropleth):\n\n    # define colourbar bounds\n    zmin = df_merged0['Percent_Unemployed'].min(); zmax = df_merged0['Percent_Unemployed'].max()\n\n    # Set the data for the map\n    data = go.Choroplethmapbox(\n            geojson = lga_json,             #this is your GeoJSON\n            locations = df_merged0.index,    #the index of this dataframe should align with the 'id' element in your geojson\n            z = df_merged0['Percent_Unemployed'], #sets the color value\n            text = df_merged0.LGA_NAME20,    #sets text for each shape\n            colorbar=dict(thickness=20, ticklen=3, tickformat='%',outlinewidth=0), #adjusts the format of the colorbar\n            marker_line_width=1, marker_opacity=0.8, colorscale=\"viridis\", #adjust format of the plot\n            zmin=zmin, zmax=zmax,           #sets min and max of the colourbar\n            hovertemplate = \"<b>%{text}</b><br>\" +\n                        \"%{z:.0%}<br>\" +\n                        \"<extra></extra>\")  # sets the format of the text shown when you hover over each shape\n\n    # Set the layout for the map\n    layout = go.Layout(\n        title = {'text': f\"<b>Queensland | % Unemployed Females | [20-24]</b>\",\n                'font': {'size':20}},       \n        mapbox1 = dict(\n            center = dict(lat= -20 , lon=145),zoom = 4,\n            accesstoken = map_token),                      \n        autosize=True,\n        height=600)\n\n    # Generate the map\n    fig=go.Figure(data=data, layout=layout)\n    fig.update_layout(margin=dict(l=30, r=30, t=70, b=30))\n    fig.update_layout(font=dict(family='sans-serif',size=14));fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **INTERACTIVE MAP: PLOTLY EXPRESS W/O MAPBOX ACCESS TOKEN**\n- We can use plotly express without having to use a <b>MapBox Access Token</b> (if you don't need the names to go on top), this may make the map let useful from an EDA point of view, nevertheless we can always refer to the hover data to get the names.\n- Let's plot the male demographic using the plotly express version. Plotly express versions can also be used with __mapbox__ as shown later using the <code>fig.update_layout()</code> option.","metadata":{}},{"cell_type":"code","source":"if(plot_choropleth):\n    tmp = df_merged2.copy()\n    tmp['Percent_Unemployed'] = tmp['Percent_Unemployed']*100\n    zmin = tmp['Percent_Unemployed'].min(); zmax = tmp['Percent_Unemployed'].max()\n    fig = px.choropleth_mapbox(tmp, geojson=lga_json, locations=tmp.index, \n                               color=tmp['Percent_Unemployed'],\n                               color_continuous_scale=\"viridis\",\n                               range_color=(zmin,zmax),\n                               hover_name = df_merged2.LGA_NAME20,   \n                               mapbox_style=\"carto-positron\",\n                               zoom=4,height=600,\n                               center = dict(lat= -19 , lon=145),  \n                               opacity=0.8,  \n                               title = f\"<b>Queensland | % Unemployed Males | [20-24]</b>\")\n    fig.update_layout(margin=dict(l=30, r=30, t=60, b=30))\n    fig.update_layout(font=dict(family='sans-serif',size=14))\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In this way, we can create choropeth maps in separate code cells and extract data we need from these interactive maps. We could also use a button based load option, which Plotly also offers. An example will be shown below.\n- An alternative to interactive maps are static maps. They work quite as well as interactive maps, especially with the addition of statics data.\n\n#### **STATIC CHOROPLETH MAPS**\n- Perhaps a little less insightful comapared to __interactive plots__ when it comes to trying to __show resuts for two very different sized bundaries__. It is very clear from the plot below that without further plots (that show for example the city centre), we really don't know what values are shown there, without some other form of figure or table.\n- The aim of the static plot below is to demonstrate that the __level of unemloyment is sightly higher for males than for females__ in the Victoria for the __age group 20-24__, in all statistical quadrant data, overall we can see the variation, even on static plots.\n- Static plots are more than sufficient as long as they can get the specific point across which was intended. We can confirm the above statement this by adding statistical information as well.","metadata":{}},{"cell_type":"code","source":"tdf = df_merged0.copy(); tdf2 = df_merged2.copy()\ntdf['Percent_Unemployed'] = tdf['Percent_Unemployed']*100\ntdf2['Percent_Unemployed'] = tdf2['Percent_Unemployed']*100\n\nfig, ax = plt.subplots(1,2, figsize=(12,6))\n\n# Legend\ndivider = make_axes_locatable(ax[0]);divider2 = make_axes_locatable(ax[1])\ncax = divider.append_axes(\"right\", size=\"2%\", pad=-0.0);\ncax2 = divider2.append_axes(\"right\", size=\"2%\", pad=-0.0)\n# Plot \ntdf.plot(column='Percent_Unemployed',cmap='viridis',\n         ax=ax[0],cax=cax,legend=True,edgecolor='black',\n         legend_kwds={'label': \"Female Unemployment Rate\"})\ntdf2.plot(column='Percent_Unemployed', cmap='viridis',\n          ax=ax[1],cax=cax2,legend=True,edgecolor='black',\n          legend_kwds={'label': \"Male Unemployment Rate\"})\n\n# Plot Aesthetics\nax[0].set_title('Female Unemployment Rate | 20-24 Year Old Demographic')\nax[1].set_title('Male Unemployment Rate | 20-24 Year Old Demographic')\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display_html\n\n# Display Multiple Dataframe in HTML format\ndef pd_html(dfs, names=[]):\n    html_str = ''\n    for i in dfs:\n        i.style.background_gradient(cmap='viridis') \n    \n    if names:\n        html_str += ('<tr>' + \n                     ''.join(f'<td style=\"text-align:center\">{name}</td>' for name in names) + \n                     '</tr>')\n    html_str += ('<tr>' + \n                 ''.join(f'<td style=\"vertical-align:top\"> {df.to_html(index=True)}</td>' \n                         for df in dfs) + \n                 '</tr>')\n    html_str = f'<table>{html_str}</table>'\n    html_str = html_str.replace('table','table style=\"display:inline\"')\n    display_html(html_str, raw=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Other Table or plots often are helpful to provide with geospatial plots\npd_html([tdf.describe(),tdf2.describe()],names=['female','male'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><span style='color:#2779d2'>EXAMPLE 2 | PERTH HOUSING DATA VISUALISATION</span></b>\n\n- In the previous example, we looked at __one interactive cholopeth plot__ per code entry. \n- In the following example, we'll look at a situation when we want to have the opportunity to __explore multiple interactive plots per code entry__, with <code>plotly</code> this is possible. \n- The goal in the current example is to provide the user with the option to explore multiple __suburb based mean value__ features of the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices). So we will need to add a very simple interactive dropdown menu, from which we can select a specific feature we would like to explore.\n\n#### **SELECT BOUNDARY DATA**\n\n- We'll use the __General District Area__ (GDA) datafile for <b>Western Australia</b>, where Perth is located. The geographic data is available in this source [data.gov](https://data.gov.au/dataset/ds-dga-6a0ec945-c880-4882-8a81-4dbcb85e74e5/distribution/dist-dga-9fff5439-7af5-42f4-9102-42c4199c5c1c/details?q=) which is part of the [WA Suburbs Data Page](https://data.gov.au/dataset/ds-dga-6a0ec945-c880-4882-8a81-4dbcb85e74e5/details?q=).\n- It's quite a heavy dataset to load, so we won't be using all of the shape data.\n- In the previous example, we used a __unique identifier__; <code>LGA_CODE20</code> to link the data with (geometry & statistics/visual data), since it was present in both of our datasets. In this example however, we will only have <code>SUBURB</code> name data in our __visual data__, so we will have to use __NAME__ as the unique identifier, making sure they __both contain the same elments__ ( in this example, the perth dataset suburb names need to be capitalised )\n\n","metadata":{}},{"cell_type":"code","source":"# Load Geometry File\nwa_gdf = gpd.read_file('/kaggle/input/wa-gda2020/WA_LOCALITY_POLYGON_SHP-GDA2020.shp')    # Load the data using \nwa_gdf.drop(['POSTCODE','PRIM_PCODE','LOCCL_CODE','DT_GAZETD','STATE_PID','DT_RETIRE','DT_CREATE','LOC_PID'],axis=1,inplace=True)\nwa_gdf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **PREPARING DATA FOR VISUALISATION**\n- We only have two unique identifiers that is related to our boundary file; <code>POSTCODE</code> & <code>SUBURB</code>. \n- Our boundary file data has a column <code>NAME</code> containing in it the Perth Suburbs, which we want, althought <code>POSTCODE</code> would have worked too.","metadata":{}},{"cell_type":"code","source":"df_perth = pd.read_csv('/kaggle/input/perth-house-prices/all_perth_310121.csv')\ndf_perth.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the data we want to plot\nfeatures = df_perth.select_dtypes('int64','float64').columns.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some Cleaning\ndf_perth.drop_duplicates(subset=['ADDRESS'],inplace=True) # Some addresses actually have multiple entries\ndf_perth.index = df_perth['ADDRESS']                      # set dataframe index, since it's not really a useful feature \ndel df_perth['ADDRESS']                                   # let's also delete the column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display Values\nwa_gdf.index = wa_gdf['NAME'] # set the geometry dataframe index to NAME/SUBURB\nmedian_price = df_perth.groupby(['SUBURB']).median()      # Suburb Median Groupby automatically sets index to suburbs\nmedian_price.index = median_price.index.str.upper()   # making sure they both are the same \nmedian_price.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge geometry & visualisation data, both use suburn name as index\ndf_merged = wa_gdf.join(median_price).dropna()   \n\n# Convert geometry to GeoJSON\ndf_merged = df_merged.to_crs(epsg=4327)\nlga_json = df_merged.__geo_interface__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of loading only one dataset into <code>go.Figure(data=data, layout=layout)</code>, we simply need to load them into a list.","metadata":{}},{"cell_type":"code","source":"trace = []    \n# Set the data for the map\nfor i in features:\n    trace.append(go.Choroplethmapbox(geojson = lga_json,locations = df_merged.index,    \n                           z = df_merged[i].values,                     \n                           text = df_merged.index,\n                           hovertemplate = \"<b>%{text}</b><br>\" +\n                                            \"%{z}<br>\" +\n                                            \"<extra></extra>\",\n                           colorbar=dict(thickness=10, ticklen=3,outlinewidth=0),\n                           marker_line_width=1, marker_opacity=0.8, colorscale=\"viridis\",\n                           visible=False)\n                    )\ntrace[0]['visible'] = True # set the visibility of the first entry class's visibility content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be able to select multiple data, we need to change the __layout__ to include __update_menus__ that will indicate which feature to load via the __visible__ argument, information which is passed through via the list, <code>lst</code>.","metadata":{}},{"cell_type":"code","source":"lst = [];ii=-1\nfor i in features:\n    ii+=1\n    tlist = [False for z in range(len(features))]\n    tlist[ii] = True\n    temp = dict(args=['visible',tlist],label=i,method='restyle') \n    lst.append(temp)\n    \nprint(lst[0])\nprint(lst[1])\nprint(lst[2]) # ...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(plot_choropleth):\n    # add a dropdown menu in the layout\n    layout.update(height=500,updatemenus=list([dict(x=0.8,y=1.1,xanchor='left',yanchor='middle',buttons=lst)]))\n\n    # The rest is the same\n    fig=go.Figure(data=trace, layout=layout)\n    fig.update_layout(title_text='SUBURB MEAN VALUES', title_x=0.01)\n    fig.update_layout(\n        hovermode='closest',\n        mapbox=dict(\n            accesstoken=map_token,\n            bearing=90,\n            center=dict(lat=-32, lon=115.9),\n            pitch=0,\n            zoom=9.5\n        )\n    )\n    fig.update_layout(margin={\"r\":0,\"t\":80,\"l\":0,\"b\":0},mapbox_style=\"light\");fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>3 | <span style='color:#023e8a'> HEXBIN MAPS </span></b>\n\nHexbins are not quite as common as __choropeth__ maps, but do provide some useful insight nevertheless. There are two main types of __Hexbin Map types__; \n- Hexbin Maps from Geospacial Objects (simplified hexagons corresponing to a specific region are created)\n- Hexbin Maps from Coordinates (hexagons are created within the viscinity of point data)\n\n#### **MAIN IDEA BEHIND HEXBIN MAPS FROM COORDINATES:**\n\n- The main idea of these plots is to __define subdomain/submap regions__ in the __viscinity of available scatter data__, similar to discretisation in mathematics, and count the points in its viscinity, displaying point count values, similar to histograms. In fact it pretty much is a density 2D plot, but with geospatial data. Such maps tells us about the __concentration of geospatial points on a map__. \n- __Mapbox layout__ integration is very handy here since __hexagons that are constructed from scattered data have no associated region name__. Interactive Plotly maps with __reduced opacity__ allows us to visualise exactly what region the hexagon corresponds to, using the interactive map.\n\n#### **HEXBIN REQUIRES:**\n- <code>longitude</code> & <code>latitude</code> point data.\n\n#### **TASK AT HAND**\n\n- In the current example we'll look at the latter, using the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices) once again, only this time we'll use the coordinate data features <code>LONGITUDE</code> & <code>LATITUDE</code>. \n- Having been given a set of <code>addresses</code> with GPS coordinates, we want very quickly get an idea of which parts of Perth had the most property sales according to this dataset.","metadata":{}},{"cell_type":"code","source":"df_perth.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.figure_factory as ff\nif(plot_hex):\n\n    fig = ff.create_hexbin_mapbox(\n        data_frame=df_perth, lat=\"LATITUDE\", lon=\"LONGITUDE\",\n        nx_hexagon=200, opacity=0.5,min_count=1, labels={\"color\": \"Point Count\"},\n        range_color = [0,20],\n        color_continuous_scale=\"mint\",\n        show_original_data=True, # show point data\n        original_data_marker=dict(size=4, opacity=0.25,color='black'), # point data options\n        zoom = 11,height=500,\n    )\n    fig.update_layout(\n        hovermode='closest',\n        mapbox=dict(\n            accesstoken=map_token,\n            bearing=90,\n            center=dict(lat=-32, lon=115.9),\n            pitch=0,\n            zoom=11\n        )\n    )\n    fig.update_layout(margin={\"r\":0,\"t\":80,\"l\":0,\"b\":0},mapbox_style=\"light\");fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator,RegressorMixin\nfrom numpy.linalg import cholesky, det, lstsq, inv, eigvalsh, pinv\nfrom scipy.optimize import minimize\npi = 4.0*np.arctan(1.0)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Usage similar to any sklearn model\nclass GPR(BaseEstimator,RegressorMixin):\n\n    ''' Class Instantiation Related Variables '''\n    # With just the one class specific GPC.kernel\n    def __init__(self,kernel='rbf',theta=10.0,sigma=10.0,sigma_n=0.01,opt=True):\n        self.theta = theta            # Hyperparameter associated with covariance function\n        self.sigma = sigma            #                       ''\n        self.sigma_n = sigma_n        # Hyperparameter associated with cov.mat's diagonal component\n        self.opt = opt                # Update hyperparameters with objective function optimisation\n        GPR.kernel = kernel           # Selection of Covariance Function, class specific instantiation\n\n    ''' local covariance functions '''\n    # Covariance Functions represent a form of weight adjustor in the matrix W/K\n    # for each of the combinations present in the feature matrix\n    @staticmethod\n    def covfn(X0,X1,theta=1.0,sigma=1.0):\n\n        ''' Radial Basis Covariance Function '''\n        if(GPR.kernel == 'rbf'):\n            r = np.sum(X0**2,1).reshape(-1,1) + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            return sigma**2 * np.exp(-0.5/theta**2*r)\n\n        ''' Matern Covariance Class of Funtions '''\n        if(GPR.kernel == 'matern'):\n            lid=2\n            r = np.sum(X0**2,1)[:,None] + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            if(lid==1):\n                return sigma**2 * np.exp(-r/theta)\n            elif(lid==2):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(3)*ratio)\n                v2 = np.exp(-np.sqrt(3)*ratio)\n                return sigma**2*v1*v2\n            elif(lid==3):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(5)*ratio+(5.0/3.0)*ratio**2)\n                v2 = np.exp(-np.sqrt(5)*ratio)\n                return sigma**2*v1*v2\n        else:\n            print('Covariance Function not defined')\n    \n    ''' Train the GPR Model'''\n    def fit(self,X,y):\n        \n        # Two Parts Associated with base GP Model:\n        # - Hyperaparemeter; theta, sigma, sigma_n selection\n        # - Definition of Training Covariance Matrix\n        # Both are recalled in Posterior Prediction, predict()\n        \n        ''' Working w/ numpy matrices'''\n        if(type(X) is np.ndarray):\n            self.X = X;self.y = y\n        else:\n            self.X = X.values; self.y = y.values\n        self.ntot,ndim = self.X.shape\n\n        ''' Optimisation Objective Function '''\n        # Optimisation of hyperparameters via the objective funciton\n        def llhobj(X,y,noise):\n            \n            # Simplified Variant\n            def llh_dir(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                return 0.5 * np.log(det(K)) + \\\n                    0.5 * y.T.dot(inv(K).dot(y)).ravel()[0] + 0.5 * len(X) * np.log(2*pi)\n\n            # Full Likelihood Equation\n            def nll_full(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                L = cholesky(K)\n                return np.sum(np.log(np.diagonal(L))) + \\\n                    0.5 * y.T.dot(lstsq(L.T, lstsq(L,y)[0])[0]) + \\\n                    0.5 * len(X) * np.log(2*pi)\n\n            return llh_dir # return one of the two, simplified variant doesn't always work well\n\n        ''' Update hyperparameters based on set objective function '''\n        if(self.opt==True):\n            # define the objective funciton\n            objfn = llhobj(self.X,self.y,self.sigma_n)\n            # search for the optimal hyperparameters based on given relation\n            res = minimize(objfn,[1,1],bounds=((1e-5,None),(1e-5, None)),method='L-BFGS-B')\n            self.theta,self.sigma = res.x # update the hyperparameters to \n\n        ''' Get Training Covariance Matrix, K^-1 '''\n        Kmat = self.covfn(self.X,self.X,self.theta,self.sigma) \\\n                 + self.sigma_n**2 * np.eye(self.ntot) # Covariance Matrix (Train/Train)\n        self.IKmat = pinv(Kmat) # Pseudo Matrix Inversion (More Stable)\n        return self  # return class & use w/ predict()\n\n    ''' Posterior Prediction;  '''\n    # Make a prediction based on what the model has learned \n    def predict(self,Xm):\n        \n        ''' Working w/ numpy matrices'''\n        if(type(Xm) is np.ndarray):\n            self.Xm = Xm\n        else:\n            self.Xm = Xm.values\n        \n        # Covariance Matrices x2 required; (Train/Train,Train/Test)\n        mtot = Xm.shape[0]  # Number of Test Matrix Instances\n        K_s = self.covfn(self.X,Xm,self.theta,self.sigma)  # Covariance Matrix (Train/Test)               \n        self.mu_s = K_s.T.dot(self.IKmat).dot(self.y)      # Posterior Mean Prediction\n        return self.mu_s # return posterior mean","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 | <span style='color:#023e8a'> CLUSTER MAPS</span></b>\n\n#### **OVERVIEW**\n\nOne of the most common type of geospatial map analysis tools;\n\n- Cluster maps are a form of __bivariate/multivariate plot__, being a two dimensional plot on a map, either representing a __bivariate__ geospatial position point only ( no <code>color</code> used ) or a __multivariate plot__, which adds additional information about the geospatial position via the <code>color</code> option. Point variation can also be achieved via the <code>size</code> option, which makes it a very versatile geospatial analysis tool.\n- Cluster maps are also commonly used together with __interpolative methods__, to estimate point values in regions we do not have, we will look at one commonly used method one such method called __kriging__.\n\n#### **CLUSTER MAPS REQUIRES:**\n- <code>longitude</code> & <code>latitude</code> spatial point & <code>size</code>/<code>color</code> visualisation data.\n\n## <b><span style='color:#2779d2'>UNIVERSAL KRIGING</span></b>\n\n- __Interpolative Methods__ are very commonly used in geospatial data analysis. Mainly to an attempt to __estimate values__ in regions where data is not available.\n- In the context of Machine Learning, interpolative methods are identical to standard machine learning models, in general. Interpolative tend to only use only use __geospaspatial location data features__ & one feature we would like to predict.\n- Kriging is also referred to a __Surrogate Model__ in Optimisation problems.\n\n__A Brief Overview of the Kriging Model is provided below for completeness:__\n\n<p>The Kriging model is built on the assumption that the data <em>Y</em> obey a Gaussian process with an assumed form for the mean function and the covariance between data points: </p>\n<p class=\"formulaDsp\">\n\\[ Y = N(m(\\vec{x}), K(\\vec{x},\\vec{x}))\\]\n</p>\n<p> where \\(m(\\vec{x})\\) is the mean function and \\(K(\\vec{x},\\vec{x})\\) represents the covariance between function values. For this work, a regression mean function is assumed. Using this form, the mean function has the following form: </p>\n<p class=\"formulaDsp\">\n\\[ m(x) = h^{T}(\\vec{x}) \\beta \\]\n</p>\n<p> where \\(h^{T}(\\vec{x})\\) represents a column vector containing the basis functions of the basis evaluated at the points \\(\\vec{x}\\). The regression parameters \\(\\beta\\) are treated as part of the Kriging model and are determined while constructioning the model. Using this form of the mean function yields a Universal Kriging model. The case of a (polyorder=0) regression (where the vector \\(h(\\vec{x})\\) reduces to unity) is referred to as <b>Ordinary Kriging</b> and is also covered by this functional form. The assumption of a vague prior on the regression parameters gives the following closed form for the parameters: </p>\n<p class=\"formulaDsp\">\n\\[ \\beta=(H K^{-1} H^{T})^{-1} H^T K^{-1} Y = A^{-1} H^T K^{-1} Y \\]\n</p>\n<p> where \\(K\\) is the covariance matrix between the training data. For a Kriging model, the covariance between function values is assumed to be only a function of the distance between points. The multi-dimension covariance function is constructed using a tensor product of one dimension functions. The multi-dimension covariance is calculated in <b>fit()</b>, which calls the static method <b>covfn</b>. The elements in ths covariance matrix are given as: </p>\n<p class=\"formulaDsp\">\n\\[ K_{i,j} = cov(y_{i},y_{j}) = \\sigma^{2} k(\\vec{X}_{i},\\vec{X}_{j}; \\theta) + \\sigma^{2}_{n} \\delta_{i,j} \\]\n</p>\n<p> The parameters \\(\\sigma\\) and \\(\\theta\\) (and in some cases \\(\\sigma_{n}\\)) are denoted as hyperparameters and are determined maximizing the likelihood equation for the Kriging model. This likelihood gives the probability that a Gaussian process with specified hyperparamters describes the training data <em>X</em> and <em>Y</em>. By picking the hyperparameters that maximize this probability, a Kriging model that best describes the data can be constructed. The hyperparameters are determined based on the <b>likelihood equation</b> for a gaussian process with a vague prior on the regression parameters. This likelihood is computed in function <b>llhobj</b> which uses the Scipy Module, <b>minimize</b>. Using this equation, optimization is used to determine all of the parameters, including the covariance magnitude \\(\\sigma\\) and noise \\(\\sigma_{n}\\). This way of determining hyperparameters should be used when the noise level of the function needs to be fitted. <br/>\n With the regression and covariance parameters determined, the final processed data can be constructed using the inverse of the covariance matrix. To make predictions from the Kriging model, the following vector is required: </p>\n<p class=\"formulaDsp\">\n\\[ V = K^{-1} (Y - H^{T} \\beta) \\]\n</p>\n<p> where \\( K \\) is the covariance matrix, the product \\(H^{T} \\beta\\) represents the mean function evaluated at the training points and \\(Y\\) represents the function values at the training points. Using this processed data, the regression parameters and covariance parameters & predictions can be made.\n\n<tr><td class=\"mdescLeft\">&#160;</td><td class=\"mdescRight\">Model predictions throughout the domain are determined by sampling from the conditional distribution \\(y_* | \\vec{X},Y\\) using the covariance between points in the domain where \\(\\vec{X},Y\\) are the input and output training data. The posterior mean predictions for an explicit mean are given by the formula: </p>\n<p class=\"formulaDsp\">\n\\[ y(\\vec{x}_{*}) | \\vec{X},Y,m(x) = m(\\vec{x}_{*}) + k_*^T K^{-1} (Y-m(\\vec{x}_{*})) \\]\n</p>\n<p> where \\(k_{*}^{T}\\) represents the covariance between the test point, \\(\\vec{x}_{*}\\), and the training points \\(\\vec{X}\\) (a row vector of length ntot). <br/>\n For a regression mean function, the function predictions take the form of: </p>\n<p class=\"formulaDsp\">\n\\[ y(\\vec{x}_{*}) | \\vec{X},Y,\\beta = h^{T}(\\vec{x}_{*}) \\beta + k_*^T K^{-1} (Y-H^{T} \\beta) \\]\n</p>\n<p> The regression parameters \\(\\beta\\) and the hyperparamters in the covariance function are supplied by <b>fit()</b>. Using only this data, function predictions can be made; however, the construction and inverse of the covariance matrix can make the function predictions expensive. Because this matrix is inverted during the construction of the Kriging model, this work can be re-used for function predictions. Defining the processed data \\(V\\) as: </p>\n<p class=\"formulaDsp\">\n\\[ V = K^{-1} (Y - H^{T} \\beta) \\]\n</p>\n<p> the function predictions are given by: </p>\n<p class=\"formulaDsp\">\n\\[ y(\\vec{x}_{*}) | \\vec{X},Y,\\beta = h^{T}(\\vec{x}_{*}) \\beta + k_*^T V \\]\n</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator,RegressorMixin\nfrom numpy.linalg import cholesky, det, lstsq, inv, pinv\nfrom scipy.optimize import minimize\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\npi = 4.0*np.arctan(1.0)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Universal Kriging Model (Polynomial Regression + Full Gaussian Process Regression Model)\n# Commonly used Ensemble Approach for geospatial interpolation and \n\nclass Kriging(BaseEstimator,RegressorMixin):\n    \n    def __init__(self,kernel='rbf',theta=10.0,sigma=10.0,sigma_n=1,opt=True,polyorder=2):\n        self.theta = theta\n        self.sigma = sigma\n        self.sigma_n = sigma_n\n        self.opt = opt\n        self.polyorder = polyorder \n        Kriging.kernel = kernel \n\n    ''' local covariance functions '''\n    @staticmethod\n    def covfn(X0,X1,theta=1.0,sigma=1.0):\n\n        ''' Radial Basis Covariance Function '''\n        if(Kriging.kernel == 'rbf'):\n            r = np.sum(X0**2,1).reshape(-1,1) + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            return sigma**2 * np.exp(-0.5/theta**2*r)\n\n        ''' Matern Covariance Class of Funtions '''\n        if(Kriging.kernel == 'matern'):\n            lid=1\n            r = np.sum(X0**2,1)[:,None] + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            if(lid==1):\n                return sigma**2 * np.exp(-r/theta)\n            elif(lid==2):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(3)*ratio)\n                v2 = np.exp(-np.sqrt(3)*ratio)\n                return sigma**2*v1*v2\n            elif(lid==3):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(5)*ratio+(5.0/3.0)*ratio**2)\n                v2 = np.exp(-np.sqrt(5)*ratio)\n                return sigma**2*v1*v2\n        else:\n            print('Covariance Function not defined')\n            \n    ''' Train the Model'''\n    def fit(self,X,y):\n        \n        ''' Working w/ numpy matrices'''\n        if(type(X) is np.ndarray):\n            self.X = X;self.y = y\n        else:\n            self.X = X.values; self.y = y.values\n        self.ntot,ndim = self.X.shape\n        \n        # Collocation Matrix\n        self.poly = PolynomialFeatures(self.polyorder)\n        self.H = self.poly.fit_transform(self.X)\n        \n        ''' Optimisation Objective Function '''\n        # Optimisation of hyperparameters via the objective funciton\n        def llhobj(X,y,noise):\n            \n            # Simplified Variant\n            def llh_dir(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                return 0.5 * np.log(det(K)) + \\\n                    0.5 * y.T.dot(inv(K).dot(y)).ravel()[0] + 0.5 * self.ntot * np.log(2*pi)\n\n            # Full Likelihood Equation\n            def nll_full(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                L = cholesky(K)\n                return np.sum(np.log(np.diagonal(L))) + \\\n                    0.5 * y.T.dot(lstsq(L.T, lstsq(L,y)[0])[0]) + \\\n                    0.5 * self.ntot * np.log(2*pi)\n            \n            return llh_dir # return one of the two, simplified variant doesn't always work well\n        \n        ''' Update hyperparameters based on set objective function '''\n        if(self.opt==True):\n            # define the objective funciton\n            objfn = llhobj(self.X,self.y,self.sigma_n)\n            # search for the optimal hyperparameters based on given relation\n            res = minimize(fun=objfn,x0=[1,1],\n                           method='Nelder-Mead',tol=1e-6)\n            self.theta,self.sigma = res.x # update the hyperparameters to \n\n        self.HT = self.H.T\n        self.Kmat = self.covfn(self.X,self.X,self.theta,self.sigma) \\\n                  + self.sigma_n**2 * np.eye(self.ntot) # Covariance Matrix (Train/Train)\n        self.IKmat = pinv(self.Kmat) # Pseudo Matrix Inversion (More Stable)\n\n        self.HK = np.dot(self.HT,self.IKmat) # HK^-1\n        HKH = np.dot(self.HK,self.H)     # HK^-1HT\n        self.A = inv(HKH)             # Variance-Covariance Weighted LS Matrix\n\n        self.W = np.dot(self.IKmat,self.y)\n        Q = np.dot(self.HT,self.W)\n        self.beta = np.dot(self.A,Q)               # Regression coefficients\n        self.V = self.W - np.dot(self.IKmat,self.H).dot(self.beta) # K^{-1} (Y - H^{T} * beta)\n        \n        return self  # return class & use w/ predict()\n\n    ''' Posterior Prediction;  '''\n    # Make a prediction based on what the model has learned \n    def predict(self,Xm):\n        \n        ''' Working w/ numpy matrices'''\n        if(type(Xm) is np.ndarray):\n            self.Xm = Xm\n        else:\n            self.Xm = Xm.values\n        self.mtot,ndim = self.Xm.shape\n        \n        self.Hm = self.poly.fit_transform(self.Xm) # Collocation Matrix\n        self.Kmat = self.covfn(self.X,self.Xm,self.theta,self.sigma) # Covariance Matrix (Train/Test)\n        yreg = np.dot(self.Hm,self.beta)               # Mean Prediction based on Regression\n        ykr = np.dot(self.Kmat.T,self.V)              # posterior mean predictions for an explicit mean \n\n        return yreg + ykr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><span style='color:#2779d2'>EXAMPLE 1 | TIME SERIES EXAMPLE</span></b>\n\n#### **TASK AT HAND**\n\n- Let's use a <code>cluster map</code> to plot __daily maximum temperature history__ at various locations in Australia using the [Australian Average Daily Maximum Temperature](https://www.kaggle.com/joshmills/australian-average-daily-maximum-tempreature) Dataset. \n- Our data contains historical data (<code>Year</code>) so let's use the __animation_frame__ option & observe the change in temerature over the available period.\n- Let's also try to note some interesting overall tendencies in the temperature recordings during the given period by using a Machine Learning Model to predict an overall trend for all locations (<code>name</code>), using __Gaussian Process Regression__.\n- And finally, let's load different city/town locations in Australia and predict their temperature values based based on the main max temperature dataset for the year, 2000, using a __Kriging__ model.","metadata":{}},{"cell_type":"code","source":"# dataset w/ temperature data\naus_maxT = pd.read_csv('/kaggle/input/australian-average-daily-maximum-tempreature/AustralianAverageMaxTemp.csv')\naus_maxT.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aus_maxT['Year'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Different places in Australia where data was collected\naus_maxT['name'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2019 evidently contains incomplete data for 2019 since the value is too abnormally high.\naus_maxT2 = aus_maxT[aus_maxT['Year'] != 2019] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **PLOTTING A TIME SERIES SCATTER CLUSTER MAP**\n- Let's look at a time series evolution of __maximum temperature__ for all regions where data is available using a cluster map.\n- We have quite a few datapoints all across Australia, let's focus our attention to the __Victoria__ when creating a model. ","metadata":{}},{"cell_type":"code","source":"# Figure factory requires physical file with mapbox code\npx.set_mapbox_access_token(open(\"/kaggle/working/mapbox_token\").read())\n\nfig = px.scatter_mapbox(aus_maxT, lat=\"lat\", lon=\"long\",color='Temp',mapbox_style='light',\n                        opacity=1.0,range_color=[15,28],size='Temp',size_max=10,\n                        animation_frame = 'Year',\n                        zoom=2.8,center=dict(lat=-27, lon=140),height=650)\nfig.update_layout(margin=dict(l=30, r=30, t=60, b=30))\nfig.update_layout(template='plotly_white',title='<b>ALL STATION MAX TEMPERATURE</b> | TIME SERIES VARIATION',showlegend=True)\nfig.update_layout(font=dict(family='sans-serif',size=12));fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In the above plot we can note some very __clear oscilations__ in maximum temperature, which have a tendency to occur periodically every decade, as well as a very small increase in temperature tendency.","metadata":{}},{"cell_type":"markdown","source":"#### **PLOTTING THE YEARLY MEAN OF MAXIMUM TEMPERATURE**\n- Let's plot the __overall mean trend__ using all data sampling stations, from the current data, we can see that there is a very minimal increase in mean maximum temperature.\n- Geospatial maps often work well together with figure plots to more clearly show the animal dynamic on the map (especially for stationally points). Let's use <code>groupby</code> and evaluate the __mean value__ for all locations.","metadata":{}},{"cell_type":"code","source":"ltdf = aus_maxT2.groupby('Year').mean()\n\n# Create an interpolation\\\nmodel = GPR(opt=False,theta=2)\nmodel.fit(X=ltdf.index[:,None],y=ltdf['Temp'])\nXm = np.arange(1975,2018,0.1)[:,None]\nym = model.predict(Xm)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=Xm[:,0],y=ym,name='GPR Model',marker=dict(color=lst_col[0])))\nfig.add_trace(go.Scatter(x=ltdf.index,y=ltdf.Temp,name='Mean Measurements',mode='markers',marker=dict(color='black')))\nfig.update_layout(template='plotly_white',title='<b>ALL STATION MEAN</b> | MAX YEARLY TEMPERATURE',height=350,showlegend=True)\nfig.update_layout(margin={\"r\":30,\"t\":80,\"l\":30,\"b\":30})\nfig.update_layout(font=dict(family='sans-serif',size=12))\nfig.update_yaxes(range=[23,28])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **GEOSPATIAL MAXIMUM TEMPERATURE INTERPOLATION**\n- Next, let's use Kriging to predict maximum temperature in different regions of __Victoria__, we'll need to load the location data first as well. ","metadata":{}},{"cell_type":"code","source":"au_cities = pd.read_csv('/kaggle/input/au-cities/au.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"au_cities.index = au_cities['city']; del au_cities['city']\nau_cities.drop(['country','iso2','admin_name','capital','population','population_proper'],axis=1,inplace=True)\nau_cities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Let's build a model to predict maximum temperature in 2000 only and predict for 'au_cities' '''\n\n# Get the data data for Training, for interpolation we only use coordinates\nX0 = aus_maxT2[['lat','long','Temp','Year']]\nX1 = X0[X0['Year'] == 2000].drop(('Year'),axis=1)\n# display(X1)\n\n# train/test split\ny = X1['Temp']\nX = X1.drop(('Temp'),axis=1)\n\n# instantiate & fit w/ hyperparameter optimisation\nmodel = Kriging(opt=True,polyorder=2)\nmodel.fit(X,y) # Nelder Mead Search optimisation of parameter results displayed in ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kriging_model = pd.Series(model.predict(au_cities),name='model_temperature')\nkriging_model.index = au_cities.index\nkriging_results = pd.concat([au_cities,kriging_model],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Let's visualise the data used for interpolation and prediction locations '''\n\n# Subset of interest \nsubset = aus_maxT2[aus_maxT2['Year']==2000]\n\n# Figure factory requires physical file with mapbox code\npx.set_mapbox_access_token(open(\"/kaggle/working/mapbox_token\").read())\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scattermapbox(\n        lat=kriging_results['lat'],\n        lon=kriging_results['lng'],\n        mode='markers',\n        marker=go.scattermapbox.Marker(size=10,color='Grey',opacity=1.0)))\n\nfig.add_trace(px.scatter_mapbox(subset,\n                                lat=\"lat\", lon=\"long\",color='Temp',\n                                opacity=1.0,size='Temp')[\"data\"][0])\n\n# Plot Aesthetics\nfig.update_layout(title_text='<b>MAX TEMP IN 2000 (LARGE)</b>  | MODEL TEMPERATURE  (SMALL)', title_x=0.01)\nfig.update_layout(\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=map_token,\n        center=dict(lat=-38, lon=145),\n        pitch=0,zoom=5.5))\nfig.update_layout(margin={\"r\":30,\"t\":80,\"l\":30,\"b\":30},mapbox_style=\"light\",height=800)\nfig.update_layout(font=dict(family='sans-serif',size=12))\nfig.update(layout_showlegend=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Let's visualise the data used for interpolation and prediction locations '''\n\n# Subset of interest \nsubset = aus_maxT2[aus_maxT2['Year']==2000]\n\n# Figure factory requires physical file with mapbox code\npx.set_mapbox_access_token(open(\"/kaggle/working/mapbox_token\").read())\n\nfig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scattermapbox(\n        lat=subset['lat'],\n        lon=subset['long'],\n        mode='markers',\n        marker=go.scattermapbox.Marker(size=20,color='Black',opacity=1.0)))\nfig.add_trace(go.Scattermapbox(\n        lat=kriging_results['lat'],\n        lon=kriging_results['lng'],\n        mode='markers',\n        marker=go.scattermapbox.Marker(size=10,color='Black',opacity=1.0)))\nfig.add_trace(px.scatter_mapbox(kriging_results,\n                                lat=\"lat\", lon=\"lng\",color='model_temperature',\n                                opacity=1.0,size='model_temperature',size_max=7)[\"data\"][0])\nfig.add_trace(px.scatter_mapbox(subset,\n                                lat=\"lat\", lon=\"long\",color='Temp',\n                                opacity=1.0,size='Temp',size_max=15)[\"data\"][0])\n\n# Plot Aesthetics\nfig.update_layout(title_text='<b>MAX TEMP IN 2000 (LARGE)</b>  | MODEL TEMPERATURE PREDICTIONS (SMALL)', title_x=0.01)\nfig.update_layout(\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=map_token,\n        center=dict(lat=-38, lon=145),\n        pitch=0,zoom=5.5))\nfig.update_layout(margin={\"r\":30,\"t\":80,\"l\":30,\"b\":30},mapbox_style=\"light\",height=800)\nfig.update_layout(font=dict(family='sans-serif',size=12))\nfig.update(layout_showlegend=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## <b><span style='color:#2779d2'>EXAMPLE 2 | CLUSTER MAPS FOR EDA</span></b>\n\n#### **TASK AT HAND**\n\n- Let's revisit the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices).\n- We are interested in different properties that were sold near the Perth CBD & their corresponding property <code>FLOOR_AREA</code>.\n- An interactive maps allow us to investigate the different properties using the interactive window & outline for example key regions with high <code>FLOOR_AREA</code>.","metadata":{}},{"cell_type":"code","source":"# Figure factory requires physical file with mapbox code\npx.set_mapbox_access_token(open(\"/kaggle/working/mapbox_token\").read())\n\nfig = px.scatter_mapbox(df_perth, lat=\"LATITUDE\", lon=\"LONGITUDE\",\n                        opacity=1.0,range_color=[0,300],size='FLOOR_AREA',\n                        color_continuous_scale=\"jet\",\n                        hover_name=df_perth.index,\n                        color=\"FLOOR_AREA\", zoom=12,height=400)\nfig.update_traces(marker=dict(size=9))\nfig.update_layout(margin={\"r\":30,\"t\":80,\"l\":30,\"b\":30},mapbox_style=\"light\",height=800)\nfig.update_layout(font=dict(family='sans-serif',size=12))\nfig.update(layout_showlegend=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 | <span style='color:#023e8a'> DENSITY HEATMAPS</span></b>\n- Density heatmaps allow us to visuaise __clustermap__ data in the form of a continuous function. The issue with heavily concentrated __clustermaps__ is that upon point overlap, it may be difficult to distinguish them apart due to point overlap.\n- Continuous functions used to define the heatmap allows us to visualise the overall local value tendencies, similar to how a bias/variance balanced model. The benefit of interactive heatmaps also lies in the ability to use <code>hover_name</code> which comes in handy as well.\n\n#### **CLUSTER MAPS REQUIRE**\n- <code>longitude</code> & <code>latitude</code> spatial point & <code>size</code>/<code>color</code> visualisation data.\n\n#### **TASK AT HAND**\n\n- Let's revisit the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices) once again. We are interested in different properties that were sold near the Perth.\n- This time, we aren't too interested in individual property EDA, rather we use heatmaps to find general trends in different parts of Perth, this is quite useful to get an overall picture.","metadata":{}},{"cell_type":"code","source":"fig = px.density_mapbox(df_perth, lat=\"LATITUDE\", lon=\"LONGITUDE\",z='FLOOR_AREA',\n                        radius=25,opacity=0.7,hover_name=df_perth.index,\n                        height=800,color_continuous_scale='viridis')\n\nfig.update_layout(\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=map_token,\n        bearing=0,\n        pitch=0,\n        zoom=12))\nfig.update_layout(margin=dict(l=30, r=30, t=60, b=30));fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 | <span style='color:#023e8a'> SUMMARY</span></b>\n- In this notebook, we looked at different tools that can be used for geospatial data analysis, mainly <code>Choropleth</code>,<code>Hexbin</code>,<code>Cluster</code> & <code>Density Heatmaps</code>.\n- Greater attention was payed to <code>Choropleth Maps</code>, due to their more complex data input structure, requiring __boundary geometry__ data alongside with the data which is desired to be shown. Compared to <code>Hexbin</code>,<code>Cluster</code> & <code>Density Heatmaps</code>, which require only __point coordinates (longitude & latitude)__.\n\n#### **BOUNDARY BASED DATA & CHOROPLETH MAPS**\n\n- Interactive <code>Choropeth</code> were shown to be quite effective at portraying data, especially when the difference in boundary sizes to be shown is very big. We also plotted static maps using __geopandas__, although most of the regions were visible, additional plots were needed in order to show all region data clearly.\n- One slightly issue arose when plotting Australian <code>Choropleth</code> maps, we needed to know specifically where to get the boundary data, which is a required step for plotting Australian based <code>Choropleth Maps</code>. These sources were outlined and a specific example for the __unemployment rate__ of specific demographics were shown and compared to one another.\n- One of the more compex parts of plotting <code>choropleth</code> maps, was the integration of two separate dataframe (boundary & visualisation), which required some data wrangling to combine and join indicies.\n\n#### **COORDINATE BASED DATA**\n\n- <code>Hexbin</code>,<code>cluster</code> & <code>density heatmaps</code>, all require geospatial point data (longitude,latitude)\n- Out of the three, <code>cluster maps</code> are probably most useful due to their ability to pinpoint data at different locations. They are also commonly used with data interpolation methods, to estimate data at points we don't yet have data.\n- We also saw that cluster maps, especially when zoomed out tend to start overlapping as demonstrated in the __Perth Housing__ dataset, in such cases, <code>density heatmaps</code> are quite useful, in order to plot continuous data.","metadata":{}}]}