{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**File Handling**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#General info about the data set attributes\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pre-processing & Correlation check**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping unwanted columns\ndf=df.drop(['EmployeeNumber','EmployeeCount','StandardHours'], axis=1)\ndf.isna().sum() #Check if there is any 'NaN' values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any() #Check if there is missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating correleation matrix showing connection rates between attributes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(30, 30))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\",annot_kws={\"size\":15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the categorial columns\ncategorial_col = df.select_dtypes(include=\"object\")\ncategorial_col.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting categorial attributions to numbers.\nfrom sklearn.preprocessing import LabelEncoder\nlr = LabelEncoder()\n\nfor i in categorial_col:\n    df[i]=lr.fit_transform(df[i])\n    \ndf[categorial_col.columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explaining the connection between categorial data to the target feature\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(font_scale=1.1)\nplt.figure(figsize=(30, 30))\n\nfor i, column in enumerate(categorial_col, 1):\n    plt.subplot(3, 3, i)\n    g = sns.barplot(x=f\"{column}\", y='Attrition', data=df)\n    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n    plt.ylabel('Attrition Count')\n    plt.xlabel(f'{column}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting attrition values to binary [No Attrition=0, Yes Attrition=1]\ndf['Attrition'] = df.Attrition.astype(\"category\").cat.codes\ndf.Attrition.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train-Test split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We split the data into test and train\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Attrition', axis=1)\ny = df.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Algorithms**\nDecision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Decision Tree Clasiffier\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the accuracy score for train\ny_train_pred = dtc.predict(X_train)\ny_train_prob = dtc.predict_proba(X_train)[0,1]\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve,confusion_matrix, f1_score\naccuracy_score(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we have accuracy score of 1.0, we have the model on overfitting, hence, we do hypermeter tuning to avoid that and then scoring\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\n\ndtc = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(dtc, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the model on the best params created by the model_selection\ndtc_best = DecisionTreeClassifier(**best_params)\ndtc_best.fit(X_train, y_train)\n#Accuracy score of train\ny_train_pred = dtc_best.predict(X_train)\ny_train_prob = dtc_best.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_DTC = dtc_best.predict(X_test)\ny_test_prob_DTC = dtc_best.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Decsision Tree Classifier: \",roc_auc_score(y_test, y_test_prob_DTC))\n#Plotting confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dtc_best, X_test, y_test,normalize=\"true\", cmap=\"Blues\")\n\nDTCfpr, DTCtpr, DTCthresholds = roc_curve(y_test, y_test_prob_DTC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the tree in order to get insights\nfrom IPython.display import Image\nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = list(df.columns)\nfeatures.remove(\"Attrition\")\ndot_data = StringIO()\nexport_graphviz(dtc_best, out_file=dot_data, feature_names=features, filled=True)\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Logisitic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nlog_reg_model = LogisticRegression(max_iter=1000, solver = \"newton-cg\")\nlog_reg_model.fit(X_train, y_train)\n#Accuracy score of train\ny_train_pred = log_reg_model.predict(X_train)\ny_train_prob = log_reg_model.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_LR = log_reg_model.predict(X_test)\ny_test_prob_LR = log_reg_model.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Logistic Regression: \",roc_auc_score(y_test, y_test_prob_LR))\nprint(classification_report(y_test, y_test_pred_LR))\n#Plotting confusion matrix\nplot_confusion_matrix(log_reg_model, X_test, y_test, normalize=\"true\", cmap=\"Blues\")\nLRfpr, LRtpr, LRthresholds = roc_curve(y_test, y_test_prob_LR)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Random Forest Classifier model\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=700,max_depth=10,n_jobs=-1,random_state=123)\nrfc.fit(X_train,y_train)\n#Accuracy score of train\ny_train_pred = rfc.predict(X_train)\ny_train_prob = rfc.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_RFC = rfc.predict(X_test)\ny_test_prob_RFC = rfc.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Random Forest Classifier: \",roc_auc_score(y_test, y_test_prob_RFC))\n#Plotting confusion matrix\nplot_confusion_matrix(rfc, X_test, y_test, normalize=\"true\", cmap=\"Blues\")\nRFCfpr, RFCtpr, RFCthresholds = roc_curve(y_test, y_test_prob_RFC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting importance of attributes that are responsible for attrition according to the RFC model\ndef plot_feature_importance(importance,names,model_type): \n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data) \n    \n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True) \n    \n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\nplot_feature_importance(rfc.feature_importances_,X_train.columns,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results using ROC AUC score graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi=150)\nplt.plot(DTCfpr, DTCtpr, color='orange', label='Decision Tree Classifier ROC')\nplt.plot(LRfpr, LRtpr, color='blue', label='Logistic Regression ROC')\nplt.plot(RFCfpr, RFCtpr, color='red', label='Random Forest Classifier ROC')\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are sorry for the uncomfortable grid on the confusion matrix. Unknown issue that isn't shown on the final report**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}