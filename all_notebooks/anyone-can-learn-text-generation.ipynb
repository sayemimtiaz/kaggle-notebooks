{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"This is a tutorial on implementing Sequence Models using Deep Learning. Text Generation is the art of generating words or sentences which will follow the given input text.\nHere, Long Short-term Memory(LSTM) model is used for the purpose which serves a vital role in preserving the context over long period of time.\n\nThe scope of this tutorial are as follows:\n\n* Understanding Text Preprocessing\n* Understanding Implementation in PyTorch\n* Understanding solving NLP problems using Deep Learning\n* Understanding the workflow of Sequence Models\n\nHere is a nice tutorial on builiding models uisng PyTorch [Link](https://www.kaggle.com/ankitjha/the-ultimate-pytorch-guide)"},{"metadata":{},"cell_type":"markdown","source":"This is the abstract architecture of the model\n\n![LSTM](https://cdn-images-1.medium.com/max/1200/1*FCVyju8lPTvfFfxT-rzInA.png)\n\n\nThis tutorial has been divided into following sections:\n\n### Text Preprocessing\n* Concatenating data to form a single string as input data\n* Text Cleaning\n* Removing sentences containing digits\n* Spell correction of input text\n* Converting Text to lowercase\n\n### Feature Selection\n* Finding suitable Features Representation of input data\n* Converting input stream of words into feature vectors\n\n### Model Training and Evaluation\n* Defining the model architecture\n* Defining the loss function and optimizers\n* Training and validating the model\n\n### Generating Predictions\n* Generating predictions for a input string of words for next words of the input string"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Importing Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import Counter\n!pip install pyspellchecker\nimport re \nfrom wordcloud import WordCloud, STOPWORDS\nfrom spellchecker import SpellChecker\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom collections import Counter\n%matplotlib inline\nimport torch.utils.data\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/Womens Clothing E-Commerce Reviews.csv')\ndf.dropna(inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We will concatenate all the text in the *Review Text* column to form a single string to learn the language model for generating predictions for possible candidates of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to check presence of a digit in a string\ndef contains_digit(string):\n    return bool(re.search(r'\\d', string))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing\n\nThe Text preprocessing involves following steps:\n\n* Cleaning the text data(Removing extra fullstops and exclamation marks.)\n* Removing numbers because these can affect the performance of the model and lead to a larger embedding size.\n* Spell correction as review dataset contains many spelling mistakes. There are many spell correcting libraries. However, I have chosen [pyspellchecker](https://pypi.org/project/pyspellchecker/) which implements a simple spell checking algorithm designed by Peter Norvig.\n* Converting data into lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextPreprocessing:\n    def __init__(self, train):\n        self.text = ''\n        self.cleaned_text = []\n        self.input_text = ''\n        self.train = train\n        \n    def gettext(self, df, colname, num_cols):\n        if self.train == True:\n            #concatenating all the column values to form a single string\n            for line in df[colname].values[:num_cols]:\n                self.text += line\n        else:\n            for line in df[colname].values[num_cols:]:\n                self.text += line\n    \n    # Cleaning the text\n    def clean_text(self):\n        self.text = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", self.text)\n        fullstops = ['...', '..', '!!!', '!!', '!']\n        for s in fullstops:\n            self.text = self.text.replace(s, \".\")\n    \n    # Removing sentences with digits\n    def remove_numbers(self):\n        for sentence in self.text.split('.'):\n            if(contains_digit(sentence) == False):\n                self.cleaned_text.append(sentence.rstrip())\n        self.input_text = '.'.join(sentence for sentence in self.cleaned_text)\n    \n    # Spell correction of misspelled words\n    def spell_correction(self):\n        _spell = SpellChecker()\n        incorrect_words = []\n        correct_words = []\n        # Finding all words in the input text\n        res = re.findall(r'\\w+', self.input_text) \n        for word in tqdm(set(res)):\n            correct = _spell.correction(word)\n            if(word != correct):\n                incorrect_words.append(word)\n                correct_words.append(correct)\n        for i, word in enumerate(incorrect_words):\n            self.input_text = self.input_text.replace(word, correct_words[i])\n        return self.input_text.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will represent each unique word as a number or 1D scalar. We can also represent it using a multidimentional embedding but for simplicity in understanding, I am representing each word as a 1D scalar."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = TextPreprocessing(train = True)\ntext_data.gettext(df, 'Review Text', 100)\ntext_data.clean_text()\ntext_data.remove_numbers()\ninput_text = text_data.spell_correction()\n# Here, input text contains the preprocessed text.\ntrain_text = input_text.split()\n\n# Fetching data for validation\nvalid_text_data = TextPreprocessing(False)\nvalid_text_data.gettext(df, 'Review Text', -100)\nvalid_text_data.clean_text()\nvalid_text_data.remove_numbers()\nvalid_input_text = valid_text_data.spell_correction()\nvalid_text = valid_input_text.split()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we got the data as a list of words. Here we need to find a representation of these sequence of words to feed into the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nwordcloud(train_text, title=\"Word Cloud of Text\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n\nHere as a feature represetation of the data, I am generating a representation of each of the word using a unique integer value. For example, this sentence *['I', 'am', 'good', 'and', 'am', 'writing']* can be represented as: *[0, 1, 2, 3, 1, 4]*"},{"metadata":{"trusted":true},"cell_type":"code","source":"words_set = set(train_text).union(set(valid_text))\nkey_val = {}\nval_key = {}\nfor i, key in enumerate(words_set):\n    key_val[key] = i\n    val_key[i] = key","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{k: key_val[k] for k in list(key_val)[:5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{k: val_key[k] for k in list(val_key)[:5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nnum_timesteps = 32\ntrain_arr = []\nvalid_arr = []\ntrain_loss = []\nvalid_loss = []\nnum_epochs = 10\nnum_batches = int(len(train_text) / (batch_size * num_timesteps))\nvalid_num_batches = int(len(valid_text) / (batch_size * num_timesteps))\n\n# Size of embedding\nembedding_size = 64\n\n# Number of units in LSTM Layer\nnum_lstm_units = 64\n\n# Using cuda if present else cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading in PyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in train_text:\n    train_arr.append(key_val[word])\n    \n# Reshaping data to pass into model\ntrain_data = np.reshape(train_arr[:num_batches * batch_size * num_timesteps], (num_batches * batch_size, -1)) \ntarget = np.reshape(np.append(train_data[1:], train_data[0]), (num_batches * batch_size, -1))\nfor word in valid_text:\n    valid_arr.append(key_val[word])\nvalidation_features = np.reshape(valid_arr[:valid_num_batches * batch_size * num_timesteps], (valid_num_batches * batch_size, -1))\nvalidation_target = np.reshape(np.append(validation_features[1:], validation_features[0]), (valid_num_batches * batch_size, -1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us have a look at how the train features look"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data -> shape: num_entries * num_timesteps\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While loading the data, these point should be closely worked upon:\n\n* Setting a batch Size\n* Shuffling the data\n* Parallelizing the tasks using multiprocecssing workers. \n\nThe DataLoader function provides all of these features and you can specify them before heading to next task"},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting numpy array into torch tensor\ntrain = torch.from_numpy(train_data)\ntargets = torch.from_numpy(target) \n\nvalidation = torch.from_numpy(validation_features)\nvalidation_target = torch.from_numpy(validation_target) \n\ntrain_set = torch.utils.data.TensorDataset(train,targets)\nvalid_set = torch.utils.data.TensorDataset(validation, validation_target)\n\n\n#Loading data into Dataloader\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = False, num_workers = 4)\nvalidation_loader = torch.utils.data.DataLoader(valid_set, batch_size = batch_size, shuffle = False, num_workers = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\n\n![](https://cdn-images-1.medium.com/max/880/1*9BnXkVxnfNZ5bLDX2tza_A.png)\n\nHere,we are using **Many to Many RNN** as the number of input is equal to the number of output.\n\nHere, the feature vectors are passed through an LSTM layer followed by a dense layer. I have chosen a simple model just as a starter to check the performance.\nLSTM is composed of input gate, cell, output gate and forget gate. These are responsible for regulating flow of information across timesteps.\nAn indepth tutorial on LSTM is explained [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n![LSTM](https://cdn-images-1.medium.com/max/1600/1*Niu_c_FhGtLuHjrStkB_4Q.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################\n#             Defining the model                #\n#################################################\nclass Model(nn.Module):\n    def __init__(self, num_words, num_timesteps, num_lstm_units, embedding_size):\n        super(Model, self).__init__()\n        self.num_timesteps = num_timesteps\n        self.num_lstm_units = num_lstm_units\n        self.embedding = nn.Embedding(num_words, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, num_lstm_units, num_layers = 1, batch_first=True)\n        self.dense = nn.Linear(num_lstm_units, num_words)\n        \n    def forward(self, x, hidden_unit):\n        embeddings = self.embedding(x)\n        prediction, state = self.lstm(embeddings, hidden_unit)\n        logits = self.dense(prediction)\n        return logits, state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(len(words_set), num_timesteps,\n              embedding_size, num_lstm_units)\nmodel = model.to(device)\n\n# Defining the Loss Function\ncriterion = nn.CrossEntropyLoss()\n\n# Defining the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![LSTM](https://cdn-images-1.medium.com/max/1200/1*FCVyju8lPTvfFfxT-rzInA.png)\n\nAs described in the image, the feature vector of a word are passed into the LSTM which generates the output corresponding to the next word of the sentence. For example:\nIn the following sentence\n</br>\n**I am going there**\n\nThe *input* and **output** will be:\n\n* *FV(I)* -> **FV(am)**\n* *FV(am)* -> **FV(going)**\n* *FV(going)* -> **FV(there)**\n\nwhere FV stands for Feature Vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to train the model\n\ndef train(epoch):\n    #####################\n    #  Train the model  #\n    #####################\n    model.train()\n    tr_loss = 0\n    # initializing hidden state and cell state with zeros\n    h_t, c_t = torch.zeros(1, batch_size, num_lstm_units), torch.zeros(1, batch_size, num_lstm_units)\n    h_t.zero_()\n    c_t.zero_()\n    h_t, c_t = h_t.to(device), c_t.to(device)\n    for batch_idx, (X_train, y) in enumerate(train_loader):\n        X_train, y = Variable(X_train), Variable(y)\n        if torch.cuda.is_available():\n            X_train = X_train.to(device)\n            y = y.to(device)\n            \n        # clearning the gradients of all optimized variables\n        optimizer.zero_grad()\n        \n        # forward propogation: computing the outputs predicted by the model\n        # The output include the target word values(logits), and the hidden state(h_t) and cell state(c_t).\n        logits, (h_t, c_t) = model(X_train, (h_t, c_t))\n        # calculating the loss\n        loss = criterion(logits.transpose(1, 2), y)\n        h_t = h_t.detach()\n        c_t = c_t.detach()\n        # backpropogation: computing loss with respect to model parameters\n        loss.backward()\n        # performing a single step of optimization\n        optimizer.step()\n        tr_loss += loss.item()\n    train_loss.append(tr_loss / len(train_loader))\n    print('Train Epoch: {} \\tTrain Loss: {:.6f}'.format(epoch, (tr_loss / len(train_loader))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to validate the model\n\ndef evaluate(data_loader):\n    ################################\n    #    Evaluating the Model      #\n    ################################\n    model.eval()\n    h_t, c_t = torch.zeros(1, batch_size, num_lstm_units), torch.zeros(1, batch_size, num_lstm_units)\n    h_t.zero_()\n    c_t.zero_()\n    h_t, c_t = h_t.to(device), c_t.to(device)\n    loss = 0\n    for data, target in data_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.to(device)\n            target = target.to(device)\n        logits, (h_t, c_t) = model(data, (h_t, c_t))\n        loss += criterion(logits.transpose(1, 2), target).item()\n        \n    loss /= len(data_loader.dataset)\n    valid_loss.append(loss)    \n    print('Train Epoch: {} \\tValidation Loss: {:.6f}'.format(epoch, loss / len(data_loader)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing number of epochs\nn_epochs = 10\nfor epoch in range(n_epochs):\n    #train the model\n    train(epoch)\n    #evaluate the model\n    evaluate(validation_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for plotting train and validation loss for each epoch\ndef plot_graphs(train_loss, valid_loss, epochs):\n    plt.style.use('ggplot')\n    fig = plt.figure(figsize=(20,4))\n    ax = fig.add_subplot(1, 2, 1)\n    plt.title(\"Train Loss\")\n    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('train_loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')\n    ax = fig.add_subplot(1, 2, 2)\n    plt.title(\"Validation Loss\")\n    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='test')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('vaidation _loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting train and validation loss\nplot_graphs(train_loss, valid_loss, n_epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Predictions\n\nThis is time to generate predictions using our trained model.\n\nHere we will pass the feature vector of a string as an input and the output will be the word which can come at the end of the string."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nh_t, c_t = torch.zeros(1, batch_size, num_lstm_units), torch.zeros(1, batch_size, num_lstm_units)\nh_t.zero_()\nc_t.zero_()\nh_t, c_t = h_t.to(device), c_t.to(device) \nwords = []\nfinal_predictions = []\n#count = 0\n##################################################\n# Generating predictions for next timestep  #\n##################################################\nfor i, (word, target_variable) in enumerate(validation_loader):\n #   if(count == 3):\n #       break\n    word = word.to(device)\n    numpy_word = word.cpu().numpy()\n    output, (h_t, c_t) = model(word, (h_t, c_t))\n    words = numpy_word.ravel()\n    predictions = torch.topk(output[0], k=3)[1].tolist()\n    print('**********************************************************************************')\n    input_string = []\n    for word in words[-25:-15]:\n        input_string.append(val_key[word])\n    s = ' '.join(input_string)\n    print('The input string is:\\t {}'.format(s))\n    string = []\n    string.append(val_key[predictions[0][0]])\n    string.append(val_key[predictions[0][1]])\n    string.append(val_key[predictions[0][2]])\n    print('\\nTop 3 predictions Timestep i + 1 for the input string are are {}\\n'.format(string))\n    print('**********************************************************************************')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}