{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction, Exploratory Analysis\nData Science is a hyped job in recent times. This notebook contains an analysis of job posts in Data Science in 2019 in the US to go in deeper to what is actually required to land such a job. \n\nTo see the visualizations, press the play button on each cell to advance. Being able to pin-point where and the amount of available jobs can give a better understanding of the state of the job market."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport plotly.graph_objects as go\nimport plotly.express as px\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is rather small, collected by scraping data from different job boards/platforms for US in the year 2019 (from Feb 2019 to October 2019)."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = None #1000 # specify 'None' if want to read whole file\nusa_jobs = pd.read_csv('/kaggle/input/data_scientist_united_states_job_postings_jobspikr.csv', delimiter=',', nrows = nRowsRead)\nusa_jobs.dataframeName = 'data_scientist_united_states_job_postings_jobspikr.csv'\nnRow, nCol = usa_jobs.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like and see the first and last dates of the posts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"usa_jobs.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"usa_jobs['crawl_timestamp'] = pd.to_datetime(usa_jobs['crawl_timestamp'])\n#Finding earliest and latest posting\nprint(f\"Earliest job post in the set: {min(usa_jobs['crawl_timestamp'])}\")\nprint(f\"Latest post in the set: {max(usa_jobs['crawl_timestamp'])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observing the demand of various hard skills and programming languages. Each time a word is mentioned in the job description, its values in the dictionary are incremented.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"requirements = {\"powerbi\":0, \" r \":0, \"tableau\":0, \"qlikview\":0, \"python\":0, \"sql\":0, \"machine learning\":0,'linux':0, 'c#':0, \\\n\" ml \":0, \"hive\":0, \"spark\":0, \"hadoop\":0, \"java\":0, \"scala\":0, \"kafka\":0, \"bachelor\":0, \"master\":0, \"phd\":0, 'year':0, 'years':0, \"c++\":0}\nfor i in range(len(usa_jobs)):\n    job_description = usa_jobs.job_description[i].lower().replace(\"\\n\", \" \")\n    for k in requirements:\n        if k in job_description:\n            requirements[k] += 1\n#print(requirements['machine learning'])\n#print(requirements[' ml '])\nrequirements['machine learning'] += requirements[' ml ']\nrequirements['year'] += requirements['years']\nrequirements['years experience'] = requirements.pop('year')\ndel requirements['years']\ndel requirements[' ml ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\nsorted_req = OrderedDict(sorted(requirements.items(), key=lambda x:x[1]))\nplt.figure(figsize=(10, 10))\nplt.bar(range(len(sorted_req)), list(sorted_req.values()), align='center')\nplt.xticks(range(len(sorted_req)), list(sorted_req.keys()), rotation='vertical')\nplt.xlabel(\"job requirement\")\nplt.ylabel(\"Number of posts\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, several technical skills and programming languages are in high demand in this field, but the most required is to have years of relevant working experience. The reason there are more appearances of 'years experience' than actual job postings is that I summed up the entries 'year' and 'years' together (in one single post, these two words could appear separately, asking for different skills, increasing the importance of the years of experience)"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Optional cell, if interested in \n# from collections import Counter\n# print(Counter(usa_jobs.inferred_state))\n# print(Counter(usa_jobs.salary_offered))\n# print(Counter(usa_jobs.job_board).most_common())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we are going to create a wordcloud with the most sought-after skills in the Data Science domain!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english'))\ndescription_example = usa_jobs.job_description[2].lower()\nword_tokens = word_tokenize(description_example)\nfiltered_description = [w for w in word_tokens if not w in stop_words]\nfiltered_description = \" \".join(filtered_description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom matplotlib.pyplot import plot\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#sorted_req","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_descriptions = \" \".join(job_description.lower() \n                      for job_description in usa_jobs.job_description)\nstopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is necessary to remove all the irrelevant stopwords which usually appear in most of the job posts. I have done this by iteratively removing words which I considered not interesting for the required skill set"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.update(['experience', 'following', 'candidates', 'big', 'background','developing', 'characteristics', 'data', 'team', 'data', 'scientist', 'strong', 'project', \n                  'solution', 'technology', 'science', 'model', 'knowledge','skill', 'work', 'build', 'will', 'knowledge', 'application','gender', 'identity', 'equal',\n                  'opportunity','related','field', 'without', 'regard', 'national', 'origin', 'religion', 'sex', 'race', 'color', 'veteran', 'status','sexual',\n                  'orientation','opportunity', 'employer', 'qualified','applicant','skills', 'job', 'summary', 'advanced', 'system', 'applicants', 'receive', 'large', 'best', 'practice', 'problem'\n                 , 'processing', 'affirmative', 'action', 'employment', 'consideration', 'receive', 'united', 'state', 'programming', 'computer', 'working', 'saying', \n                  'preferred', 'qualification', 'disability', 'protected', 'structured', 'unstructured', 'problems', 'technical', 'internal', 'external', 'non',\n                 'subject', 'matter', 'please', 'apply', 'using', 'dental', 'reasonable', 'accomodation', 'join', 'us', 'tools', 'individuals', 'disabilities'\n                 , 'type', 'full', 'wide', 'range', 'duties', 'responsibilities', 'stakeholder', 'oral', 'written', 'ideal', 'candidate', 'ability', 'qualifications', 'well',\n                  'must', 'able', 'unit', 'member', 'posted', 'today', 'service', 'clearance', 'days', 'ago', 'high', 'quality', 'level', 'every', 'use', 'case', 'additional'])\nwordcloud = WordCloud(stopwords=stopwords, background_color='white',\n                     width=1000, height=700).generate(aggregate_descriptions)\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very interesting! Even though before we focused on the technical skills, we can see that they make up a minority in the actual job description. Technologies like python, SQL and machine learning are still visible but many other skills are required, such as: decision making, attention to detail, read people and many more."},{"metadata":{"trusted":true},"cell_type":"code","source":"states = usa_jobs.inferred_state.unique()\nsum_in_states = []\nfor state in states:\n    total_jobs_state = len(usa_jobs[usa_jobs['inferred_state']==state])\n    sum_in_states.append(int(total_jobs_state))\njobs_in_states = {'state':states, 'Total jobs':sum_in_states}\njobs_in_states = pd.DataFrame(jobs_in_states)\njobs_in_states = jobs_in_states.sort_values(by='Total jobs', ascending=False)\njobs_in_states = jobs_in_states.reset_index(drop=True)\njobs_in_states = jobs_in_states.drop(jobs_in_states.index[len(jobs_in_states)-1])\njobs_in_states[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see the top 10 states with the most jobs. Let's see them plotted on the US map as well!"},{"metadata":{"trusted":true},"cell_type":"code","source":"latitude = [32.318231,35.20105,34.048928,36.778261,39.550051,41.603221,\n38.905985,38.910832,27.664827,32.157435,19.898682,41.878003,44.068202,\n40.633125,40.551217,39.011902,37.839333,31.244823,42.407211,39.045755,\n45.253783,44.314844,46.729553,37.964253,32.354668,46.879682,35.759573,\n47.551493,41.492537,43.193852,40.058324,34.97273,38.80261,43.299428,\n40.417287,35.007752,43.804133,41.203322,41.580095,33.836081,43.969515,\n35.517491,31.968599,39.32098,37.431573,44.558803,47.751074,43.78444,\n38.597626,43.075968, 38.895]\nlongitude = [-86.902298,-91.831833,-111.093731,-119.417932,-105.782067,\n-73.087749,-77.033418,-75.52767,-81.515754,-82.907123,-155.665857,-93.097702,\n-114.742041,-89.398528,-85.602364,-98.484246,-84.270018,-92.145024,-71.382437,\n-76.641271,-69.445469,-85.602364,-94.6859,-91.831833,-89.398528,-110.362566,\n-79.0193,-101.002012,-99.901813,-71.572395,-74.405661,-105.032363,-116.419389,\n-74.217933,-82.907123,-97.092877,-120.554201,-77.194525,-71.477429,-81.163725,\n-99.901813,-86.580447,-99.901813,-111.093731,-78.656894,-72.577841,-120.740139,\n-88.787868,-80.454903,-107.290284, -77.0366]\nstate_names = ['Alabama','Arkansas','Arizona','California','Colorado','Connecticut',\n'District of columbia','Delaware','Florida','Georgia','Hawaii','Iowa',\n'Idaho','Illinois','Indiana','Kansas','Kentucky','Louisiana','Massachusetts',\n'Maryland','Maine','Michigan','Minnesota','Missouri','Mississippi',\n'Montana','North carolina','North dakota','Nebraska','New hampshire',\n'New jersey','New mexico','Nevada','New york','Ohio','Oklahoma','Oregon',\n'Pennsylvania','Rhode island','South carolina','South dakota','Tennessee',\n'Texas','Utah','Virginia','Vermont','Washington','Wisconsin','West virginia',\n'Wyoming', 'Washington d.c.']\nstate_dict = {'state':state_names, 'latitude':latitude, 'longitude':longitude}\nstate_df = pd.DataFrame(state_dict, columns=['state', 'latitude', 'longitude'])\nstate_coords = pd.merge(state_df, jobs_in_states, how='right', on='state')\nstate_coords = state_coords.sort_values(by='Total jobs', ascending=False)\nstate_coords = state_coords.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_geo(data_frame=state_coords, lat='latitude', scope='north america', hover_name='state',\n                    lon='longitude', size='Total jobs', projection='hammer')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_of_posting = []\nfor i in range(len(usa_jobs)):\n    month_of_posting.append(usa_jobs['crawl_timestamp'][i].month)\nusa_jobs['month'] = month_of_posting\nmonths = [x for x in range(2, 11)]\nsum_in_months = []\nfor month in months:\n    total_jobs_in_month = len(usa_jobs[usa_jobs['month']==month])\n    sum_in_months.append(total_jobs_in_month)\njobs_in_months = {'month':months, 'Total jobs':sum_in_months}\njobs_in_months = pd.DataFrame(jobs_in_months)\n#dropping the last month of october, because it is not fully included in this set\njobs_in_months = jobs_in_months.drop([8])\njobs_in_months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months_plot = go.Figure()\nmonths_plot.add_trace(go.Scatter(x=jobs_in_months.month, \n                                y=jobs_in_months['Total jobs']))\nmonths_plot.update_layout(title='US job posts in Data science by month in 2019',\n                         xaxis_title='Month', yaxis_title='Amount of job posts')\nmonths_plot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only months February to September are completely available, it's a pity that we do not have the entire year included in the data but we can still distinguish a strong rise in the summer months for employment offers.  \n## The part below is still work-in-progress. Stay tuned!\nLooking for years of relevant experience"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom word2number import w2n\nimport statistics\ndef search_text_left_of_word(text, word, n):\n    \"\"\"Searches for a text and retrieves n words on left side of the text\"\"\"\n    words = re.findall(r'\\w+', text)\n    try:\n        index = words.index(word)\n    except ValueError:\n        return \" \"\n    return words[index - n:index]\ndef search_year_word(text):\n    return text.find('year')\ndef search_number_around_word(word_surroundings):          #this function adds all the numbers found to a list. It also converts the words to numbers, if it is the case\n    word_surroundings = \" \".join(word_surroundings)\n    word_surroundings = word_tokenize(word_surroundings)\n    pos_tags = nltk.pos_tag(word_surroundings)\n    numbers_list = []\n    for a in pos_tags:\n        if a[1] in 'CD':\n            if a[0].isalpha():       #sometimes the numbers are written as words, e.g. 'Three' instead of 3\n                try:\n                    numbers_list.append(w2n.word_to_num(a[0]))\n                except ValueError:\n                    return \"\"\n            else:\n                numbers_list.append(a[0])\n    return numbers_list\nyears_experience_req = []\n\ndef convert_to_int(list_elem):\n    try:\n        converted_int = int(list_elem)\n        if converted_int <= 10:\n            return int(list_elem)\n    except ValueError:\n        return\nfor post_index in range (len(usa_jobs)):\n    current_job = usa_jobs.job_description[post_index]\n    word_surroundings = search_text_left_of_word(current_job, 'years', 2)\n    if current_job.find(' year ') > -1:\n        years_experience_req.append(['1'])\n    years_experience_req.append(search_number_around_word(word_surroundings))\n    #print(post_index, search_number_around_word(word_surroundings))\nyears_experience_req = [convert_to_int(item) for sublist in years_experience_req for item in sublist]\nyears_experience_req = [i for i in years_experience_req if i != None]\n#print(years_experience_req)\nprint(\"An average of \", statistics.mean(years_experience_req), \"  years is required in most job offerings. \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Just checking if the last 7's in the data actually correspond to \nusa_jobs.job_description[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statistics.mean([3, 4, None])","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}