{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# About\nComing from a SQL background and entering the world of machine learning I soon stumbled upon Pandas. It is considered a basic skill for data arranging and preprocessing when programming in python. When you are learning python from skretch it is probabily pretty obvious how to use the library. But with an SQL biased mind as mine it might be quite a bit confusing in some places. \n\nBy now I'm using the library for about two years as part of my ML hobby in my spare time, but I'm still spending a lot of time on stackoverflow to get things done. It's time to digg into Pandas, and recalibrate my awareness how to deal with data. \n\nThis notebook I translate some common and some advanced techniques from SQL to pandas step-by-step. I didn't just want to write a plain cheat sheet (actually Pandas has a good one to get started: [Comparison SQL](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html)). I also wanted to unwind some concepts that might be helpful for a SQL developer who now and then deals deals with pandas.\n\n# Technical setup\n\nThe scripts will load the data into Pandas DataFrames (e.g. `df_colors`) and also into SQL tables (e.g. `colors`). You can choos between **SQLite** (because it's part of the basic python setup) or **BigQuery** (because it's fun to learn something new).\nThe code below is written so you can use either SQL database.  Though you need to make very few changes to the code if you use BigQuery. I marked the code with a `#BQ` or `--BQ` where you have to adjust. \n\nThese are changes you have to make when using BigQuery:\n- set the `USE_BIGQUERY` flag to `True`,\n- uncomment the `<<` in the `%%sql` magic line where SQL output is written to a DataFrame (The notebook overrides the `%%bigquery` magic as `%%sql` so the code doesn't need to be changed any further.).\n- Connect your google account to the notebook session (in the notebook menu: Add-ons ==> Google Cloud Services).\n- Set your cloud project_id.\n\nAdditional changes when running BQ on kaggle:\n- On the first run %%BIGQUERY didn't work because of credential issues. The next day after restarting the notebook and browser it just worked.\n- Setting the `default_dataset` on the magic didn't work on kaggle notebooks. So you have to add the dataset in the SQL-code cells to every table (e.g. `colors` --> `lego.colors`).\n\n> **Warning !**\n> \n> If you use **BigQuery** keep in mind that the scripts might generate costs and you get charged! Don't use it in Production environment or with an account pointing to a Production environment!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BQ:\n\n# Either use BigQuery (True) or SQLite (False)\nUSE_BIGQUERY = False\n# Set your own gcp project id here\nBQ_PROJECT = 'XXXXXXXXXXXX'\n\n## BigQuery related settings\n# if using BigQuery, drop and recreate tables (True)\nRECREATE_BQ_TABLES = True\nDATASET_ID = 'lego'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!sqlite3 lego.db .databases .quit","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import sqlite3\nimport sqlalchemy\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n# import sql2pandas_config\n\n#https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries\nfrom google.cloud import bigquery\nfrom google.cloud.bigquery import magics\n\n# Path to Google credentials (*.json)\n# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=sql2pandas_config.GOOGLE_APPLICATION_CREDENTIALS\nin_path = Path('../input/lego-database')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# ==== CONFIG ====\n\n# sql magic\nif USE_BIGQUERY:\n\n    %load_ext google.cloud.bigquery\n    # define %%sql as an alias for %%bigquery\n    %alias_magic sql bigquery\n\nelse:\n    # load jupyter extention\n    %load_ext sql\n    # disable autocommit\n    %config SqlMagic.autocommit=False\n\n#%matplotlib inline\n#%pylab inline\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# ==== Set up BigQuery ====\n\nif USE_BIGQUERY:\n\n    client = bigquery.Client(project=BQ_PROJECT) \n\n    default_config = bigquery.QueryJobConfig(\n        default_dataset = f'{BQ_PROJECT}.{DATASET_ID}',\n        maximum_bytes_billed = 100000000\n    )\n    client.default_query_job_config = default_config\n    \n    # setting config to the %%sql magic \n    magics.context.default_query_job_config = default_config\n    magics.context.project = BQ_PROJECT\n    \n    ## if %%bigquery magic is not working try to restart browser and notebook or experiment with:\n    # https://google-auth.readthedocs.io/en/latest/user-guide.html#obtaining-credentials\n    #import google.oauth2.credentials\n    #credentials = google.oauth2.credentials.Credentials('access_token')\n    #magics.context.credentials = credentials\n    \n    # create data 'lego' set if not exists\n    ds_exists = False\n\n    for ds in list(client.list_datasets()):\n        if ds.dataset_id == DATASET_ID:\n            ds_exists = True\n            print(f'Dataset {BQ_PROJECT}.{ds.dataset_id} already exists.')\n            break;\n\n    if not ds_exists:\n        dataset = bigquery.Dataset(f'{BQ_PROJECT}.{DATASET_ID}')\n        dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n        print(f'Created dataset {BQ_PROJECT}.{dataset.dataset_id}')  ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# ==== Set up SQLite ====\n\nif not USE_BIGQUERY:\n    %sql sqlite:///lego.db\n    \n    # Test connection\n    v1 = \"Ready to go!\"\n    %sql df_test << SELECT :v1 as \"Test\"\n    assert(df_test.DataFrame().values[0]==v1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# ==== Loading the data into DataFrames and into SQL database ====\n# Load dataframes\n\ndf_colors = pd.read_csv(in_path/'colors.csv')\ndf_inventories = pd.read_csv(in_path/'inventories.csv')\ndf_inventory_parts = pd.read_csv(in_path/'inventory_parts.csv')\ndf_inventory_sets = pd.read_csv(in_path/'inventory_sets.csv')\ndf_part_categories = pd.read_csv(in_path/'part_categories.csv')\ndf_parts = pd.read_csv(in_path/'parts.csv')\ndf_sets = pd.read_csv(in_path/'sets.csv')\ndf_themes = pd.read_csv(in_path/'themes.csv')\n\ntable_dict = {'colors': df_colors, \n             'inventories': df_inventories, \n             'inventory_parts': df_inventory_parts, \n             'inventory_sets': df_inventory_sets, \n             'part_categories': df_part_categories, \n             'parts': df_parts, \n             'sets': df_sets, \n             'themes': df_themes}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Load sql tables\n\nif USE_BIGQUERY:\n# https://cloud.google.com/bigquery/docs/pandas-gbq-migration#loading_a_pandas_dataframe_to_a_table\n    \n    if RECREATE_BQ_TABLES:\n        for tbl in table_dict:\n            job = client.delete_table(table= f'lego.{tbl}', not_found_ok = True)\n            #wait for job to finish\n            if job!=None: \n                job.result()\n            job = client.load_table_from_dataframe(table_dict[tbl], f'lego.{tbl}')\n            #wait for job to finish\n            job.result()\n    else:\n        print('Skipped table init')\n\n    display(client.query('select count(*) from `lego.colors` limit 1000', project=BQ_PROJECT).to_dataframe())\n\nelse:\n    # SQLite\n    for tbl in table_dict:\n        table_dict[tbl].to_sql(tbl, con='sqlite:///lego.db', if_exists='replace', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Table of contents\n\n* [SQL and Pandas](#sql_and_pandas)\n* [Missing bricks](#missing_bricks)\n* [A simple Filter *(The behaviour of brackets.)*](#simple_filter)\n* [Indexing *(What actually is an index?)*](#indexing)\n* [Joins *(Why merge doesn't mean upsert.)*](#joins)\n* [Conditional Joins and Aggregation (*Almost done!*)](#agg)\n* [Recursion (*Lost in trees?!*)](#rec)\n* [Summary *(Got it!)*](#sum)\n* [References](#ref)\n\n<a id=\"sql_and_pandas\"></a>\n# SQL ~~vs.~~ and Pandas\n\nI love SQL. It's been around for decades to arrange and analyse data. Data is kept in tables which are stored in a relational structure. Consistancy and data integraty is kept in mind when designing a relational data model.\n\nHowever, when it comes to machine learning other data structures such as matrices and tensors become important to feat the underlying algorithms and make data processing more efficient.\n\nThat's where Pandas steps in. From a SQL-developer perspective it is the library to close the gap between your data storage and the ml frameworks.\n\n<a id=\"missing_bricks\"></a>\n# Missing bricks\n\nFirst listen to this imaginary dialogue that guides us throug the coding:\n\n<span style=\"color:green\">*I miss all red bricks of the Lego Pizzeria. I definetly need a new one.*</span>\n\n<span style=\"color:blue\">*Don't worry. We can try to solve this with data. That will be fun. :-)*</span>\n\n<span style=\"color:green\">*!@#%&*</span>\n\nNow that we have the database set up and a mission we are ready to figuere out how to deal with missing bricks.\nFirst we inspect the tables. They are organized as shown in the relational diagram."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image.open(in_path/'downloads_schema.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are colors, parts, sets and inventories. We should start by searching for the *Pizzeria* in the `sets` table using the set number (*41311*).\n![Pizzeria](https://raw.githubusercontent.com/joatom/blog-resources/main/handful_bricks/img/piz.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"simple_filter\"></a>\n# A simple Filter *(The behaviour of brackets.)*\nA simple `like`-filter on the `sets` table will return the set info.\n\n```sql\nSELECT *\n  FROM sets s\n WHERE s.set_num like '41311%'\n```\n"},{"metadata":{},"cell_type":"markdown","source":"There are several ways to apply a filter in Pandas. The most SQL-like code utilizes the `query`-function which basicaly substitutes the `where` clause."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.query(\"set_num.str.contains('41311')\", engine='python')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the `query` function expects a String as input parameter we loose syntax highlighting and syntax checking for our filter expression.\n\nTherefore a more commonly used expression consists of the **bracket notation** (The behaviour of the bracket notation of a class in python is implementated in the class function `__getitem__`)\n\nSee how we can apply an equals filter using brackets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.query(\"set_num == '41311-1'\")\n\n# or\n\ndf_sets[df_sets.set_num == '41311-1']\n\n# or\n\ndf_sets[df_sets['set_num'] == '41311-1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot going on in this expression.\n\nLet's take it apart.\n\n`df_sets['set_num']` returns a single column (a Pandas.Series object). A Pandas Dataframe is basically a collection of Series. Additionaly there is a row index (often just called *index*) and a column index (*columnnames*). Think of a column store database.\n![DF](https://raw.githubusercontent.com/joatom/blog-resources/main/handful_bricks/img/df.png)"},{"metadata":{},"cell_type":"markdown","source":"Applying a boolean condition (`== '41311-1'`) to a Series of the DataFrame (`df_sets['set_num']`) will result in a boolean collection of the size of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"bool_coll = df_sets['set_num'] == '41311-1'\n\n# only look at the position 3580 - 3590 of the collection\nbool_coll[3580:3590]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boolean collection now gets past to the DataFrame and filters the rows:\n```python\ndf_sets[bool_coll]\n# or \ndf_sets[df_sets['set_num'] == '41311-1']\n```\nDepending on what type of object we pass to the square brackets the outcome result in very different behaviours. \n\nWe have already seen in the example above that if we pass a **boolean collection** with the size of number of rows, the collection is been handled as a **row filter**. \n\nBut if we pass a column name or a **list of column** names to the brackets instead, the given columns are selected like in the `SELECT` clause of an SQL statement. \n```sql\nSELECT name,\n       year\n  FROM lego.sets;\n```\n=> \n```python\ndf_sets[['name', 'year']]\n```\n\nRow filter and column selection can be combined like this:\n```sql\nSELECT s.name,\n       s.year\n  FROM lego.sets s\n WHERE s.set_num = '41311-1';\n```\n=>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp = df_sets[df_sets['set_num'] == '41311-1']\ndf_temp[['name','year']]\n\n# or simply:\n\ndf_sets[df_sets['set_num'] == '41311-1'][['name','year']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"indexing\"></a>\n# Indexing *(What actually is an index?)*\nAnother way to access a row in pandas is by using the row index. With the `loc` function (and brackets) we select the *Pizzeria* and another arbitrary set. We use the row numbers to filter the rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.loc[[236, 3582]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we inspect the DataFrame closely we realize that it doesn't realy look like a simple table but rather like a **cross table**. \n\nThe first column on the left is a row index and the table header is the column index. In the center the values of the columns are displayed. \n\nIf we think of the values as a matrix the rows are dimension 0 and columns are dimension 1. The dimension is often used in DataFrame functions as `axis` parameter. E.g. dropping columns can be done using dimensional information:\n\n```sql\n-- EXCEPT in SELECT clause only works with BIGQUERY\nSELECT s.* EXCEPT s.year\n  FROM lego.sets s;\n```\n\n==> "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# %%sql SELECT s.* EXCEPT s.year FROM lego.sets s LIMIT 5;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.drop(['year'], axis = 1).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The indexes can be accessed with the `index` and `columns` variable. `axes` contains both."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.index # row index\ndf_sets.columns # column index\n\ndf_sets.axes # both","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The row index doesn't necessarely be the row number. We can also convert a column into a row index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.set_index('set_num').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also possible to define hierarchicle indicies for multi dimensional representation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.set_index(['year', 'set_num']).sort_index(axis=0).head() # axis = 0 => row index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sometimes it is usefull to reset the index, hence reset the row numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.loc[[236, 3582]].reset_index(drop = True) # set drop = False to keep the old index as new column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we get a sence what is meant by an index in pandas in contrast to SQL.\n\n**Indices in SQL** are hidden data structures (in form of e.g. b-trees or hash-tables). They are built to **access data more quickly**, to avoiding full table scans when appropriate or to mantain consistancies when used with constraints.\n\nAn **index in Pandas** can rather be seen as a **dimensional access** to the data values. They can be distingueshed between row and column indices."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"joins\"></a>\n# Joins *(Why merge doesn't mean upsert.)*\n\n<span style=\"color:green\">*What are we gonna do now about my missing parts?*</span>\n\n<span style=\"color:blue\">*We don't have all the information we need, yet. We need to join the other tables.*</span>\n\nThough there is a function called `join` to join DataFrames I always use the `merge` function. This can be a bit confusing, when you are used to Oracle where *merge* means upsert/updelete rather then combining two tables.\n\nWhen combining two DataFrames with the `merge` function in Pandas we have to define the relationship more explicitly. If your used to SQL thats what you want. \n\nIn contrast the `join` function implicitly combines the DataFrames by their index or column names. It also enables multiply DataFrame joins in one statement as long as the join columns are matchable by name.\n```sql\nSELECT * \n  FROM sets s\n INNER JOIN\n       inventories i\n    ON s.set_num = i.set_num -- USING (set_num)\nLIMIT 5\n```"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%sql\nSELECT * \n  FROM sets s\n INNER JOIN\n       inventories i\n    ON s.set_num = i.set_num \nLIMIT 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those Pandas statements all do the same:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets.merge(df_inventories, how = 'inner', on = 'set_num').head(5)\n\n#or if columns are matching\n\ndf_sets.merge(df_inventories, on = 'set_num').head(5)\n\n#or explicitly defined columns\n\ndf_sets.merge(df_inventories, how = 'inner', left_on = 'set_num', right_on = 'set_num').head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see witch parts are needed for the *Pizzeria* we combine some tables. We look for the inventory of the set and gather all parts. Then we get color and part category information.\n\nWe end up with an inventory list:\n```sql\nSELECT s.set_num, \n       s.name set_name, \n       p.part_num, \n       p.name part_name, \n       ip.quantity,\n       c.name color,\n       pc.name part_cat\n  FROM sets s,\n       inventories i,\n       inventory_parts ip,\n       parts p,\n       colors c,\n       part_categories pc\n WHERE s.set_num = i.set_num\n   AND i.id = ip.inventory_id\n   AND ip.part_num = p.part_num\n   AND ip.color_id = c.id\n   AND p.part_cat_id = pc.id \n   AND s.set_num in ('41311-1')\n   AND i.version = 1\n   AND ip.is_spare = 'f'\n ORDER BY p.name, s.set_num, c.name\nLIMIT 10\n```"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%sql\nSELECT s.set_num, \n       s.name set_name, \n       p.part_num, \n       p.name part_name, \n       ip.quantity,\n       c.name color,\n       pc.name part_cat\n  FROM sets s,\n       inventories i,\n       inventory_parts ip,\n       parts p,\n       colors c,\n       part_categories pc\n WHERE s.set_num = i.set_num\n   AND i.id = ip.inventory_id\n   AND ip.part_num = p.part_num\n   AND ip.color_id = c.id\n   AND p.part_cat_id = pc.id \n   AND s.set_num in ('41311-1')\n   AND i.version = 1\n   AND ip.is_spare = 'f'\n ORDER BY p.name, s.set_num, c.name\nLIMIT 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create a general `inventory_list` as view and make the statement more readable and comparable with ANSI-syntax (separating filters/predicates from join conditions).\n```sql\nDROP VIEW IF EXISTS inventory_list;\nCREATE VIEW inventory_list AS\nSELECT s.set_num, \n       s.name set_name, \n       s.theme_id,\n       s.num_parts,\n       p.part_num, \n       ip.quantity,\n       p.name part_name, \n       c.name color,\n       pc.name part_cat\n  FROM sets s\n INNER JOIN\n       inventories i\n USING (set_num)\n INNER JOIN\n       inventory_parts ip\n    ON (i.id = ip.inventory_id)\n INNER JOIN\n       parts p\n USING (part_num)\n INNER JOIN\n       colors c\n    ON (ip.color_id = c.id)\n INNER JOIN\n       part_categories pc\n    ON (p.part_cat_id = pc.id)\n WHERE i.version = 1\n   AND ip.is_spare = 'f';\n```"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%sql\n-- BQ: add schema infront of table, e.g. lego.colors\nDROP VIEW IF EXISTS inventory_list;\nCREATE VIEW inventory_list AS\nSELECT s.set_num, \n       s.name set_name, \n       s.theme_id,\n       s.num_parts,\n       p.part_num, \n       ip.quantity,\n       p.name part_name, \n       c.name color,\n       pc.name part_cat\n  FROM sets s\n--  FROM lego.sets s\n INNER JOIN\n       inventories i\n--       lego.inventories i\n USING (set_num)\n INNER JOIN\n       inventory_parts ip\n--       lego.inventory_parts ip\n    ON (i.id = ip.inventory_id)\n INNER JOIN\n       parts p\n--       lego.parts p\n USING (part_num)\n INNER JOIN\n       colors c\n--       lego.colors c\n    ON (ip.color_id = c.id)\n INNER JOIN\n       part_categories pc\n--       lego.part_categories pc\n    ON (p.part_cat_id = pc.id)\n WHERE i.version = 1\n   AND ip.is_spare = 'f';\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we translate the view to pandas. We see how the structure relates to sql. The parameter `how` defines the type of join (here: `inner`), `on` represents the `USING` clause whereas `left_on` and `right_on` stand for the SQL `ON` condition.\n\nIn SQL usualy an optimizer defines based in rules or statistics the execution plan (the order in which the tables are accessed, combined and filtered). I'm not sure if pandas follows a similar approache. To be safe, I assume the order and early column dropping might matter for performance and memory management."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_inventory_list = df_sets[['set_num', 'name', 'theme_id', 'num_parts']] \\\n                    .merge(\n                    df_inventories[df_inventories['version'] == 1][['id', 'set_num']],\n                        how = 'inner',\n                        on = 'set_num'\n                    ) \\\n                    .merge(\n                    df_inventory_parts[df_inventory_parts['is_spare'] == 'f'][['inventory_id', 'part_num', 'color_id', 'quantity']],\n                        how = 'inner',\n                        left_on = 'id',\n                        right_on = 'inventory_id'\n                    ) \\\n                    .merge(\n                    df_parts[['part_num', 'name', 'part_cat_id']],\n                        how = 'inner',\n                        on = 'part_num'\n                    ) \\\n                    .merge(\n                    df_colors[['id', 'name']],\n                        how = 'inner',\n                        left_on = 'color_id',\n                        right_on = 'id'\n                    ) \\\n                    .merge(\n                    df_part_categories[['id', 'name']],\n                        how = 'inner',\n                        left_on = 'part_cat_id',\n                        right_on = 'id'\n                    )\n\n# remove some columns and use index as row number (reset_index)\ndf_inventory_list = df_inventory_list.drop(['id_x', 'inventory_id', 'color_id', 'part_cat_id', 'id_y', 'id'], axis = 1).reset_index(drop = True)\n\n# rename columns\ndf_inventory_list.columns = ['set_num', 'set_name', 'theme_id', 'num_parts', 'part_num', 'quantity', 'part_name', 'color', 'part_cat']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lots of code here. So we better check if our pandas code matches the results of our SQL code.\n\nSelect the inventory list for our example, write it to a dataframe (`df_test_from_sql`) and compare the results.\n```sql\ndf_test_from_sql <<\nSELECT il.* \n  FROM inventory_list il\n WHERE il.set_num in ('41311-1')\n ORDER BY \n       il.part_name, \n       il.set_num, \n       il.color\nLIMIT 10;\n```"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%sql df_test_from_sql <<\nSELECT il.* \n  FROM inventory_list il\n WHERE il.set_num in ('41311-1')\n ORDER BY \n       il.part_name, \n       il.set_num, \n       il.color\nLIMIT 10;\n-- BQ: remove <<","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_from_sql","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\ndf_test_from_df = df_inventory_list[df_inventory_list['set_num'].isin(['41311-1'])].sort_values(by=['part_name', 'set_num', 'color']).head(10)\n\ndf_test_from_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assert equals\n\nif not USE_BIGQUERY:\n    df_test_from_sql = df_test_from_sql.DataFrame()\n\npd._testing.assert_frame_equal(df_test_from_sql, df_test_from_df.reset_index(drop = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are equal as expected.\n\nSince we are only interested in the red bricks we create a list of those missing parts.\n```sql\nSELECT *\n  FROM inventory_list il\n WHERE il.set_num = '41311-1'\n   AND part_cat like '%Brick%'\n   AND color = 'Red'\n ORDER BY \n       il.color, \n       il.part_name, \n       il.set_num;\n```"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%sql\nSELECT *\n  FROM inventory_list il\n WHERE il.set_num = '41311-1'\n   AND part_cat like '%Brick%'\n   AND color = 'Red'\n ORDER BY \n       il.color, \n       il.part_name, \n       il.set_num;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to watchout for the brackets when combining filters in Dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_missing_parts = df_inventory_list[(df_inventory_list['set_num'] == '41311-1') & \n                                     (df_inventory_list['part_cat'].str.contains(\"Brick\")) &\n                                     (df_inventory_list['color'] == 'Red')\n                                    ].sort_values(by=['color', 'part_name', 'set_num']).reset_index(drop = True)\n\ndf_missing_parts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">*There we go, we are missing one 2x2 brick and tw0 2x2 double convex.*</span>\n\n<span style=\"color:green\">*Yup, that's the roof of the fireplace. I knew that before.*</span>\n\n<a id =\"agg\"></a>\n# Conditional Joins and Aggregation *(Almost done!)*\n\nNext we search for sets that contain the missing parts. The quantity of the parts in the found sets must be greater or equal the quantity of the missing parts.\n\nIn SQL it is done with an **conditional join** `il.quantity >= mp.quantity`.\n\n```sql\nsets_with_missing_parts << \n\n-- A list of missing parts\n\nWITH missing_parts AS (\n  SELECT il.set_name,\n         il.part_num, \n         il.part_name,\n         il.color,\n         il.quantity\n    FROM inventory_list il\n   WHERE il.set_num = '41311-1'\n     AND il.part_cat like '%Brick%'\n     AND il.color = 'Red'\n)\n\n-- Looking for set that contains as much of the missing parts as needed\n\nSELECT mp.set_name as searching_for_set,\n       il.set_num, \n       il.set_name, \n       il.part_name,\n       -- total number of parts per set\n       il.num_parts,\n       -- how many of the missing parts were found in the set \n       COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set\n  FROM inventory_list il\n INNER JOIN\n       missing_parts mp\n    ON (il.part_num = mp.part_num\n        AND il.color = mp.color\n        -- searching for a set that contains at least as much parts as there are missing\n        AND il.quantity >= mp.quantity\n       )\n -- don't search in the Pizzeria set\n WHERE il.set_num <> '41311-1'\n -- prioritize sets with all the missing parts and as few parts as possible\n ORDER BY \n       matches_per_set DESC, \n       il.num_parts, \n       il.set_num,\n       il.part_name\nLIMIT 16\n```"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%sql sets_with_missing_parts << \nWITH missing_parts AS (\n  SELECT il.set_name,\n         il.part_num, \n         il.part_name,\n         il.color,\n         il.quantity\n    FROM inventory_list il\n   WHERE il.set_num = '41311-1'\n     AND il.part_cat like '%Brick%'\n     AND il.color = 'Red'\n)\nSELECT mp.set_name as searching_for_set,\n       il.set_num, \n       il.set_name, \n       il.part_name,\n       il.num_parts,\n       COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set\n  FROM inventory_list il\n INNER JOIN\n       missing_parts mp\n    ON (il.part_num = mp.part_num\n        AND il.color = mp.color\n        AND il.quantity >= mp.quantity\n       )\n WHERE il.set_num <> '41311-1'\n ORDER BY \n       matches_per_set DESC, \n       il.num_parts, \n       il.set_num,\n       il.part_name\nLIMIT 16;\n\n-- BQ: remove <<","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conditional Join\n\nThere is no intuitive way to do a conditional join on DataFrames. The easiest I've [seen](https://stackoverflow.com/questions/23508351/how-to-do-workaround-a-conditional-join-in-python-pandas) so far is a two step solution.\nAs substitution for the SQL `WITH`-clause we can reuse `df_missing_parts`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. merge on the equal conditions\ndf_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = 'inner', on = ['part_num', 'color'], suffixes = ('_found', '_missing'))\n# 2. apply filter for the qreater equals condition\ndf_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts['quantity_found'] >= df_sets_with_missing_parts['quantity_missing']]\n\n# select columns\ncols = ['set_num', 'set_name', 'part_name', 'num_parts']\ndf_sets_with_missing_parts = df_sets_with_missing_parts[['set_name_missing'] + [c + '_found' for c in cols]]\ndf_sets_with_missing_parts.columns = ['searching_for_set'] + cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregation\n\nIn the next step the aggregation of the analytic function \n```sql\nCOUNT(*) OVER (PARTITION BY il.set_num) matches_per_set\n```\nneeds to be calculated. Hence the number of not-NaN values will be counted per `SET_NUM` group and assigned to each row in a new column (`matches_per_set`).\n\nBut before translating the analytic function, let's have a look at a regular aggregation, first. Say, we simply want to count the entries per `set_num` on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this:\n```sql\nSELECT s.set_num,\n       COUNT(*) AS matches_per_set\n       SUM(s.num_parts) AS total_num_parts\n  FROM ...\n WHERE ...\n GROUP BY \n       s.set_num;\n```\nAll selected columns must either be aggregated by a function (`COUNT`, `SUM`) or defined as a group (`GROUP BY`).\nThe result is a two column list with the group `set_num` and the aggregations `matches_per_set` and `total_num_part`.\n\nNow see how the counting is done with pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets_with_missing_parts.groupby(['set_num']).count()  .sort_values('set_num', ascending = False)\n\n# for sum and count:\n# df_sets_with_missing_parts.groupby(['set_num']).agg(['count', 'sum']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, that's different! The aggregation function is applied to every column independently and the group is set as row index. \nBut it is also possible to define the aggregation function for each column explicitly like in SQL:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sets_with_missing_parts.groupby(['set_num'], as_index = False) \\\n    .agg(matches_per_set = pd.NamedAgg(column = \"set_num\", aggfunc = \"count\"), \n         total_num_parts = pd.NamedAgg(column = \"num_parts\", aggfunc = \"sum\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks more familiar. With the `as_index` argument the group becomes a column (rather than a row index).\n\nSo, now return to our initial task translating the `COUNT(*) OVER(PARTITION BY)` clause. One approach could be to join the results of the above aggregated DataFrame with the origanal dataframe, like\n```python\ndf_sets_with_missing_parts.merge(my_agg_df, on = 'set_num')\n```\nA more common why is to use the `transform()` function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add aggregatiom\ndf_sets_with_missing_parts['matches_per_set'] = df_sets_with_missing_parts.groupby(['set_num'])['part_name'].transform('count')\n\ndf_sets_with_missing_parts.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's elaborate the magic that's happening.\n```python\ndf_sets_with_missing_parts.groupby(['set_num'])['part_name']\n```\nreturns a `GroupByDataFrame` which contains the group names (from `set_num`) and all row/column indicies and values related to the groups. Here only one column `['part_name']` is selected. In the next step [`transform` applies](https://github.com/pandas-dev/pandas/blob/v1.1.4/pandas/core/groupby/generic.py#L514) the given function (`count`) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group.\n\n\nNow that we have gathered all the data we arange the results so that they can be compared to the SQL data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort and pick top 16\ndf_sets_with_missing_parts = df_sets_with_missing_parts.sort_values(['matches_per_set', 'num_parts', 'set_num', 'part_name'], ascending = [False, True, True, True]).reset_index(drop = True).head(16)\n\ndf_sets_with_missing_parts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assert equals\n\nif not USE_BIGQUERY:\n    sets_with_missing_parts = sets_with_missing_parts.DataFrame()\n\npd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are matching!\n\n<span style=\"color:blue\">We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-)</span>\n\n<span style=\"color:green\">(#@ยง?$!*#) Are you sure your data is usefull for anything?</span>"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"rec\"></a>\n# Recursion *(Lost in trees?)*\nWe solved the red brick problem. But since we have the data already open, let's have a closer look at the *Fire Engine*, set number *336-1*.\n```sql\nSELECT s.name AS set_name,\n       s.year,\n       t.id,\n       t.name AS theme_name,\n       t.parent_id\n  FROM sets s,\n       themes t\n WHERE t.id = s.theme_id\n   AND s.set_num = '336-1';\n```    "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%sql\n\nSELECT s.name AS set_name,\n       s.year,\n       t.id,\n       t.name AS theme_name,\n       t.parent_id\n  FROM sets s,\n       themes t\n WHERE t.id = s.theme_id\n   AND s.set_num = '336-1';\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *fire engine* is quiet old (from 1968) and it belongs to the theme *Fire*. \nThe `themes` table also includes as column called `parent_id`. This suggests that `themes` is a hierarchical structure.\nWe can check this with an recursive `WITH`-clause in SQL. (*BQ: recursive WITH is not implemented in BIGQUERY. Instead you can try [LOOPs](https://towardsdatascience.com/advent-of-code-sql-bigquery-31e6a04964d4)*)\n```sql\nWITH RECURSIVE hier(name, parent_id, level) AS ( \n    \n    -- init recursion\n    SELECT t.name, \n           t.parent_id, \n           0 AS level \n      FROM themes t \n     WHERE id = 376\n    \n    UNION ALL\n    \n    -- recursive call\n    SELECT t.name, \n           t.parent_id, \n           h.level +1 \n      FROM themes t, \n           hier h \n     WHERE t.id = h.parent_id\n    \n)\nSELECT COUNT(1) OVER() - level AS level, \n       name as theme, \n       GROUP_CONCAT(name,' --> ') over(order by level desc) path\n  FROM hier \n ORDER BY level;\n```"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%sql\nWITH RECURSIVE hier(name, parent_id, level) AS ( \n    \n    -- init recursion\n    SELECT t.name, \n           t.parent_id, \n           0 AS level \n      FROM themes t \n     WHERE id = 376\n    \n    UNION ALL\n    \n    -- recursive call\n    SELECT t.name, \n           t.parent_id, \n           h.level +1 \n      FROM themes t, \n           hier h \n     WHERE t.id = h.parent_id\n    \n)\nSELECT COUNT(1) OVER() - level AS level, \n       name as theme, \n       GROUP_CONCAT(name,' --> ') over(order by level desc) path\n  FROM hier \n ORDER BY level;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, that looks like a reasonable hierarchie. The `path` column includes the parents and grant parents of a theme.\nWhat if we want to reverse the order of the path. Unfortunately `GROUP_CONCAT` in SQLite doesn't allow us to specify a sort order in the aggregation.\nIt's possible to add custom aggregation function in some databases. In SQLite we can compile [application defined function](https://www.sqlite.org/appfunc.html) or in Oracle we can define [customized aggregation function](https://docs.oracle.com/cd/B28359_01/appdev.111/b28425/aggr_functions.htm) even at runtime as types.\n\nQuiet some steps need to be taken to make the database use costumized aggregation efficently, hence we can use them like regulare aggregation and windowing function. In Oracle for instance we have to define:\n1. initial values: \n    ```plsql\n    total := 0; \n    n := 0;\n    ```\n2. calculation per iteration step: \n    ```plsql\n    total := total + this_step_value; \n    n := n + 1;\n    ```\n3. deletion per iteration for windowing: \n    ```plsql\n    total := total - removed_step_value; \n    n := n - 1;\n    ```\n4. merging for parallel execution:\n    ```\n    total := total_worker1 + total_worker2; \n    n := n_worker1 + n_worker2; \n    ```\n5. termination: \n    ```plsql\n    my_avg := total / nullif(n-1, 0)\n    ```"},{"metadata":{},"cell_type":"markdown","source":"Now how ar we gonna do this in Pandas? We start traversing the hierarchie."},{"metadata":{"trusted":true},"cell_type":"code","source":"fire_engine_info = df_themes[df_themes['id'] == 376].copy()\nfire_engine_info['level'] = 0\n\nparent_id = fire_engine_info.parent_id.values[0]\n\n\nlvl = 0\nwhile not np.isnan(parent_id) and lvl < 10:\n    lvl+= 1\n    new_info = df_themes[df_themes['id'] == parent_id].copy()\n    new_info['level'] = lvl\n    parent_id = new_info.parent_id.values[0]\n    fire_engine_info = fire_engine_info.append(new_info)\n\nfire_engine_info['grp']=0\nfire_engine_info    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the on hand this is pretty need, since we can do what ever we want in a manuely coded loop. On the other hand I doubt that it is very efficent when we have mto deal with lots of data. But to be fair, Recursive `WITH` isn't that fast either in SQL.\n\nFinaly, how are we doing customized aggregation. We could do it in the loop above or we can rather use the libraries `transform` or `apply` functions.\n\nWe define a custom aggregation function `cat_sorted` and then use the `apply` function like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_sorted(ser, df, val_col = None, sort_col = None):\n    u=' --> '.join(df[df.id.isin(ser)].sort_values(sort_col)[val_col].values)\n    return [u]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fire_engine_info.apply(lambda x: cat_sorted(x, fire_engine_info, 'name', 'level'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also apply rolling or windowing behaviour.\n> Note that a rolling representation or windowing on string values is not possible because Pandas only allows numeric values for those action."},{"metadata":{"trusted":true},"cell_type":"code","source":"fire_engine_info.rolling(10,min_periods=1)['level'].apply(lambda x: sum(10**x), raw = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we not only understand the numbers on the lego package but also have a better understandig of Pandas.\n\n<a id=\"sum\"></a>\n# Summary *(Got it!)*\n\nSQL stays my favourite language to access structured data arranged over many tables. Pandas shines when data already is gathered together and easily accessable (e.g. as csv file).\nThere are alternatives to Pandas to build ml pipelines, such as [Dask](https://docs.dask.org/en/latest/) or [CUDF](https://docs.rapids.ai/api/cudf/stable/). But learning Pandas is a good foundation to learn more of them.\n\n<a id = \"ref\"></a>\n# References\n- The Lego dataset: https://www.kaggle.com/rtatman/lego-database\n- Loading datasets from kaggle: https://towardsdatascience.com/how-to-use-kaggle-datasets-in-google-colab-bca5e452a676\n- Jupyter sql magic: https://towardsdatascience.com/jupyter-magics-with-sql-921370099589\n- Setting up bigquery: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries\n- Bigquery and Pandas: https://cloud.google.com/bigquery/docs/pandas-gbq-migration"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}