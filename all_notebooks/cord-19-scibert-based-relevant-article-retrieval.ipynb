{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID 19: Using Novel Language Models to Effectively Identify Articles related to Therapeutics and Vaccines\n* Team: MD-Lab, ASU\n* Author: Ashwin Karthik Ambalavanan, Email: aambalav@asu.edu, Kaggle ID: ashwinambal96\n* Team Members: Rishab Banerjee, Hong Guan, Jitesh Pabla, Mihir Parmar, Murthy Devarakonda\n* Email ID: loccapollo@gmail.com, hguan6@asu.edu, jpabla1@asu.edu, mparmar3@asu.edu, Murthy.Devarakonda@asu.edu\n* Kaggle ID: loccapollo, hongguan, jiteshpabla, mihir3031, murthydevarakonda\n* This is a Team Submission\n* Here are the links to our teams Kernels: <br>\n[Using Novel Language Models and Web scraping to Effectively Identify Articles related to Therapeutics and Vaccines](https://www.kaggle.com/jiteshpabla/scoring-cord-19-using-google-training-on-scibert) <br>\n[Using Novel Language Models and elasticsearch to Effectively Identify Articles related to Therapeutics and Vaccines](https://www.kaggle.com/jiteshpabla/classifying-cord-19-articles-using-elasticbert/) <br>\n[COVID-19: BERT-based STS Method to Effectively Identify Articles related to Therapeutics and Vaccines](https://www.kaggle.com/mihir3031/bert-sts-for-searching-relevant-research-papers) <br>\n[Using embeddings from BERTModel, BioBERT, BertForSequenceClassification to classify articles related to Vaccines and Therapeutics](https://www.kaggle.com/loccapollo/lexicon-based-similarity-scoring-with-bert-biobert) <br>\n[Micro-scorers for COVID-19 Open Challenge](https://www.kaggle.com/hongguan/micro-scorers-for-covid-19-open-challenge/) <br>\nThe final ensembling that combines everything together: [Ensemble model for COVID-19 Open Challenge](https://www.kaggle.com/hongguan/ensemble-model-for-covid-19-open-challenge/) <br>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"### On March 19, 2020, the White House Office of Science and Technology Policy (WH-OSTP) issued a statement announcing the release of an extensive machine-readable collection of scientific articles about COVID-19, SARS-CoV-2, and the coronavirus group, jointly by several institutions including National Library of Medicine and Allen Institute for AI, and WH-OSTP: \n> .. [joined] the institutions in issuing a call to action to the Nation’s artificial intelligence experts to develop new text and data mining techniques that can help the science community answer high-priority scientific questions related to COVID-19\n### The dataset called COVID-19 Open Research Dataset (CORD-19) presently has nearly 59,000 articles (extracted from various archives), with more than 35,000 of which have full text. The institutions further compiled a series of questions to be answered. For example, some questions related to COVID-19 vaccines and therapeutics are:\n* Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n* Exploration of use of best animal models and their predictive value for a human vaccine.\n* Efforts targeted at a universal coronavirus vaccine.\n* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers"},{"metadata":{},"cell_type":"markdown","source":"![Vaccine and Therapeutics for COVID-19](https://qtxasset.com/fiercebiotech/1584710738/Screen%20Shot%202020-03-13%20at%2010.09.06%20AM.png/Screen%20Shot%202020-03-13%20at%2010.09.06%20AM.png?afFuGG0s3SKhTTkKAPfwWsMUeTddeDna)"},{"metadata":{},"cell_type":"markdown","source":"# Problems Addressed And Their Solution"},{"metadata":{},"cell_type":"markdown","source":"#### *Problem 1*: The process of screening clinically relevant publications remains a formidable challenge despite earlier efforts to improve accuracy of existing search engines themselves. \n#### *Problem 2*: The COVID-19 related questions are complex and the answers may be anywhere in the article's full text. \n#### *Solution*: We approach these problems in 2 steps:\n#### Step 1: We use the new generation of neural network, BERT (Bi-directional Encoder Representations from Transformers), pre-trained on biomedical corpus. Instead of using one model for the task, we will use a “crowd” of models each fine-tuned on a different but related task using existing publicly available training datasets. These models independently score an article for relevance to Treatment and Vaccine. Our hypothesis is that the “wisdom of the crowd” is more effective than a single approach.\n#### Step 2: We will mine relevant articles for specific information, such as the effectiveness, by identifying key passages that contain the information. In order to do so, we define a novel concept called semantic information availability (SIA) in a passage p relative to the question q, we propose to develop a dataset to train the BERT model to score SIA from a variety of passages and queries. Top-scored passages from the most relevant articles provide the answers to the question, as an extracted summary. This approach generalizes beyond COVID-19 dataset and can be used to automate information gathering for systematic reviews and other meta-analysis. (Aim to develop for June 16 deadline)"},{"metadata":{},"cell_type":"markdown","source":"#### ***Note*:** In this Kernel, we will tackle part of Step 1 by forming 2 powerful scorers which will contribute to the Crowd Wisdom mentioned above. The Crowd Wisdom which accumulates multiple scorers created by each member of our team (including the ones in this kernel) is available in this link (TBD)."},{"metadata":{},"cell_type":"markdown","source":"### All Code, Models, Data and Results mentioned in this Kernel are well documented and available in this [GitHub Link for Scorer 1](https://github.com/md-labs/covid19-kaggle/tree/master/document_nsp_text_sim) and this [GitHub Link for Scorer 2](https://github.com/md-labs/covid19-kaggle/tree/master/clinical_hedges_classification)\n\n#### *Note*: The code below had run on high performance GPU's and may take several hours to execute."},{"metadata":{},"cell_type":"markdown","source":"![BERT NSP Methodology](https://pytorch.org/tutorials/_images/bert1.png)"},{"metadata":{},"cell_type":"markdown","source":"# Scorer 1- Next Sentence Prediction for Article Identification:\n1. We model the problem using the Next Sentence Prediction (NSP) property that is used as a pretraining strategy in the novel transformer Language model- BERT ([paper](https://arxiv.org/abs/1810.04805)). We particularly use SciBERT ([link](https://github.com/allenai/scibert)) which is a  type of BERT that is pre-trained on a huge corpus of scientific articles from semantic scholar. \n2. NSP in BERT is pre-trained using the following format for input: `[CLS] + Text_A + [SEP] + Text_B + [SEP]`\n3. We use hand-picked sentences from BioMedical Papers from renowned conferences and use these as the query (Text_A). These queries are formulated such that they signify strongly Therapeutics and Vaccines separately (one for each). \n4. We then use the title + abstract + journal_id of the papers in the COVID dataset ([link](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)) as Text_B.\n5. This combined text (title + abstract + journal_id) is input along with the query of Vaccine and Therapeutics separately and a Feed Forward Network acts as the NSP head. The NSP Head takes as input, the embedding corresponding to the [CLS] token generated by SciBERT. Furthermore, with the help of a softmax layer we get the text similarity (probability) scores between the two text elements. \n6. A threshold of 0.999 is set and if the similarity score exceed this, we classify it as relevant to Treatment or Vaccine according to which query it is most similar to.  \n"},{"metadata":{},"cell_type":"markdown","source":"#### Next we will describe our manually formulated Vaccine and Therapeutic Queries formed by manually picking sentences from scientific articles from Elsevier and other medical publishers. "},{"metadata":{},"cell_type":"markdown","source":"#### Therapeutics Query:\n   Therapeutics is the branch of medicine concerned with the treatment of disease and the action of remedial agents. There is no specific antiviral therapy and treatment given by doctors is largely supportive, consisting of supplemental oxygen and conservative fluid administration. Drugs like Chloroquine, Hydroxychloroquine, Lopinavir, Ritonavir, Azithromycin and Tocilizumab are being prescribed by doctors in ICU testing. The drug Remdesivir has shown promise against other coronaviruses in animal models. Patients with respiratory failure require intubation. Patients in shock require urgent fluid resuscitation and administration of empiric antimicrobial therapy. Corticosteroid therapy is not recommended for viral pneumonia; however, use may be considered for patients with refractory shock or acute respiratory distress syndrome\n    \n#### Vaccine Query:\n   Vaccine is a substance used to stimulate the production of antibodies and provide immunity against diseases. They are treated to act as an antigen without inducing the disease. When the virulent version of an agent comes along, the immune system is prepared to respond due to the generation of B cells (memory and plasma cells), which will generate antibodies that will bind to pathogens and destroy them. Vaccine researchers are working on the development of a vaccine candidate expressing the viral spike protein of SARS-CoV-2 using a messenger RNA vaccine. Scientists are also focusing on the development of a chimpanzee adenovirus-vectored vaccine candidate against COVID-19. In addition, scientists are also working to see if vaccines developed for SARS coronavirus are effective against COVID-19.\n   \n### Note that the first sentence is a definition of the word therapeutics and vaccine in the respective queries. All subsequent sentences are formulated by keeping in mind that treatment and vaccine related to corona virus and covid-19 are emphasized"},{"metadata":{},"cell_type":"markdown","source":"#### The code below is tested using 4 variants of SciBERT FineTuning which are described below:\n1. Pretrained SciBERT model (provided by AllenAI) with NSP (without fine tuning) (Yes! The NSP head is randomly initialized in this step)\n\n2. Fine Tuned SciBERT Model using MLM on the Abstract / Title Text of the COVID Dataset and then use this for NSP (This is used for FineTuning the model using the Masked Language Model and Next Sentence Prediction Training Methods employed in the BERT paper. Intermediate FineTuning seems to help as mentioned in this work: [ULMFiT paper](https://arxiv.org/abs/1801.06146)). Refer this GitHub [link](https://github.com/md-labs/covid19-kaggle/tree/master/document_nsp_text_sim/src/lm_finetuning) for more details. (The NSP head is randomly initialized in this variant too)\n\n3. Fine Tuning the Pretrained SciBERT model in Variant 1 with the *Opioid NSP Question-Answer dataset and then use this model for NSP\n4. Fine Tuning the Fine-Tuned SciBERT model in Variant 2 with the *Opioid NSP Question-Answer dataset and then use this model for NSP\n\nEach of these models can be found [here](https://www.dropbox.com/sh/ko0d8jayaapb7xq/AABZ1yPVCLFuKUrPoBXBfjD0a?dl=0)\n\nLabels for the folders are self explanatory about which models depict which variant\n\n*The Opioid NSP Question-Answer dataset mentioned above is a weak and noisy dataset that is scraped from Reddit Opioid Forums. The dataset contains Questions asked my opioid addicts and the answers they have received in the forum. This data is used to fine tune the SciBERT model in order to initialize the weights of the NSP head.\n\n***Note***: Although the Opioid NSP QA Dataset doesn't directly complement the prediction time COVID-19 Data, our intuition is that it helps weakly initialize the NSP head to give more robust Similarity scores than randomly initializing it as in the first two variants. Better datasets like the Clinical STS are available (which complement the Prediction time COVID data) to use but we have another scorer that uses this dataset in a similar manner and hence we wanted to distinctly separate the two scorers to get different perspectives when Ensembling the scores as described above."},{"metadata":{},"cell_type":"markdown","source":"# Scorer 2- Classification for Article Identification:\n1. #### We model the problem as a text classification problem using the SciBERT model.\n\n2. #### Classification in BERT uses the following format for input: `[CLS] + Text_A + [SEP]`\n\n3. #### The [Clinical Hedges dataset](https://hiru.mcmaster.ca/hiru/HIRU_Hedges_home.aspx) is a set of articles which are manually annotated for a bunch of categories like Format (Original, Review, etc), Purpose (Treatment, Etiology, etc), Rigor (How closely the methodology in the paper correspond to the purpose of the paper) and whether the papers are related to Human Health Care (HHC). \n \n4. #### We use the Purpose column of this dataset (as described above) and label documents as Treatment and Not Treatment based on the annotations. Our model is then trained on this training dataset.\n \n5. #### Now, assuming our nsp-text-sim model (described above) performs perfectly, we use the text documents categorized as Treatment or Vaccine by that model as the input to the Clinical Hedges model (here) at prediction time. Our Clinical Hedges model categorizes the text as Treatment or Not Treatment and if our dataset contains only Treatment and Vaccine related documents, our model will effectively categorize it as Treatment and Not Treatment (or Vaccine) related documents. \n\n6. #### Our architecture consists of the SciBERT model (Fine Tuned on Clinical Hedges Data) with a FFN on top to give the probability scores for classification. Our Text Input to the model is the Title + Abstract + Journal as used previously"},{"metadata":{},"cell_type":"markdown","source":"![](https://yashuseth.files.wordpress.com/2019/06/fig1-1.png)"},{"metadata":{},"cell_type":"markdown","source":"### Error Analysis and Future Work:\n* Articles of different languages are included in the dataset and may have to be removed so that articles of interest are generalized to the English Domain\n* In the NSP modeling, fine tuning on the Opioid Dataset might be adding excessive noise to the model and hence might be contributing negatively in the Ensemble. Training with Clinical STS might show improvement in this work.\n* The Classification model is completely dependent on the performance of the NSP models and cannot be used to separate Vaccine and Therapeutics from Other articles on its own\n* FineTuning BERT with a lot of other related Clinical datasets would be a possible Future work to improve results significantly"},{"metadata":{"_uuid":"4c3500c9-1796-4a32-b0be-abf2f3558d6f","_cell_guid":"d6ff8a15-23cd-4acf-9dbb-1a8da3f329d2","trusted":true},"cell_type":"code","source":"\"\"\"\nSource: https://github.com/md-labs/covid19-kaggle/blob/master/document_nsp_text_sim/src/create_nsp_data.py\nCode to create the input to the BERT Next Sentence Prediction Model (run_nsp.py)\nInput:  vaccine_and_therapeutics_query.json, metadata.csv\nOutput: NSP Formatted Data in the form: [CLS] Query [SEP] (Title + Abstract) Text [SEP]\n\"\"\"\nimport os\nimport json\nimport csv\nimport shutil\nimport sys\nfrom document_nsp_text_sim.utils import ReadData\n\ncsv.field_size_limit(sys.maxsize)\n\npath_to_data = os.path.abspath(\"../data\")\n\n\ndef WriteDataNSP(dataset, path, directory):\n    if os.path.exists(os.path.join(path_to_data, directory)):\n        shutil.rmtree(os.path.join(path_to_data, directory))\n    os.mkdir(os.path.join(path_to_data, directory))\n    with open(os.path.join(path, directory, 'Test_Data.tsv'), 'w', newline='', encoding='utf-8') as fp:\n        writer = csv.writer(fp, delimiter='\\t')\n        writer.writerow(['ID', 'Text_A', 'Text_B', 'DEF'])\n        for row in dataset:\n            writer.writerow(row)\n\nRetCovData = ReadData(path_to_data + '/raw_data', \"metadata.csv\")\nvctdict = json.load(open(os.path.join(path_to_data, \"vaccine_and_therapeutics_query.json\")))\ndataset = []\nfor i, row in enumerate(RetCovData):\n    if i == 0:\n        continue\n    if row[2] == '' or row[3] == '':\n        continue\n    dataset.append([row[3] + 'V', vctdict['vaccine'], ' '.join((row[3] + ' ' + row[8] + ' ' + row[11]).split('\\n')),\n                    'VC'])\n    dataset.append([row[3] + 'T', vctdict['therapeutics'], ' '.join((row[3] + ' ' + row[8] + ' ' + row[11]).split('\\n')),\n                    'TR'])\nWriteDataNSP(dataset, path_to_data, 'COVID_NSP_Data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The run_nsp code below, is run using the following command:\n`python src/run_nsp.py --model_file=models/scibert_scivocab_uncased --bert_model=models/scibert_scivocab_uncased --do_lower_case --task_name=covid --data_dir=data/COVID_NSP_Data --learning_rate=2e-5 --num_train_epochs=10 --output_dir=models/scibert_scivocab_uncased/ --cache_dir=./BERT_CACHE --eval_batch_size=16 --max_seq_length=512 --train_batch_size=16 --do_eval`\n#### For more details visit this GitHub ([link](https://github.com/md-labs/covid19-kaggle/tree/master/document_nsp_text_sim))"},{"metadata":{"_uuid":"df134edc-cb55-4157-9b96-795197e10b9f","_cell_guid":"c49858fb-d0a6-4974-9e39-c65a34941180","trusted":true},"cell_type":"code","source":"\"\"\"\nSource: https://github.com/md-labs/covid19-kaggle/blob/master/document_nsp_text_sim/src/run_nsp.py\nBERT finetuning runner for Next Sentence Prediction\nInput: [CLS] Query [SEP] (Title + Abstract) Text [SEP]\nOutput: Probability of similarity between Query and Text\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport csv\nimport logging\nimport os\nimport random\nimport sys\nimport copy\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom pytorch_pretrained_bert.modeling import BertForNextSentencePrediction, BertConfig, WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for i, line in enumerate(reader):\n                if i == 0:\n                    continue\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass COVIDProcessor(DataProcessor):\n    \"\"\"Processor for the CLPsych data set.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"Train_Data.tsv\")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Train_Data.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Dev_Data.tsv\")), \"dev\")\n\n    def get_test_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Test_Data.tsv\")), \"test\")\n\n    def get_labels(self):\n        \"\"\"See base class.\"\"\"\n        return [\"VC\", \"TR\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n        random.seed(42)\n        req = list()\n        for i in range(0, len(lines)):\n            req.append(i)\n        req_final = random.sample(req, len(lines))\t\n        for i in req_final:\n            # print(lines[i])\n            guid = lines[i][0]\n            text_a = lines[i][1]\n            text_b = lines[i][2]\n            label = lines[i][-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples   \n\n\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer, output_mode=\"classification\"):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    label_map = {label : i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with \"- 2\"\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where \"type_ids\" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the \"sentence vector\". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n        segment_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [\"[SEP]\"]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        if output_mode == \"classification\":\n            label_id = label_map[example.label]\n        elif output_mode == \"regression\":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n\n        if ex_index < 5:\n            logger.info(\"*** Example ***\")\n            logger.info(\"guid: %s\" % (example.guid))\n            logger.info(\"tokens: %s\" % \" \".join(\n                    [str(x) for x in tokens]))\n            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n            logger.info(\n                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_id))\n    return features\n\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(\"--req_pretrained\", default=\"True\", type=str, required=True)\n    parser.add_argument(\"--model_dir\", default=None, type=str, required=True)\n    parser.add_argument(\"--data_dir\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n    parser.add_argument(\"--vocab_dir\", default=\"\", type=str,\n                        required=True,\n                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n    parser.add_argument(\"--task_name\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The name of the task to train.\")\n    parser.add_argument(\"--output_dir\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n\n    ## Other parameters\n    parser.add_argument(\"--cache_dir\",\n                        default=\"\",\n                        type=str,\n                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n    parser.add_argument(\"--max_seq_length\",\n                        default=512,\n                        type=int,\n                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n                             \"than this will be padded.\")\n    parser.add_argument(\"--do_train\",\n                        action='store_true',\n                        help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\",\n                        action='store_true',\n                        help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\"--do_lower_case\",\n                        action='store_true',\n                        help=\"Set this flag if you are using an uncased model.\")\n    parser.add_argument(\"--train_batch_size\",\n                        default=32,\n                        type=int,\n                        help=\"Total batch size for training.\")\n    parser.add_argument(\"--eval_batch_size\",\n                        default=32,\n                        type=int,\n                        help=\"Total batch size for eval.\")\n    parser.add_argument(\"--learning_rate\",\n                        default=5e-5,\n                        type=float,\n                        help=\"The initial learning rate for Adam.\")\n    parser.add_argument(\"--num_train_epochs\",\n                        default=3.0,\n                        type=float,\n                        help=\"Total number of training epochs to perform.\")\n    parser.add_argument(\"--warmup_proportion\",\n                        default=0.1,\n                        type=float,\n                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n                             \"E.g., 0.1 = 10%% of training.\")\n    parser.add_argument(\"--no_cuda\",\n                        action='store_true',\n                        help=\"Whether not to use CUDA when available\")\n    parser.add_argument(\"--local_rank\",\n                        type=int,\n                        default=-1,\n                        help=\"local_rank for distributed training on gpus\")\n    parser.add_argument('--seed',\n                        type=int,\n                        default=42,\n                        help=\"random seed for initialization\")\n    parser.add_argument('--gradient_accumulation_steps',\n                        type=int,\n                        default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n    parser.add_argument('--fp16',\n                        action='store_true',\n                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n    parser.add_argument('--loss_scale',\n                        type=float, default=0,\n                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n                             \"0 (default value): dynamic loss scaling.\\n\"\n                             \"Positive power of 2: static loss scaling value.\\n\")\n    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n    args = parser.parse_args()\n\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    processors = {\n\t\t\"covid\": COVIDProcessor,\n    }\n\n    num_labels_task = {\n        \"covid\": 2,\n    }\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend='nccl')\n    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n                            args.gradient_accumulation_steps))\n\n    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    if not args.do_train and not args.do_eval:\n        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n        \n\n    task_name = args.task_name.lower()\n\n    if task_name not in processors:\n        raise ValueError(\"Task not found: %s\" % (task_name))\n\n    processor = processors[task_name]()\n    print(processor)\n    num_labels = num_labels_task[task_name]\n    print(num_labels)\n    label_list = processor.get_labels()\n    print(label_list)\n    \n    tokenizer = BertTokenizer.from_pretrained(args.vocab_dir, do_lower_case=args.do_lower_case)\n\n    train_examples = None\n    num_train_optimization_steps = None\n    if args.do_train:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_optimization_steps = int(\n            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n        if args.local_rank != -1:\n            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n\n    # Prepare model\n    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n    model = BertForNextSentencePrediction.from_pretrained(args.model_dir,\n              cache_dir=cache_dir)\n    if args.fp16:\n        model.half()\n    model.to(device)\n    if args.local_rank != -1:\n        try:\n            from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        model = DDP(model)\n    elif n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.learning_rate,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=args.learning_rate,\n                             warmup=args.warmup_proportion,\n                             t_total=num_train_optimization_steps)\n\n    global_step = 0\n    nb_tr_steps = 0\n    tr_loss = 0\n    if args.do_train:\n        train_features = convert_examples_to_features(\n            train_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(\"***** Running training *****\")\n        logger.info(\"  Num examples = %d\", len(train_examples))\n        logger.info(\"  Batch size = %d\", args.train_batch_size)\n        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n        for ep in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n            model.train()\n            tr_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n            for step, batch in enumerate(train_dataloader):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                loss = model(input_ids, segment_ids, input_mask, label_ids)\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps\n\n                if args.fp16:\n                    optimizer.backward(loss)\n                else:\n                    loss.backward()\n\n                tr_loss += loss.item()\n                nb_tr_examples += input_ids.size(0)\n                nb_tr_steps += 1\n                if (step + 1) % args.gradient_accumulation_steps == 0:\n                    if args.fp16:\n                        # modify learning rate with special warm up BERT uses\n                        # if args.fp16 is False, BertAdam is used that handles this automatically\n                        lr_this_step = args.learning_rate * warmup_linear(global_step/num_train_optimization_steps, args.warmup_proportion)\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr_this_step\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n            eval_examples = processor.get_dev_examples(args.data_dir)\n            eval_features = convert_examples_to_features(\n                eval_examples, label_list, args.max_seq_length, tokenizer)\n            print(\"\\n\")\n            print(\"Running evaluation for epoch: {}\".format(ep))\n            all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n            all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n            all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n            all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n            eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n            # Run prediction for full data\n            eval_sampler = SequentialSampler(eval_data)\n            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    \t\t\t\t\t\n            model.eval()\n            eval_loss, eval_accuracy = 0, 0\n            nb_eval_steps, nb_eval_examples = 0, 0\n    \n            for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n                input_ids = input_ids.to(device)\n                input_mask = input_mask.to(device)\n                segment_ids = segment_ids.to(device)\n                label_ids = label_ids.to(device)\n    \n                with torch.no_grad():\n                    tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n                    logits = model(input_ids, segment_ids, input_mask)\n    \n                logits = logits.detach().cpu().numpy()\n                label_ids = label_ids.to('cpu').numpy()\n                tmp_eval_accuracy = accuracy(logits, label_ids)\n    \n                eval_loss += tmp_eval_loss.mean().item()\n                eval_accuracy += tmp_eval_accuracy\n    \n                nb_eval_examples += input_ids.size(0)\n                nb_eval_steps += 1\n    \n            eval_loss = eval_loss / nb_eval_steps\n            eval_accuracy = eval_accuracy / nb_eval_examples\n            loss = tr_loss/nb_tr_steps if args.do_train else None\n            result = {'eval_loss': eval_loss,\n                      'eval_accuracy': eval_accuracy,\n                      'global_step': global_step,\n                      'loss': loss}\n    \n            for key in sorted(result.keys()):\n                print(key, str(result[key]))\n            print()\n            \n    if args.do_train and args.do_eval:\n        # Save a trained model and the associated configuration\n        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n        with open(output_config_file, 'w') as f:\n            f.write(model_to_save.config.to_json_string())\n        # Load a trained model and config that you have fine-tuned\n        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n        config = BertConfig(output_config_file)\n        model = BertForNextSentencePrediction(config)\n        model.load_state_dict(torch.load(output_model_file))\n    elif args.do_train:\n        # Save a trained model and the associated configuration\n        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n        with open(output_config_file, 'w') as f:\n            f.write(model_to_save.config.to_json_string())\t\n    else:\n        # Load a trained model and config that you have fine-tuned\n        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n        config = BertConfig(output_config_file)\n        if args.req_pretrained == \"True\":\n            model = BertForNextSentencePrediction.from_pretrained(args.model_dir,\n                                                                  cache_dir=cache_dir)\n        else:\n            model = BertForNextSentencePrediction(config)\n            model.load_state_dict(torch.load(output_model_file))\n\n    model.to(device)\n\n    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        eval_examples = processor.get_test_examples(args.data_dir)\n        eval_features = convert_examples_to_features(\n            eval_examples, label_list, args.max_seq_length, tokenizer)\n        complete_user_ids = list()\n        for example in eval_examples:\n            complete_user_ids.append(example.guid)\n        logger.info(\"***** Running evaluation *****\")\n        logger.info(\"  Num examples = %d\", len(eval_examples))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        complete_label_ids = list()\n        complete_outputs = list()\n        complete_probs = list()\n        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            label_ids = label_ids.to(device)\n            with torch.no_grad():\n                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n                logits = model(input_ids, segment_ids, input_mask)\n\n            last_layer_op = copy.deepcopy(logits)\n            logits = logits.detach().cpu().numpy()\n            print(logits)\n            sm = torch.nn.Softmax()\n            probabilities = sm(last_layer_op)\n            probabilities = probabilities.detach().cpu().numpy()\n            label_ids = label_ids.to('cpu').numpy()\n            tmp_eval_accuracy = accuracy(logits, label_ids)\n            outputs = np.argmax(logits, axis=1)\n            complete_outputs.extend(outputs)\n            complete_label_ids.extend(label_ids)\n            complete_probs.extend(probabilities[:, 1])\n\n        outcsv = open(os.path.join(args.output_dir, \"Reqd_Labels.csv\"),'w', encoding = 'utf8', newline='')\n        writer = csv.writer(outcsv,quotechar = '\"')\n        writer.writerow([\"ID\", \"Probs\"])\n        for user,true, pred, probs in zip(complete_user_ids, complete_label_ids, complete_outputs, complete_probs):\n            writer.writerow([user, probs])","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSource: https://github.com/md-labs/covid19-kaggle/blob/master/document_nsp_text_sim/src/analyze_nsp_results.py\nCode to read the output csv from run_nsp.py and metadata.csv. It filters papers based on a threshold set as 0.999 for\neither Treatment or Vaccine Queries. It assigns label based on whichever is higher beyond 0.999 and O if it isn't beyond\nthe threshold.\nInput: metadata.csv, Reqd_Labels.csv from run_nsp.py\nOutput: Threshold Filtered Results formatted as 'Title', 'Text', 'Label', 'Prob_T', 'Prob_V'\n\"\"\"\n\n\nfrom document_nsp_text_sim.utils import ReadData\nimport os\nimport csv\nimport sys\n\npath_to_data = os.path.abspath(\"../data\")\npath_to_results = os.path.abspath(\"../results\")\ncsv.field_size_limit(sys.maxsize)\n\n\ndef main():\n    RetCovData = ReadData(path_to_data + '/raw_data', \"metadata.csv\")\n    RetCovDict = dict()\n    text = []\n    for i, row in enumerate(RetCovData):\n        if i == 0:\n            continue\n        RetCovDict[row[3]] = ' '.join((row[3] + ' ' + row[8] + ' ' + row[11]).split('\\n'))\n        text.append(' '.join((row[3] + ' ' + row[8] + ' ' + row[11]).split('\\n')))\n    labels = ReadData(os.path.join(path_to_results, \"nsp_results_final\"), \"Reqd_Labels_Before_FineTuning.csv\")\n    label_dict = dict()\n    for i, row in enumerate(labels):\n        if i == 0:\n            continue\n        if row[0][:-1] not in label_dict:\n            label_dict[row[0][:-1]] = dict()\n        label_dict[row[0][:-1]][row[0][-1]] = float(row[1])\n    count = 0\n    final_labels = []\n    for key in label_dict.keys():\n        if key == 'paper_id':\n            continue\n        dictionary = label_dict[key]\n        if 'T' not in dictionary or 'V' not in dictionary:\n            continue\n        th = 0.999\n        if dictionary['T'] > th or dictionary['V'] > th:\n            label = 'TR' if dictionary['T'] > dictionary['V'] else 'VC'\n            final_labels.append([key, RetCovDict[key], label, dictionary['T'], dictionary['V']])\n            count += 1\n        else:\n            final_labels.append([key, RetCovDict[key], 'O', dictionary['T'], dictionary['V']])\n    final_labels = list(sorted(final_labels, key=lambda x: x[3], reverse=True))\n    with open(os.path.join(path_to_results, 'filtered_results_final', 'Filter_Before_FineTuning.tsv'), 'w', newline='',\n              encoding='utf-8') as fp:\n        writer = csv.writer(fp, delimiter='\\t')\n        writer.writerow(['Title', 'Text', 'Label', 'Prob_T', 'Prob_V'])\n        for row in final_labels:\n            writer.writerow(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', -1)\nimport os\nimport csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata = dict()\nwith open(\"/kaggle/input/cord19metadata/metadata.csv\") as fp:\n    reader = csv.reader(fp)\n    for row in reader:\n        metadata[row[3]] = [row[0], row[8], row[11]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results From Scorer 1 After Filtering Results from each of the 4 Variants by setting a threshold similarity score of 0.999:"},{"metadata":{},"cell_type":"markdown","source":"# **Variant 1- SciBERT Before FineTuning:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-nsp-filtered-results/Filter_Before_FineTuning.tsv', delimiter='\\t')\ndf = df[['ID', 'Text', 'Label']]\ndf.columns = ['Title', 'Abstract', 'Label']\ndf['CORD_ID'] = df.apply(lambda row: metadata[row[0]][0] , axis=1)\ndf['Abstract'] = df.apply(lambda row: metadata[row[0]][1] , axis=1)\ndf['Journal'] = df.apply(lambda row: metadata[row[0]][2] , axis=1)\ndf = df.set_index('CORD_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TREATMENT CLASSIFIED ARTICLES\ndf[df['Label'] == 'TR'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VACCINE CLASSIFIED ARTICLES\ndf.loc[df['Label'] == 'VC'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **Variant 2- SciBERT After FineTuning:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-nsp-filtered-results/Filter_After_FineTuning.tsv', delimiter='\\t')\ndf = df[['ID', 'Text', 'Label']]\ndf.columns = ['Title', 'Abstract', 'Label']\ndf['CORD_ID'] = df.apply(lambda row: metadata[row[0]][0] , axis=1)\ndf['Abstract'] = df.apply(lambda row: metadata[row[0]][1] , axis=1)\ndf['Journal'] = df.apply(lambda row: metadata[row[0]][2] , axis=1)\ndf = df.set_index('CORD_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TREATMENT CLASSIFIED ARTICLES\ndf[df['Label'] == 'TR'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VACCINE CLASSIFIED ARTICLES\ndf[df['Label'] == 'VC'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Variant 3: Pretrained SciBERT After Fine Tuning with Opioid NSP**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-nsp-filtered-results/Filter_Before_FineTuning_Classification.tsv', delimiter='\\t')\ndf = df[['ID', 'Text', 'Label']]\ndf.columns = ['Title', 'Abstract', 'Label']\ndf['CORD_ID'] = df.apply(lambda row: metadata[row[0]][0] , axis=1)\ndf['Abstract'] = df.apply(lambda row: metadata[row[0]][1] , axis=1)\ndf['Journal'] = df.apply(lambda row: metadata[row[0]][2] , axis=1)\ndf = df.set_index('CORD_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TREATMENT CLASSIFIED ARTICLES\ndf[df['Label'] == 'TR'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VACCINE CLASSIFIED ARTICLES\ndf[(df['Label'] == 'VC') & (df['Abstract'] != '')].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Variant 4: FineTuned SciBERT After Fine Tuning with Opioid NSP**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-nsp-filtered-results/Filter_After_FineTuning_Classification.tsv', delimiter='\\t')\ndf = df[['ID', 'Text', 'Label']]\ndf.columns = ['Title', 'Abstract', 'Label']\ndf['CORD_ID'] = df.apply(lambda row: metadata[row[0]][0] , axis=1)\ndf['Abstract'] = df.apply(lambda row: metadata[row[0]][1] , axis=1)\ndf['Journal'] = df.apply(lambda row: metadata[row[0]][2] , axis=1)\ndf = df.set_index('CORD_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TREATMENT CLASSIFIED ARTICLES\ndf[(df['Label'] == 'TR') & (df['Abstract'] != '')].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VACCINE CLASSIFIED ARTICLES\ndf[(df['Label'] == 'VC') & (df['Abstract'] != '')].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference from Results of Scorer 1\n### The results above show that FineTuning on the Opioid Dataset seems to have added excessive noise to the model thereby generalizing it to domains other than Coronavirus. On the other hand, doing the intermediate Fine Tuning (MLM + NSP) of the SciBERT model on the Title + Abstract text before the prediction seems to have helped the model predict Treatment and Vaccine related documents more accurately\n\n#### Although not perfect, these methods help to filter relevant articles from thousands of irrelevant ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSource: https://github.com/md-labs/covid19-kaggle/blob/master/clinical_hedges_classification/src/combine_nsp_results.py\nCode to combine all text which is classified as Vaccine or Therapeutics by the Query-Document NSP Model and prepare\ninput to the Clinical Hedges classifier (run_classifier.py)\nInput: Directory of filtered results from NSP model\nOutput: Data prepped in format required for input to Clinical Hedges Classification Model\nOutput File Header Format: [\"ID\", \"Text\", \"Label\", \"Prob\"]\n\"\"\"\n\nimport csv\nimport os\n\npath_to_results = os.path.abspath('../../document_nsp_text_sim/results/filtered_results_final')\ndirListing = os.listdir(path_to_results)\n\ncombined_data_dict = dict()\nfor file in dirListing:\n    with open(os.path.join(path_to_results, file)) as fp:\n        reader = csv.reader(fp, delimiter='\\t')\n        for i, row in enumerate(reader):\n            if i == 0:\n                continue\n            combined_data_dict[row[0]] = row[1:]\n\n\nwith open(os.path.abspath(\"../data/Pred_Data_COVID/Test_Data.tsv\"), 'w') as fp:\n    writer = csv.writer(fp, delimiter='\\t')\n    writer.writerow([\"ID\", \"Text\", \"Label\", \"Prob\"])\n    for key in combined_data_dict.keys():\n        if combined_data_dict[key][1] == 'O':\n            continue\n        writer.writerow([key] + combined_data_dict[key])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The run_classifier code below, is run using the following command:\n`python src/run_classifier.py  --task_num=3 --model_file=models/scibert_scivocab_uncased --vocab_dir=models/scibert_scivocab_uncased --do_lower_case --task_name=clinicalhedges --data_dir=data/Pred_Data_COVID --learning_rate=2e-5 --num_train_epochs=10 --output_dir=models/SciBERT_Trained_Treatment_Model/ --eval_batch_size=16 --max_seq_length=400 --train_batch_size=16 --do_eval`\n#### For more details visit this GitHub ([link](https://github.com/md-labs/covid19-kaggle/tree/master/clinical_hedges_classification))"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSource: https://github.com/md-labs/covid19-kaggle/edit/master/clinical_hedges_classification/src/run_classifier.py\nBERT finetuning runner for Text Classification\nInput: [CLS] (Title + Abstract) Text [SEP]\nOutput: Classification Labels of Treatment or Other for the Text Input\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport csv\nimport logging\nimport os\nimport random\nimport sys\nimport shutil\nimport copy\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam\n\nfrom sklearn.metrics import classification_report\n\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self, index):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass InputProcessor(DataProcessor):\n    \"\"\"Processor for the Clinical Hedges data set.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Train_Data.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Dev_Data.tsv\")), \"dev\")\n\n    def get_test_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"Test_Data.tsv\")), \"test\")\n\n    def get_labels(self, index):\n        \"\"\"See base class.\"\"\"\n        index = int(index)\n        if(index == 0):\n            return [\"NA\", \"O\"]\n        elif(index == 1):\n            return [\"F\", \"T\"]\n        elif(index == 2):\n            return [\"NA\", \"TR\"]\n        elif(index == 3):\n            return [\"F\", \"T\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n        req = list()\n        for i in range(0, len(lines)):\n            req.append(i)\n        req_final = random.sample(req, len(lines))\n        for i in req_final:\n            if i == 0:\n                continue\n            guid = lines[i][0]\n            text_a = lines[i][1]\n            text_b = None\n            label = lines[i][2]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\ndef accuracy(out, labels):\n    outputs = np.argmax(out, axis=1)\n    return np.sum(outputs == labels)\n\n\ndef get_tp_fp_fn(logits, labels):\n  assert labels.shape[1] == 1\n  labels = labels.squeeze()\n  predictions = np.argmax(logits, axis=1)\n  labels, predictions = labels.astype(int), predictions.astype(int)\n  tp = np.sum(np.logical_and(predictions == 1, labels == 1))\n  fp = np.sum(np.logical_and(predictions == 1, labels == 0))\n  fn = np.sum(np.logical_and(predictions == 0, labels == 1))\n  return tp, fp, fn\n\n\ndef compute_metrics(tp, fp, fn):\n  precision = tp / (tp + fp + np.finfo(float).eps)\n  recall = tp / (tp + fn + np.finfo(float).eps)\n  f1 = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n  return precision, recall, f1\n\n\ndef convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    label_map = {label : i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with \"- 2\"\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where \"type_ids\" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambigiously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the \"sentence vector\". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n        segment_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [\"[SEP]\"]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        label_id = label_map[example.label]\n        #if ex_index < 5:\n        #    logger.info(\"*** Example ***\")\n        #    logger.info(\"guid: %s\" % (example.guid))\n        #    logger.info(\"tokens: %s\" % \" \".join(\n        #            [str(x) for x in tokens]))\n        #    logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n        #    logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n        #    logger.info(\n        #            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n        #    logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_id))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(\"--data_dir\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n    parser.add_argument(\"--model_dir\", default=\"\", type=str,\n                        required=True,\n                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n                        \"bert-base-multilingual-cased, bert-base-chinese or any pretrained model directory with model.bin and config file\")\n    parser.add_argument(\"--vocab_dir\", default=\"\", type=str,\n                        required=True,\n                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n    parser.add_argument(\"--task_name\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The name of the task to train.\")\n    parser.add_argument(\"--output_dir\",\n                        default=\"\",\n                        type=str,\n                        required=True,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n\n    parser.add_argument(\"--task_num\",\n                        default=-1,\n                        type=int,\n                        required=True,\n                        help=\"The task number of Clinical Hedges Tasks to run\")\n\n    ## Other parameters\n    parser.add_argument(\"--cache_dir\",\n                        default=\"\",\n                        type=str,\n                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n    parser.add_argument(\"--max_seq_length\",\n                        default=512,\n                        type=int,\n                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n                             \"than this will be padded.\")\n    parser.add_argument(\"--do_train\",\n                        action='store_true',\n                        help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\",\n                        action='store_true',\n                        help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\"--do_lower_case\",\n                        action='store_true',\n                        help=\"Set this flag if you are using an uncased model.\")\n    parser.add_argument(\"--train_batch_size\",\n                        default=32,\n                        type=int,\n                        help=\"Total batch size for training.\")\n    parser.add_argument(\"--eval_batch_size\",\n                        default=32,\n                        type=int,\n                        help=\"Total batch size for eval.\")\n    parser.add_argument(\"--learning_rate\",\n                        default=5e-5,\n                        type=float,\n                        help=\"The initial learning rate for Adam.\")\n    parser.add_argument(\"--num_train_epochs\",\n                        default=3.0,\n                        type=float,\n                        help=\"Total number of training epochs to perform.\")\n    parser.add_argument(\"--warmup_proportion\",\n                        default=0.1,\n                        type=float,\n                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n                             \"E.g., 0.1 = 10%% of training.\")\n    parser.add_argument(\"--no_cuda\",\n                        action='store_true',\n                        help=\"Whether not to use CUDA when available\")\n    parser.add_argument(\"--local_rank\",\n                        type=int,\n                        default=-1,\n                        help=\"local_rank for distributed training on gpus\")\n    parser.add_argument('--seed',\n                        type=int,\n                        default=42,\n                        help=\"random seed for initialization\")\n    parser.add_argument('--gradient_accumulation_steps',\n                        type=int,\n                        default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n    parser.add_argument('--fp16',\n                        action='store_true',\n                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n    parser.add_argument('--loss_scale',\n                        type=float, default=0,\n                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n                             \"0 (default value): dynamic loss scaling.\\n\"\n                             \"Positive power of 2: static loss scaling value.\\n\")\n    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n    args = parser.parse_args()\n\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    processors = {\n        \"clinicalhedges\": InputProcessor,\n    }\n\n    num_labels_task = {\n        \"clinicalhedges\": [2, 2, 2, 2],\n    }\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend='nccl')\n    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n                            args.gradient_accumulation_steps))\n\n    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    if not args.do_train and not args.do_eval:\n        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n\n\n    task_name = args.task_name.lower()\n    task_num = args.task_num\n    if task_name not in processors:\n        raise ValueError(\"Task not found: %s\" % task_name)\n\n    processor = processors[task_name]()\n    print(processor)\n    num_labels = num_labels_task[task_name][task_num-1]\n    print(num_labels)\n    label_list = processor.get_labels(task_num-1)\n    print(label_list)\n\n    tokenizer = BertTokenizer.from_pretrained(args.vocab_dir, do_lower_case=args.do_lower_case)\n    file = open(os.path.join(args.output_dir, \"Classification_Reports_Task_{}.txt\".format(task_num)), 'w')\n\n    train_examples = None\n    num_train_optimization_steps = None\n    if args.do_train:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_optimization_steps = int(\n            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n        if args.local_rank != -1:\n            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n\n    # Prepare model\n    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n    model = BertForSequenceClassification.from_pretrained(args.model_dir,\n              cache_dir=cache_dir,\n              num_labels = num_labels)\n    if args.fp16:\n        model.half()\n    model.to(device)\n    if args.local_rank != -1:\n        try:\n            from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        model = DDP(model)\n    elif n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.learning_rate,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=args.learning_rate,\n                             warmup=args.warmup_proportion,\n                             t_total=num_train_optimization_steps)\n\n    global_step = 0\n    nb_tr_steps = 0\n    tr_loss = 0\n    if args.do_train:\n        train_features = convert_examples_to_features(\n            train_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(\"***** Running training for Task {}*****\".format(task_num))\n        logger.info(\"  Num examples = %d\", len(train_examples))\n        logger.info(\"  Batch size = %d\", args.train_batch_size)\n        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n        for ep in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n            model.train()\n            tr_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n            for step, batch in enumerate(train_dataloader):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                loss = model(input_ids, segment_ids, input_mask, label_ids)\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps\n\n                if args.fp16:\n                    optimizer.backward(loss)\n                else:\n                    loss.backward()\n\n                tr_loss += loss.item()\n                nb_tr_examples += input_ids.size(0)\n                nb_tr_steps += 1\n                if (step + 1) % args.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n            eval_examples = processor.get_dev_examples(args.data_dir)\n            eval_features = convert_examples_to_features(\n                eval_examples, label_list, args.max_seq_length, tokenizer)\n            print(\"\\n\")\n            print(\"Running evaluation for epoch: {}\".format(ep))\n            all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n            all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n            all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n            all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n            eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n            # Run prediction for full data\n            eval_sampler = SequentialSampler(eval_data)\n            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n            model.eval()\n            eval_loss, eval_accuracy = 0, 0\n            nb_eval_steps, nb_eval_examples = 0, 0\n\n            for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n                input_ids = input_ids.to(device)\n                input_mask = input_mask.to(device)\n                segment_ids = segment_ids.to(device)\n                label_ids = label_ids.to(device)\n\n                with torch.no_grad():\n                    tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n                    logits = model(input_ids, segment_ids, input_mask)\n\n                logits = logits.detach().cpu().numpy()\n                label_ids = label_ids.to('cpu').numpy()\n                tmp_eval_accuracy = accuracy(logits, label_ids)\n\n                eval_loss += tmp_eval_loss.mean().item()\n                eval_accuracy += tmp_eval_accuracy\n\n                nb_eval_examples += input_ids.size(0)\n                nb_eval_steps += 1\n\n            eval_loss = eval_loss / nb_eval_steps\n            eval_accuracy = eval_accuracy / nb_eval_examples\n            loss = tr_loss/nb_tr_steps if args.do_train else None\n            result = {'eval_loss': eval_loss,\n                      'eval_accuracy': eval_accuracy,\n                      'global_step': global_step,\n                      'loss': loss}\n\n            for key in sorted(result.keys()):\n                print(key, str(result[key]))\n            print()\n\n    if args.do_train:\n        # Save a trained model and the associated configuration\n        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n        if(os.path.exists(os.path.join(args.output_dir, \"Model_Task_{}\".format(task_num)))):\n            shutil.rmtree(os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num)))\n        os.mkdir(os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num)))\n        output_model_file = os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num), WEIGHTS_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n        output_config_file = os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num), CONFIG_NAME)\n        with open(output_config_file, 'w') as f:\n            f.write(model_to_save.config.to_json_string())\n    if args.do_eval:\n        # Load a trained model and config that you have fine-tuned\n        output_model_file = os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num), WEIGHTS_NAME)\n        output_config_file = os.path.join(args.output_dir, \"Model_Part_Task_{}\".format(task_num), CONFIG_NAME)\n        config = BertConfig(output_config_file)\n        model = BertForSequenceClassification(config, num_labels=num_labels)\n        model.load_state_dict(torch.load(output_model_file, map_location='cpu'))\n    model.to(device)\n\n    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        eval_examples = processor.get_test_examples(args.data_dir)\n        eval_features = convert_examples_to_features(\n            eval_examples, label_list, args.max_seq_length, tokenizer)\n        complete_user_ids = list()\n        for example in eval_examples:\n            complete_user_ids.append(example.guid)\n        logger.info(\"***** Running Test for Task {}*****\".format(task_num))\n        logger.info(\"  Num examples = %d\", len(eval_examples))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        complete_label_ids = list()\n        complete_outputs = list()\n        complete_probs = list()\n        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            label_ids = label_ids.to(device)\n            with torch.no_grad():\n                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n                logits = model(input_ids, segment_ids, input_mask)\n\n            last_layer_op = copy.deepcopy(logits)\n            logits = logits.detach().cpu().numpy()\n            sm = torch.nn.Softmax()\n            probabilities = sm(last_layer_op)\n            probabilities = probabilities.detach().cpu().numpy()\n            label_ids = label_ids.to('cpu').numpy()\n            tmp_eval_accuracy = accuracy(logits, label_ids)\n            outputs = np.argmax(logits, axis=1)\n            complete_outputs.extend(outputs)\n            complete_label_ids.extend(label_ids)\n            complete_probs.extend(probabilities[:,1])\n\n            eval_loss += tmp_eval_loss.mean().item()\n            eval_accuracy += tmp_eval_accuracy\n\n            nb_eval_examples += input_ids.size(0)\n            nb_eval_steps += 1\n\n        outcsv = open(os.path.join(args.output_dir, \"Reqd_Labels_Task_{}.csv\".format(task_num)),'w', encoding = 'utf8', newline='')\n        writer = csv.writer(outcsv, quotechar = '\"')\n        writer.writerow([\"ID\", \"True\", \"Pred\"])\n        for user, true, pred, prob in zip(complete_user_ids, complete_label_ids, complete_outputs, complete_probs):\n            writer.writerow([user,true,pred, prob])\n        outcsv.close()\n        eval_loss = eval_loss / nb_eval_steps\n        eval_loss = eval_loss / nb_eval_steps\n\n\n        eval_loss = eval_loss / nb_eval_steps\n        eval_accuracy = eval_accuracy / nb_eval_examples\n        loss = tr_loss/nb_tr_steps if args.do_train else None\n        result = {'eval_loss': eval_loss,\n                  'eval_accuracy': eval_accuracy,\n                  'global_step': global_step,\n                  'loss': loss}\n        print(result)\n\n        file.write(\"\\nClassification Report\\n\\n\" + classification_report(complete_label_ids, complete_outputs) + \"\\n\\n\\n\")\n    file.close()\n\nif __name__ == \"__main__\":\n    main()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results of Scorer 2:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-ch-classification-results/COVID_Labels_After_Classification_CH.tsv', delimiter='\\t')\ndf = df[['ID', 'Text', 'Label']]\ndf.columns = ['Title', 'Abstract', 'Label']\ndf['CORD_ID'] = df.apply(lambda row: metadata[row[0]][0] , axis=1)\ndf['Abstract'] = df.apply(lambda row: metadata[row[0]][1] , axis=1)\ndf['Journal'] = df.apply(lambda row: metadata[row[0]][2] , axis=1)\ndf = df.set_index('CORD_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TREATMENT CLASSIFIED ARTICLES\ndf[(df['Label'] == 'TR') & (df['Abstract'] != '')].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VACCINE CLASSIFIED ARTICLES\ndf[(df['Label'] == 'VC') & (df['Abstract'] != '')].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference from Results of Scorer 2\n### The performance of this method is completely dependent on the performance efficiency of our NSP models. Although, the model predicts with high accuracy the Treatment related articles they are in small numbers."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}