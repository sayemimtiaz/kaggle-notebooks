{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T18:34:34.792935Z","iopub.execute_input":"2021-07-27T18:34:34.793577Z","iopub.status.idle":"2021-07-27T18:34:34.815109Z","shell.execute_reply.started":"2021-07-27T18:34:34.793481Z","shell.execute_reply":"2021-07-27T18:34:34.813706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict automobile price using Machine Learning\n\nUse Machine learning to predict the price of a car based on several characteristics. The objective is to build a model to understand the factors that drive the car of the price. This will help your automobile company launch their new car in the market effectively by pricing it better. Tasks:\n\n* Perform EDA on the data\n* Perform data cleanup as required\n* Pick the best variable for making a simple linear regression model\n* Perform train test split\n* Build model using best variable and report the R2\n* Make a multiple regression model o Apply feature selection approaches discussed in the class\n* Final model should be interpretable o What is your understanding of the factors that drive price?","metadata":{}},{"cell_type":"markdown","source":"**1. Reading and Understanding the Data**","metadata":{}},{"cell_type":"code","source":"# import all libraries and dependencies for dataframe\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as ticker\nimport matplotlib.ticker as plticker","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:34.816742Z","iopub.execute_input":"2021-07-27T18:34:34.817154Z","iopub.status.idle":"2021-07-27T18:34:35.864257Z","shell.execute_reply.started":"2021-07-27T18:34:34.817118Z","shell.execute_reply":"2021-07-27T18:34:35.863387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the automobile consulting company file on which analysis needs to be done\n\ndf_auto = pd.read_csv('/kaggle/input/automobile-dataset/AutoData.csv')\n\ndf_auto.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:35.865689Z","iopub.execute_input":"2021-07-27T18:34:35.86611Z","iopub.status.idle":"2021-07-27T18:34:35.922392Z","shell.execute_reply.started":"2021-07-27T18:34:35.86608Z","shell.execute_reply":"2021-07-27T18:34:35.92124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Understanding the Dataframe**","metadata":{}},{"cell_type":"code","source":"# shape of the data\ndf_auto.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:35.92446Z","iopub.execute_input":"2021-07-27T18:34:35.924891Z","iopub.status.idle":"2021-07-27T18:34:35.931641Z","shell.execute_reply.started":"2021-07-27T18:34:35.924848Z","shell.execute_reply":"2021-07-27T18:34:35.930394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# information of the data\ndf_auto.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:35.933458Z","iopub.execute_input":"2021-07-27T18:34:35.933974Z","iopub.status.idle":"2021-07-27T18:34:35.964208Z","shell.execute_reply.started":"2021-07-27T18:34:35.933927Z","shell.execute_reply":"2021-07-27T18:34:35.963102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# description of the data\ndf_auto.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:35.965678Z","iopub.execute_input":"2021-07-27T18:34:35.966104Z","iopub.status.idle":"2021-07-27T18:34:36.030582Z","shell.execute_reply.started":"2021-07-27T18:34:35.966055Z","shell.execute_reply":"2021-07-27T18:34:36.029588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Cleaning the Data¶**","metadata":{}},{"cell_type":"code","source":"# Calculating the Missing Values % contribution in DF\n\ndf_null = df_auto.isna().mean().round(4) * 100\n\ndf_null.sort_values(ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.031781Z","iopub.execute_input":"2021-07-27T18:34:36.032089Z","iopub.status.idle":"2021-07-27T18:34:36.058217Z","shell.execute_reply.started":"2021-07-27T18:34:36.032061Z","shell.execute_reply":"2021-07-27T18:34:36.057366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datatypes\ndf_auto.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.059272Z","iopub.execute_input":"2021-07-27T18:34:36.059691Z","iopub.status.idle":"2021-07-27T18:34:36.067889Z","shell.execute_reply.started":"2021-07-27T18:34:36.059654Z","shell.execute_reply":"2021-07-27T18:34:36.067021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #Outlier Analysis of target variable with maximum amount of Inconsistency\n\noutliers = ['price']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data=df_auto[outliers], orient=\"v\", palette=\"Set1\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Price Range\", fontweight = 'bold')\nplt.xlabel(\"Continuous Variable\", fontweight = 'bold')\ndf_auto.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.070699Z","iopub.execute_input":"2021-07-27T18:34:36.071387Z","iopub.status.idle":"2021-07-27T18:34:36.321224Z","shell.execute_reply.started":"2021-07-27T18:34:36.071206Z","shell.execute_reply":"2021-07-27T18:34:36.319999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\nThere are some price ranges above 36000 which can be termed as outliers but lets not remove it rather we will use standarization scaling.","metadata":{}},{"cell_type":"code","source":"# Extracting Car Company from the CarName i.e, make as per direction in Problem \n\ndf_auto['make'] = df_auto['make'].str.split(' ',expand=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.324034Z","iopub.execute_input":"2021-07-27T18:34:36.324531Z","iopub.status.idle":"2021-07-27T18:34:36.331964Z","shell.execute_reply.started":"2021-07-27T18:34:36.324485Z","shell.execute_reply":"2021-07-27T18:34:36.331157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique Car company\n\ndf_auto['make'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.333042Z","iopub.execute_input":"2021-07-27T18:34:36.333494Z","iopub.status.idle":"2021-07-27T18:34:36.347725Z","shell.execute_reply.started":"2021-07-27T18:34:36.333463Z","shell.execute_reply":"2021-07-27T18:34:36.346675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Typo Error in Car Company name**\n* maxda = mazda\n* Nissan = nissan\n* porsche = porcshce\n* toyota = toyouta\n* vokswagen = volkswagen = vw","metadata":{}},{"cell_type":"code","source":"# Renaming the typo errors in Car Company names\n\ndf_auto['make'] = df_auto['make'].replace({'maxda': 'mazda', 'nissan': 'Nissan', 'porcshce': 'porsche', 'toyouta': 'toyota', \n                            'vokswagen': 'volkswagen', 'vw': 'volkswagen'})","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.349208Z","iopub.execute_input":"2021-07-27T18:34:36.349833Z","iopub.status.idle":"2021-07-27T18:34:36.358895Z","shell.execute_reply.started":"2021-07-27T18:34:36.349781Z","shell.execute_reply":"2021-07-27T18:34:36.358016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing the datatype of symboling as it is categorical variable as per dictionary file\n\ndf_auto['symboling'] = df_auto['symboling'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.360259Z","iopub.execute_input":"2021-07-27T18:34:36.360886Z","iopub.status.idle":"2021-07-27T18:34:36.373721Z","shell.execute_reply.started":"2021-07-27T18:34:36.360833Z","shell.execute_reply":"2021-07-27T18:34:36.372531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for duplicates\n\ndf_auto.loc[df_auto.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.375368Z","iopub.execute_input":"2021-07-27T18:34:36.375706Z","iopub.status.idle":"2021-07-27T18:34:36.405857Z","shell.execute_reply.started":"2021-07-27T18:34:36.375676Z","shell.execute_reply":"2021-07-27T18:34:36.404938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Segregation of Numerical and Categorical Variables/Columns\n\ncat_col = df_auto.select_dtypes(include=['object']).columns\nnum_col = df_auto.select_dtypes(exclude=['object']).columns\ndf_cat = df_auto[cat_col]\ndf_num = df_auto[num_col]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.407238Z","iopub.execute_input":"2021-07-27T18:34:36.407531Z","iopub.status.idle":"2021-07-27T18:34:36.417882Z","shell.execute_reply.started":"2021-07-27T18:34:36.407504Z","shell.execute_reply":"2021-07-27T18:34:36.417114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualising the Data**\n\nHere we will identify if some predictors directly have a strong association with the outcome variable price.","metadata":{}},{"cell_type":"code","source":"# Visualizing the different car names available\n\nplt.rcParams['figure.figsize'] = [15,8]\nax=df_auto['make'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')\nax.title.set_text('make')\nplt.xlabel(\"Names of the Car\",fontweight = 'bold')\nplt.ylabel(\"Count of Cars\",fontweight = 'bold')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.419092Z","iopub.execute_input":"2021-07-27T18:34:36.419515Z","iopub.status.idle":"2021-07-27T18:34:36.906794Z","shell.execute_reply.started":"2021-07-27T18:34:36.419485Z","shell.execute_reply":"2021-07-27T18:34:36.905605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n* Toyota seems to be the most favoured cars.\n* Mercury seems to be the least favoured cars.","metadata":{}},{"cell_type":"markdown","source":"**Visualizing the distribution of car prices¶¶**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\nplt.title('Car Price Distribution Plot')\nsns.distplot(df_auto['price'])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:36.908534Z","iopub.execute_input":"2021-07-27T18:34:36.908949Z","iopub.status.idle":"2021-07-27T18:34:37.377313Z","shell.execute_reply.started":"2021-07-27T18:34:36.908901Z","shell.execute_reply":"2021-07-27T18:34:37.376281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The plots seems to be right skewed, the prices of almost all cars looks like less than 18000.","metadata":{}},{"cell_type":"markdown","source":"**Visualising Numeric Variables**\n\nPairplot of all the numeric variables\n\n","metadata":{}},{"cell_type":"code","source":"ax = sns.pairplot(df_auto[num_col])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:34:37.378825Z","iopub.execute_input":"2021-07-27T18:34:37.379374Z","iopub.status.idle":"2021-07-27T18:35:19.459123Z","shell.execute_reply.started":"2021-07-27T18:34:37.379325Z","shell.execute_reply":"2021-07-27T18:35:19.457407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n* carwidth , carlength, curbweight ,enginesize ,horsepowerseems to have a positive correlation with price.\n* carheight doesn't show any significant trend with price.\n* citympg , highwaympg - seem to have a significant negative correlation with price.","metadata":{}},{"cell_type":"markdown","source":"**Visualising few more Categorical Variables**\n\nBoxplot of all the categorical variables\n\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'doornumber', y = 'price', data = df_auto)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'fueltype', y = 'price', data = df_auto)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'aspiration', y = 'price', data = df_auto)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'carbody', y = 'price', data = df_auto)\nplt.subplot(3,3,5)\nsns.boxplot(x = 'enginelocation', y = 'price', data = df_auto)\nplt.subplot(3,3,6)\nsns.boxplot(x = 'drivewheel', y = 'price', data = df_auto)\nplt.subplot(3,3,7)\nsns.boxplot(x = 'enginetype', y = 'price', data = df_auto)\nplt.subplot(3,3,8)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = df_auto)\nplt.subplot(3,3,9)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = df_auto)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:19.460605Z","iopub.execute_input":"2021-07-27T18:35:19.460931Z","iopub.status.idle":"2021-07-27T18:35:21.677784Z","shell.execute_reply.started":"2021-07-27T18:35:19.460899Z","shell.execute_reply":"2021-07-27T18:35:21.676645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights**\n* The cars with fueltype as diesel are comparatively expensive than the cars with fueltype as gas.\n* All the types of carbody is relatively cheaper as compared to convertible carbody.\n* The cars with rear enginelocation are way expensive than cars with front enginelocation.\n* The price of car is directly proportional to no. of cylinders in most cases.\n* Enginetype ohcv comes into higher price range cars.\n* DoorNumber isn't affecting the price much.\n* HigerEnd cars seems to have rwd drivewheel\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 6))\n\nplt.subplot(1,3,1)\nplt1 = df_auto['cylindernumber'].value_counts().plot(kind = 'bar')\nplt.title('Number of cylinders')\nplt1.set(xlabel = 'Number of cylinders', ylabel='Frequency of Number of cylinders')\n\nplt.subplot(1,3,2)\nplt1 = df_auto['fueltype'].value_counts().plot(kind = 'bar')\nplt.title('Fuel Type')\nplt1.set(xlabel = 'Fuel Type', ylabel='Frequency of Fuel type')\n\nplt.subplot(1,3,3)\nplt1 = df_auto['carbody'].value_counts().plot(kind = 'bar')\nplt.title('Car body')\nplt1.set(xlabel = 'Car Body', ylabel='Frequency of Car Body')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:21.679258Z","iopub.execute_input":"2021-07-27T18:35:21.679722Z","iopub.status.idle":"2021-07-27T18:35:22.233602Z","shell.execute_reply.started":"2021-07-27T18:35:21.679676Z","shell.execute_reply":"2021-07-27T18:35:22.232529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n* The number of cylinders used in most cars is four.\n* Number of Gas fueled cars are way more than diesel fueled cars.\n* Sedan is the most prefered car type.","metadata":{}},{"cell_type":"markdown","source":"**Relationship between fuelsystem vs price with hue fueltype**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(x = 'fuelsystem', y = 'price', hue = 'fueltype', data = df_auto)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:22.234936Z","iopub.execute_input":"2021-07-27T18:35:22.235257Z","iopub.status.idle":"2021-07-27T18:35:22.609457Z","shell.execute_reply.started":"2021-07-27T18:35:22.23522Z","shell.execute_reply":"2021-07-27T18:35:22.608391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Relationship between carbody vs price with hue enginelocation**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(x = 'carbody', y = 'price', hue = 'enginelocation', data = df_auto)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:22.612018Z","iopub.execute_input":"2021-07-27T18:35:22.612333Z","iopub.status.idle":"2021-07-27T18:35:22.992364Z","shell.execute_reply.started":"2021-07-27T18:35:22.612305Z","shell.execute_reply":"2021-07-27T18:35:22.991319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Relationship between cylindernumber vs price with hue fueltype**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(x = 'cylindernumber', y = 'price', hue = 'fueltype', data = df_auto)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:22.99596Z","iopub.execute_input":"2021-07-27T18:35:22.996309Z","iopub.status.idle":"2021-07-27T18:35:23.414297Z","shell.execute_reply.started":"2021-07-27T18:35:22.996278Z","shell.execute_reply":"2021-07-27T18:35:23.413294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Derived Metrices**\n* Average Price","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\n\ndf_autox = pd.DataFrame(df_auto.groupby(['make'])['price'].mean().sort_values(ascending = False))\ndf_autox.plot.bar()\nplt.title('Car Company Name vs Average Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:23.415592Z","iopub.execute_input":"2021-07-27T18:35:23.415873Z","iopub.status.idle":"2021-07-27T18:35:23.827691Z","shell.execute_reply.started":"2021-07-27T18:35:23.415847Z","shell.execute_reply":"2021-07-27T18:35:23.826625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\nJaguar, Buick and porsche seems to have the highest average price.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\n\ndf_autoy = pd.DataFrame(df_auto.groupby(['carbody'])['price'].mean().sort_values(ascending = False))\ndf_autoy.plot.bar()\nplt.title('Car Body Type vs Average Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:23.828999Z","iopub.execute_input":"2021-07-27T18:35:23.829294Z","iopub.status.idle":"2021-07-27T18:35:24.043971Z","shell.execute_reply.started":"2021-07-27T18:35:23.829266Z","shell.execute_reply":"2021-07-27T18:35:24.043231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nhardtop and convertible seems to have the highest average price.","metadata":{}},{"cell_type":"code","source":"#Binning the Car Companies based on avg prices of each car Company.\n\ndf_auto['price'] = df_auto['price'].astype('int')\ndf_auto_temp = df_auto.copy()\nt = df_auto_temp.groupby(['make'])['price'].mean()\ndf_auto_temp = df_auto_temp.merge(t.reset_index(), how='left',on='make')\nbins = [0,10000,20000,40000]\nlabel =['Budget_Friendly','Medium_Range','TopNotch_Cars']\ndf_auto['Cars_Category'] = pd.cut(df_auto_temp['price_y'],bins,right=False,labels=label)\ndf_auto.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.045183Z","iopub.execute_input":"2021-07-27T18:35:24.045448Z","iopub.status.idle":"2021-07-27T18:35:24.094153Z","shell.execute_reply.started":"2021-07-27T18:35:24.045422Z","shell.execute_reply":"2021-07-27T18:35:24.093142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Significant variables after Visualization**\n* Cars_Category , Engine Type, Fuel Type\n* Car Body , Aspiration , Cylinder Number\n* Drivewheel , Curbweight , Car Length\n* Car Length , Car width , Engine Size\n* Boreratio , Horse Power , Wheel base\n* citympg , highwaympg , symboling","metadata":{}},{"cell_type":"code","source":"sig_col = ['price','Cars_Category','enginetype','fueltype', 'aspiration','carbody','cylindernumber', 'drivewheel',\n            'wheelbase','curbweight', 'enginesize', 'boreratio','horsepower', \n                    'citympg','highwaympg', 'carlength','carwidth']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.099857Z","iopub.execute_input":"2021-07-27T18:35:24.100278Z","iopub.status.idle":"2021-07-27T18:35:24.105428Z","shell.execute_reply.started":"2021-07-27T18:35:24.100245Z","shell.execute_reply":"2021-07-27T18:35:24.104246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_auto = df_auto[sig_col]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.109428Z","iopub.execute_input":"2021-07-27T18:35:24.109753Z","iopub.status.idle":"2021-07-27T18:35:24.120594Z","shell.execute_reply.started":"2021-07-27T18:35:24.109722Z","shell.execute_reply":"2021-07-27T18:35:24.119543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Preparation**¶","metadata":{}},{"cell_type":"markdown","source":"**Dummy Variables**\n\nThe variable carbody has five levels. We need to convert these levels into integer. Similarly we need to convert the categorical variables to numeric.\nFor this, we will use something called dummy variables.","metadata":{}},{"cell_type":"code","source":"sig_cat_col = ['Cars_Category','fueltype','aspiration','carbody','drivewheel','enginetype','cylindernumber']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.12197Z","iopub.execute_input":"2021-07-27T18:35:24.122291Z","iopub.status.idle":"2021-07-27T18:35:24.130997Z","shell.execute_reply.started":"2021-07-27T18:35:24.12226Z","shell.execute_reply":"2021-07-27T18:35:24.13009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the dummy variables for the categorical feature and store it in a new variable - 'dummies'\n\ndummies = pd.get_dummies(df_auto[sig_cat_col])\ndummies.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.132455Z","iopub.execute_input":"2021-07-27T18:35:24.132888Z","iopub.status.idle":"2021-07-27T18:35:24.157519Z","shell.execute_reply.started":"2021-07-27T18:35:24.132849Z","shell.execute_reply":"2021-07-27T18:35:24.156227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummies = pd.get_dummies(df_auto[sig_cat_col], drop_first = True)\ndummies.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.159142Z","iopub.execute_input":"2021-07-27T18:35:24.159521Z","iopub.status.idle":"2021-07-27T18:35:24.177849Z","shell.execute_reply.started":"2021-07-27T18:35:24.159486Z","shell.execute_reply":"2021-07-27T18:35:24.176905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the results to the original dataframe\n\ndf_auto = pd.concat([df_auto, dummies], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.179427Z","iopub.execute_input":"2021-07-27T18:35:24.179994Z","iopub.status.idle":"2021-07-27T18:35:24.187444Z","shell.execute_reply.started":"2021-07-27T18:35:24.179951Z","shell.execute_reply":"2021-07-27T18:35:24.18663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the original cat variables as dummies are already created\n\ndf_auto.drop( sig_cat_col, axis = 1, inplace = True)\ndf_auto.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.188648Z","iopub.execute_input":"2021-07-27T18:35:24.189254Z","iopub.status.idle":"2021-07-27T18:35:24.204584Z","shell.execute_reply.started":"2021-07-27T18:35:24.18921Z","shell.execute_reply":"2021-07-27T18:35:24.203445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the Data into Training and Testing Sets\n\nAs we know, the first basic step for regression is performing a train-test split.","metadata":{}},{"cell_type":"code","source":"# import all libraries and dependencies for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.base import TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:24.206064Z","iopub.execute_input":"2021-07-27T18:35:24.206746Z","iopub.status.idle":"2021-07-27T18:35:25.335078Z","shell.execute_reply.started":"2021-07-27T18:35:24.206699Z","shell.execute_reply":"2021-07-27T18:35:25.334278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_auto","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.336384Z","iopub.execute_input":"2021-07-27T18:35:25.336968Z","iopub.status.idle":"2021-07-27T18:35:25.374821Z","shell.execute_reply.started":"2021-07-27T18:35:25.336924Z","shell.execute_reply":"2021-07-27T18:35:25.373516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We specify this so that the train and test data set always have the same rows, respectively\n# We divide the df into 70/30 ratio\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(df_auto, train_size = 0.7, test_size = 0.3, random_state = 100)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.376539Z","iopub.execute_input":"2021-07-27T18:35:25.376947Z","iopub.status.idle":"2021-07-27T18:35:25.384394Z","shell.execute_reply.started":"2021-07-27T18:35:25.376901Z","shell.execute_reply":"2021-07-27T18:35:25.383378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.385588Z","iopub.execute_input":"2021-07-27T18:35:25.385867Z","iopub.status.idle":"2021-07-27T18:35:25.419858Z","shell.execute_reply.started":"2021-07-27T18:35:25.38584Z","shell.execute_reply":"2021-07-27T18:35:25.418802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rescaling the Features**\n\nFor Simple Linear Regression, scaling doesn't impact model. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. There are two common ways of rescaling:\n\n1. Min-Max scaling\n2. Standardisation (mean-0, sigma-1)\nHere, we will use Standardisation Scaling.","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.421064Z","iopub.execute_input":"2021-07-27T18:35:25.421354Z","iopub.status.idle":"2021-07-27T18:35:25.431464Z","shell.execute_reply.started":"2021-07-27T18:35:25.421327Z","shell.execute_reply":"2021-07-27T18:35:25.4306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sig_num_col = ['wheelbase','carlength','carwidth','curbweight','enginesize','boreratio','horsepower','citympg','highwaympg','price']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.432506Z","iopub.execute_input":"2021-07-27T18:35:25.432903Z","iopub.status.idle":"2021-07-27T18:35:25.446645Z","shell.execute_reply.started":"2021-07-27T18:35:25.432874Z","shell.execute_reply":"2021-07-27T18:35:25.445535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply scaler() to all the columns except the 'dummy' variables\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_train[sig_num_col] = scaler.fit_transform(df_train[sig_num_col])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.44973Z","iopub.execute_input":"2021-07-27T18:35:25.450133Z","iopub.status.idle":"2021-07-27T18:35:25.471058Z","shell.execute_reply.started":"2021-07-27T18:35:25.450094Z","shell.execute_reply":"2021-07-27T18:35:25.469982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.472518Z","iopub.execute_input":"2021-07-27T18:35:25.472872Z","iopub.status.idle":"2021-07-27T18:35:25.497092Z","shell.execute_reply.started":"2021-07-27T18:35:25.47284Z","shell.execute_reply":"2021-07-27T18:35:25.496008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (20, 20))\nsns.heatmap(df_train.corr(), cmap=\"RdYlGn\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:25.500186Z","iopub.execute_input":"2021-07-27T18:35:25.500897Z","iopub.status.idle":"2021-07-27T18:35:26.693141Z","shell.execute_reply.started":"2021-07-27T18:35:25.500853Z","shell.execute_reply":"2021-07-27T18:35:26.692267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see scatterplot for few correlated variables vs price.**","metadata":{}},{"cell_type":"code","source":"col = ['highwaympg','citympg','horsepower','enginesize','curbweight','carwidth']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:26.694294Z","iopub.execute_input":"2021-07-27T18:35:26.69472Z","iopub.status.idle":"2021-07-27T18:35:26.699941Z","shell.execute_reply.started":"2021-07-27T18:35:26.694688Z","shell.execute_reply":"2021-07-27T18:35:26.698784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter Plot of independent variables vs dependent variables\n\nfig,axes = plt.subplots(2,3,figsize=(18,15))\nfor seg,col in enumerate(col):\n    x,y = seg//3,seg%3\n    an=sns.scatterplot(x=col, y='price' ,data=df_auto, ax=axes[x,y])\n    plt.setp(an.get_xticklabels(), rotation=45)\n   \nplt.subplots_adjust(hspace=0.5)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:26.701626Z","iopub.execute_input":"2021-07-27T18:35:26.701961Z","iopub.status.idle":"2021-07-27T18:35:28.360732Z","shell.execute_reply.started":"2021-07-27T18:35:26.70193Z","shell.execute_reply":"2021-07-27T18:35:28.359652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see there is a line we can fit in above plots","metadata":{}},{"cell_type":"markdown","source":"# Dividing into X and Y sets for the model building","metadata":{}},{"cell_type":"code","source":"y_train = df_train.pop('price')\nX_train = df_train\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.36203Z","iopub.execute_input":"2021-07-27T18:35:28.36236Z","iopub.status.idle":"2021-07-27T18:35:28.36816Z","shell.execute_reply.started":"2021-07-27T18:35:28.362329Z","shell.execute_reply":"2021-07-27T18:35:28.36687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Linear Model","metadata":{}},{"cell_type":"code","source":"X_train_1 = X_train['horsepower']\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.369633Z","iopub.execute_input":"2021-07-27T18:35:28.370061Z","iopub.status.idle":"2021-07-27T18:35:28.38144Z","shell.execute_reply.started":"2021-07-27T18:35:28.370029Z","shell.execute_reply":"2021-07-27T18:35:28.380244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a constant\nX_train_1c = sm.add_constant(X_train_1)\n\n# Create a first fitted model\nlr_1 = sm.OLS(y_train, X_train_1c).fit()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.383375Z","iopub.execute_input":"2021-07-27T18:35:28.383889Z","iopub.status.idle":"2021-07-27T18:35:28.400704Z","shell.execute_reply.started":"2021-07-27T18:35:28.383842Z","shell.execute_reply":"2021-07-27T18:35:28.399411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check parameters created\n\nlr_1.params","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.40251Z","iopub.execute_input":"2021-07-27T18:35:28.402874Z","iopub.status.idle":"2021-07-27T18:35:28.414648Z","shell.execute_reply.started":"2021-07-27T18:35:28.402835Z","shell.execute_reply":"2021-07-27T18:35:28.413377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualise the data with a scatter plot and the fitted regression line\n\nplt.scatter(X_train_1c.iloc[:, 1], y_train)\nplt.plot(X_train_1c.iloc[:, 1], 0.8062*X_train_1c.iloc[:, 1], 'r')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.416306Z","iopub.execute_input":"2021-07-27T18:35:28.41673Z","iopub.status.idle":"2021-07-27T18:35:28.663618Z","shell.execute_reply.started":"2021-07-27T18:35:28.41669Z","shell.execute_reply":"2021-07-27T18:35:28.662394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_1.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.665368Z","iopub.execute_input":"2021-07-27T18:35:28.665979Z","iopub.status.idle":"2021-07-27T18:35:28.681583Z","shell.execute_reply.started":"2021-07-27T18:35:28.665925Z","shell.execute_reply":"2021-07-27T18:35:28.680364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding another variable\n\nThe R-squared value obtained is 0.65. Since we have so many variables, we can clearly do better than this. So let's go ahead and add the other highly correlated variable, i.e. curbweight.","metadata":{}},{"cell_type":"code","source":"X_train_2 = X_train[['horsepower', 'curbweight']]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.683086Z","iopub.execute_input":"2021-07-27T18:35:28.683418Z","iopub.status.idle":"2021-07-27T18:35:28.689826Z","shell.execute_reply.started":"2021-07-27T18:35:28.683386Z","shell.execute_reply":"2021-07-27T18:35:28.688639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a constant\nX_train_2c = sm.add_constant(X_train_2)\n\n# Create a second fitted model\nlr_2 = sm.OLS(y_train, X_train_2c).fit()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.691436Z","iopub.execute_input":"2021-07-27T18:35:28.691831Z","iopub.status.idle":"2021-07-27T18:35:28.707656Z","shell.execute_reply.started":"2021-07-27T18:35:28.691794Z","shell.execute_reply":"2021-07-27T18:35:28.706129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lr_2.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.709319Z","iopub.execute_input":"2021-07-27T18:35:28.709731Z","iopub.status.idle":"2021-07-27T18:35:28.725815Z","shell.execute_reply.started":"2021-07-27T18:35:28.709688Z","shell.execute_reply":"2021-07-27T18:35:28.724787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The R-squared incresed from 0.650 to 0.797","metadata":{}},{"cell_type":"markdown","source":"**Adding another variable**\n\nThe R-squared value obtained is 0.797. Since we have so many variables, we can clearly do better than this. So lets add another correlated variable, i.e. enginesize.","metadata":{}},{"cell_type":"code","source":"X_train_3 = X_train[['horsepower', 'curbweight', 'enginesize']]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.727297Z","iopub.execute_input":"2021-07-27T18:35:28.727658Z","iopub.status.idle":"2021-07-27T18:35:28.738365Z","shell.execute_reply.started":"2021-07-27T18:35:28.727628Z","shell.execute_reply":"2021-07-27T18:35:28.737288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a constant\nX_train_3c = sm.add_constant(X_train_3)\n\n# Create a third fitted model\nlr_3 = sm.OLS(y_train, X_train_3c).fit()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.73979Z","iopub.execute_input":"2021-07-27T18:35:28.740216Z","iopub.status.idle":"2021-07-27T18:35:28.752951Z","shell.execute_reply.started":"2021-07-27T18:35:28.740156Z","shell.execute_reply":"2021-07-27T18:35:28.751751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_3.params","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.754648Z","iopub.execute_input":"2021-07-27T18:35:28.75516Z","iopub.status.idle":"2021-07-27T18:35:28.767232Z","shell.execute_reply.started":"2021-07-27T18:35:28.755114Z","shell.execute_reply":"2021-07-27T18:35:28.766153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lr_3.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.76853Z","iopub.execute_input":"2021-07-27T18:35:28.769004Z","iopub.status.idle":"2021-07-27T18:35:28.78708Z","shell.execute_reply.started":"2021-07-27T18:35:28.76897Z","shell.execute_reply":"2021-07-27T18:35:28.785985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have achieved a R-squared of 0.819 by manually picking the highly correlated variables. Now lets use RFE to select the independent variables which accurately predicts the dependent variable price.","metadata":{}},{"cell_type":"markdown","source":"# RFE\n\nLet's use Recursive feature elimination since we have too many independent variables","metadata":{}},{"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             \nrfe = rfe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.788666Z","iopub.execute_input":"2021-07-27T18:35:28.788992Z","iopub.status.idle":"2021-07-27T18:35:28.874979Z","shell.execute_reply.started":"2021-07-27T18:35:28.788964Z","shell.execute_reply":"2021-07-27T18:35:28.87413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.876053Z","iopub.execute_input":"2021-07-27T18:35:28.876505Z","iopub.status.idle":"2021-07-27T18:35:28.886508Z","shell.execute_reply.started":"2021-07-27T18:35:28.876473Z","shell.execute_reply":"2021-07-27T18:35:28.885399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the variables which are in support\n\ncol_sup = X_train.columns[rfe.support_]\ncol_sup","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.88791Z","iopub.execute_input":"2021-07-27T18:35:28.8885Z","iopub.status.idle":"2021-07-27T18:35:28.899409Z","shell.execute_reply.started":"2021-07-27T18:35:28.888462Z","shell.execute_reply":"2021-07-27T18:35:28.898354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating X_train dataframe with RFE selected variables\n\nX_train_rfe = X_train[col_sup]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.900686Z","iopub.execute_input":"2021-07-27T18:35:28.900993Z","iopub.status.idle":"2021-07-27T18:35:28.91013Z","shell.execute_reply.started":"2021-07-27T18:35:28.900963Z","shell.execute_reply":"2021-07-27T18:35:28.909145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After passing the arbitary selected columns by RFE we will manually evaluate each models p-value and VIF value. Unless we find the acceptable range for p-values and VIF we keep dropping the variables one at a time based on below criteria.\n\n* High p-value High VIF : Drop the variable\n* High p-value Low VIF or Low p-value High VIF : Drop the variable with high p-value first\n* Low p-value Low VIF : accept the variable","metadata":{}},{"cell_type":"code","source":"# Adding a constant variable and Build a first fitted model\nimport statsmodels.api as sm  \nX_train_rfec = sm.add_constant(X_train_rfe)\nlm_rfe = sm.OLS(y_train,X_train_rfec).fit()\n\n#Summary of linear model\nprint(lm_rfe.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.911495Z","iopub.execute_input":"2021-07-27T18:35:28.912025Z","iopub.status.idle":"2021-07-27T18:35:28.941139Z","shell.execute_reply.started":"2021-07-27T18:35:28.91199Z","shell.execute_reply":"2021-07-27T18:35:28.940441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Looking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables) and we need to drop it","metadata":{}},{"cell_type":"markdown","source":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model.","metadata":{}},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.942219Z","iopub.execute_input":"2021-07-27T18:35:28.942642Z","iopub.status.idle":"2021-07-27T18:35:28.976426Z","shell.execute_reply.started":"2021-07-27T18:35:28.942611Z","shell.execute_reply":"2021-07-27T18:35:28.975691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We generally want a VIF that is less than 5. So there are clearly some variables we need to drop.","metadata":{}},{"cell_type":"markdown","source":"# Dropping the variable and updating the model\n\nDropping cylindernumber_twelve beacuse its p-value is 0.393 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe1 = X_train_rfe.drop('cylindernumber_twelve', 1,)\n\n# Adding a constant variable and Build a second fitted model\n\nX_train_rfe1c = sm.add_constant(X_train_rfe1)\nlm_rfe1 = sm.OLS(y_train, X_train_rfe1c).fit()\n\n#Summary of linear model\nprint(lm_rfe1.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:28.977468Z","iopub.execute_input":"2021-07-27T18:35:28.977913Z","iopub.status.idle":"2021-07-27T18:35:29.006206Z","shell.execute_reply.started":"2021-07-27T18:35:28.977883Z","shell.execute_reply":"2021-07-27T18:35:29.005203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe1.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe1.values, i) for i in range(X_train_rfe1.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.007546Z","iopub.execute_input":"2021-07-27T18:35:29.007864Z","iopub.status.idle":"2021-07-27T18:35:29.041565Z","shell.execute_reply.started":"2021-07-27T18:35:29.007831Z","shell.execute_reply":"2021-07-27T18:35:29.040327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping cylindernumber_six beacuse its p-value is 0.493 and we want p-value less than 0.05 and hence rebuilding the model.","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe2 = X_train_rfe1.drop('cylindernumber_six', 1,)\n\n# Adding a constant variable and Build a third fitted model\n\nX_train_rfe2c = sm.add_constant(X_train_rfe2)\nlm_rfe2 = sm.OLS(y_train, X_train_rfe2c).fit()\n\n#Summary of linear model\nprint(lm_rfe2.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.042927Z","iopub.execute_input":"2021-07-27T18:35:29.043242Z","iopub.status.idle":"2021-07-27T18:35:29.070838Z","shell.execute_reply.started":"2021-07-27T18:35:29.04321Z","shell.execute_reply":"2021-07-27T18:35:29.069778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe2.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe2.values, i) for i in range(X_train_rfe2.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.072158Z","iopub.execute_input":"2021-07-27T18:35:29.072538Z","iopub.status.idle":"2021-07-27T18:35:29.103969Z","shell.execute_reply.started":"2021-07-27T18:35:29.072508Z","shell.execute_reply":"2021-07-27T18:35:29.102778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping carbody_hardtop beacuse its p-value is 0.238 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe3 = X_train_rfe2.drop('carbody_hardtop', 1,)\n\n# Adding a constant variable and Build a fourth fitted model\nX_train_rfe3c = sm.add_constant(X_train_rfe3)\nlm_rfe3 = sm.OLS(y_train, X_train_rfe3c).fit()\n\n#Summary of linear model\nprint(lm_rfe3.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.10543Z","iopub.execute_input":"2021-07-27T18:35:29.105734Z","iopub.status.idle":"2021-07-27T18:35:29.131972Z","shell.execute_reply.started":"2021-07-27T18:35:29.105705Z","shell.execute_reply":"2021-07-27T18:35:29.130934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe3.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe3.values, i) for i in range(X_train_rfe3.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.133551Z","iopub.execute_input":"2021-07-27T18:35:29.133975Z","iopub.status.idle":"2021-07-27T18:35:29.16693Z","shell.execute_reply.started":"2021-07-27T18:35:29.133931Z","shell.execute_reply":"2021-07-27T18:35:29.165691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping enginetype_ohc beacuse its p-value is 0.110 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe4 = X_train_rfe3.drop('enginetype_ohc', 1,)\n\n# Adding a constant variable and Build a fifth fitted model\nX_train_rfe4c = sm.add_constant(X_train_rfe4)\nlm_rfe4 = sm.OLS(y_train, X_train_rfe4c).fit()\n\n#Summary of linear model\nprint(lm_rfe4.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.168559Z","iopub.execute_input":"2021-07-27T18:35:29.168999Z","iopub.status.idle":"2021-07-27T18:35:29.196456Z","shell.execute_reply.started":"2021-07-27T18:35:29.168952Z","shell.execute_reply":"2021-07-27T18:35:29.195204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe4.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe4.values, i) for i in range(X_train_rfe4.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.198039Z","iopub.execute_input":"2021-07-27T18:35:29.198514Z","iopub.status.idle":"2021-07-27T18:35:29.230629Z","shell.execute_reply.started":"2021-07-27T18:35:29.198469Z","shell.execute_reply":"2021-07-27T18:35:29.229511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping cylindernumber_five beacuse its p-value is 0.104 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe5 = X_train_rfe4.drop('cylindernumber_five', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe5c = sm.add_constant(X_train_rfe5)\nlm_rfe5 = sm.OLS(y_train, X_train_rfe5c).fit()\n\n#Summary of linear model\nprint(lm_rfe5.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.23396Z","iopub.execute_input":"2021-07-27T18:35:29.234298Z","iopub.status.idle":"2021-07-27T18:35:29.25857Z","shell.execute_reply.started":"2021-07-27T18:35:29.234267Z","shell.execute_reply":"2021-07-27T18:35:29.257772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe5.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe5.values, i) for i in range(X_train_rfe5.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.259707Z","iopub.execute_input":"2021-07-27T18:35:29.260184Z","iopub.status.idle":"2021-07-27T18:35:29.287914Z","shell.execute_reply.started":"2021-07-27T18:35:29.260133Z","shell.execute_reply":"2021-07-27T18:35:29.287182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping enginetype_ohcv beacuse its p-value is 0.180 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe6 = X_train_rfe5.drop('enginetype_ohcv', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe6c = sm.add_constant(X_train_rfe6)\nlm_rfe6 = sm.OLS(y_train, X_train_rfe6c).fit()\n\n#Summary of linear model\nprint(lm_rfe6.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.289259Z","iopub.execute_input":"2021-07-27T18:35:29.289596Z","iopub.status.idle":"2021-07-27T18:35:29.313497Z","shell.execute_reply.started":"2021-07-27T18:35:29.289558Z","shell.execute_reply":"2021-07-27T18:35:29.312198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe6.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe6.values, i) for i in range(X_train_rfe6.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.315972Z","iopub.execute_input":"2021-07-27T18:35:29.316583Z","iopub.status.idle":"2021-07-27T18:35:29.348944Z","shell.execute_reply.started":"2021-07-27T18:35:29.316473Z","shell.execute_reply":"2021-07-27T18:35:29.347973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping curbweight beacuse its VIF is 8.1 and we want VIF less than 5 and hence rebuilding the model\n\n","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe7 = X_train_rfe6.drop('curbweight', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe7c = sm.add_constant(X_train_rfe7)\nlm_rfe7 = sm.OLS(y_train, X_train_rfe7c).fit()\n\n#Summary of linear model\nprint(lm_rfe7.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.350402Z","iopub.execute_input":"2021-07-27T18:35:29.350794Z","iopub.status.idle":"2021-07-27T18:35:29.377306Z","shell.execute_reply.started":"2021-07-27T18:35:29.350753Z","shell.execute_reply":"2021-07-27T18:35:29.375952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe7.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe7.values, i) for i in range(X_train_rfe7.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.378983Z","iopub.execute_input":"2021-07-27T18:35:29.379582Z","iopub.status.idle":"2021-07-27T18:35:29.406541Z","shell.execute_reply.started":"2021-07-27T18:35:29.379531Z","shell.execute_reply":"2021-07-27T18:35:29.405293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping cylindernumber_four beacuse its VIF is 5.66 and we want VIF less than 5 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe8 = X_train_rfe7.drop('cylindernumber_four', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe8c = sm.add_constant(X_train_rfe8)\nlm_rfe8 = sm.OLS(y_train, X_train_rfe8c).fit()\n\n#Summary of linear model\nprint(lm_rfe8.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.407972Z","iopub.execute_input":"2021-07-27T18:35:29.408436Z","iopub.status.idle":"2021-07-27T18:35:29.434414Z","shell.execute_reply.started":"2021-07-27T18:35:29.408393Z","shell.execute_reply":"2021-07-27T18:35:29.432507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe8.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe8.values, i) for i in range(X_train_rfe8.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.435994Z","iopub.execute_input":"2021-07-27T18:35:29.436425Z","iopub.status.idle":"2021-07-27T18:35:29.463603Z","shell.execute_reply.started":"2021-07-27T18:35:29.43638Z","shell.execute_reply":"2021-07-27T18:35:29.462615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets drop carbody_sedan and see if there is any drastic fall in R squared.If not we can drop carbody sedan. Our aim is to explain the maximum variance with minimum variable.","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe9 = X_train_rfe8.drop('carbody_sedan', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe9c = sm.add_constant(X_train_rfe9)\nlm_rfe9 = sm.OLS(y_train, X_train_rfe9c).fit()\n\n#Summary of linear model\nprint(lm_rfe9.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.464875Z","iopub.execute_input":"2021-07-27T18:35:29.465204Z","iopub.status.idle":"2021-07-27T18:35:29.488206Z","shell.execute_reply.started":"2021-07-27T18:35:29.46515Z","shell.execute_reply":"2021-07-27T18:35:29.487217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The R squared value just dropped by 0.005.Hence we can proceed with dropping carbody_sedan.\n","metadata":{}},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe9.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe9.values, i) for i in range(X_train_rfe9.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.494835Z","iopub.execute_input":"2021-07-27T18:35:29.495216Z","iopub.status.idle":"2021-07-27T18:35:29.518532Z","shell.execute_reply.started":"2021-07-27T18:35:29.495182Z","shell.execute_reply":"2021-07-27T18:35:29.517389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping carbody_wagon beacuse its p-value is 0.315 and we want p-value less than 0.05 and hence rebuilding the model","metadata":{}},{"cell_type":"code","source":"# Dropping highly correlated variables and insignificant variables\n\nX_train_rfe10 = X_train_rfe9.drop('carbody_wagon', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\nX_train_rfe10c = sm.add_constant(X_train_rfe10)\nlm_rfe10 = sm.OLS(y_train, X_train_rfe10c).fit()\n\n#Summary of linear model\nprint(lm_rfe10.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.52063Z","iopub.execute_input":"2021-07-27T18:35:29.52105Z","iopub.status.idle":"2021-07-27T18:35:29.544887Z","shell.execute_reply.started":"2021-07-27T18:35:29.521007Z","shell.execute_reply":"2021-07-27T18:35:29.543943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe10.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe10.values, i) for i in range(X_train_rfe10.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.54614Z","iopub.execute_input":"2021-07-27T18:35:29.546438Z","iopub.status.idle":"2021-07-27T18:35:29.566287Z","shell.execute_reply.started":"2021-07-27T18:35:29.54641Z","shell.execute_reply":"2021-07-27T18:35:29.565263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the VIFs and p-values both are within an acceptable range. So we can go ahead and make our predictions using model lm_rfe10 and lm_rfe8.","metadata":{}},{"cell_type":"markdown","source":"### Here, we are proposing Business 2 Models which can be used to predict the car prices.","metadata":{}},{"cell_type":"markdown","source":"## MODEL I\n\n* With lm_rfe10 which has basically 5 predictor variables.","metadata":{}},{"cell_type":"markdown","source":"### Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of it.","metadata":{}},{"cell_type":"code","source":"# Predicting the price of training set.\ny_train_price = lm_rfe10.predict(X_train_rfe10c)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.567535Z","iopub.execute_input":"2021-07-27T18:35:29.567825Z","iopub.status.idle":"2021-07-27T18:35:29.572823Z","shell.execute_reply.started":"2021-07-27T18:35:29.567798Z","shell.execute_reply":"2021-07-27T18:35:29.571824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.574152Z","iopub.execute_input":"2021-07-27T18:35:29.574604Z","iopub.status.idle":"2021-07-27T18:35:29.935964Z","shell.execute_reply.started":"2021-07-27T18:35:29.574553Z","shell.execute_reply":"2021-07-27T18:35:29.934693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final model.","metadata":{}},{"cell_type":"markdown","source":"**Applying the scaling on the test sets**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_test[sig_num_col] = scaler.transform(df_test[sig_num_col])\ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.937374Z","iopub.execute_input":"2021-07-27T18:35:29.937694Z","iopub.status.idle":"2021-07-27T18:35:29.953418Z","shell.execute_reply.started":"2021-07-27T18:35:29.937663Z","shell.execute_reply":"2021-07-27T18:35:29.952361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dividing test set into X_test and y_test","metadata":{}},{"cell_type":"code","source":"y_test = df_test.pop('price')\nX_test = df_test","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.954876Z","iopub.execute_input":"2021-07-27T18:35:29.955214Z","iopub.status.idle":"2021-07-27T18:35:29.962787Z","shell.execute_reply.started":"2021-07-27T18:35:29.955158Z","shell.execute_reply":"2021-07-27T18:35:29.961765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding constant\nX_test_1 = sm.add_constant(X_test)\n\nX_test_new = X_test_1[X_train_rfe10c.columns]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.963773Z","iopub.execute_input":"2021-07-27T18:35:29.96406Z","iopub.status.idle":"2021-07-27T18:35:29.985001Z","shell.execute_reply.started":"2021-07-27T18:35:29.964033Z","shell.execute_reply":"2021-07-27T18:35:29.983961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions using the final model\ny_pred = lm_rfe10.predict(X_test_new)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.986401Z","iopub.execute_input":"2021-07-27T18:35:29.986702Z","iopub.status.idle":"2021-07-27T18:35:29.99213Z","shell.execute_reply.started":"2021-07-27T18:35:29.986674Z","shell.execute_reply":"2021-07-27T18:35:29.990914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)   \nplt.xlabel('y_test ', fontsize=18)                       \nplt.ylabel('y_pred', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:29.99383Z","iopub.execute_input":"2021-07-27T18:35:29.9948Z","iopub.status.idle":"2021-07-27T18:35:30.308595Z","shell.execute_reply.started":"2021-07-27T18:35:29.994747Z","shell.execute_reply":"2021-07-27T18:35:30.307465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE Score","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.310151Z","iopub.execute_input":"2021-07-27T18:35:30.310609Z","iopub.status.idle":"2021-07-27T18:35:30.319809Z","shell.execute_reply.started":"2021-07-27T18:35:30.310563Z","shell.execute_reply":"2021-07-27T18:35:30.318526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The R2 score of Training set is 0.912 and Test set is 0.909 which is quite close. Hence, We can say that our model is good enough to predict the Car prices using below predictor variables**\n\n* horsepower\n* carwidth\n* Cars_Category_TopNotch_Cars\n* carbody_hatchback\n* enginetype_dohcv","metadata":{}},{"cell_type":"markdown","source":"**Equation of Line to predict the Car prices values**\n\nCarprice=−0.0925+0.3847×horsepower+0.3381×carwidth+1.3179×Carscategorytopnotchcars−0.1565×carbodyhatchback−1.5033×enginetypedohcv","metadata":{}},{"cell_type":"markdown","source":"## Model I Conclusions:\n\n* R-sqaured and Adjusted R-squared - 0.912 and 0.909 - 90% variance explained.\n\n* F-stats and Prob(F-stats) (overall model fit) - 284.8 and 1.57e-70(approx. 0.0) - Model fit is significant and explained 90%\n \n* variance is just not by chance.\n \n* p-values - p-values for all the coefficients seem to be less than the significance level of 0.05.\n \n-meaning that all the predictors are statistically significant.","metadata":{}},{"cell_type":"markdown","source":"# MODEL II\n\n* With lm_rfe8 which has basically 7 predictor variables.","metadata":{}},{"cell_type":"markdown","source":"### Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of it.","metadata":{}},{"cell_type":"code","source":"# Predicting the price of training set.\ny_train_price2 = lm_rfe8.predict(X_train_rfe8c)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.321502Z","iopub.execute_input":"2021-07-27T18:35:30.322155Z","iopub.status.idle":"2021-07-27T18:35:30.329106Z","shell.execute_reply.started":"2021-07-27T18:35:30.322108Z","shell.execute_reply":"2021-07-27T18:35:30.327929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price2), bins = 20)\nfig.suptitle('Error Terms Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.330805Z","iopub.execute_input":"2021-07-27T18:35:30.331291Z","iopub.status.idle":"2021-07-27T18:35:30.687526Z","shell.execute_reply.started":"2021-07-27T18:35:30.331256Z","shell.execute_reply":"2021-07-27T18:35:30.686328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the model.","metadata":{}},{"cell_type":"code","source":"X_test_2 = X_test_1[X_train_rfe8c.columns]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.688884Z","iopub.execute_input":"2021-07-27T18:35:30.689186Z","iopub.status.idle":"2021-07-27T18:35:30.694793Z","shell.execute_reply.started":"2021-07-27T18:35:30.689145Z","shell.execute_reply":"2021-07-27T18:35:30.693592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions using the final model\ny_pred2 = lm_rfe8.predict(X_test_2)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.696198Z","iopub.execute_input":"2021-07-27T18:35:30.696494Z","iopub.status.idle":"2021-07-27T18:35:30.708201Z","shell.execute_reply.started":"2021-07-27T18:35:30.696464Z","shell.execute_reply":"2021-07-27T18:35:30.707123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred2)\nfig.suptitle('y_test vs y_pred2', fontsize=20)   \nplt.xlabel('y_test ', fontsize=18)                       \nplt.ylabel('y_pred2', fontsize=16)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.709729Z","iopub.execute_input":"2021-07-27T18:35:30.710037Z","iopub.status.idle":"2021-07-27T18:35:30.993439Z","shell.execute_reply.started":"2021-07-27T18:35:30.710008Z","shell.execute_reply":"2021-07-27T18:35:30.992334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RMSE Score","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_pred2)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T18:35:30.994903Z","iopub.execute_input":"2021-07-27T18:35:30.995276Z","iopub.status.idle":"2021-07-27T18:35:31.002231Z","shell.execute_reply.started":"2021-07-27T18:35:30.995242Z","shell.execute_reply":"2021-07-27T18:35:31.001276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The R2 score of Training set is 0.918 and Test set is 0.915 which is quite close. Hence, We can say that our model is good enough to predict the Car prices using below predictor variables**\n\n* horsepower\n* carwidth\n* Cars_Category_TopNotch_Cars\n* carbody_hatchback\n* enginetype_dohcv\n* carbody_sedan\n* carbody_wagon","metadata":{}},{"cell_type":"markdown","source":"**Equation of Line to predict the Car prices values**\n\nCarprice=0.2440+0.3599×horsepower+0.3652×carwidth+1.2895×Carscategorytopnotchcars−0.4859×carbodyhatchback−1.4450×enginetypedohcv−0.3518×carbodysedan−0.4023×carbodywagon","metadata":{}},{"cell_type":"markdown","source":"### Model II Conclusions:\n\n* R-sqaured and Adjusted R-squared - 0.918 and 0.915 - 90% variance explained.\n \n* F-stats and Prob(F-stats) (overall model fit) - 215.9 and 4.70e-70(approx. 0.0) - Model fit is significant and explained 90% variance is just not by chance.\n \n* p-values - p-values for all the coefficients seem to be less than the significance level of 0.05.\n \n* meaning that all the predictors are statistically significant.","metadata":{}},{"cell_type":"markdown","source":"##  Closing Statement:\n\nBoth the models are good enough to predict the carprices which explains the variance of data upto 90% and the model is significant.","metadata":{}}]}