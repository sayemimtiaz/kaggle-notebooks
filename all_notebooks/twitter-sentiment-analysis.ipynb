{"cells":[{"metadata":{"_uuid":"52bf27bebf8e95c02bb8b123a1d535075e8376c4"},"cell_type":"markdown","source":"# School of AI Algiers Challenge - Twitter Sentiment Analysis"},{"metadata":{"_uuid":"ff9e92acf175c4e823c9536f6c5cc4268a0e8115"},"cell_type":"markdown","source":"This kernel is the solution for the challenge launched by School of AI - Algiers, which consist of building a system that can classify tweets as Sad or Happy."},{"metadata":{"_uuid":"85b0c81cca1fe16ef1b65c470e5aa989656c8fa2"},"cell_type":"markdown","source":"## Solution"},{"metadata":{"_uuid":"61b9efbf1cce15100bb9526da2017193b001fac5"},"cell_type":"markdown","source":"We will start by reading some tweets so we can understand our data better. We will then try to transform our tweets into something usable by different ML models, where we are going to choose the more efficient. We will finally fine tune our model and then test it to see its efficiency on new data.\n\nFor those who are not familiar with text analysis, this is a good [link](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html) to get you started.\n\nif you are here only to test your tweet, please run the kernel then go [here](#Test-your-tweet)"},{"metadata":{"_uuid":"6a11a0e30ff6e91ff073074521706bbcff59c47e"},"cell_type":"markdown","source":"# Update\n\nAfter getting some comments on the School of AI - Algiers group, especially from [Belkacem](https://www.kaggle.com/sambelkacem), I updated the following:\n- I used lemmatization instead of steaming\n\nI also noticed that I was mistaken when I stopped the max_features parameter at 20000 while doing GridSearch, I should have tested a bigger one, because if it stopped at 20000 (which is the max), it may get better using a bigger one. I just added None (no limit)."},{"metadata":{"_uuid":"1a6afc9118eca64bc4bb552cced34b42ca9c73f8"},"cell_type":"markdown","source":"### Content\n\n- [Loading the data](#Load-the-data)\n- [Visualize the tweets](#Visualize-the-tweets)\n    - [Emoticons](#Emoticons)\n    - [Most used words](#Most-used-words)\n    - [Stop words](#Stop-words)\n    - [Stemming](#Stemming)\n- [Prepare the data](#Prepare-the-data)\n    - [Bag of Words](#Bag-of-Words)\n    - [Building the pipeline](#Building-the-pipeline)\n- [Select a model](#Select-a-model)\n- [Fine tune the model](#Fine-tune-the-model)\n- [Testing the model](#Test)\n    - [Test your tweet](#Test-your-tweet)"},{"metadata":{"_uuid":"414a6b7e831613c3c9a88d9633fa412de72ebd8a"},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# This is for making some large tweets to be displayed\npd.options.display.max_colwidth = 100\n\n# I got some encoding issue, I didn't knew which one to use !\n# This post suggested an encoding that worked!\n# https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\ntrain_data = pd.read_csv(\"../input/train.csv\", encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2df12f6473bd02427771d3ca0ae2de291ec03b83"},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20b16d52f53f8b3005e5728ee36b3e3b8f5e6461"},"cell_type":"markdown","source":"# Visualize the tweets"},{"metadata":{"_uuid":"21098fa5bbbbcdb299b45ba1f7ab46013b4656f2"},"cell_type":"markdown","source":"From the tweets above, we can already make some remarks about the data:\n\n- We can see that there is some garbage like '&amp', '&lt' (which are basically used in HTML) that aren't gonna help us in our classification\n- In twitter, people mention their friends with tags like @username, there is a lot of them in our data. I was discussing with a friend about the usefulness of tags in our classification, for him, people tend to mention more friends when they are happy, but I think that people may mention people because they made bad things. When we face this kind of uncertainty, it's better to try the different options and evaluate which will do well, this is what we are gonna do."},{"metadata":{"trusted":false,"_uuid":"c2dfe4d1b2a68571cf47b270e61d7ec01458efa6"},"cell_type":"code","source":"# We will now take a look at random tweets\n# to gain more insights\n\nrand_indexs = np.random.randint(1,len(train_data),50).tolist()\ntrain_data[\"SentimentText\"][rand_indexs]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28f30b8fef70f32335fafa9dc847cec11321b759"},"cell_type":"markdown","source":"#### Note\nYou will not have the same results at each execution because of the randomization. For me, after some execution, I noticed this:\n- There is tweets with a url (like tweet 35546): we must think about a way to handle URLs, I thought about deleting them because a domain name or the protocol used will not make someone happy or sad unless the domain name is 'food.com'.\n- The use of hashtags: we should keep only the words without '#' so words like python and the hashtag '#python' can be seen as the same word, and of course they are.\n- Words like 'as', 'to' and 'so' should be deleted, because they only serve as a way to link phrases and words"},{"metadata":{"_uuid":"e1ae5c3aa81d998b37d87fa926d9682cd71358f2"},"cell_type":"markdown","source":"#### Emoticons\nThe internet language includes so many emoticons, people also tend to create their own, so we will first analyze the emoticons included in our dataset, try to classify them as happy and said, and make sure that our model know about them."},{"metadata":{"trusted":false,"_uuid":"406019e45c7d1c06f1739be273534a42e8002f42"},"cell_type":"code","source":"# We are gonna find what emoticons are used in our dataset\nimport re\ntweets_text = train_data.SentimentText.str.cat()\nemos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\nemos_count = []\nfor emo in emos:\n    emos_count.append((tweets_text.count(emo), emo))\nsorted(emos_count,reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8c5735902af0c0e4f485fde08cbd4af267bb37d"},"cell_type":"markdown","source":"We should by now know which emoticons are used (and its frequency) to build two regex, one for the happy ones and another for the sad ones. We will then use them in the preprocessing process to mark them as using happy emoticons or sad ones."},{"metadata":{"trusted":false,"_uuid":"fd47063d8a7d3be29b1e24f39ca4e4ce8411f815"},"cell_type":"code","source":"HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\nSAD_EMO = r\" (:'?[/|\\(]) \"\nprint(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\nprint(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0b0964a5bab89f2f9e235bb5a6bf384fe86c3c6"},"cell_type":"markdown","source":"#### Most used words\nWhat we are going to do next is to define a function that will show us top words, so we may fix things before running our learning algorithm. This function takes as input a text and output words sorted according to their frequency, starting with the most used word."},{"metadata":{"trusted":false,"_uuid":"d3eea0c5a024a8844060fea507a5ff374d765254"},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\n# Uncomment this line if you haven't downloaded punkt before\n# or just run it as it is and uncomment it if you got an error.\n#nltk.download('punkt')\ndef most_used_words(text):\n    tokens = word_tokenize(text)\n    frequency_dist = nltk.FreqDist(tokens)\n    print(\"There is %d different words\" % len(set(tokens)))\n    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"44c3a1b1bf2b665c537534bddebc1b78714a8f73"},"cell_type":"code","source":"most_used_words(train_data.SentimentText.str.cat())[:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27922113ab86ed096339614ee6616dfd6cf259fb"},"cell_type":"markdown","source":"#### Stop words"},{"metadata":{"_uuid":"3c24b5a72ed93b029c8492aee5e527dc81e898c7"},"cell_type":"markdown","source":"What we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them."},{"metadata":{"trusted":false,"_uuid":"760928a6057f2b8480d3c4f10ed3b2709ff62c6e"},"cell_type":"code","source":"from nltk.corpus import stopwords\n\n#nltk.download(\"stopwords\")\n\nmw = most_used_words(train_data.SentimentText.str.cat())\nmost_words = []\nfor w in mw:\n    if len(most_words) == 1000:\n        break\n    if w in stopwords.words(\"english\"):\n        continue\n    else:\n        most_words.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e8150fd219b8c83541820f373e345baaae29c1e0"},"cell_type":"code","source":"# What we did is to filter only non stop words.\n# We will now get a look to the top 1000 words\nsorted(most_words)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"875d77c51bc67d1fdc257e2951ef9722f2c3c3f9"},"cell_type":"markdown","source":"#### Stemming"},{"metadata":{"_uuid":"7d4a7d34f774493598373948c7ae886ef0eb49da"},"cell_type":"markdown","source":"You should have noticed something, right? There are words that have the same meaning, but written in a different manner, sometimes in the plural and sometimes with a suffix (ing, es ...), this will make our model think that they are different words and also make our vocabulary bigger (waste of memory and time for the learning process). The solution is to reduce those words with the same root, this is called stemming."},{"metadata":{"trusted":false,"_uuid":"2b8bf4694b417cafb10c592cde1c044390e43be8"},"cell_type":"code","source":"# I'm defining this function to use it in the \n# Data Preparation Phase\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n#nltk.download('wordnet')\ndef stem_tokenize(text):\n    stemmer = SnowballStemmer(\"english\")\n    stemmer = WordNetLemmatizer()\n    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n\ndef lemmatize_tokenize(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f2e05bc049b0a1c7b829fe1e888413382211831"},"cell_type":"markdown","source":"I will stop here, but you can visualize tweets more and more to gain insights and take decisions about how to transform your data."},{"metadata":{"_uuid":"2c7ab7592afcdc9199eb2aa95b4d4fb819527d5b"},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{"_uuid":"9cb9263cf1c21e8e0cf549ee570401d58b4f2c90"},"cell_type":"markdown","source":"In this phase, we will transform our tweets into a more usable data by our ML models."},{"metadata":{"_uuid":"b0e17b35a467bebb0839afb7df91f955d5313f05"},"cell_type":"markdown","source":"#### Bag of Words\nWe are going to use the Bag of Words algorithm, which basically takes a text as input, extract words from it (this is our vocabulary) to use them in the vectorization process. When a tweet comes in, it will vectorize it by counting the number of occurrences of each word in our vocabulary.\n\nFor example, we have this two tweets: \"I learned a lot today\" and \"hahaha I got you\".\n\n|tweet / words| I | learned | a | lot | today | hahaha | got | you |\n|-----|---|---------|---|-----|-------|--------|-----|-----|\n| first | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |\n| second | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n\nWe first extract the words present in the two tweets, then for each tweet we count the occurrences of each word in our vocabulary.\n\nThis is the simplest form of the Bag of Words algorithm, however, there is other variants, we are gonna use the TF-IDF (Term Frequency - Inverse Document Frequency) variant. You can read about it in the chapter I have provided in the beginning or in the official doc of scikit-learn [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"},{"metadata":{"trusted":false,"_uuid":"e8ec37f0fd4a0035440da78495500c0d8ecef5c1"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ac3748efa62b760d9c6d814d8bf727a75165e89"},"cell_type":"markdown","source":"#### Building the pipeline"},{"metadata":{"_uuid":"28b6d60ca26ddddcbf3f0317b411e7a2a4425c3d"},"cell_type":"markdown","source":"It's always a good practice to make a pipeline of transformation for your data, it will make the process of data transformation really easy and reusable.\nWe will implement a pipeline for transforming our tweets to something that our ML models can digest (vectors)."},{"metadata":{"trusted":false,"_uuid":"6d02e45a46b632277e17c3e6de2de1ecda255ed3"},"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae1998949f35a5a8f8e9c3e2fc8fd32441a2cb73"},"cell_type":"code","source":"# We need to do some preprocessing of the tweets.\n# We will delete useless strings (like @, # ...)\n# because we think that they will not help\n# in determining if the person is Happy/Sad\n\nclass TextPreProc(BaseEstimator,TransformerMixin):\n    def __init__(self, use_mention=False):\n        self.use_mention = use_mention\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # We can choose between keeping the mentions\n        # or deleting them\n        if self.use_mention:\n            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n        else:\n            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n            \n        # Keeping only the word after the #\n        X = X.str.replace(\"#\", \"\")\n        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n        # Removing HTML garbage\n        X = X.str.replace(r\"&\\w+;\", \"\")\n        # Removing links\n        X = X.str.replace(r\"https?://\\S*\", \"\")\n        # replace repeated letters with only two occurences\n        # heeeelllloooo => heelloo\n        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n        # mark emoticons as happy or sad\n        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n        X = X.str.replace(SAD_EMO, \" sademoticons \")\n        X = X.str.lower()\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99f19e43bfeedd6c51bb96e3bdbcc7d901088085"},"cell_type":"code","source":"# This is the pipeline that will transform our tweets to something eatable.\n# You can see that we are using our previously defined stemmer, it will\n# take care of the stemming process.\n# For stop words, we let the inverse document frequency do the job\nfrom sklearn.model_selection import train_test_split\n\nsentiments = train_data['Sentiment']\ntweets = train_data['SentimentText']\n\n# I get those parameters from the 'Fine tune the model' part\nvectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\npipeline = Pipeline([\n    ('text_pre_processing', TextPreProc(use_mention=True)),\n    ('vectorizer', vectorizer),\n])\n\n# Let's split our data into learning set and testing set\n# This process is done to test the efficency of our model at the end.\n# You shouldn't look at the test data only after choosing the final model\nlearn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n\n# This will tranform our learning data from simple text to vector\n# by going through the preprocessing tranformer.\nlearning_data = pipeline.fit_transform(learn_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81d61a981d1c32ddbe6932f0b968b9ab224538af"},"cell_type":"markdown","source":"# Select a model"},{"metadata":{"_uuid":"1e0257a8fb34da5109b27ca9a759b7202e58f3b5"},"cell_type":"markdown","source":"When we have our data ready to be processed by ML models, the question we should ask is which model to use?\n\nThe answer varies depending on the problem and data, for example, it's known that Naive Bias has proven good efficacy against Text Based Problems.\n\nA good way to choose a model is to try different candidate, evaluate them using cross validation, then chose the best one which will be later tested against our test data."},{"metadata":{"trusted":false,"_uuid":"0c4d82dc1ffb4872c8bf605a9f6212c7f785dc65"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\n\nlr = LogisticRegression()\nbnb = BernoulliNB()\nmnb = MultinomialNB()\n\nmodels = {\n    'logitic regression': lr,\n    'bernoulliNB': bnb,\n    'multinomialNB': mnb,\n}\n\nfor model in models.keys():\n    scores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\n    print(\"===\", model, \"===\")\n    print(\"scores = \", scores)\n    print(\"mean = \", scores.mean())\n    print(\"variance = \", scores.var())\n    models[model].fit(learning_data, sentiments_learning)\n    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f2ccb47aebf8e8c3596ca3103ed1a57cd88765f"},"cell_type":"markdown","source":"None of those models is likely to be overfitting, I will choose the multinomialNB."},{"metadata":{"_uuid":"569dfd9fd7e6244e1b20fc5bacf877799b2ddc85"},"cell_type":"markdown","source":"# Fine tune the model"},{"metadata":{"_uuid":"d1460670d69abd24d2758f52f4e2edf57e3d4148"},"cell_type":"markdown","source":"I'm going to use the GridSearchCV to choose the best parameters to use. \n\nWhat the GridSearchCV does is trying different set of parameters, and for each one, it runs a cross validation and estimate the score. At the end we can see what are the best parameter and use them to build a better classifier."},{"metadata":{"trusted":false,"_uuid":"ded567b9a20de3f5f47ebb4b64dc6602622e375e"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_search_pipeline = Pipeline([\n    ('text_pre_processing', TextPreProc()),\n    ('vectorizer', TfidfVectorizer()),\n    ('model', MultinomialNB()),\n])\n\nparams = [\n    {\n        'text_pre_processing__use_mention': [True, False],\n        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n        'vectorizer__ngram_range': [(1,1), (1,2)],\n    },\n]\ngrid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\ngrid_search.fit(learn_data, sentiments_learning)\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16c0270e6a97b641762eb9291bcf7d5ad491f450"},"cell_type":"markdown","source":"# Test"},{"metadata":{"_uuid":"93e6d4307224f92c00df9f9c10d91ee619ab685e"},"cell_type":"markdown","source":"Testing our model against data other than the data used for training our model will show how well the model is generalising on new data.\n\n### Note\nWe shouldn't test to choose the model, this will only let us confirm that the choosen model is doing well."},{"metadata":{"trusted":false,"_uuid":"30627c10aeb7c4cee488a431a7871a6e80c51baa"},"cell_type":"code","source":"mnb.fit(learning_data, sentiments_learning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1778d039bff617c9c38dd6cf73d7e814d819ab7c"},"cell_type":"code","source":"testing_data = pipeline.transform(test_data)\nmnb.score(testing_data, sentiments_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b69de6760ed7dde1db1ce9e361a1fd5fe1d42d0"},"cell_type":"markdown","source":"Not bad for my first attempt to solve a sentiment analysis problem. I will try to make it better if I got more free time."},{"metadata":{"trusted":true,"_uuid":"de2fd39a5c5c67e96ba8860d7f272795021d7253"},"cell_type":"code","source":"# Predecting on the test.csv\nsub_data = pd.read_csv(\"../input/test.csv\", encoding='ISO-8859-1')\nsub_learning = pipeline.transform(sub_data.SentimentText)\nsub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\nsub[\"Sentiment\"] = mnb.predict(sub_learning)\nprint(sub)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d2d4913e550c2722324fed22e08dc45be54d132"},"cell_type":"markdown","source":"### Test your tweet"},{"metadata":{"_uuid":"2875ccc4b702d56330d6b807b9d987679df822d8"},"cell_type":"markdown","source":"The most exciting part ! Don't be too hard with my classifier..."},{"metadata":{"trusted":false,"_uuid":"4e4cc5b6c3430c3eafe8a7b76250ebd613c6efb5"},"cell_type":"code","source":"# Just run it\nmodel = MultinomialNB()\nmodel.fit(learning_data, sentiments_learning)\ntweet = pd.Series([input(),])\ntweet = pipeline.transform(tweet)\nproba = model.predict_proba(tweet)[0]\nprint(\"The probability that this tweet is sad is:\", proba[0])\nprint(\"The probability that this tweet is happy is:\", proba[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c6d545fb7c7855c66512abdea266648f10984fe0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}