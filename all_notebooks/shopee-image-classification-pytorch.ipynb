{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a fine-tuning notebook which uses EfficientNet-b0 imagenet pretrained model as backbone. You can use any model from the `timm` library. Available models can be found via the `timm.list_models()` function.\n\nThe models can be used as a backbone in the ongoing [Shopee - Price Match Guarantee](https://www.kaggle.com/c/shopee-product-matching/) Challange."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install timm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\n\nimport cv2\nimport albumentations\n\nimport timm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DATA_DIR = Path(\"../input/shopee-product-detection/\")\n\ndf_train = pd.read_csv(BASE_DATA_DIR / \"train.csv\")\ndf_test = pd.read_csv(BASE_DATA_DIR / \"test.csv\")\n\ndf_train = df_train.loc[~df_train.filename.isin([\"64faf0b221af4767ba8c167b228fde00.jpg\", \n                                                 \"d946ee19ac1d2997bac5f18ce75656cb.jpg\"])].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = df_train.category.value_counts()\ndf_train.category.max(), df_train.category.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nplt.bar(counts.index, counts)\nplt.xticks(range(42));\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom contextlib import contextmanager\n\nLOGS_PATH = Path(\"logs\")\nLOGS_PATH.mkdir(exist_ok=True)\n\n\ndef init_logger(log_file=LOGS_PATH / 'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\nLOGGER = init_logger()\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_IMG_DIR = Path(\"../input/shopee-product-detection/train/train/\")\n\ndef read_img_and_cvt_format(img_path, clr_format=cv2.COLOR_BGR2RGB):\n    return cv2.cvtColor(cv2.imread(img_path), clr_format)\n\ndef visualize_batch(img_ids, labels):\n    \n    plt.figure(figsize=(16, 12))\n    \n    for idx, (img_id, label) in enumerate(zip(img_ids, labels)):\n        plt.subplot(3, 3, idx + 1)\n        img_fn = str(BASE_IMG_DIR / img_id)\n        img = read_img_and_cvt_format(img_fn)\n        plt.imshow(img)\n        plt.title(f\"Class: {label}\", fontsize=9)\n        plt.axis(\"off\")\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampled_df = df_train.sample(9)\nimg_ids = sampled_df[\"filename\"].values\nlabels = sampled_df[\"category\"].values\n\nvisualize_batch(img_ids, labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass ShopeeDataset(Dataset):\n    \n    def __init__(self, image_paths, labels=None, transform=None):\n        \n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        \n        img_filepath = self.image_paths[idx]\n        img = read_img_and_cvt_format(img_filepath)\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n        \n        label = 0\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx]).long()\n        return img, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_paths = [f\"{BASE_IMG_DIR}/{img_id}\" for img_id in df_train[\"filename\"].values]\ntrain_dataset = ShopeeDataset(image_paths=train_img_paths, \n                               labels=df_train[\"category\"].values,\n                               transform=None)\n\nfor i in range(1):\n    img, label = train_dataset[i]\n    \n    plt.title(f\"Label: {label}\")\n    plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train.category.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config:\n    \n    model_name = \"efficientnet_b0\" # resnet34\n    n_epochs = 10\n    batch_size = 32\n    img_size = 512\n    n_classes = len(df_train.category.unique())\n    lr = 1e-3\n    weight_decay = 1e-6\n    gradient_accumulation_steps = 1\n    max_grad_norm = 1000\n    seed = 42\n    scheduler = \"\"\n    n_fold = 1\n    train_fold = [0, 1, 2, 3, 4]\n    train = True\n    print_every = 100\n    num_workers = 4\n    \n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=Config.seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass Classifier(nn.Module):\n    \n    def __init__(self, model_name, pretrained=False):\n        super(Classifier, self).__init__()\n        \n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        if model_name.startswith(\"eff\"):\n            n_features = self.model.classifier.in_features\n            self.model.classifier = nn.Linear(n_features, Config.n_classes)\n        else:    \n            n_features = self.model.fc.in_features\n            self.model.fc = nn.Linear(n_features, Config.n_classes)\n        \n    def forward(self, x):\n        return self.model(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations.pytorch import ToTensorV2\nfrom torchvision import transforms as T\n\ndef get_train_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(\n            Config.img_size, Config.img_size),\n        albumentations.Transpose(),\n        albumentations.HorizontalFlip(),\n        albumentations.VerticalFlip(),\n        albumentations.ShiftScaleRotate(),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225]),\n        albumentations.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n        ToTensorV2(),\n    ])\n\n\ndef get_test_transforms():\n    \n    return albumentations.Compose([\n        albumentations.Resize(Config.img_size, Config.img_size),\n        albumentations.Normalize(mean=[0.485, 0.456, 0.406], \n                  std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metric Tracking"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport time\n\n\nclass AverageMeter:\n    \n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \n        \ndef as_minutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return f\"{m}m {s}s\"\n\n\ndef time_since(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / percent\n    rs = es - s\n    return f\"{as_minutes(s)} (remain {as_minutes(rs)})\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(model, data_loader, criterion, optimizer, epoch, scheduler, device):\n    \"\"\"\n    There is no scheduler update currently.\n    \"\"\"\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    # scores = AverageMeter()\n    \n    model.train()\n    start = end = time.time()\n    # global_step = 0\n    total_len = len(data_loader)\n    \n    for step, (images, labels) in enumerate(data_loader):\n        \n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        preds = model(images)\n        loss = criterion(preds, labels)\n        losses.update(loss.item(), batch_size)\n        \n        if Config.gradient_accumulation_steps > 1:\n            loss = loss / Config.gradient_accumulation_steps\n        \n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), \n                                                   Config.max_grad_norm)\n        if (step + 1) % Config.gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n            # global_step += 1\n        \n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % Config.print_every == 0 or step == (total_len - 1):\n            print(f\"Epoch: [{epoch+1}][{step}/{total_len}] \"\n                  f\"Data: {data_time.val:.3f} ({data_time.avg:.3f}) \"\n                  f\"Batch: {batch_time.val:.3f} ({batch_time.avg:.3f}) \"\n                  f\"Elapsed: {time_since(start, float(step + 1) / (total_len))} \"\n                  f\"Loss: {losses.val:.5f}({losses.avg:.5f}) \"\n                  f\"Grad: {grad_norm:.4f}\" # LR: {lr:.6f}\n                 )\n        \n    return losses.avg\n            \n\ndef valid_step(model, data_loader, criterion, device):\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    model.eval()\n    start = end = time.time()\n    total_len = len(data_loader)\n    predictions = []\n    \n    for step, (images, labels) in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        \n        with torch.no_grad():\n            preds = model(images)\n        \n        loss = criterion(preds, labels)\n        losses.update(loss.item(), batch_size)\n        predictions.append(preds.softmax(1).cpu().numpy())\n        \n        if Config.gradient_accumulation_steps > 1:\n            loss = loss / Config.gradient_accumulation_steps\n            \n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % Config.print_every == 0 or step == (total_len - 1):\n            print(f\"Eval: [{step}/{total_len}] \"\n                  f\"Data: {data_time.val:.3f} ({data_time.avg:.3f}) \"\n                  f\"Batch: {batch_time.val:.3f} ({batch_time.avg:.3f}) \"\n                  f\"Elapsed: {time_since(start, float(step + 1) / total_len)} \"\n                  f\"Loss: {losses.val:.5f} ({losses.avg:.5f})\"\n                 )\n    \n    predictions = np.concatenate(predictions)\n    return losses.avg, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !rm -rf models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nMODELS_DIR = Path(\"models\")\nMODELS_DIR.mkdir(exist_ok=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(df_tr, df_val):\n\n    train_img_paths = [f\"{BASE_IMG_DIR}/{img_id}\" for img_id in df_tr[\"filename\"].values]\n    valid_img_paths = [f\"{BASE_IMG_DIR}/{img_id}\" for img_id in df_val[\"filename\"].values]\n    \n    train_dataset = ShopeeDataset(\n        train_img_paths, \n        labels=df_tr[\"category\"].values, \n        transform=get_train_transforms()\n    )\n    \n    valid_dataset = ShopeeDataset(\n        valid_img_paths,\n        labels=df_val[\"category\"].values,\n        transform=get_test_transforms()\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset, batch_size=Config.batch_size, \n        shuffle=True, num_workers=Config.num_workers\n    )\n    valid_data_loader = DataLoader(\n        valid_dataset, batch_size=Config.batch_size, \n        shuffle=False, num_workers=Config.num_workers\n    )\n    \n    model = Classifier(Config.model_name, pretrained=True)\n    model.to(device)\n    # amsgrad = False\n    optimizer = optim.Adam(model.parameters(), \n                           lr=Config.lr, \n                           weight_decay=Config.weight_decay)\n    criterion = nn.CrossEntropyLoss()\n\n        \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n                                                                     T_0=10, \n                                                                     T_mult=1, \n                                                                     eta_min=1e-6, \n                                                                     last_epoch=-1)\n    \n    best_score = 0.0\n    best_loss = np.inf\n\n    for epoch in range(Config.n_epochs):\n        \n        start_time = time.time()\n        avg_epoch_loss = train_step(model, \n                                    train_data_loader, \n                                    criterion, \n                                    optimizer, \n                                    epoch, \n                                    scheduler=scheduler, \n                                    device=device)\n\n        avg_valid_loss, valid_preds = valid_step(model, \n                                                 valid_data_loader, \n                                                 criterion, \n                                                 device)\n        valid_labels = df_val[\"category\"].values\n        accuracy = accuracy_score(valid_labels, valid_preds.argmax(1))\n        classification_result = classification_report(valid_labels, \n                                                      valid_preds.argmax(1))\n        elapsed = time.time() - start_time\n        LOGGER.info(f\"Epoch: {epoch+1} - avg_epoch_loss: {avg_epoch_loss:.5f} - avg_val_loss: {avg_valid_loss:.5f} - time: {elapsed:.0f}s\")\n        LOGGER.info(f\"Epoch: {epoch+1} - Accuracy: {accuracy}\")\n        print(classification_result)\n        \n        if accuracy > best_score:\n            best_score = accuracy\n            LOGGER.info(f\"Epoch: {epoch+1} - Save best score: {best_score:.4f} Model\")\n            torch.save({\n                \"model\": model.state_dict(),\n                \"preds\": valid_preds\n            }, str(MODELS_DIR / f\"{Config.model_name}_best.pth\"))\n            \n#     check_point = torch.load(str(MODELS_DIR / f\"{Config.model_name}_fold_{fold}_best.pth\"))\n#     valid_folds[[str(c) for c in range(5)]] = check_point[\"preds\"]\n#     valid_folds[\"preds\"] = check_point[\"preds\"].argmax(1)\n#     return valid_folds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_, df_valid_ = train_test_split(df_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train_.category.unique()), len(df_valid_.category.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loop(df_train_, df_valid_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}