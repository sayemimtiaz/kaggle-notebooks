{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hands-on Machine LEarning with Scikit-learn, Keras & Tensorflow\n\n## Aurelien Geron\n\n## Second EDITION\n\n### OREILLY\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"\nimport os\nimport tarfile\nimport urllib\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\n# Ipython magic line which indicate the graphs depends on Matplotlib as backend for displaying\n%matplotlib inline \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main Functions \n\n\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n\n    def fit(self, X, y=None):\n        return self # I do not need to fit anything but because of sklearn syntax I pass X and Y\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nclass SelectTopFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, features_important, k_features):\n        self.features_important = features_important\n        self.k_features = k_features\n\n    def fit(self, X, y=None):\n        self.features_indeces = top_k_features(self.features_important, self.k_features)\n        return self \n    def transform(self, X):\n        return X[:, self.features_indeces]\n\ndef top_k_features(features_importances, k):\n    return np.sort(features_importances.argsort()[-k:])\n\ndef split_train_test(data, test_ratio):\n    np.random.seed(42) # to genrate same permutation of indices each time\n    # shuffle indices from 0 to length of data\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio) # .2 * length of the data\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n\n\n\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_file = pd.read_csv(\"../input/california-housing-prices/housing.csv\")\ndf_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Info() Method\n\nThe info() method help us to get intuition about each column(attribute) in our dataset either in:\n-  The data type of each column.\n- The number of Non-Null values that this column has(overall cell that not null).\n- The memory that this file takes from our RAM.","metadata":{}},{"cell_type":"code","source":"df_file.info() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# value_counts() Method\n\nWhen it comes to categorical variables which hold some of the discrete values like classification problem, in general \nyou have number of classes like all cell in this columns will have one of these values either [0,1,2], so you have three classes, but what about the cell that contain **class 1** then the *value counts method* will count this for each class. ","metadata":{}},{"cell_type":"code","source":"# You can see the discrete categorical in the column ocean_proximity on left side and their counts on right side\ndf_file.ocean_proximity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# describe() Method\n\nIn statics we have different measurement of analysis, which get your intuition about the data you dealing with like the average  of the data [2, 3, 1,4, 3, 5], so you will first rearrange them not for the mean but for other statics calculation, so it will be [1,2,3, 3,4,5] then the mean(average) = (1+2+3+3+4+5) = 18 /  how many numbers you summation = 18/6 = 3, also the **median** will be 3 the mode will be 3, other values we are trying to get is which called **first Quarter Range**, **IQR** and **Third Quarter Range**, which here repsendted as 25%, 50%, 75%.\n\n- Others like Standard Deviation which the spread of data points from the mean value.\n- min = the minium value in this columns.\n- count = the number of non-null cells in this column\n- max = the max value in this columns\n\n# Note !\n\nYou should be noticed that there are just 9 columns even of the dataset is contain 10 columns but this because should we know these statistics are only worked on numbers.","metadata":{}},{"cell_type":"code","source":"df_file.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# hist() Method\n\nThe histogram method give you a quick review of the values in your data and their frequency.\nThis is helpful for a quick overview of how the most frquent range of values, also about the maxmium and minum of ranges, display also which columns need to hande the features scling for.\nIt also suggests questions for you to look for just like:\n\nThe housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will useyour systemâ€™s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n- Collect proper labels for the districts whose labels were capped.\n- remove those districts from the training set (and also from the test set, since your system shouldnot be evaluated poorly if it predicts values beyond $500,000)","metadata":{}},{"cell_type":"code","source":"df_file.hist(bins=20, figsize=(15, 15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NOTES !!\n\nWe can noticed some of the points that related to graphs above like the different scales that the attributes have and for machine learning its better to pass the attributes at some ranges, what we called features scaling.\n\nAlso the mendain_income attribute is from .5 to 15 and its ambigous so you need to check this with the owner to know how it going from maybe it means 15,000 or multiply by spesific value.\n\nAlso, the price of the house if should go beyond the max and min values or not, and the ranges that have a small number of instances should be removed or not because maybe it causes misleading for the model.\n\nSome of the attributes have skewed right graphs, why?\n\nAll of these are questions you can extract from the graphs to ask and trying to find answers.","metadata":{}},{"cell_type":"code","source":"# Call the hist method on just one column\ndf_file['total_bedrooms'].hist(bins=20, figsize=(7, 7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a Test Data\n\nThis is not easy step or stage, its not actually about split the data, but its about how you ensure that your train and test dataset are included all of the different spectrum of the data which means not bias to some category of the data.\n","metadata":{}},{"cell_type":"code","source":"train_set, test_set = split_train_test(df_file, .2)\nprint(len(train_set))\nprint(len(test_set))\ntrain_set[:3] # see that first unnamed column have the random indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note !\n\nThe problem here is that every time you will run you will have another test and train set which after you train your model with time your model will train overall the data as it with a time its see the test data, but to avoid this either to save your test data in separate file or to make a **seed**, and its will generate the same shuffle indices.","metadata":{}},{"cell_type":"code","source":"# using sklearn library\ntrain_set, test_set = train_test_split(df_file, test_size=.2, random_state=42)\nprint(len(train_set))\nprint(len(test_set))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Size of the dataset\n\nWhen the dataset are large enough relative to number of attributes(features), it okay to consider these random method, but if not, you run risk of introducing a signigcant sampling **bias**.\n\nIn this case we use the method called ** Stratified Random Sampling **, is a method of sampling that involves the division of a population into smaller sub-groups known as strata, the strata are formed based on members shared attributes or characteristics such as income or educational attainment.\n\nStratified random sampling is also called proportional random sampling or quota random sampling. \n\nStratified random sampling allows researchers to obtain a sample population that best represents the entire population being studied.\n","metadata":{}},{"cell_type":"code","source":"# Call the hist method on just one column\ndf_file['median_income'].hist(bins=20, figsize=(7, 7))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the problem above in this example, let us imagine that you have split the dataset to train and test randomly using some of straightforward method, as we can see may be your split will take all random instance in the train between 1 to 8 since most of the median income and and test the same was happend except a few example from medain income above 8, but like these problems will be baised since most of your instance are from range 1 to 8, also what happen if it split most of your train in range from 1 to 8 and you find that the test have medain income from 1 to 14, it will misleading your model.\n\n**So its important ot have a sufficient number of instance(rows) in your dataset for each stratum. Or it will cause to bais**","metadata":{}},{"cell_type":"code","source":"df_file['income_cat'] = pd.cut(df_file['median_income'],\n                              bins=[0., 1.5, 3.0, 4.5, 6., np.inf], \n                              labels=[1, 2, 3, 4, 5])\ndf_file['income_cat'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is we can see that its create a new attribute with 5 categories from 1 to 5 and each category have range like category 1 have range from 0 to 1.5 and category 2 have range from 1.5 to 3.\n\n**Since the median income attribute have its own effect on the price of the house we can make this variable is resposible for our spliting to the dataset instead of the simple methods.**","metadata":{}},{"cell_type":"code","source":"def measure_income_compare(data):\n    class_list = [0, 0, 0, 0, 0]\n    for i in data['median_income']:\n        if i >= 0 and i < 1.5:\n            class_list[0] +=1\n        elif i >= 1.5 and i < 3:\n            class_list[1] +=1\n        elif i >= 3 and i < 4.5:\n            class_list[2] +=1\n        elif i >= 4.5 and i < 6:\n            class_list[3] +=1\n        else: class_list[4] +=1\n    return class_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=.2 , random_state=42)\nfor train_index, test_index in split.split(df_file, df_file['income_cat']):\n    strat_train_set = df_file.loc[train_index] # retrive rows with these indces\n    strat_test_set = df_file.loc[test_index] # retrive rows with these indces\nstrat_train_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of each category like from 0 to 1.5 about some precentage\nstratified = np.array(strat_test_set['income_cat'].value_counts() / len(strat_test_set))\nrandom = np.array(measure_income_compare(test_set))/len(test_set)\noverall = np.array(measure_income_compare(df_file))/ len(df_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stratified.sort()\nrandom.sort()\noverall.sort()\ncompare_dict = {'overall': overall, 'stratified': stratified, 'random': random}\ncompare_df = pd.DataFrame(compare_dict)\n\n# In table below we can see how stratified static method most similar to overall coverage of our dataset\n# look also at the simple random Vs overall\ncompare_df['Stratified. %error'] = np.abs((compare_df['stratified'] - compare_df['overall']) * 100)\ncompare_df['Random. %error'] = np.abs((compare_df['random'] - compare_df['overall']) * 100)\ncompare_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strat_train_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now remove income cat to back data to its original form\nstrat_train_set.drop(\"income_cat\", axis=1, inplace=True)\nstrat_test_set.drop(\"income_cat\", axis=1, inplace=True)\nstrat_train_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discover and Visualize the data to gain insights\n\nWith some of the analysis above we get a quick intuation about the dataset, but more to go in depth of understanding the data we dealing with, we need to discover these data more than this quick intuation, and the graphs are the most common to discover the data since our brain is very good spotting patterns in pictures.\n\nFirst keep your test set aside and make the discover on the training and also take a copy to keep the original training set unchanged, beside of that if the training data is large just look into sample of the data to consider time and memory usage.\n## Visualizing Geographic Data\n\nSince there is a geographical information (longtuide and latuide), it's helpful to know about which area have the most house or the density of your data in which are, it may help you to know also why price in some area are greater than the other areas.","metadata":{}},{"cell_type":"code","source":"df_file.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will give you a littile information,because you have not see any pattern of information.\n\nThen in like these cases the paramters of the graph can help a lot to help you go deeper in discover process.\n\n- What about change the opacity of the circle of scatter plot\n\n\n","metadata":{}},{"cell_type":"code","source":"df_file.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It help you to know the density by overlapping the circle if this area have a lot of house there, also other area that have small number of house will be appear because of its opacity will be low.**\n\n- Not just alpha, other parameters of the graph can help you discover more pattern","metadata":{}},{"cell_type":"code","source":"df_file.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=.3,\n            s=df_file['population']/50, label=\"population\", figsize=(10,5),\n            c=\"median_house_value\", cmap=plt.get_cmap('jet'), colorbar=True)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters\n\n- s, is the size of the data point, and s=df_file['population']/50, it help us know which area has most density.\n- c, is the color that represent the price, which represent range from blue(low price) to red (high price) as we can see in the colorbar.\n- cmap, color map which replace the default (black and white) of c, to blue and red.\n- colorbar, true to represent the range of color to notice explain the point on the graph.\n\n**From the graph you can now get a clear intuition about how the location affect the price of the house., not just the location also the population density**\n","metadata":{}},{"cell_type":"markdown","source":"# Looking For correlation\n\nAs we know our target variables, it's helpful to get the correlation of this target variables against your attributes.","metadata":{}},{"cell_type":"code","source":"corr_matrix = df_file.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that median_house_value is so correlated with it self and from this point we should noticed that the values are in range from -1 to 1 and 1 which represent positive higher dpendancy, between that the relationship are close to be or no as in independent variables with 0 correlation or -1 when its strong negative relationship.\n\nActually the correlation is like the linear equation between x & y, if its 1 then x & y are equal which represent a straight line and for other values it will be different.\n\n**Now maybe we need to get close to median income and graph it against median_house_value as it make the most affect on the price Or we can display scatter_matrix for the most three or four attributes againsit each other**","metadata":{}},{"cell_type":"code","source":"# Scatter_matrix \npd.plotting.scatter_matrix(df_file[['median_house_value', 'median_income', 'total_rooms']], figsize=(10, 7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get close to median house value against median income\ndf_file.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note !\n\n**This attribute median income have a lot of affect on the price of the house as we can see even with small alpha, not just that, it also can represent some of the horizontal lines at 500,000 and between 3000,00 and 4000, 00, and this may indicate that there is no affect of the median income when in either small or large price the house value is still large, and in this case may be we need to remove like these points from our dataset because it may cause the learning algorithm to overfitting or misleading learning**.\n\n\n\n# Attribute Combination\n\nThe most helpful thing of machine learning is that it accept the creativity of your solution not just what the data present you should use, no you have the ability to improve the dataset maybe remove some attribute as it has no effect, combine two attributes via math equation, or create new attribute from existing ones.\n\n- median_house_value    1.000000\n- median_income         0.688075\n- total_rooms           0.134153\n- housing_median_age    0.105623\n- households            0.065843\n- total_bedrooms        0.049686\n- population           -0.024650\n- longitude            -0.045967\n- latitude             -0.144160\n\n\nWe can see here the correlation of the previous stage and if we noticed that the total_rooms can help in finding the number of rooms per house not total room per households, also total_bedrooms and population can help us create a new correlation between attributes.\n","metadata":{}},{"cell_type":"code","source":"df_file['rooms_per_household']     = df_file['total_rooms']    / df_file['households']\ndf_file['bedrooms_per_room']       = df_file['total_bedrooms'] / df_file['total_rooms']\ndf_file['pupulation_per_ousehold'] = df_file['population'] / df_file['households']\n# Now look at the new correlation and find new insights\ncorr_matrix = df_file.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)\ndf_file.head() # look at the dataset now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some new insights\n\n- Bedrooms_per_room decrease with increasing of median_house_value (So strong correlation).\n- Rooms_per_household, increase the price of the house by increasing the rooms per house.\n\n\n# Prepare Data for Machine Learning Algorithm\n\nWe doing a pipeline for this process to use with future work or similar data or with the training and testing data or even on the system when we lunch model to work on, we need the new data to feed in this pipeline.\n\n**We will separate the target variables from the other attributes**\n\n","metadata":{}},{"cell_type":"code","source":"# drop median_house_value and save rest of strat_train_set into df_file\ndf_file = strat_train_set.drop(\"median_house_value\", axis=1) \ndf_file_labels = strat_train_set['median_house_value'].copy() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\n\nThere are three option to handle missing values(Nan) values:\n- Get rid of corresponding rows\n- Get rid of the attribute itself\n- Replace with meaningful value, in numbers may be median, mode, zero, and other suitable static based on the problem.","metadata":{}},{"cell_type":"code","source":"# only one attribute has missing values which is total_bedrooms\ndf_file.dropna(subset=[\"total_bedrooms\"])\ndf_file.drop(\"total_bedrooms\", axis=1)\nmedian = df_file['total_bedrooms'].median()\ndf_file['total_bedrooms'].fillna(median, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note !\n\nWhen we use option 3, we should save the median for test dat and other new data when system go live.\n\nAlso, we can use the sklearn impute library to handle missing values.\n\nAlso, we will rid the one-categorical attribute because the imputer can not deal with text once you pass static median. and we make this process on all the numbers attributes , because maybe in the large dataset which has a lot of attributes it will be difficult to check each attribute or even when the system go life maybe some of attributes can returned without values.","metadata":{}},{"cell_type":"code","source":"imputer = SimpleImputer(strategy=\"median\")\ndf_file_num = df_file.drop(\"ocean_proximity\", axis=1)\nimputer.fit(df_file_num)\n# As we save our median for future sklearn imputer handle this and save it for you in statistics_ attribute \nimputer.statistics_ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To ensure of these values calculated by sklearn\ndf_file_num.median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = imputer.transform(df_file_num) # the result is numpy array\ndf_file_tr = pd.DataFrame(X, columns=df_file_num.columns, index=df_file_num.index)\ndf_file_tr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling Text and Categorical Attributes\n\nAfter we have looked at the numerical attributes we also need to take a look on the text or categorical attributes, because machine learning models can not deals with text.","metadata":{}},{"cell_type":"code","source":"df_file_cat = df_file['ocean_proximity']\nprint(type(df_file_cat))\ndf_file_cat = df_file[['ocean_proximity']]\nprint(type(df_file_cat))\ndf_file_cat.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can noticed that, it's a categorical attribute not just any text, and its repeated with same case and characters, we can use machine learning OrdinalEncoder class to deal with this categorical attribute.**","metadata":{}},{"cell_type":"code","source":"ordinal_encoder = OrdinalEncoder()\ndf_file_cat_encoded = ordinal_encoder.fit_transform(df_file_cat)\nprint(df_file_cat_encoded[:10])\n# _ means learned parameters while **categories** without _ means hyperparameters\nprint(ordinal_encoder.categories_) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In fitting categorical data should we know it's ordinal category or not, because like this case as it not ordicanal the ml may learn pattern from that, and we do not need like this to happen**\n\nThe solution in this case is to use binary representation for each category with 1 to such appearing category and 0 for the others, so it will be vector of 4 numbers * the number of the dataset.\n\n**Sklearn also has onehotencoder method to handle this.**","metadata":{}},{"cell_type":"code","source":"one_hot_encoder = OneHotEncoder()\ndf_file_cat_1hot = one_hot_encoder.fit_transform(df_file_cat)\ndf_file_cat_1hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sparse Matrix\n\nBecause the one-hot encoded method give 1 for which category and 0 for others, this will generate a sparse matrix with most of 0, since of that sklearn save only the location of each 1 in the matrix and ignore the zeros, but we can retrieve the whole matrix using **toarray** method associated with the df_file_cat_1hot object.","metadata":{}},{"cell_type":"code","source":"df_file_cat_1hot.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_encoder.categories_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Transformer\n\nEverything has an end and the library you use maybe it provide you with a lot of helpful APIs, but sometimes you need to create your transformer, which based on the analysis you made.\n\nAnd to combine this with sklearn it helpful to apply it in the form that sklearn do, as [fit, transform, fit_transform].\n\nAlso you can have other methods or attributes as you need, maybe you have another function of cleaning and you need to make a pipeline for each of stage you have.\n\nOther than that you can combine attributes of hyperparameters for tuning.\n","metadata":{}},{"cell_type":"code","source":"attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\ndf_file_extr_attribs = attr_adder.transform(df_file.values) # as numpy array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features Scaling\n\nOne of the most interesting and helpful stages is the feature scaling when the dataset attributes go to different ranges like some attribute between [1-100], others [1000, 10000000], and others are different,  in this case of numerical ranges of different attributes, it's helpful to standardize all the numeric attributes to specific range always [0-1], which applied by the Max-Min function, which divides each value by max minus min which will restrict all values to be between 0 and 1, other function is using standard deviation and mean, but its goes to the different distribution of ranges. \n\nThis is help machine learning to perform well in this case of **Features Scaling**, also some times when the model in learning if we do not apply **Features Scaling** and values of some attributes are large, this can cause explode of values in the iteration when the model trying to minimize the cost function.\n","metadata":{}},{"cell_type":"markdown","source":"# Transformation Pipelines\n\nAs we can see, we have passed through different data transformation stages, because of that we should care of the order these transformer are applied to the data, So we can use the idea of **PipeLine** either create your own transformer function or use sklearn pipeline.\n","metadata":{}},{"cell_type":"code","source":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('attribs_addder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\ndf_file_num_tr = num_pipeline.fit_transform(df_file_num)\ndf_file_num_tr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PipeLine\n\nThe pipeline constructor of sklearn takes a list of tuples each of these tables are pairs of **name/estimator**, the Last table should contain transformer estimator.\n\nWhen we cal the fit() method of Pipeline object, it calls fit_transform() **fit_transform() of object in the pipeline** methods sequentially on all transformers, then it pass the result output of each call to next call until the end of pipeline\n\n# Numerical & Categorical Attributes\n\nWe also need to make our pipeline to handle the two different types of data Numerical and Categorical, so we designed another pipeline that works with all columns (attributes).\n\nAlso, sklearn has **ColumnTransformer** that we can feed the full pipeline to and it works fine with pandas data frames.","metadata":{}},{"cell_type":"code","source":"num_attribs = list(df_file_num)\nnum_attribs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_attribs = [\"ocean_proximity\"]\ndf_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs),\n])\ndf_file_prepared = full_pipeline.fit_transform(df_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select and Train a Model\n\nNow we are ready to select and train the Machine Learning Model, after all these previous steps, now the rest is simple than what we have before.\n\nJust we can import and use linear regression model, lets do it.\n","metadata":{}},{"cell_type":"code","source":"df_file_prepared.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now its done we fit the model we can test some prediction and see the score\nsome_data = df_file.iloc[:5]\nsome_labels = df_file_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Prediction: \", lin_reg.predict(some_data_prepared))\nprint(\"Real Lables: \", list(some_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets do it on the whole dataset\ndf_file_predictions = lin_reg.predict(df_file_prepared)\nlin_mse = mean_squared_error(df_file_predictions, df_file_labels)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Underfitting\nAs we can see how its bad prediction since of this large error, and as we talked in chapter 1 its underfitting problem, which the model have not fit the data well, and this may back to number of things:\n\n- The Model is so simple to get pattern from the data, or to fit the data as it linear equation.\n- The dataset itself may contain insufficient attributes to let the model learn from.\n- insufficient quantity of the training data.\n- Regularization part if we have.\n- irrelevant features(attributes), which can mean that features does not provide the model with enough information to make a good prediction.\n","metadata":{}},{"cell_type":"markdown","source":"# DecisionTreeRegressor\n\nNow we are going to use more complex model which can find a nonlinear relationship in the data.","metadata":{}},{"cell_type":"code","source":"tree_reg = DecisionTreeRegressor()\ntree_reg.fit(df_file_prepared, df_file_labels)\ndf_file_predictions = tree_reg.predict(df_file_prepared)\ntree_mse = mean_squared_error(df_file_predictions, df_file_labels)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overfitting\n\nAs we also talked about earlier in the previous chapter the model may be overfitting your training data, and for that, we kept aside part of the data which the test set to test our model on unseen examples, and see how it works,the model has no error on the training data, but what if we have predicted the test data.\n\nBut as we also know it's helpful to cut part of training data as it called (dev-set) for validating the model, because we should use the test data when we are ready to launch the model to be confident of this model.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Better Evaluation Using Cross-Validation\n\nWe can use sklearn train_test_split function to split the training set into train and validation sets, then train model on this training set and evaluate it on validation set.\n\nBut a better way is to use K-Cross-Validation method, which help you to train and validate your model on the whole training set, since it devide your training set into K flods, use one of them to evalute as validation set and K-1 to train on, next iteration use another flod and repeat the process to end of K-foods which have trained and evaluated on.\n","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(tree_reg, df_file_prepared, df_file_labels, cv=10,\n                        scoring=\"neg_mean_squared_error\")\ntree_rmse_scores = np.sqrt(-scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard deviation: \",scores.std())\n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_scores(tree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note !\n**As we can see that the tree model is same as linear regression even of it fit well on the training data it cause overfitting problem and have fall on the validation set**","metadata":{}},{"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, df_file_prepared, df_file_labels, cv=10,\n                            scoring='neg_mean_squared_error')\nlin_rmse_scores = np.sqrt(-lin_scores)\n\ndisplay_scores(lin_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomForestRegressor\n\nAnother model can help us to ensure that the underfitting problem is away of simpler or complex usage models, it can be one of the other problems that underfitting problems happen because of it.\n\n**Instead of using just one Decision tree the  RandomForestRegressor trying many Decision trees on random subset of the features (attributes), then averaging out their prediction**\n\n# Note !\nLike this models **RandomForestRegressor** called **Ensemble** since it building a model on top of many other models which can push machine learning algorithm to go further than expect.","metadata":{}},{"cell_type":"code","source":"forest_reg = RandomForestRegressor()\nforest_reg.fit(df_file_prepared, df_file_labels)\ndf_file_predictions = forest_reg.predict(df_file_prepared)\nforest_mse = mean_squared_error(df_file_predictions, df_file_labels)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, df_file_prepared, df_file_labels, cv=10,\n                            scoring='neg_mean_squared_error')\nforest_reg_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_reg_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Note !\n\nThe score now looks better than other models, but we still have the overfitting problem as we can see how it fit well on the training, and fill in evaluation on the validation set.\n\nWe can work with overfitting problems via:\n\n- If We can get more training data.\n- We can provide the model with suitable regularization.\n- Look at the number of attributes against number of training examples, if they are close we can rid of some attributes.\n- Reduce the noise of the data.\n- Try simpler models.","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tune Models\n\nThis stage comes late because after we have run our pipeline, then trying different models, we have to choose which best of them even if there is more than one model, you next need to **fine-tune** the parameters of the model to make it doing best as you can, after this step of cross-validation that we have and since we choose the model, now we are in the last stage which with the chosen model we are trying to fine-tune these models with best parameters that make the model fit well then we can test our chosen model on the test set.\n\n# Grid Search\n\nBecause each model associated with a not small number of hyperparameters which you need to tune the model using these parameters, is not a handy way to make it your self manually as a lot of time you should consider in all of the combinations of these hyperparameters.\nSo an alternative  solution is to use the best way **Grid Search**.\n\nWith **Grid Search** all just you need is to feed what hyperparameters you need to use and for each of them what the different values you need to feed the model with and the **Grid Search** will make all the combinations for you and provide you at the end with the best hyperparameters values.\n\n","metadata":{}},{"cell_type":"code","source":"param_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, # combination one\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, # combination Two\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error',\n                          return_train_score=True)\ngrid_search.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.refit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Radomized Search\n\nThe grid search approach is fine but with few relative combination of hyperparameters, but when it seems to have multiple or large number of hyperparameters, and it work in same way of **Grid Search**, but instead of trying all combinations, it randomize these hyperparameters and choose some of them for each iteration.\n\nIt help you in:\n- Randmoize maybe for 1000 different value for each hyperparamter, instead of few predefind.\n- This work depends on your situation of sources that you have, because it takes time computing and other consumed budget, it let you to iterate the number you need.\n\n# Ensemble Methods\n\nInstead of test each model individually, this method combine some of the model that you see can help you them combine them together to see how they work.","metadata":{}},{"cell_type":"markdown","source":"# Analyze the best Models and their errors\n\nBy all of these trials you have consider which best model that you will use on the test set, and the best model you choose can indicate which attributes of the data indicate to the best accurate result.\n\nAll we need is to list best_estimator_ with their corresponding attributes.\n\n\n# Evaluate Your Models on Test set\n\nAfter all of these steps from pipeline on the data, to tune the model paramters and trying different models for best result, should we have choose best of these models and now its work for test the models on testset.\n\nAfter looking at these values associated with each attributes, you can decide which of them will be helpful, and you can drop some of less useful of these attributes.\n\nYou should look at your system and try to understand which makes these error, and how you can solve like these errors.","metadata":{}},{"cell_type":"code","source":"features_importances = grid_search.best_estimator_.feature_importances_\nfeatures_importances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(features_importances, attributes), reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set['median_house_value'].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(final_predictions, y_test)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exercises\n#### Q1-\n\nTry a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don't worry about what these hyperparameters mean for now. How does the best SVR predictor perform?","metadata":{}},{"cell_type":"code","source":"param_grid = [\n    {'kernel': [\"linear\", \"rbf\"], 'C': [0.01, .1, 1], 'gamma': ['scale', 'auto']}, # combination one\n]\n\nsvr_model = SVR()\ngrid_search = GridSearchCV(svr_model, param_grid, cv=3, \n                           scoring='neg_mean_squared_error',\n                          return_train_score=True)\ngrid_search.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Q2\n\nTry replacing GridSearchCV with RandomizedSearchCV.","metadata":{}},{"cell_type":"code","source":"param_grid = [\n    {'kernel': [\"linear\", \"rbf\", \"sigmoid\"], 'C': [0.01, .1, 1, 3, 5], 'gamma': ['scale', 'auto']}, # combination one\n]\n\nsvr_model = SVR()\nrandom_search = RandomizedSearchCV(svr_model, param_grid, n_iter=10, cv=5, \n                           scoring='neg_mean_squared_error',\n                          return_train_score=True)\nrandom_search.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvres = random_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n    {'kernel': [\"linear\"], 'C': np.linspace(5, 1000, 20), 'gamma': ['scale']}, # combination one\n]\n\nsvr_model = SVR()\nrandom_search = RandomizedSearchCV(svr_model, param_grid, n_iter=20, cv=3, \n                           scoring='neg_mean_squared_error',\n                          return_train_score=True)\nrandom_search.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvres = random_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sqrt(-random_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, # combination one\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error',\n                          return_train_score=True)\ngrid_search.fit(df_file_prepared, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_importances = grid_search.best_estimator_.feature_importances_\nfeatures_importances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', SelectTopFeatures(features_importances, 5))\n])\n\nhousing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(df_file)\nhousing_prepared_top_k_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Q-4\n\nTry creating a single pipeline that does the full data preparation plus the final prediction.","metadata":{}},{"cell_type":"code","source":"preparation_and_feature_selection__pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_select', SelectTopFeatures(features_importances, 5)),\n    ('svr_reg', SVR(kernel=\"linear\", C=842.9, gamma=\"scale\")),\n])\n\nhousing_prepared_top_k_features = preparation_and_feature_selection__pipeline.fit(df_file, df_file_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now its done we fit the model we can test some prediction and see the score\nsome_data = df_file.iloc[:5]\nsome_labels = df_file_labels.iloc[:5]\nprint(\"Prediction: \", housing_prepared_top_k_features.predict(some_data))\nprint(\"Real Lables: \", list(some_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **As we can see one pipeline is handle all of our stages.**","metadata":{}},{"cell_type":"markdown","source":"# Q-5\n\nAutomatically explore some preparation options using GridSearchCV.\n\n## Try to solve","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}