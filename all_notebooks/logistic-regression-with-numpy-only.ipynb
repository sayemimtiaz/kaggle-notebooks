{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Hypothesis which uses the following sigmoid function </h2>\n<h1> $$ \\frac{1}{(1 + e^{-\\theta^\\intercal X})} $$ </h1>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def sigmoid(theta, X):\n    return 1 / (1 + np.exp((-np.matmul(X,theta.transpose()))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Calculates cost with regularization </h2>\n<h2> Note that $ \\theta_{0} $ is not included in the calculation of cost </h2>\n<h1> $$ \\frac{1}{m} \\sum_{i=1}^m (-y^{(i)}*ln(h_{\\theta}(x^{(i)}))-(1-y^{(i)})ln(1-h_{\\theta}(x^{(i)}))) + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$ </h1>\n<h4> where m = number of training examples, n = number of weights, $y^{(i)}$ and $x^{(i)}$ are training example i </h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_cost(theta, X, y, lbda): # theta is dimensions n x 1, X is dimensions m x n, y is dimensions m x 1, lbda is regularization constant\n    m = X.shape[0]\n    h = sigmoid(theta, X)\n    cost = (1/m)*(-y*np.log(h)-(1-y)*np.log(1-h)).sum() + (lbda / (2*m))*np.square(theta).sum()\n    cost -= (lbda / (2*m)) * theta[0]**2 # remove contribution of theta_zero as it should not be included in cost\n    return cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Vectorized implementation gradient calculation together with regularization </h2>\n<h2> Note that for $\\theta_{0}$, regularization is not needed </h2>\n<h1> $$ \\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m} \\theta_{j}$$ </h1>\n<h3> for j = 1,2,...,n where n is the number of weights theta, and m is the number of training examples </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_grad(theta, X, y, lbda):\n    m = X.shape[0]\n    h = sigmoid(theta, X)\n    grad = np.matmul(X.transpose(),h - y) # vectorized implementation of gradient\n    grad += (lbda/m) * theta\n    grad[0] -= (lbda/m) * theta[0] # remove contribution of theta_zero as it should not be included in grad calculation\n    return grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(X, y, alpha, iterations, test_X, test_y):\n    theta = np.random.rand(X.shape[1]) # randomly initiates weights\n    m = X.shape[0]\n    costs_train = []\n    costs_test = []\n    for i in range(iterations):\n        costs_train.append(calculate_cost(theta, X, y, 1))\n        theta -= alpha * (1/m)*calculate_grad(theta, X ,y, 1)\n        costs_test.append(calculate_cost(theta, test_X, test_y, 1))\n    x_graph = np.arange(0,iterations,1);    \n    plt.plot(x_graph,costs_train, label='train') \n    plt.plot(x_graph,costs_test, label='test')\n    plt.legend()\n    return theta  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(theta, X, threshold):\n    pred = sigmoid(theta, X)\n    pred_result = (pred>=threshold).astype(int) # those above threshold = 1, 0 otherwise\n    return pred_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(X, mean, std):\n    return (X-mean) / std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/diabetes-dataset/diabetes2.csv\")\ndf.isnull().sum()\nprint(len(df[df['Outcome']==1]))\nprint(len(df[df['Outcome']==0])) # making sure data is balanced, a bit of imbalance but should be okay\ntrain=df.sample(frac=0.75,random_state=150) #random state is a seed value\ntest=df.drop(train.index)\n\ntrain_x = train.loc[:,train.columns != \"Outcome\"] # splitting dependent and independent variables\ntest_x = test.loc[:,test.columns != \"Outcome\"]\ntrain_y = train['Outcome'].values\ntest_y = test['Outcome'].values\n\ntrain_mean = train_x.mean(axis=0) # mean normalization\ntrain_std = train_x.std(axis=0)\ntrain_x = normalize(train_x,train_mean ,train_std)\ntest_x = normalize(test_x,train_mean ,train_std )\n\ntrain_x.insert(0, 'One', 1) # adding column of ones for theta that is independent of features\ntest_x.insert(0, 'One', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = logistic_regression(train_x.values, train_y, 0.05, 500, test_x.values, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = predict(theta, test_x.values, 0.6)\nresult = pred_y == test_y\npred_train_y = predict(theta, train_x.values, 0.6)\nresult_train = pred_train_y == train_y\nprint(sum(result) / len(result))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}