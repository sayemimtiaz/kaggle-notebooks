{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Vectorization & Embeddings[ELMo, BERT/GPT]\n\nAuthored by [abhilash1910](https://kaggle.com/abhilash1910/)\n\n### Movie Reviews !!\n\nThis is an extension of the [original Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop) which has been separately provided for a piecewise analysis of creating  NLP Embeddings with Transformers and sophiosticated Neural architectures. For more details other kernels are also provided:\n\n- [Kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop)\n- [Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india)\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the Dataset\n\nThis is the primary step of the entire pipeline. In this case, we have to load the dataset using pandas.\nIn this case, we will be exploring different datasets for our use case. We will be using the [IMDB Movie Reviews Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) primarily for the initial use case. \n\nThe data is collated from [Stanford Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) and the sentiment of the text corpus is either positive or negative.\n\nWe will be analysing the data first in terms of the columns which it has."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Running the Preprocessing and cleaning phase as well as the TFIDF Vectorization\n\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Lemmatize the corpus\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ndef tfidf(data):\n    tfidfv = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf\n\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_html(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_url(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\ncount_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']\ntrain_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert Input DataFrame to a List\n\nThis phase is helpful if we would like to investigate individual word embeddings or sentence embeddings. Differentiating the individual rows of text makes it easier to pass into static and dynamic embedding models."},{"metadata":{"trusted":true},"cell_type":"code","source":"check_df=list(train_df['review'].str.split())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Semantic Embeddings\n\nIn this context, we will be looking into semantic embeddings. These include embeddings which can either by static and dynamic. Word Embeddings fall under this category. \n\nWord Embeddings: These are vector space transformations of the words present in the corpus. When converted to vectors, several metrics can be applied like finding similarity, distance measurement between the vectors, numerical transforms of the vectors. With word vectors, we can specify semantic similarity between different words or collection of words. A pictorial representation of word vectors compressed with Dimension reduction methods is [provided below](https://www.tensorflow.org/tutorials/text/word_embeddings):\n\n<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\">\n\n\nIn this scenario, we will be focussing on all embedding algorithms. Primarily we be starting with Word2Vec and will be understanding advanced BERT/GPT architectures."},{"metadata":{},"cell_type":"markdown","source":"## Dynamic Embeddings\n\nDeep contextual embeddings and sentence/word vectors falls under dynamic embeddings. These embeddings are current SOTA and these are deep contextual embeddings ,implying that there is a need for robust Neural NEtwork models for these architectures.\n\nSince we will not be going into depth about the model architectures of each of these, we will be brushing over the concetps required for creating these embedding models. The following models lie within the scope of these embeddings:\n\n- [ELMO](https://arxiv.org/abs/1802.05365)\n- [Transformers](https://arxiv.org/abs/1706.03762)\n\nBoth these papers are essentially important for their contributions to contextual deep embeddings."},{"metadata":{},"cell_type":"markdown","source":"## ELMO - Brief Overview\n\nELMO is a contextualised deep embedding model, which is dynamic and semi supervised.\nWord representations are functions of entire input sequence,and are computed on top of 2 bidirectional LSTM with character convolutions. The standard architecture for ELMO is as follows:\n\n<img src=\"https://jalammar.github.io/images/Bert-language-modeling.png\">\n\nThe most important aspect is tasks specific combinations of intermediate layer representations of the Bilstm which allows retention of the words/long sentence sequences in the embedding space .\n\n[Jay's blog provides a good walkthrough](http://jalammar.github.io/illustrated-bert/).\n\nBidirectional transfer learning (BiLSTM architecture) is important in this aspect.\n\n<img src=\"http://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png\">\n"},{"metadata":{},"cell_type":"markdown","source":"## Using ELMO Embeddings\n\nTraditionally, Elmo embeddings could be used from [tensorflow hub](https://tfhub.dev/). The original implementation can be found in [AllenNLP](https://allennlp.org/elmo). Some resources for using ELMO :\n\n- [TFHub](https://tfhub.dev/google/elmo/1)\n- [ELMO-Good article](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n- [ELMO-article](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)\n"},{"metadata":{},"cell_type":"markdown","source":"## Restrictions with Tensorflow TF 2.0\n\n\nPreviously there used to be no issues, working with ELMO from tf hub with tensorflow version <2.0. With TF versions morethan 2.0, there are some issues with loading the embeddings (as eager execution during graph computation fails). Hence it is recommended to use ELMO-2 with Tensorflow <2.0 (favourably 1.15).\n\nELMO-3 can be used with Tensorflow 2.0 "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tensorflow -y\n!pip uninstall tensorflow-cloud -y\n!pip install -U tensorflow==1.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall pytorch-lightning -y\n!pip uninstall tensorflow-probability -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as tf_hub\n\nelmo = tf_hub.Module(\"https://tfhub.dev/google/elmo/2\")\nembeddings = elmo(\n    [\"the cat is on the mat\", \"dogs are in the fog\"],\n    signature=\"default\",\n    as_dict=True)[\"elmo\"]\nembeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as tf_hub\n\n# from keras.layers import Input, Lambda, Dense\n# from keras import backend as k\nelmo_embed=tf_hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)\n\n#Creating the elmo embeddings by squeezing the inputs\ndef create_embedding(z):\n    return elmo_embed(tf.squeeze(tf.cast(z,tf.string)),signature='default',as_dict=True)[\"default\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the textual reviews to list for analysing sentences(sentence vectors)\nz=train_df['review'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Tensorflow Hub ELMO-2\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\nelmo = hub.load(\"https://tfhub.dev/google/elmo/2\")\n\ndef create_elmo_embeddings(data):\n    embed=elmo(data,signature=\"default\",as_dict=True)[\"elmo\"]\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.tables_initializer())\n        out_x=sess.run(embed)\n        #out_y=ses.run(tf.reduce_mean(embed,1))\n        return out_x\nelmo_input=z[:2]\nelmo_output=create_elmo_embeddings(elmo_input)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformers\n\n\n\n<img src=\"https://static01.nyt.com/images/2007/07/02/arts/Trans1600.jpg?quality=75&auto=webp&disable=upscale\">\n\n\nWe come to Transformer Embeddings  for which the most important aspect is the Transformer architecture. Since we will be diving in depth into architectures in the Machine LEarning Training session (model building), it is safe to have a glimpse of a traditional Transformer Architecture.\n\n<img src=\"https://i0.wp.com/esciencegroup.com/wp-content/uploads/2020/02/01.png?resize=506%2C641&ssl=1\">\n\nWe will be working with the [HuggingFace](https://huggingface.co/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n\n- [Transformer Keras](https://keras.io/examples/nlp/text_classification_with_transformer/)\n- [Kaggle Kernel](https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb)\n\n\nHowever in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n\n\nIt is recommended to follow this [article](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) before going further\n\n<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">"},{"metadata":{},"cell_type":"markdown","source":"## BERT Embeddings\n\n[BERT](https://arxiv.org/abs/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n\n- Msked Language Model (MLM)\n- Next Sentence Prediction(NSP)\n\nThe bidirectional pre-training is essentially helpful to be used for any tasks. The [Huggingface](https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png) implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n\n<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n\n\nFor fine-tuning and pre-training for different downstream tasks like Q/A, Classification, Language Modelling, Multiple Choice, NER etc. different layers of the BERT are used. \n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/df2b0e26d0599ce3e70df8a9da02e51594e0e992/15-Figure4-1.png\">\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Finetuning BERT for Embeddings\n\nFor finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https://tfhub.dev/s?module-type=text-embedding) or from [Google-Research repository](https://github.com/google-research/bert). The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n\n- input_ids\n- attention_masks\n- token_ids\n\nLet us first try to analyse and understand how BERT  tokenizers, and model can be used in this context. The [BERT](https://huggingface.co/transformers/model_doc/bert.html) documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n\nGenerally by virtue of transfer learning through weight transfer, we use pretrained [BERT models](https://huggingface.co/transformers/pretrained_models.html) from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n\n\nSome important resources which may be helpful:\n\n- [Blog](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)\n- [Extensive Nice Blog](https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)\n- [Good Kernel](https://www.kaggle.com/shirishsharma/nlp-from-embeddings-and-rnns-to-bert)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenize and encode the inputs\n\nimport transformers\nfrom transformers import BertTokenizer,TFBertModel\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\nbert_model = transformers.TFBertModel.from_pretrained('bert-large-uncased')\ndef bert_encode(data,maximum_length) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n        \n          data[i],\n          add_special_tokens=True,\n          max_length=maximum_length,\n          pad_to_max_length=True,\n        \n          return_attention_mask=True,\n        \n        )\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)\n\ntrain_input_ids,train_attention_masks = bert_encode(train_df['review'][:5],1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize the attention masks and input ids.\ntrain_attention_masks,train_input_ids\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Build a miniature model for extracting the embeddings\nimport tensorflow as tf\nfrom keras.models import Sequential,Model\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom tensorflow.keras import layers\ninput_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\ninput_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\nbert_output=bert_model([input_ids,input_masks_ids])[0]\nbert_output.shape\nbert_output[:,0,:]\nmodel=Model(inputs=[input_ids,input_masks_ids],outputs=[bert_output])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Realize the Size of the BERT Model\n\nThe size of the simple NN model built with BERT as intermediate Embedding Layer can be observed. Bert-large-uncased has 24-layer, 1024-hidden, 16-heads, 336M parameters and trained on lower-cased English text."},{"metadata":{},"cell_type":"markdown","source":"## Using the Transfomer Method\n\n\nIn this case , we will be using the HuggingFace Transformer method for extracting sentence embeddings. This is a rather simpler method as we only need to extract the last layer from from the model output. The model in this case is bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.Trained on lower-cased English text.).This code segment is model agnostic and can be used for any variats of BERT (except T5, GPT variants).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"z=train_df['review'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the tokenizer and model  from the Transformers and determine the output features from the last hidden layer.\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\ndef get_embeddings(model_name,tokenizer,name,inp):\n    tokenizer = tokenizer.from_pretrained(name)\n    model = model_name.from_pretrained(name)\n    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0]\n    cls_token=last_hidden_states[0]\n    return cls_token\ncls_token=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',z[0])\ncls_token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For visualizing the embeddings \nimport matplotlib.pyplot as plt\nprint(cls_token.shape)\nplt.plot(cls_token[0])\nplt.plot(cls_token[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Alternate Strategy With Transformers-One For All\n\nSentence Vectors can be determined with the help of [Pipeline in Transformers](https://huggingface.co/transformers/main_classes/pipelines.html). This is a robust and efficient way to generate sentence vectors and compute corrspoinding distances between those vectors. It is a faster way which applies to almost all transformers.\n\n<img src=\"https://www.sideshow.com/wp/wp-content/uploads/2019/05/InfinityStones-Infographic-01.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT\n\n[This](https://huggingface.co/transformers/model_doc/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. [DistilBERT Paper](https://arxiv.org/abs/1910.01108) provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n\n<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\n\nA very neat representation on the model workflow is provided here:\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">"},{"metadata":{},"cell_type":"markdown","source":"## The Cosine Distance Metric\n\n\nIn this context, we will be using Cosine similarity metric from [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html). But we can use them after we extract the last hidden layer from the model output(similar to BERT). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distil BERT Embeddings\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\nembedding_features1=transformer_embedding('distilbert-base-uncased',z[0],TFDistilBertModel)\nembedding_features2=transformer_embedding('distilbert-base-uncased',z[1],TFDistilBertModel)\ndistance=1-cosine(embedding_features1[0],embedding_features2[0])\nprint(distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize embeddings\nplt.plot(embedding_features1[0])\nplt.plot(embedding_features2[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Embeddings with Alternate Strategy\n\nHere we see the new methodology applied for BERT using the Pipeline module of Trasnformers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT embeddings\nfrom transformers import AutoTokenizer, pipeline, TFBertModel\nbert_features1=transformer_embedding('bert-base-uncased',z[0],TFBertModel)\nbert_features2=transformer_embedding('bert-base-uncased',z[1],TFBertModel)\ndistance=1-cosine(bert_features1[0],bert_features2[0])\nprint(distance)\nplt.plot(bert_features1[0])\nplt.plot(bert_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Roberta Model\n\n[Roberta Model](https://huggingface.co/transformers/model_doc/roberta.html) is a robust and large model built by [Facebook Research](https://github.com/pytorch/fairseq/tree/master/examples/roberta), to alleviate undertrained nature of BERT. It trains in much larger mini-batch sizes. [This](https://cloud.google.com/tpu/docs/tutorials/roberta-pytorch) provides a good model of how to train Roberta on Google cloud.The original paper can be found [here](https://arxiv.org/abs/1907.11692), and the model architecture is provided.\n\n<img src=\"https://camo.githubusercontent.com/f5c0d05eb0635cdd0e17e137265af23fa825b1d4/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f584c4d2f786c6d5f6669677572652e6a7067\">\n\n\nResources:\n\n- [Blog](https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6)\n- [Blog-2](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Roberta Embeddings\nfrom transformers import AutoTokenizer, pipeline, TFRobertaModel\nroberta_features1=transformer_embedding('roberta-base',z[0],TFRobertaModel)\nroberta_features2=transformer_embedding('roberta-base',z[1],TFRobertaModel)\ndistance=1-cosine(roberta_features1[0],roberta_features2[0])\nprint(distance)\nplt.plot(roberta_features1[0])\nplt.plot(roberta_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XLNet Embeddings\n\nThis [paper](https://arxiv.org/abs/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n\nIt is a permutation language model and a pictorial representation can be :\n\n<img src=\"https://zdnet2.cbsistatic.com/hub/i/r/2019/06/21/2a4e6548-9dee-491d-b638-8cfae9bbb2fe/resize/1200x900/ab279544c2631111754a357ada50ef29/google-xlnet-architecture-2019.png\">\n\n\nResources:\n- [Blog](https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/)\n- [Xlnet](https://www.borealisai.com/en/blog/understanding-xlnet/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline, TFXLNetModel\nxlnet_features1=transformer_embedding('xlnet-base-cased',z[0],TFXLNetModel)\nxlnet_features2=transformer_embedding('xlnet-base-cased',z[1],TFXLNetModel)\ndistance=1-cosine(xlnet_features1[0],xlnet_features2[0])\nprint(distance)\nplt.plot(xlnet_features1[0])\nplt.plot(xlnet_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BART Model\n\n[This](https://arxiv.org/abs/1910.13461) is alternate SOTA model to denoise sentence2 sentence pretraining for natural language generation,comprehension etc. The most important points can be summarized as:\n\n\n- Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n\n- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n\nThe architecture contains these encoder -decoder modules :\n<img src=\"https://miro.medium.com/max/3138/1*Qss9gtS1nw_sgcG1pMAM2A.png\">\n\n<img src=\"https://miseciara.files.wordpress.com/2013/11/bart.gif\">\n\n\nSome resources:\n- [Blog](https://medium.com/dair-ai/bart-are-all-pretraining-techniques-created-equal-e869a490042e)\n- [Blog-BART](https://medium.com/analytics-vidhya/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline, BartModel\nbart_features1=transformer_embedding('facebook/bart-base',z[0],BartModel)\nbart_features2=transformer_embedding('facebook/bart-base',z[1],BartModel)\ndistance=1-cosine(bart_features1[0],bart_features2[0])\nprint(distance)\nplt.plot(bart_features1[0])\nplt.plot(bart_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Albert Model\n\n[This](https://arxiv.org/abs/1909.11942) is a lighter version of BERT which splits the embedding matrix into 2 smaller matrices and uses repeated splitting in between the transformer layers.Some important points:\n\n\n- ALBERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.\n\nResources:\n-[Source code](https://github.com/google-research/ALBERT)\n- [Blog](https://medium.com/@lessw/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28)\n- [Blog](https://medium.com/doxastar/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762)\n\n<img src=\"data:image/jpeg;base64,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\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline, TFAlbertModel\nalbert_features1=transformer_embedding('albert-base-v1',z[0],TFAlbertModel)\nalbert_features2=transformer_embedding('albert-base-v1',z[1],TFAlbertModel)\ndistance=1-cosine(albert_features1[0],albert_features2[0])\nprint(distance)\nplt.plot(albert_features1[0])\nplt.plot(albert_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sophisticated variants of BERT\nfrom transformers import AutoTokenizer, pipeline, FlaubertModel\nflaubert_features1=transformer_embedding('flaubert/flaubert_base_cased',z[0],FlaubertModel)\nflaubert_features2=transformer_embedding('flaubert/flaubert_base_cased',z[1],FlaubertModel)\ndistance=1-cosine(flaubert_features1[0],flaubert_features2[0])\nprint(distance)\nplt.plot(flaubert_features1[0])\nplt.plot(flaubert_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT-Generative Pretraining\n\n[This](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) is a different model from BERT and its variants built primarily for NLG (generative modelling).  GPT has the following important points:\n\n\n- GPT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nSome resources are helpful:\n\n- [GPT](https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4)\n- [Openai](https://openai.com/blog/better-language-models/)\n- [Imgae GPT](https://openai.com/blog/image-gpt/)\n\n<img src=\"https://www.topbots.com/wp-content/uploads/2019/04/cover_GPT_web.jpg\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GPT embeddings\nfrom transformers import AutoTokenizer, pipeline, TFOpenAIGPTModel\ndef transformer_gpt_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    tokenizer.pad_token = \"[PAD]\"\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\ngpt_features1=transformer_gpt_embedding('openai-gpt',z[0],TFOpenAIGPTModel)\ngpt_features2=transformer_gpt_embedding('openai-gpt',z[1],TFOpenAIGPTModel)\ndistance=1-cosine(gpt_features1[0],gpt_features2[0])\nprint(distance)\nplt.plot(gpt_features1[0])\nplt.plot(gpt_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT-2\n\n<img src=\"http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png\">\n\nIt is a [robust model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\n- GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nResources:\n\n- [GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n- [Source Code](https://github.com/openai/gpt-2)\n- [Blog](https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/)\n- [Blog](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)\n\nIt is important to note the effect of Attention and masking in GPT-2 model. These are represented in the diagram:\n\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png\">\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-3-2.png\">\n\nSelf Attention:\n\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png\">\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GPT-2\nfrom transformers import AutoTokenizer, pipeline, TFGPT2Model\n\ngpt2_features1=transformer_gpt_embedding('openai-gpt',z[0],TFGPT2Model)\ngpt2_features2=transformer_gpt_embedding('openai-gpt',z[1],TFGPT2Model)\ndistance=1-cosine(gpt2_features1[0],gpt2_features2[0])\nprint(distance)\nplt.plot(gpt2_features1[0])\nplt.plot(gpt2_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Electra\n\nfrom transformers import AutoTokenizer, pipeline, TFElectraModel\nelectra_features1=transformer_embedding('google/electra-small-discriminator',z[0],TFElectraModel)\nelectra_features2=transformer_embedding('google/electra-small-discriminator',z[1],TFElectraModel)\ndistance=1-cosine(electra_features1[0],electra_features2[0])\nprint(distance)\nplt.plot(electra_features1[0])\nplt.plot(electra_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Longformer\nfrom transformers import AutoTokenizer, pipeline, TFLongformerModel\nlongformer_features1=transformer_embedding('allenai/longformer-base-4096',z[0],TFLongformerModel)\nlongformer_features2=transformer_embedding('allenai/longformer-base-4096',z[1],TFLongformerModel)\ndistance=1-cosine(longformer_features1[0],longformer_features2[0])\nprint(distance)\nplt.plot(longformer_features1[0])\nplt.plot(longformer_features2[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Embeddings From Pytorch\n\nSince we have explored Transformer Embeddings, we will be looking into using pretrained BERT embedding from [Pytorch](https://pytorch.org/hub/huggingface_pytorch-transformers/).\nWe will be downloading BERT from [Pytorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/).\nThis is one of the initial starting libraries for Huggingface.\n\n\nSome important resources:\n\n- [Notebook](https://www.kaggle.com/christofhenkel/bert-embeddings-lstm)\n- [Blog](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1)\n\n\n<img src=\"https://miro.medium.com/max/875/1*Pvyx8-QWdo_WBghNOd-zGA.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import BERT and the variables\nimport os\nBERT_MODEL = 'bert-base-uncased'\nCASED = 'uncased' in BERT_MODEL\nINPUT = '../input/'\nTEXT_COL = 'comment_text'\nMAXLEN = 250\n# os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Error Cause\nos.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load  from pytorch pretrained model- weights\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\n#BERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'\n#Function for creating BERT embeddings-matrix\ndef bert_embedding_matrix():\n    bert = BertModel.from_pretrained('bert-base-uncased')\n    print(bert)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat\nembedding_matrix = bert_embedding_matrix()\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(embedding_matrix[0])\nplt.plot(embedding_matrix[1])\nplt.plot(embedding_matrix[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embeddings Conclusion and Key Takeaways\n\nThis Notebook has provided an idea of using a code segment for multiple Transformer based embedding models . For this it is to be kept in mind that most of these embedding models are very big architectures-Transformers - which we will be taking up in the next session on Model building. Since the codes are mostly in Tensorflow Keras, some additional resources are provided for running them in Pytorch:\n\n- [BERT Embeddings Pytorch](https://www.kaggle.com/abhilash1910/bertsimilarity-library)\n- [NLP Workshop](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n- [Workshop1](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india)\n\nWill be updating with more reseources. We have explored embeddings (static and dynamic) and also had an idea on the entire NLP pipeline till embeddings!! Now we know the power of model agnostic code  in NLP.\n\n<img src=\"https://img.cinemablend.com/filter:scale/quill/2/0/d/8/1/6/20d81656d25ffbc06d6b2e382241661c629972e1.jpg?mw=600\">\n\n\nNext module- [Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}