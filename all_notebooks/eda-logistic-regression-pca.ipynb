{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **EDA + Logistic Regression + PCA**\n\n\nHello friends,\n\nThis kernel is all about **Principal Component Analysis** - a **Dimensionality Reduction** technique.\n\nI have discussed **Principal Component Analysis (PCA)**. In particular, I have introduced PCA, explained variance ratio, Logistic Regression with PCA, find right number of dimensions and plotting explained variance ratio with number of dimensions.\n\nI have used the **adult** data set for this kernel. This dataset is very small for PCA purpose. My main purpose is to demonstrate PCA implementation with this dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\nThe contents of this kernel is divided into various topics which are as follows:-\n\n-   The Curse of Dimensionality\n-\tIntroduction to Principal Component Analysis\n-\tImport Python libraries\n-\tImport dataset\n-\tExploratory data analysis\n-\tSplit data into training and test set\n-\tFeature engineering\n-\tFeature scaling\n-\tLogistic regression model with all features\n-\tLogistic Regression with PCA\n-\tSelect right number of dimensions\n-\tPlot explained variance ratio with number of dimensions\n-\tConclusion\n-\tReferences\n\t\n"},{"metadata":{},"cell_type":"markdown","source":"## The Curse of Dimensionality\n\nGenerally, real world datasets contain thousands or millions of features to train for. This is very time consuming task as this makes training extremely slow. In such cases, it is very difficult to find a good solution. This problem is often referred to as the curse of dimensionality.\n\n\n**The curse of dimensionality** refers to various phenomena that arise when we analyze and organize data in high dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance.\n\n\nIn real-world problems, it is often possible to reduce the number of dimensions considerably. This process is called **dimensionality reduction**. It refers to the process of reducing the number of dimensions under consideration by obtaining a set of principal variables. It helps to speed up training and is also extremely useful for data visualization.\n\n\nThe most popular dimensionality reduction technique is Principal Component Analysis (PCA), which is discussed below.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction to Principal Component Analysis (PCA)\n\n\n**Principal Component Analysis (PCA)** is a dimensionality reduction technique that can be used to reduce a larger set of feature variables into a smaller set that still contains most of the variance in the larger set. \n\n### Preserve the variance\n\nPCA, first identifies the hyperplane that lies closest to the data and then it projects the data onto it. Before, we can project the training set onto a lower-dimensional hyperplane, we need to select the right hyperplane. The projection can be done in such a way so as to preserve the maximum variance. This is the idea behind PCA.\n\n### Principal Components\n\nPCA identifies the axes that accounts for the maximum amount of cumulative sum of variance in the training set. These are called Principal Components. PCA assumes that the dataset is centered around the origin. Scikit-Learn’s PCA classes take care of centering the data automatically.\n\n### Projecting down to d Dimensions\n\nOnce, we have identified all the principal components, we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. This ensures that the projection will preserve as much variance as possible.\n\n\n\nNow, let's get to the implementation.\n"},{"metadata":{},"cell_type":"markdown","source":"## Import Python libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# import libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n# Working with os module - os is a module in Python 3.\n# Its main purpose is to interact with the operating system. \n# It provides functionalities to manipulate files and folders.\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check file size"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print('# File sizes')\nfor f in os.listdir('../input'):\n    print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfile = ('../input/adult.csv')\ndf = pd.read_csv(file, encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Check shape of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 32561 instances and 15 attributes in the data set."},{"metadata":{},"cell_type":"markdown","source":"### Preview dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View summary of dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Summary of the dataset shows that there are no missing values. But the preview shows that the dataset contains values coded as `?`. So, I will encode `?` as NaN values."},{"metadata":{},"cell_type":"markdown","source":"### Encode `?` as `NaNs`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df == '?'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Again check the summary of dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the summary shows that the variables - `workclass`, `occupation` and `native.country` contain missing values. All of these variables are categorical data type. So, I will impute the missing values with the most frequent value- the mode."},{"metadata":{},"cell_type":"markdown","source":"### Impute missing values with mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['workclass', 'occupation', 'native.country']:\n    df[col].fillna(df[col].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check again for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that there are no missing values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"### Setting feature vector and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income'], axis=1)\n\ny = df['income']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into separate training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression model with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression with PCA\n\nScikit-Learn's PCA class implements PCA algorithm using the code below. Before diving deep, I will explain another important concept called explained variance ratio.\n\n\n### Explained Variance Ratio\n\nA very useful piece of information is the **explained variance ratio** of each principal component. It is available via the `explained_variance_ratio_ ` variable. It indicates the proportion of the dataset’s variance that lies along the axis of each principal component.\n\nNow, let's get to the PCA implementation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_train = pca.fit_transform(X_train)\npca.explained_variance_ratio_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\n- We can see that approximately 97.25% of variance is explained by the first 13 variables. \n\n- Only 2.75% of variance is explained by the last variable. So, we can assume that it carries little information. \n\n- So, I will drop it, train the model again and calculate the accuracy. \n\n"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with first 13 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income','native.country'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 13 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\n- We can see that accuracy has been decreased from 0.8218 to 0.8213 after dropping the last feature.\n\n- Now, if I take the last two features combined, then we can see that approximately 7% of variance is explained by them.\n\n- I will drop them, train the model again and calculate the accuracy.\n"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with first 12 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income','native.country', 'hours.per.week'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 12 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\n- Now, it can be seen that the accuracy has been increased to 0.8227, if the model is trained with 12 features.\n\n- Lastly, I will take the last three features combined. Approximately 11.83% of variance is explained by them.\n\n- I will repeat the process, drop these features, train the model again and calculate the accuracy.\n"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with first 11 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income','native.country', 'hours.per.week', 'capital.loss'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 11 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\n- We can see that accuracy has significantly decreased to 0.8187 if I drop the last three features.\n\n- Our aim is to maximize the accuracy. We get maximum accuracy with the first 12 features and the accuracy is 0.8227."},{"metadata":{},"cell_type":"markdown","source":"## Select right number of dimensions\n\n- The above process works well if the number of dimensions are small.\n\n- But, it is quite cumbersome if we have large number of dimensions.\n\n- In that case, a better approach is to compute the number of dimensions that can explain significantly large portion of the variance.\n\n- The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 90% of the training set variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\n\npca= PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\ndim = np.argmax(cumsum >= 0.90) + 1\nprint('The number of dimensions required to preserve 90% of variance is',dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\n- With the required number of dimensions found, we can then set number of dimensions to `dim` and run PCA again.\n\n- With the number of dimensions set to `dim`, we can then calculate the required accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Plot explained variance ratio with number of dimensions\n\n- An alternative option is to plot the explained variance as a function of the number of dimensions.\n\n- In the plot, we should look for an elbow where the explained variance stops growing fast.\n\n- This can be thought of as the intrinsic dimensionality of the dataset.\n\n- Now, I will plot cumulative explained variance ratio with number of components to show how variance ratio varies with number of components."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,14,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment\n\nThe above plot shows that almost 90% of variance is explained by the first 12 components."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Conclusion\n\n-\tIn this kernel, I have discussed Principal Component Analysis – the most popular dimensionality reduction technique.\n-\tI have demonstrated PCA implementation with Logistic Regression on the adult dataset.\n-\tI found the maximum accuracy with the first 12 features and it is found to be 0.8227.\n-\tAs expected, the number of dimensions required to preserve 90 % of variance is found to be 12.\n-\tFinally, I plot the explained variance ratio with number of dimensions. The graph confirms that approximately 90% of variance is explained by the first 12 components.\n"},{"metadata":{},"cell_type":"markdown","source":"## References\n\nThe ideas and concepts in this kernel are taken from the following book.\n\n- Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}