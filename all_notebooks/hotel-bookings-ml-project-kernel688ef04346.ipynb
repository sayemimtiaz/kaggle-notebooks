{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Project : Hotel bookings\n<p><img width=\"150\" style=\"float: right;margin:10px 30px 10px 10px\" src=\"https://food.jumia.ug/blog/wp-content/uploads/2016/09/Hotel-booking-iStock_000089313057_Medium-940x529-660x400.jpg\"></p>\n<br><p>I have been working on this dataset as a school project for the last three weeks. My intention was to try out the major supervised learning algorithms we saw on the course, compare their respective performances and try to bring up some insights from the data.<br>\nIndeed, the dataset includes all the bookings of two hotels along with information about the guests, the room types, check-in and check-out dates, etcâ€¦ We will apply techniques of Exploratory Data Analysis (EDA) to discover patterns in the data, and then apply basic machine learning methods to predict if a booking will be cancelled in the future or not.</p>\n## Exploratory Data Analysis\n### Import libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import project files\n First, we will import the file containing the descriptions of the columns in the CSV file, to be used in understanding the content of each column. Then the CSV file will be imported for analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"booking_data = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')\nprint('The shape of the overall database is: ', booking_data.shape,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preliminary observations and Findings \nFirst, let's explore the booking columns across the dataset to look for <strong>aberrant or missing values</strong><br>\nThe first observation is that some columns are not fully populated, and this means we need to replace the missing values. And there are <strong>four</strong> columns obeying this observation:\n<strong>'Children', 'Country', 'agent' and 'company'.</strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"booking_data.loc[:,['is_canceled','children','country', 'agent', 'company']].info()\nbooking_data.country.fillna('-', inplace=True)\nbooking_data.agent.fillna(0, inplace=True)\nbooking_data.company.fillna(0, inplace=True)\nbooking_data.children.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <u>Q. What type of hotels suffer more from cancellation?</u>\nNot all bookings are the same!<br>\nAs you may know, each row in this dataset corresponds to a booking, but one row may be related to one person while antoher may be for several. The same thing stands for the nights stayed (one would book a weekend while another may stay longer). \n<br>Let's build some useful columns for later calculations, such as total stay nights and the number of booked people (except babies) per row. Another column will be added to account for the difference between the <i>reserved_room_type</i> and the <i>assigned_room_type</i> and included in our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"booking_data['pax'] = booking_data.children+booking_data.adults\nbooking_data['stay_nights'] = booking_data.stays_in_week_nights+booking_data.stays_in_weekend_nights\nbooking_data['bill']=booking_data.stay_nights*booking_data.adr\nbooking_data['room_assignment'] = booking_data['reserved_room_type']==booking_data['assigned_room_type']\nprint('* Overall, %2.0f bookings were canceled, accounting for %2.0f percent of booked stays.'\n      %(booking_data.is_canceled.sum(), booking_data.is_canceled.mean()*100))\nplt.figure(figsize=(8,3))\nsns.set_style(\"white\")\nsns.countplot(x='hotel', hue='is_canceled', data=booking_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Cancellation Financial impact')\nsns.catplot(x='hotel', y='bill', hue='is_canceled', estimator=sum, ci=None, kind='bar', data=booking_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong>The majority of canceled bookings are in 'City Hotels' even though both establishments suffer from cancellation to a certain degree.</strong>\n#### <u>Q. Do people from a specific country tend to cancel their booking more than the others?</u>\n\nWe will now build a grouped table to calculate the column values per country. We will use this to first display the top countries by number of fulfilled bookings."},{"metadata":{"trusted":true},"cell_type":"code","source":"booking_country = pd.DataFrame(booking_data.groupby('country').sum())\nbooking_country = booking_country.loc[:,['is_canceled','stay_nights','pax']]\nbooking_country['booking_count'] = booking_data.groupby('country').hotel.count()\nbooking_country['cancellation_rate'] =  booking_country.is_canceled.div(booking_country.booking_count)\nbooking_country['fulfillment_rate'] =  1-booking_country.is_canceled.div(booking_country.booking_count)\nbooking_country['fulfilled_bookings'] =  booking_country['booking_count']-booking_country['is_canceled']\nsns.set(style=\"whitegrid\")\ntoprint = booking_country.reset_index().sort_values(by='fulfilled_bookings', ascending=False).head(10)\ng = sns.PairGrid(toprint, x_vars=toprint.columns[7:8], y_vars=['country'], height=4)\nsns.despine(left=True, bottom=True)\ng.map(sns.stripplot, size=20, orient=\"h\", palette=\"ch:s=1,r=-.1,h=1_r\", linewidth=2, edgecolor=\"w\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The countries on the top of the chart are all european, with a remarkable lead for <strong>Portugal</strong>. If we hypothesize that the dataset is for bookings in the same geographical area, we may claim that the establishments object of the bookings are located in the <strong>Iberian peninsula</strong> in order to justify the affluence of customers from this region. And given that <b>Portugal</b> has a small population (10M) compared to the UK, France and Spain, it would be hard to claim that the hotels are Spain. Thus, we conclude that the stablishments are in <b>Portugal</b>.\n<br><br>Now let's move back to our dataset and analyze the countries with the highest cancellation rates. It is common sense that a high number of cancellations for a given country could be due to the fact that the overall reservations are also important. So the suitable approach would be to compare percentages."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3,3))\nsns.set_style(\"white\")\nsns.boxplot(data=booking_country, y='cancellation_rate')\nplt.annotate('Filter countries with this rate or more', xy=(0.01, 0.48), xytext=(-0.4, 0.8),\n            arrowprops=dict(facecolor='black', shrink=1),\n            )\nplt.title('Distribution of cancellation rates among countries')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we've distributed the countries per rate of cancellation, we will filter the countries higher than the 75th percentile.\n<br>The calculations determined that the 75th quantile is at a cancellation rate of 45%. Nonetheless, we will neglect the countries with low reservations count (<100) even if they present high cancellation rates for the sake of the analysis.<br>\nThe over-sized points are countries with high cancellation rates while at the same time having an important number of reservations overall. Let's zoom in on the cluster at the left to enumerate those countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(booking_country.quantile(0.75))\nbooking_country_plot = booking_country\nbooking_country_plot['hue'] = (booking_country['cancellation_rate']<0.45)|(booking_country['booking_count']<100)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,3))\nsns.set_style('whitegrid')\nax1.set_title('Countries with high cancellation rates')\nsns.scatterplot(data=booking_country_plot, x='booking_count', y='is_canceled', size='hue', hue='hue', sizes=(20,150), alpha=0.8, legend=False, ax=ax1)\nax2.set_title('High cancel rates (zoom on the cluster)')\nsns.scatterplot(data=booking_country_plot, x='booking_count', y='is_canceled', hue='hue', size='hue', sizes=(20,150), alpha=0.8, legend=False, ax=ax2)\nplt.xlim(0,1200)\nplt.ylim(0,600)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('If we do not account for countries with negligible booking counts, the TOP 3 countries in terms of cancelling reservations are : Portugal, China and Angola.')\ndisplay(booking_country_plot[booking_country_plot['hue']==False].sort_values(by='is_canceled', ascending=False).loc[:,'is_canceled':'cancellation_rate'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <u>Q. In which period of the year the number of bookings peak? Can you spot any seasonality?</u>\nNow let's address the seasonality of the bookings, by finding the months during which the bookings peak."},{"metadata":{"trusted":true},"cell_type":"code","source":"bookings = booking_data[booking_data['is_canceled']==0].pivot_table(index='arrival_date_month', columns='arrival_date_year', values='hotel', aggfunc=len, fill_value=0)\nbookings.index = pd.CategoricalIndex(bookings.index, categories=['January', 'February', 'March', 'April','May','June','July', 'August','September', 'October', 'November', 'December'], ordered=True)\nbookings = bookings.sort_index()\nmask = np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]])\nf, ax = plt.subplots(figsize=(5, 3))\nsns.heatmap(bookings, center=2000, annot=True, mask=mask, fmt=\"d\", ax=ax, cmap=\"YlGnBu\")\nsns.set_context('paper')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This matrix shows that the number of fulfilled reservations peak each year at the months of <strong>May</strong> and <strong>October</strong>. We may at a later stage leave out the <i>'arrival_year'</i> feature for the training since there is a certain seasonality.\n#### <u>Q. What are the features that are more correlated with booking cancellation?</u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The maximum correlation between \\'is_canceled\\' and any other numeric feature is  %2.2F'\n      %booking_data.corr().loc['lead_time':,'is_canceled'].abs().max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer that there is little correlation between any numerical feature and the target label <i>'is_canceled'</i>, the closest one being <i>'lead_time'</i>. Let's explore the correlation with categorical features.<br>\nBy previewing each feature by itself while counting the fulfilled bookings vs the cancelled ones, we can detect some patterns. In fact, most of the features have a homogenous distribution of cancellations among the unique values, which is a clear indication that there is little chance for the feature to have a correlation with the label to predict. The columns <i>'arrival_date_month'</i> and <i>'arrival_date_week_number'</i> illustrate this effect:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,3))\nsns.countplot(data=booking_data, x='arrival_date_week_number', hue='is_canceled', ax=ax1)\nsns.countplot(data=booking_data, x='arrival_date_month', hue='is_canceled', ax=ax2)\nplt.xticks(rotation=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In contrast, other features displayed a certain bias when segregated by cancellation. This pointed us towards some features of interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nfig, axes = plt.subplots(2, 3, figsize=(15,8))\nsns.countplot(data=booking_data, x='deposit_type', hue='is_canceled', ax=axes[0][0])\nsns.countplot(data=booking_data, x='market_segment', hue='is_canceled', ax=axes[0][1])\nsns.countplot(data=booking_data, x='distribution_channel', hue='is_canceled', ax=axes[0][2])\nsns.countplot(data=booking_data, x='is_repeated_guest', hue='is_canceled', ax=axes[1][0])\nsns.countplot(data=booking_data, x='room_assignment', hue='is_canceled', ax=axes[1][1])\nsns.countplot(data=booking_data, x='reservation_status', hue='is_canceled', ax=axes[1][2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, we deduced that these are the columns with the highest correlation with the label to predict:\n>* In regards to 'deposit_type': 'Non Refund' bookings are canceled almost all the time\n* Also, the cancellation rate varies a lot for different values of 'market_segment' and 'distribution_channel'\n* Repeated guests cancel their bookings a lot fewer than non-repeated guests do.\n\nAs for the last attribute, there is a perfect connection between the values. For each row that has 'reservation_status' set to 'Check-Out', the label 'is_canceled' is null, and is equal to '1' on the other cases. So this feature will be left out during the training.\n<br><i>NB: A test has been done with the feature 'reservation_status', after creating the dummies, and it generated a model with a perfect score with DecisionTree as the algorithm.</i>"},{"metadata":{},"cell_type":"markdown","source":"Before moving on to the ML part, one peculiar thing that got our attention while cleaning the data, is the presence of 715 rows with null numbers of stay nights (weekends or throughout the week) and a null average daily rate. We investigated the rows to find patterns, and observed that they are scattered along all the features (hotel, arrival dates, meals...), except that:<br>\n* The majority are locals (PRT) and the bookings are 'checked-out'\n* They are related to 'transient' clients and signed for B&B\n* They had 'No-deposits' \n* Booked for 2 people mostly (sometimes 1 person) and almost all of them had 0 'days in waiting list'\n\nWe immediately started hypothesizing:<br>\n<b>H1</b>: They could be related to clients showing up at he last minute (0 days waiting list) and leaving immediately (maybe not liking the establishment, price...), but they're all labeled as 'Checked-out'<br>\n<b>H2</b>: Neither is it plausible to say that it could be clients who book lunch/dinner or to the bar, as it does not make sense to create a booking for that, and a lot of bookings are B&B.<br>\n...among another 3 or 4 hypotheses that were far-fetched. We finally decided it has something to do with the validity of the data. So, whether it is maliciously rigged or just a mistake in record keeping, this means that there are bills not accounted for in the establishement. Let's calculate the supposed losses, and drop the rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Average bill amount = average adr for checked-out bookings * average stay duration\nbb= booking_data[booking_data['is_canceled']==0].adr.mean()*booking_data[booking_data['is_canceled']==0].stay_nights.mean()\n#Subset of fulfilled bookings = 680\nrr = booking_data[(booking_data['stay_nights']==0) & (booking_data['is_canceled']==0)].is_canceled.value_counts()\nprint('The average amount to be expected from the bookings is $%.2f' %(bb*rr))\nbooking_data = booking_data[booking_data['stay_nights']!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning\n### Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the inputs\nTo tackle the model traning part, we need to deal with categorical features, by replacing them with dummy columns. The features at hand are:<br>\n<i>hotel, market_segment, distribution_channel, room_assignmnt, deposit_type, is_repeated_guest</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bookings = booking_data[['is_canceled', 'lead_time', 'country', 'hotel', 'market_segment', 'distribution_channel', 'room_assignment', 'deposit_type', 'customer_type', 'is_repeated_guest']]\nbooking_data_dummies=pd.get_dummies(data=bookings, columns=['hotel', 'country', 'market_segment', 'distribution_channel', 'room_assignment', 'deposit_type', 'customer_type', 'is_repeated_guest'])\nprint(booking_data_dummies.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we need to split the dataframe into features (booking columns) and the label to predict ('is_canceled'). Next, we will split the dataframe into a training set (<strong>X_train and y_train</strong>) and a test set (<strong>X_test and y_test</strong>)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = booking_data_dummies.drop(['is_canceled'], axis=1).values\ny = booking_data_dummies.is_canceled\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the model\nBy preparing the model, we mean choosing the algorithm, instantiating it and then fitting it to the training set. The algorithms that are suited for the problem at hand, which is a classification, are the following:\n<ul><li>Decision trees (CART)</li><li>KNN (with confusion matrix)</li><li>Logistic regression (with ROC)</li><li>A boosted model with a combination of all the above.</li>    \n</ul>\n\n#### Decision Tree Classifier\n<br>We'll start with a  <strong>decision tree classifier</strong> as a first attempt to model the problem. We will tackle this through the conventional paradigm (instantiate, fit, predict, assess performance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_t = DecisionTreeClassifier(criterion='entropy', random_state=7)\nmodel_t.fit(X_train, y_train)\ny_pred = model_t.predict(X_test)\nprint('Score :', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the use of GridSearchCV, it is clear that increasing the '<i>max_depth</i>' parameter increases the model accuracy without finding an optimum. So the choice will be trade-off between execution time and score improvement.\n#### K-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_k = KNN(n_neighbors=4)\nmodel_k.fit(X_train, y_train)\ny_pred = model_k.predict(X_test)\nprint('Score :', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_l = LogisticRegression(random_state=7, solver='liblinear')\nmodel_l.fit(X_train, y_train)\ny_pred = model_l.predict(X_test)\nprint('Score :', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [('Logistic Regression', model_l),\n('K Nearest Neighbours', model_k),\n('Classification Tree', model_t)]\nvc = VotingClassifier(estimators=classifiers)\nvc.fit(X_train, y_train)\ny_pred = vc.predict(X_test)\nprint('Voting Classifier: {:.3f}'.format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"bc = BaggingClassifier(base_estimator=model_l, n_estimators=100)\nbc.fit(X_train, y_train)\ny_pred = bc.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=7)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of Random Forest Classifier: {:.3f}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ADABoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(max_depth=15, random_state=7)\nadb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=500)\nadb_clf.fit(X_train, y_train)\ny_pred_proba = adb_clf.predict_proba(X_test)[:,1]\nadb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of ADABoosted Descision Tree Classifier: {:.3f}'.format(adb_clf_roc_auc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, the best strategy to predict the outcomes of bookings was <strong>adaptive boosting</strong> on Decision trees, which yields a score as high as 85%.\nIn real life, we wouldn't go the management comittee of the holtel chain to tell them that the model predicts the fulfillment of reservations 85% of the time, as this would mean nothing and everything. One obvious comment would be on the ability of the model to predict impact on revenues, aka bottom line numbers.\n##### So let's crunch the numbers !"},{"metadata":{"trusted":true},"cell_type":"code","source":"booking_sample = booking_data.sample(4420).sort_values(by='bill', ascending=False)\nbooking_sample.bill.sum()\nbooking_sample_0 = booking_sample[booking_sample['is_canceled']==0].sort_values(by='bill', ascending=False).head(int(4420*0.113)).bill.sum()\nbooking_sample_1 = booking_sample[booking_sample['is_canceled']==1].sort_values(by='bill', ascending=False).head(int(4420*0.113)).bill.sum()\nupper_bound = booking_sample.bill.sum() + booking_sample_0\nlower_bound = booking_sample.bill.sum() - booking_sample_1\nprint('The actual month revenue if between %.2f and %.2f of the predicted sum.' %(upper_bound/booking_sample.bill.sum(), lower_bound/booking_sample.bill.sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To recap we've just done, we took the mean count of bookings the hotels may get on a regular month. We supposed the worst case scenarios are the ones where the wrong predictions are on the reservations with the biggest bills.<br>\nSince the model is 88.7% accurate, we suppose that in one scenario we have the wrong predictions on 11% of the bookings that were actually canceled, this will give us false hope of redeeming the corresponding value (lower bound), and vice versa (scenario where we wrongly assume that the most important 11% of the bookings are going to be canceled, when in fact they will be fulfilled, thus offsetting the total revenue to the upper bound).\nIn conclusion, <b>our model of 88.7% accuracy only helps in predicting the revenue to a whopping +/- 25%. Further statistical significance testing is required (p-values) to determine whether the revenue prediction can be narrowed down to a smaller interval.</b>"},{"metadata":{},"cell_type":"markdown","source":"Another use of the prediction model would be to intentionally overbook the establishments in peak season, to compensate for the bookings to-be-canceled. Given that cancellation rates have a mean of 40% in peak months, we can use this rate to overbook, whenever the predicted fulfilled bookings reach nominal hotel capacity.\nTo adjust for the error of the model, we suppose that 11.3% of the bookings predicted to-be-canceled are going to turn up at the hotel counter once the reserved date comes, and deduct the number from the overbooked capacity."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('* All in all, a rough calculation to optimally overbook the establishment at peak seasons is', int(88.7/0.6),'%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}