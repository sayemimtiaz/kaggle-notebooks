{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **INTRODUCTION**","metadata":{}},{"cell_type":"markdown","source":"## **Aim of the study:**\n\nTo create new features using a naive method which will be called 'Brute Force' feature engineering for the purpose of this exercise.","metadata":{}},{"cell_type":"markdown","source":"## **Operational definitions:**\n\nBrute force method: The method of trying all combinations of available features. This is limited only by the imagination of the user. Followed by Feature Selection.","metadata":{}},{"cell_type":"markdown","source":"## **Rationale/Purpose of study:**\n\nI remain data agnostic at this point of career, hence my functional knowledge in some areas which the data deals with are shallow. So the 'Brute Force' feature engineering helps create new more useful features. If this method remains relevant for most data cases of different functional areas then we can use it as a means of improving our models. Before even attempting 'Brute Force' on the features we can identify that this will work only with continuous variables.","metadata":{}},{"cell_type":"markdown","source":"# **METHODOLOGY**","metadata":{}},{"cell_type":"markdown","source":"## **Settings & Sample:**\n\n813 records of the dataset, after removing the Null records.","metadata":{}},{"cell_type":"markdown","source":"## **Procedure:**\n\nOur aim is to find new useful features. To achieve that we will do the following:\n1. Multiply every feature to all other features. Upto 3 features will be used to do this.\n1. Divide each feature with every other feature.\n1. Break each feature into different groups based on the number of target variables classes.\n1. Do a shapiro test to check normalcy and then perform the Kruskal-Wallis H-test between each group to check whether the medians are significantly different.\n1. Record all the p values in a seperate table.\n1. From the p-value table select all the combinations (product or division) where all p values are less than 0.05. Which would mean that when each group under the feature combination has its median significantly different from each other, then we will select that combination.\n1. Create a new dataframe with all the selected feature combinations.\n\nNote: Original features were dropped here, although we should note that if original features passed the criterias set for selection at the start then they would be a part of the final dataset.","metadata":{}},{"cell_type":"markdown","source":"# **CODE**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro, kruskal\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndf_vehicle = pd.read_csv(\"/kaggle/input/vehicle/vehicle.csv\")\n\n# Display basic information\ndf_vehicle.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Cleaning**\n\n**If we remove all the Null records then we still have around 813 records left with which we can work with. So we will simply drop the Null records instead of imputing them.**","metadata":{}},{"cell_type":"code","source":"# Dropping records with a ny Null value\ndf_clean = df_vehicle.drop(df_vehicle[df_vehicle.isnull().any(axis=1)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the miltivariate pairplot\nsns.pairplot(data=df_clean, hue=\"class\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As evidenced from the graph above, few columns are highly correlated with each other and boundaries between different classes are quite overlapped. Let's see if we can create new features with more distinct boundaries.**\n    \n**The following part will be the main purpose of my work on this dataset. My functional knowledge in this area is shallow so we will use something akin to a brute force to create new features. We will try all combinations of available features upto the 3rd degree and also division of each feature. Then we will let the significance test of the medians of each attribute of the new feature decide if the feature should be used. This is to compensate for the lack of fiunctional knowledge.**\n\n**Although p-statisitic of medians will not generate distinct decision boundaries, it will however be able to check whether majority of the class is significantly on the correct side of the decision boundary.**\n\n**Our aim is to find new useful features. To achieve that we will do the following:-**\n1. Multiply every feature to all other features. Upto 3 features will be used to do this.\n1. Divide each feature with every other feature.\n1. Break each feature into different groups based on the number of target variables classes.\n1. Do a shapiro test to check normalcy and then perform the Kruskal-Wallis H-test between each group to check whether the medians are significantly different.\n1. Record all the p values in a seperate table.\n1. From the p-value table select all the combination (product or division) where all p values are less than 0.05. Which would mean that when each group under the feature combination has its median significantly different from each other, then we will select that combination.\n1. Create a new dataframe with all the selected feature combinations.\n    \n**The number of features selected will generally be quite high. If that is the case we can use PCA or other dimension reduction techniques to reduce the cost of processing.**\n\n**Using PCA will have another useful side effect, which is to remove correlation between new features. Based on the fact that many features will be created from a combination of a few, hence most selected features will be corelated with others.**\n\n**Please note that I will keep things simple in these runs and not use PCA unless I absolutely have to. I will also not do any further feature engineering or feature selection. This is because my sole aim is to check how the brute force method of creating new features perform.**","metadata":{}},{"cell_type":"code","source":"# Creating a function which will return required outputs after performing shapiro test\ndef significance_test(x, y):\n    # If Shapiro test clears both groups (Confidence of 95%) then perform welch test else display appropriate message\n    if (shapiro(x)[1]<0.05) and (shapiro(y)[1]<0.05):\n        # Kruskal Test\n        t, p = kruskal(x, y)\n    else:\n        p = np.nan\n    return(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating analysis dictionary to store results\nanalysis = {}\n\n# Creating a column of values 1 so that product of 2 and single columns are also considered \ndf_clean.insert(df_clean.shape[1]-1, '', 1)\n\n# Performing Welch's t test on normal and abnormal groups for all independent variables\n# Running a for loop to extract each attribute name individually\nfor idx1 in range(len(df_clean.columns[:-2])):\n    \n    # Assign col1 identity\n    col1 = df_clean.columns[idx1]\n    for idx2 in range(len(df_clean.columns[:-1])):\n        \n        # Assign col2 identity\n        col2 = df_clean.columns[idx2]\n        \n        # Avoiding duplicates and cancelling off through if condition\n        if not (idx2==df_clean.shape[1]-2 or idx1==idx2):\n            # Creating 3 groups based on Dependent variable labels for division\n            group1 = df_clean[col1][df_clean[\"class\"]==\"car\"]/df_clean[col2][df_clean[\"class\"]==\"car\"]\n            group2 = df_clean[col1][df_clean[\"class\"]==\"bus\"]/df_clean[col2][df_clean[\"class\"]==\"bus\"]\n            group3 = df_clean[col1][df_clean[\"class\"]==\"van\"]/df_clean[col2][df_clean[\"class\"]==\"van\"]\n            \n            # Storing results in analysis dictionary\n            analysis[col1+'/'+col2] = []\n            analysis[col1+'/'+col2].append(significance_test(group1, group2))\n            analysis[col1+'/'+col2].append(significance_test(group3, group2))\n            analysis[col1+'/'+col2].append(significance_test(group1, group3))\n\n        for idx3 in range(len(df_clean.columns[:-1])):\n            \n            # Assign col3 identity\n            col3 = df_clean.columns[idx3]\n            \n            if idx1<=idx2 and idx2<=idx3:\n                \n                # Creating 3 groups based on Dependent variable labels for product\n                group1 = df_clean[col1][df_clean[\"class\"]==\"car\"]*df_clean[col2][df_clean[\"class\"]==\"car\"]*df_clean[col3][df_clean[\"class\"]==\"car\"]\n                group2 = df_clean[col1][df_clean[\"class\"]==\"bus\"]*df_clean[col2][df_clean[\"class\"]==\"bus\"]*df_clean[col3][df_clean[\"class\"]==\"bus\"]\n                group3 = df_clean[col1][df_clean[\"class\"]==\"van\"]*df_clean[col2][df_clean[\"class\"]==\"van\"]*df_clean[col3][df_clean[\"class\"]==\"van\"]\n\n                # Storing results in analysis dictionary\n                analysis[col1+'*'+col2+'*'+col3] = []\n                analysis[col1+'*'+col2+'*'+col3].append(significance_test(group1, group2))\n                analysis[col1+'*'+col2+'*'+col3].append(significance_test(group3, group2))\n                analysis[col1+'*'+col2+'*'+col3].append(significance_test(group1, group3))\n                \n# Create a dataframe from dictionary for convenience\ndf_analysis = pd.DataFrame(analysis, index=[\"CarvsBus\", \"VanvsBus\", \"CarvsVan\"]).T\n\n# Store the index of all feature combination with Null Hypothesis rejected (@ 95% confidence) on all groups in a list\nls_best = df_analysis[(df_analysis<0.05).all(axis=1)].index\n\n# Initialise a new dataframe\ndf_new = pd.DataFrame()\n\n# Loop through the best combination names from list to assess which column to operate on\nfor comb in ls_best:\n    \n    # Split names into required column names if division\n    if '/' in comb:\n        tmp = comb.split('/')\n        # Create the new feature\n        df_new[comb] = df_clean[tmp[0]]/df_clean[tmp[1]]\n        \n    # Split names into required column names if product\n    elif '*' in comb:\n        tmp = comb.split('*')\n        # Create the new feature\n        df_new[comb] = df_clean[tmp[0]]*df_clean[tmp[1]]*df_clean[tmp[2]]\n\n# Drop the column of 1's as it is no longer required                        \ndf_clean.drop('', axis=1, inplace=True)\n\n# Display stats of new dataset\ndf_new.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Seperating Predictor variables, we know the target variable \"Class\" is the last column\nX = df_new\n\n# Seperating Target variables, we know the target variable is called \"Class\"\ny = df_clean[\"class\"]\n\n# Label Encoding target variable\ny = y.replace({'car': 0, 'bus': 1, 'van': 2})\n\n# Performing the train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Verifying the proportions of y_train and y_test is the same as original dataset\nprint(\"y_train proportions\")\nprint(np.unique(y_train, return_counts=True)[1]/len(y_train))\n\nprint(\"y_test proportions\")\nprint(np.unique(y_test, return_counts=True)[1]/len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Grid Search for best Parameters**","metadata":{}},{"cell_type":"code","source":"# parameter grid\nparam_grid = {\"C\": [0.001, 0.01, 0.1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 10, 100],\n              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n             }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the SVC model\nmodel = svm.SVC(class_weight=\"balanced\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run grid search on 3 folds\nfolds = 3\ngrid_search_SVC = GridSearchCV(model, \n                               cv = folds,\n                               param_grid=param_grid,\n                               return_train_score=True,                         \n                               verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit \ngrid_search_SVC.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cv results\ncv_results = pd.DataFrame(grid_search_SVC.cv_results_)\ncv_results.sort_values(\"rank_test_score\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Final Model**","metadata":{}},{"cell_type":"code","source":"# Creating the SVC model with the best hyper parameters\nmodel = svm.SVC(class_weight=\"balanced\", random_state=42, C=0.001, gamma=0.001, kernel='linear')\n\n# Fit the model to the entire train set\nmodel.fit(X_train, y_train)\n\n# Get Predictions\ny_pred = model.predict(X_test)\n\n# Create confusion matrix dataframe\ndf_cm = pd.crosstab(y_test, y_pred)\n\n# Display confusion matrix dataframe\ndf_cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Analysis**","metadata":{}},{"cell_type":"code","source":"# Creating a dictionary with the class information\ndict_classes = {0: \"car\", 1: 'bus', 2: 'van'}\n\n# Displaying the statistics of the classification Report\n# Initialising precision and recall variables\nprec_avg = 0\nrec_avg = 0\nfor i in range(df_cm.shape[0]):\n    # Calculating Precision and Recall\n    prec = df_cm.iloc[i, i]/sum(df_cm.iloc[:, i])\n    rec = df_cm.iloc[i, i]/sum(df_cm.iloc[i, :])\n    \n    #Totalling up precision and recall for each label\n    prec_avg += prec\n    rec_avg += rec\n    print(dict_classes[i])\n    print(\"Precision: \", prec)\n    print(\"Recall: \", rec)\n    print()\nprec_avg = prec_avg/3\nrec_avg = rec_avg/3\nf1_avg = 2 * (prec_avg * rec_avg)/(prec_avg+rec_avg)\nprint(\"F1 Score of the entire model\", f1_avg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Results and Discussion:**\n\nThe number of features selected will generally be quite high. If that is the case we can use PCA or other dimension reduction techniques to reduce the cost of processing.\n\nUsing PCA will have another useful side effect, which is to remove correlation between new features. Based on the fact that many features will be created from a combination of a few, hence most selected features will be correlated with others.\n\nPlease note that I kept things simple in this run and did not use PCA. I also did not do any further feature engineering or feature selection. This is because my sole aim is to check how the \"Brute Force\" method of creating new features performs. Results were quite impressive (in the high 90's). However, this dataset alone cannot decide the usefulness. I will leave it to the reader to assess whether they want to use this method.","metadata":{}},{"cell_type":"markdown","source":"# **CONCLUSION**","metadata":{}},{"cell_type":"markdown","source":"## **Limitations:** \n\nThis dataset alone cannot decide the usefulness. Further experimentation of the brute force method is necessary.\n\n## **Recommendations:** \n\nIt is also advisable to choose fewer feature combinations by selecting the feature combinations whose medians are the most significant. This can be easily achieved by ordering the analysis dataframe created during the run, by ascending order of the total of all p-values in a particular row. Then selecting only top 'n' feature combinations as independent variables.\n\n## **Implications:**\n\nThe Brute Force Method can be used to combine Continuous variables. Datasets predominant with Continuous variables are the ideal candidates where Brute Force Method can be applied to check if it improves performance. Another aspect to be considered is that Functional Knowledge of the Domain in which the dataset resides should be used first. Only where there is a dearth of such information should Brute Force Method be used. Also we should consider applying Brute Force Method only when there is a chance that combination of features will uncover more relevant information.","metadata":{}}]}