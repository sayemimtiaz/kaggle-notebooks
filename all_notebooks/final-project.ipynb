{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Understanding: Airbnb\n\n## **An Overview**\n\nAirbnb is a platform for individuals to rent out their primary residences as lodgings for travelers. Typically, renters seek accommodations with a homey feel that hotels cannot provide, while most hosts are willing to rent out their homes to supplement their income. The majority of Airbnb‚Äôs revenue comes from service fees from bookings charged to both guests and hosts [1], and it currently covers more than 81,000 cities and 191 countries worldwide [2].\n\n## **The Advantages of Airbnb**\n\n* Wide Selection: \nAirbnb hosts list many different kinds of properties‚Äîsingle rooms, a suite of rooms, apartments, moored yachts, houseboats, entire houses, even a castle‚Äîon the Airbnb website.\n* Free Listings: \nHosts do not have to pay to list their properties.\n* Hosts Can Set Their Own Price: \nIt is up to each host to decide how much to charge per night, per week, or per month.\n* Customizable Searches: \nGuests can search the Airbnb database‚Äînot only by date and location, but by price, type of property, amenities, and the language of the host. They can also add keywords (such as ‚Äúclose to the XXX‚Äù) to further narrow their search.\n* Protections for Guests and Hosts: \nAs a protection for guests, Airbnb holds the guest‚Äôs payment for 24 hours after check-in before releasing the funds to the host. For hosts, Airbnb‚Äôs Host Guarantee program ‚Äúprovides protection for up to $1,000,000 in damages to covered property in the rare event of guest damage, in eligible countries.‚Äù [2]\n\n## **The Disadvantages of Airbnb**\n\n* What You See May Not Be What You Get:\nIndividual hosts create their own listings, and some may be more honest than others. However, previous guests often post comments about their experiences, which can provide a more objective view.\n* Potential Damages:\nProbably the biggest risk for hosts is that their property will be damaged.\n* Added Fees:\nAirbnb imposes a number of additional fees (as, of course, do hotels and other lodging providers). Guests pay a guest service fee of 0% to 20% on top of the reservation fee, to cover Airbnb‚Äôs customer support and other services, and while listings are free, Airbnb charges hosts a service fee of at least 3% for each reservation to cover the cost of processing the transaction.\n* It Is Not Legal Everywhere. [2]\n\nTo sum up, for hosts, participating in Airbnb is a way to earn extra income from their property, but with the main risk that the guest might do serious damage to it. For guests, the advantage can be relatively inexpensive and homey accommodations than a hotel room, but with the risk that the property will not be as appealing as the listing made it seem.\n\nSource:\n[1] Nath, Trevir. ‚ÄúHow Airbnb Makes Money.‚Äù Investopedia.com. N.p., 16 Nov. 2020. Web. 29 Nov. 2020.\n[2] Folger, Jean. ‚ÄúAirbnb: Advantages and Disadvantages.‚Äù Investopedia.com. N.p., 28 Aug. 2020. Web. 29 Nov. 2020.\n\n\n# Data Understanding\n\n## **About Data Understanding**\n\nIn this section, we will give an overview of how the dataset looks like. We will introduce the source and the size of this dataset. We will present the number of instances and attributes (which are equivalent to the number of rows and columns). Specifically, we will discuss the meaning of the attributes and the data type associated with them. \n\n## **Data Understanding of the Dataset**\n\nIn this project, we investigate the [Airbnb ratings dataset](http://www.kaggle.com/samyukthamurali/airbnb-ratings-dataset) which we found on Kaggle. Our goal is to discover which attributes are the most influential to determining the prices of the Airbnbs in the U.S.. The original dataset contains four sub-datasets: LA_Listings, NY_Listings, airbnb_ratings_new and  airbnb-reviews. The first three datasets contain 59.9k instances and 35 attributes, including customer ID, host ID, locations, layouts, furnishings, prices of the residences, review scores, etc. The last dataset contains 1325 instances and 6 attributes, which are customer ID, host ID, review ID, reviewer name, date and comments. \n\n\nFor our case, we picked attributes that are meaningful to our analysis from the four datasets and form our own dataset by filtering and combining the data (which will be discussed in the data preparation section). This new dataset has 295,452 instances and 19 attributes. The description of each attribute is listed below[3]. A specific description of the data of each attribute(count, sd, mean, min, 25\\% quantile, 50\\% quantile, 75\\% quantile, max) are shown in the last table of the data prepartion section. \n\n**Listing ID**: the ID number of an Airbnb \n\n**Host ID** the ID of the host \n\n**Host total listings count**: the total number of host listings \n\n**Longitude**: the longitude of the Airbnb \n\n**Accommodates**: the number of people an Airbnb can accommodate\n\n**Bathrooms**: number of bathrooms \n\n**Bedrooms**: number of bedrooms \n\n**Price**: price of an Airbnb per day\n\n**Minimum nights**: the minimum number of nights a guest stay \n\n**Maximum nights**: the maximum number of nights a guest stay \n\n**Availability 365**: the number of days available in a year \n\n**Number of reviews**: the total number of reviews\n\n**Review Scores Accuracy**: how accurately did the listing page represent an Airbnb? \n\n**Review Scores Cleanliness**: how clean and tidy did the guests feel about an Airbnb? \n\n**Review Scores Checkin**: how smoothly did check-in go?\n\n**Review Scores Communication**: how well did the guests communicate with the hosts before and during the stay?\n\n**Review Scores Location**: how did guests feel about the neighborhood? (Whether there's an accurate description for proximity and access to transportation, shopping centers, city center, etc., and a description that includes special considerations, like noise, and family safety.) \n\n**Review Scores Value**Ôºödid the guest feel that the listing provided good value for the price? \n\n**Reviews per month**: the number of reviews a host receives per month \n\n\n## **Data Type:**\n\nAsides from knowing the meanings of the attributes, it is also necessary to know the data type of the attibutes, since the data type of an attribue affects the methods we can use to analyze and understand the data. \n\nIn general, an attribute can be classified as one of the four data types[4]: \n\n**Nominal**: data that can be categorized by labelling them in exclusive groups (names for categories, classes and states of things, etc)\n\n**Ordinal**: data that can be categorized and ranked, but cannot know the differences between data \n\n**Interval**: a numerical scale where the order of the data is known as well as the difference between the data, but there is no true zero point\n\n**Ratio**: data that can be measured on a scale that not only produces the order of the data but also the difference between data. This type of data has a true zero.\n\n**In our case**, Listing ID and Host ID are of nominal data type, and the rest of the attributes are of ratio data type. \n\n\nSources:\n\n[3] ‚ÄúHow Do Star Ratings Work for Stays? - Airbnb Help Center.‚Äù Airbnb, www.airbnb.com/help/article/1257/how-do-star-ratings-work-for-stays. \n\n[4] https://www.questionpro.com/blog/nominal-ordinal-interval-ratio/\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation \n\nTo begin the exloratory data analysis and train machine learning model, we need to do the data preparation first.\n\nWe will import, combine, and filter the data we need and output an csv file for the further use.\n\n### Import Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, import libraries and define functions for plotting the data using matplotlib."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nimport scipy.stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's read in the data and prepare to combine the csvs into one.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the LA_Lising.csv\ndf = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/LA_Listings.csv', encoding='ISO-8859-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the NY_Listings.csv\ndf2 = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/NY_Listings.csv', encoding='ISO-8859-1')\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finnally, we need to read 'airbnb_ratings_new.csv'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the airbnb_ratings_new.csv\ndf3 = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/airbnb_ratings_new.csv', encoding='ISO-8859-1')\npd.set_option('display.max_columns', None)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in 'airbnb_ratings_new.csv' file, this dataset includes airbnb list from a lot of different countries such as Italy, China Hong Kong, Austria... But we only want to analysis the listings inside U.S because the price changes a lot in diffrent countris, so we need to filter the 'Country' collumn."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered = df3[df3['Country'] == 'United States']\n\ndf_filtered.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's get more infomation with our datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's combine those three datasets into one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedDf = df.append(df2)\ndf_final = combinedDf.append(df_filtered)\n\ndf_final.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, 'df_final' has 295,452 lines of data and ready to use.**"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis \n\nIn This part, we will do exploratory data analysis by examine the correlation between Price with number of bedrooms, bathrooms and review scores.\n\n## Distribution plots of variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport scipy.stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Price\"\nsns.distplot(df_final['Price'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we can see, becaue of the large range of 'Price', we need to \nfilter those unnessary data which could influence our analysis. After observation, we found set the range from 0 to 500 is appropriate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the Price to below 500\nPriceFilteredData = df_final[df_final['Price'] < 500]\n\n# Density Plot and Histogram of variable \"Price\"\nsns.distplot(PriceFilteredData['Price'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see the distribution of numbers of Bedrooms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Bedrooms\"\nsns.distplot(df_final['Bedrooms'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most houses have bedrooms from 0 to 6, so let's filter the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the Bedrooms to below 6\nBedroomsFilteredData = df_final[df_final['Bedrooms'] < 6]\n\n# Density Plot and Histogram of variable \"Price\"\nsns.distplot(PriceFilteredData['Bedrooms'], hist=True, kde=False, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this graph, we can see that the shape of the distributions of 'Numbers of Bedrooms' and the distributions of 'Price' are very similar, which indicates the possibilities between them, and we will do further investigations later. Before that, let's do more distribution graph on other variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the Bathrooms to below 6\nBedroomsFilteredData = df_final[df_final['Bathrooms'] < 6]\n\n# Density Plot and Histogram of variable \"Bathrooms\"\nsns.distplot(BedroomsFilteredData['Bathrooms'], hist= True, kde=False, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the distribution with more varibles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Bedrooms\"\nsns.distplot(df_final['Availability 365'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Review Scores Value\"\nsns.distplot(df_final['Review Scores Value'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Review Scores Value\"\nsns.distplot(df_final['Reviews per month'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's filter the Varible:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the Bathrooms to below 6\nReviewsFilteredData = df_final[df_final['Reviews per month'] < 10]\n\n\n# Density Plot and Histogram of variable \"Review Scores Value\"\nsns.distplot(ReviewsFilteredData['Reviews per month'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density Plot and Histogram of variable \"Review Scores Value\"\nsns.distplot(df_final['Number of reviews'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the Bathrooms to below 6\nReviewsFilteredData = df_final[df_final['Number of reviews'] < 60]\n\n\n# Density Plot and Histogram of variable \"Review Scores Value\"\nsns.distplot(ReviewsFilteredData['Number of reviews'], hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most listing have 0 - 10 reviews.\n\nNext, let's do the correlation test to find out the potential correa\n\n# Correlation Test"},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's do Pairplot using Seanborn Library, to see if there exist some correlation between two variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"BedroomsFilteredData = PriceFilteredData[PriceFilteredData['Bedrooms'] < 6]\nBathroomsFilteredData = BedroomsFilteredData[BedroomsFilteredData['Bathrooms'] < 6]\nfilteredData = BedroomsFilteredData[BedroomsFilteredData['Reviews per month'] < 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['Price','Bedrooms','Bathrooms','Review Scores Value','Reviews per month','Review Scores Accuracy']\nsns.pairplot(filteredData[cols])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph, we cannot find linear pattern between 'Price' and other variables, and we will do correlation test to ensure this assumption. \n\nA correlation coefficient measures the extent to which two variables tend to change together. The coefficient describes both the strength and the direction of the relationship.\n\nAs we know, The Pearson correlation evaluates the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables.\n\nIn our case, because we want to find the correlation between the 'Price' which is continous and the 'Number of Bedrooms' which is ordinal, and the 'Number of Badthrooms' which is also ordinal, the 'Reviews per month' - ordinal, and 'Reviews Score' - ordinal, we should use Spearman method.\n\n\nNow, let's do spearman correlation test on our data: "},{"metadata":{"trusted":true},"cell_type":"code","source":"filteredData.corr(method='spearman')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the result table, we found that 'Price' and 'Accommodates' have a correlation coefficient of 0.55, which indicates they are moderately correlated, and 'number of Bedrooms' has a correlation coefficient of 0.46 with 'Price', which is the second highest value in all variables, which can be understand, because more bedrooms a house has, the higher the price can be, and more people a house can accommodates, more expensive it will be."},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning: Using Linear Regression for Predicting the Prices of Airbnb Residences "},{"metadata":{},"cell_type":"markdown","source":"## **Setup**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn ‚â•0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n#Scikit-learn for implemeting LinearRegression from a existing algorithm.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import resample\n\n# Common imports\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import clear_output\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nnp.random.seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef computeCost(X, y, theta):\n    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Partition the dataset**\n\nSince we would like to test how well our predictive model would perform in the df_final data, we need to test the model's performance with data that has not shown before. To achieve this, we will first partition our dataset in a training and a test set. We will do an 70/30 partition using train_test_split from sklearn.model_selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_final, test = train_test_split(df_final, test_size=0.3, random_state=43)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Remove rows with NA values**\n\nTo make sure the dataset we used to train the model does not contain any NA values, we remove the rows with NA values from our training and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop any rows with null values\ndf_final.dropna(axis=0, how='any', inplace=True)\ntest.dropna(axis=0, how='any', inplace=True)\n\ncolumn_names = ['Host total listings count', 'longitude', 'Accommodates', 'Bathrooms', 'Bedrooms', 'Minimum nights', 'Maximum nights', 'Availability 365', 'Number of reviews',\n                'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value', 'Reviews per month']\nX = df_final[column_names]\ny = df_final['Price']\n\nX_test = test[column_names]\ny_test = test['Price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Fit Multivariate Linear Regression Model**\n\nWe fit the training set to the linear regression model to decide which parameters are useful for our analysis (does not have zero coefficient). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit Multivariate Linear Regression Model\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X, y)\n\n#Calculate the model's parameters error\nnp.random.seed(1)\nerr = np.std([model.fit(*resample(X, y)).coef_\n              for i in range(1000)], 0)\n\nparams = pd.Series(model.coef_, index=X.columns)\nprint(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Remove unnecessary parameters**\n\nNow we remove the variables that have coefficients of zero from the training and test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop any rows with null values\ndf_final.dropna(axis=0, how='any', inplace=True)\ntest.dropna(axis=0, how='any', inplace=True)\n\ncolumn_names = ['Accommodates', 'Bathrooms', 'Bedrooms', 'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value', 'Reviews per month']\nX = df_final[column_names]\ny = df_final['Price']\n\nX_test = test[column_names]\ny_test = test['Price']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We perform multivariate linear regression model again on the new training set to make sure none of the variables have unmeaningful coefficients (coefficients=0). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit Multivariate Linear Regression Model\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X, y)\n\n#Calculate the model's parameters error\nnp.random.seed(1)\nerr = np.std([model.fit(*resample(X, y)).coef_\n              for i in range(1000)], 0)\n\nparams = pd.Series(model.coef_, index=X.columns)\nprint(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge Linear Regression "},{"metadata":{},"cell_type":"markdown","source":"In our project, we decided to use Ridge linear regression model since it can be computed very efficiently and hardly have any computational cost comparing to the original linear regression model [5]. Ridge Regression, also known as $ùêø_2$ regularization, puts constraint on the model coefficients. The penalty on the model fit is: \n \n$P = \\lambda\\sum_{j=1}^{N}\\theta^2_j$ [5]\n\nwhere $\\lambda$ is the penalty term that regularizes the coefficients such that if the coefficients take large values, the optimization function will be penalized [6].\n\nRidge regression can reduce model complexity and prevent over-fitting which may result from simple linear regression [6]. When ùúÜ‚Üí0 , we recover the standard linear regression result; when  ùúÜ‚Üí‚àû , all model responses will be suppressed [5].\n\nSource:\n\n[5] Day_26_Regularized_Linear_Regression.ipynb\n\n[6] https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b"},{"metadata":{},"cell_type":"markdown","source":"## **Setup**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python ‚â•3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n#import warnings\n#warnings.filterwarnings('ignore')\n\n# Scikit-Learn ‚â•0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n#Scikit-learn for implemeting LinearRegression from a existing algorithm.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\n\n# Common imports\nimport numpy as np\nimport os\n\nfrom IPython.display import clear_output\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nnp.random.seed(42)\n\n\ndef computeCost(X, y, theta):\n    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Fit data to Ridge Regression Model**\n\nSince we do not know what degree of polynomial to use, we tried out multiply ones to figure out the most suitable degree value for our model.\n\n### **Degree = 1**\n\n### **Import Ridge Linear Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Ridge Linear Regression\nfrom sklearn.linear_model import Ridge\n\nlambda_term=10\nDegree_of_the_Polynomial_Model=1\npolybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\nstd_scaler = StandardScaler()\n\nRidge_lin_reg = Ridge(alpha=lambda_term)\n\nridge_regression_pipeline = Pipeline([\n        (\"poly_features\", polybig_features),\n        (\"std_scaler\", std_scaler),\n        (\"Ridge_lin_reg\", Ridge_lin_reg),])\n\n#Fit Ridge Linear Regression Model\nridge_regression_pipeline.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Calculate the model's parameters error**\n\nHere we use bootstrap resamplings of the data to compute the uncertainties of the model's parameters.\n\nIn the output, **effect** represents the coefficient estimate of each parameter and **error** is the variation of the coefficient.\n\nTake the parameter Bedrooms as an example. For each additional bedroom in an Airbnb, the price of that Airbnb increases 34$\\pm$1 dollars. "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\nerr = np.std([ridge_regression_pipeline.fit(*resample(X, y)).named_steps.Ridge_lin_reg.coef_\n              for i in range(1000)], 0)\n\nparams = pd.Series(ridge_regression_pipeline.named_steps.Ridge_lin_reg.coef_, \n                   index=ridge_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\npd.DataFrame({'effect': params.round(0),'error': err.round(0)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Calculate model RMSE**\n\nNext, we calculate some performance metrics to visualize how well our Linear Regression model did on the training set and testing set. \n\nRMSE stands for Root Mean Squared Error. It is the error rate by the square root of Mean Squared Error: \n\n$$ RMSE = \\sqrt{MSE}= \\sqrt{ \\frac{1}{m}\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Set\n\nRMSE_training_Ridge=np.sqrt(mean_squared_error(y, ridge_regression_pipeline.predict(X)))\nprint(\"RMSE Ridge LR from Training=\" + str(round(RMSE_training_Ridge,4)))\n\n#Plot predicted values \ndf_final['predicted'] = ridge_regression_pipeline.predict(X)\ndf_final[['Price', 'predicted']].plot(alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing Set\n\nRMSE_test_Ridge=np.sqrt(mean_squared_error(y_test, ridge_regression_pipeline.predict(X_test)))\nprint(\"RMSE Ridge LR from Test=\" + str(round(RMSE_test_Ridge,4)))\n\ntest['predicted'] = ridge_regression_pipeline.predict(X_test)\ntest[['Price', 'predicted']].plot(alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Degree = 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Ridge Linear Regression\nfrom sklearn.linear_model import Ridge\n\nlambda_term=10\nDegree_of_the_Polynomial_Model=2\npolybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\nstd_scaler = StandardScaler()\n\nRidge_lin_reg = Ridge(alpha=lambda_term)\n\nridge_regression_pipeline = Pipeline([\n        (\"poly_features\", polybig_features),\n        (\"std_scaler\", std_scaler),\n        (\"Ridge_lin_reg\", Ridge_lin_reg),])\n\nridge_regression_pipeline.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate the model's parameters error\nnp.random.seed(1)\nerr = np.std([ridge_regression_pipeline.fit(*resample(X, y)).named_steps.Ridge_lin_reg.coef_\n              for i in range(1000)], 0)\n\nparams = pd.Series(ridge_regression_pipeline.named_steps.Ridge_lin_reg.coef_, \n                   index=ridge_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\npd.DataFrame({'effect': params.round(0),'error': err.round(0)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate model RMSE for Training Set\n\nRMSE_training_Ridge=np.sqrt(mean_squared_error(y, ridge_regression_pipeline.predict(X)))\nprint(\"RMSE Ridge LR from Training=\" + str(round(RMSE_training_Ridge,4)))\n\n#Plot predicted values \ndf_final['predicted'] = ridge_regression_pipeline.predict(X)\ndf_final[['Price', 'predicted']].plot(alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate model RMSE for Test Set\n\nRMSE_test_Ridge=np.sqrt(mean_squared_error(y_test, ridge_regression_pipeline.predict(X_test)))\nprint(\"RMSE Ridge LR from Test=\" + str(round(RMSE_test_Ridge,4)))\n\ntest['predicted'] = ridge_regression_pipeline.predict(X_test)\ntest[['Price', 'predicted']].plot(alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Degree = 3**\n"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Import Ridge Linear Regression\nfrom sklearn.linear_model import Ridge\n\nlambda_term=10\nDegree_of_the_Polynomial_Model=3\npolybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\nstd_scaler = StandardScaler()\n\nRidge_lin_reg = Ridge(alpha=lambda_term)\n\nridge_regression_pipeline = Pipeline([\n        (\"poly_features\", polybig_features),\n        (\"std_scaler\", std_scaler),\n        (\"Ridge_lin_reg\", Ridge_lin_reg),])\n\nridge_regression_pipeline.fit(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate the model's parameters error\nnp.random.seed(1)\nerr = np.std([ridge_regression_pipeline.fit(*resample(X, y)).named_steps.Ridge_lin_reg.coef_\n              for i in range(1000)], 0)\n\nparams = pd.Series(ridge_regression_pipeline.named_steps.Ridge_lin_reg.coef_, \n                   index=ridge_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\npd.DataFrame({'effect': params.round(0),'error': err.round(0)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate model RMSE for Training Set\n\nRMSE_training_Ridge=np.sqrt(mean_squared_error(y, ridge_regression_pipeline.predict(X)))\nprint(\"RMSE Ridge LR from Training=\" + str(round(RMSE_training_Ridge,4)))\n\n#Plot predicted values \ndf_final['predicted'] = ridge_regression_pipeline.predict(X)\ndf_final[['Price', 'predicted']].plot(alpha=0.5);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate model RMSE for Test Set\n\nRMSE_test_Ridge=np.sqrt(mean_squared_error(y_test, ridge_regression_pipeline.predict(X_test)))\nprint(\"RMSE Ridge LR from Test=\" + str(round(RMSE_test_Ridge,4)))\n\ntest['predicted'] = ridge_regression_pipeline.predict(X_test)\ntest[['Price', 'predicted']].plot(alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\n* Degree of the polynomial model equals 1:\n\nRMSE Ridge LR from Training = 89.8186\n\nRMSE Ridge LR from Testing = 91.3785\n\n* Degree of the polynomial model equals 2:\n\nRMSE Ridge LR from Training = 87.2148\n\nRMSE Ridge LR from Testing = 88.8126\n\n* Degree of the polynomial model equals 3:\n\nRMSE Ridge LR from Training = 85.6238 \n\nRMSE Ridge LR from Testing = 87.9751\n\nAfter comparing performance metrics (RMSE from training and testing respectively) for three different degrees (1,2,3) and visually inspect how well each of our Linear Regression models did on the training set and testing set, we decided to use the Ridge Linear Regression model with degree of 1. Utilizing a higher degree of the polynomial model did not, to significant extent, increase the performance of our model.\n\nHowever, according to the plots depicting both actual and predicted price of Airbnb, they (actual prices and predicted prices) are not overlapped with each other, which indicates that we have missed some key features. Either our features are not complete (i.e., Airbnb‚Äôs price based on more than just these variables) or there are some nonlinear relationships that we have failed to take into account. Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the Ridge Linear Model (with degree of 1) to estimate how much each feature contributes to the price of Airbnb:\n\nHere's a recap of the parameter estimates we got from fitting the training set to the Ridge Linear Regression model with degree 1: \n\n                                    effect\terror\n                                \n    Accommodates\t                 40.0\t 1.0\n\n    Bathrooms\t                     7.0\t 1.0\n\n    Bedrooms\t                    34.0\t  1.0\n\n    Review Scores Accuracy\t         2.0\t 1.0\n \n    Review Scores Cleanliness\t    12.0\t  1.0\n \n    Review Scores Checkin\t        -9.0\t  1.0\n\n    Review Scores Communication\t    -4.0\t1.0\n\n    Review Scores Location\t        27.0\t 1.0\n    \n    Review Scores Value\t           -20.0\t1.0\n\n    Reviews per month\t            -11.0\t  0.0\n    \n  \nFrom the result above, we see that the price of an Airbnb has a negative relationship with the review scores of checkin, communication and value as well as reviews per month, while accommodates, bathrooms, bedrooms, review scores accuracy, cleanliness and location are positively related to price. Among these, accommodates and bedrooms are particularly influential to the price of an Airbnb -- for each additional accommodates, the price of an Airbnb will increases 40.0 $\\pm$1 dollars, and for each additional bedroom, the price of an Airbnb will increase 34$\\pm$1 dollars. This result corresponds with our analysis of correlation in the EDA section, where we found out price is mostly correlated with accommodates and bedrooms.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}