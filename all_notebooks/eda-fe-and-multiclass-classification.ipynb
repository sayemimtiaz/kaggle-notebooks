{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MARATÓN BEHIND THE CODE 2020\n\n## DESAFÍO 2: TORTUGA CODE"},{"metadata":{},"cell_type":"markdown","source":"### Introducción"},{"metadata":{},"cell_type":"markdown","source":"En proyectos de ciencia de datos destinados a construir modelos de *aprendizaje automático*, o aprendizaje estadístico, es muy inusual que los datos iniciales ya estén en el formato ideal para la construcción de modelos. Se requieren varios pasos intermedios de preprocesamiento de datos, como la codificación de variables categóricas, normalización de variables numéricas, tratamiento de datos faltantes, etc. La biblioteca **scikit-learn**, una de las bibliotecas de código abierto más populares para *aprendizaje automático* en el mundo, ya tiene varias funciones integradas para realizar las transformaciones de datos más utilizadas. Sin embargo, en un flujo común de un modelo de aprendizaje automático, es necesario aplicar estas transformaciones al menos dos veces: la primera vez para \"entrenar\" el modelo, y luego nuevamente cuando se envían nuevos datos como entrada para ser clasificados por este modelo.\n"},{"metadata":{},"cell_type":"markdown","source":"### Trabajando scikit-learn"},{"metadata":{},"cell_type":"raw","source":"# Primero, realizamos la instalación de scikit-learn\n!pip install scikit-learn --upgrade"},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install xgboost --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A continuación importaremos varias bibliotecas que se utilizarán:\n\n# Biblioteca para trabajar con JSON\nimport json\n\n# Biblioteca para realizar solicitudes HTTP\nimport requests\n\n# Biblioteca para exploración y análisis de datos\nimport pandas as pd\n\n# Biblioteca con métodos numéricos y representaciones matriciales\nimport numpy as np\n\n# Biblioteca para construir un modelo basado en la técnica Gradient Boosting\nimport xgboost as xgb\n\n# Paquetes scikit-learn para preprocesamiento de datos\n# \"SimpleImputer\" es una transformación para completar los valores faltantes en conjuntos de datos\nfrom sklearn.impute import SimpleImputer\n\n# Paquetes de scikit-learn para entrenamiento de modelos y construcción de pipelines\n# Método para separar el conjunto de datos en muestras de testes y entrenamiento\nfrom sklearn.model_selection import train_test_split\n# Método para crear modelos basados en árboles de decisión\nfrom sklearn.tree import DecisionTreeClassifier\n# Clase para crear una pipeline de machine-learning\nfrom sklearn.pipeline import Pipeline\n\n# Paquetes scikit-learn para evaluación de modelos\n# Métodos para la validación cruzada del modelo creado\nfrom sklearn.model_selection import KFold, cross_validate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   \nimport itertools\n%matplotlib inline\ndef plot_confusion_matrix(y_true, y_pred, class_names,title=\"Confusion matrix\",normalize=False,onehot = False, size=4):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    if onehot :\n        cm = confusion_matrix([y_i.argmax() for y_i in y_true], [y_ip.argmax() for y_ip in y_pred])\n    else:\n        cm = confusion_matrix(y_true, y_pred)\n    figure = plt.figure(figsize=(size, size))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    # Normalize the confusion matrix.\n    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2) if normalize else cm\n\n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"red\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    #return figure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importar  un .csv a tu proyecto en IBM Cloud Pak for Data al Kernel de este notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_be_scored = pd.read_csv(r'../input/tech-students-profile-prediction/to_be_scored_tortuga.csv')\ndf_to_be_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Primero, importaremos el conjunto de datos proporcionado para el desafío, que ya está incluido en este proyecto.\n\n#!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/maratonadev-la/desafio-2-2020/master/Assets/Data/dataset-tortuga-desafio-2.csv\ndf_training_dataset = pd.read_csv('../input/tech-students-profile-prediction/dataset-tortuga.csv')\ndf_training_dataset.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tenemos 16 columnas presentes en el set de datos proporcionado, 15 de las cuales son variables features (datos de entrada) y una de ellas es una variable target (que queremos que nuestro modelo va a predecir).\n\nLas variables features son:\n\n    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n    NAME                                - Nombre del estudiante\n    USER_ID                             - Número de identificación del estudiante\n    HOURS_DATASCIENCE                   - Número de horas de estudio en Data Science\n    HOURS_BACKEND                       - Número de horas de estudio en Web (Back-End)\n    HOURS_FRONTEND                      - Número de horas de estudio en Web (Front-End)\n    NUM_COURSES_BEGINNER_DATASCIENCE    - Número de cursos de nivel principiante en Data Science completados por el estudiante\n    NUM_COURSES_BEGINNER_BACKEND        - Número de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n    NUM_COURSES_BEGINNER_FRONTEND       - Número de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_DATASCIENCE    - Número de cursos de nivel avanzado en Data Science completados por el estudiante\n    NUM_COURSES_ADVANCED_BACKEND        - Número de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_FRONTEND       - Número de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n    \nLa variable target es:\n\n    PROFILE                             - Perfil de carrera del estudiante (puede ser uno de 6)\n    \n        - beginner_front_end\n        - advanced_front_end\n        - beginner_back_end\n        - advanced_back_end\n        - beginner_data_science\n        - advanced_data_science\n        \nCon un modelo capaz de clasificar a un alumno en una de estas categorías, podemos recomendar contenidos a los alumnos de forma personalizada según las necesidades de cada alumno."},{"metadata":{},"cell_type":"markdown","source":"### Explorando los datos proporcionados\n\nPodemos continuar la exploración de los datos proporcionados con la función ``info()``:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_training_dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualización (visualizations)\n\nPara ver el conjunto de datos suministrado, podemos usar las bibliotecas ``matplotlib`` y ``seaborn``:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_training_dataset['HOURS_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_training_dataset['HOURS_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_training_dataset['HOURS_FRONTEND'].dropna(), ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(28, 8))\n\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_DATASCIENCE'].dropna(), ax=axes[0][0] )\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_BACKEND'].dropna(), ax=axes[0][1] )\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_FRONTEND'].dropna(), ax=axes[0][2])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_DATASCIENCE'].dropna(), ax=axes[1][0])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_BACKEND'].dropna(), ax=axes[1][1])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_FRONTEND'].dropna(), ax=axes[1][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_training_dataset['AVG_SCORE_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_training_dataset['AVG_SCORE_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_training_dataset['AVG_SCORE_FRONTEND'].dropna(), ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(28, 4))\n\nsns.countplot(ax=axes, x='PROFILE', data=df_training_dataset)\ndf_training_dataset['PROFILE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE ENGINEERING"},{"metadata":{},"cell_type":"markdown","source":"## Drop columns\n\nPodemos borrar datos que no vinculantes con las caracteristicas del alumno\n* Unnamed: 0\n* USER_ID\n* NAME"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_training_dataset.drop(columns = [ 'Unnamed: 0', 'USER_ID','NAME' ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filling Nan Values\n\nVamos a llenar nos valores perdidos, agrupando los datos según el profile y llenando los datos con la tecnica de SKITLEARN KNN, dentro de cada clase."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\ndef fill_categorical_valuesUsingNumerical_rfc(data, aux_table, columns2fill, columnsBase_num,columnsBase_cat=None,max_depth=8):\n    \"\"\"\n    columnsBase: es una lista de columnas numéricas\n    columns2fill: es una lista de columnas categóricas\n    aux_table: tabla sin nulos\n    data: data a completar\n    \"\"\"\n    aux_table = aux_table.reset_index(drop=True).copy()\n    data = data.copy()\n#     for ind , table in enumerate(data):\n#         table['Genero'] = table['Genero'].replace('0', np.nan)\n#         data[ind] = table\n    models= defaultdict(list)\n    \n    encoder = OneHotEncoder(handle_unknown='ignore')\n    def fill_nan_categorical(x, model,column,columnsBase_num,mean_values,columnsBase_cat,mode_values):\n        if x[column] == x[column]: ## solo deben entrar los valores nulos\n            return x[column] \n        valores2look = [x[col_name] if x[col_name] == x[col_name] else mean_values[col_name]  for col_name in columnsBase_num]\n        #valores2look = valores2look\n        valores2look = np.stack(valores2look).reshape(1, -1)\n        #print(valores2look)\n        return model.predict(valores2look )[0] \n    \n    def fill_nan_categorical_catBase(x, model,column,columnsBase_num,mean_values,columnsBase_categ,mode_values,encoder):\n        if x[column] == x[column]: ## solo deben entrar los valores nulos\n            return x[column] \n        ## Tomando los valores númericos para la predicción\n        #print(columnsBase_num)\n        valores2look = []\n        if len(columnsBase_num)>0:\n            valores2look = [x[col_name] if x[col_name] == x[col_name] else mean_values[col_name]  for col_name in columnsBase_num]\n        if len(columnsBase_categ)> 0:\n            ## Tomando los valores categóricos para la predicción\n            valores2look_cat = [x[col_name] if x[col_name] == x[col_name] else mode_values[col_name]  for col_name in columnsBase_categ]\n            ##     Transformando con el encoder\n            valores2look_cat = onehot_transfor(encoder, valores2look_cat)\n            ############ SUMANDO LAS DOS LISTAS DE VALORES\n            valores2look = valores2look + valores2look_cat\n        #ADAPTANDO AL FORMATO DE ENTRADA DEL CLASIFICADOR\n        valores2look = np.stack(valores2look).reshape(1, -1)\n        #print(valores2look)\n        value = model.predict(valores2look)[0]\n        return value if type(value)== str else round(value) \n    \n    def upsampling_classes(X, Y , column):\n        df_train_umb = X.join(Y).copy()\n        count4bal = df_train_umb[column].value_counts().sort_values(ascending=True)\n        class_sorter = count4bal.index.tolist()[:-1]\n        class_mayor = count4bal.index[-1]  ##  clase mayoritaria\n        mayority_sample = count4bal[-1]  ## cantidad de muestras en la clase mayoritaria\n        df_train_balanced = df_train_umb.loc[df_train_umb[column]==class_mayor]\n        for class_i in class_sorter:\n            df_minor_upsampled = resample(df_train_umb.loc[df_train_umb[column]==class_i], \n                                     replace=True,     # sample with replacement\n                                     n_samples=mayority_sample,    # to match majority class\n                                     random_state=17) # reproducible results\n            df_train_balanced = pd.concat([df_train_balanced,df_minor_upsampled ],ignore_index=True)\n\n        return df_train_balanced.drop(columns=[column]), df_train_balanced[column]\n    \n    def onehot_transfor(encoder, lista):\n        output = encoder.transform(np.stack([lista[0]]).reshape(-1, 1)).toarray()\n        if len(lista)>1:\n            for cat in lista[1:]:\n                output = output +  encoder.transform(np.stack([cat]).reshape(-1, 1)).toarray()\n            return output[0].tolist()\n        else:\n            return output[0].tolist()\n        \n    def best_model(X, Y,column):\n        #display(X.columns)\n        if type(Y[0]) == object or type(Y[0]) == str:\n            #X, Y = upsampling_classes(X, Y , column)\n            clf_rfr = GridSearchCV(estimator=RandomForestClassifier(random_state=25, n_jobs=-1), \n                        param_grid=[{'n_estimators':[10,20,100,120],'criterion':['entropy','gini']}], # 'max_depth':[None,20], \n                        scoring='f1_macro', n_jobs=-1, cv=5)\n            clf_rfr.fit(X,Y)\n            print(clf_rfr.best_params_)########\n            return clf_rfr\n        else: \n            clf_rfr = GridSearchCV(estimator=RandomForestRegressor(random_state=25, n_jobs=-1), \n                        param_grid=[{ 'n_estimators':[100,120],'criterion':['mse','mae']}], #'max_depth':[None,],\n                        scoring='r2', n_jobs=-1, cv=KFold(n_splits=5) )\n            clf_rfr.fit(X,Y)\n            print(clf_rfr.best_params_)########\n            return clf_rfr\n            \n    ### MODO SOLO NUMÉRICO COMO BASE ####################################################\n    columnsBase = columnsBase_num.copy() + columnsBase_cat.copy()\n     ### LLENANDO VALORES USANDO EL CLASSIFICADOR ADD DOC\n    mean_values = aux_table[columnsBase_num].mean()\n    mode_values = aux_table[columnsBase_cat].mode() if columnsBase_cat else None\n    for column in tqdm(columns2fill):\n        if column in columnsBase:\n            columnsBase_cat_ad = columnsBase_cat.copy()\n            columnsBase_num_ad = columnsBase_num.copy()\n            if column in columnsBase_cat:\n                columnsBase_cat_ad.remove(column)\n            if column in columnsBase_num:\n                columnsBase_num_ad.remove(column)\n\n            if len(columnsBase_cat_ad)>0:\n                base_cat_list = np.stack([str(item)+str(ind)   for ind , column in enumerate(columnsBase_cat_ad) for item in np.unique(aux_table[column].to_numpy())]).reshape(-1, 1)\n                ##print(base_cat_list)\n                encoder.fit(base_cat_list)\n\n            aux_column_set = columnsBase.copy()\n            aux_column_set.remove(column)\n            model = best_model(pd.get_dummies(aux_table[aux_column_set]),aux_table[column],column)\n            ##print(pd.get_dummies(aux_table[aux_column_set]).columns.tolist())\n            models[column] = model.best_score_\n            ### RELLENANDO DATOS\n            for ind, table in enumerate(data):\n                table[column] = table.apply(fill_nan_categorical_catBase, args=(model,column,columnsBase_num_ad,mean_values,columnsBase_cat_ad,mode_values,encoder ), axis=1)\n                data[ind] = table\n        else:\n            base_cat_list = np.stack([str(item)+str(ind)   for ind , column in enumerate(columnsBase_cat)    for item in np.unique(aux_table[column].to_numpy())]).reshape(-1, 1)\n            #print(base_cat_list)\n            encoder.fit(base_cat_list)\n            model = best_model(pd.get_dummies(aux_table[columnsBase]),aux_table[column],column)\n            #print(pd.get_dummies(aux_table[columnsBase]).columns.tolist())\n            models[column] = model.best_score_\n            #models[column] = model\n            ### RELLENANDO DATOS\n            for ind, table in enumerate(data):\n                table[column] = table.apply(fill_nan_categorical_catBase, args=(model,column,columnsBase_num,mean_values,columnsBase_cat,mode_values,encoder ), axis=1)\n                data[ind] = table\n    display(models)\n    return data[0], data[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp_aux = df_train.dropna().reset_index(drop=True).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columnsBase_num = ['HOURS_DATASCIENCE', 'HOURS_BACKEND', 'HOURS_FRONTEND',\n       'NUM_COURSES_BEGINNER_DATASCIENCE', 'NUM_COURSES_BEGINNER_BACKEND',\n       'NUM_COURSES_BEGINNER_FRONTEND', 'NUM_COURSES_ADVANCED_DATASCIENCE',\n       'NUM_COURSES_ADVANCED_BACKEND', 'NUM_COURSES_ADVANCED_FRONTEND',\n       'AVG_SCORE_DATASCIENCE', 'AVG_SCORE_BACKEND', 'AVG_SCORE_FRONTEND' ]\ncolumnsBase_cat = []\n#columnsBase_num = []\ncolumns2bfilled = df_train.columns.tolist()\n# columns2bfilled.remove('Banca_movil_userfriendly')\n# columns2bfilled.remove('Frecuencia_tarjeta_virtual_mes')\ncolumns2bfilled.remove('PROFILE')\ndf_train_fill,df_test_fill = fill_categorical_valuesUsingNumerical_rfc(data = [df_train,df_to_be_scored], aux_table = df_temp_aux.copy(), \n                                                              columns2fill=columns2bfilled, columnsBase_num=columnsBase_num,\n                                                              columnsBase_cat=columnsBase_cat )\ndf_train_fill.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import resample\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.impute import KNNImputer\n\nclasses = df_train['PROFILE'].unique()\nclasses = classes.tolist()\ncolumns = df_train.columns.tolist()\ncolumns.remove('PROFILE')\ndf_healthy = pd.DataFrame(columns=columns)\n\nwhole_profile = df_train[['PROFILE']]\n\nfor class_i in  classes:\n    knnimputer = KNNImputer(n_neighbors=6, weights=\"uniform\")\n    feature_class = df_train.loc[df_train['PROFILE']==class_i ].drop(columns=['PROFILE'])\n    knnimputer.fit(feature_class)\n    feature_class[columns] = knnimputer.transform(feature_class)\n    df_healthy = pd.concat([df_healthy,feature_class])\ndf_healthy = df_healthy.join(whole_profile )\ndf_healthy.sort_index(inplace=True)\n\n\n##################################### Creando KNN_imputer para las muestras de Testeo ####################\ntest_knn_imputer = KNNImputer(n_neighbors=6, weights=\"uniform\")\ntest_knn_imputer.fit(df_healthy.drop(columns=['PROFILE']))\n########\ndf_healthy.head()"},{"metadata":{},"cell_type":"markdown","source":"## Corrigiendo los datos de las columnas de valores enteros"},{"metadata":{"trusted":false},"cell_type":"code","source":"#df_train_fill,df_test_fill \ncolumns_int = ['NUM_COURSES_BEGINNER_DATASCIENCE',\n 'NUM_COURSES_BEGINNER_BACKEND',\n 'NUM_COURSES_BEGINNER_FRONTEND',\n 'NUM_COURSES_ADVANCED_DATASCIENCE',\n 'NUM_COURSES_ADVANCED_BACKEND',\n 'NUM_COURSES_ADVANCED_FRONTEND',]\ndf_train_fill[columns_int] = df_train_fill[columns_int].apply(lambda x: round(x,0)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UPsampling Data\nLa diferencia entre clases no es muy alta pero por buenas prácticas, vamos a nivelar las clases para que los modelos no tengan preferencias por desbalance."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.utils import resample\ncount4bal = df_train_fill['PROFILE'].value_counts().sort_values(ascending=True)\nclass_sorter = count4bal.index.tolist()[:-1]\nclass_mayor = count4bal.index[-1]\nmayority_sample = count4bal[-1]\ndf_balanced = df_train_fill.loc[df_train_fill['PROFILE']==class_mayor]\nfor class_i in class_sorter:\n    df_minor_upsampled = resample(df_train_fill.loc[df_train_fill['PROFILE']==class_i], \n                             replace=True,     # sample with replacement\n                             n_samples=mayority_sample,    # to match majority class\n                             random_state=17) # reproducible results\n\n    df_balanced = pd.concat([df_balanced,df_minor_upsampled ],ignore_index=True)\ndf_balanced['PROFILE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creando nuevas variables\n\nPuedo extraer caracteristicas de los individuos:\n* En dónde tiene su nota más alta\n* Donde tiene la mayor cantidad de cursos\n* Dónde tiene la mayor cantidad de horas <br>\n===> Horas/curso = Cantidad de horas_mayor / Candidad de cursos_mayor <br>\n===> Área de mayor nota         = MAX avg note"},{"metadata":{},"cell_type":"markdown","source":"* Creando la caracteristica | CANTIDAD DE CURSOS X3 | Horas/Especialidad X 3 | Eficiencia_en_especialidad X3"},{"metadata":{"trusted":false},"cell_type":"code","source":"def add_new_features(data):\n    def horasxarea(x):\n        total_backend = x['NUM_COURSES_BEGINNER_BACKEND'] + x['NUM_COURSES_ADVANCED_BACKEND'] \n        total_ds      = x['NUM_COURSES_BEGINNER_DATASCIENCE'] + x['NUM_COURSES_ADVANCED_DATASCIENCE']\n        total_frontend= x['NUM_COURSES_ADVANCED_FRONTEND'] + x['NUM_COURSES_BEGINNER_FRONTEND']\n\n        HR_A_DS = round(x['HOURS_DATASCIENCE']/total_ds) if total_ds != 0 else 0\n        HR_A_BE = round(x['HOURS_BACKEND']/total_backend) if total_backend != 0 else 0\n        HR_A_FE = round(x['HOURS_FRONTEND']/total_frontend)if total_frontend != 0 else 0\n\n        SCORE_HR_A_DS =round( x['AVG_SCORE_DATASCIENCE']/HR_A_DS )if HR_A_DS !=0 else 0\n        SCORE_HR_A_BE = round(x['AVG_SCORE_BACKEND']/HR_A_BE )    if HR_A_BE !=0 else 0\n        SCORE_HR_A_FE = round(x['AVG_SCORE_FRONTEND']/HR_A_FE )   if HR_A_FE !=0 else 0\n\n        return pd.Series([total_backend,total_ds,total_frontend,HR_A_DS, HR_A_BE, HR_A_FE,SCORE_HR_A_DS,SCORE_HR_A_BE,SCORE_HR_A_FE], index = ['NUM_CURS_BE','NUM_CURS_DS', 'NUM_CURS_FE', 'HR_A_DS', 'HR_A_BE', 'HR_A_FE','SCORE_HR_A_DS','SCORE_HR_A_BE','SCORE_HR_A_FE'])\n\n    return data.join(data.apply(horasxarea, axis=1))\n    \ndf_balanced_improve =  add_new_features(df_balanced.copy())\ndf_balanced_improve.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_balanced_improve.groupby(['PROFILE']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graficando las variables saneadas"},{"metadata":{"jupyter":{"outputs_hidden":true}},"cell_type":"raw","source":"columns = df_balanced_improve.columns.tolist()\ncolumns.remove('PROFILE')\n\nfig, axes = plt.subplots(nrows=round(len(columns)/3), ncols=3, figsize=(28, 24))\nfor ax, column in zip(axes.flatten(),columns):\n    sns.distplot(df_balanced_improve[column], ax=ax )"},{"metadata":{},"cell_type":"markdown","source":"# CORR MATRIX - de una sola clase [aleatoria]\n"},{"metadata":{},"cell_type":"raw","source":"import seaborn as sns\n# compute correlation matrix using pandas corr() function\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n#sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\ncorr_df = df_balanced_improve.corr(method='pearson')  # .loc[df_balanced_improve['PROFILE']=='advanced_backend']\nmatrix = np.triu(corr_df)\nhmap=sns.heatmap(corr_df,annot=True, ax=ax, mask=matrix, vmin=-.8,vmax=.8, center=0)"},{"metadata":{},"cell_type":"markdown","source":"# Reducción de dimensionalidad [n_components selection]\n\nImplementaremos el KNN para reducción de dimensionalidad, dada la naturaleza multiclase del problema es más conveniente esta alternativa, manteniendo los parámetros por default\n\n*Resultados*\nLa reducción de dimensionalidad no resulta util para este studio del caso, el modelo lineal no logra mejorar la puntuación "},{"metadata":{},"cell_type":"raw","source":"from sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.metrics import classification_report\nfrom collections import defaultdict\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV \n# columns_improve = df_balanced_improve.columns.tolist()\n# n_components=12\n# nca = NCA(n_components=n_components,random_state=17)\n# nca.fit(df_balanced_improve.drop(columns=['PROFILE']),df_balanced_improve['PROFILE']) \n# df_nca = pd.DataFrame(data = nca.transform(df_balanced_improve.drop(columns=['PROFILE']))).join( df_balanced_improve['PROFILE'] )\n# df_nca.head()\n\nscaler_grid = MinMaxScaler()\nbest_parameters = defaultdict(list) # 'xgb': XGBClassifier(),\nX_train       = df_balanced_improve.drop(columns=['PROFILE']).to_numpy()\ny_train       = df_balanced_improve['PROFILE']\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)\nN_FEATURES_OPTIONS = [5,10, 12, 16,18 ]\npipe_nca = Pipeline([\n    # the reduce_dim stage is populated by the param_grid\n    ('reduce_dim', 'passthrough'),\n    ('scaler',scaler_grid ),\n    ('classify', LR(C= 10.0, dual=False, multi_class= 'multinomial', solver= 'newton-cg'))\n])\n\nparam_grid = [\n    {\n        'reduce_dim': [NCA(random_state=17)],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS\n    }]\n\n# scores = ['f1']\n# for score in scores:\n#     print(\"# %s - Tuning hyper-parameters for %s\" % ('NCA n_features', score))\n# #     clf_i = GridSearchCV(pipe_nca, param_grid, scoring='%s_macro' % score, n_jobs=8, cv=skf)\n# #     clf_i.fit(X_train,y_train)\n#     print(\"Best parameters set found on development set:\")\n#     print()\n#     print(clf_i.best_params_)\n#     #best_parameters[model_name] = clf_i.best_params_\n#     print(\"Grid scores on development set:\")\n#     print()\n#     means = clf_i.cv_results_['mean_test_score']\n#     stds = clf_i.cv_results_['std_test_score']\n#     for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#         print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n#     print(\"Detailed classification report:\")\n#     print()\n#     y_true, y_pred = y_train, clf_i.predict(X_train)\n#     print(classification_report(y_true, y_pred, digits=4 ))"},{"metadata":{},"cell_type":"markdown","source":"## Scaling data\nUtilizaremo sel minMax Scaler para colocar algunas caracteristicas dentro del rango de 0-1"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nn_components = 12 #n_components\nscaler = StandardScaler()#MinMaxScaler()\n# scaler.fit(df_nca[[i for i in range(n_components)]])\n# df_nca[[i for i in range(n_components)]] = scaler.transform(df_nca[[i for i in range(n_components)]])\n# df_nca.head()\ncolumns = df_balanced_improve.columns.tolist()\ncolumns.remove('PROFILE')\n\nscaler.fit(df_balanced_improve.drop(columns=['PROFILE']))\ndf_balanced_improve[columns] = scaler.transform(df_balanced_improve[columns])\ndf_balanced_improve.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class to Transform DATA"},{"metadata":{"trusted":false},"cell_type":"code","source":"class raw2test():\n    def __init__(self, columns_int, scaler, auggfunc):\n        self.columns_int = columns_int\n        self.scaler = scaler\n        self.auggfunc = auggfunc\n        \n    def transform(self, X):\n        X = X.drop(columns = [ 'Unnamed: 0', 'USER_ID','NAME' ])\n        columns = X.columns.tolist()\n        #X[columns] = self.knn_imputer.transform(X) \n        X[self.columns_int] = X[self.columns_int].apply(lambda x: round(x,0)).astype(int)\n        X = self.auggfunc(X)\n        X = self.scaler.transform(X)\n        return X       #test_knn_imputer\npretest = raw2test(columns_int =columns_int, scaler =  scaler,auggfunc=add_new_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL TIME"},{"metadata":{"trusted":false},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV,StratifiedKFold\n#from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier,ExtraTreesClassifier, StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n#from sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.decomposition import PCA\n#from sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\nfrom collections import defaultdict\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nbest_parameters = defaultdict(list) # 'xgb': XGBClassifier(),\nclassifiers = defaultdict(list) # 'xgb': XGBClassifier(), X_train_whole,y_train_whole\nX_train_whole       = df_balanced_improve.drop(columns=['PROFILE']).to_numpy()\ny_train_whole       = df_balanced_improve['PROFILE']\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_whole,y_train_whole, stratify = y_train_whole, random_state= 17,test_size = 0.2 )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##NOTA:\n# El metodo df_catdumm (cat+PCA+scalar) no sirve, entrega un pesimo rendimiento        F1 = 0.64\n# EL metodo df_cat (usar bandas -10 para agrupar valores) entregó un rendimiento medio F1=0.81\n# EL método df_balanced entregó un rendimiento de F1 = 0.93\n# X_train       = df_balanced.drop(columns=['PROFILE'])\n# y_train       = df_balanced['PROFILE']\n\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameters to tune\n# SVC\ntuned_parameters_svc = [{'penalty': ['l2','l1'], 'loss': ['squared_hinge','hinge'],'C': [0.1,1.0,10,100], 'max_iter': [4000], 'random_state':[15], 'dual':[False]}]\n# XGB\ntuned_parameters_xgb = [{'learning_rate':[0.2,0.3,0.4,0.5],'n_estimators':[140,160,180,220,250,300],'min_child_weight':[.01],'subsample':[.4,.5,.8,1.0],'colsample_bytree':[.5,.8,1.0],\n                    'objective':['multi:softmax','binary:logistic'],'n_jobs':[-1],'random_state':[15] }]\n\n#KNC\ntuned_parameters_knc = [{'n_neighbors':[2,4,6,8,10,12,14,16],'n_jobs':[-1],'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],'p':[1,2]}]\n\n#RFC\ntuned_parameters_rfc = [{'n_estimators':[100,120,140,180],'criterion':['gini','entropy'],'min_samples_split':[2,3,4,5],'n_jobs':[-1],'random_state':[15]}]\n\n#GBC\ntuned_parameters_gbc = [{'loss':['deviance','exponential'],'learning_rate':[0.1,0.2,0.3,0.4],'n_estimators':[160,190,220,250,270],'subsample':[1.0],'criterion':[ 'mse', 'friedman_mse'],'max_depth':[3,8,10],'random_state':[15]}]\n\n#ETC\ntuned_parameters_etc = [{ 'min_samples_split':[.2,.4,.8], 'n_estimators':[80,120,150,180,200,250],'warm_start':[True],'bootstrap':[True],\n                         'n_jobs':[-1], 'random_state':[15], 'min_samples_leaf':[3,4,5,6] ,'criterion':['gini', 'entropy'],'max_features':[ 'sqrt']   }]\n\n#LR\ntuned_parameters_lr = [{'C':[0.1,1.0,10.0], 'dual':[False], 'solver':['newton-cg', 'saga','sag'],'multi_class':['ovr', 'multinomial']}]\n\n# Parameter tunning\nscores = ['f1']\n\n# Best parameters\n#  'lr': LR(random_state=17), 'svc':LinearSVC(), 'knc': KNC(),\nmodels = { 'xgb': XGBClassifier(),  'rfc':RandomForestClassifier(), 'etc':ExtraTreesClassifier() ,'gbc':GradientBoostingClassifier() }\nparameters = {'lr':tuned_parameters_lr, 'xgb': tuned_parameters_xgb, 'knc': tuned_parameters_knc,'rfc':tuned_parameters_rfc,'svc':tuned_parameters_svc, 'gbc':tuned_parameters_gbc, 'etc':tuned_parameters_etc }\n\n#;ista = ['etc']\nfor model_name in models.keys():\n    print(\"######### MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s ###############################################################\" % (model_name, score))\n        clf_i = GridSearchCV(models[model_name], parameters[model_name], scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters[model_name] = clf_i.best_params_\n        classifiers[model_name] = clf_i\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_parameters = defaultdict(list,\n            {'xgb': {'colsample_bytree': 1.0,\n              'learning_rate': 0.4,\n              'min_child_weight': 0.01,\n              'n_estimators': 300,\n              'n_jobs': -1,\n              'objective': 'multi:softmax',\n              'random_state': 15,\n              'subsample': 0.8},\n             'rfc': {'criterion': 'gini',\n              'min_samples_split': 2,\n              'n_estimators': 140,\n              'n_jobs': -1,\n              'random_state': 15},\n             'etc': {'bootstrap': True,\n              'criterion': 'gini',\n              'max_features': 'sqrt',\n              'min_samples_leaf': 3,\n              'min_samples_split': 0.2,\n              'n_estimators': 250,\n              'n_jobs': -1,\n              'random_state': 15,\n              'warm_start': True},\n             'gbc': {'criterion': 'friedman_mse',\n              'learning_rate': 0.4,\n              'loss': 'deviance',\n              'max_depth': 8,\n              'n_estimators': 160,\n              'random_state': 15,\n              'subsample': 1.0},\n             'knc': {'algorithm': 'auto',\n              'n_jobs': -1,\n              'n_neighbors': 12,\n              'p': 2},\n             'svc': {'C': 0.1,\n              'dual': False,\n              'loss': 'squared_hinge',\n              'max_iter': 4000,\n              'penalty': 'l2',\n              'random_state': 15},\n             'lr': {'C': 1.0,\n              'dual': False,\n              'multi_class': 'multinomial',\n              'solver': 'newton-cg'}})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging Classifier MODEL - GRIDSEARCH + CV"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_gbc = GradientBoostingClassifier(**best_parameters['gbc'] )#n_estimators=80, n_jobs=-1)# 20 - 80\nclf_RFC = RandomForestClassifier(**best_parameters['rfc'] )#\nclf_KNC = KNC(**best_parameters['knc'] )\nxgb_model = XGBClassifier(**best_parameters['xgb'])\nsvc = LinearSVC(**best_parameters['svc'])\n#etc = ExtraTreesClassifier(**best_parameters['etc'])\n#lr  = LR(**best_parameters['lr'])\n\nmodels = {'knc': clf_KNC,'svc': svc}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_bagging = defaultdict(list)\n#lista = ['etc']\nfor model_name in models.keys():\n    params_bagging = [{'n_estimators': [50,100], 'max_samples':[1.0],'base_estimator': [models[model_name]],'n_jobs':[-1] }]\n    print(\"######### Bagging MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s #########################################################\" % (model_name, score))\n        clf_i = GridSearchCV(BaggingClassifier(), params_bagging, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_bagging[model_name] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Bagging classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_parameters_bagging = defaultdict(list,\n            {'knc': {'base_estimator': KNC(**best_parameters['knc']),\n              'max_samples': 1.0,\n              'n_estimators': 50,\n              'n_jobs': -1},})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improved Models\n\nDependiendo de los resultados del bagging debemos elegir si quedarnos o no el modelo en bagging"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_f = XGBClassifier(**best_parameters['xgb'])\nKNC_f = BaggingClassifier( **best_parameters_bagging['knc'] )\nRFC_f = RandomForestClassifier(**best_parameters['rfc'] )\nsvc_f = LinearSVC( **best_parameters['svc'] )\ngbc_f = GradientBoostingClassifier(**best_parameters['gbc'] )\netc_f = ExtraTreesClassifier(**best_parameters['etc'])\nlr_f  = LR( **best_parameters['lr'] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voting Classifier Model + GridSerch"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold,cross_val_score #( X_train_PCA , label)\nfrom sklearn.metrics import f1_score\n# Ir retirnando los modelos más deviles progresivamente\nestimators0= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators2= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f)]\nestimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)] # **** ganador \nestimators4= [ ('xgb', xgb_f),('rfc',RFC_f),('gbc',gbc_f)] \nestimators5= [ ('rfc',RFC_f),('gbc',gbc_f)]\n\nestimators = [ estimators0,estimators1,estimators2,estimators3,estimators4,estimators5]\nvoting = ['hard','soft']\n\nparams_voting = [{'voting':voting,'n_jobs':[-1] }] # 'estimators': estimators, \n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_voting = defaultdict(list)\n# Scores\nscore_f1 = []\n\nprint(\"######### Voting MODEL tunning hyper-parameters for %s\" % model_name)\nfor ind , set_estimator in enumerate(estimators):\n    model_vc_i = VotingClassifier(estimators = set_estimator)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s #############################################################\" % ('Voting', score))\n        clf_i = GridSearchCV(model_vc_i, params_voting, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_voting['set'+str(ind)] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n    #         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n    #             print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Bagging classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        score_f1.append(np.nan_to_num( means).max())\n        best_parameters_voting['scores'] = score_f1\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Testing - Entrenando con el 80% de los datos"},{"metadata":{"trusted":false},"cell_type":"code","source":"estimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)] # **** ganador \n## X_train, X_test, y_train, y_test\nmodel_VC = VotingClassifier (estimators = estimators3, voting='hard',n_jobs=-1) ## editar\nmodel_VC.fit(X_train,y_train)\ny_true , y_pred =y_test,  model_VC.predict(X_test)\n\nprint(\"Detailed Voting classification report:\")\nprint(classification_report(y_true, y_pred, digits=4))\n#plot_confusion_matrix(y_true=y_train_whole, y_pred=y_pred, class_names=np.unique(y_pred),title=\"VotingClassifier\",normalize=True,size=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STACKING CLASSIFIER + GRIDSEARCH ESTIMADORES"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DTC\ndtc = DTC()\n# Stratify Kfold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\nskf1 = StratifiedKFold(n_splits=4, shuffle=True, random_state=15)\nskf2 = StratifiedKFold(n_splits=8, shuffle=True, random_state=15)\nskf3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\n\n# Ir retirnando los modelos más deviles progresivamente\nestimators0= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** Winner\nestimators2= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f)]\nestimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)]\nestimators4= [ ('xgb', xgb_f),('rfc',RFC_f),('gbc',gbc_f)]  \nestimators5= [ ('rfc',RFC_f),('gbc',gbc_f)]\n\n## Parámetro ganador\n##params_winer = {'cv': StratifiedKFold(n_splits=8, random_state=15, shuffle=True), 'final_estimator': SVC(), 'n_jobs': -1}\n\nestimators = [ estimators0,estimators1,estimators2,estimators3,estimators4,estimators5]\nskf_method = [skf,skf1,skf2,skf3]\n\nparams_sc = [{'cv':skf_method,'n_jobs':[-1], 'final_estimator':[ LR(random_state=17),LinearSVC(random_state=17)] }] # 'estimators': estimators, \n\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_sc = defaultdict(list)\n# Scores\nscore_sc_f1 = []\n\nprint(\"######### Stacking MODEL tunning hyper-parameters\" )\nfor ind , set_estimator in enumerate(estimators):\n    model_sc_i = StackingClassifier(estimators = set_estimator)\n    for score in scores:\n        print(\"# %s - %s Tuning hyper-parameters for %s #############################################################\" % ('Stacking',ind, score))\n        clf_i = GridSearchCV(model_sc_i, params_sc, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_sc['set'+str(ind)] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n    #         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n    #             print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Staking classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        score_sc_f1.append(np.nan_to_num( means).max())\n        best_parameters_sc['scores'] = score_sc_f1\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking Classifier - GridSearch final_estimator"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score #( X_train_PCA , label)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n# Stratify Kfold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\nskf1 = StratifiedKFold(n_splits=4, shuffle=True, random_state=15)\nskf2 = StratifiedKFold(n_splits=8, shuffle=True, random_state=15)\nskf3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\nskf_method = [skf,skf2]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** Winner\nmodel_SC = StackingClassifier (estimators=estimators1 , final_estimator = LogisticRegression(random_state=17) )\n\nparams_SC = {'final_estimator__C': [0.1,1.0, 10.0], 'final_estimator__solver': [ 'sag', 'saga','newton-cg' ], 'final_estimator__max_iter':[200], 'cv': skf_method,'n_jobs':[-1], }\n\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_SC_final_estim = defaultdict(list)\n#lista = ['etc']\nfor score in scores:\n    print(\"# %s - Tuning hyper-parameters for %s\" % ('Stacking Classifier', score))\n    clf_i = GridSearchCV(estimator=model_SC, \n                    param_grid=params_SC, \n                    scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n\n    clf_i.fit(X_train,y_train)\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf_i.best_params_)\n    best_parameters_SC_final_estim['SC'] = clf_i.best_params_\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf_i.cv_results_['mean_test_score']\n    stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+/-%0.03f) for %r\"% (score, mean, std * 2, params))\n    print(\"Detailed Staking classification report:\")\n    print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n    score_sc_f1.append(np.nan_to_num( means).max())\n    best_parameters_sc['scores'] = score_sc_f1\n    print()\n    y_true, y_pred = y_test, clf_i.predict(X_test)\n    print(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Stacking classifier Testing - Entrenando con el 80% de los datos"},{"metadata":{"trusted":false},"cell_type":"code","source":"estimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** XGB optimizado + GBC optimizado\n\nmodel_sc = StackingClassifier(estimators = estimators1, final_estimator = LR(C=10.0, max_iter=200,solver = 'sag', random_state=17), cv = StratifiedKFold(n_splits=5, random_state=15, shuffle=True), n_jobs=-1)\nmodel_sc.fit(X_train,y_train)\ny_true, y_pred = y_test, model_sc.predict(X_test) ## improve 0.9723 from 0.9698 from 0.9693\nprint(\"Detailed Staking classification report:\") \nprint(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Stacking Classifier - Entrenamiento final con todos los datos"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scf = StackingClassifier(estimators = estimators1, final_estimator = LR(C=10.0, max_iter=200,solver = 'sag', random_state=17), cv = StratifiedKFold(n_splits=5, random_state=15, shuffle=True), n_jobs=-1)\nmodel_scf.fit(X_train_whole,y_train_whole)\ny_true, y_pred = y_train_whole, model_scf.predict(X_train_whole)\nprint(\"Detailed Staking classification report whole data:\") \nprint(classification_report(y_true, y_pred, digits=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STACKING CLASSIFIER | Two Layers"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DTC\ndtc = DTC()\n\nskf = StratifiedKFold(n_splits=10 )\n\n\nmodel_sc_l0 = StackingClassifier (estimators=[ ('tree', dtc),('svc',svc_f)], final_estimator = LR(random_state=17, C=1.0, solver='sag'), cv = skf,n_jobs=-1)\n\nmodel_SC = StackingClassifier (estimators=[ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)], final_estimator = model_sc_l0, cv = skf,n_jobs=-1)\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train_whole,y_train_whole, stratify = y_train_whole, random_state= 25,test_size = 0.2 )\n\nmodel_SC.fit(X_train1,y_train1)\ny_pred = model_SC.predict(X_test1)\n\nprint('F1 Score for Stacking model = {}'.format(f1_score(y_test1, y_pred, average='macro')))\nprint(\"Detailed Staking classification report whole data:\") \nprint(classification_report(y_test1, y_pred, digits=4))\n#plot_confusion_matrix(y_true=y_true, y_pred=y_pred, class_names=np.unique(y_pred),title=\"Stacking Classifier\",normalize=True,size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}