{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plot graph\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write Function"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#hidden_layer is node in hidden layer\n#hidden_layer sample [5,5,X] X for clasification = number of class\ndef neural_network_create_weight_bias(feature_list, hidden_layer):\n    #create empty weight list\n    weight = []\n    #create empty bias list\n    bias = []\n    #for in layer of hidden layer\n    for i in range(len(hidden_layer)):\n        \n        #create bias_i \n        #bias appear in hidden_layer only\n        #create bias_i row = 1,column = number of node of  hidden_layer[i]\n        bias_i_column = hidden_layer[i]\n        #create bias_i\n        bias_i = np.random.randn(1, bias_i_column)\n        \n        #create weigth_i\n        if i <= 0:\n            #first weigth_i\n            #create weigth_i row = column of feature\n            weight_i_row = feature_list.shape[1]\n            \n        else:\n            #other weigth_i\n            #create weigth_i row = number of node of hidden_layer[i-1]\n            weight_i_row = hidden_layer[i-1]\n        \n        #create weigth_i column = number of node of  hidden_layer[i]\n        weight_i_column = hidden_layer[i]            \n        #create weigth_i\n        weight_i = np.random.randn(weight_i_row, weight_i_column)\n        \n        #improve weight and bias\n        #improve bias_i using devine by square root of number of node in hidden_layer[i]\n        bias_i = bias_i/np.sqrt(hidden_layer[i])\n        #improve weight_i using devine by square root of number of node in hidden_layer[0]\n        weight_i = weight_i/np.sqrt(hidden_layer[0])\n        \n        #add list\n        #add bias_i to bias list\n        bias.append(bias_i)\n        #add weight_i to weight list\n        weight.append(weight_i)\n        \n    return weight, bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_forward(feature_list, weight, bias, activate_function):\n    #create empty list of output\n    output = []\n    #create empty list of activated_output\n    activated_output = []\n    \n    #for activate_function\n    for i in range(len(activate_function)):\n        \n        if i <= 0:\n            #first layer calculate output_i from feature_list\n            output_i = np.dot(feature_list, weight[i]) + bias[i]\n        else:\n            #first layer calculate output_i from last activated_output\n            output_i = np.dot(activated_output[-1], weight[i]) + bias[i]\n        \n        #compute_activated_output by activate_function[i]\n        activated_output_i = neural_network_compute_activated_output(output_i, activate_function[i])\n                \n        #add output_i and activated_output_i to list\n        output.append(output_i)\n        activated_output.append(activated_output_i)\n        \n    return output,activated_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_compute_activated_output(output_i, activate_function):\n    if type(activate_function) == str:\n        \n        if activate_function == 'sigmoid':\n            #sigmoid : activated_output_i = 1/(1+e^(-output_i))\n            activated_output_i = 1/(1 + np.exp(-output_i))\n            \n        elif activate_function == 'tanh':\n            #hyperbolic tangent : activated_output_i = ((e^output_i)-(e^-output_i))/((e^output_i)+(e^-output_i))\n            activated_output_i = (np.exp(output_i) - np.exp(-output_i))/(np.exp(output_i) + np.exp(-output_i))\n            \n        elif activate_function == 'ReLU':\n            #rectified linear unit : activated_output_i = if output_i <= 0 : 0, if output_i > 0 : output_i\n            activated_output_i = output_i * (output_i > 0)\n            \n        elif activate_function == 'softmax':\n            #softmax : e^output_i/sum(e^output_i)\n            activated_output_i = np.exp(output_i)/np.exp(output_i).sum(axis=1, keepdims = True)\n            \n    elif type(activate_function) == list:\n        \n        if activate_function[0] == 'PReLU':\n            #parametric rectified linear unit : activated_output_i = if output_i <= 0 : output_i * new_slope , if output_i > 0 : output_i \n            #remark slope != 1\n            activated_output_i = output_i * (output_i > 0) + activate_function[1] * output_i * (output_i <= 0)\n            \n    return activated_output_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_compute_different(output_i, activated_output_i, activate_function_i):\n    \n    if type(activate_function_i) == str:\n        \n        if activate_function_i == 'sigmoid':\n            different_i = activated_output_i * (1 - activated_output_i)\n            \n        elif activate_function_i == 'tanh':\n            different_i = 1 - activated_output_i**2\n            \n        elif activate_function_i == 'ReLU':\n            different_i = (output_i > 0)\n            \n    elif type(activate_function_i) == list:\n        \n        if activate_function_i[0] == 'PReLU':\n            different_i = (output_i > 0) + activate_function_i[1] * (output_i <= 0)\n            \n    return different_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_compute_error(delta_i, different_i):\n    error_i = delta_i * different_i\n    return error_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_error(target_list, output, error_type):\n    \n    if error_type == 'SSE':\n        error = find_sum_square_error(target_list, output)\n        \n    elif error_type == 'MSE':\n        error = find_mean_square_error(target_list, output)\n        \n    elif error_type == 'MAE':\n        error = find_mean_absolute_error(target_list, output)\n        \n    elif error_type == 'MAPE':\n        error = find_mean_absolute_percentage_error(target_list, output)\n        \n    elif error_type == 'Entropy':\n        error = find_entropy_error(target_list, output)\n        \n    elif error_type == 'Binary':\n        error = find_binary_class_error(target_list, output)\n        \n    elif error_type == 'Multiclass':\n        error = find_multi_class_error(target_list, output)\n        \n    return error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_sum_square_error(target_list, output):\n    sum_square_error = ((target_list - output)**2).sum()\n    return sum_square_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_mean_square_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    sum_square_error = ((target_list - output)**2).sum()\n    mean_square_error = sum_square_error/number_of_sample\n    return mean_square_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_mean_absolute_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    mean_absolute_error = (np.abs(target_list - output)).sum()/number_of_sample\n    return mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_mean_absolute_percentage_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    mean_absolute_percentage_error = np.abs((target_list - output)/target_list).sum()*100/number_of_sample\n    return mean_absolute_percentage_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_entropy_error(target_list, output):\n    log_output = np.log(output)\n    entropy_error = (-target_list*log_output).sum()\n    return entropy_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_binary_class_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    _target_list = np.round(target_list, 0)\n    _output = np.round(output, 0)\n    binary_class_error = 100*(_target_list != _output).sum()/number_of_sample\n    return binary_class_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_multi_class_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    argmax_of_target_list = np.argmax(target_list, axis=1)\n    argmax_of_output = np.argmax(output, axis=1)\n    multi_class_error = 100*(argmax_of_target_list != argmax_of_output).sum()/number_of_sample\n    return multi_class_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_onehot_target(label):\n    \n    #define unique_label for column\n    unique_label = len(np.unique(label))\n    \n    #define number_of_label for row\n    number_of_label = label.shape[0]\n    \n    #create zeros metrix column = number_of_label, row = unique_label\n    onehot = np.zeros([number_of_label, unique_label])\n    \n    for i in range(number_of_label):\n        #add 1 at label type for each row in zeros metrix\n        onehot[i, label[i]] = 1\n        \n    return onehot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_classification_find_weight_bias(feature_list, target_list, hidden_layer, activate_function, weight = [], bias = [], epoch = 1000, learning_rate = 0.01, lambda1 = 0, lambda2 = 0, dropout = False, prob_drop = [0]):\n   \n    #number_of_layer = length of hidden_layer\n    number_of_layer = len(hidden_layer)\n    #number_of_trianing_data = row of feature_list\n    number_of_trianing = feature_list.shape[0]\n    \n    #if weight == [] create a new one\n    if not weight:        \n        weight, bias = neural_network_create_weight_bias(feature_list, hidden_layer)\n        \n    #create empty error_list\n    error_list = []\n    percent = 0\n    for i in range(epoch):\n        \n        #print progress\n        new_percent = int(i*100/epoch)\n        if(new_percent > percent):\n            percent = new_percent\n            print(percent)\n            \n        #calculate output and activated output for each hidden layer\n        output, activated_output = neural_network_forward(feature_list, weight, bias, activate_function)\n        \n        #find error using output of last layer\n        #find error by entropy error using \"Entropy\"\n        error = find_error(target_list, activated_output[-1], 'Entropy')\n         #collact error in error_list for error trend\n        error_list.append(error)\n        \n        #calculate slope of weight and bias by backpropagation\n        slope_of_weight, slope_of_bias = neural_network_classification_backpropagation(feature_list, weight, bias, output, activated_output, target_list, activate_function, dropout, prob_drop)\n        \n        #loop = number of layer for create new weight,bias using gradient descent\n        for layer_i in range(number_of_layer):\n            #gradient descent weight_new = weight - (learning_rate * (1/n) * slope_of_weight)\n            #add L1 and L2 regularization using lambda1 and lambda2\n            weight[layer_i] = weight[layer_i] + (learning_rate * (1/number_of_trianing) * slope_of_weight[layer_i]) - lambda1*np.sign(weight[layer_i]) - lambda2*weight[layer_i]\n            bias[layer_i] = bias[layer_i] + (learning_rate * (1/number_of_trianing) * slope_of_bias[layer_i]) - lambda1*np.sign(bias[layer_i]) - lambda2*bias[layer_i]\n    return weight, bias, error_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_classification_backpropagation(feature_list, weight, bias, output, activated_output, target_list, activate_function, dropout, prob_drop):\n    \n    #number_of_layer = length of activate_function\n    number_of_layer = len(activate_function)\n    \n    #create empty list\n    slope_weight = []\n    slope_bias = []\n    \n    #for begin in last layer\n    index_of_last_layer = number_of_layer - 1\n    #step -1, stop at 0    \n    #range(start,stop,step)\n    for i in range(index_of_last_layer, -1, -1):\n        \n        if i >= number_of_layer - 1:\n            #last layer delta = target_list - activated_output of last layer \n            delta_i = target_list - activated_output[i]\n            different_i = 1\n        else:\n            #other layer delta_i = error_i of layer [i+1] . transpose(weight of layer [i+1])\n            #remark use error_i = error_i of layer [i+1] because error_i is collected by previous round of loop\n            delta_i = np.dot(error_i, weight[i+1].T)\n            different_i = neural_network_compute_different(output[i], activated_output[i], activate_function[i])\n            \n        #collect error_i to error_i for calculate previous layer\n        error_i = neural_network_compute_error(delta_i, different_i)\n        \n        if i <= 0:\n            #at first layer slope_weight_i = transpose(feature_list) . error_i\n            slope_weight_i = np.dot(feature_list.T, error_i)\n        else:\n            #other layer slope_weight_i = transpose(activated_output of [i-1] layer) . error_i\n            slope_weight_i = np.dot(activated_output[i-1].T, error_i)\n        \n        if dropout == False:\n            if i <= 0:\n                #at first layer slope_weight_i = transpose(feature_list) . error_i\n                slope_weight_i = np.dot(feature_list.T, error_i)\n            else:\n                #other layer slope_weight_i = transpose(activated_output of [i-1] layer) . error_i\n                slope_weight_i = np.dot(activated_output[i-1].T, error_i)\n\n                #slope_bias_i = sum(error_i)\n                slope_bias_i = error_i.sum(axis=0)\n        else:\n            #create drop out node list for each layer\n            dropout_node_list = neural_network_random_dropout_node(hidden_layer, prob_drop)\n            \n            if i > 0:\n                #at other layer drop\n                slope_weight_i = np.dot((activated_output[i-1]*dropout_node_list[i-1]).T, error_i)\n            else:\n                #at first layer not drop\n                slope_weight_i = np.dot(feature_list.T, error_i)\n                \n            if i >= number_of_layer - 1:\n                #at last layer not drop\n                slope_bias_i = error_i.sum(axis=0)\n            else:\n                #at other layer drop\n                slope_bias_i = (error_i*dropout_node_list[i]).sum(axis=0)\n        \n        \n        #slope_bias_i = sum(error_i)\n        slope_bias_i = error_i.sum(axis=0)\n        \n        #add slope_weight_i,slope_bias_i to list\n        slope_weight.append(slope_weight_i)\n        slope_bias.append(slope_bias_i)\n        \n    #convert [slope_weight_3,slope_weight_2,slope_weight_1] to [slope_weight_1,slope_weight_2,slope_weight_3] \n    slope_weight =  slope_weight[::-1]\n    slope_bias =  slope_bias[::-1]\n    \n    return slope_weight, slope_bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_random_dropout_node(hidden_layer, prob_drop):\n    dropout_node_list = []\n    for i in range(len(hidden_layer)):\n        dropout_node = np.random.choice([False, True], [1, hidden_layer[i]], p = [prob_drop[i], 1 - prob_drop[i]])\n        dropout_node_list.append(dropout_node)\n    return dropout_node_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(feature_list, target_list, train_size_percent = 80):\n    \n    #define N\n    number_of_data = feature_list.shape[0]\n    \n    #random for split\n    arr_rand = np.random.rand(number_of_data)\n    \n    #split random array using train_size_percent \n    split = arr_rand < np.percentile(arr_rand, train_size_percent)\n    \n    #split\n    feature_list_train = feature_list[split]\n    target_list_train = target_list[split]\n    feature_list_test =  feature_list[~split]\n    target_list_test = target_list[~split]\n    \n    return feature_list_train,target_list_train,feature_list_test,target_list_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardization(data, mean_norm, std_norm):\n    data_norm = (data - mean_norm)/std_norm\n    return data_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def std_for_norm(data):\n    _std = data.std(axis=0)\n    return _std.reshape(1, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_for_norm(data):\n    _mean = data.mean(axis=0)\n    return _mean.reshape(1, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def de_standardization(data_norm, mean_norm, std_norm):\n    data = data_norm*std_norm + mean_norm\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data & Prepare Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#read data values\nraw_csv = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nraw_data = raw_csv.values\nfeature = raw_data[:,:-1]\ntarget = raw_csv['DEATH_EVENT'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train-test data\nfeature_list_train,target_list_train,feature_list_test,target_list_test = train_test_split(feature,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find standard deviation\nstd_feature_list_train = std_for_norm(feature_list_train)\n\n#find mean\nmean_feature_list_train = mean_for_norm(feature_list_train)\n\n#normalize with standardization\nfeature_list_train_norm = standardization(feature_list_train, mean_feature_list_train, std_feature_list_train)\nfeature_list_test_norm = standardization(feature_list_test, mean_feature_list_train, std_feature_list_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create one hot matrix for classification\ntarget_list_train_onehot = create_onehot_target(target_list_train)\ntarget_list_test_onehot = create_onehot_target(target_list_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#last layer of clasification = unique target\nunique_label = len(np.unique(target_list_train))\n\n#defind node in hidden layer\nhidden_layer = [20,20,unique_label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define activated for each layer \n#for clasification use softmax in last layer\nactivated_function = ['tanh',['PReLU',0.1],'softmax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random initial weight, bias for train model\nweight, bias = neural_network_create_weight_bias(feature_list_train_norm, hidden_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\nweight, bias, error_list = neural_network_classification_find_weight_bias(feature_list_train_norm, target_list_train_onehot, hidden_layer, activated_function, weight = weight, bias = bias, epoch = 10000, learning_rate = 0.1,lambda1 = 0,lambda2 = 0,dropout = True, prob_drop = [0.5,0.5,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot error list\nplt.plot(error_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_list[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict Yhat_train\nZhat_train, Yhat_train = neural_network_forward(feature_list_train_norm, weight, bias, activated_function)\n#find Yhat_train muticlass error\nerror_train = find_error(target_list_train_onehot, Yhat_train[-1], 'Multiclass')\n#print\nprint(error_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict Yhat_test\nZhat_test, Yhat_test = neural_network_forward(feature_list_test_norm, weight, bias, activated_function)\n#find Yhat_test muticlass error\nerror_test = find_error(target_list_test_onehot, Yhat_test[-1], 'Multiclass')\n#print\nprint(error_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view error prediction image\nnot_match_count = 0\n#for in feature_list_test\nfor i in range(len(feature_list_test)):\n        #argmax predicted value\n        predicted = np.argmax(Yhat_test[-1], axis=1)[i]\n        #label value\n        label = np.argmax(target_list_test_onehot, axis=1)[i]\n        #check predicted\n        if(predicted != label):\n            #count not_match]\n            not_match_count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view percent error and not match count\nprint(\"not match count : {0}\".format(not_match_count))\nprint(\"feature list test count : {0}\".format(len(feature_list_test)))\nprint(\"accuracy : {:.4f}%\".format((1-(not_match_count/len(feature_list_test)))*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_to_submit = pd.DataFrame(weight)\n# data_to_submit.to_csv('./weight.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_to_submit = pd.DataFrame(bias)\n# data_to_submit.to_csv('./bias.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}