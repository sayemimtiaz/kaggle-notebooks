{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Project Objective and Brief**\n\n## *In this project, rule-based and Deep-Learning algorithms are used with an aim to first appropriately detect different type of emotions contained in a collection of Tweets and then accurately predict the overall emotions of the Tweets is done.*"},{"metadata":{},"cell_type":"markdown","source":"## **Preprocessor is a preprocessing library used for tweet data written in Python.While building Machine Learning systems based on tweet data, a preprocessing is required. This library makes it easy to clean, parse or tokenize the tweets.The same is imported here.** "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tweet-preprocessor 2>/dev/null 1>/dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Importing Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import preprocessor as pcr\nimport numpy as np \nimport pandas as pd \nimport emoji\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom sklearn import preprocessing,  model_selection\nfrom keras.preprocessing import sequence, text\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport plotly.express as px\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tokenizers import Tokenizer, models \nfrom tensorflow.keras.layers import SpatialDropout1D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data_1 = pd.read_csv(\"../input/tweetscsv/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = df_data_1[[\"tweet_id\",\"airline_sentiment\",\"text\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Correcting Spelling of data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_spell = pd.read_csv(\"../input/spelling/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\ndata_spell.misspell = data_spell.misspell.str.strip()\ndata_spell.misspell = data_spell.misspell.str.split(\" \")\ndata_spell = data_spell.explode(\"misspell\").reset_index(drop=True)\ndata_spell.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(data_spell.misspell, data_spell.correction))\n\n#Sample of the dict\n{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_spell(v):\n    for a in v.split(): \n        if a in miss_corr.keys(): \n            v = v.replace(a, miss_corr[a]) \n    return v\n\ndf_data[\"clean_content\"] = df_data.text.apply(lambda a : correct_spell(a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Using a Python library for expanding and creating common English contractions in text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"contract = pd.read_csv(\"../input/contractions/contractions.csv\")\ncont_dict = dict(zip(contract.Contraction, contract.Meaning))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def contract_to_meaning(v): \n  \n    for a in v.split(): \n        if a in cont_dict.keys(): \n            v = v.replace(a, cont_dict[a]) \n    return v\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.clean_content = df_data.clean_content.apply(lambda a : contract_to_meaning(a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removal of URLs and Mentions from dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pcr.set_options(pcr.OPT.MENTION, pcr.OPT.URL)\npcr.clean(\"hello guys @alx #sportüî• 1245 https://github.com/s/preprocessor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data[\"clean_content\"]=df_data.text.apply(lambda a : pcr.clean(a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removal of Punctuations and Emojis from dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def punct(v): \n  \n    punct = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n  \n    for a in v.lower(): \n        if a in punct: \n            v = v.replace(a, \" \") \n    return v\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct(\"test @ #ldfldlf??? !! \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.clean_content = df_data.clean_content.apply(lambda a : ' '.join(punct(emoji.demojize(a)).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(v):\n    v = correct_spell(v)\n    v = contract_to_meaning(v)\n    v = pcr.clean(v)\n    v = ' '.join(punct(emoji.demojize(v)).split())\n    \n    return v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cleaning(\"isn't üí° adultry @ttt good bad ... ! ? \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removing empty comments from dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = df_data[df_data.clean_content != \"\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.airline_sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Modeling**"},{"metadata":{},"cell_type":"markdown","source":"## **Encoding the data and train, test and split it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"id_for_sentiment = {\"neutral\":0, \"negative\":1,\"positive\":2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data[\"sentiment_id\"] = df_data['airline_sentiment'].map(id_for_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_label = LabelEncoder()\nencoding_integer = encoding_label.fit_transform(df_data.sentiment_id)\n\nencoding_onehot = OneHotEncoder(sparse=False)\nencoding_integer = encoding_integer.reshape(len(encoding_integer), 1)\nY = encoding_onehot.fit_transform(encoding_integer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM: Long short-term memory** \n\n### **It is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras tokenizer here\ntkn = text.Tokenizer(num_words=None)\nmaximum_length = 160\nEpoch = 15\ntkn.fit_on_texts(list(X_train) + list(X_test))\nX_train_pad = sequence.pad_sequences(tkn.texts_to_sequences(X_train), maxlen=maximum_length)\nX_test_pad = sequence.pad_sequences(tkn.texts_to_sequences(X_test), maxlen=maximum_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_idx = tkn.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dimension = 160\nlstm_out = 250\n\nmodel_sql = Sequential()\nmodel_sql.add(Embedding(len(t_idx) +1 , embedding_dimension,input_length = X_test_pad.shape[1]))\nmodel_sql.add(SpatialDropout1D(0.2))\nmodel_sql.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel_sql.add(keras.layers.core.Dense(3, activation='softmax'))\n#adam rmsprop \nmodel_sql.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model_sql.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_of_batch = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sql.fit(X_train_pad, y_train, epochs = Epoch, batch_size=size_of_batch,validation_data=(X_test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_emotion(model_sql,text_1):\n    text_1 = text_cleaning(text_1)\n    #tokenize\n    tweet = tkn.texts_to_sequences([text_1])\n    tweet = sequence.pad_sequences(tweet, maxlen=maximum_length, dtype='int32')\n    emotion = model_sql.predict(tweet,batch_size=1,verbose = 2)\n    emo = np.round(np.dot(emotion,100).tolist(),0)[0]\n    rslt = pd.DataFrame([id_for_sentiment.keys(),emo]).T\n    rslt.columns = [\"sentiment\",\"percentage\"]\n    rslt=rslt[rslt.percentage !=0]\n    return rslt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def result_plotting(df):\n    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])\n    #fig.show()\n    clrs={'neutral':'rgb(213,0,0)','negative':'rgb(0,0,0)',\n                    'positive':'rgb(0,142,248)'}\n    col={}\n    for i in rslt.sentiment.to_list():\n        col[i]=clrs[i]\n    figure = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col,hole=0.3)\n    figure.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_sql,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_sql,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-3"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_sql,\"I hate this game so much,It make me angry all the time \")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM with GloVe 6B 200d word embedding**\n### **GloVe algorithm is an extension to the word2vec method for efficiently learning word vectors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_reading(file):\n    with open(file,'r') as z:\n        word_vocabulary = set() \n        word_vector = {}\n        for line in z:\n            line_1 = line.strip() \n            words_Vector = line_1.split()\n            word_vocabulary.add(words_Vector[0])\n            word_vector[words_Vector[0]] = np.array(words_Vector[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocabulary))\n    return word_vocabulary,word_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary, word_to_index =data_reading(\"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix_embedding = np.zeros((len(t_idx) + 1, 200))\nfor word, i in t_idx.items():\n    vector_embedding = word_to_index.get(word)\n    if vector_embedding is not None:\n        matrix_embedding[i] = vector_embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dimension = 200\nlstm_out = 250\n\nmodel_lstm = Sequential()\nmodel_lstm.add(Embedding(len(t_idx) +1 , embedding_dimension,input_length = X_test_pad.shape[1],weights=[matrix_embedding],trainable=False))\nmodel_lstm.add(SpatialDropout1D(0.2))\nmodel_lstm.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel_lstm.add(keras.layers.core.Dense(3, activation='softmax'))\n#adam rmsprop \nmodel_lstm.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model_lstm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_of_batch = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM with GloVe Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lstm.fit(X_train_pad, y_train, epochs = Epoch, batch_size=size_of_batch,validation_data=(X_test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM GloVe**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_lstm,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM GloVe**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_lstm,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Result of LSTM GloVe**"},{"metadata":{},"cell_type":"markdown","source":"### Paragraph-3"},{"metadata":{"trusted":true},"cell_type":"code","source":"rslt =get_emotion(model_lstm,\"I hate this game so much,It make me angry all the time \")\nresult_plotting(rslt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Algorithms used to detect different types of emotion from paragraph are**\n\n**1- LSTM (Long Short Term Memory)**-It is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n**2- LSTM GloVe- GloVe algorithm is an extension to the word2vec method for efficiently learning word vectors.**\n\nIt has been concluded that using LSTM algorithm it is easier to classify the Tweets and a more accurate result is obtained."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}