{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Get and Explore Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the data we have available to get a better understanding of what we are dealing with"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Drop some unwanted values that would be problematic for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Split data to train and test sets. We will use the train set for fitting our model to the data and the use the test set in order to make our predictions and see how well the model generalizes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.email, dataset.label, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"> We create a custom sklearn transformer that performes stemming (stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form) using an ntlk stemmer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom nltk.stem.porter import *\n\nclass EmailStemming(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        x_temp = []\n        stemmer = PorterStemmer()\n        for email in X:\n            x_temp.append([stemmer.stem(word) for word in email.split()])\n        X = x_temp\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This custom transformer creates a corpus from all the unique words available on all emails, and then creates a vector for each email that count the words of that email present on the corpus\n\n> For example:\n> If the corpus contained [best, the, is, big] the email \"He is the best of the best\" would be vector [2, 2, 1, 0]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass FeatureVectors(BaseEstimator, TransformerMixin):\n    def __init__(self, corpus=None, corpus_len=100):\n        self.corpus = corpus\n        self.corpus_len = corpus_len\n        \n    def fit(self, X, y=None):\n        if self.corpus == None:\n            self.corpus = Counter([\n                word \n                for email in X\n                for word in email\n            ]).most_common(self.corpus_len)\n        return self\n    \n    def transform(self, X):\n        corpus_list = [key for key, _ in self.corpus]\n        x_temp = []\n        for email in X:\n            x_temp.append(np.array([email.count(word) for word in corpus_list]))\n        X = x_temp\n        return np.array(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We create a data processing pipeline, that will apply our custom transformers to the data passed to it"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\ncorpus_len = 1000\npreprocess_pipeline = Pipeline([\n    ('email_stemming', EmailStemming()),\n    ('feature_vectors', FeatureVectors(corpus_len=corpus_len)),\n])\n\nX_train_proc = preprocess_pipeline.fit_transform(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_proc.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Select Models"},{"metadata":{},"cell_type":"markdown","source":"> Lets try some different Machine Learning algorithms to see what fits our training dataset best"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train_proc, y_train, cv=10)\nsvm_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(penalty='l2', max_iter=1000, fit_intercept=True)\nlog_scores = cross_val_score(log_clf, X_train_proc, y_train, cv=10)\nlog_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100)\nforest_scores = cross_val_score(forest_clf, X_train_proc, y_train, cv=10)\nforest_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn_scores = cross_val_score(knn_clf, X_train_proc, y_train, cv=10)\nknn_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Visualization"},{"metadata":{},"cell_type":"markdown","source":"> We train the model on subsets of the dataset in order to get a sense of learning progress marked by the blue line. As we can see the f1 score starts from 40% and gets gradually better it converges near the end with the red line"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n    train_errors, val_errors = [], []\n    for m in range(20, len(X_train), 10):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(f1_score(y_train[:m], y_train_predict) * 100)\n        val_errors.append(f1_score(y_val, y_val_predict) * 100)\n\n    plt.plot(train_errors, \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(val_errors, \"b-\", linewidth=3, label=\"val\")\n    plt.legend(loc=\"upper right\", fontsize=14)\n    plt.xlabel(\"\", fontsize=14)\n    plt.ylabel(\"F1\", fontsize=14)             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, fit_intercept=True)\nplot_learning_curves(log_clf, X_train_proc, y_train)\nplt.axis([0, 100, 0, 100])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{},"cell_type":"markdown","source":"> Logistic regression fits well to the training set so it will be used for making predictions on the test set after passing it through the processing pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_proc = preprocess_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, fit_intercept=True)\nlog_clf.fit(X_train_proc, y_train)\ny_pred = log_clf.predict(X_test_proc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"F1 score and Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\nprint(f'F1 score: {f1_score(y_pred, y_test)}')\nprint(f'Accuracy score: {accuracy_score(y_pred, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank you"},{"metadata":{},"cell_type":"markdown","source":"Thank you very much for the read, consider upvoting if you found something useful on this Notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}