{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"The goal of this project is to use unsupervised learning techniques to identify wine categories. Our dataset is composed of 11 numerical physical-chemical measurements which will be used by a gaussian mixture model to identify these distinct categories. We will not assume any particular number of clusters beforehand but will rather use the silhouette score as an indicator of the best number of clusters to segment our data by. Our dataset is a combination of a red wine dataset and a white wine dataset but we will not use the color by our clustering algorithm. Instead, we are interested in whether our algorithm will naturally segment wine into white and red categories. \n\nThe dataset can be found here:\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport numpy as np\nimport pandas as pd\nimport random\nfrom IPython.display import display #for displaying dataframes nicely\nimport matplotlib.pyplot as plt\n\n\nrandom.seed(137)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine red and white datasets\nreds= pd.read_csv('../input/wine-quality-selection/winequality-red.csv')\nreds['type']= 'red'\nwhites= pd.read_csv('../input/wine-quality-selection/winequality-white.csv')\nwhites['type']='white'\n\n#drop the \"quality\" column as it is not a physical property that we are interested in\nwines= pd.concat([reds, whites])\nwines.drop(columns=['quality'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('rows, columns: {}'.format(wines.shape))\ndisplay(wines.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wines.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, our values have very different ranges. We will need to rescale our data for optimal performance with typical machine learning algorithms. Before doing so, let's check for highly correlated features. If some features are found to be highly correlated, they will likely be redundant in the learning process and we can drop all but one of each group of highly correlated features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use a heatmap to visualize the greatest correlations\nplt.style.use('default')\nplt.figure(figsize=(7, 7))\nplt.imshow(wines.corr(), cmap='Reds', interpolation= 'nearest')\nplt.xticks(np.arange(len(wines.corr().index.values)), wines.corr().index.values, fontsize=12, rotation=-60)\nplt.yticks(np.arange(len(wines.corr().index.values)), wines.corr().index.values, fontsize=12)\nplt.title('Heatmap of Correlations')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View the numeric data corresponding to the above\nwines.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetch top correlations\n\n#Return a copy of dataframe with only metric columns\ndef drop_dims(df):\n    df= df.copy()\n    for i, e in zip(df.columns.values, df.dtypes):\n        if e not in [np.float64, np.int64]:\n            df.drop(i, inplace=True, axis=1)\n    return df\n\n#Every pair of metrics shows up twice\n#This function removes one version of each pair (along with pricipal axis in which all correlations equal 1.0)\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    \n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n#return the highest correlations\ndef get_top_abs_correlations(df, n=3):\n    df= drop_dims(df)\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\\n\")\ndisplay(get_top_abs_correlations(wines))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest correlation found was between free sulfur dioxide and total sulfur dioxide, which was 0.72. This is too low to justify dropping a column, especially for a small dataset which shouldn’t pose any significant performance concerns. \n\nLet’s now look at the distributions of each metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a subplot showing a histogram for each metric\ndef show_metric_dist(df, title):\n    plt.style.use('ggplot')\n    plt.rcParams['figure.figsize']= [10, 8]\n    fig = plt.figure()\n    plt.subplots_adjust(hspace=0.4)\n    \n    fig.suptitle(title, fontsize=20, y=0.98)\n    j=1\n    for i, e in zip(df.columns.values, df.dtypes):\n        if e not in [np.float64, np.int64]:\n            continue\n        ax=fig.add_subplot(3, 4, j)\n        df[i].hist(bins=50, color='maroon')\n        ax.set_title(i)\n        j+=1\n        \n    plt.show()\n\n\nshow_metric_dist(wines, title='Metric Distributions (Before Normalizing & Scaling)')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the distributions of many of these metrics is significantly skewed. In general, machine learning algorithms works best when metrics have a roughly gaussian distribution. We will need to normalize this data in addition to scaling it. \n\nWe first normalize the data using boxcox. A shift is manually added as suggested in the documentation:\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n\n\nWe use the MinMaxScaler to normalize the data such that all metrics range from 0 to 1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#test alternative normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\n\n#scale to gaussian\nwines_mets= drop_dims(wines)\nwines_scaled = wines_mets.apply(lambda x: stats.boxcox(x+1e-8)[0], axis=0)\n\nscaler = MinMaxScaler()\n\nwines_norm = pd.DataFrame(wines_scaled)\n\nwines_norm = pd.DataFrame(scaler.fit_transform(wines_scaled), columns=wines_scaled.columns)\n\n# Show an example of a record with scaling applied\ndisplay(wines_norm.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metric_dist(wines_norm, title='Metric Distributions (After Normalizing & Scaling)')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, all metrics are now roughly gaussian and have a range from 0 to 1. This data is now optimal input for typical machine learning algorithms. \n\nBefore proceeding, we will perform a principal component analysis (PCA). PCA is a technique in which a set of normal axes are greedily chosen such that each axis is oriented such that variance is maximized along that axis. We will perform PCA in order to determine several things:\n\n1) The minimum number of dimensions necessary to preserve the majority of the information (variance) contained in our data. This will be of interest to use if we decide to use fewer dimensions to our ML algorithm in the interest of boosting performance.\n\n2) This will help us determine whether a 3D visualization will be sufficient to see some of the inherent structure in our data. \n\nWe will perform a PCA transformation using the same number of dimensions as our existing dataset. We will then calculate the percentage of the overall variance can be encoded in 1, 2, ... n dimensions. We will then examine the first component of our PCA transformed data to identify which metrics account for the most variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA by fitting the data with the same number of dimensions as features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=wines_norm.shape[1], random_state=51)\npca.fit(wines_norm)\n\n#Transform wines_norm using the PCA fit above\npca_samples = pca.transform(wines_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(wines_norm.shape[1]):\n\n    first_n = pca.explained_variance_ratio_[0:i+1].sum()*100\n    print('Percent variance explained by first {} components: {:.1f}%'.format(i+1, first_n))\n\n\nprint('\\nFirst principle component contributions:\\n')\nfirst_comp= zip(wines_norm.columns.values, pca.components_[0])\n\nfor i, j in first_comp:\n    print(i, '%.3f' % j)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the majority of the variance in our data (>95%) can be encoded in 8 of our 11 dimensions. We see that over 65% of our variance can be encoded in 3 dimensions. This suggests that we can expect to see some of the underlying structure in a 3D visualization, but much of it will still be hidden. \n\nIn the breakdown of the first principle component that follows, we see that residual sugar, alcohol, and total sulfur dioxide were assigned the highest coefficients, which indicate these are the metrics with the greatest contribution to the variance of our data. This seems to agree with the transformed histogram distributions displayed earlier. \n\nWhile we will use all 11 metrics in our clustering algorithm, we will first map our our data to 3 dimensions using PCA and visualize. We will return to this compressed dataset after assigning labels to all of our records to visualize the results of our clustering efforts. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_3d = PCA(n_components=3, random_state=51)\npca_3d.fit(wines_norm)\n\n#transform wines_norm using the PCA fit above\npca_samples_3d = pca_3d.transform(wines_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize our PCA transformed data\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.style.use('fivethirtyeight')\nfig = plt.figure(figsize= (10, 10))\nax = Axes3D(fig)\n\nax.scatter(pca_samples_3d[:,0], pca_samples_3d[:,1], pca_samples_3d[:,2], alpha=0.4, color= 'b')\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If viewing in jupyter notebook, feel free to rotate this plot. From visual inspection, it looks like there are two distinct groups of wine here (likely red and white). Considering that only ~65% of the data's variance is encoded in this 3D plot, we will use an analytical approach before performing our clustering task rather than assuming that 2 clusters is the most reasonable number of groups. \n\nThis analytical will be done as follows:\n\n1) We will fit our clustering algorithm using several different values of k where k is the number of clusters. We will have k range from 2 to 20. \n\n2) For each value of k, we will evaluate the clustering results using the average silhouette score. We will plot the sillhouette score against each k value and identify which number of clusters leads to the best results. \n\n3) We will then assign cluster labels to our dataset using the fitted clustering model with the optimal number of clusters k. \n\nThe sillhouette score can be roughly described as a measure of how close a sample is to members of its own cluster as compared to members of other clusters. The silhoette score ranges from -1 to 1. A score close to one indicates that a record is very close to other members of its cluster and far from members of other clusters. A score of 0 indicates that a record lies on the decision boundary between two clusters. A negative score indicates that a sample is closer to members of a cluster other than its own. By taking the average silhouette score for all records when various number of clusters are used in our clustering algorithm, we can find the optimal number of clusters that promotes cohesion within individual clusters and good seperability between the clusters. \n\nFor the clustering algorithm itself, a Gaussian Mixture Model was chosen. This was chosen for several reasons, including the fact that gaussian mixture models allow for mixed membership; GMM models assign probabilities that a given record belongs to a given cluster. This property may be useful for classifying wines which are blends of multiple types. For example, a wine may be a blend of Cabernet Sauvignon, Merlot. \n\nAnother reason for choosing a GMM is that they are more flexible with regards to cluster shapes which deviate from a hyper-spherical one. It is impossible for us to directly observe the actual cluster shapes visually since they exist in an 11-dimensional space, so it is helpful to have a clustering algorithm with such flexibility. \n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\nmax_k=20\n\nsil_scores=[]\nfor i in range(2, max_k+1):\n    clusterer = GaussianMixture(n_components=i, random_state=51, n_init=5)\n    clusterer.fit(wines_norm)\n\n    #Predict the cluster for each data point\n    preds = clusterer.predict(wines_norm)\n\n    #Find the cluster centers\n    centers = clusterer.means_\n\n    #Predict the cluster for each transformed sample data point\n    sample_preds = clusterer.predict(wines_norm)\n\n    #Calculate the mean silhouette coefficient for the number of clusters chosen\n    score = silhouette_score(wines_norm, preds)\n    sil_scores.append(score)\n    \nsil_scores= pd.Series(sil_scores, index= range(2,max_k+1))\nmax_score= sil_scores.max()\nn_clusters= sil_scores.idxmax()\nprint('Max Silhouette Score: {:.3f}'.format(max_score))\nprint('Number of clusters: {}\\n'.format(max_k))\n\nprint('First 3 Silhouette Scores')\nprint(sil_scores[0:3])\n\n#refit the model to the K with the max silhouette score\nclusterer = GaussianMixture(n_components=n_clusters, random_state=51, n_init=5)\nclusterer.fit(wines_norm)\n\n#Predict the cluster for each data point\npreds = clusterer.predict(wines_norm)\n\n#Find the cluster centers\ncenters = clusterer.means_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.figure(figsize=(10,8))\nplt.title('Silhouette Score vs. Number of Clusters', fontsize=14)\nplt.ylabel('Silhouette Score')\nplt.xlabel('Number of Clusters')\nplt.xticks(np.arange(2, max_k+1, 1))\nplt.plot(sil_scores.index.values, sil_scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the silhouette score is maximized when 3 clusters are used. Therefore, we fit the GMM using 3 clusters. We will now color each of our clusters and visualized the data in 3 dimensions again. We will append the wine type back to our dataset (red vs. white) to see how this property is distributed among our clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#append cluster labels\npca_3d_clusters= np.append(pca_samples_3d, preds.reshape(-1, 1), axis=1)\n#append wine type (red, white)\npca_3d_clusters= np.append(pca_3d_clusters, np.asarray(wines['type']).reshape(-1, 1), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nplt.style.use('fivethirtyeight')\nfig = plt.figure(figsize= (10, 10))\nax = Axes3D(fig)\n\nmapping= {0:'b', 1:'c', 2:'g'}\nmapping= {0:'r', 1:'c', 2:'b'}\ncolors= [mapping[x] for x in preds]\nax.scatter(pca_3d_clusters[:,0], pca_3d_clusters[:,1], pca_3d_clusters[:,2], alpha=0.4, color= colors, marker= 'o')\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that our clusters are pretty well separated in 3d space. Now we will plot each of these clusters individually and color them red if the wine is a red wine and yellow if the wine is a white wine."},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize']= [10, 3.5]\nfig = plt.figure()\n\nfor i in range(3):\n\n    ax=fig.add_subplot(1, 3, i+1, projection='3d')\n    \n    cluster_subset= pca_3d_clusters[pca_3d_clusters[:,3]==i]\n    type_colors= np.where(cluster_subset[:,4]=='red', 'r', 'y')\n    ax.scatter(cluster_subset[:,0], cluster_subset[:,1], cluster_subset[:,2], alpha=0.4, color= type_colors, marker= 'o')\n    \n    ax.set_title('Cluster {}'.format(i))\n    \n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_zticklabels([])\n    \nplt.tight_layout(pad=2.0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the first cluster is almost entirely composed of red wines, and the other two clusters are almost entirely composed of white wines. It appears that our clustering algorithm has recognized the distinction of red and white wines and has also recognized two distinct categories of white wines. Let's now look at some descriptive statistics of each of the clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"wines['cluster']= pca_3d_clusters[:, 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(3):\n    print('Cluster {}'.format(i))\n    subset= wines[wines['cluster']==i]\n    display(subset.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the results above, it looks like the main quality distinguishing red wines from white wines is the sulfur dioxide content. This makes sense since red wines generally contain less sulfites than white wines. As for the two categories of white wines, it appears that the main quality distinguishing the two groups of white wines is the residual sugar. Cluster 1, with its higher residual sugar content, is likely composed of sweeter wines such as Rieslings and Ice Wines. Cluster 2, with its lower residual sugar content is likely composed of dryer wines such as Chardonnay and Muscadet."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}