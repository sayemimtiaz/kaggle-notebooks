{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Introduction</p>\n\nEmotion Detection and Recognition from text is a recent field of research that is closely related to Sentiment Analysis. Sentiment Analysis aims to detect positive, neutral, or negative feelings from text, whereas Emotion Analysis aims to detect and recognize types of feelings through the expression of texts, such as anger, disgust, fear, happiness, sadness, and surprise.\n\n![emotions](https://s01.sgp1.cdn.digitaloceanspaces.com/article/107360-byxhzxvmlb-1544009839.jpg)\n\nOne of the challenges in emotion detection is the lack of a labelled emotion database to enable active innovation. But we have a datset here that is labelled with 13 emotions. Let's look at it."},{"metadata":{},"cell_type":"markdown","source":"### Reading data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv', delimiter=',')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">EDA</p>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def basic_eda(df, row_limit=5, list_elements_limit=10):\n    ### rows and columns\n    print('Info : There are {} columns in the dataset'.format(df.shape[1]))\n    print('Info : There are {} rows in the dataset'.format(df.shape[0]))\n    \n    print(\"==================================================\")\n    \n    ## data types\n    print(\"\\nData type information of different columns\")\n    dtypes_df = pd.DataFrame(df.dtypes).reset_index().rename(columns={0:'dtype', 'index':'column_name'})\n    cat_df = dtypes_df[dtypes_df['dtype']=='object']\n    num_df = dtypes_df[dtypes_df['dtype']!='object']\n    print('Info : There are {} categorical columns'.format(len(cat_df)))\n    print('Info : There are {} numerical columns'.format(len(dtypes_df)-len(cat_df)))\n    \n    if list_elements_limit >= len(cat_df):\n        print(\"Categorical columns : \", list(cat_df['column_name']))\n    else:\n        print(\"Categorical columns : \", list(cat_df['column_name'])[:list_elements_limit])\n        \n    if list_elements_limit >= len(num_df):\n        print(\"Numerical columns : \", list(num_df['column_name']))\n    else:\n        print(\"Numerical columns : \", list(num_df['column_name'])[:list_elements_limit])\n    \n    #dtypes_df['dtype'].value_counts().plot.bar()\n    display(dtypes_df.head(row_limit))\n    \n    print(\"==================================================\")\n    print(\"\\nDescription of numerical variables\")\n    \n    #### Describibg numerical columns\n    desc_df_num = df[list(num_df['column_name'])].describe().T.reset_index().rename(columns={'index':'column_name'})\n    display(desc_df_num.head(row_limit))\n    \n    print(\"==================================================\")\n    print(\"\\nDescription of categorical variables\")\n    \n    desc_df_cat = df[list(cat_df['column_name'])].describe().T.reset_index().rename(columns={'index':'column_name'})\n    display(desc_df_cat.head(row_limit))\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_eda(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quickly check for mising values\ntotal = df.isnull().sum()\ntotal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of sentiments in the data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"col = 'sentiment'\nfig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nexplode = list((np.array(list(df[col].dropna().value_counts()))/sum(list(df[col].dropna().value_counts())))[::-1])[:10]\nlabels = list(df[col].dropna().unique())[:10]\nsizes = df[col].value_counts()[:10]\n#ax.pie(sizes, explode=explode, colors=bo, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.add_artist(plt.Circle((0,0),0.6,fc='white'))\nsns.countplot(y =col, data = df, ax=ax1)\nax1.set_title(\"Count of each emotion\")\nax2.set_title(\"Percentage of each emotion\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that there are 13 different classes and some of the are having very few examples. (i.e. Anger, Boredom, Empty etc...). This is a very imbalanced dataset and it will not allow the model to converge. We'll reduce the number of classes.\nlike this"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'] = df['sentiment'].apply(lambda x : x if x in ['happiness', 'sadness', 'worry', 'neutral', 'love'] else \"other\") ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"col = 'sentiment'\nfig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nexplode = list((np.array(list(df[col].dropna().value_counts()))/sum(list(df[col].dropna().value_counts())))[::-1])[:10]\nlabels = list(df[col].dropna().unique())[:10]\nsizes = df[col].value_counts()[:10]\n#ax.pie(sizes, explode=explode, colors=bo, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.add_artist(plt.Circle((0,0),0.6,fc='white'))\nsns.countplot(y =col, data = df, ax=ax1)\nax1.set_title(\"Count of each emotion\")\nax2.set_title(\"Percentage of each emotion\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are good to go!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['char_length'] = df['content'].apply(lambda x : len(x))\ndf['token_length'] = df['content'].apply(lambda x : len(x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of character length and token length overall"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nsns.distplot(df['char_length'], ax=ax1)\nsns.distplot(df['token_length'], ax=ax2)\nax1.set_title('Number of characters in the tweet')\nax2.set_title('Number of token(words) in the tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of character length sentiment-wise [Top 5 sentiments]"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,8))\nfor sentiment in df['sentiment'].value_counts().sort_values()[-5:].index.tolist():\n    #print(sentiment)\n    sns.kdeplot(df[df['sentiment']==sentiment]['char_length'],ax=ax, label=sentiment)\nax.legend()\nax.set_title(\"Distribution of character length sentiment-wise [Top 5 sentiments]\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of token length sentiment-wise [Top 5 sentiments]"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nfor sentiment in df['sentiment'].value_counts().sort_values()[-5:].index.tolist():\n    #print(sentiment)\n    sns.kdeplot(df[df['sentiment']==sentiment]['token_length'],ax=ax, label=sentiment)\nax.legend()\nax.set_title(\"Distribution of token length sentiment-wise [Top 5 sentiments]\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the most common character and token length"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_df = df.groupby('sentiment').agg({'char_length':'mean', 'token_length':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\nax1.bar(avg_df.index, avg_df['char_length'])\nax2.bar(avg_df.index, avg_df['token_length'], color='green')\nax1.set_title('Avg number of characters')\nax2.set_title('Avg number of token(words)')\nax1.set_xticklabels(avg_df.index, rotation = 45)\nax2.set_xticklabels(avg_df.index, rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations \n- There are 13 different emotions.\n- \"Neurtal\" and \"Worry\" are the most frequent emotions in the dataset.\n- Most of the tweets have around 45 characters.\n- The most frequent token length is around 10.\n- people having \"empty\" and \"neutral\" emotion write smaller tweet."},{"metadata":{},"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Text Processing</p>\n\nWe'll do all text pre-processing in a single step using this generic function. Details of each step provided in the function details"},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install inflect","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install bs4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preparation and text-preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport inflect\nimport contractions\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### text preprocessing\n\ndef text_preprocessing_platform(df, text_col, remove_stopwords=True):\n    \n    ## Define functions for individual steps\n    # First function is used to denoise text\n    def denoise_text(text):\n        # Strip html if any. For ex. removing <html>, <p> tags\n        soup = BeautifulSoup(text, \"html.parser\")\n        text = soup.get_text()\n        # Replace contractions in the text. For ex. didn't -> did not\n        text = contractions.fix(text)\n        return text\n    \n    ## Next step is text-normalization\n    \n    # Text normalization includes many steps.\n    \n    # Each function below serves a step.\n    \n    \n    def remove_non_ascii(words):\n        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def to_lowercase(words):\n        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = word.lower()\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def remove_punctuation(words):\n        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = re.sub(r'[^\\w\\s]', '', word)\n            if new_word != '':\n                new_words.append(new_word)\n        return new_words\n    \n    \n    def replace_numbers(words):\n        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n        p = inflect.engine()\n        new_words = []\n        for word in words:\n            if word.isdigit():\n                new_word = p.number_to_words(word)\n                new_words.append(new_word)\n            else:\n                new_words.append(word)\n        return new_words\n    \n    \n    def remove_stopwords(words):\n        \"\"\"Remove stop words from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            if word not in stopwords.words('english'):\n                new_words.append(word)\n        return new_words\n    \n    \n    def stem_words(words):\n        \"\"\"Stem words in list of tokenized words\"\"\"\n        stemmer = LancasterStemmer()\n        stems = []\n        for word in words:\n            stem = stemmer.stem(word)\n            stems.append(stem)\n        return stems\n    \n    \n    def lemmatize_verbs(words):\n        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n        lemmatizer = WordNetLemmatizer()\n        lemmas = []\n        for word in words:\n            lemma = lemmatizer.lemmatize(word, pos='v')\n            lemmas.append(lemma)\n        return lemmas\n    \n    \n    ### A wrap-up function for normalization\n    def normalize_text(words, remove_stopwords):\n        words = remove_non_ascii(words)\n        words = to_lowercase(words)\n        words = remove_punctuation(words)\n        words = replace_numbers(words)\n        if remove_stopwords:\n            words = remove_stopwords(words)\n        #words = stem_words(words)\n        words = lemmatize_verbs(words)\n        return words\n    \n    # All above functions work on word tokens we need a tokenizer\n    \n    # Tokenize tweet into words\n    def tokenize(text):\n        return nltk.word_tokenize(text)\n    \n    \n    # A overall wrap-up function\n    def text_prepare(text):\n        text = denoise_text(text)\n        text = ' '.join([x for x in normalize_text(tokenize(text), remove_stopwords)])\n        return text\n    \n    # run every-step\n    df[text_col] = [text_prepare(x) for x in df[text_col]]\n    \n    \n    # return processed df\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before Text Preprocessing\")\ndisplay(df.head()[['content']])\nprocessed_df = text_preprocessing_platform(df, 'content', remove_stopwords=False)\nprint(\"After Text Preprocessing\")\ndisplay(processed_df.head()[['content']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Uni-gram Analysis\n\nLet's find out the most frequent words in the corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_word_cloud(df, sentiment):\n\n    print(\"Word cloud of most frequent words for the sentiment : {}\".format(sentiment))\n\n    temp_df = df[df['sentiment']==sentiment]\n    print(\"Number of Rows : \", len(temp_df))\n\n    corpus = ''\n    for text in temp_df.content:\n        text = str(text)\n        corpus += text\n        \n    total = 0\n    count = defaultdict(lambda: 0)\n    for word in corpus.split(\" \"):\n        total += 1\n        count[word] += 1\n        \n    top20pairs = sorted(count.items(), key=lambda kv: kv[1], reverse=True)[:20]\n    top20words = [i[0] for i in top20pairs]\n    top20freq = [i[1] for i in top20pairs]\n    \n    xs = np.arange(len(top20words))\n    width = 0.5\n\n    fig = plt.figure(figsize=(10,6))                                                               \n    ax = fig.gca()  #get current axes\n    ax.bar(xs, top20freq, width, align='center')\n\n    ax.set_xticks(xs)\n    ax.set_xticklabels(top20words)\n    plt.xticks(rotation=45)\n    \n    \n    stopwords = set(STOPWORDS)\n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(max_font_size=50, max_words=50,stopwords=stopwords, background_color=\"white\").generate(corpus)\n    plt.figure(figsize = (12, 12), facecolor = None)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_word_cloud(df, 'sadness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_word_cloud(df, 'happiness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word cloud makes sense as per the emotion"},{"metadata":{},"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Model Development</p>\n\n### Bidirectional LSTM (using GloVe Embedding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Building\nfrom keras.layers import Dropout, Dense, GRU, Embedding, LSTM, Bidirectional, TimeDistributed, Flatten\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n# Logging\nimport logging\nlogging.basicConfig(level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding textual data using GloVe Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train = text[0:len(X_train), ]\n    X_test = text[len(X_train):, ]\n    embeddings_index = {}\n    f = open(\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\", encoding='utf-8')\n    for line in f:\n        try:\n            values = line.split()\n            word = values[0]\n            try:\n                coefs = np.asarray(values[1:], dtype='float32')\n            except:\n                pass\n            embeddings_index[word] = coefs\n        except UnicodeDecodeError:\n            pass\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_index))\n    return (X_train, X_test, word_index,embeddings_index, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Bidirectional LSTM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n    # Model building\n    model = Sequential()\n    hidden_layer = 2\n    lstm_node = 32\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) != len(embedding_vector):\n                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n    model.add(Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True))\n    print(lstm_node)\n    for i in range(0,hidden_layer):\n        model.add(Bidirectional(LSTM(lstm_node,return_sequences=True, recurrent_dropout=0.5)))\n        model.add(Dropout(dropout))\n    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.5)))\n    model.add(Dropout(dropout))\n    #model.add(TimeDistributed(Dense(256)))\n    #model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(nclasses, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation Utilities"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"###Utility\n\ndef get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)/(tp+fp)\n    recall = (tp)/(tp+fn)\n    f1 = (2*(precision*recall))/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\n\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\ndef class_balance(df, target):\n  cls = df[target].value_counts()\n  cls.plot(kind='bar')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running Experiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess = True\ntext = 'content'\ntarget = 'sentiment'\nMAX_SEQUENCE_LENGTH = 60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# traindf = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv', delimiter=',')\n\n# if preprocess:\n#     traindf = text_preprocessing_platform(traindf, text)\n    \ntrain_final = processed_df[['content', 'sentiment']]\nprint(\"Train DataFrame\")\ndisplay(train_final.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding target column\nle = LabelEncoder()\ntrain_final['sentiment'] = le.fit_transform(train_final['sentiment'])\n\n## df for training and prediction\ndf = train_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train - test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[text]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nprint(\"Generating Glove Embeddings...\")\nX_train_Glove,X_test_Glove, word_index,embeddings_index, tokenizer = loadData_Tokenizer(X_train,X_test, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Training\nwith warnings.catch_warnings():\n    print(\"Building Model ...\")\n    model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 6)\n    model_RNN.summary()\n    print(\"\\n Starting Training ... \\n\")\n    history = model_RNN.fit(X_train_Glove, y_train,\n                              validation_data=(X_test_Glove, y_test),\n                              epochs=5,\n                              batch_size=128,\n                              verbose=1)\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n Plotting results ... \\n\")\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nprint(\"\\n Evaluating Model ... \\n\")\npredicted = model_RNN.predict_classes(X_test_Glove)\n#print(predicted)\nprint(metrics.classification_report(y_test, predicted))\n# print(\"\\n\")\n# logger = logging.getLogger(\"logger\")\n# result = compute_metrics(y_test, predicted)\n# for key in (result.keys()):\n#   logger.info(\"  %s = %s\", key, str(result[key]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The model is performing really bad."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}