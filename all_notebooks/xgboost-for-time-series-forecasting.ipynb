{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **XGBoost for time series forecasting**\n\nThe purpose of this notebook is to develop a XGBoost model in order to make 24 hours ahead predictions of the electricity price in the Spanish electricity market for the 2020 timespan. First section is focused on explaining how XGBoost works, and then I proceed with the implementation of the model in python. This notebook asumes that you understand the fundamentals behind decision trees, gradient boosting and time series. ","metadata":{}},{"cell_type":"markdown","source":"# **Table of Contents** \n\n- [1. XGBoost theoretical explanation](#1)\n    - [1.1. XGBoost Intuitive explanation](#1.1)\n    - [1.2. Mathematics behind XGBoost ](#1.2)\n- [2. XGBoost in practice ](#2)\n    - [2.1. XGBoost model](#2.1)\n","metadata":{}},{"cell_type":"markdown","source":"# **1. XGBoost theoretical explanation** <a class=\"anchor\" id=\"1\"></a>\n\nIn this section I will explain first the intuitive idea of how XGBoost works and then the mathematics behind it. If you want to complement this information with other sources, here are the sources that helped me the most to understand this algorithm:\n\n- StatQuest with Josh Stammer:  https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\n\nThis is a Youtube Channel (that you will problably know if you study statistics and machine learning) where you can find lots of videos about decision trees, gradient boosting and XGBoost. It is ideal if you want to have and intuitive idea of how XGBoost works.\n\n- XGBoost: A Scalable Tree Boosting System. https://arxiv.org/pdf/1603.02754.pdf\n\nThis is the original XGBoost paper, where you can delve into the nature of the algorithm.\n\n- XGBoost: A Scalable Tree Boosting System (presentation). https://www.youtube.com/watch?v=Vly8xGnNiWs&ab_channel=RealDataScienceUSA%28formerlyDataScience.LA%29\n\nHere you can find a presentation of XGBoost by its author, Tianqi Chen. \n\n- XGBoost documentation: https://xgboost.readthedocs.io/en/latest/index.html\n\nOf course, the official documentation, indispensable in order to understand the code and tune hyperparameters.\n\n## **1.1 XGBoost Intuitive explanation** <a class=\"anchor\" id=\"1.1\"></a>\n\nAs in all supervised learning problems, the goal here is to find the right model parameters that minimize a loss function. A loss function compares predicted values with real values. In addittion, it usually contains a regularization term in order to deal with overfitting. The simplified loss function for XGBoost is the following:\n$$\\sum^{n}_{i=1} L(y_i, p_i) + \\frac{1}{2} \\lambda \\omega^2$$","metadata":{}},{"cell_type":"markdown","source":"Where $\\omega$ is the output value for each leaf and $\\lambda$ a regularization parameter.\n\nIn order to make predictions, XGBoost starts with a default prediction of 0.5. Then, the first tree starts with a single leaf, and all of the residuals for the 0.5 prediction go to that leaf. With this, we calculate the *similarity score* for that leaf. For now, we will define the *similarity score* as a number that measures how well the leaf does its job.\nThat being said, the question now is whether or not we can do a better job clustering similar residuals if we split them into two groups. In order to answer this question, we split the leaf (which becomes a node) into 2 leaves by a treshold (computed as in decision trees algorithm), and we compute the gain of splitting the residuales into two groups:\n$$Gain = Left_{similarity} + Rigth_{similarity} - Root_{similarity}$$\n\nIf the gain value is lower than a hyperparameter called gamma ($\\gamma$), we remove the branch. If not, we continue creating new leaves. As we can see, we will prune an XGBoost tree based on its Gain values, and therefore based on the similarity scores.\n\n## **1.2 Mathematics behind XGBoost** <a class=\"anchor\" id=\"1.2\"></a>\n\nIn order to minimize the loss function, XGBoost uses the second order Taylor approximation to rewrite the loss function as follows: \n\n$$ L(y, p_i + \\omega) \\approx L(y, p_i) + \\frac{\\partial L(y,p_i)}{\\partial p_i} \\omega + \\frac{1}{2} \\frac{\\partial^2 L(y,p_i)}{\\partial p_i^2}\\omega^2 $$\n\n\nFirst term corresponds with the loss function from previous prediction, second term corresponds with the first derivative of the loss function from previous prediction (the gradient) and the third term corresponds with the second derivative of the loss function (the hessian). Thus, we can rewrite: \n\n$$ L(y, p_i + \\omega) \\approx L(y, p_i) + g \\omega + \\frac{1}{2} h \\omega^2 $$\n\n\nwhere $g$ is the gradient and $h$ the hessian.\nKnowing this, if we add the regularization term, the problem to solve is the following:\n\n$$ \\frac{\\partial (g_1 + g_2 + \\dots + g_n) \\omega}{\\partial \\omega}  + \\frac{1}{2} (h_1 + h_2 + \\dots + h_n +\\lambda) \\omega^2 = 0$$\n\n\nAnd the solution turns to be:\n\n$$ \\omega = \\frac{- (g_1 + g_2 + \\dots + g_n)}{h_1 + h_2 + \\dots + h_n + \\lambda} $$\n\n \n \n In order to compute the similarity score, we start with the following approximation of the loss function, which is the result of using the second order Taylor approximation and removing constant terms: \n \n $$(g_1 + g_2 + \\dots + g_n)\\omega + \\frac{1}{2}(h_1 + h_2 + \\dots + h_n + \\lambda)\\omega^2$$\n \n If we multiply by -1, replace $\\omega$ by its expression and simplify, we have: \n \n $$ \\frac{1}{2}\\frac{(g_1 + g_2 + \\dots + g_n)^2}{(h_1 + h_2 + \\dots + h_n + \\lambda)} $$\n \n \n $\\frac{1}{2}$ is omitted as similarity score is a relative measure, thus the final expression is:\n \n $$Similarity \\: Score = \\frac{(g_1 + g_2 + \\dots + g_n)^2}{(h_1 + h_2 + \\dots + h_n + \\lambda)}$$\n \n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **2. XGBoost in practice** <a class=\"anchor\" id=\"2\"></a>\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:43:20.135737Z","iopub.execute_input":"2021-07-25T10:43:20.136114Z","iopub.status.idle":"2021-07-25T10:43:20.143921Z","shell.execute_reply.started":"2021-07-25T10:43:20.136066Z","shell.execute_reply":"2021-07-25T10:43:20.142903Z"}}},{"cell_type":"markdown","source":"In order to develop the code, we first need to import the following libraries:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_acf #acf\nfrom statsmodels.graphics.tsaplots import plot_pacf #pacf\n\nimport xgboost as xgb # Library for implementing XGBoost algorithm\nfrom matplotlib import pyplot # Data visualization\nfrom sklearn.model_selection import GridSearchCV # For hyperparameters optimization\nfrom sklearn.metrics import mean_absolute_error\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:23:08.814633Z","iopub.execute_input":"2021-08-07T15:23:08.814992Z","iopub.status.idle":"2021-08-07T15:23:10.220549Z","shell.execute_reply.started":"2021-08-07T15:23:08.814963Z","shell.execute_reply":"2021-08-07T15:23:10.219453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we create a function that splits the datasets in a training set and a test set. We will use data from 2014 to 2018 as training set, data from 2019 as validation set (in order to optimize hyperparameters), and data from 2020 as test set. In this function, we have to specify the data we want to split and the year we want to use as test set (training set will be all of the previous data). It is worth mentioning that with time series prediction we cannot use cross validation.","metadata":{}},{"cell_type":"code","source":"# Split dataset into train and test data\ndef split_dataset(data,test_year):\n    if test_year == 2019:\n        train, test = data[:-17544], data[-17712:-8784]\n    elif test_year == 2020:\n        train, test = data[:-8784], data[-8952:]\n    return np.array(train), np.array(test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:23:16.87328Z","iopub.execute_input":"2021-08-07T15:23:16.87366Z","iopub.status.idle":"2021-08-07T15:23:16.88026Z","shell.execute_reply.started":"2021-08-07T15:23:16.873626Z","shell.execute_reply":"2021-08-07T15:23:16.878985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After doing this, we need to transform the \"time series problem\" to a \"supervised learning problem\" (labeled data). This is key in order to predict time series with supervised learning algorithms. In order to do this, the lags selected to predict (example: $p_{t-24}$ is useful to predict $p_t$) will be the input (what we call x_train or x_test), and the value we want to predict will be what we call y_train and y_test. In the following function we need to specify the data we are going to use and the lags selected (array).\n\nWe do this with the following function:","metadata":{}},{"cell_type":"code","source":"# Time series data to supervised learning problem\ndef to_supervised(data, lags):\n    X = list()\n    y = np.array([])\n    j = lags[-1]\n    i = 0\n    for _ in range(len(data)-lags[-1]):\n        x_data = data[i:j]\n        x_input = np.array([])\n        for k in lags:\n            x_input = np.append(x_input, x_data[-k])\n        X.append(x_input)\n        y = np.append(y, data[j:j+1,0])\n        j += 1\n        i += 1\n\n    return np.array(X), y","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:23:19.402326Z","iopub.execute_input":"2021-08-07T15:23:19.402682Z","iopub.status.idle":"2021-08-07T15:23:19.410087Z","shell.execute_reply.started":"2021-08-07T15:23:19.402651Z","shell.execute_reply":"2021-08-07T15:23:19.409141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" This two functions are enough to prepare the data for the supervised learning scheme. Next step is reading the data (note: precio is the spanish word for price): ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/spanishelectricitymarket/precios_14_20.txt\", sep=\",\", index_col=0)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:23:22.078483Z","iopub.execute_input":"2021-08-07T15:23:22.078872Z","iopub.status.idle":"2021-08-07T15:23:22.201188Z","shell.execute_reply.started":"2021-08-07T15:23:22.078838Z","shell.execute_reply":"2021-08-07T15:23:22.20021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the lags selection, we proceed with multiples of 24 until reaching the data corresponding with the same hour seven days before (which is 7x24=168). These values are the most significant when we look at the ACF and PACF: ","metadata":{}},{"cell_type":"code","source":"plot_acf(df,lags=(200), alpha=None)\nplot_pacf(df,lags=(200), alpha=None)\nlags = np.array([24,48,72,96,120,144,168])","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:25:09.83603Z","iopub.execute_input":"2021-08-07T15:25:09.8364Z","iopub.status.idle":"2021-08-07T15:25:13.329916Z","shell.execute_reply.started":"2021-08-07T15:25:09.836364Z","shell.execute_reply":"2021-08-07T15:25:13.328957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we transform the data to a supervised learning problem:","metadata":{}},{"cell_type":"code","source":"train, test =split_dataset(df, 2019)\n\ntrain_x, train_y = to_supervised(train, lags)\n\ntest_x, test_y = to_supervised(test, lags)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:08:51.296571Z","iopub.execute_input":"2021-07-26T19:08:51.296992Z","iopub.status.idle":"2021-07-26T19:08:55.331186Z","shell.execute_reply.started":"2021-07-26T19:08:51.29696Z","shell.execute_reply":"2021-07-26T19:08:55.329995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are also going to use exogenous variables. We select wind energy and demand, as they are the most correlated exogenous variables with the electricity price.","metadata":{}},{"cell_type":"code","source":"\n# Exogenous variables\n\n# wind energy\ndf2 = pd.read_csv(\"../input/spanishelectricitymarket/eolica_14_20.csv\", sep=\",\", index_col=0)\n# demand\ndf3 = pd.read_csv(\"../input/spanishelectricitymarket/demanda_14_20.csv\", sep=\",\", index_col=0)\n\n\ntrain_wind, test_wind = split_dataset(df2, 2019)\ntrain_demand, test_demand = split_dataset(df3, 2019)\n\ntrain_x_wind = train_wind[144:-24]\ntrain_x_demand = train_demand[144:-24]\n\ntest_x_wind = test_wind[144:-24]\ntest_x_demand = test_demand[144:-24]\n\ntrain_x_final = np.append(train_x, train_x_wind, axis=1)\ntrain_x_final = np.append(train_x_final, train_x_demand, axis=1)\n\ntest_x_final = np.append(test_x, test_x_wind, axis=1)\ntest_x_final = np.append(test_x_final, test_x_demand, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:08:55.332915Z","iopub.execute_input":"2021-07-26T19:08:55.333206Z","iopub.status.idle":"2021-07-26T19:08:55.632172Z","shell.execute_reply.started":"2021-07-26T19:08:55.33318Z","shell.execute_reply":"2021-07-26T19:08:55.631376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, for the exogenous variables we only use lag 24, i.e., we pick data from $p_{t-24}$ to predict the value in $p_{t}$)","metadata":{}},{"cell_type":"markdown","source":"## **2.1 XGBoost model** <a class=\"anchor\" id=\"2.1\"></a>\n\nWe need to create and XGBoost model for regression. In order to optimize hyperparameters, we will use GridSearchCV from Sklearn. The following code is commented as it takes a lot of time to execute.","metadata":{}},{"cell_type":"code","source":"\n## FIRST ROUND\n## select the hyperparameters we want to tune and the values\n# param_grid = {\n#     'learning_rate': [0.1, 0.01, 0.05],\n#     'min_split_loss': [1000, 2000, 500], # gamma\n#     'reg_lambda': [0, 0.5, 10.0], # lambda\n#     'max_depth': [8, 9, 10]\n# }\n# \n## build XGBoost for regression \n# optimal_params = GridSearchCV(\n#     estimator = xgb.XGBRegressor(objective = 'reg:squarederror',\n#                          eval_metric='mae', n_estimators = 300),\n#     param_grid = param_grid,\n#     scoring = 'neg_mean_absolute_error', \n#     verbose = 2\n# )\n# \n## fit the model\n# optimal_params.fit(train_x_final,\n#                    train_y,\n#                    early_stopping_rounds=10,\n#                    eval_set=[(test_x_final, test_y)],\n#                    verbose=False)\n# \n# print(optimal_params.best_params_)\n## {'learning_rate': 0.05, 'max_depth': 8, 'min_split_loss': 2000, 'reg_lambda': 10.0}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:08:55.633431Z","iopub.execute_input":"2021-07-26T19:08:55.634025Z","iopub.status.idle":"2021-07-26T19:08:55.638665Z","shell.execute_reply.started":"2021-07-26T19:08:55.633981Z","shell.execute_reply":"2021-07-26T19:08:55.637694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## SECOND ROUND\n# param_grid = {\n#     'min_split_loss': [2000, 3000, 5000],\n#     'reg_lambda': [10, 30, 100],\n#     'max_depth': [8, 7, 6]\n# }\n# optimal_params = GridSearchCV(\n#     estimator = xgb.XGBRegressor(objective = 'reg:squarederror',\n#                          eval_metric='mae', n_estimators = 300, learning_rate=0.05),\n#     param_grid = param_grid,\n#     scoring = 'neg_mean_absolute_error',\n#     verbose = 2\n# )\n# optimal_params.fit(train_x_final,\n#                    train_y,\n#                    early_stopping_rounds=10,\n#                    eval_set=[(test_x_final, test_y)],\n#                    verbose=False)\n# \n# print(optimal_params.best_params_)\n# \n## {'max_depth': 8, 'min_split_loss': 2000, 'reg_lambda': 100}","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:08:55.640124Z","iopub.execute_input":"2021-07-26T19:08:55.640715Z","iopub.status.idle":"2021-07-26T19:08:55.652106Z","shell.execute_reply.started":"2021-07-26T19:08:55.640673Z","shell.execute_reply":"2021-07-26T19:08:55.651252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we now have the values for the hyperparameters, we can set values from 2020 as the test set in order to use them to define the final model.","metadata":{}},{"cell_type":"code","source":"# Set 2020 as test set for price\ntrain, test = split_dataset(df, 2020)\n\ntrain_x, train_y = to_supervised(train, lags)\ntest_x, test_y = to_supervised(test, lags)\n\n# Set 2020 as test set for wind and demand\ntrain_wind, test_wind = split_dataset(df2, 2020)\ntrain_demand, test_demand = split_dataset(df3, 2020)\n\ntrain_x_wind = train_wind[144:-24]\ntrain_x_demand = train_demand[144:-24]\n\ntest_x_wind = test_wind[144:-24]\ntest_x_demand = test_demand[144:-24]\n\n# Append all variables\ntrain_x_final = np.append(train_x, train_x_wind, axis=1)\ntrain_x_final = np.append(train_x_final, train_x_demand, axis=1)\n\ntest_x_final = np.append(test_x, test_x_wind, axis=1)\ntest_x_final = np.append(test_x_final, test_x_demand, axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:08:55.653436Z","iopub.execute_input":"2021-07-26T19:08:55.654044Z","iopub.status.idle":"2021-07-26T19:09:00.533615Z","shell.execute_reply.started":"2021-07-26T19:08:55.654001Z","shell.execute_reply":"2021-07-26T19:09:00.53279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once all this is done, when can now define the final model.","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective = 'reg:squarederror',\n                         eval_metric='mae',\n                         learning_rate = 0.05,\n                         min_split_loss = 2000,  # gamma\n                         reg_lambda = 100,  # lambda\n                         max_depth = 8,\n                         n_estimators = 300)\n\n\neval_set = [(train_x_final, train_y), (test_x_final, test_y)] \nmodel.fit(train_x_final, train_y, eval_metric=\"mae\", eval_set=eval_set, verbose=True, early_stopping_rounds=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:09:00.536131Z","iopub.execute_input":"2021-07-26T19:09:00.536564Z","iopub.status.idle":"2021-07-26T19:09:11.947481Z","shell.execute_reply.started":"2021-07-26T19:09:00.536518Z","shell.execute_reply":"2021-07-26T19:09:11.946599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to compare this results with other state-of-the-art models you can check  my end-of-degree project, where I have developed ARIMA, transfer function models, Recurrent Neural Networks and Convolutional Neural Networks for this same problem (code and data in: https://github.com/Adricarpin/TFG.git). I have also built a naive model in order to see if these models were skillful. If we compare XGBoost results with the results from these models, we find that on the one hand XGBoost outperforms the naive model, thus it can be considered as skillful, but on the other hand it doesn't outperform ARIMA and NN models. ","metadata":{}},{"cell_type":"markdown","source":"We can compare train errors with test errors in order to know if the model is overfitted or underfitted. We do this with the following graph:","metadata":{}},{"cell_type":"code","source":"results = model.evals_result()\n\nepochs = len(results['validation_0']['mae'])\nx_axis = range(0, epochs)\n\n\nfig, ax = pyplot.subplots()\nax.set_ylim([0, 40])\nax.plot(x_axis, results['validation_0']['mae'], label='Train')\nax.plot(x_axis, results['validation_1']['mae'], label='Test')\nax.legend()\npyplot.ylabel('mae')\npyplot.title('XGBoost mae')\npyplot.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:09:11.949078Z","iopub.execute_input":"2021-07-26T19:09:11.949585Z","iopub.status.idle":"2021-07-26T19:09:12.130745Z","shell.execute_reply.started":"2021-07-26T19:09:11.949554Z","shell.execute_reply":"2021-07-26T19:09:12.12963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also have a look at the trees structure. The following code depicts the structure for the first tree.","metadata":{}},{"cell_type":"code","source":"\nclf_xgb = xgb.XGBRegressor(objective = 'reg:squarederror',\n                         eval_metric='mae',\n                         learning_rate = 0.05,\n                         min_split_loss = 2000,\n                         reg_lambda = 100,\n                         max_depth = 8,\n                         n_estimators = 1)\n\nclf_xgb.fit(train_x_final, train_y)\n\nnode_params = {'shape': 'box',\n               'style': 'filled, rounded',\n               'fillcolor': '#78cbe'}\nleaf_params = {'shape': 'box',\n               'style': 'filled',\n               'fillcolor': '#e48038'}\n\ngraph_data = xgb.to_graphviz(clf_xgb, num_trees=0, size=\"10,10\",\n                condition_node_params=node_params,\n                leaf_node_params=leaf_params)\n\ngraph_data.view(filename='xgboost_tree')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:09:12.132121Z","iopub.execute_input":"2021-07-26T19:09:12.132417Z","iopub.status.idle":"2021-07-26T19:09:12.33417Z","shell.execute_reply.started":"2021-07-26T19:09:12.132389Z","shell.execute_reply":"2021-07-26T19:09:12.332639Z"},"trusted":true},"execution_count":null,"outputs":[]}]}