{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tables of Content:\n\n**1. [Libraries and Dataset](#Libraries_data)** <br>\n    - Importing libraries\n    - Importing dataset\n**2. [Define some basic Functions](#Basic_functions)** <br>\n    - Change string to list\n    - Remove constant columns\n    - Remove datetime columns\n    - Check categorical columns' unique values\n    - Replace blank by NA\n    - Check y\n    - Check print step\n    - X vriable\n    - Check breaks list\n    - Check special values\n    - Overview of dataset\n**3. [Functions for Explotary Data Analysis](#EDA)** <br>\n    - Plot target distribution(interactive)\n    - Plot distribution of a variable(interactive)\n    - Plot distribution of a numerical variable by target(non-interactive)\n    - Box plot of a continous variable vs. a catergorical variable(non-interactive)\n    - Box plot of a continous variable vs. a catergorical variable(interactive)\n    - Violin plot of a continous variable vs. a catergorical variable(non-interactive)\n    - Violin plot of a continous variable vs. a catergorical variable(interactive)\n    - Count plot of a categorical variable by target(interactive)\n    - Check Correlation\n    - Check Multicollinearity\n**4. [Functions for Information Entropy&Gini Impurity](#Entropy_Gini)** <br>\n    - Information Entropy Given a Dataframe\n    - Information Entropy Given x and y\n    - Information Entropy based on good/bad\n    - Impurity Gini Given a Dataframe\n    - Impurity Gini Given x and y\n    - Impurity Gini based on good/bad\n**5. [Functions for Information Value](#Info_value)** <br>\n    - Information Value Given a Dataframe\n    - Information Value Given x and y\n    - Information Value based on good/bad\n    - Information Value of each Bin\n    - WOE of each Bin\n**6. [Functions for One Hot Encoding](#One_Hot_Encode)** <br>\n    - One Hot Encoding\n**7. [Functions for Model Performance](#Performance)** <br>\n    - KS&Lift Dataframe\n    - Plotting KS Curve\n    - Plotting Lift Curve\n    - Calculating ROC\n    - Plotting ROC Curve\n    - Plotting Precision-Recall Curve\n    - plotting F1\n    - Show Performance Evaluation Metrics(KS/ROC/Lift/PR)\n    - Population Stability Index(PSI)\n    - Show Score Bin Scorting\n    - Score Distribution\n**8.  [Functions for Scorecard](#scorecard)** <br>\n    - Coefficients in scorecard\n    - Creating a scorecard\n    - Apply to scorecard\n**9.  [Functions for Splitting Dataset](#split_dataset)** <br>\n**10. [Functions for Variable Filter](#variable_filter)** <br>\n**11. [Functions for WOE Binning](#WOE)** <br>\n    - Converting vector (breaks & special_values) to dataframe\n    - Add missing value to special value\n    - Count number of good or bad in y\n    - Split dataframe into binning dataframe with special values and datafram without speical_values\n    - Check empty bins for unnumeric variable\n    - Binning based on given breaks\n    - Pretty breakpoints\n    - Return initial binning\n    - Add 1 best break for tree-like binning\n    - Return tree-like binning\n    - Return chimerge binning\n    - Format binning output\n    - WOE Binning for only two columns\n    - Get bins breaklist\n    - Generate Optimal Binning\n    - Transform original value to woe values for one variable\n    - Transform original value to woe values for a dataframe\n    - Plot binning of one variable\n    - WOE Binning Visualization\n    - Print basic information in woebinning adjusting process\n    - Plot adjusted binning in woebinning adjustment process\n    - WOE Binning Adjustment\n**12. [Functions for Over Sampling](#Oversampling)** <br>\n    - SMOTE(Synthetic Minority Over-sampling Technique)\n**13. [Example: Generate scorecard on Germancredit dataset](#Example)** <br>\n    - Load germancredit data\n    - Overview of data\n    - Plot target distribution(interactive)\n    - Plot distribution of a variable(Credit amount)\n    - Box plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing)\n    - Violin plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing)\n    - Count plot of a categorical variable by target(Housing)\n    - Check Multicollinearity\n    - Check Correlation\n    - Filter variable via missing rate, iv, identical value rate and change target to 1/0 \n    - Breaking dt into train and test\n    - Generate WOE binning\n    - Binning adjustment(manually)\n    - Converting train and test into woe values\n    - Oversampling by SMOTE\n    - Logistic regression model\n    - Predict probability based on the LR model\n    - Plot model performance(KS&ROC)\n    - Generate scorecard\n    - Show score\n    - Plot PSI of score between train and test"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Libraries_data\"></a> <br>\n# **1. Libraries and Dataset** \n- Importing libraries\n- Importing dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## Importing libraries ##\nimport numpy as np \nimport pandas as pd \nimport scipy as sp\nimport statsmodels.api as sm\nimport warnings\nimport re\nimport time\nimport platform\nimport sklearn\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport multiprocessing as mp\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport os\nfrom scipy import stats\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas.api.types import is_string_dtype\nfrom collections import OrderedDict\nfrom collections import Counter\nfrom patsy import dmatrices\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom imblearn.over_sampling import BorderlineSMOTE  \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n## Importing dataset ##\ndata_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Basic_functions\"></a> <br>\n# **2. Define some basic Functions**\n- Change string to list\n- Remove constant columns\n- Remove datetime columns\n- Check categorical columns' unique values\n- Replace blank by NA\n- Check y\n- Check print step\n- X vriable\n- Check breaks list\n- Check special values\n- Overview of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Change string to list ##\ndef str_to_list(x):\n    if x is not None and isinstance(x, str):\n        x = [x]\n    return x\n    \n## Remove constant columns ##\ndef check_const_cols(dat):\n    # remove only 1 unique values variable \n    unique1_cols = [i for i in list(dat) if len(dat[i].unique())==1]\n    if len(unique1_cols) > 0:\n        warnings.warn(\"There are {} columns have only one unique values, which are removed from input dataset. \\n (ColumnNames: {})\".format(len(unique1_cols), ', '.join(unique1_cols)))\n        dat=dat.drop(unique1_cols, axis=1)\n    return dat\n\n## Remove datetime columns ##\ndef check_datetime_cols(dat):\n    datetime_cols = dat.apply(pd.to_numeric,errors='ignore').select_dtypes(object).apply(pd.to_datetime,errors='ignore').select_dtypes('datetime64').columns.tolist()\n    #datetime_cols = dat_time.dtypes[dat_time.dtypes == 'datetime64[ns]'].index.tolist()\n    if len(datetime_cols) > 0:\n        warnings.warn(\"There are {} date/time type columns are removed from input dataset. \\n (ColumnNames: {})\".format(len(datetime_cols), ', '.join(datetime_cols)))\n        dat=dat.drop(datetime_cols, axis=1)\n    return dat\n\n## Check categorical columns' unique values ##\ndef check_cateCols_uniqueValues(dat, var_skip = None):\n    # character columns with too many unique values\n    char_cols = [i for i in list(dat) if not is_numeric_dtype(dat[i])]\n    if var_skip is not None: \n        char_cols = list(set(char_cols) - set(str_to_list(var_skip)))\n    char_cols_too_many_unique = [i for i in char_cols if len(dat[i].unique()) >= 50]\n    if len(char_cols_too_many_unique) > 0:\n        print('>>> There are {} variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \\n{}'.format(len(char_cols_too_many_unique), ', '.join(char_cols_too_many_unique)))\n        print('>>> Continue the binning process?')\n        print('1: yes \\n2: no')\n        cont = int(input(\"Selection: \"))\n        while cont not in [1, 2]:\n            cont = int(input(\"Selection: \"))\n        if cont == 2:\n            raise SystemExit(0)\n    return None\n\n\n## Replace blank by NA ##\ndef rep_blank_na(dat): # cant replace blank string in categorical value with nan\n    # remove duplicated index\n    if dat.index.duplicated().any():\n        dat = dat.reset_index(drop = True)\n        warnings.warn('There are duplicated index in dataset. The index has been reseted.')\n    \n    blank_cols = [i for i in list(dat) if dat[i].astype(str).str.findall(r'^\\s*$').apply(lambda x:0 if len(x)==0 else 1).sum()>0]\n    if len(blank_cols) > 0:\n        warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(len(blank_cols), ', '.join(blank_cols)))\n       # dat[dat == [' ','']] = np.nan\n       # dat2 = dat.apply(lambda x: x.str.strip()).replace(r'^\\s*$', np.nan, regex=True)\n        dat.replace(r'^\\s*$', np.nan, regex=True)\n    return dat\n\n\n## Check y ##\ndef check_y(dat, y, positive):\n    positive = str(positive)\n    # ncol of dt\n    if not isinstance(dat, pd.DataFrame):\n        raise Exception(\"Incorrect inputs; dat should be a DataFrame.\")\n    elif dat.shape[1] <= 1:\n        raise Exception(\"Incorrect inputs; dat should be a DataFrame with at least two columns.\")\n    \n    # y ------\n    y = str_to_list(y)\n    # length of y == 1\n    if len(y) != 1:\n        raise Exception(\"Incorrect inputs; the length of y should be one\")\n    \n    y = y[0]\n    # y not in dat.columns\n    if y not in dat.columns: \n        raise Exception(\"Incorrect inputs; there is no \\'{}\\' column in dat.\".format(y))\n    \n    # remove na in y\n    if dat[y].isnull().any():\n        warnings.warn(\"There are NaNs in \\'{}\\' column. The rows with NaN in \\'{}\\' were removed from dat.\".format(y,y))\n        dat = dat.dropna(subset=[y])\n        # dat = dat[pd.notna(dat[y])]\n    \n    # numeric y to int\n    if is_numeric_dtype(dat[y]):\n        dat.loc[:,y] = dat[y].apply(lambda x: x if pd.isnull(x) else int(x)) #dat[y].astype(int)\n    \n    # length of unique values in y\n    unique_y = np.unique(dat[y].values)\n    if len(unique_y) == 2:\n        # if [v not in [0,1] for v in unique_y] == [True, True]:\n        if True in [bool(re.search(positive, str(v))) for v in unique_y]:\n            y1 = dat[y]\n            y2 = dat[y].apply(lambda x: 1 if str(x) in re.split('\\|', positive) else 0)\n            if (y1 != y2).any():\n                dat.loc[:,y] = y2#dat[y] = y2\n                warnings.warn(\"The positive value in \\\"{}\\\" was replaced by 1 and negative value by 0.\".format(y))\n        else:\n            raise Exception(\"Incorrect inputs; the positive value in \\\"{}\\\" is not specified\".format(y))\n    else:\n        raise Exception(\"Incorrect inputs; the length of unique values in y column \\'{}\\' != 2.\".format(y))\n    \n    return dat\n\n\n## Check print_step ##\ndef check_print_step(print_step):\n    if not isinstance(print_step, (int, float)) or print_step<0:\n        warnings.warn(\"Incorrect inputs; print_step should be a non-negative integer. It was set to 1.\")\n        print_step = 1\n    return print_step\n\n\n## X variable ##\ndef x_variable(dat, y, x, var_skip=None):\n    y = str_to_list(y)\n    if var_skip is not None: y = y + str_to_list(var_skip)\n    x_all = list(set(dat.columns) - set(y))\n    \n    if x is None:\n        x = x_all\n    else:\n        x = str_to_list(x)\n            \n        if any([i in list(x_all) for i in x]) is False:\n            x = x_all\n        else:\n            x_notin_xall = set(x).difference(x_all)\n            if len(x_notin_xall) > 0:\n                warnings.warn(\"Incorrect inputs; there are {} x variables are not exist in input data, which are removed from x. \\n({})\".format(len(x_notin_xall), ', '.join(x_notin_xall)))\n                x = set(x).intersection(x_all)\n            \n    return list(x)\n\n\n## Check breaks_list ##\ndef check_breaks_list(breaks_list, xs):\n    if breaks_list is not None:\n        # is string\n        if isinstance(breaks_list, str):\n            breaks_list = eval(breaks_list)\n        # is not dict\n        if not isinstance(breaks_list, dict):\n            raise Exception(\"Incorrect inputs; breaks_list should be a dict.\")\n    return breaks_list\n\n\n## Check special_values ##\ndef check_special_values(special_values, xs):\n    if special_values is not None:\n        # # is string\n        # if isinstance(special_values, str):\n        #     special_values = eval(special_values)\n        if isinstance(special_values, list):\n            warnings.warn(\"The special_values should be a dict. Make sure special values are exactly the same in all variables if special_values is a list.\")\n            sv_dict = {}\n            for i in xs:\n                sv_dict[i] = special_values\n            special_values = sv_dict\n        elif not isinstance(special_values, dict): \n            raise Exception(\"Incorrect inputs; special_values should be a list or dict.\")\n    return special_values\n\n\n## Overview of Dataset ##\ndef iv_gb(x, y):\n    # good bad func\n    def goodbad(df):\n        names = {'good': (df['y']=='good').sum(),'bad': (df['y']=='bad').sum()}\n        return pd.Series(names)\n    # iv calculation\n    iv_total = pd.DataFrame({'x':x.astype('str'),'y':y}) \\\n      .fillna('missing') \\\n      .groupby('x') \\\n      .apply(goodbad) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(iv = lambda x: (x.DistrBad-x.DistrGood)*np.log(x.DistrBad/x.DistrGood)) \\\n      .iv.sum()\n    # return iv\n    return iv_total\ndef iv_gb01(x, y):\n    # good bad func\n    def goodbad(df):\n        names = {'good': (df['y']==0).sum(),'bad': (df['y']==1).sum()}\n        return pd.Series(names)\n    # iv calculation\n    iv_total = pd.DataFrame({'x':x.astype('str'),'y':y}) \\\n      .fillna('missing') \\\n      .groupby('x') \\\n      .apply(goodbad) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(iv = lambda x: (x.DistrBad-x.DistrGood)*np.log(x.DistrBad/x.DistrGood)) \\\n      .iv.sum()\n    # return iv\n    return iv_total\n\ndef overview_data_bf(dat):\n    print(f\"Dataset Shape: {dat.shape}\")\n    overview = pd.DataFrame(dat.dtypes,columns=['dtypes'])\n    overview = overview.reset_index()\n    overview['Name'] = overview['index']\n    overview = overview[['Name','dtypes']]\n    overview['Missing'] = dat.isnull().sum().values   \n    overview['%Missing'] = dat.isnull().sum().values/dat.shape[0]\n    overview['%Missing'] = overview['%Missing'].apply(lambda x: format(x, '.2%'))\n    overview['Uniques'] = dat.nunique().values\n    overview['%Unique'] = dat.nunique().values/dat.shape[0]\n    overview['%Unique'] = overview['%Unique'].apply(lambda x: format(x, '.2%'))\n    overview['First Value'] = dat.loc[0].values\n    overview['Second Value'] = dat.loc[1].values\n    overview['Third Value'] = dat.loc[2].values\n    for name in overview['Name'].value_counts().index:\n        overview.loc[overview['Name'] == name, 'Entropy'] = round(stats.entropy(dat[name].value_counts(normalize=True), base=2),2) \n    for name in overview['Name'].value_counts().index:\n        overview.loc[overview['Name'] == name, 'iv'] = round(iv_gb(dat[name],dat.iloc[:,-1]),2)\n    return overview\n\ndef overview_data_af(dat):\n    print(f\"Dataset Shape: {dat.shape}\")\n    overview = pd.DataFrame(dat.dtypes,columns=['dtypes'])\n    overview = overview.reset_index()\n    overview['Name'] = overview['index']\n    overview = overview[['Name','dtypes']]\n    overview['Missing'] = dat.isnull().sum().values   \n    overview['%Missing'] = dat.isnull().sum().values/dat.shape[0]\n    overview['%Missing'] = overview['%Missing'].apply(lambda x: format(x, '.2%'))\n    overview['Uniques'] = dat.nunique().values\n    overview['%Unique'] = dat.nunique().values/dat.shape[0]\n    overview['%Unique'] = overview['%Unique'].apply(lambda x: format(x, '.2%'))\n    overview['First Value'] = dat.loc[0].values\n    overview['Second Value'] = dat.loc[1].values\n    overview['Third Value'] = dat.loc[2].values\n    for name in overview['Name'].value_counts().index:\n        overview.loc[overview['Name'] == name, 'Entropy'] = round(stats.entropy(dat[name].value_counts(normalize=True), base=2),2) \n    for name in overview['Name'].value_counts().index:\n        overview.loc[overview['Name'] == name, 'iv'] = round(iv_gb01(dat[name],dat.iloc[:,-1]),2)\n    return overview\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a> <br>\n# **3. Functions for Explotary Data Analysis**\n - Plot target distribution(interactive)\n - Plot distribution of a variable(interactive)\n - Plot distribution of a numerical variable by target(non-interactive)\n - Box plot of a continous variable vs. a catergorical variable(non-interactive)\n - Box plot of a continous variable vs. a catergorical variable(interactive)\n - Violin plot of a continous variable vs. a catergorical variable(non-interactive)\n - Violin plot of a continous variable vs. a catergorical variable(interactive)\n - Count plot of a categorical variable by target(interactive)\n - Correlation Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot target distribution(interactive) ##\ndef plot_target_dist_interact(df,target_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Plot the distribution of target(good/bad proportion) interactively\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    plot_target_dist_interact(data_gc,target_col='Risk',negative_cat='good',positive_cat='bad')\n    '''\n    trace0 = go.Bar(\n            x = df[df[target_col]== negative_cat][target_col].value_counts().index.values,\n            y = df[df[target_col]== negative_cat][target_col].value_counts().values,\n            name=f'negative_cat')\n    trace1 = go.Bar(\n            x = df[df[target_col]== positive_cat][target_col].value_counts().index.values,\n            y = df[df[target_col]== positive_cat][target_col].value_counts().values,\n            name=f'positive_cat')\n    data = [trace0, trace1]\n    layout = go.Layout()\n    layout = go.Layout(\n    yaxis=dict(title='Count'),\n    xaxis=dict(title='Target Variable'),\n    title='Target variable distribution')\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='grouped-bar')\n    \n\n## Plot distribution of a variable(interactive) ##\ndef plot_variable_dist_interact(df,target_col,variable_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Plot the distribution of any variable in the dataframe interactively\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    variable_col: the name of the column we want to plot\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    plot_variable_dist_interact(data_gc,target_col='Risk',variable_col='Credit amount',negative_cat='good',positive_cat='bad')\n    '''\n    df_negative = df.loc[df[target_col] == negative_cat][variable_col].values.tolist()\n    df_positive = df.loc[df[target_col] == positive_cat][variable_col].values.tolist()\n    df_variable = df[variable_col].values.tolist()\n    #First plot\n    trace0 = go.Histogram(\n    x=df_negative,\n    histnorm='probability',\n    name=f\"{negative_cat}\")\n    #Second plot\n    trace1 = go.Histogram(\n    x=df_positive,\n    histnorm='probability',\n    name=f\"{positive_cat}\")\n    #Third plot\n    trace2 = go.Histogram(\n    x=df_variable,\n    histnorm='probability',\n    name=\"overall\")\n    #Creating the grid\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=(f'{negative_cat}',f'{positive_cat}', 'Overall Distribuition'))\n    #setting the figs\n    fig.append_trace(trace0, 1, 1)\n    fig.append_trace(trace1, 1, 2)\n    fig.append_trace(trace2, 2, 1)\n    fig['layout'].update(showlegend=True, title=f\"{variable_col} Distribution\", bargap=0.05)\n    py.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n    \n\n## Plot distribution of a numerical variable by target(non-interactive) ##\ndef plot_num_variable_dist(df,target_col,variable_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Plot distribution of a numerical variable by target(non-interactive)\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    variable_col: the name of the column we want to plot\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    plot_num_variable_dist(data_gc,target_col='Risk',variable_col='Credit amount',negative_cat='good',positive_cat='bad')\n    '''\n    df_negative = df[df[target_col] == negative_cat]\n    df_positive = df[df[target_col] == positive_cat]\n    fig, axs = plt.subplots(1,1,figsize=(12,8),squeeze=False)\n    plt.subplots_adjust(top = 0.6)\n    g1 = sns.distplot(df_negative[variable_col], ax=axs[0,0], \n             color=\"g\",label=f'negative_cat')\n    g1 = sns.distplot(df_positive[variable_col], ax=axs[0,0], \n             color='r',label=f'positive_cat')\n    g1.set_title(f\"{variable_col} Distribuition\", fontsize=15)\n    g1.set_xlabel(f\"{variable_col}\")\n    g1.set_ylabel(\"Frequency\")\n    fig.legend(labels=['good','bad'])\n    plt.show()\n    \n\n## Box plot of a continous variable vs. a catergorical variable(non-interactive) ##    \ndef box_plot_convar_vs_catvar(df,target_col,con_var_col,cat_var_col):\n    '''\n    Purpose\n    ------\n    Box plot of a continous variable vs. a catergorical variable(non-interactive)\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    con_var_col: the name of the continous variable column\n    cat_var_col: the name of the categorical variable column\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    box_plot_convar_vs_catvar(data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing')\n    '''\n    fig, ax = plt.subplots(figsize=(12,12),squeeze=False)\n    g1 = sns.boxplot(x=f\"{cat_var_col}\", y=f\"{con_var_col}\", data=df, \n                palette=\"hls\", ax=ax[0,0], hue=f\"{target_col}\")\n    g1.set_title(f\"{con_var_col} by {cat_var_col}\", fontsize=15)\n    g1.set_xlabel(f\"{cat_var_col}\", fontsize=12)\n    g1.set_ylabel(f\"{con_var_col}\", fontsize=12)\n    plt.subplots_adjust(hspace = 0.4,top = 0.6)\n    plt.show()\n\n\n## Box plot of a continous variable vs. a catergorical variable(interactive) ##\ndef box_plot_convar_vs_catvar_interact(df,target_col,con_var_col,cat_var_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Box plot of a continous variable vs. a catergorical variable interactively\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    con_var_col: the name of the continous variable column\n    cat_var_col: the name of the categorical variable column\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    box_plot_convar_vs_catvar_interact(data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing',negative_cat='good',positive_cat='bad')\n    '''\n    df_negative = df[df[target_col] == negative_cat]\n    df_positive = df[df[target_col] == positive_cat]\n    trace0 = go.Box(\n        y=df_negative[con_var_col],\n        x=df_positive[cat_var_col],\n        name=f'{negative_cat}',\n        marker=dict(\n        color='#3D9970'))\n    trace1 = go.Box(\n        y=df_positive[con_var_col],\n        x=df_positive[cat_var_col],\n        name=f'{positive_cat}',\n        marker=dict(\n            color='#FF4136'))   \n    data = [trace0, trace1]\n    layout = go.Layout(\n        yaxis=dict(\n            title=f'{con_var_col}',\n            zeroline=False),\n        xaxis=dict(\n            title=f'{cat_var_col}'),\n        boxmode='group')\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename=f'box-{cat_var_col}-cat')\n\n\n## Violin plot of a continous variable vs. a catergorical variable(non-interactive) ##    \ndef violin_plot_convar_vs_catvar(df,target_col,con_var_col,cat_var_col):\n    '''\n    Purpose\n    ------\n    Violin plot of a continous variable vs. a catergorical variable(non-interactive)\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    con_var_col: the name of the continous variable column\n    cat_var_col: the name of the categorical variable column\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    violin_plot_convar_vs_catvar(data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing')\n    '''\n    fig, ax = plt.subplots(figsize=(12,12), squeeze=False)\n    g2 = sns.violinplot(x=f'{cat_var_col}', y=f\"{con_var_col}\", data=df, ax=ax[0,0],  \n                   hue=f\"{target_col}\", split=True, palette=\"hls\")\n    g2.set_title(f\"{con_var_col} vs {cat_var_col}\", fontsize=15)\n    g2.set_xlabel(f\"{cat_var_col}\", fontsize=12)\n    g2.set_ylabel(f\"{con_var_col}\", fontsize=12)\n    plt.subplots_adjust(hspace = 0.4,top = 0.6)\n    plt.show()\n\n        \n## Violin plot of a continous variable vs. a catergorical variable(interactive) ##\ndef violin_plot_convar_vs_catvar_interact(df,target_col,con_var_col,cat_var_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Violin plot of a continous variable vs. a catergorical variable interactively\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    con_var_col: the name of the continous variable column\n    cat_var_col: the name of the categorical variable column\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    violin_plot_convar_vs_catvar_interact(data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing',negative_cat='good',positive_cat='bad')\n    '''\n    df_negative = df[df[target_col] == negative_cat]\n    df_positive = df[df[target_col] == positive_cat]\n    fig = {\n        \"data\": [\n            {\n                \"type\": 'violin',\n                \"x\": df_negative[f'{cat_var_col}'],\n                \"y\": df_negative[f'{con_var_col}'],\n                \"legendgroup\": f'{negative_cat}',\n                \"scalegroup\": 'No',\n                \"name\": f'{negative_cat}',\n                \"side\": 'negative',\n                \"box\": {\"visible\": True},\n                \"meanline\": {\"visible\": True},\n                \"line\": {\"color\": 'green'}\n            },\n            {\n                \"type\": 'violin',\n                \"x\": df_positive[f'{cat_var_col}'],\n                \"y\": df_positive[f'{con_var_col}'],\n                \"legendgroup\": f'{positive_cat}',\n                \"scalegroup\": 'No',\n                \"name\": f'{positive_cat}',\n                \"side\": 'positive',\n                \"box\": {\"visible\": True},\n                \"meanline\": {\"visible\": True},\n                \"line\": {\"color\": 'red'}  \n            }],\n        \"layout\" : {\n                \"yaxis\": {\n                \"zeroline\": False,},\n                \"violingap\": 0,\n                \"violinmode\": \"overlay\"}}\n    py.iplot(fig, filename = 'violin/split', validate = False)\n    \n\n## Count plot of a categorical variable by target(interactive) ##\ndef cnt_plot_catvar_by_target_interact(df,target_col,cat_var_col,negative_cat,positive_cat):\n    '''\n    Purpose\n    ------\n    Count plot of a categorical variable by target interatively\n    \n    Params\n    ------\n    df: the dataframe\n    target_col: the name of target column\n    cat_var_col: the name of the categorical variable column\n    negative_cat: the name of the negative category of target,eg.'good'\n    positive_cat: the name of the positive category of target,eg.'bad'\n    \n    Example\n    ------\n    data_gc=pd.read_csv(../input/german-credit-data-with-risk/german_credit_data.csv,index_col=0)\n    cnt_plot_catvar_by_target_interact(data_gc,target_col='Risk',cat_var_col='Housing',negative_cat='good',positive_cat='bad')\n    '''\n    #First plot\n    trace0 = go.Bar(\n        x = df[df[f\"{target_col}\"]== f'{negative_cat}'][f\"{cat_var_col}\"].value_counts().index.values,\n        y = df[df[f\"{target_col}\"]== f'{negative_cat}'][f\"{cat_var_col}\"].value_counts().values,\n        name=f'{negative_cat}')\n    #Second plot\n    trace1 = go.Bar(\n        x = df[df[f\"{target_col}\"]== f'{positive_cat}'][f\"{cat_var_col}\"].value_counts().index.values,\n        y = df[df[f\"{target_col}\"]== f'{positive_cat}'][f\"{cat_var_col}\"].value_counts().values,\n        name=f\"{positive_cat}\")\n    data = [trace0, trace1]\n    layout = go.Layout(\n        title=f'{cat_var_col} Distribuition')\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename=f'{cat_var_col}-Grouped')\n    \n## Check Correlation ## \ndef correlation_check(df,show_plot=True):\n    corr = df.corr()\n    if show_plot==True:\n        fig = plt.figure()\n        sns.heatmap(corr, vmin=-1, vmax=1,annot=True, cmap='RdBu_r', annot_kws={'size': 12, 'weight': 'bold', 'color': 'blue'})#绘制相关性系数热力图\n        plt.show()\n    return corr\n\n## Check Multicollinearity ##\ndef multicollinearity_check(df,y,threshold=5,only_final_vif=True):\n    '''\n    Calculates the Generalized VIF (GVIF, Fox and Monette 1992) for a data set. GVIF ** (1 / (2 * Df)) ** 2 < 5 is equivalent to VIF.\n    The function assumes that categorical data are typed as 'category' or 'object' and automatically performs one-hot encoding. The function\n    will work properly if the data frame has columns previously one-hot encoded from binary data, but it will not work properly if the data \n    frame has multi-nomial columns that have been previously one-hot encoded.\n    Args:\n        df\n        y\n        threshold\n        only_final_vif\n    \n    Returns:\n        pandas data frame: a data frame, indexed by factor of the GVIF, GVIF^(1/2Df), VIF^(1/2Df)^2 \n        dictionary: Dictionary of column names (keys) and GVIF ** (1 / (2 * Df)) ** 2 (values)\n    '''\n\n    df_x = df.drop(y,axis=1)\n    # Save categorical column names, append with prefix\n    onehot_list = list(df_x.select_dtypes(include=['category', 'object', 'string']).columns)\n\n    # Since we do not include all of the indicator variables in the model so as to avoid the dummy variable trap, one of the indicator variables is dropped\n    df_1hot = pd.get_dummies(df_x, drop_first=True, dummy_na=False, prefix_sep='_')\n\n    # Create empty df to store GVIF results\n    gvif_df = pd.DataFrame(columns = ['factor','GVIF','Df','GVIF^(1/2Df)', 'VIF'])\n\n    # Iterate over columns\n    for (columnName, columnData) in df_x.iteritems():\n\n        # Select predictor as response: if dummy encoded, select all columns for variable\n        # Could all be done in the first condition, but that could result in incorrect column selection with similar column names\n        if columnName in onehot_list:\n            X1 = df_1hot.loc[:, df_1hot.columns.str.startswith(columnName)]\n            X2 = df_1hot.loc[:, ~df_1hot.columns.str.startswith(columnName)]\n        else:\n            X1 = df_1hot[[columnName]].values\n            X2 = df_1hot.loc[:, df_1hot.columns != columnName].values\n\n        # Calculate gvif\n        gvif = np.linalg.det(np.array(np.corrcoef(X1, rowvar=False), ndmin=2)) * np.linalg.det(np.corrcoef(X2, rowvar=False)) / np.linalg.det(np.corrcoef(np.append(X1, X2, axis=1), rowvar=False))\n\n        gvif_12df = np.power(gvif, 1 / (2 * X1.shape[1]))\n        gvif_12df_sq = gvif_12df ** 2\n        DF =  X1.shape[1]\n    \n\n        # Update results df\n        new_row = {'factor': columnName, 'GVIF': gvif,'Df':DF, 'GVIF^(1/2Df)': gvif_12df, 'VIF': gvif_12df_sq}\n        gvif_df = gvif_df.append(new_row, ignore_index=True)\n\n    gvif_df = gvif_df.set_index('factor')\n    if only_final_vif:\n        gvif_df_final = gvif_df.drop(['GVIF','Df','GVIF^(1/2Df)'],axis=1)\n    else:\n        gvif_df_final = gvif_df \n    gvif_filter = gvif_df.loc[gvif_df['VIF'] >= threshold]['VIF'].to_dict()\n    if gvif_filter:\n        for i in gvif_filter.keys():\n            df_x_m = df_x.drop([i],axis=1)\n    else:\n        df_x_m = df_x\n    df_m=pd.concat([df_x_m,df[y]],axis=1)\n    return gvif_df_final,gvif_filter,df_m","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Entropy_Gini\"></a> <br>\n# **4. Functions for Information Entropy&Gini Impurity**\n- Information Entropy Given a Dataframe\n- Information Entropy Given x and y\n- Information Entropy based on good/bad\n- Impurity Gini Given a Dataframe\n- Impurity Gini Given x and y\n- Impurity Gini based on good/bad"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Information Entropy Given a Dataframe ##\ndef ie(dt, y='Risk', x=None, order=True):\n    \n    '''\n    Information Entropy\n    ------\n    This function calculates information entropy (ie) for # multiple x variables.\n    ent(D) = -\\sum_k(p_k*log_2(p_k)), if p_k=0 then p_k*log_2(p_k)=0\n    \n    Params\n    ------\n    dt:A data frame with both x (predictor/feature) and y # (response/label) variables.\n    y:Name of y variable.\n    x:Name of x variables. Default is NULL. If x is NULL, # then all variables except y are counted as x variables.\n    order:Logical, default is TRUE. If it is TRUE, the # output will descending order via ie.\n\n    Returns\n    ------\n    return Information Entropy details\n    \n    Examples\n    ------\n  \n    # Information Entropy\n    dt_info_ent = ie(data_gc, y = \"Risk\")\n    '''\n    # remove date/time col\n    dt = check_datetime_cols(dt)\n    # replace \"\" by NA\n    dt = rep_blank_na(dt)\n    # check y\n    # dt = check_y(dt, y, positive)\n    # x variable names\n    x = x_variable(dt, y, x)\n    # info_ent\n    ielist = pd.DataFrame({\n        'variable': x,\n        'info_ent': [ie_xy(dt[i], dt[y]) for i in x]\n    }, columns=['variable', 'info_ent'])\n    # sorting\n    if order:\n        ielist = ielist.sort_values(by='info_ent', ascending=False)\n    # return\n    return ielist\n\n## Information Entropy Given x and y ##\ndef ie_xy(x, y):\n   \n    '''\n    Information Entropy\n    ------\n    This function calculates information entropy (ie) for # multiple x variables.\n    ent(D) = -\\sum_k(p_k*log_2(p_k)), if p_k=0 then p_k*log_2(p_k)=0\n    \n    Params\n    ------\n    y:Name of y variable.\n    x:Name of x variables. \n    \n    Returns\n    ------\n    return Information Entropy details\n    \n    Examples\n    ------\n    x = ['A','B','B','A','C',\"A\",\"B\",\"B\",\"B\",\"A\",\"C\",\"C\",\"A\",\"C\",\"B\",\"C\",\"A\"]\n    y = np.repeat(np.array([1,0]), [8,9])\n    ie_xy(x,y)\n    '''\n    # if x is None: x=0\n    # xy_N\n    df_xy = pd.DataFrame({'x':x,'y':y}).groupby(['x','y']).size().reset_index(name='xy_N')\n    # x_N\n    df_xy['x_N'] = df_xy.groupby('x')['xy_N'].transform(np.sum)\n    # p\n    df_xy['p'] = df_xy.xy_N/df_xy.x_N\n    df_xy['enti'] = df_xy.p.apply(lambda x: 0 if x==0 else x*np.log2(x))\n    # ent\n    df_enti = df_xy.groupby('x')\\\n      .agg({'xy_N':'sum', 'enti': lambda x:-sum(x)})\\\n      .rename(columns={'xy_N':'x_N','enti':'ent'}).replace(np.nan, 0)\n    df_enti['xN_distr'] = df_enti.apply({'x_N':lambda x: x/sum(x)})\n    # return\n    return sum(df_enti.ent*df_enti.xN_distr)\n\n\n## Information Entropy based on good/bad ##\ndef ie_01(good, bad):\n    \n    '''\n    Information Entropy\n    ------\n    This function calculates ie of total based on good and bad vectors\n    \n    Params\n    ------\n    good:good vector of good numbers\n    bad:bad vector of bad numbers\n    \n    Returns\n    ------\n    return Information Entropy\n    \n    Examples\n    ------\n    ie_01(good, bad)\n    dtm = melt(dt, id = 'Risk')[, .(good = sum(Risk==\"good\"), bad = sum(Risk==\"bad\")), keyby = c(\"variable\", \"value\")]\n    dtm[, .(ie = lapply(.SD, ie_01, bad)), by=\"variable\", .SDcols# =\"good\"]\n    '''\n    # enti function\n    enti = lambda x: 0 if x==0 else x*np.log2(x)\n    # df_enti\n    df_enti=pd.DataFrame({'good':good,'bad':bad})\\\n    .assign(\n        p0 = lambda x: x.good/(x.good+x.bad),\n        p1 = lambda x: x.bad/(x.good+x.bad),\n        count = lambda x: x.good+x.bad\n    ) \\\n    .assign(\n        enti = lambda x: -(x.p0.apply(enti)+x.p1.apply(enti))\n    )\n    # xN_distr\n    df_enti['xN_distr'] = df_enti.apply({'count':lambda x: x/sum(x)})\n    # return\n    return sum(df_enti.enti*df_enti.xN_distr)\n\n\n## Impurity Gini ##\ndef ig(dt, y, x=None, order=True):\n    \n    '''\n    Impurity Gini\n    ------\n    This function calculates gini impurity (used by the CART # Decision Tree) for multiple x variables.\n    gini impurity (CART) ------\n    gini(D) = 1-\\sum_k(p_k^2)\n    gini_impurity(D) = \\sum_v(abs(\\frac{D^v}{D})*gini(D^v))\n    \n    Params\n    ------\n    dt:A data frame with both x (predictor/feature) and y (response/label) variables. \n    y:y Name of y variable. \n    x:x Name of x variables. Default is NULL. If x is NULL, then all variables except y are counted as x variables.\n    order: Logical, default is TRUE. If it is TRUE, the output will descending order via gini\n    \n    Returns\n    ------\n    return gini impurity details\n    \n    Examples\n    ------\n    dt_gini = ig(data_gc, y = \"Risk\")\n    '''\n    # remove date/time col\n    dt = check_datetime_cols(dt)\n    # replace \"\" by NA\n    dt = rep_blank_na(dt)\n    # check y\n    # dt = check_y(dt, y, positive)\n    # x variable names\n    x = x_variable(dt, y, x)\n    # info_ent\n    iglist = pd.DataFrame({\n        'variable': x,\n        'gini_impurity': [ig_xy(dt[i], dt[y]) for i in x]\n    }, columns=['variable', 'gini_impurity'])\n    # sorting\n    if order:\n        iglist = iglist.sort_values(by='gini_impurity', ascending=False)\n    # return\n    return iglist\n    \n## Impurity Gini Given x and y ##    \ndef ig_xy(x, y):\n    # if x is None: x=0\n    # xy_N\n    df_xy = pd.DataFrame({'x':x,'y':y}).groupby(['x','y']).size().reset_index(name='xy_N')\n    # x_N\n    df_xy['x_N'] = df_xy.groupby('x')['xy_N'].transform(np.sum)\n    # p\n    df_xy['p'] = df_xy.xy_N/df_xy.x_N\n    df_xy['enti'] = df_xy.p.apply(lambda x: x**2)\n    # ent\n    df_gini = df_xy.groupby('x')\\\n      .agg({'xy_N':'sum', 'enti': lambda x:1-sum(x)})\\\n      .rename(columns={'xy_N':'x_N','enti':'ent'}).replace(np.nan, 0)\n    df_gini['xN_distr'] = df_gini.apply({'x_N':lambda x: x/sum(x)})\n    # return\n    return sum(df_gini.ent*df_gini.xN_distr)\n\n## Impurity Gini based on good/bad ##\ndef ig_01(good, bad):\n    # df_gini\n    df_gini=pd.DataFrame({'good':good,'bad':bad})\\\n    .assign(\n        p0 = lambda x: x.good/(x.good+x.bad),\n        p1 = lambda x: x.bad/(x.good+x.bad),\n        count = lambda x: x.good+x.bad\n    ) \\\n    .assign(\n        bin_ig = lambda x: 1-(x.p0**2+x.p1**2)\n    )\n    # xN_distr\n    df_gini['xN_distr'] = df_gini.apply({'count':lambda x: x/sum(x)})\n    # return\n    return sum(df_gini.bin_ig*df_gini.xN_distr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Info_value\"></a> <br>\n# **5. Functions for Information Value**\n- Information Value Given a Dataframe\n- Information Value Given x and y\n- Information Value based on good/bad\n- Information Value of each Bin\n- WOE of each Bin"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Information Value Given a Dataframe ##\ndef iv(dt, y, x=None, positive='bad|1', order=True):\n    '''\n    Information Value Given a Dataframe\n    ------\n    This function calculates information value (IV) for multiple x variables. \n    It treats each unique x value as a group and counts the number of y \n    classes. If there is a zero number of y class, it will be replaced by \n    0.99 to make sure it calculable.\n    \n    Params\n    ------\n    dt:A data frame with both x (predictor/feature) and \n    y (response/label) variables.\n    y:Name of y variable.\n    x: Name of x variables. Default is NULL. If x is NULL, then all variables except y are counted as x variables.\n    positive:Value of positive class, default is \"bad|1\".\n    order:Logical, default is TRUE. If it is TRUE, the output will descending order via iv.\n    \n    Returns\n    ------\n    DataFrame of Information Value\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # information values\n    dt_info_value = sc.iv(data_gc, y = \"Risk\")\n    '''\n    dt = dt.copy(deep=True)\n    if isinstance(y, str):\n        y = [y]\n    if isinstance(x, str) and x is not None:\n        x = [x]\n    if x is not None: \n        dt = dt[y+x]\n    # remove date/time col\n    # dt = rmcol_datetime_unique1(dt)\n    # replace \"\" by NA\n    #dt = rep_blank_na(dt)\n    # check y\n    dt = check_y(dt, y, positive)\n    # x variable names\n    xs = x_variable(dt, y, x)\n    # info_value\n    ivlist = pd.DataFrame({\n        'variable': xs,\n        'info_value': [iv_xy(dt[i], dt[y[0]]) for i in xs]\n    }, columns=['variable', 'info_value'])\n    # sorting iv\n    if order: \n        ivlist = ivlist.sort_values(by='info_value', ascending=False)\n    return ivlist\n\n\n## Information Value Given x and y ##\ndef iv_xy(x, y):\n    # good bad func\n    def goodbad(df):\n        names = {'good': (df['y']==0).sum(),'bad': (df['y']==1).sum()}\n        return pd.Series(names)\n    # iv calculation\n    iv_total = pd.DataFrame({'x':x.astype('str'),'y':y}) \\\n      .fillna('missing') \\\n      .groupby('x') \\\n      .apply(goodbad) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(iv = lambda x: (x.DistrBad-x.DistrGood)*np.log(x.DistrBad/x.DistrGood)) \\\n      .iv.sum()\n    # return iv\n    return iv_total\n\n\n## Information Value based on good/bad ##\ndef iv_01(good, bad):\n    '''\n    Information Value based on good/bad\n    ------\n    This function calculating IV of total based on good and bad vectors\n    \n    Params\n    ------\n    good:good vector of good numbers   \n    bad:bad vector of bad numbers      \n    \n    Returns\n    ------\n    Information Value\n    \n    Examples\n    ------\n    iv_01(good, bad)\n    dtm = melt(dt, id = 'Risk')[, .(good = sum(Risk==\"good\"), bad = sum(Risk==\"bad\")), keyby = c(\"variable\", \"value\")]\n    dtm[, .(iv = lapply(.SD, iv_01, bad)), by=\"variable\", .SDcols# =\"good\"]\n    '''\n    # iv calculation\n    iv_total = pd.DataFrame({'good':good,'bad':bad}) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(iv = lambda x: (x.DistrBad-x.DistrGood)*np.log(x.DistrBad/x.DistrGood)) \\\n      .iv.sum()\n    # return iv\n    return iv_total\n\n\n## Information Value of each Bin ##\ndef miv_01(good, bad):\n    '''\n    Information Value of each Bin\n    ------\n    This function calculating calculating IV of each bin based on good and bad vectors\n    \n    Params\n    ------\n    good:good vector of good numbers   \n    bad:bad vector of bad numbers      \n    \n    Returns\n    ------\n    Information Value of each bin\n    \n    Examples\n    ------\n    miv_01(good, bad)\n    dtm = melt(dt, id = 'Risk')[, .(good = sum(Risk==\"good\"), bad = sum(Risk==\"bad\")), keyby = c(\"variable\", \"value\")]\n    dtm[, .(iv = lapply(.SD, iv_01, bad)), by=\"variable\", .SDcols# =\"good\"]\n    '''\n    # iv calculation\n    infovalue = pd.DataFrame({'good':good,'bad':bad}) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(iv = lambda x: (x.DistrBad-x.DistrGood)*np.log(x.DistrBad/x.DistrGood)) \\\n      .iv\n    # return iv\n    return infovalue\n\n\n## WOE of each Bin ##\ndef woe_01(good, bad):\n    '''\n    WOE of each Bin\n    ------\n    This function calculating WOE of each bin based on good and bad vectors\n    \n    Params\n    ------\n    good:good vector of good numbers   \n    bad:bad vector of bad numbers      \n    \n    Returns\n    ------\n    WOE of each bin\n    \n    Examples\n    ------\n    woe_01(good, bad)\n    dtm = melt(dt, id = 'Risk')[, .(good = sum(Risk==\"good\"), bad = sum(Risk==\"bad\")), keyby = c(\"variable\", \"value\")]\n    dtm[, .(iv = lapply(.SD, iv_01, bad)), by=\"variable\", .SDcols# =\"good\"]\n    '''\n    # woe calculation\n    woe = pd.DataFrame({'good':good,'bad':bad}) \\\n      .replace(0, 0.9) \\\n      .assign(\n        DistrBad = lambda x: x.bad/sum(x.bad),\n        DistrGood = lambda x: x.good/sum(x.good)\n      ) \\\n      .assign(woe = lambda x: np.log(x.DistrBad/x.DistrGood)) \\\n      .woe\n    # return woe\n    return woe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"One_Hot_Encode\"></a> <br>\n# **6. Functions for One Hot Encoding**\n- One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"## One Hot Encoding ##\ndef one_hot(dt, cols_skip = None, cols_encode = None, nacol_rm = False, \n            replace_na = -1, category_to_integer = False):\n    '''\n    One Hot Encoding\n    ------\n    One-hot encoding on categorical variables. It is not needed when creating \n    a standard scorecard model, but required in models that without doing woe \n    transformation.\n    \n    Params\n    ------\n    dt:A data frame.\n    cols_skip:Name of categorical variables that will skip and without doing one-hot encoding. Default is None.\n    cols_encode:Name of categorical variables to be one-hot encoded, default is None. If it is None, then all categorical variables except in cols_skip are counted.\n    nacol_rm:Logical. One-hot encoding on categorical variable contains missing values, whether to remove the column generated to indicate the presence of NAs. Default is False.\n    replace_na:Replace missing values with a specified value such as -1 by default, or the mean/median value of the variable in which they occur. If it is None, then no missing values will be replaced.\n    factor_to_integer:Logical. Converting categorical variables to integer. Default is False.\n    \n    Returns\n    ------\n    A one-hot encoded data frame.\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    import pandas as pd\n    \n    # load data\n    dat1 = data_gc\n    dat2 = pd.DataFrame({'Risk':['good','bad']}).sample(50, replace=True)\n    dat = pd.concat([dat1, dat2], ignore_index=True)\n    \n    dt_oh0 = sc.one_hot(dat, cols_skip = 'Risk', nacol_rm = False) # default\n    dt_oh1 = sc.one_hot(dat, cols_skip = 'Risk', nacol_rm = True)\n    \n    dt_oh2 = sc.one_hot(dat, cols_skip = 'Risk', replace_na = -1) # default\n    dt_oh3 = sc.one_hot(dat, cols_skip = 'Risk', replace_na = 'median')\n    dt_oh4 = sc.one_hot(dat, cols_skip = 'Risk', replace_na = None)\n    '''\n    \n    # if it is str, converting to list\n    cols_skip, cols_encode = str_to_list(cols_skip), str_to_list(cols_encode)\n    # category columns into integer\n    if category_to_integer:\n        cols_cate = dt.dtypes[dt.dtypes == 'category'].index.tolist()\n        if cols_skip is not None:\n            cols_cate = list(set(cols_cate) - set(cols_skip))\n        dt[cols_cate] = dt[cols_cate].apply(lambda x: pd.factorize(x, sort=True)[0])\n    # columns encoding\n    if cols_encode is None:\n        cols_encode = char_cols = [i for i in list(dt) if not is_numeric_dtype(dt[i]) \n            and dt[i].dtypes != 'datetime64[ns]']\n    else:\n        cols_encode = x_variable(dt, y=cols_skip, x=cols_encode)\n    # columns skip\n    if cols_skip is not None:\n        cols_encode = list(set(cols_encode) - set(cols_skip))\n    # one hot encoding\n    if cols_encode is None or len(cols_encode) == 0:\n        dt_new = dt\n    else:\n        temp_dt = pd.get_dummies(dt[cols_encode], dummy_na = not nacol_rm)\n        # remove cols that unique len == 1 and has _nan\n        rm_cols_nan1 = [i for i in list(temp_dt) if len(temp_dt[i].unique())==1 and '_nan' in i]\n        dt_new = pd.concat([dt.drop(cols_encode, axis=1), temp_dt.drop(rm_cols_nan1, axis=1)], axis=1)\n    # replace missing values with fillna\n    def rep_na(x, repalce_na):\n        if x.isna().values.any():\n            # dtype is numeric\n            xisnum = is_numeric_dtype(x)\n            if isinstance(repalce_na, (int, float)):\n                fill_na = repalce_na\n            elif replace_na in ['mean', 'median'] and xisnum:\n                fill_na = getattr(np, 'mean')(x)\n            else:\n                fill_na = -1\n            # set fill_na as str if x is not num\n            if not xisnum:\n                fill_na = str(fill_na)\n            x = x.fillna(fill_na)\n        return x\n    if replace_na is not None:\n        names_fillna = list(dt_new) \n        if cols_skip is not None: names_fillna = list(set(names_fillna)-set(cols_skip))\n        dt_new[names_fillna] = dt_new[names_fillna].apply(lambda x: rep_na(x, replace_na))\n    # return\n    return dt_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Performance\"></a> <br>\n# **7. Functions for Model Performance**\n- KS&Lift Dataframe\n- Plotting KS Curve\n- Plotting Lift Curve\n- Calculating ROC\n- Plotting ROC Curve\n- Plotting Precision-Recall Curve\n- plotting F1\n- Show Performance Evaluation Metrics(KS/ROC/Lift/PR)\n- Population Stability Index(PSI)\n- Show Score Bin Sorting\n- Score Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## KS&Lift Dataframe ##\ndef eva_dfkslift(df, groupnum=None):\n    if groupnum is None: groupnum=len(df.index)\n    # good bad func\n    def n0(x): return sum(x==0)\n    def n1(x): return sum(x==1)\n    df_kslift = df.sort_values('pred', ascending=False).reset_index(drop=True)\\\n      .assign(group=lambda x: np.ceil((x.index+1)/(len(x.index)/groupnum)))\\\n      .groupby('group')['label'].agg([n0,n1])\\\n      .reset_index().rename(columns={'n0':'good','n1':'bad'})\\\n      .assign(\n        group=lambda x: (x.index+1)/len(x.index),\n        good_distri=lambda x: x.good/sum(x.good), \n        bad_distri=lambda x: x.bad/sum(x.bad), \n        badrate=lambda x: x.bad/(x.good+x.bad),\n        cumbadrate=lambda x: np.cumsum(x.bad)/np.cumsum(x.good+x.bad),\n        lift=lambda x: (np.cumsum(x.bad)/np.cumsum(x.good+x.bad))/(sum(x.bad)/sum(x.good+x.bad)),\n        cumgood=lambda x: np.cumsum(x.good)/sum(x.good), \n        cumbad=lambda x: np.cumsum(x.bad)/sum(x.bad)\n      ).assign(ks=lambda x:abs(x.cumbad-x.cumgood))\n    # bind 0\n    df_kslift=pd.concat([\n      pd.DataFrame({'group':0, 'good':0, 'bad':0, 'good_distri':0, 'bad_distri':0, 'badrate':0, 'cumbadrate':np.nan, 'cumgood':0, 'cumbad':0, 'ks':0, 'lift':np.nan}, index=np.arange(1)),\n      df_kslift\n    ], ignore_index=True)\n    # return\n    return df_kslift\n     \n## Plotting KS Curve ##\ndef eva_pks(dfkslift, title):\n    dfks = dfkslift.loc[lambda x: x.ks==max(x.ks)].sort_values('group').iloc[0]\n    ###### plot ###### \n    # fig, ax = plt.subplots()\n    # ks, cumbad, cumgood\n    plt.plot(dfkslift.group, dfkslift.ks, 'tomato', \n      dfkslift.group, dfkslift.cumgood, 'c-', \n      dfkslift.group, dfkslift.cumbad, 'c-')\n    # ks vline\n    plt.plot([dfks['group'], dfks['group']], [0, dfks['ks']], 'tomato')\n    # set xylabel\n    plt.gca().set(title=title+'K-S', \n      xlabel='% of population', ylabel='% of total Good/Bad', \n      xlim=[0,1], ylim=[0,1], aspect='equal')\n    # text\n    # plt.text(0.5,0.96,'K-S', fontsize=15,horizontalalignment='center')\n    plt.text(0.2,0.8,'Bad',horizontalalignment='center')\n    plt.text(0.8,0.55,'Good',horizontalalignment='center')\n    plt.text(dfks['group'], dfks['ks'], 'KS:'+ str(round(dfks['ks'],4)), horizontalalignment='center',color='tomato')\n    # plt.grid()\n    # plt.show()\n    # return fig\n\n## Plotting Lift Curve ##\ndef eva_plift(dfkslift, title):\n    badrate_avg = sum(dfkslift.bad)/sum(dfkslift.good+dfkslift.bad)\n    ###### plot ###### \n    # fig, ax = plt.subplots()\n    # ks, cumbad, cumgood\n    plt.plot(dfkslift.group, dfkslift.cumbadrate, 'k-')\n    # ks vline\n    plt.plot([0, 1], [badrate_avg, badrate_avg], 'r--')\n    # set xylabel\n    plt.gca().set(title=title+'Lift', \n      xlabel='% of population', ylabel='% of Bad', \n      xlim=[0,1], ylim=[0,1], aspect='equal')\n    # text\n    # plt.text(0.5,0.96,'Lift', fontsize=15,horizontalalignment='center')\n    plt.text(0.7,np.mean(dfkslift.cumbadrate),'cumulate badrate',horizontalalignment='center')\n    plt.text(0.7,badrate_avg,'average badrate',horizontalalignment='center')\n    # plt.grid()\n    # plt.show()\n    # return fig\n    \n\n## Calculating ROC ##\ndef eva_dfrocpr(df):\n    def n0(x): return sum(x==0)\n    def n1(x): return sum(x==1)\n    dfrocpr = df.sort_values('pred')\\\n      .groupby('pred')['label'].agg([n0,n1,len])\\\n      .reset_index().rename(columns={'n0':'countN','n1':'countP','len':'countpred'})\\\n      .assign(\n        FN = lambda x: np.cumsum(x.countP), \n        TN = lambda x: np.cumsum(x.countN) \n      ).assign(\n        TP = lambda x: sum(x.countP) - x.FN, \n        FP = lambda x: sum(x.countN) - x.TN\n      ).assign(\n        TPR = lambda x: x.TP/(x.TP+x.FN), \n        FPR = lambda x: x.FP/(x.TN+x.FP), \n        precision = lambda x: x.TP/(x.TP+x.FP), \n        recall = lambda x: x.TP/(x.TP+x.FN)\n      ).assign(\n        F1 = lambda x: 2*x.precision*x.recall/(x.precision+x.recall)\n      )\n    return dfrocpr\n\n\n## Plotting ROC Curve ##\ndef eva_proc(dfrocpr, title):\n    dfrocpr = pd.concat(\n      [dfrocpr[['FPR','TPR']], pd.DataFrame({'FPR':[0,1], 'TPR':[0,1]})], \n      ignore_index=True).sort_values(['FPR','TPR'])\n    auc = dfrocpr.sort_values(['FPR','TPR'])\\\n          .assign(\n            TPR_lag=lambda x: x['TPR'].shift(1), FPR_lag=lambda x: x['FPR'].shift(1)\n          ).assign(\n            auc=lambda x: (x.TPR+x.TPR_lag)*(x.FPR-x.FPR_lag)/2\n          )['auc'].sum()\n    ###### plot ###### \n    # fig, ax = plt.subplots()\n    # ks, cumbad, cumgood\n    plt.plot(dfrocpr.FPR, dfrocpr.TPR, 'c-')\n    # ks vline\n    x=np.array(np.arange(0,1.1,0.1))\n    plt.plot(x, x, 'tomato')\n    # fill \n    plt.fill_between(dfrocpr.FPR, 0, dfrocpr.TPR, color='cyan', alpha=0.1)\n    # set xylabel\n    plt.gca().set(title=title+'ROC',\n      xlabel='FPR', ylabel='TPR', \n      xlim=[0,1], ylim=[0,1], aspect='equal')\n    # text\n    # plt.text(0.5,0.96, 'ROC', fontsize=15, horizontalalignment='center')\n    plt.text(0.55,0.45, 'AUC:'+str(round(auc,4)), horizontalalignment='center', color='tomato')\n    # plt.grid()\n    # plt.show()\n    # return fig\n\n    \n## Plotting Precision-Recall Curve ##\ndef eva_ppr(dfrocpr, title):\n    ###### plot ###### \n    # fig, ax = plt.subplots()\n    # ks, cumbad, cumgood\n    plt.plot(dfrocpr.recall, dfrocpr.precision, 'k-')\n    # ks vline\n    x=np.array(np.arange(0,1.1,0.1))\n    plt.plot(x, x, 'r--')\n    # set xylabel\n    plt.gca().set(title=title+'P-R', \n      xlabel='Recall', ylabel='Precision', \n      xlim=[0,1], ylim=[0,1], aspect='equal')\n    # text\n    # plt.text(0.5,0.96, 'P-R', fontsize=15, horizontalalignment='center')\n    # plt.grid()\n    # plt.show()\n    # return fig\n    \n    \n## plotting F1 ##\ndef eva_pf1(dfrocpr, title):\n    dfrocpr=dfrocpr.assign(pop=lambda x: np.cumsum(x.countpred)/sum(x.countpred))\n    ###### plot ###### \n    # fig, ax = plt.subplots()\n    # ks, cumbad, cumgood\n    plt.plot(dfrocpr['pop'], dfrocpr['F1'], 'k-')\n    # ks vline\n    F1max_pop = dfrocpr.loc[dfrocpr['F1'].idxmax(),'pop']\n    F1max_F1 = dfrocpr.loc[dfrocpr['F1'].idxmax(),'F1']\n    plt.plot([F1max_pop,F1max_pop], [0,F1max_F1], 'r--')\n    # set xylabel\n    plt.gca().set(title=title+'F1', \n      xlabel='% of population', ylabel='F1', \n      xlim=[0,1], ylim=[0,1], aspect='equal')\n    # pred text\n    pred_0=dfrocpr.loc[dfrocpr['pred'].idxmin(),'pred']\n    pred_F1max=dfrocpr.loc[dfrocpr['F1'].idxmax(),'pred']\n    pred_1=dfrocpr.loc[dfrocpr['pred'].idxmax(),'pred']\n    if np.mean(dfrocpr.pred) < 0 or np.mean(dfrocpr.pred) > 1: \n        pred_0 = -pred_0\n        pred_F1max = -pred_F1max\n        pred_1 = -pred_1\n    plt.text(0, 0, 'pred \\n'+str(round(pred_0,4)), horizontalalignment='left',color='b')\n    plt.text(F1max_pop, 0, 'pred \\n'+str(round(pred_F1max,4)), horizontalalignment='center',color='b')\n    plt.text(1, 0, 'pred \\n'+str(round(pred_1,4)), horizontalalignment='right',color='b')\n    # title F1\n    plt.text(F1max_pop, F1max_F1, 'F1 max: \\n'+ str(round(F1max_F1,4)), horizontalalignment='center',color='b')\n    # plt.grid()\n    # plt.show()\n    # return fig\n    \n## Show Performance Evaluation Metrics(KS/ROC/Lift/PR) ##\ndef perf_eva(label, pred, title=None, groupnum=None, plot_type=[\"ks\", \"roc\"], show_plot=True, positive=\"bad|1\", seed=186):\n    '''\n    KS, ROC, Lift, PR\n    ------\n    perf_eva provides performance evaluations, such as \n    kolmogorov-smirnow(ks), ROC, lift and precision-recall curves, \n    based on provided label and predicted probability values.\n    \n    Params\n    ------\n    label:Label values, such as 0s and 1s, 0 represent for good and 1 for bad.\n    pred:Predicted probability or score.\n    title:Title of plot, default is \"performance\".\n    groupnum:The group number when calculating KS.  Default NULL, which means the number of sample size.\n    plot_type:Types of performance plot, such as \"ks\", \"lift\", \"roc\", \"pr\". Default c(\"ks\", \"roc\").\n    show_plot:Logical value, default is TRUE. It means whether to show plot.\n    positive:Value of positive class, default is \"bad|1\".\n    seed:Integer, default is 186. The specify seed is used for random sorting data.\n    \n    Returns\n    ------\n    dict of ks, auc, gini values, and figure objects\n    \n    Details\n    ------\n    Accuracy = (true positive and true negative)/total cases\n    Error rate = (false positive and false negative)/total cases\n    TPR, True Positive Rate(Recall or Sensitivity) = true positive/total actual positive\n    PPV, Positive Predicted Value(Precision) = true positive/total predicted positive\n    TNR, True Negative Rate(Specificity) = true negative/total actual negative\n    NPV, Negative Predicted Value = true negative/total predicted negative\n        \n    Examples\n    ------\n    import scorecardpy\n    \n    # filter variable via missing rate, iv, identical value rate\n    dt_sel = sc.var_filter(data_gc, \"Risk\")\n    \n    # woe binning ------\n    bins = sc.woebin(dt_sel, \"Risk\")\n    dt_woe = sc.woebin_ply(dt_sel, bins)\n    \n    y = dt_woe.loc[:,'Risk']\n    X = dt_woe.loc[:,dt_woe.columns != 'Risk']\n    \n    # logistic regression ------\n    from sklearn.linear_model import LogisticRegression\n    lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')\n    lr.fit(X, y)\n    \n    # predicted proability\n    dt_pred = lr.predict_proba(X)[:,1]\n    # performace ------\n    # Example I # only ks & auc values\n    sc.perf_eva(y, dt_pred, show_plot=False)\n    \n    # Example II # ks & roc plot\n    sc.perf_eva(y, dt_pred)\n    \n    # Example III # ks, lift, roc & pr plot\n    sc.perf_eva(y, dt_pred, plot_type = [\"ks\",\"lift\",\"roc\",\"pr\"])\n    '''\n    # inputs checking\n    if len(label) != len(pred):\n        warnings.warn('Incorrect inputs; label and pred should be list with the same length.')\n    # if pred is score\n    if np.mean(pred) < 0 or np.mean(pred) > 1:\n        warnings.warn('Since the average of pred is not in [0,1], it is treated as predicted score but not probability.')\n        pred = -pred\n    # random sort datatable\n    df = pd.DataFrame({'label':label, 'pred':pred}).sample(frac=1, random_state=seed)\n    # remove NAs\n    if any(np.unique(df.isna())):\n        warnings.warn('The NANs in \\'label\\' or \\'pred\\' were removed.')\n        df = df.dropna()\n    # check label\n    df = check_y(df, 'label', positive)\n    # title\n    title='' if title is None else str(title)+': '\n    \n    ### data ###\n    # dfkslift ------\n    if any([i in plot_type for i in ['ks', 'lift']]):\n        dfkslift = eva_dfkslift(df, groupnum)\n        if 'ks' in plot_type: df_ks = dfkslift\n        if 'lift' in plot_type: df_lift = dfkslift\n    # dfrocpr ------\n    if any([i in plot_type for i in [\"roc\",\"pr\",'f1']]):\n        dfrocpr = eva_dfrocpr(df)\n        if 'roc' in plot_type: df_roc = dfrocpr\n        if 'pr' in plot_type: df_pr = dfrocpr\n        if 'f1' in plot_type: df_f1 = dfrocpr\n    ### return list ### \n    rt = {}\n    # plot, KS ------\n    if 'ks' in plot_type:\n        rt['KS'] = round(dfkslift.loc[lambda x: x.ks==max(x.ks),'ks'].iloc[0],4)\n    # plot, ROC ------\n    if 'roc' in plot_type:\n        auc = pd.concat(\n          [dfrocpr[['FPR','TPR']], pd.DataFrame({'FPR':[0,1], 'TPR':[0,1]})], \n          ignore_index=True).sort_values(['FPR','TPR'])\\\n          .assign(\n            TPR_lag=lambda x: x['TPR'].shift(1), FPR_lag=lambda x: x['FPR'].shift(1)\n          ).assign(\n            auc=lambda x: (x.TPR+x.TPR_lag)*(x.FPR-x.FPR_lag)/2\n          )['auc'].sum()\n        ### \n        rt['AUC'] = round(auc, 4)\n        rt['Gini'] = round(2*auc-1, 4)\n    \n    ### export plot ### \n    if show_plot:\n        plist = [\"eva_p\"+i+'(df_'+i+',title)' for i in plot_type]\n        subplot_nrows = np.ceil(len(plist)/2)\n        subplot_ncols = np.ceil(len(plist)/subplot_nrows)\n        \n        fig = plt.figure()\n        for i in np.arange(len(plist)):\n            plt.subplot(subplot_nrows,subplot_ncols,i+1)\n            eval(plist[i])\n        plt.show()\n        rt['pic'] = fig\n    # return \n    return rt\n    \n\n## Population Stability Index(PSI) ##\ndef perf_psi(score, label=None, title=None, x_limits=None, x_tick_break=50, show_plot=True, seed=186, return_distr_dat=False):\n    '''\n    PSI\n    ------\n    perf_psi calculates population stability index (PSI) and provides \n    credit score distribution based on credit score datasets.\n    \n    Params\n    ------\n    score:A list of credit score for actual and expected data samples. For example, score = list(actual = score_A, expect = score_E), both score_A and score_E are dataframes with the same column names.\n    label:A list of label value for actual and expected data samples. The default is NULL. For example, label = list(actual = label_A, expect = label_E), both label_A and label_E are vectors or dataframes. The label values should be 0s and 1s, 0 represent for good and 1 for bad.\n    title:Title of plot, default is NULL.\n    x_limits:x-axis limits, default is None.\n    x_tick_break:x-axis ticker break, default is 50.\n    show_plot:Logical, default is TRUE. It means whether to show plot.\n    return_distr_dat:Logical, default is FALSE.\n    seed:Integer, default is 186. The specify seed is used for random sorting data.\n    \n    Returns\n    ------\n    dict of psi values and figure objects\n        \n    Details\n    ------\n    The population stability index (PSI) formula is displayed below: \n    \\deqn{PSI = \\sum((Actual\\% - Expected\\%)*(\\ln(\\frac{Actual\\%}{Expected\\%}))).} \n    The rule of thumb for the PSI is as follows: Less than 0.1 inference \n    insignificant change, no action required; 0.1 - 0.25 inference some \n    minor change, check other scorecard monitoring metrics; Greater than \n    0.25 inference major shift in population, need to delve deeper.\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)    \n    # filter variable via missing rate, iv, identical value rate\n    dt_sel = sc.var_filter(data_gc, \"Risk\")\n    \n    # breaking dt into train and test ------\n    train, test = sc.split_df(dt_sel, 'Risk').values()\n    \n    # woe binning ------\n    bins = sc.woebin(train, \"Risk\")\n    \n    # converting train and test into woe values\n    train_woe = sc.woebin_ply(train, bins)\n    test_woe = sc.woebin_ply(test, bins)\n    \n    y_train = train_woe.loc[:,'Risk']\n    X_train = train_woe.loc[:,train_woe.columns != 'Risk']\n    y_test = test_woe.loc[:,'Risk']\n    X_test = test_woe.loc[:,train_woe.columns != 'Risk']\n    # logistic regression ------\n    from sklearn.linear_model import LogisticRegression\n    lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')\n    lr.fit(X_train, y_train)\n    \n    # predicted proability\n    pred_train = lr.predict_proba(X_train)[:,1]\n    pred_test = lr.predict_proba(X_test)[:,1]\n    \n    # performance ks & roc ------\n    perf_train = sc.perf_eva(y_train, pred_train, title = \"train\")\n    perf_test = sc.perf_eva(y_test, pred_test, title = \"test\")\n    \n    # score ------\n    # scorecard\n    card = sc.scorecard(bins, lr, X_train.columns)\n    # credit score\n    train_score = sc.scorecard_ply(train, card)\n    test_score = sc.scorecard_ply(test, card)\n    \n    # Example I # psi\n    psi1 = sc.perf_psi(\n      score = {'train':train_score, 'test':test_score},\n      label = {'train':y_train, 'test':y_test},\n      x_limits = [250, 750],\n      x_tick_break = 50\n    )\n    \n    # Example II # credit score, only_total_score = FALSE\n    train_score2 = sc.scorecard_ply(train, card, only_total_score=False)\n    test_score2 = sc.scorecard_ply(test, card, only_total_score=False)\n    # psi\n    psi2 = sc.perf_psi(\n      score = {'train':train_score2, 'test':test_score2},\n      label = {'train':y_train, 'test':y_test},\n      x_limits = [250, 750],\n      x_tick_break = 50\n    )\n    '''\n    \n    # inputs checking\n    ## score\n    if not isinstance(score, dict) and len(score) != 2:\n        raise Exception(\"Incorrect inputs; score should be a dictionary with two elements.\")\n    else:\n        if any([not isinstance(i, pd.DataFrame) for i in score.values()]):\n            raise Exception(\"Incorrect inputs; score is a dictionary of two dataframes.\")\n        score_columns = [list(i.columns) for i in score.values()]\n        if set(score_columns[0]) != set(score_columns[1]):\n            raise Exception(\"Incorrect inputs; the column names of two dataframes in score should be the same.\")\n    ## label\n    if label is not None:\n        if not isinstance(label, dict) and len(label) != 2:\n            raise Exception(\"Incorrect inputs; label should be a dictionary with two elements.\")\n        else:\n            if set(score.keys()) != set(label.keys()):\n                raise Exception(\"Incorrect inputs; the keys of score and label should be the same. \")\n            for i in label.keys():\n                if isinstance(label[i], pd.DataFrame):\n                    if len(label[i].columns) == 1:\n                        label[i] = label[i].iloc[:,0]\n                    else:\n                        raise Exception(\"Incorrect inputs; the number of columns in label should be 1.\")\n    # score dataframe column names\n    score_names = score[list(score.keys())[0]].columns\n    # merge label with score\n    for i in score.keys():\n        score[i] = score[i].copy(deep=True)\n        if label is not None:\n            score[i].loc[:,'y'] = label[i]\n        else:\n            score[i].copy(deep=True).loc[:,'y'] = np.nan\n    # dateset of score and label\n    dt_sl = pd.concat(score, names=['ae', 'rowid']).reset_index()\\\n      .sample(frac=1, random_state=seed)\n      # ae refers to 'Actual & Expected'\n    \n    # PSI function\n    def psi(dat):\n        dt_bae = dat.groupby(['ae','bin']).size().reset_index(name='N')\\\n          .pivot_table(values='N', index='bin', columns='ae').fillna(0.9)\\\n          .agg(lambda x: x/sum(x))\n        dt_bae.columns = ['A','E']\n        psi_dt = dt_bae.assign(\n          AE = lambda x: x.A-x.E,\n          logAE = lambda x: np.log(x.A/x.E)\n        ).assign(\n          bin_PSI=lambda x: x.AE*x.logAE\n        )['bin_PSI'].sum()\n        return psi_dt\n    \n    # return psi and pic\n    rt_psi = {}\n    rt_pic = {}\n    rt_dat = {}\n    rt = {}\n    for sn in score_names:\n        # dataframe with columns of ae y sn\n        dat = dt_sl[['ae', 'y', sn]]\n        if len(dt_sl[sn].unique()) > 10:\n            # breakpoints\n            if x_limits is None:\n                x_limits = dat[sn].quantile([0.02, 0.98])\n                x_limits = round(x_limits/x_tick_break)*x_tick_break\n                x_limits = list(x_limits)\n        \n            brkp = np.unique([np.floor(min(dt_sl[sn])/x_tick_break)*x_tick_break]+\\\n              list(np.arange(x_limits[0], x_limits[1], x_tick_break))+\\\n              [np.ceil(max(dt_sl[sn])/x_tick_break)*x_tick_break])\n            # cut\n            labels = ['[{},{})'.format(int(brkp[i]), int(brkp[i+1])) for i in range(len(brkp)-1)]\n            dat.loc[:,'bin'] = pd.cut(dat[sn], brkp, right=False, labels=labels)\n        else:\n            dat.loc[:,'bin'] = dat[sn]\n        # psi ------\n        rt_psi[sn] = pd.DataFrame({'PSI':psi(dat)},index=np.arange(1)) \n    \n        # distribution of scorecard probability\n        def good(x): return sum(x==0)\n        def bad(x): return sum(x==1)\n        distr_prob = dat.groupby(['ae', 'bin'])\\\n          ['y'].agg([good, bad])\\\n          .assign(N=lambda x: x.good+x.bad,\n            badprob=lambda x: x.bad/(x.good+x.bad)\n          ).reset_index()\n        distr_prob.loc[:,'distr'] = distr_prob.groupby('ae')['N'].transform(lambda x:x/sum(x))\n        # pivot table\n        distr_prob = distr_prob.pivot_table(values=['N','badprob', 'distr'], index='bin', columns='ae')\n            \n        # plot ------\n        if show_plot:\n            ###### param ######\n            ind = np.arange(len(distr_prob.index))    # the x locations for the groups\n            width = 0.35       # the width of the bars: can also be len(x) sequence\n            ###### plot ###### \n            fig, ax1 = plt.subplots()\n            ax2 = ax1.twinx()\n            title_string = sn+'_PSI: '+str(round(psi(dat),4))\n            title_string = title_string if title is None else str(title)+' '+title_string\n            # ax1\n            p1 = ax1.bar(ind, distr_prob.distr.iloc[:,0], width, color=(24/254, 192/254, 196/254), alpha=0.6)\n            p2 = ax1.bar(ind+width, distr_prob.distr.iloc[:,1], width, color=(246/254, 115/254, 109/254), alpha=0.6)\n            # ax2\n            p3 = ax2.plot(ind+width/2, distr_prob.badprob.iloc[:,0], color=(24/254, 192/254, 196/254))\n            ax2.scatter(ind+width/2, distr_prob.badprob.iloc[:,0], facecolors='w', edgecolors=(24/254, 192/254, 196/254))\n            p4 = ax2.plot(ind+width/2, distr_prob.badprob.iloc[:,1], color=(246/254, 115/254, 109/254))\n            ax2.scatter(ind+width/2, distr_prob.badprob.iloc[:,1], facecolors='w', edgecolors=(246/254, 115/254, 109/254))\n            # settings\n            ax1.set_ylabel('Score distribution')\n            ax2.set_ylabel('Bad probability')#, color='blue')\n            # ax2.tick_params(axis='y', colors='blue')\n            # ax1.set_yticks(np.arange(0, np.nanmax(distr_prob['distr'].values), 0.2))\n            # ax2.set_yticks(np.arange(0, 1+0.2, 0.2))\n            ax1.set_ylim([0,np.ceil(np.nanmax(distr_prob['distr'].values)*10)/10])\n            ax2.set_ylim([0,1])\n            plt.xticks(ind+width/2, distr_prob.index)\n            plt.title(title_string, loc='left')\n            ax1.legend((p1[0], p2[0]), list(distr_prob.columns.levels[1]), loc='upper left')\n            ax2.legend((p3[0], p4[0]), list(distr_prob.columns.levels[1]), loc='upper right')\n            # show plot \n            plt.show()\n            \n            # return of pic\n            rt_pic[sn] = fig\n        \n        # return distr_dat ------\n        if return_distr_dat:\n            rt_dat[sn] = distr_prob[['N','badprob']].reset_index()\n    # return rt\n    rt['psi'] = pd.concat(rt_psi).reset_index().rename(columns={'level_0':'variable'})[['variable', 'PSI']]\n    rt['pic'] = rt_pic\n    if return_distr_dat: rt['dat'] = rt_dat\n    return rt\n\n## Show Score Bin sorting ##\ndef score_bin_sort(score, label=None, title=None, x_limits=None, x_tick_break=50, seed=186):\n    # inputs checking\n    ## score\n    if not isinstance(score, dict) and len(score) != 2:\n        raise Exception(\"Incorrect inputs; score should be a dictionary with two elements.\")\n    else:\n        if any([not isinstance(i, pd.DataFrame) for i in score.values()]):\n            raise Exception(\"Incorrect inputs; score is a dictionary of two dataframes.\")\n        score_columns = [list(i.columns) for i in score.values()]\n        if set(score_columns[0]) != set(score_columns[1]):\n            raise Exception(\"Incorrect inputs; the column names of two dataframes in score should be the same.\")\n    ## label\n    if label is not None:\n        if not isinstance(label, dict) and len(label) != 2:\n            raise Exception(\"Incorrect inputs; label should be a dictionary with two elements.\")\n        else:\n            if set(score.keys()) != set(label.keys()):\n                raise Exception(\"Incorrect inputs; the keys of score and label should be the same. \")\n            for i in label.keys():\n                if isinstance(label[i], pd.DataFrame):\n                    if len(label[i].columns) == 1:\n                        label[i] = label[i].iloc[:,0]\n                    else:\n                        raise Exception(\"Incorrect inputs; the number of columns in label should be 1.\")\n    # score dataframe column names\n    score_names = score[list(score.keys())[0]].columns\n    # merge label with score\n    for i in score.keys():\n        score[i] = score[i].copy(deep=True)\n        if label is not None:\n            score[i].loc[:,'y'] = label[i]\n        else:\n            score[i].copy(deep=True).loc[:,'y'] = np.nan\n    # dateset of score and label\n    dt_sl = pd.concat(score, names=['ae', 'rowid']).reset_index()\\\n      .sample(frac=1, random_state=seed)\n      # ae refers to 'Actual & Expected'\n    \n    # PSI function\n    def psi(dat):\n        dt_bae = dat.groupby(['ae','bin']).size().reset_index(name='N')\\\n          .pivot_table(values='N', index='bin', columns='ae').fillna(0.9)\\\n          .agg(lambda x: x/sum(x))\n        dt_bae.columns = ['A','E']\n        psi_dt = dt_bae.assign(\n          AE = lambda x: x.A-x.E,\n          logAE = lambda x: np.log(x.A/x.E)\n        ).assign(\n          bin_PSI=lambda x: x.AE*x.logAE\n        )['bin_PSI'].sum()\n        return psi_dt\n    \n    # return psi and pic\n    rt_psi = {}\n    rt_pic = {}\n    rt_dat = {}\n    rt = {}\n    for sn in score_names:\n        # dataframe with columns of ae y sn\n        dat = dt_sl[['ae', 'y', sn]]\n        if len(dt_sl[sn].unique()) > 10:\n            # breakpoints\n            if x_limits is None:\n                x_limits = dat[sn].quantile([0.02, 0.98])\n                x_limits = round(x_limits/x_tick_break)*x_tick_break\n                x_limits = list(x_limits)\n        \n            brkp = np.unique([np.floor(min(dt_sl[sn])/x_tick_break)*x_tick_break]+\\\n              list(np.arange(x_limits[0], x_limits[1], x_tick_break))+\\\n              [np.ceil(max(dt_sl[sn])/x_tick_break)*x_tick_break])\n            # cut\n            labels = ['[{},{})'.format(int(brkp[i]), int(brkp[i+1])) for i in range(len(brkp)-1)]\n            dat.loc[:,'bin'] = pd.cut(dat[sn], brkp, right=False, labels=labels)\n        else:\n            dat.loc[:,'bin'] = dat[sn]\n        # psi ------\n        rt_psi[sn] = pd.DataFrame({'PSI':psi(dat)},index=np.arange(1)) \n    \n        # distribution of scorecard probability\n        def good(x): return sum(x==0)\n        def bad(x): return sum(x==1)\n        distr_prob = dat.groupby(['ae', 'bin'])\\\n          ['y'].agg([good, bad])\\\n          .assign(N=lambda x: x.good+x.bad,\n            badprob=lambda x: x.bad/(x.good+x.bad)\n          ).reset_index()\n        distr_prob.loc[:,'distr'] = distr_prob.groupby('ae')['N'].transform(lambda x:x/sum(x))\n        # pivot table\n        distr_prob = distr_prob.pivot_table(values=['N','badprob', 'distr'], index='bin', columns='ae')\n        rt_dat[sn] = distr_prob[['N','badprob']].reset_index()\n        score_bin_sort = pd.DataFrame()\n        score_bin_sort['score'] = rt_dat[sn]['bin']\n        score_bin_sort['total'] = rt_dat[sn][('N','test')]+rt_dat[sn][('N','train')]\n        score_bin_sort['good'] = (rt_dat[sn][('N','test')]*(1-rt_dat[sn][('badprob','test')])+rt_dat[sn][('N','train')]*(1-rt_dat[sn][('badprob','train')])).map(lambda x: int(x))\n        score_bin_sort['bad'] = (rt_dat[sn][('N','test')]*rt_dat[sn][('badprob','test')]+rt_dat[sn][('N','train')]*rt_dat[sn][('badprob','train')]).map(lambda x: int(x))\n        score_bin_sort['bad rate%'] = (score_bin_sort['bad']/score_bin_sort['total']*100).map(lambda x: int(x))\n        score_bin_sort['bin'] = score_bin_sort.index.values\n        score_bin_sort = score_bin_sort[['bin','score','total','good','bad','bad rate%']]\n    return score_bin_sort\n\n\n## Score Distribution ##\ndef plot_score_distribution(score1,score2,label1,label2,title):\n    sns.distplot(score1, color=\"darkturquoise\", label=label1)\n    sns.distplot(score2, color=\"tomato\", label=label2)\n    plt.legend(labels=[label1, label2])\n    plt.title(title)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"scorecard\"></a> <br>\n# **8. Functions for Scorecard**\n- Coefficients in scorecard\n- Creating a scorecard\n- Apply to scorecard"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Coefficients in scorecard ##\ndef ab(points0=600, odds0=1/19, pdo=50):\n    # sigmoid function\n    # library(ggplot2)\n    # ggplot(data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = function(x) 1/(1+exp(-x)))\n  \n    # log_odds function\n    # ggplot(data.frame(x = c(0, 1)), aes(x)) + stat_function(fun = function(x) log(x/(1-x)))\n  \n    # logistic function\n    # p(y=1) = 1/(1+exp(-z)),\n        # z = beta0+beta1*x1+...+betar*xr = beta*x\n    ##==> z = log(p/(1-p)),\n        # odds = p/(1-p) # bad/good <==>\n        # p = odds/1+odds\n    ##==> z = log(odds)\n    ##==> score = a - b*log(odds)\n  \n    # two hypothesis\n    # points0 = a - b*log(odds0)\n    # points0 - PDO = a - b*log(2*odds0)\n    # if pdo > 0:\n    #     b = pdo/np.log(2)\n    # else:\n    #     b = -pdo/np.log(2)\n    b = pdo/np.log(2)\n    a = points0 + b*np.log(odds0) #log(odds0/(1+odds0))\n    return {'a':a, 'b':b}\n\n\n## Creating a scorecard ##\ndef scorecard(bins, model, xcolumns, points0=600, odds0=1/19, pdo=50, basepoints_eq0=False, digits=0):\n    '''\n    Creating a Scorecard\n    ------\n    `scorecard` creates a scorecard based on the results from `woebin` and LogisticRegression of sklearn.linear_model\n    \n    Params\n    ------\n    bins:Binning information generated from `woebin` function.\n    model:A LogisticRegression model object.\n    points0:Target points, default 600.\n    odds0:Target odds, default 1/19. Odds = p/(1-p).\n    pdo:Points to Double the Odds, default 50.\n    basepoints_eq0:Logical, default is FALSE. If it is TRUE, the basepoints will equally distribute to each variable.\n    digits:The number of digits after the decimal point for points calculation. Default 0.\n    \n    Returns\n    ------\n    DataFrame of scorecard\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)    \n\n    # filter variable via missing rate, iv, identical value rate\n    dt_sel = sc.var_filter(data_gc, \"Risk\")\n    \n    # woe binning ------\n    bins = sc.woebin(dt_sel, \"Risk\")\n    dt_woe = sc.woebin_ply(dt_sel, bins)\n    \n    y = dt_woe.loc[:,'Risk']\n    X = dt_woe.loc[:,dt_woe.columns != 'Risk']\n    \n    # logistic regression ------\n    from sklearn.linear_model import LogisticRegression\n    lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')\n    lr.fit(X, y)\n    \n    # # predicted proability\n    # dt_pred = lr.predict_proba(X)[:,1]\n    # # performace\n    # # ks & roc plot\n    # sc.perf_eva(y, dt_pred)\n    \n    # scorecard\n    # Example I # creat a scorecard\n    card = sc.scorecard(bins, lr, X.columns)\n    \n    # credit score\n    # Example I # only total score\n    score1 = sc.scorecard_ply(dt_sel, card)\n    # Example II # credit score for both total and each variable\n    score2 = sc.scorecard_ply(dt_sel, card, only_total_score = False)\n    '''\n    # coefficients\n    aabb = ab(points0, odds0, pdo)\n    a = aabb['a'] \n    b = aabb['b']\n    # odds = pred/(1-pred); score = a - b*log(odds)\n    \n    # bins # if (is.list(bins)) rbindlist(bins)\n    if isinstance(bins, dict):\n        bins = pd.concat(bins, ignore_index=True)\n    xs = [re.sub('_woe$', '', i) for i in xcolumns]\n    # coefficients\n    coef_df = pd.Series(model.coef_[0], index=np.array(xs))\\\n      .loc[lambda x: x != 0]#.reset_index(drop=True)\n    \n    # scorecard\n    len_x = len(coef_df)\n    basepoints = a - b*model.intercept_[0]\n    card = {}\n    if basepoints_eq0:\n        card['basepoints'] = pd.DataFrame({'variable':\"basepoints\", 'bin':np.nan, 'points':0}, index=np.arange(1))\n        for i in coef_df.index:\n            card[i] = bins.loc[bins['variable']==i,['variable', 'bin', 'woe']]\\\n              .assign(points = lambda x: round(-b*x['woe']*coef_df[i] + basepoints/len_x), ndigits=digits)\\\n              [[\"variable\", \"bin\", \"points\",\"woe\"]]\n    else:\n        card['basepoints'] = pd.DataFrame({'variable':\"basepoints\", 'bin':np.nan, 'points':round(basepoints, ndigits=digits)}, index=np.arange(1))\n        for i in coef_df.index:\n            card[i] = bins.loc[bins['variable']==i,['variable', 'bin', 'woe']]\\\n              .assign(points = lambda x: round(-b*x['woe']*coef_df[i]), ndigits=digits)\\\n              [[\"variable\", \"bin\", \"points\",\"woe\"]]\n    card_print=pd.DataFrame()\n    for i in card.keys():\n        card_print=pd.concat([card_print,card[i]],axis=0)\n    return card_print\n\n\n## Apply to Scorecard ##\ndef scorecard_ply(dt, card, only_total_score=True, print_step=0, replace_blank_na=True, var_kp = None):\n    '''\n    Score Transformation\n    ------\n    `scorecard_ply` calculates credit score using the results from `scorecard`.\n    \n    Params\n    ------\n    dt:Original data\n    card:Scorecard generated from `scorecard`.\n    only_total_score:Logical, default is TRUE. If it is TRUE, then the output includes only total credit score; Otherwise, if it is FALSE, the output includes both total and each variable's credit score.\n    print_step:A non-negative integer. Default is 1. If print_step>0, print variable names by each print_step-th iteration. If print_step=0, no message is print.\n    replace_blank_na:Logical. Replace blank values with NA. Defaults to True. This parameter should be the same with woebin's.\n    var_kp:Name of force kept variables, such as id column. Defaults to None.\n    \n    Return\n    ------\n    DataFrame of Credit score\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)    \n\n    # filter variable via missing rate, iv, identical value rate\n    dt_sel = sc.var_filter(dat, \"Risk\")\n    \n    # woe binning ------\n    bins = sc.woebin(dt_sel, \"Risk\")\n    dt_woe = sc.woebin_ply(dt_sel, bins)\n    \n    y = dt_woe.loc[:,'Risk']\n    X = dt_woe.loc[:,dt_woe.columns != 'Risk']\n    \n    # logistic regression ------\n    from sklearn.linear_model import LogisticRegression\n    lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')\n    lr.fit(X, y)\n    \n    # # predicted proability\n    # dt_pred = lr.predict_proba(X)[:,1]\n    # # performace\n    # # ks & roc plot\n    # sc.perf_eva(y, dt_pred)\n    \n    # scorecard\n    # Example I # creat a scorecard\n    card = sc.scorecard(bins, lr, X.columns)\n    \n    # credit score\n    # Example I # only total score\n    score1 = sc.scorecard_ply(dt_sel, card)\n    # Example II # credit score for both total and each variable\n    score2 = sc.scorecard_ply(dt_sel, card, only_total_score = False)\n    '''\n  \n    dt = dt.copy(deep=True)\n    # remove date/time col\n    # dt = rmcol_datetime_unique1(dt)\n    # replace \"\" by NA\n    if replace_blank_na: dt = rep_blank_na(dt)\n    # print_step\n    print_step = check_print_step(print_step)\n    # card # if (is.list(card)) rbindlist(card)\n    if isinstance(card, dict):\n        card_df = pd.concat(card, ignore_index=True)\n    elif isinstance(card, pd.DataFrame):\n        card_df = card.copy(deep=True)\n    # x variables\n    xs = card_df.loc[card_df.variable != 'basepoints', 'variable'].unique()\n    # length of x variables\n    xs_len = len(xs)\n    # initial datasets\n    dat = dt.loc[:,list(set(dt.columns)-set(xs))]\n    \n    # loop on x variables\n    for i in np.arange(xs_len):\n        x_i = xs[i]\n        # print xs\n        if print_step>0 and bool((i+1)%print_step): \n            print(('{:'+str(len(str(xs_len)))+'.0f}/{} {}').format(i, xs_len, x_i))\n        \n        cardx = card_df.loc[card_df['variable']==x_i]\n        dtx = dt[[x_i]]\n        # score transformation\n        dtx_points = woepoints_ply1(dtx, cardx, x_i, woe_points=\"points\")\n        dat = pd.concat([dat, dtx_points], axis=1)\n    \n    # set basepoints\n    card_basepoints = list(card_df.loc[card_df['variable']=='basepoints','points'])[0] if 'basepoints' in card_df['variable'].unique() else 0\n    # total score\n    dat_score = dat[xs+'_points']\n    dat_score.loc[:,'score'] = card_basepoints + dat_score.sum(axis=1)\n    # dat_score = dat_score.assign(score = lambda x: card_basepoints + dat_score.sum(axis=1))\n    # return\n    if only_total_score: dat_score = dat_score[['score']]\n    \n    # check force kept variables\n    if var_kp is not None:\n        if isinstance(var_kp, str):\n            var_kp = [var_kp]\n        var_kp2 = list(set(var_kp) & set(list(dt)))\n        len_diff_var_kp = len(var_kp) - len(var_kp2)\n        if len_diff_var_kp > 0:\n            warnings.warn(\"Incorrect inputs; there are {} var_kp variables are not exist in input data, which are removed from var_kp. \\n {}\".format(len_diff_var_kp, list(set(var_kp)-set(var_kp2))) )\n        var_kp = var_kp2 if len(var_kp2)>0 else None\n    if var_kp is not None: dat_score = pd.concat([dt[var_kp], dat_score], axis = 1)\n    return dat_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"split_dataset\"></a> <br>\n# **9. Functions for Splitting Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Split Dataset ##\ndef split_df(dt, y=None, ratio=0.7, seed=186):\n    \n    '''\n    Split a dataset\n    ------\n    Split a dataset into train and test\n    \n    Params\n    ------\n    dt:A data frame.\n    y:Name of y variable, default is NULL. The input data will split based on the predictor y, if it is provide.\n    ratio:A numeric value, default is 0.7. It indicates the ratio of total rows contained in one split, must less than 1.\n    seed:A random seed, default is 186.\n    \n    Returns\n    ------\n    dict: a dictionary of train and test\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)    \n\n    # split train and test\n    train, test = sc.split_df(data_gc, 'Risk').values()\n    \n    # set ratios for both train and test\n    train2, test2 = sc.split_df(data_gc, 'Risk', ratio = [0.5, 0.2]).values()\n    \n    '''\n    dt = dt.copy(deep=True)\n    # remove date/time col\n    # dt = rmcol_datetime_unique1(dt)\n    # replace \"\" by NA\n    dt = rep_blank_na(dt)\n    \n    # set ratio range\n    if isinstance(ratio, float): \n        ratio = [ratio]\n    if not all(isinstance(i, float) for i in ratio) or len(ratio)>2 or sum(ratio)>1:\n        warnings.warn(\"Incorrect inputs; ratio must be a numeric that length equal to 1 and less than 1. It was set to 0.7.\")\n        ratio = [0.7, 0.3]\n    else:\n        ratio_ = 1.0-sum(ratio)\n        if (ratio_ > 0): ratio = ratio + [ratio_]\n    # split into train and test\n    if y is None:\n        train = dt.sample(frac=ratio[0], random_state=seed).sort_index()\n        test = dt.loc[list(set(dt.index.tolist())-set(train.index.tolist()))].sort_index()\n    else:\n        train = dt.groupby(y, group_keys=False)\\\n          .apply(lambda x: x.sample(frac=ratio[0], random_state=seed))\\\n          .sort_index()\n        test = dt.loc[list(set(dt.index.tolist())-set(train.index.tolist()))].sort_index()\n        if len(ratio) == 3:\n            test = test.groupby(y, group_keys=False)\\\n                .apply(lambda x: x.sample(frac=ratio[1]/sum(ratio[1:]), random_state=seed))\\\n                .sort_index()\n    # return\n    rt = OrderedDict()\n    rt['train'] = train\n    rt['test'] = test\n    return rt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"variable_filter\"></a> <br>\n# **10. Functions for Variable Filter**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n## Variable Filter ##\ndef var_filter(dt, y, x=None, iv_limit=0.02, missing_limit=0.95, \n               identical_limit=0.95, var_rm=None, var_kp=None, \n               return_rm_reason=False, positive='bad|1'):\n    '''\n    Variable Filter\n    ------\n    This function filter variables base on specified conditions, such as information value, missing rate, identical value rate.\n    \n    Params\n    ------\n    dt:A data frame with both x (predictor/feature) and y (response/label) variables.\n    y:Name of y variable.\n    x:Name of x variables. Default is NULL. If x is NULL, then all variables except y are counted as x variables.\n    iv_limit:The information value of kept variables should>=iv_limit. The default is 0.02.\n    missing_limit:The missing rate of kept variables should<=missing_limit. The default is 0.95.\n    identical_limit:The identical value rate (excluding NAs) of kept variables should <= identical_limit. The default is 0.95.\n    var_rm:Name of force removed variables, default is NULL.\n    var_kp:Name of force kept variables, default is NULL.\n    return_rm_reason:Logical, default is FALSE.\n    positive:Value of positive class, default is \"bad|1\".\n    \n    Returns\n    ------\n    DataFrame\n        A data.table with y and selected x variables\n    Dict(if return_rm_reason == TRUE)\n        A DataFrame with y and selected x variables and a DataFrame with the reason of removed x variable.\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)    \n\n    # variable filter\n    dt_sel = sc.var_filter(dat, y = \"Risk\")\n    '''\n    # start time\n    start_time = time.time()\n    print('[INFO] filtering variables ...')\n    \n    dt = dt.copy(deep=True)\n    if isinstance(y, str):\n        y = [y]\n    if isinstance(x, str) and x is not None:\n        x = [x]\n    if x is not None: \n        dt = dt[y+x]\n    # remove date/time col\n#    dt = rmcol_datetime_unique1(dt)\n    # replace \"\" by NA\n#    dt = rep_blank_na(dt)\n    # check y\n    dt = check_y(dt, y, positive)\n    # x variable names\n    x = x_variable(dt,y,x)\n    \n    # force removed variables\n    if var_rm is not None: \n        if isinstance(var_rm, str):\n            var_rm = [var_rm]\n        x = list(set(x).difference(set(var_rm)))\n    # check force kept variables\n    if var_kp is not None:\n        if isinstance(var_kp, str):\n            var_kp = [var_kp]\n        var_kp2 = list(set(var_kp) & set(x))\n        len_diff_var_kp = len(var_kp) - len(var_kp2)\n        if len_diff_var_kp > 0:\n            warnings.warn(\"Incorrect inputs; there are {} var_kp variables are not exist in input data, which are removed from var_kp. \\n {}\".format(len_diff_var_kp, list(set(var_kp)-set(var_kp2))) )\n        var_kp = var_kp2 if len(var_kp2)>0 else None\n  \n    # -iv\n    iv_list = iv(dt, y, x, order=False)\n    # -na percentage\n    nan_rate = lambda a: a[a.isnull()].size/a.size\n    na_perc = dt[x].apply(nan_rate).reset_index(name='missing_rate').rename(columns={'index':'variable'})\n    # -identical percentage\n    idt_rate = lambda a: a.value_counts().max() / a.size\n    identical_perc = dt[x].apply(idt_rate).reset_index(name='identical_rate').rename(columns={'index':'variable'})\n    \n    # dataframe iv na idt\n    dt_var_selector = iv_list.merge(na_perc,on='variable').merge(identical_perc,on='variable')\n    # remove na_perc>95 | ele_perc>0.95 | iv<0.02\n    # variable datatable selected\n    dt_var_sel = dt_var_selector.query('(info_value >= {}) & (missing_rate <= {}) & (identical_rate <= {})'.format(iv_limit,missing_limit,identical_limit))\n    \n    # add kept variable\n    x_selected = dt_var_sel.variable.tolist()\n    if var_kp is not None: \n        x_selected = np.unique(x_selected+var_kp).tolist()\n    # data kept\n    dt_kp = dt[x_selected+y]\n    \n    # runingtime\n    runingtime = time.time() - start_time\n    if (runingtime >= 10):\n        # print(time.strftime(\"%H:%M:%S\", time.gmtime(runingtime)))\n        print('Variable filtering on {} rows and {} columns in {} \\n{} variables are removed'.format(dt.shape[0], dt.shape[1], time.strftime(\"%H:%M:%S\", time.gmtime(runingtime)), dt.shape[1]-len(x_selected+y)))\n    # return remove reason\n    if return_rm_reason:\n        dt_var_rm = dt_var_selector.query('(info_value < {}) | (missing_rate > {}) | (identical_rate > {})'.format(iv_limit,missing_limit,identical_limit)) \\\n          .assign(\n            info_value = lambda x: ['info_value<{}'.format(iv_limit) if i else np.nan for i in (x.info_value < iv_limit)], \n            missing_rate = lambda x: ['missing_rate>{}'.format(missing_limit) if i else np.nan for i in (x.missing_rate > missing_limit)],\n            identical_rate = lambda x: ['identical_rate>{}'.format(identical_limit) if i else np.nan for i in (x.identical_rate > identical_limit)]\n          )\n        dt_rm_reason = pd.melt(dt_var_rm, id_vars=['variable'], var_name='iv_mr_ir').dropna()\\\n        .groupby('variable').apply(lambda x: ', '.join(x.value)).reset_index(name='rm_reason')\n        \n        if var_rm is not None: \n            dt_rm_reason = pd.concat([\n              dt_rm_reason, \n              pd.DataFrame({'variable':var_rm,'rm_reason':\"force remove\"}, columns=['variable', 'rm_reason'])\n            ])\n        if var_kp is not None:\n            dt_rm_reason = dt_rm_reason.query('variable not in {}'.format(var_kp))\n        \n        dt_rm_reason = pd.merge(dt_rm_reason, dt_var_selector, how='outer', on = 'variable')\n        return {'dt': dt_kp, 'rm':dt_rm_reason}\n    else:\n        return dt_kp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"WOE\"></a> <br>\n# **11. Functions for WOE Binning**\n- Converting vector (breaks & special_values) to dataframe\n- Add missing value to special value\n- Count number of good or bad in y\n- Split dataframe into binning dataframe with special values and datafram without speical_values\n- Check empty bins for unnumeric variable\n- Binning based on given breaks\n- Pretty breakpoints\n- Return initial binning\n- Add 1 best break for tree-like binning\n- Return tree-like binning\n- Return chimerge binning\n- Format binning output\n- WOE Binning for only two columns\n- Get bins breaklist\n- Generate Optimal Binning\n- Transform original value to woe values for one variable\n- Transform original value to woe values for a dataframe\n- Plot binning of one variable\n- WOE Binning Visualization\n- Print basic information in woebinning adjusting process\n- Plot adjusted binning in woebinning adjustment process\n- WOE Binning Adjustment"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converting vector (breaks & special_values) to dataframe ##\ndef split_vec_todf(vec):\n    '''\n    Create a dataframe based on provided vector. \n    Split the rows that including '%,%' into multiple rows. \n    Replace 'missing' by np.nan.\n    \n    Params\n    ------\n    vec:list\n    \n    Returns\n    ------\n    pandas.DataFrame\n        returns a dataframe with three columns\n        {'bin_chr':orginal vec, 'rowid':index of vec, 'value':splited vec}\n    '''\n    if vec is not None:\n        vec = [str(i) for i in vec]\n        a = pd.DataFrame({'bin_chr':vec}).assign(rowid=lambda x:x.index)\n        b = pd.DataFrame([i.split('%,%') for i in vec], index=vec)\\\n        .stack().replace('missing', np.nan) \\\n        .reset_index(name='value')\\\n        .rename(columns={'level_0':'bin_chr'})[['bin_chr','value']]\n        # return\n        return pd.merge(a,b,on='bin_chr')\n\n\n## Add missing value to special value ##\ndef add_missing_spl_val(dtm, breaks, spl_val):\n    '''\n    add missing to spl_val if there is nan in dtm.value and \n    missing is not specified in breaks and spl_val\n    \n    Params\n    ------\n    dtm:melt dataframe\n    breaks:breaks list\n    spl_val:speical values list\n    \n    Returns\n    ------\n    list\n        returns spl_val list\n    '''\n    if dtm.value.isnull().any():\n        if breaks is None:\n            if spl_val is None:\n                spl_val=['missing']\n            elif any([('missing' in str(i)) for i in spl_val]):\n                spl_val=spl_val\n            else:\n                spl_val=['missing']+spl_val\n        elif any([('missing' in str(i)) for i in breaks]):\n            spl_val=spl_val\n        else:\n            if spl_val is None:\n                spl_val=['missing']\n            elif any([('missing' in str(i)) for i in spl_val]):\n                spl_val=spl_val\n            else:\n                spl_val=['missing']+spl_val\n    # return\n    return spl_val\n\n\n## Count number of good or bad in y ##\ndef n0(x): return sum(x==0)\ndef n1(x): return sum(x==1)\n\n\n## Split dtm into bin_sv and dtm (without speical_values) ##\ndef dtm_binning_sv(dtm, breaks, spl_val):\n    '''\n    Split the orginal dtm (melt dataframe) into \n    binning_sv (binning of special_values) and \n    a new dtm (without special_values).\n    \n    Params\n    ------\n    dtm:melt dataframe\n    spl_val:speical values list\n    \n    Returns\n    ------\n    list\n        returns a list with binning_sv and dtm\n    '''\n    spl_val = add_missing_spl_val(dtm, breaks, spl_val)\n    if spl_val is not None:\n        # special_values from vector to dataframe\n        sv_df = split_vec_todf(spl_val)\n        # value \n        if is_numeric_dtype(dtm['value']):\n            sv_df['value'] = sv_df['value'].astype(dtm['value'].dtypes)\n            # sv_df['bin_chr'] = sv_df['bin_chr'].astype(dtm['value'].dtypes).astype(str)\n            sv_df['bin_chr'] = np.where(\n              np.isnan(sv_df['value']), sv_df['bin_chr'], \n              sv_df['value'].astype(dtm['value'].dtypes).astype(str))\n            # sv_df = sv_df.assign(value = lambda x: x.value.astype(dtm['value'].dtypes))\n        # dtm_sv & dtm\n        dtm_sv = pd.merge(dtm.fillna(\"missing\"), sv_df[['value']].fillna(\"missing\"), how='inner', on='value', right_index=True)\n        dtm = dtm[~dtm.index.isin(dtm_sv.index)].reset_index() if len(dtm_sv.index) < len(dtm.index) else None\n        # dtm_sv = dtm.query('value in {}'.format(sv_df['value'].tolist()))\n        # dtm    = dtm.query('value not in {}'.format(sv_df['value'].tolist()))\n        \n        if dtm_sv.shape[0] == 0:\n            return {'binning_sv':None, 'dtm':dtm}\n        # binning_sv\n        binning_sv = pd.merge(\n          dtm_sv.fillna('missing').groupby(['variable','value'])['y'].agg([n0, n1])\\\n          .reset_index().rename(columns={'n0':'good','n1':'bad'}),\n          sv_df.fillna('missing'), \n          on='value'\n        ).groupby(['variable', 'rowid', 'bin_chr']).agg({'bad':sum,'good':sum})\\\n        .reset_index().rename(columns={'bin_chr':'bin'})\\\n        .drop('rowid', axis=1)\n    else:\n        binning_sv = None\n    # return\n    return {'binning_sv':binning_sv, 'dtm':dtm}\n    \n\n## Check empty bins for unnumeric variable ##\ndef check_empty_bins(dtm, binning):\n    # check empty bins\n    bin_list = np.unique(dtm.bin.astype(str)).tolist()\n    if 'nan' in bin_list: \n        bin_list.remove('nan')\n    binleft = set([re.match(r'\\[(.+),(.+)\\)', i).group(1) for i in bin_list]).difference(set(['-inf', 'inf']))\n    binright = set([re.match(r'\\[(.+),(.+)\\)', i).group(2) for i in bin_list]).difference(set(['-inf', 'inf']))\n    if binleft != binright:\n        bstbrks = sorted(list(map(float, ['-inf'] + list(binright) + ['inf'])))\n        labels = ['[{},{})'.format(bstbrks[i], bstbrks[i+1]) for i in range(len(bstbrks)-1)]\n        dtm.loc[:,'bin'] = pd.cut(dtm['value'], bstbrks, right=False, labels=labels)\n        binning = dtm.groupby(['variable','bin'])['y'].agg([n0, n1])\\\n          .reset_index().rename(columns={'n0':'good','n1':'bad'})\n        # warnings.warn(\"The break points are modified into '[{}]'. There are empty bins based on the provided break points.\".format(','.join(binright)))\n        # binning\n        # dtm['bin'] = dtm['bin'].astype(str)\n    # return\n    return binning\n\n\n## Binning based on given breaks ## - required in woebin2 \ndef woebin2_breaks(dtm, breaks, spl_val):\n    '''\n    get binning if breaks is provided\n    \n    Params\n    ------\n    dtm:melt dataframe\n    breaks:breaks list\n    spl_val:speical values list\n    \n    Returns\n    ------\n    DataFrame\n        returns a binning datafram\n    '''\n    \n    # breaks from vector to dataframe\n    bk_df = split_vec_todf(breaks)\n    # dtm $ binning_sv\n    dtm_binsv_list = dtm_binning_sv(dtm, breaks, spl_val)\n    dtm = dtm_binsv_list['dtm']\n    binning_sv = dtm_binsv_list['binning_sv']\n    if dtm is None: return {'binning_sv':binning_sv, 'binning':None}\n    \n    # binning\n    if is_numeric_dtype(dtm['value']):\n        # best breaks\n        bstbrks = ['-inf'] + list(set(bk_df.value.tolist()).difference(set([np.nan, '-inf', 'inf', 'Inf', '-Inf']))) + ['inf']\n        bstbrks = sorted(list(map(float, bstbrks)))\n        # cut\n        labels = ['[{},{})'.format(bstbrks[i], bstbrks[i+1]) for i in range(len(bstbrks)-1)]\n        dtm.loc[:,'bin'] = pd.cut(dtm['value'], bstbrks, right=False, labels=labels)\n        dtm['bin'] = dtm['bin'].astype(str)\n        \n        binning = dtm.groupby(['variable','bin'])['y'].agg([n0, n1])\\\n          .reset_index().rename(columns={'n0':'good','n1':'bad'})\n        # check empty bins for unmeric variable\n        binning = check_empty_bins(dtm, binning)\n        \n        # sort bin\n        binning = pd.merge(\n          binning.assign(value=lambda x: [float(re.search(r\"^\\[(.*),(.*)\\)\", i).group(2)) if i != 'nan' else np.nan for i in binning['bin']] ),\n          bk_df.assign(value=lambda x: x.value.astype(float)), \n          how='left',on='value'\n        ).sort_values(by=\"rowid\").reset_index(drop=True)\n        # merge binning and bk_df if nan isin value\n        if bk_df['value'].isnull().any():\n            binning = binning.assign(bin=lambda x: [i if i != 'nan' else 'missing' for i in x['bin']])\\\n              .fillna('missing').groupby(['variable','rowid'])\\\n              .agg({'bin':lambda x: '%,%'.join(x), 'good':sum, 'bad':sum})\\\n              .reset_index()\n    else:\n        # merge binning with bk_df\n        binning = pd.merge(\n          dtm, \n          bk_df.assign(bin=lambda x: x.bin_chr),\n          how='left', on='value'\n        ).fillna('missing').groupby(['variable', 'rowid', 'bin'])['y'].agg([n0,n1])\\\n        .rename(columns={'n0':'good','n1':'bad'})\\\n        .reset_index().drop('rowid', axis=1)\n    # return\n    return {'binning_sv':binning_sv, 'binning':binning}\n    \n\n## Pretty breakpoints ## - required in woebin2_init_bin\ndef pretty(low, high, n):\n    '''\n    pretty breakpoints, the same as pretty function in R\n    \n    Params\n    ------\n    low:minimal value \n    low:maximal value \n    n:number of intervals\n    \n    Returns\n    ------\n    numpy.ndarray\n        returns a breakpoints array\n    '''\n    # nicenumber\n    def nicenumber(x):\n        exp = np.trunc(np.log10(abs(x)))\n        f   = abs(x) / 10**exp\n        if f < 1.5:\n            nf = 1.\n        elif f < 3.:\n            nf = 2.\n        elif f < 7.:\n            nf = 5.\n        else:\n            nf = 10.\n        return np.sign(x) * nf * 10.**exp\n    # pretty breakpoints\n    d     = abs(nicenumber((high-low)/(n-1)))\n    miny  = np.floor(low  / d) * d\n    maxy  = np.ceil (high / d) * d\n    return np.arange(miny, maxy+0.5*d, d)\n\n\n## Return initial binning ## - required in woebin2 \ndef woebin2_init_bin(dtm, init_count_distr, breaks, spl_val):\n    '''\n    initial binning\n    \n    Params\n    ------\n    dtm:melt dataframe\n    init_count_distr:the minimal precentage in the fine binning process\n    breaks:breaks\n    spl_val:speical values list\n    \n    Returns\n    ------\n    dict\n        returns a dict with initial binning and special_value binning\n    '''\n    # dtm $ binning_sv\n    dtm_binsv_list = dtm_binning_sv(dtm, breaks, spl_val)\n    dtm = dtm_binsv_list['dtm']\n    binning_sv = dtm_binsv_list['binning_sv']\n    if dtm is None: return {'binning_sv':binning_sv, 'initial_binning':None}\n    # binning\n    if is_numeric_dtype(dtm['value']): # numeric variable\n        xvalue = dtm['value'].astype(float)\n        # breaks vector & outlier\n        iq = xvalue.quantile([0.01, 0.25, 0.75, 0.99])\n        iqr = iq[0.75] - iq[0.25]\n        if iqr == 0:\n          prob_down = 0.01\n          prob_up = 0.99\n        else:\n          prob_down = 0.25\n          prob_up = 0.75\n        xvalue_rm_outlier = xvalue[(xvalue >= iq[prob_down]-3*iqr) & (xvalue <= iq[prob_up]+3*iqr)]\n        # number of initial binning\n        n = np.trunc(1/init_count_distr)\n        len_uniq_x = len(np.unique(xvalue_rm_outlier))\n        if len_uniq_x < n: n = len_uniq_x\n        # initial breaks\n        brk = np.unique(xvalue_rm_outlier) if len_uniq_x < 10 else pretty(min(xvalue_rm_outlier), max(xvalue_rm_outlier), n)\n        \n        brk = list(filter(lambda x: x>np.nanmin(xvalue) and x<=np.nanmax(xvalue), brk))\n        brk = [float('-inf')] + sorted(brk) + [float('inf')]\n        # initial binning datatable\n        # cut\n        labels = ['[{},{})'.format(brk[i], brk[i+1]) for i in range(len(brk)-1)]\n        dtm.loc[:,'bin'] = pd.cut(dtm['value'], brk, right=False, labels=labels)#.astype(str)\n        # init_bin\n        init_bin = dtm.groupby('bin')['y'].agg([n0, n1])\\\n        .reset_index().rename(columns={'n0':'good','n1':'bad'})\n        # check empty bins for unmeric variable\n        init_bin = check_empty_bins(dtm, init_bin)\n        \n        init_bin = init_bin.assign(\n          variable = dtm['variable'].values[0],\n          brkp = lambda x: [float(re.match('^\\[(.*),.+', i).group(1)) for i in x['bin']],\n          badprob = lambda x: x['bad']/(x['bad']+x['good'])\n        )[['variable', 'bin', 'brkp', 'good', 'bad', 'badprob']]\n    else: # other type variable\n        # initial binning datatable\n        init_bin = dtm.groupby('value')['y'].agg([n0,n1])\\\n        .rename(columns={'n0':'good','n1':'bad'})\\\n        .assign(\n          variable = dtm['variable'].values[0],\n          badprob = lambda x: x['bad']/(x['bad']+x['good'])\n        ).reset_index()\n        # order by badprob if is.character\n        if dtm.value.dtype.name not in ['category', 'bool']:\n            init_bin = init_bin.sort_values(by='badprob').reset_index()\n        # add index as brkp column\n        init_bin = init_bin.assign(brkp = lambda x: x.index)\\\n            [['variable', 'value', 'brkp', 'good', 'bad', 'badprob']]\\\n            .rename(columns={'value':'bin'})\n    \n    # remove brkp that good == 0 or bad == 0 ------\n    while len(init_bin.query('(good==0) or (bad==0)')) > 0:\n        # brkp needs to be removed if good==0 or bad==0\n        rm_brkp = init_bin.assign(count = lambda x: x['good']+x['bad'])\\\n        .assign(\n          count_lag  = lambda x: x['count'].shift(1).fillna(len(dtm)+1),\n          count_lead = lambda x: x['count'].shift(-1).fillna(len(dtm)+1)\n        ).assign(merge_tolead = lambda x: x['count_lag'] > x['count_lead'])\\\n        .query('(good==0) or (bad==0)')\\\n        .query('count == count.min()').iloc[0,]\n        # set brkp to lead's or lag's\n        shift_period = -1 if rm_brkp['merge_tolead'] else 1\n        init_bin = init_bin.assign(brkp2  = lambda x: x['brkp'].shift(shift_period))\\\n        .assign(brkp = lambda x:np.where(x['brkp'] == rm_brkp['brkp'], x['brkp2'], x['brkp']))\n        # groupby brkp\n        init_bin = init_bin.groupby('brkp').agg({\n          'variable':lambda x: np.unique(x),\n          'bin': lambda x: '%,%'.join(x),\n          'good': sum,\n          'bad': sum\n        }).assign(badprob = lambda x: x['bad']/(x['good']+x['bad']))\\\n        .reset_index()\n    # format init_bin\n    if is_numeric_dtype(dtm['value']):\n        init_bin = init_bin\\\n        .assign(bin = lambda x: [re.sub(r'(?<=,).+%,%.+,', '', i) if ('%,%' in i) else i for i in x['bin']])\\\n        .assign(brkp = lambda x: [float(re.match('^\\[(.*),.+', i).group(1)) for i in x['bin']])\n    # return \n    return {'binning_sv':binning_sv, 'initial_binning':init_bin}\n\n\n## Add 1 best break for tree-like binning ## - required in woebin2_tree \ndef woebin2_tree_add_1brkp(dtm, initial_binning, count_distr_limit, bestbreaks=None):\n    '''\n    add a breakpoint into provided bestbreaks\n    \n    Params\n    ------\n    dtm\n    initial_binning\n    count_distr_limit\n    bestbreaks\n    \n    Returns\n    ------\n    DataFrame\n        a binning dataframe with updated breaks\n    '''\n    # dtm removed values in spl_val\n    # total_iv for all best breaks\n    def total_iv_all_breaks(initial_binning, bestbreaks, dtm_rows):\n        # best breaks set\n        breaks_set = set(initial_binning.brkp).difference(set(list(map(float, ['-inf', 'inf']))))\n        if bestbreaks is not None: breaks_set = breaks_set.difference(set(bestbreaks))\n        breaks_set = sorted(breaks_set)\n        # loop on breaks_set\n        init_bin_all_breaks = initial_binning.copy(deep=True)\n        for i in breaks_set:\n            # best break + i\n            bestbreaks_i = [float('-inf')]+sorted(bestbreaks+[i] if bestbreaks is not None else [i])+[float('inf')]\n            # best break datatable\n            labels = ['[{},{})'.format(bestbreaks_i[i], bestbreaks_i[i+1]) for i in range(len(bestbreaks_i)-1)]\n            init_bin_all_breaks.loc[:,'bstbin'+str(i)] = pd.cut(init_bin_all_breaks['brkp'], bestbreaks_i, right=False, labels=labels)#.astype(str)\n        # best break dt\n        total_iv_all_brks = pd.melt(\n          init_bin_all_breaks, id_vars=[\"variable\", \"good\", \"bad\"], var_name='bstbin', \n          value_vars=['bstbin'+str(i) for i in breaks_set])\\\n          .groupby(['variable', 'bstbin', 'value'])\\\n          .agg({'good':sum, 'bad':sum}).reset_index()\\\n          .assign(count=lambda x: x['good']+x['bad'])\n          \n        total_iv_all_brks['count_distr'] = total_iv_all_brks.groupby(['variable', 'bstbin'])\\\n          ['count'].apply(lambda x: x/dtm_rows).reset_index(drop=True)\n        total_iv_all_brks['min_count_distr'] = total_iv_all_brks.groupby(['variable', 'bstbin'])\\\n          ['count_distr'].transform(lambda x: min(x))\n          \n        total_iv_all_brks = total_iv_all_brks\\\n          .assign(bstbin = lambda x: [float(re.sub('^bstbin', '', i)) for i in x['bstbin']] )\\\n          .groupby(['variable','bstbin','min_count_distr'])\\\n          .apply(lambda x: iv_01(x['good'], x['bad'])).reset_index(name='total_iv')\n        # return \n        return total_iv_all_brks\n    # binning add 1best break\n    def binning_add_1bst(initial_binning, bestbreaks):\n        if bestbreaks is None:\n            bestbreaks_inf = [float('-inf'),float('inf')]\n        else:\n            if not is_numeric_dtype(dtm['value']):\n                bestbreaks = [i for i in bestbreaks if int(i) != min(initial_binning.brkp)]\n            bestbreaks_inf = [float('-inf')]+sorted(bestbreaks)+[float('inf')]\n        \n        labels = ['[{},{})'.format(bestbreaks_inf[i], bestbreaks_inf[i+1]) for i in range(len(bestbreaks_inf)-1)]\n        binning_1bst_brk = initial_binning.assign(\n          bstbin = lambda x: pd.cut(x['brkp'], bestbreaks_inf, right=False, labels=labels)\n        )\n        if is_numeric_dtype(dtm['value']):\n            binning_1bst_brk = binning_1bst_brk.groupby(['variable', 'bstbin'])\\\n            .agg({'good':sum, 'bad':sum}).reset_index().assign(bin=lambda x: x['bstbin'])\\\n            [['bstbin', 'variable', 'bin', 'good', 'bad']]\n        else:\n            binning_1bst_brk = binning_1bst_brk.groupby(['variable', 'bstbin'])\\\n            .agg({'good':sum, 'bad':sum, 'bin':lambda x:'%,%'.join(x)}).reset_index()\\\n            [['bstbin', 'variable', 'bin', 'good', 'bad']]\n        # format\n        binning_1bst_brk['total_iv'] = iv_01(binning_1bst_brk.good, binning_1bst_brk.bad)\n        binning_1bst_brk['bstbrkp'] = [float(re.match(\"^\\[(.*),.+\", i).group(1)) for i in binning_1bst_brk['bstbin']]\n        # return\n        return binning_1bst_brk\n    # dtm_rows\n    dtm_rows = len(dtm.index)\n    # total_iv for all best breaks\n    total_iv_all_brks = total_iv_all_breaks(initial_binning, bestbreaks, dtm_rows)\n    # bestbreaks: total_iv == max(total_iv) & min(count_distr) >= count_distr_limit\n    bstbrk_maxiv = total_iv_all_brks.loc[lambda x: x['min_count_distr'] >= count_distr_limit]\n    if len(bstbrk_maxiv.index) > 0:\n        bstbrk_maxiv = bstbrk_maxiv.loc[lambda x: x['total_iv']==max(x['total_iv'])]\n        bstbrk_maxiv = bstbrk_maxiv['bstbin'].tolist()[0]\n    else:\n        bstbrk_maxiv = None\n    # bestbreaks\n    if bstbrk_maxiv is not None:\n        # add 1best break to bestbreaks\n        bestbreaks = bestbreaks+[bstbrk_maxiv] if bestbreaks is not None else [bstbrk_maxiv]\n    # binning add 1best break\n    bin_add_1bst = binning_add_1bst(initial_binning, bestbreaks)\n    # return\n    return bin_add_1bst\n    \n    \n## Return tree-like binning ## -  required in woebin2 \ndef woebin2_tree(dtm, init_count_distr=0.02, count_distr_limit=0.05, \n                 stop_limit=0.1, bin_num_limit=8, breaks=None, spl_val=None):\n    '''\n    binning using tree-like method\n    \n    Params\n    ------\n    dtm:\n    init_count_distr:\n    count_distr_limit:\n    stop_limit:\n    bin_num_limit:\n    breaks:\n    spl_val:\n    \n    Returns\n    ------\n    dict\n        returns a dict with initial binning and special_value binning\n    '''\n    # initial binning\n    bin_list = woebin2_init_bin(dtm, init_count_distr=init_count_distr, breaks=breaks, spl_val=spl_val)\n    initial_binning = bin_list['initial_binning']\n    binning_sv = bin_list['binning_sv']\n    if len(initial_binning.index)==1: \n        return {'binning_sv':binning_sv, 'binning':initial_binning}\n    # initialize parameters\n    len_brks = len(initial_binning.index)\n    bestbreaks = None\n    IVt1 = IVt2 = 1e-10\n    IVchg = 1 ## IV gain ratio\n    step_num = 1\n    # best breaks from three to n+1 bins\n    binning_tree = None\n    while (IVchg >= stop_limit) and (step_num+1 <= min([bin_num_limit, len_brks])):\n        binning_tree = woebin2_tree_add_1brkp(dtm, initial_binning, count_distr_limit, bestbreaks)\n        # best breaks\n        bestbreaks = binning_tree.loc[lambda x: x['bstbrkp'] != float('-inf'), 'bstbrkp'].tolist()\n        # information value\n        IVt2 = binning_tree['total_iv'].tolist()[0]\n        IVchg = IVt2/IVt1-1 ## ratio gain\n        IVt1 = IVt2\n        # step_num\n        step_num = step_num + 1\n    if binning_tree is None: binning_tree = initial_binning\n    # return \n    return {'binning_sv':binning_sv, 'binning':binning_tree}\n    \n    \n# examples\n# import time\n# start = time.time()\n# # binning_dict = woebin2_init_bin(dtm, init_count_distr=0.02, breaks=None, spl_val=None) \n# # woebin2_tree_add_1brkp(dtm, binning_dict['initial_binning'], count_distr_limit=0.05) \n# # woebin2_tree(dtm, binning_dict['initial_binning'], count_distr_limit=0.05)\n# end = time.time()\n# print(end - start)\n\n\n## Return chimerge binning ## - required in woebin2 \ndef woebin2_chimerge(dtm, init_count_distr=0.02, count_distr_limit=0.05, \n                     stop_limit=0.1, bin_num_limit=8, breaks=None, spl_val=None):\n    '''\n    binning using chimerge method\n    \n    Params\n    ------\n    dtm:\n    init_count_distr:\n    count_distr_limit:\n    stop_limit:\n    bin_num_limit:\n    breaks:\n    spl_val:\n    \n    Returns\n    ------\n    dict\n        returns a dict with initial binning and special_value binning\n    '''\n    # [chimerge](http://blog.csdn.net/qunxingvip/article/details/50449376)\n    # [ChiMerge:Discretization of numeric attributs](http://www.aaai.org/Papers/AAAI/1992/AAAI92-019.pdf)\n    # chisq = function(a11, a12, a21, a22) {\n    #   A = list(a1 = c(a11, a12), a2 = c(a21, a22))\n    #   Adf = do.call(rbind, A)\n    #\n    #   Edf =\n    #     matrix(rowSums(Adf), ncol = 1) %*%\n    #     matrix(colSums(Adf), nrow = 1) /\n    #     sum(Adf)\n    #\n    #   sum((Adf-Edf)^2/Edf)\n    # }\n    # function to create a chisq column in initial_binning\n    def add_chisq(initial_binning):\n        chisq_df = pd.melt(initial_binning, \n          id_vars=[\"brkp\", \"variable\", \"bin\"], value_vars=[\"good\", \"bad\"],\n          var_name='goodbad', value_name='a')\\\n        .sort_values(by=['goodbad', 'brkp']).reset_index(drop=True)\n        ###\n        chisq_df['a_lag'] = chisq_df.groupby('goodbad')['a'].apply(lambda x: x.shift(1))#.reset_index(drop=True)\n        chisq_df['a_rowsum'] = chisq_df.groupby('brkp')['a'].transform(lambda x: sum(x))#.reset_index(drop=True)\n        chisq_df['a_lag_rowsum'] = chisq_df.groupby('brkp')['a_lag'].transform(lambda x: sum(x))#.reset_index(drop=True)\n        ###\n        chisq_df = pd.merge(\n          chisq_df.assign(a_colsum = lambda df: df.a+df.a_lag), \n          chisq_df.groupby('brkp').apply(lambda df: sum(df.a+df.a_lag)).reset_index(name='a_sum'))\\\n        .assign(\n          e = lambda df: df.a_rowsum*df.a_colsum/df.a_sum,\n          e_lag = lambda df: df.a_lag_rowsum*df.a_colsum/df.a_sum\n        ).assign(\n          ae = lambda df: (df.a-df.e)**2/df.e + (df.a_lag-df.e_lag)**2/df.e_lag\n        ).groupby('brkp').apply(lambda x: sum(x.ae)).reset_index(name='chisq')\n        # return\n        return pd.merge(initial_binning.assign(count = lambda x: x['good']+x['bad']), chisq_df, how='left')\n    # initial binning\n    bin_list = woebin2_init_bin(dtm, init_count_distr=init_count_distr, breaks=breaks, spl_val=spl_val)\n    initial_binning = bin_list['initial_binning']\n    binning_sv = bin_list['binning_sv']\n    # return initial binning if its row number equals 1\n    if len(initial_binning.index)==1: \n        return {'binning_sv':binning_sv, 'binning':initial_binning}\n\n    # dtm_rows\n    dtm_rows = len(dtm.index)    \n    # chisq limit\n    from scipy.special import chdtri\n    chisq_limit = chdtri(1, stop_limit)\n    # binning with chisq column\n    binning_chisq = add_chisq(initial_binning)\n    \n    # param\n    bin_chisq_min = binning_chisq.chisq.min()\n    bin_count_distr_min = min(binning_chisq['count']/dtm_rows)\n    bin_nrow = len(binning_chisq.index)\n    # remove brkp if chisq < chisq_limit\n    while bin_chisq_min < chisq_limit or bin_count_distr_min < count_distr_limit or bin_nrow > bin_num_limit:\n        # brkp needs to be removed\n        if bin_chisq_min < chisq_limit:\n            rm_brkp = binning_chisq.assign(merge_tolead = False).sort_values(by=['chisq', 'count']).iloc[0,]\n        elif bin_count_distr_min < count_distr_limit:\n            rm_brkp = binning_chisq.assign(\n              count_distr = lambda x: x['count']/sum(x['count']),\n              chisq_lead = lambda x: x['chisq'].shift(-1).fillna(float('inf'))\n            ).assign(merge_tolead = lambda x: x['chisq'] > x['chisq_lead'])\n            # replace merge_tolead as True\n            rm_brkp.loc[np.isnan(rm_brkp['chisq']), 'merge_tolead']=True\n            # order select 1st\n            rm_brkp = rm_brkp.sort_values(by=['count_distr']).iloc[0,]\n        elif bin_nrow > bin_num_limit:\n            rm_brkp = binning_chisq.assign(merge_tolead = False).sort_values(by=['chisq', 'count']).iloc[0,]\n        else:\n            break\n        # set brkp to lead's or lag's\n        shift_period = -1 if rm_brkp['merge_tolead'] else 1\n        binning_chisq = binning_chisq.assign(brkp2  = lambda x: x['brkp'].shift(shift_period))\\\n        .assign(brkp = lambda x:np.where(x['brkp'] == rm_brkp['brkp'], x['brkp2'], x['brkp']))\n        # groupby brkp\n        binning_chisq = binning_chisq.groupby('brkp').agg({\n          'variable':lambda x:np.unique(x),\n          'bin': lambda x: '%,%'.join(x),\n          'good': sum,\n          'bad': sum\n        }).assign(badprob = lambda x: x['bad']/(x['good']+x['bad']))\\\n        .reset_index()\n        # update\n        ## add chisq to new binning dataframe\n        binning_chisq = add_chisq(binning_chisq)\n        ## param\n        bin_nrow = len(binning_chisq.index)\n        if bin_nrow == 1:\n            break\n        bin_chisq_min = binning_chisq.chisq.min()\n        bin_count_distr_min = min(binning_chisq['count']/dtm_rows)\n        \n    # format init_bin # remove (.+\\\\)%,%\\\\[.+,)\n    if is_numeric_dtype(dtm['value']):\n        binning_chisq = binning_chisq\\\n        .assign(bin = lambda x: [re.sub(r'(?<=,).+%,%.+,', '', i) if ('%,%' in i) else i for i in x['bin']])\\\n        .assign(brkp = lambda x: [float(re.match('^\\[(.*),.+', i).group(1)) for i in x['bin']])\n    # return \n    return {'binning_sv':binning_sv, 'binning':binning_chisq}\n     \n     \n## Format binning output ## - required in woebin2 \ndef binning_format(binning):\n    '''\n    format binning dataframe\n    \n    Params\n    ------\n    binning:with columns of variable, bin, good, bad\n    \n    Returns\n    ------\n    DataFrame\n        binning dataframe with columns of 'variable', 'bin', \n        'count', 'count_distr', 'good', 'bad', 'badprob', 'woe', \n        'bin_iv', 'total_iv',  'breaks', 'is_special_values'\n    '''\n    binning['count'] = binning['good'] + binning['bad']\n    binning['count_distr'] = binning['count']/sum(binning['count'])\n    binning['badprob'] = binning['bad']/binning['count']\n    # binning = binning.assign(\n    #   count = lambda x: (x['good']+x['bad']),\n    #   count_distr = lambda x: (x['good']+x['bad'])/sum(x['good']+x['bad']),\n    #   badprob = lambda x: x['bad']/(x['good']+x['bad']))\n    # new columns: woe, iv, breaks, is_sv\n    binning['woe'] = woe_01(binning['good'],binning['bad'])\n    binning['bin_iv'] = miv_01(binning['good'],binning['bad'])\n    binning['total_iv'] = binning['bin_iv'].sum()\n    # breaks\n    binning['breaks'] = binning['bin']\n    if any([r'[' in str(i) for i in binning['bin']]):\n        def re_extract_all(x): \n            gp23 = re.match(r\"^\\[(.*), *(.*)\\)((%,%missing)*)\", x)\n            breaks_string = x if gp23 is None else gp23.group(2)+gp23.group(3)\n            return breaks_string\n        binning['breaks'] = [re_extract_all(i) for i in binning['bin']]\n    # is_sv    \n    binning['is_special_values'] = binning['is_sv']\n    # return\n    return binning[['variable', 'bin', 'count', 'count_distr', 'good', 'bad', 'badprob', 'woe', 'bin_iv', 'total_iv',  'breaks', 'is_special_values']]\n\n\n## WOE Binning for only two columns ##\n# This function provides woe binning for only two columns (one x and one y) dataframe.\ndef woebin2(dtm, breaks=None, spl_val=None, \n            init_count_distr=0.02, count_distr_limit=0.05, \n            stop_limit=0.1, bin_num_limit=8, method=\"tree\"):\n    '''\n    woebin2\n    ------\n    provides woe binning for only two series\n    \n    Params\n    ------\n    \n    Returns\n    ------\n    DataFrame   \n    '''\n    # binning\n    if breaks is not None:\n        # 1.return binning if breaks provided\n        bin_list = woebin2_breaks(dtm=dtm, breaks=breaks, spl_val=spl_val)\n    else:\n        if stop_limit == 'N':\n            # binning of initial & specialvalues\n            bin_list = woebin2_init_bin(dtm, init_count_distr=init_count_distr, breaks=breaks, spl_val=spl_val)\n        else:\n            if method == 'tree':\n                # 2.tree-like optimal binning\n                bin_list = woebin2_tree(\n                  dtm, init_count_distr=init_count_distr, count_distr_limit=count_distr_limit, \n                  stop_limit=stop_limit, bin_num_limit=bin_num_limit, breaks=breaks, spl_val=spl_val)\n            elif method == \"chimerge\":\n                # 2.chimerge optimal binning\n                bin_list = woebin2_chimerge(\n                  dtm, init_count_distr=init_count_distr, count_distr_limit=count_distr_limit, \n                  stop_limit=stop_limit, bin_num_limit=bin_num_limit, breaks=breaks, spl_val=spl_val)\n    # rbind binning_sv and binning\n    binning = pd.concat(bin_list, keys=bin_list.keys()).reset_index()\\\n              .assign(is_sv = lambda x: x.level_0 =='binning_sv')\n    # return\n    return binning_format(binning)\n\n\n## Get bins breaklist ##\ndef bins_to_breaks(bins, dt, to_string=False, save_string=None):\n    if isinstance(bins, dict):\n        bins = pd.concat(bins, ignore_index=True)\n\n    # x variables\n    xs_all = bins['variable'].unique()\n    # dtypes of  variables\n    vars_class = pd.DataFrame({\n      'variable': xs_all,\n      'not_numeric': [not is_numeric_dtype(dt[i]) for i in xs_all]\n    })\n    \n    # breakslist of bins\n    bins_breakslist = bins[~bins['breaks'].isin([\"-inf\",\"inf\",\"missing\"]) & ~bins['is_special_values']]\n    bins_breakslist = pd.merge(bins_breakslist[['variable', 'breaks']], vars_class, how='left', on='variable')\n    bins_breakslist.loc[bins_breakslist['not_numeric'], 'breaks'] = '\\''+bins_breakslist.loc[bins_breakslist['not_numeric'], 'breaks']+'\\''\n    bins_breakslist = bins_breakslist.groupby('variable')['breaks'].agg(lambda x: ','.join(x))\n    \n    if to_string:\n        bins_breakslist = \"breaks_list={\\n\"+', \\n'.join('\\''+bins_breakslist.index[i]+'\\': ['+bins_breakslist[i]+']' for i in np.arange(len(bins_breakslist)))+\"}\"\n        if save_string is not None:\n            brk_lst_name = '{}_{}.py'.format(save_string, time.strftime('%Y%m%d_%H%M%S', time.localtime(time.time())))\n            with open(brk_lst_name, 'w') as f:\n                f.write(bins_breakslist)\n            print('[INFO] The breaks_list is saved as {}'.format(brk_lst_name))\n            return \n    return bins_breakslist\n\n\n## Generate Optimal Binning ##\ndef woebin(dt, y, x=None, \n           var_skip=None, breaks_list=None, special_values=None, \n           stop_limit=0.1, count_distr_limit=0.05, bin_num_limit=8, \n           # min_perc_fine_bin=0.02, min_perc_coarse_bin=0.05, max_num_bin=8, \n           positive=\"bad|1\", no_cores=None, print_step=0, method=\"tree\",\n           ignore_const_cols=True, ignore_datetime_cols=True, \n           check_cate_num=True, replace_blank=True, \n           save_breaks_list=None, **kwargs):\n    '''\n    WOE Binning\n    ------\n    `woebin` generates optimal binning for numerical, factor and categorical \n    variables using methods including tree-like segmentation or chi-square \n    merge. woebin can also customizing breakpoints if the breaks_list or \n    special_values was provided.\n    \n    The default woe is defined as ln(Distr_Bad_i/Distr_Good_i). If you \n    prefer ln(Distr_Good_i/Distr_Bad_i), please set the argument `positive` \n    as negative value, such as '0' or 'good'. If there is a zero frequency \n    class when calculating woe, the zero will replaced by 0.99 to make the \n    woe calculable.\n    \n    Params\n    ------\n    dt:A data frame with both x (predictor/feature) and y (response/label) variables.\n    y:Name of y variable.\n    x:Name of x variables. Default is None. If x is None, then all variables except y are counted as x variables.\n    var_skip:Name of variables that will skip for binning. Defaults to None.\n    breaks_list:List of break points, default is None. If it is not None, variable binning will based on the provided breaks.\n    special_values:the values specified in special_values will be in separate bins. Default is None.\n    count_distr_limit:The minimum percentage of final binning class number over total. Accepted range: 0.01-0.2; default is 0.05.\n    stop_limit:Stop binning segmentation when information value gain ratio less than the stop_limit, or stop binning merge when the minimum of chi-square less than 'qchisq(1-stoplimit, 1)'. \n    Accepted range:0-0.5; default is 0.1.\n    bin_num_limit:Integer. The maximum number of binning.\n    positive:Value of positive class, default \"bad|1\".\n    no_cores:Number of CPU cores for parallel computation. Defaults None. If no_cores is None, the no_cores will set as 1 if length of x variables less than 10, and will set as the number of all CPU cores if the length of x variables greater than or equal to 10.\n    print_step:A non-negative integer. Default is 1. If print_step>0, print variable names by each print_step-th iteration. If print_step=0 or no_cores>1, no message is print.\n    method:Optimal binning method, it should be \"tree\" or \"chimerge\". Default is \"tree\".\n    ignore_const_cols:Logical. Ignore constant columns. Defaults to True.\n    ignore_datetime_cols:Logical. Ignore datetime columns. Defaults to True.\n    check_cate_num:Logical. Check whether the number of unique values in categorical columns larger than 50. It might make the binning process slow if there are too many unique categories. Defaults to True.\n    replace_blank:Logical. Replace blank values with None. Defaults to True.\n    save_breaks_list:The file name to save breaks_list. Default is None.\n    \n    Returns\n    ------\n    dictionary\n        Optimal or customized binning dataframe.\n    \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)\n    \n    # Example I\n    # binning of two variables in germancredit dataset\n    bins_2var = sc.woebin(data_gc, y = \"Risk\", \n      x = [\"Credit amount\", \"purpose\"])\n    \n    # Example II\n    # binning of the germancredit dataset\n    bins_germ = sc.woebin(data_gc, y = \"Risk\")\n    \n    # Example III\n    # customizing the breakpoints of binning\n    dat2 = pd.DataFrame({'Risk':['good','bad']}).sample(50, replace=True)\n    dat_nan = pd.concat([data_gc, dat2], ignore_index=True)\n    \n    breaks_list = {\n      'Age': [26, 35, 37, \"Inf%,%missing\"],\n      'Housing': [\"own\", \"for free%,%rent\"]\n    }\n    special_values = {\n      'Credit amount': [2600, 9960, \"6850%,%missing\"],\n      'Purpose': [\"education\", \"others%,%missing\"]\n    }\n    \n    bins_cus_brk = sc.woebin(dat_nan, y=\"Risk\",\n      x=[\"Age\",\"Credit amount\",\"Housing\",\"Purpose\"],\n      breaks_list=breaks_list, special_values=special_values)\n    '''\n    # start time\n    start_time = time.time()\n    \n    # arguments\n    ## print_info\n    print_info = kwargs.get('print_info', True)\n    ## init_count_distr\n    min_perc_fine_bin = kwargs.get('min_perc_fine_bin', None)\n    init_count_distr = kwargs.get('init_count_distr', min_perc_fine_bin)\n    if init_count_distr is None: init_count_distr = 0.02\n    ## count_distr_limit\n    min_perc_coarse_bin = kwargs.get('min_perc_coarse_bin', None)\n    if min_perc_coarse_bin is not None: count_distr_limit = min_perc_coarse_bin\n    ## bin_num_limit\n    max_num_bin = kwargs.get('max_num_bin', None)\n    if max_num_bin is not None: bin_num_limit = max_num_bin\n    \n    # print infomation\n    if print_info: print('[INFO] creating woe binning ...')\n    \n    dt = dt.copy(deep=True)\n    if isinstance(y, str):\n        y = [y]\n    if isinstance(x, str) and x is not None:\n        x = [x]\n    if x is not None: \n        dt = dt[y+x]\n    # check y\n    dt = check_y(dt, y, positive)\n    # remove constant columns\n    if ignore_const_cols: dt = check_const_cols(dt)\n    # remove date/time col\n    if ignore_datetime_cols: dt = check_datetime_cols(dt)\n    # check categorical columns' unique values\n    if check_cate_num: check_cateCols_uniqueValues(dt, var_skip)\n    # replace black with na\n    if replace_blank: dt = rep_blank_na(dt)\n      \n    # x variable names\n    xs = x_variable(dt, y, x, var_skip)\n    xs_len = len(xs)\n    # print_step\n    print_step = check_print_step(print_step)\n    # breaks_list\n    breaks_list = check_breaks_list(breaks_list, xs)\n    # special_values\n    special_values = check_special_values(special_values, xs)\n    ### ### \n    # stop_limit range\n    if stop_limit<0 or stop_limit>0.5 or not isinstance(stop_limit, (float, int)):\n        warnings.warn(\"Incorrect parameter specification; accepted stop_limit parameter range is 0-0.5. Parameter was set to default (0.1).\")\n        stop_limit = 0.1\n    # init_count_distr range\n    if init_count_distr<0.01 or init_count_distr>0.2 or not isinstance(init_count_distr, (float, int)):\n        warnings.warn(\"Incorrect parameter specification; accepted init_count_distr parameter range is 0.01-0.2. Parameter was set to default (0.02).\")\n        init_count_distr = 0.02\n    # count_distr_limit\n    if count_distr_limit<0.01 or count_distr_limit>0.2 or not isinstance(count_distr_limit, (float, int)):\n        warnings.warn(\"Incorrect parameter specification; accepted count_distr_limit parameter range is 0.01-0.2. Parameter was set to default (0.05).\")\n        count_distr_limit = 0.05\n    # bin_num_limit\n    if not isinstance(bin_num_limit, (float, int)):\n        warnings.warn(\"Incorrect inputs; bin_num_limit should be numeric variable. Parameter was set to default (8).\")\n        bin_num_limit = 8\n    # method\n    if method not in [\"tree\", \"chimerge\"]:\n        warnings.warn(\"Incorrect inputs; method should be tree or chimerge. Parameter was set to default (tree).\")\n        method = \"tree\"\n    ### ### \n    # binning for each x variable\n    # loop on xs\n    if (no_cores is None) or (no_cores < 1):\n        all_cores = mp.cpu_count() - 1\n        no_cores = int(np.ceil(xs_len/5 if xs_len/5 < all_cores else all_cores*0.9))\n    if platform.system() == 'Windows': \n        no_cores = 1\n            \n    # ylist to str\n    y = y[0]\n    # binning for variables\n    if no_cores == 1:\n        # create empty bins dict\n        bins = {}\n        for i in np.arange(xs_len):\n            x_i = xs[i]\n            # print(x_i)\n            # print xs\n            if print_step>0 and bool((i+1)%print_step): \n                print(('{:'+str(len(str(xs_len)))+'.0f}/{} {}').format(i, xs_len, x_i), flush=True)\n            # woebining on one variable\n            bins[x_i] = woebin2(\n              dtm = pd.DataFrame({'y':dt[y], 'variable':x_i, 'value':dt[x_i]}),\n              breaks=breaks_list[x_i] if (breaks_list is not None) and (x_i in breaks_list.keys()) else None,\n              spl_val=special_values[x_i] if (special_values is not None) and (x_i in special_values.keys()) else None,\n              init_count_distr=init_count_distr,\n              count_distr_limit=count_distr_limit,\n              stop_limit=stop_limit, \n              bin_num_limit=bin_num_limit,\n              method=method\n            )\n            # try catch:\n            # \"The variable '{}' caused the error: '{}'\".format(x_i, error-info)\n    else:\n        pool = mp.Pool(processes=no_cores)\n        # arguments\n        args = zip(\n          [pd.DataFrame({'y':dt[y], 'variable':x_i, 'value':dt[x_i]}) for x_i in xs], \n          [breaks_list[i] if (breaks_list is not None) and (i in list(breaks_list.keys())) else None for i in xs],\n          [special_values[i] if (special_values is not None) and (i in list(special_values.keys())) else None for i in xs],\n          [init_count_distr]*xs_len, [count_distr_limit]*xs_len, \n          [stop_limit]*xs_len, [bin_num_limit]*xs_len, [method]*xs_len\n        )\n        # bins in dictionary\n        bins = dict(zip(xs, pool.starmap(woebin2, args)))\n        pool.close()\n    \n    # runingtime\n    runingtime = time.time() - start_time\n    if runingtime >= 10 and print_info:\n        # print(time.strftime(\"%H:%M:%S\", time.gmtime(runingtime)))\n        print('Binning on {} rows and {} columns in {}'.format(dt.shape[0], dt.shape[1], time.strftime(\"%H:%M:%S\", time.gmtime(runingtime))))\n    if save_breaks_list is not None:\n        bins_to_breaks(bins, dt, to_string=True, save_string=save_breaks_list)\n    # return\n    return bins\n\n\n## Transform original value to woe values for one variable ##\ndef woepoints_ply1(dtx, binx, x_i, woe_points):\n    '''\n    Transform original values into woe or points for one variable.\n    \n    Params\n    ------\n    \n    Returns\n    ------\n    '''\n    # woe_points: \"woe\" \"points\"\n    # binx = bins.loc[lambda x: x.variable == x_i] \n    # https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n    binx = pd.merge(\n      binx[['bin']].assign(v1=binx['bin'].str.split('%,%')).explode('v1'),\n      binx[['bin', woe_points]],\n      how='left', on='bin'\n    ).rename(columns={'v1':'V1',woe_points:'V2'})\n    \n    # dtx\n    ## cut numeric variable\n    if is_numeric_dtype(dtx[x_i]):\n        is_sv = pd.Series(not bool(re.search(r'\\[', str(i))) for i in binx.V1)\n        binx_sv = binx.loc[is_sv]\n        binx_other = binx.loc[~is_sv]\n        # create bin column\n        breaks_binx_other = np.unique(list(map(float, ['-inf']+[re.match(r'.*\\[(.*),.+\\).*', str(i)).group(1) for i in binx_other['bin']]+['inf'])))\n        labels = ['[{},{})'.format(breaks_binx_other[i], breaks_binx_other[i+1]) for i in range(len(breaks_binx_other)-1)]\n        \n        dtx = dtx.assign(xi_bin = lambda x: pd.cut(x[x_i], breaks_binx_other, right=False, labels=labels))\\\n          .assign(xi_bin = lambda x: [i if (i != i) else str(i) for i in x['xi_bin']])\n        # dtx.loc[:,'xi_bin'] = pd.cut(dtx[x_i], breaks_binx_other, right=False, labels=labels)\n        # dtx.loc[:,'xi_bin'] = np.where(pd.isnull(dtx['xi_bin']), dtx['xi_bin'], dtx['xi_bin'].astype(str))\n        #\n        mask = dtx[x_i].isin(binx_sv['V1'])\n        dtx.loc[mask,'xi_bin'] = dtx.loc[mask, x_i].astype(str)\n        dtx = dtx[['xi_bin']].rename(columns={'xi_bin':x_i})\n    ## to charcarter, na to missing\n    if not is_string_dtype(dtx[x_i]):\n        dtx.loc[:,x_i] = dtx.loc[:,x_i].astype(str).replace('nan', 'missing')\n    # dtx.loc[:,x_i] = np.where(pd.isnull(dtx[x_i]), dtx[x_i], dtx[x_i].astype(str))\n    dtx = dtx.replace(np.nan, 'missing').assign(rowid = dtx.index).sort_values('rowid')\n    # rename binx\n    binx.columns = ['bin', x_i, '_'.join([x_i,woe_points])]\n    # merge\n    dtx_suffix = pd.merge(dtx, binx, how='left', on=x_i).sort_values('rowid')\\\n      .set_index(dtx.index)[['_'.join([x_i,woe_points])]]\n    return dtx_suffix\n    \n    \n## Transform original value to woe values for a dataframe ##    \ndef woebin_ply(dt, bins, no_cores=None, print_step=0, replace_blank=True, **kwargs):\n    '''\n    WOE Transformation\n    ------\n    `woebin_ply` converts original input data into woe values \n    based on the binning information generated from `woebin`.\n    \n    Params\n    ------\n    dt:A data frame.\n    bins:Binning information generated from `woebin`.\n    no_cores:Number of CPU cores for parallel computation. Defaults None. If no_cores is None, the no_cores will set as 1 if length of x variables less than 10, and will set as the number of all CPU cores if the length of x variables greater than or equal to 10.\n    print_step:    A non-negative integer. Default is 1. If print_step>0, print variable names by each print_step-th iteration. If print_step=0 or no_cores>1, no message is print.\n    replace_blank: Logical. Replace blank values with None. Defaults to True.\n    \n    Returns\n    -------\n    DataFrame\n        a dataframe of woe values for each variables \n    \n    Examples\n    -------\n    import scorecardpy as sc\n    import pandas as pd\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)\n    \n    # Example I\n    dt = data_gc[[\"Risk\", \"Credit amount\", \"Purpose\"]]\n    # binning for dt\n    bins = sc.woebin(dt, y = \"Risk\")\n    \n    # converting original value to woe\n    dt_woe = sc.woebin_ply(dt, bins=bins)\n    \n    # Example II\n    # binning for germancredit dataset\n    bins_germancredit = sc.woebin(data_gc, y=\"Risk\")\n    \n    # converting the values in germancredit to woe\n    ## bins is a dict\n    germancredit_woe = sc.woebin_ply(data_gc, bins=bins_germancredit) \n    ## bins is a dataframe\n    germancredit_woe = sc.woebin_ply(data_gc, bins=pd.concat(bins_germancredit))\n    '''\n    # start time\n    start_time = time.time()\n    ## print_info\n    print_info = kwargs.get('print_info', True)\n    if print_info: print('[INFO] converting into woe values ...')\n    \n    # remove date/time col\n    # dt = rmcol_datetime_unique1(dt)\n    # replace \"\" by NA\n    if replace_blank: dt = rep_blank_na(dt)\n    # ncol of dt\n    # if len(dt.index) <= 1: raise Exception(\"Incorrect inputs; dt should have at least two columns.\")\n    # print_step\n    print_step = check_print_step(print_step)\n    \n    # bins # if (is.list(bins)) rbindlist(bins)\n    if isinstance(bins, dict):\n        bins = pd.concat(bins, ignore_index=True)\n    # x variables\n    xs_bin = bins['variable'].unique()\n    xs_dt = list(dt.columns)\n    xs = list(set(xs_bin).intersection(xs_dt))\n    # length of x variables\n    xs_len = len(xs)\n    # initial data set\n    dat = dt.loc[:,list(set(xs_dt) - set(xs))]\n    \n    # loop on xs\n    if (no_cores is None) or (no_cores < 1):\n        all_cores = mp.cpu_count() - 1\n        no_cores = int(np.ceil(xs_len/5 if xs_len/5 < all_cores else all_cores*0.9))\n    if platform.system() == 'Windows': \n        no_cores = 1 \n            \n    # \n    if no_cores == 1:\n        for i in np.arange(xs_len):\n            x_i = xs[i]\n            # print xs\n            # print(x_i)\n            if print_step>0 and bool((i+1) % print_step): \n                print(('{:'+str(len(str(xs_len)))+'.0f}/{} {}').format(i, xs_len, x_i), flush=True)\n            #\n            binx = bins[bins['variable'] == x_i].reset_index()\n                 # bins.loc[lambda x: x.variable == x_i] \n                 # bins.loc[bins['variable'] == x_i] # \n                 # bins.query('variable == \\'{}\\''.format(x_i))\n            dtx = dt[[x_i]]\n            dat = pd.concat([dat, woepoints_ply1(dtx, binx, x_i, woe_points=\"woe\")], axis=1)\n    else:\n        pool = mp.Pool(processes=no_cores)\n        # arguments\n        args = zip(\n          [dt[[i]] for i in xs], \n          [bins[bins['variable'] == i] for i in xs], \n          [i for i in xs], \n          [\"woe\"]*xs_len\n        )\n        # bins in dictionary\n        dat_suffix = pool.starmap(woepoints_ply1, args)\n        dat = pd.concat([dat]+dat_suffix, axis=1)\n        pool.close()\n    # runingtime\n    runingtime = time.time() - start_time\n    if runingtime >= 10 and print_info:\n        # print(time.strftime(\"%H:%M:%S\", time.gmtime(runingtime)))\n        print('Woe transformating on {} rows and {} columns in {}'.format(dt.shape[0], xs_len, time.strftime(\"%H:%M:%S\", time.gmtime(runingtime))))\n    return dat\n\n\n## Plot binning of one variable ## - required in woebin_plot\ndef plot_bin(binx, title, show_iv):\n    '''\n    plot binning of one variable\n    \n    Params\n    ------\n    binx:\n    title:\n    show_iv:\n    \n    Returns\n    ------\n    matplotlib fig object\n    '''\n    # y_right_max\n    y_right_max = np.ceil(binx['badprob'].max()*10)\n    if y_right_max % 2 == 1: y_right_max=y_right_max+1\n    if y_right_max - binx['badprob'].max()*10 <= 0.3: y_right_max = y_right_max+2\n    y_right_max = y_right_max/10\n    if y_right_max>1 or y_right_max<=0 or y_right_max is np.nan or y_right_max is None: y_right_max=1\n    ## y_left_max\n    y_left_max = np.ceil(binx['count_distr'].max()*10)/10\n    if y_left_max>1 or y_left_max<=0 or y_left_max is np.nan or y_left_max is None: y_left_max=1\n    # title\n    title_string = binx.loc[0,'variable']+\"  (iv:\"+str(round(binx.loc[0,'total_iv'],4))+\")\" if show_iv else binx.loc[0,'variable']\n    title_string = title+'-'+title_string if title is not None else title_string\n    # param\n    ind = np.arange(len(binx.index))    # the x locations for the groups\n    width = 0.35       # the width of the bars: can also be len(x) sequence\n    ###### plot ###### \n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n    # ax1\n    p1 = ax1.bar(ind, binx['good_distr'], width, color=(24/254, 192/254, 196/254))\n    p2 = ax1.bar(ind, binx['bad_distr'], width, bottom=binx['good_distr'], color=(246/254, 115/254, 109/254))\n    for i in ind:\n        ax1.text(i, binx.loc[i,'count_distr']*1.02, str(round(binx.loc[i,'count_distr']*100,1))+'%, '+str(binx.loc[i,'count']), ha='center')\n    # ax2\n    ax2.plot(ind, binx['badprob'], marker='o', color='blue')\n    for i in ind:\n        ax2.text(i, binx.loc[i,'badprob']*1.02, str(round(binx.loc[i,'badprob']*100,1))+'%', color='blue', ha='center')\n    # settings\n    ax1.set_ylabel('Bin count distribution')\n    ax2.set_ylabel('Bad probability', color='blue')\n    ax1.set_yticks(np.arange(0, y_left_max+0.2, 0.2))\n    ax2.set_yticks(np.arange(0, y_right_max+0.2, 0.2))\n    ax2.tick_params(axis='y', colors='blue')\n    plt.xticks(ind, binx['bin'])\n    plt.title(title_string, loc='left')\n    plt.legend((p2[0], p1[0]), ('bad', 'good'), loc='upper right')\n    # show plot \n    # plt.show()\n    return fig\n\n\n## WOE Binning Visualization ##\ndef woebin_plot(bins, x=None, title=None, show_iv=True):\n    '''\n    WOE Binning Visualization\n    ------\n    `woebin_plot` create plots of count distribution and bad probability \n    for each bin. The binning informations are generates by `woebin`.\n    \n    Params\n    ------\n    bins:A list or data frame. Binning information generated by `woebin`.\n    x:Name of x variables. Default is None. If x is None, then all variables except y are counted as x variables.\n    title:String added to the plot title. Default is None.\n    show_iv: Logical. Default is True, which means show information value in the plot title.\n    \n    Returns\n    ------\n    dict\n        a dict of matplotlib figure objests\n        \n    Examples\n    ------\n    import scorecardpy as sc\n    import matplotlib.pyplot as plt\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)\n    \n    # Example I\n    dt1 = data_gc[[\"Risk\", \"Credit amount\"]]\n    \n    bins1 = sc.woebin(dt1, y=\"Risk\")\n    p1 = sc.woebin_plot(bins1)\n    plt.show(p1)\n    \n    # Example II\n    bins = sc.woebin(data_gc, y=\"Risk\")\n    plotlist = sc.woebin_plot(bins)\n    \n    # # save binning plot\n    # for key,i in plotlist.items():\n    #     plt.show(i)\n    #     plt.savefig(str(key)+'.png')\n    '''\n    xs = x\n    # bins concat \n    if isinstance(bins, dict):\n        bins = pd.concat(bins, ignore_index=True)\n    # good bad distr\n    def gb_distr(binx):\n        binx['good_distr'] = binx['good']/sum(binx['count'])\n        binx['bad_distr'] = binx['bad']/sum(binx['count'])\n        return binx\n    bins = bins.groupby('variable').apply(gb_distr)\n    # x variable names\n    if xs is None: xs = bins['variable'].unique()\n    # plot export\n    plotlist = {}\n    for i in xs:\n        binx = bins[bins['variable'] == i].reset_index()\n        plotlist[i] = plot_bin(binx, title, show_iv)\n    return plotlist\n\n\n## Print basic information in woebinning adjusting process ##\ndef woebin_adj_print_basic_info(i, xs, bins, dt, bins_breakslist):\n    '''\n    print basic information of woebinnig in adjusting process\n    \n    Params\n    ------\n    \n    Returns\n    ------\n    '''\n    x_i = xs[i-1]\n    xs_len = len(xs)\n    binx = bins.loc[bins['variable']==x_i]\n    print(\"--------\", str(i)+\"/\"+str(xs_len), x_i, \"--------\")\n    # print(\">>> dt[\"+x_i+\"].dtypes: \")\n    # print(str(dt[x_i].dtypes), '\\n')\n    # \n    print(\">>> dt[\"+x_i+\"].describe(): \")\n    print(dt[x_i].describe(), '\\n')\n    \n    if len(dt[x_i].unique()) < 10 or not is_numeric_dtype(dt[x_i]):\n        print(\">>> dt[\"+x_i+\"].value_counts(): \")\n        print(dt[x_i].value_counts(), '\\n')\n    else:\n        dt[x_i].hist()\n        plt.title(x_i)\n        plt.show()\n        \n    ## current breaks\n    print(\">>> Current breaks:\")\n    print(bins_breakslist[x_i], '\\n')\n    ## woebin plotting\n    plt.show(woebin_plot(binx)[x_i])\n    \n    \n## Plot adjusted binning in woebinning adjustment process ##\ndef woebin_adj_break_plot(dt, y, x_i, breaks, stop_limit, sv_i, method):\n    '''\n    update breaks and provies a binning plot\n    \n    Params\n    ------\n    \n    Returns\n    ------\n    '''\n    if breaks == '':\n        breaks = None\n    breaks_list = None if breaks is None else {x_i: eval('['+breaks+']')}\n    special_values = None if sv_i is None else {x_i: sv_i}\n    # binx update\n    bins_adj = woebin(dt[[x_i,y]], y, breaks_list=breaks_list, special_values=special_values, stop_limit = stop_limit, method=method)\n    \n    ## print adjust breaks\n    breaks_bin = set(bins_adj[x_i]['breaks']) - set([\"-inf\",\"inf\",\"missing\"])\n    breaks_bin = ', '.join(breaks_bin) if is_numeric_dtype(dt[x_i]) else ', '.join(['\\''+ i+'\\'' for i in breaks_bin])\n    print(\">>> Current breaks:\")\n    print(breaks_bin, '\\n')\n    # print bin_adj\n    plt.show(woebin_plot(bins_adj))\n    # return breaks \n    if breaks == '' or breaks is None: breaks = breaks_bin\n    return breaks\n    \n\n## WOE Binning Adjustment(interactively) ##\ndef woebin_adj(dt, y, bins, adj_all_var=False, special_values=None, method=\"tree\", save_breaks_list=None, count_distr_limit=0.05):\n    '''\n    WOE Binning Adjustment\n    ------\n    `woebin_adj` interactively adjust the binning breaks.\n    \n    Params\n    ------\n    dt:A data frame.\n    y:Name of y variable.\n    bins:A list or data frame. Binning information generated from woebin.\n    adj_all_var:Logical, whether to show monotonic woe variables. Default is True\n    special_values:the values specified in special_values will in separate bins. Default is None.\n    method:optimal binning method, it should be \"tree\" or \"chimerge\". Default is \"tree\".\n    save_breaks_list:  The file name to save breaks_list. Default is None.\n    count_distr_limit: The minimum percentage of final binning class number over total. Accepted range: 0.01-0.2; default is 0.05.\n    \n    Returns\n    ------\n    dict\n        dictionary of breaks\n        \n    Examples\n    ------\n    import scorecardpy as sc\n    \n    # load data\n    data_gc = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)\n    \n    # Example I\n    dt = data_gc[[\"Risk\", \"Age\", \"Credit amount\"]]\n    \n    bins = sc.woebin(dt, y=\"Risk\")\n    breaks_adj = sc.woebin_adj(dt, y=\"Risk\", bins=bins)\n    bins_final = sc.woebin(dt, y=\"Risk\", breaks_list=breaks_adj)\n    \n    # Example II\n    binsII = sc.woebin(data_gc, y=\"Risk\")\n    breaks_adjII = sc.woebin_adj(data_gc, \"Risk\", binsII)\n    bins_finalII = sc.woebin(data_gc, y=\"Risk\", breaks_list=breaks_adjII)\n    '''\n    # bins concat \n    if isinstance(bins, dict):\n        bins = pd.concat(bins, ignore_index=True)\n    # x variables\n    xs_all = bins['variable'].unique()\n    # adjust all variables\n    if not adj_all_var:\n        bins2 = bins.loc[~((bins['bin'] == 'missing') & (bins['count_distr'] >= count_distr_limit))].reset_index(drop=True)\n        bins2['badprob2'] = bins2.groupby('variable').apply(lambda x: x['badprob'].shift(1)).reset_index(drop=True)\n        bins2 = bins2.dropna(subset=['badprob2']).reset_index(drop=True)\n        bins2 = bins2.assign(badprob_trend = lambda x: x.badprob >= x.badprob2)\n        xs_adj = bins2.groupby('variable')['badprob_trend'].nunique()\n        xs_adj = xs_adj[xs_adj>1].index\n    else:\n        xs_adj = xs_all\n    # length of adjusting variables\n    xs_len = len(xs_adj)\n    # special_values\n    special_values = check_special_values(special_values, xs_adj)\n    \n    # breakslist of bins\n    bins_breakslist = bins_to_breaks(bins,dt)\n    # loop on adjusting variables\n    if xs_len == 0:\n        warnings.warn('The binning breaks of all variables are perfect according to default settings.')\n        breaks_list = \"{\"+', '.join('\\''+bins_breakslist.index[i]+'\\': ['+bins_breakslist[i]+']' for i in np.arange(len(bins_breakslist)))+\"}\"\n        return breaks_list\n    # else \n    def menu(i, xs_len, x_i):\n        print('>>> Adjust breaks for ({}/{}) {}?'.format(i, xs_len, x_i))\n        print('1: next \\n2: yes \\n3: back')\n        adj_brk = input(\"Selection: \")\n        adj_brk = int(adj_brk)\n        if adj_brk not in [0,1,2,3]:\n            warnings.warn('Enter an item from the menu, or 0 to exit.')\n            adj_brk = input(\"Selection: \")\n            adj_brk = int(adj_brk)\n        return adj_brk\n        \n    # init param\n    i = 1\n    breaks_list = None\n    while i <= xs_len:\n        breaks = stop_limit = None\n        # x_i\n        x_i = xs_adj[i-1]\n        sv_i = special_values[x_i] if (special_values is not None) and (x_i in special_values.keys()) else None\n        # if sv_i is not None:\n        #     sv_i = ','.join('\\'')\n        # basic information of x_i variable ------\n        woebin_adj_print_basic_info(i, xs_adj, bins, dt, bins_breakslist)\n        # adjusting breaks ------\n        adj_brk = menu(i, xs_len, x_i)\n        if adj_brk == 0: \n            return \n        while adj_brk == 2:\n            # modify breaks adj_brk == 2\n            breaks = input(\">>> Enter modified breaks: \")\n            breaks = re.sub(\"^[,\\.]+|[,\\.]+$\", \"\", breaks)\n            if breaks == 'N':\n                stop_limit = 'N'\n                breaks = None\n            else:\n                stop_limit = 0.1\n            try:\n                breaks = woebin_adj_break_plot(dt, y, x_i, breaks, stop_limit, sv_i, method=method)\n            except:\n                pass\n            # adj breaks again\n            adj_brk = menu(i, xs_len, x_i)\n        if adj_brk == 3:\n            # go back adj_brk == 3\n            i = i-1 if i>1 else i\n        else:\n            # go next adj_brk == 1\n            if breaks is not None and breaks != '': \n                bins_breakslist[x_i] = breaks\n            i += 1\n    # return \n    breaks_list = \"{\"+', '.join('\\''+bins_breakslist.index[i]+'\\': ['+bins_breakslist[i]+']' for i in np.arange(len(bins_breakslist)))+\"}\"\n    if save_breaks_list is not None:\n        bins_adj = woebin(dt, y, x=bins_breakslist.index, breaks_list=breaks_list)\n        bins_to_breaks(bins_adj, dt, to_string=True, save_string=save_breaks_list)\n    return breaks_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Oversampling\"></a> <br>\n# **12. Over Sampling**\n- Over sampling by SMOTE(Synthetic Minority Over-sampling Technique)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def oversampling_by_smote(X,y,sampling_strategy='auto',random_state=186,k_neighbors=5,n_jobs=None,m_neighbors=10,kind=\"borderline-1\"):\n    sampler = BorderlineSMOTE(sampling_strategy=sampling_strategy,random_state=random_state,k_neighbors=k_neighbors,n_jobs=n_jobs,m_neighbors=m_neighbors,kind=kind)\n    X_res, y_res = sampler.fit_resample(X, y)\n    print('Resampled dataset shape %s' % Counter(y_res))\n    return X_res,y_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Example\"></a> <br>\n# **13. Example**\n- Load germancredit data\n- Overview of data\n- Plot target distribution(interactive)\n- Plot distribution of a variable(Credit amount)\n- Box plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing)\n- Violin plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing)\n- Count plot of a categorical variable by target(Housing)\n- Check Multicollinearity\n- Check Correlation\n- Filter variable via missing rate, iv, identical value rate and change target to 1/0 \n- Breaking dt into train and test\n- Generate WOE binning\n- Binning adjustment(manually)\n- Converting train and test into woe values\n- Over Sampling by SMOTE\n- Logistic regression model\n- Predict probability based on the LR model\n- Plot model performance(KS&ROC)\n- Generate scorecard\n- Show score\n- Plot PSI of score between train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load germancredit data ##\ndata_gc= pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Overview of data ##\noverview_data_bf(data_gc)[['Name','%Missing','%Unique','iv']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot target distribution(interactive) ##\nplot_target_dist_interact(data_gc,target_col='Risk',negative_cat='good',positive_cat='bad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot distribution of a variable(Credit amount) ##\nplot_variable_dist_interact(df=data_gc,target_col='Risk',variable_col='Credit amount',negative_cat='good',positive_cat='bad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Box plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing) ##\nbox_plot_convar_vs_catvar_interact(df=data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing',negative_cat='good',positive_cat='bad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Violin plot of a continous variable vs. a catergorical variable(Credit amount vs. Housing) ##\nviolin_plot_convar_vs_catvar_interact(df=data_gc,target_col='Risk',con_var_col='Credit amount',cat_var_col='Housing',negative_cat='good',positive_cat='bad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Count plot of a categorical variable by target(Housing) ##\ncnt_plot_catvar_by_target_interact(data_gc,target_col='Risk',cat_var_col='Housing',negative_cat='good',positive_cat='bad')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check Multicollinearity ##\ngvif,filted,dat_remain = multicollinearity_check(data_gc,y='Risk',threshold=5,only_final_vif = False)\nprint(gvif)\nprint(filted)\nprint(dat_remain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_remain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation Check ##\ncorrelation_check(dat_remain,show_plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Filter variable via missing rate, iv, identical value rate and change target to 1/0 ##\ndt_s = var_filter(dat_remain, y=\"Risk\")\noverview_data_af(dt_s)[['Name','%Missing','%Unique','iv']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Breaking dt into train and test ##\ntrain, test = split_df(dt_s, 'Risk').values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generate WOE binning ##\nbins = woebin(dt_s, y=\"Risk\")\nwoebin_plot(bins)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Binning adjustment(manually) ##\nbreaks_adj = {\n    'Age': [26, 35, 40],\n}\nbins_adj = woebin(dt_s, y=\"Risk\", breaks_list=breaks_adj)\nwoebin_plot(bins_adj)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converting train and test into woe values ##\ntrain_woe = woebin_ply(train, bins_adj)\ntest_woe = woebin_ply(test, bins_adj)\ny_train = train_woe.loc[:,'Risk']\nX_train = train_woe.loc[:,train_woe.columns != 'Risk']\ny_test = test_woe.loc[:,'Risk']\nX_test = test_woe.loc[:,train_woe.columns != 'Risk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Over sampling by SMOTE ##\nX_res,y_res = oversampling_by_smote(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic regression model ##\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\nlr.fit(X_res, y_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## predict proability from LR model ##\ntrain_pred = lr.predict_proba(X_train)[:,1]\ntest_pred = lr.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot model performance(ks & roc) ##\ntrain_perf = perf_eva(y_train, train_pred, title = \"train\")\ntest_perf = perf_eva(y_test, test_pred, title = \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get score card ##\ncard = scorecard(bins_adj, lr, X_train.columns)\ncard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show credit score(train/test/good/bad) ##\ndata_gc_score=scorecard_ply(data_gc,card,print_step=0)\ntrain_score = scorecard_ply(train, card, print_step=0)\ntest_score = scorecard_ply(test, card, print_step=0)\ntest_score_all_var = scorecard_ply(test, card, print_step=0,only_total_score=False)\nscore_with_target = pd.concat([data_gc_score,dt_s['Risk']],axis=1)\nscore_total = score_with_target['score']\nscore_good = score_with_target[score_with_target.Risk == 0]['score']\nscore_bad = score_with_target[score_with_target.Risk==1]['score']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score_all_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot Score Divergence(good/bad) ##\nplot_score_distribution(score_good,score_bad,'good','bad','Score Divergence')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show score sorting ##\nscore_bin_sort = score_bin_sort(\n    score = {'train':train_score, 'test':test_score},\n    label = {'train':y_train, 'test':y_test}\n)\nscore_bin_sort.sort_values(by='bad rate%',ascending=False).style.bar(color='tomato',subset='bad rate%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_bin_sort.index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot psi of score ##\nperf_psi(\n  score = {'train':train_score, 'test':test_score},\n  label = {'train':y_train, 'test':y_test}\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}