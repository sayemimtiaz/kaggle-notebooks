{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Foreword:** This notebook was maintained simultaneously when I was learning Linear Regression. This notebook is first part of the three-notebook series, which tend to explain process of applying Linear Regression on a dataset. You can find all parts by clicking given links:\n\n[**1. First Linear Regression Model**](https://www.kaggle.com/salmankhi/my-first-regression-mode)\n\n[**2. Multiple Variable Linear Regression Model**](https://www.kaggle.com/salmankhi/multiple-linear-regression)\n\n[**3. Linear Regression on Categorical Data**](https://www.kaggle.com/salmankhi/regression-on-categorical-data)","metadata":{}},{"cell_type":"markdown","source":"**What is Regression?**\n\nIt is a statistical method used in finance, investing, and other disciplines that attempts to determine the causal relationship between one variable (dependent, usually denoted by Y) and a series of other variables (independent variables, denoted by x1, x2, x3,...,xk).\n\nAmong different types of regression models, the most basic one is **Simple Linear Regression Model** and in this notebook we will try to predict dependant variable with that.","metadata":{}},{"cell_type":"markdown","source":"**Linear Regression Model** is the approximation of linear causal relationship between two or more variables.\n\nLinear Regression Model: (for population) <center>Y = β0 + β1*x1 + error</center>\n\nHere is Simple Linear Regression Model Equation: (for sample) <center>y_hat = b0 + b1*x1</center>\n\nwhere,\n- y_hat - dependant variable (approximated) (y has a hat on it when we write this equation, like ^)\n- x1 - independent variable (regressor)\n- β0/b1 - constant\n- β1/b1 - factor defining effect og x1 on y (coefficient of regressor)","metadata":{}},{"cell_type":"markdown","source":"**Now for the example,** we have some real estate data comprising of two columns. One column has the prices of some houses and other has sizes. Now these two(sizes and prices) seem to have causal relationship between them. We will be making a model that will estimate price of a house of given size.","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Required Libraries\n\nimport **libraryname** as **alias**","metadata":{}},{"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  # data manipulation and procession\nimport matplotlib.pyplot as plt  # data visualization\nimport seaborn as sns  # more attractive visualizations\nimport statsmodels.api as sm  # will help us apply regression model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()  # (optional), will make all matplotlib visualizations appear in seaborn skins","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Reading Data\n**pd.read_csv()** is used to save data as data structure(Series/DataFrame), argument takes csv file location.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/real-estate-prices-and-sizes/real_estate_price_size.csv\")  # saving all the data in variable 'data'\ndata.head()  # prints first 5 rows of the data, if no argument given.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** The relationship on which we are applying regression should make sense. Like here, it is more sensible to think that prices of the houses will depend upon sizes, not the other way around.\n\nHence, size will be the independent variable(x1) here and price, the dependent one (y).","metadata":{}},{"cell_type":"markdown","source":"## 3. Saving Dependent and Independent Variables\nWe will be saving both in the form of 'Series'.","metadata":{}},{"cell_type":"code","source":"y = data[\"price\"]  # saving variable 'price' in y\nx1 = data[\"size\"]  # saving variable 'size' in x1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Scatter Plot\nPlotting a scatter chart with independant variable on x-axis and dependent variable on y-axis.\n\nMatplotlib(a.k.a. plt) will be used for this step.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x1, y)  # returns a scatter plot, scatter(<var on x-axis>, <var on y-axis)\nplt.xlabel(\"Sizes\", fontsize = 20)  # labelling x-axis\nplt.ylabel(\"Prices\", fontsize = 20)  #labelling y-axis\nplt.show()  # returns graph with above given properties","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be interpreted from the graph that their is a strong relationship between prices and sizes of the houses. Bigger house; more expensive.","metadata":{}},{"cell_type":"markdown","source":"## 5. Adding a Regression Line (Applying Linear Regression on Data Set)\nWe are yet to apply regression model on this data. On applying regression, there will be a regression line on the scatter plot that will approximate linear relationship between the two variables. That line will be as close to all points together as possible.\n\nstatsmodels will be used to apply regression on the data!","metadata":{}},{"cell_type":"code","source":"# adding a constant, actually we are adding a column of '1s' of length equal to the length of x1\nx = sm.add_constant(x1)\n\n# fitting the model using OLS(Ordinary Least Squares) method, with dependent 'y' and independent 'x'\nresults = sm.OLS(y, x).fit()\n\n# printing the summary obtained on the application of the OLS regression\n# summaries are the strong suit of statsmodels\nresults.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Also note** that there are various methods to apply Simple Linear Regression. OLS, that is used here, is one of the most common methods for estimating the linear regression equation. Other methods for linear regression are:\n- Generalized Least Squares\n- Maximum Likelihood Estimation\n- Bayesian Regression\n- Kernel Regression\n- Gaussian Process Regressi","metadata":{}},{"cell_type":"markdown","source":"## 6. Interpretation of the Summary\nThe last function used i.e. summary() gave us some results that we have gotten on applying OLS Regression on the data set. These results contain Model Summary, Coefficient Table and some other information. We will look into some important values from them. We have\n- dependent variable: price\n- Model, Method - OLS, Least Squares\n- constant coefficient - b0 (1.019e+05 here)\n- size coefficient - b1 (affect of size on the price) (223.1787 here)\n- standard error - (lower, the better) (1.19e+04)\n- t-statistic\n- P>|t| - for the H0 that size does not affect price (0.000 means we can reject this H0)","metadata":{}},{"cell_type":"markdown","source":"## 7. Plotting Regression Line on the Graph\nOkay, before we go forward and plot regression line on the graph, it should be understood that the line on the scatter plot is not the regression itself and it is only a graphical representation of the method.\n\nPart of this code is similar as we have done earlier in scatter plot, we will just add a line on that graph.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x1, y)\ny_hat = 1.019e+05 + (223.1787 * x1)  # substituting b0 and b1 in Simple Linear Regression Model Equation\nfig = plt.plot(x1, y_hat, lw = 4, c = \"orange\", label = \"Regression Line\")  # plotting line\nplt.xlabel(\"Size\", fontsize = 20)\nplt.ylabel(\"Price\", fontsize = 20)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use this equation, y_hat = 1.019e+05 + (223.1787 * x1) to approximate the price of houses by substituting sizes in place of x1.","metadata":{}},{"cell_type":"markdown","source":"## 8. Decomposition of Variability\nThere are few measures that helps us to analyze our model. There are three of them.\n\n- Sums of Square Totals (SST):\n\n    It is the measure of the dispersion of the Data Set.\n    To find it\n    1. We take square of differences of each sample-y and mean-y\n    2. And them take the sum of all the squared differnces\n    (sample-y is each observation and mean-y is all observations' mean)\n\n\n- Sums of Squares by Regression (SSR):\n\n    It is a measure to tell how dispersed are our approximated-y from mean-y\n    To find it\n    1. We take square of differences of each approximated-y and mean-y\n    2. And them take the sum of all the squared differnces\n    (approximated-y is the value we get by plugging x in linear regression equation)\n    \n- Sum of Square Errors (SSE):\n    It is a measure to tell how far our approximated-y from sammple-y\n    1. We take square of differences of each approximated-y and mean-y\n    2. And them take the sum of all the squared differnces\n    \nFor a perfect Linear Regression Model:\n- SST = SSR and SSE = 0\n- Lesser SSE implies strong model","metadata":{}},{"cell_type":"markdown","source":"## 9. R-Squared\nR-squared is the ratio of SSR and SST.\n- R-squared = 1 => SSR = SST, means that x explains entire variablity of y\n- R-squared = 0 => means x explains NONE of the variability of y\n- R-squared can range from 0 to 1\n\nLower value of R-squared not necessarily means that model is wrong, it just suggest that there might be other variables also that affect y and they are not incorporated in the model.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}