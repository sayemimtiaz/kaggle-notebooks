{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Import the necessary libraries"},{"metadata":{"trusted":true,"_uuid":"d6fb32fd69316596e236eab5fb8cf77c848508c3"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f674695f1742479cefdeec0e81ab469f7b6ec90f"},"cell_type":"markdown","source":"### Load the data into Pandas dataframe"},{"metadata":{"trusted":true,"_uuid":"aca2f1d9da3f35d104763166fe4d25448410d8f2"},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv',delimiter=',',encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53083ccecf39523cff290495a6cc768061ba9b46"},"cell_type":"markdown","source":"Drop the columns that are not required for the neural network."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"95a8b5d6f19cf42d4f55c6d2842faf1d0d55c1d0"},"cell_type":"code","source":"df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7060084470000f39a2dcc15b656586dcd6e9fd"},"cell_type":"markdown","source":"Understand the distribution better."},{"metadata":{"trusted":true,"_uuid":"a12002f521dd8eaeb0f69a932cbf23815ffd09d7"},"cell_type":"code","source":"sns.countplot(df.v1)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"353a8191f86c3a22843a729b5d4a5acefbf94be8"},"cell_type":"markdown","source":"* Create input and output vectors.\n* Process the labels."},{"metadata":{"trusted":true,"_uuid":"a1a345c1683e2fcc7173ecae867a5da87f2dde24"},"cell_type":"code","source":"X = df.v2\nY = df.v1\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150e244a39b814d8a41bbe0e419bc5f28e457dd6"},"cell_type":"markdown","source":"Split into training and test data."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"aa3386af09469682c66cc53a1830a4e42f0e70b6"},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5378d55c271e01480c1ac07f94ff99a80f900d6"},"cell_type":"markdown","source":"### Process the data\n* Tokenize the data and convert the text to sequences.\n* Add padding to ensure that all the sequences have the same shape.\n* There are many ways of taking the *max_len* and here an arbitrary length of 150 is chosen."},{"metadata":{"trusted":true,"_uuid":"bdca14f2b8cd7bd7cb5ee66fd40ea522217c03c6"},"cell_type":"code","source":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad8706caa7a447fb49b44919fd109129e4082a93"},"cell_type":"markdown","source":"### RNN\nDefine the RNN structure."},{"metadata":{"trusted":true,"_uuid":"78fff25b8be1de575bff071a2027f3dd2b11b911"},"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d7c489e32bff6d12b8c08c07a91e9ba5d302e0e"},"cell_type":"markdown","source":"Call the function and compile the model."},{"metadata":{"trusted":true,"_uuid":"a0ede32d4127e8b4990fd74fe97fadef9e565d17"},"cell_type":"code","source":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2e0a3ec50d14c790b82d66f9255456ec6a69da"},"cell_type":"markdown","source":"Fit on the training data."},{"metadata":{"trusted":true,"_uuid":"98f6d6318352420ea49c532cda158f715f940f4b"},"cell_type":"code","source":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448ab38c2f804e47df48eb45385393aaec168032"},"cell_type":"markdown","source":"The model performs well on the validation set and this configuration is chosen as the final model."},{"metadata":{"_uuid":"ccca7839445a7d663ee7bc425a16e247df3e0e5b"},"cell_type":"markdown","source":"Process the test set data."},{"metadata":{"trusted":true,"_uuid":"80036135a11387d952becaf2fecf653a65c02328"},"cell_type":"code","source":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b60d7d2bcc0aabf77c8c8766c59f8d73cd34547"},"cell_type":"markdown","source":"Evaluate the model on the test set."},{"metadata":{"trusted":true,"_uuid":"0db183049b59d96388812a98efedfc865b7cc141"},"cell_type":"code","source":"accr = model.evaluate(test_sequences_matrix,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3e121ab83f4a0b9f7376ab24aa25d67051171f89"},"cell_type":"code","source":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd9d4cfc125942ea07adcd55da75b996465c2ba"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9064bae0e16602fbbaa652140173e32bd1c04840"},"cell_type":"markdown","source":"source for attention: [https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py](http://)\nsource for other code: [https://www.kaggle.com/kredy10/simple-lstm-for-text-classification/notebook](http://)"},{"metadata":{"trusted":true,"_uuid":"9cada4b5e83fc646b6dacd96ab67dc3b8e6f1d05"},"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e5287b50dfb6bde299617766d1f7ac52e1c5ae2"},"cell_type":"code","source":"SINGLE_ATTENTION_VECTOR = False\nAPPLY_ATTENTION_BEFORE_LSTM = False\ndef attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1), name='attention_vec')(a)\n    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n    output_attention_mul = multiply([inputs, a_probs])\n    return output_attention_mul","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"877dac6c5b8d2ea732109a9f20ae74cf50b96fab"},"cell_type":"code","source":"def model_attention_applied_after_lstm():\n    #inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    \n    lstm_units = 64\n    lstm_out = LSTM(lstm_units, return_sequences=True)(layer)\n    attention_mul = attention_3d_block(lstm_out)\n    attention_mul = Flatten()(attention_mul)\n    output = Dense(1, activation='sigmoid')(attention_mul)\n    model = Model(input=[inputs], output=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a7166deb080cb4834d874bff9fcda8fba7be5fa"},"cell_type":"code","source":"from keras.layers import merge\nfrom keras.layers import multiply\nfrom keras.layers.core import *\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import *\n\nfrom keras.utils.vis_utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed17fe8472484cf65b88edd0251c2a29841131a9"},"cell_type":"code","source":"INPUT_DIM = 50\nTIME_STEPS = max_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b730b78aa48156b586cbec3f56e08663514d6872"},"cell_type":"code","source":"sequences_matrix.shape,Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce87ec6400c6ffc4f0133040b80243f96cdaab39"},"cell_type":"code","source":"m = model_attention_applied_after_lstm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6110d70ac05da2e102a78cf077bbf86599995d4"},"cell_type":"code","source":"m.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"916ddc97e1142214e685e12ff638d4a8f953690e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a29d2aaada65d13128593b1dfa207aa913ec98"},"cell_type":"code","source":"m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nm.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3da604ff769e9178f1b9e47e431ef799af38454"},"cell_type":"code","source":"accr = m.evaluate(test_sequences_matrix,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c20f8c03423651d8075671118175b0d1adce29b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}