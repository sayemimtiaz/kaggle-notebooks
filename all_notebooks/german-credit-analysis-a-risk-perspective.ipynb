{"cells":[{"metadata":{"_uuid":"af2b264911cec06236317956685c9a3373a96974","_cell_guid":"e8b69ffa-82f4-4f10-885d-be5372d3f115"},"cell_type":"markdown","source":"<img src=\"https://xixerone.com/wp-content/uploads/2017/11/D%C3%B3nde-dormir-en-Frankfurt-Mejores-zonas-y-hoteles.jpg\">\n<h1 style=\"text-align:center;\"> German Credit Analysis </h1>\nWe will explore in this data analysis the behavior of German borrowers. Questions such as for what <b>purpose</b> do German borrowers apply for credit loans?,  How many jobs do each borrower have? , What patterns (if any) determine whether the loan will have a  good or bad risk ? Of course, many more questions will be answered through our in-depth analysis of German credit borrowers. To make our visualizations more interactive we will be using all our charts with plotly, an interactive library that will allow us to have a better insight about our data. <br>\n\n*“Fools say that they learn by experience. I prefer to profit by others experience.” *- <b>Otto Von Bismarck</b>\n\n<b>Note: This project will be subjected to constant updates in the near future. Also, I am open to listen insights from Kagglers about this project. </b> \n\n<h3> Goals for this Project </h3>\n<ul>\n<li> Explore our data and detecting <b>key patterns</b>. </li>\n<li> Develop a <b>Neural Network</b> to predict whether a loan will be of a good or bad risk. </li>\n<li> Most importantly, have <b>fun</b> while doing this project. </li>\n<ul>\n"},{"metadata":{"_uuid":"4a05896d90c7f8e61dd359278e8451de7be386a7","_cell_guid":"df152610-d67a-42cc-a272-f98a74d12647"},"cell_type":"markdown","source":"<h2> Outline : </h2>\n1) [Brief Overview](#brief_overview) <br><br>\n\n2) **Analysis by Type of Group**<br>\na) [Gender Analysis](#gender_analysis) <br>\nb) [Age Groups](#age_groups)<br>\nc) [Wealth Analysis](#wealth_analysis)<br><br> \n\n3) **Correlations and Purposes of Loans**<br>\na) [Correlations](#correlations)<br>\nb) [Loan Purpose](#purpose_loans)<br>\n\n4) **Modeling**<br>\na) [Predictive Modelling](#predictive_modelling)\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport warnings\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\n\n# Importing our data\ndf = pd.read_csv(\"../input/german_credit_data.csv\",index_col=0)\n\noriginal_df = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"329b29742347ba4c7f6d8e8fc64484902c59a313","_cell_guid":"edb75691-916a-4914-86c2-e5deb93128bc"},"cell_type":"markdown","source":"## Brief Overview: \n<a id=\"brief_overview\"></a>\nThe first phase of this project is to see what is our data made about. Which variables are numerical or categorical and which columns have \"Null\" values, which is something we will address in the feature engineering phase.\n\n## Summary:\n<ul>\n<li> We have four <b>numeric</b> and four <b>categorical</b> features. </li>\n<li> The <b>average age</b>  of people in our dataset is 35.54</li>\n<li> The <b>average credit amount</b> borrowed is 3271 </li>\n</ul>\n\n"},{"metadata":{"_uuid":"f024127885b5561c6084ce651ba5bae329cb393d","_cell_guid":"01e5dc2d-9d25-41f8-b6cc-c3af1185f4bf","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = df.rename(columns={\"Credit amount\": \"Credit_amount\"})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be987262954a51e0e76f540d8f03a9122fdc8998","_cell_guid":"82fbc93c-5803-4191-9b51-1e8a25b8cb3a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f396b23b8eacdaae84cf32171f3f86b9df657bd","_cell_guid":"7b0dc47a-5f88-4ff4-8467-ff3db7254814","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ceed43be4f21642194375d4c7a49b55e0c2aa6e8","_cell_guid":"ea0be571-d8fb-418b-ab53-9029f65dddfa","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa16152ff27980efbf394dd97a968b988d30d3ff","_cell_guid":"dd55877b-bf10-4f0f-94d2-dd0ef9ab5de8","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check missing values\ndf.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f1cafba4243d1cec329667cebc6e9aa8386ed71","_kg_hide-input":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"927ae42df037804c9f1791067cd87d007161fa34","_kg_hide-input":true},"cell_type":"code","source":"# Distribution of Credit_Amount for each Gender\nmale_credit = df[\"Credit_amount\"].loc[df[\"Sex\"] == \"male\"].values\nfemale_credit = df[\"Credit_amount\"].loc[df[\"Sex\"] == \"female\"].values\ntotal_credit = df['Credit_amount'].values\n\nfig, ax = plt.subplots(1, 3, figsize=(16,4))\n\nsns.distplot(male_credit, ax=ax[0], color=\"#FE642E\")\nax[0].set_title(\"Male Credit Distribution\", fontsize=16)\nsns.distplot(female_credit, ax=ax[1], color=\"#F781F3\")\nax[1].set_title(\"Female Credit Distribution\", fontsize=16)\nsns.distplot(total_credit, ax=ax[2], color=\"#2E64FE\")\nax[2].set_title(\"Total Credit Distribution\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c9205df06788a4e2c12e05237e9a9750855b18c","_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(13,6)) #figure size\ng = sns.boxplot(x='Purpose', y='Credit_amount', \n                   data=df, palette=\"RdBu\")\n\n\ng.set_title(\"Credit Distribution by Purpose\", fontsize=16)\ng.set_xticklabels(g.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\ng.set_xlabel('Device Names', fontsize=18) # Xlabel\ng.set_ylabel('Trans Revenue(log) Dist', fontsize=18) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0a060d3747a9e53c09b0d27676305f3f09605ad","_cell_guid":"f59664aa-1b3b-423a-8a0a-bbc91eb5e0fd"},"cell_type":"markdown","source":"# Analysis by Group:\n\n## Gender Analysis:\n<a id=\"gender_analysis\"></a>\nIn this section analyze the gender section of our dataset.\n\n### Objectives: \n<ul>\n<li> Find the distribution of genders in our dataset. </li>\n<li> See the distribution o each gender by the age (For instance, we have a higher number of young males than younger females) </li>\n<li> What were the main application reasons for a credit loan? Does it vary by Gender?</li>\n<li> How many jobs does each gender have? How many are Unemployed?</li>\n</ul>\n\n### Summary:\n<ul>\n<li>Theres <b>2x more males</b> than females in our dataset. </li>\n<li>Most females that applied for a credit loan were<b> less than 30</b> .</li>\n<li>Most of the males that applied for a loan ranged from their <b>20s-40s</b></li>\n<li>  Females were more likely to apply for a credit loan tobuy <b>furniture and equipment</b>. (10% more than males)</li>\n<li> Males applied 2x more than females for a credit loan to invest in a <b>business</b>. </li>\n<li> 2x of females were <b>unemployed</b> compared to males. </li>\n<li> 2x of males <b>worked 3 jobs</b> compared to females. </li>\n<li> Suprisingly, most people that applied for a credit loan have <b>two jobs</b>! </li>\n\n</ul>\n\n"},{"metadata":{"_uuid":"703850e4a041c92535a655b549c7bf1f59bbf432","_cell_guid":"93cfb73c-4f91-48bc-b8ee-f38b101467e2","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# We have 2x more German males applying for Credit Loans than Females.\ndf[\"Sex\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"408dd45c9b6ba169051403b928d8b2ec81387781","_kg_hide-input":true,"_cell_guid":"978f947e-88a1-4e5d-884a-e203e9e647ed","trusted":true},"cell_type":"code","source":"from plotly import tools\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\nby_age = df['Age'].values.tolist()\nmale_age = df['Age'].loc[df['Sex'] == 'male'].values.tolist()\nfemale_age = df['Age'].loc[df['Sex'] == 'female'].values.tolist()\n\ntrace0 = go.Histogram(\n    x=male_age,\n    histnorm='probability',\n    name=\"German Male\",\n    marker = dict(\n        color = 'rgba(100, 149, 237, 0.6)',\n    )\n)\ntrace1 = go.Histogram(\n    x=female_age,\n    histnorm='probability',\n    name=\"German Female\",\n    marker = dict(\n        color = 'rgba(255, 182, 193, 0.6)',\n    )\n)\ntrace2 = go.Histogram(\n    x=by_age,\n    histnorm='probability',\n    name=\"Overall Gender\",\n     marker = dict(\n        color = 'rgba(169, 169, 169, 0.6)',\n    )\n)\nfig = tools.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Males','Female', 'All Genders'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='Distribution of Gender', bargap=0.05)\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c88abe5b8ce1a38ca797150f649cbc12f78cce30","_cell_guid":"f37b7369-d022-4045-8cb7-a0c0af788f50","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Gender vs Purpose let's see the purpose of having credit loans for each gender.\ndf[\"Purpose\"].unique()\nsex_purpose = pd.crosstab(df['Purpose'], df['Sex']).apply(lambda x: x/x.sum() * 100)\nsex_purpose","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13aaee4f8e7ff4ee347a134deb70b2ca04ef5f46","_kg_hide-input":true,"_cell_guid":"98754d43-183d-4fc0-8115-fd58cc8af1c6","trusted":true},"cell_type":"code","source":"# Values into list\nm_purpose = sex_purpose['male'].values.tolist()\nf_purpose = sex_purpose['female'].values.tolist()\n\n# Round values\nmale_purpose = ['%.2f' % elem for elem in m_purpose]\nfemale_purpose = ['%.2f' % elem for elem in f_purpose]\n\nmale = go.Bar(\n    y=['business', 'car', 'domestic appliances', 'education', 'furniture/equipment',\n      'radio/TV', 'repairs', 'vacation/others'],\n    x=male_purpose,\n    name='German Males',\n    text='(%)',\n    orientation = 'h',\n    marker = dict(\n        color = 'rgba(72, 92, 238, 0.6)',\n        line = dict(\n            color = 'rgba(72, 92, 238, 1.0)',\n            width = 3)\n    )\n)\nfemale = go.Bar(\n    y=['business', 'car', 'domestic appliances', 'education', 'furniture/equipment',\n      'radio/TV', 'repairs', 'vacation/others'],\n    x=female_purpose,\n    name='German Females',\n    text='(%)',\n    orientation = 'h',\n    marker = dict(\n        color = 'rgba(217, 128, 234, 0.6)',\n        line = dict(\n            color = 'rgba(217, 128, 234, 1.0)',\n            width = 3)\n    )\n)\n\ndata = [male, female]\nlayout = dict(\n    title='Application Reasons for a Loan by Gender',\n     xaxis=dict(\n        title='(% of Gender)',\n        titlefont=dict(\n            family='Arial, sans-serif',\n            size=11,\n            color='black'\n        ),\n        showticklabels=True,\n        tickangle=45,\n        tickfont=dict(\n            family='Old Standard TT, serif',\n            size=14,\n            color='black'\n        ),\n        exponentformat='e',\n        showexponent='all'\n    ),\n    yaxis=dict(\n        title='Purpose for the Loan',\n        titlefont=dict(\n            family='Arial, sans-serif',\n            size=11,\n            color='black'\n        ),\n        showticklabels=True,\n        tickangle=40,\n        tickfont=dict(\n            family='Old Standard TT, serif',\n            size=10,\n            color='black'\n        ),\n        exponentformat='e',\n        showexponent='all'\n    ),\n    barmode='stack'\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='marker-h-bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ce8b4d45601dbc8531032f718a7614132f2d6a","_cell_guid":"d5a6f4d3-d16c-4261-8b2b-7cda6666e387","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Percent distribution of Gender (how many jobs does each gender have in %)\nnumber_of_jobs = pd.crosstab(df[\"Job\"], df[\"Sex\"]).apply(lambda x: x/x.sum() * 100)\nnumber_of_jobs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9570e3d03734c68c5e5bc33d0858fac19676326d","_kg_hide-input":true,"_cell_guid":"3b285a70-dc4a-4dc8-bf3d-6bc0037abafc","trusted":true},"cell_type":"code","source":"# # Values to list\n# job_number_m = number_of_jobs[\"male\"].values.tolist()\n# job_number_f = number_of_jobs[\"female\"].values.tolist()\n\n# # Round elements\n# job_number_male = [\"%.2f\" % element for element in job_number_m]\n# job_number_female = [\"%.2f\" % element for element in job_number_f]\n\n# fig = {\n#   \"data\": [\n#     {\n#       \"values\": job_number_male,\n#       \"labels\": [\n#         \"Unemployed\",\n#         \"One Job\",\n#         \"Two Jobs\",\n#         \"Three Jobs\"\n#       ],\n#         'marker': {'colors': ['rgb(232, 71, 64)',\n#                              'rgb(64, 232, 102)',\n#                              'rgb(64, 181, 232)',\n#                              'rgb(163, 64, 232)'],\n#                   'line': {\n#                   'color': '#000000', 'width':2,\n#               },\n#                   },\n#         \"text\": \"Males\",\n#       \"domain\": {\"x\": [0, .48]},\n#       \"name\": \"German Males\",\n#       \"hoverinfo\":\"label+percent+name\",\n#       \"hole\": .4,\n#       \"type\": \"pie\"\n#     },     \n#     {\n#       \"values\": job_number_female,\n#       \"labels\": [\n#         \"Unemployed\",\n#         \"One Job\",\n#         \"Two Jobs\",\n#         \"Three Jobs\"\n#       ],\n#     'marker': {'colors': ['rgb(232, 71, 64)',\n#                              'rgb(64, 232, 102)',\n#                              'rgb(64, 181, 232)',\n#                              'rgb(163, 64, 232)'],\n#               'line': {\n#                   'color': '#000000', 'width':2,\n#               },\n#               },\n#       \"text\":\"Females\",\n#       \"textposition\":\"inside\",\n#       \"domain\": {\"x\": [.52, 1]},\n#       \"name\": \"German Females\",\n#       \"hoverinfo\":\"label+percent+name\",\n#       \"hole\": .4,\n#       \"type\": \"pie\"\n#     }],\n#   \"layout\": {\n#         \"title\":\"Number of Jobs taken by Gender\",\n#       \"font\": {\n#           \"size\": 14\n#               },\n#         \"annotations\": [\n#             {\n#                 \"font\": {\n#                     \"size\": 12\n#                 },\n#                 \"showarrow\": False,\n#                 \"text\": \"Males\",\n#                 \"x\": 0.20,\n#                 \"y\": 0.5\n#             },\n#             {\n#                 \"font\": {\n#                     \"size\": 12\n#                 },\n#                 \"showarrow\": False,\n#                 \"text\": \"Females\",\n#                 \"x\": 0.815,\n#                 \"y\": 0.5\n#             }\n#         ]\n#     }\n# }\n# iplot(fig, filename='donut')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bef97f594277f549a991c7c2c8eced2af3dd5731","_cell_guid":"8b199fd4-c533-4fac-a4cb-93d433ebf2ee"},"cell_type":"markdown","source":"## Age Groups:\n<a id=\"age_groups\"></a>\nIn this section we will create categorical groups based on the age column. The following categorical variables will belong to the <b> \"Age_Group\"</b> column:\n<ul>\n<li> <b>Young: </b> Clients  age ranges from (19 - 29). </li>\n<li> <b>Young Adults: </b> Clients age ranges from (30-40) </li>\n<li> <b> Senior: </b>  Clients age ranges from (41-55) </li>\n<li> <b>Elder: </b> Clients age is more than 55 years old </li>\n</ul>\n\n## What we want to accomplish:\n<ul>\n<li> Create different age groups based on their age. </li>\n<li> See the Credit amounts borrowed by clients belonging to each age group.</li>\n<li> Get deeper in our analysis and determine which loans were high risk and see if there is any patterns with regards to age groups.  </li>\n</ul>\n\n## Summary:\n<ul>\n<li> The <b>younger age group</b> tended to ask slightly for higher loans compared to the older age groups. </li>\n<li> The young and elederly groups had the <b>highest ratio</b> of high risk loans. With <b>45.29%</b> of all the clients that belong to the young age group being considered of high risk. </li>\n<li> The number of loans that were considered of high risk within the elderly group is<b> 44.28%</b> of the total amount of people considered in the elderly group. </li>\n<li> Interesting enough these are the groups that are most likely to be<b> unemployed or working part-time</b>, since the youngest group either <b>don't have the experience</b> to have a job or they are <b>studying in a university</b> so they don't have enough time to work in a full-time job. </li>\n<li>In the elderly group side, this is the group that are most likely receiving their money from their <b>pensions</b>, meaning the elderly group is most likely unemployed or working part-time. </li>\n</ul>"},{"metadata":{"_uuid":"e0bddb5c85d8df61fe2660de44e5cf9dd54a9919","_cell_guid":"d0862a9f-be55-4899-a72d-ddcec2f147f7","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ok we have to create for each group risky and non-risky loans.\ndf['Age_Group'] = np.nan\n\nlst = [df]\n\nfor col in lst:\n    col.loc[(col['Age'] > 18) & (col['Age'] <= 29), 'Age_Group'] = 'Young'\n    col.loc[(col['Age'] > 29) & (col['Age'] <= 40), 'Age_Group'] = 'Young Adults'\n    col.loc[(col['Age'] > 40) & (col['Age'] <= 55), 'Age_Group'] = 'Senior'\n    col.loc[col['Age'] > 55, 'Age_Group'] = 'Elder' \n    \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34f1b281d4372442ae3652cca128bb55e3a6cc7b","_kg_hide-input":true,"_cell_guid":"3b762e57-2c47-4aa4-9893-87e1ecaf3d14","trusted":true},"cell_type":"code","source":"# Take credit amount values into a list\nyoung = df['Credit_amount'].loc[df['Age_Group'] == 'Young'].values.tolist()\nyoung_adults = df['Credit_amount'].loc[df['Age_Group'] == 'Young Adults'].values.tolist()\nsenior = df['Credit_amount'].loc[df['Age_Group'] == 'Senior'].values.tolist()\nelder_credit = df['Credit_amount'].loc[df['Age_Group'] == 'Elder'].values.tolist()\n\n# Create the box plots by age category\nyoung_credit = go.Box(\n    y = young,\n    name = \"Young\",\n    jitter = 0.3,\n    pointpos = -1.8,\n    boxpoints = 'all',\n    marker = dict(\n        color = 'rgb(150, 198, 109)'),\n    line = dict(\n        color = 'rgb(111, 200, 37)')\n)\n\nyoung_adults_credit = go.Box(\n    y = young_adults,\n    name = \"Young Adults\",\n    jitter = 0.3,\n    pointpos = -1.8,\n    boxpoints = 'all',\n    marker = dict(\n        color = 'rgb(124, 236, 212)'),\n    line = dict(\n        color = 'rgb(38, 214, 177)')\n)\n\nsenior_credit = go.Box(\n    y = senior,\n    name = \"Seniors\",\n    jitter = 0.3,\n    pointpos = -1.8,\n    boxpoints = 'all',\n    marker = dict(\n        color = 'rgb(241, 93, 93)'),\n    line = dict(\n        color = 'rgb(225, 44, 44)')\n)\n\nelder_credit = go.Box(\n    y = elder_credit,\n    name = \"Elders\",\n    jitter = 0.3,\n    pointpos = -1.8,\n    boxpoints = 'all',\n    marker = dict(\n        color = 'rgb(180, 121, 72)'),\n    line = dict(\n        color = 'rgb(115, 77, 46)')\n)\n\ndata = [young_credit, young_adults_credit, senior_credit, elder_credit]\n\nlayout = dict(\n    title=\"Credit Amount by Age Group Segment\", \n    xaxis = dict(title=\"Age Group\"),\n    yaxis= dict(title=\"Credit Amount\")\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename=\"Box Plot\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45c87f741076ba855ecebb3442d694ece0d21474","_cell_guid":"48f9520a-48b2-4282-88a2-18d26334f0fa","trusted":true},"cell_type":"code","source":"good_credit_df = df.loc[df['Risk'] == \"good\"]\nbad_credit_df = df.loc[df['Risk'] == \"bad\"]\n\ngood_credit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f9071fe63fc6730a60077374fb8e34639df4529","_kg_hide-input":true,"_cell_guid":"8cd4d104-e777-4ce7-9f0f-3a95fbe7ed96","trusted":true},"cell_type":"code","source":"fig = {\n    \"data\": [\n        {\n            \"type\": 'violin',\n            \"x\": good_credit_df['Age_Group'],\n            \"y\": good_credit_df['Credit_amount'],\n            \"legendgroup\": 'Good Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Good Credit',\n            \"side\": 'negative',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'rgb(34, 178, 178)'\n            }\n        },\n        {\n            \"type\": 'violin',\n            \"x\": bad_credit_df['Age_Group'],\n            \"y\": bad_credit_df['Credit_amount'],\n            \"legendgroup\": 'Bad Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Bad Credit',\n            \"side\": 'positive',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'rgb(178, 34, 34)'\n            }\n        }\n    ],\n    \"layout\" : {\n        \"title\": \"Distribution of Credit Borrowed by Age Group\",\n        \"yaxis\": {\n            \"zeroline\": False,\n        },\n        \"violingap\": 0,\n        \"violinmode\": \"overlay\"\n    }\n}\n\n\niplot(fig, filename = 'Age-Housing', validate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aa4ce53fe0d5505fb7e9d44de6e8edb45679c48","_kg_hide-input":true,"_cell_guid":"9a108709-86f3-4f77-8f57-dc1f5ac85736","trusted":true},"cell_type":"code","source":"# Lets find loans by age group and by the level of risk and plot them in a bar chart.\n\n# Age Group Segments\nyoung_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Young') & (df['Risk'] == 'good')].sum()\nyoung_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Young') & (df['Risk'] == 'bad')].sum()\nyoung_adult_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Young Adults') & (df['Risk'] == 'good')].sum()\nyoung_adult_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Young Adults') & (df['Risk'] == 'bad')].sum()\nsenior_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Senior') & (df['Risk'] == 'good')].sum()\nsenior_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Senior') & (df['Risk'] == 'bad')].sum()\nelder_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Elder') & (df['Risk'] == 'good')].sum()\nelder_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Elder') & (df['Risk'] == 'bad')].sum()\n\n# Percents\nyoung_good_p = young_good/(young_good + young_bad) * 100\nyoung_bad_p = young_bad/(young_good + young_bad) * 100\nyoung_adult_good_p = young_adult_good/(young_adult_good + young_adult_bad) * 100\nyoung_adult_bad_p = young_adult_bad/(young_adult_good + young_adult_bad) * 100\nsenior_good_p = senior_good/(senior_good + senior_bad) * 100\nsenior_bad_p =  senior_bad/(senior_good + senior_bad) * 100\nelder_good_p = elder_good/(elder_good + elder_bad) * 100\nelder_bad_p = elder_bad/(elder_good + elder_bad) * 100\n\n# Round Percents\nyoung_good_p = str(round(young_good_p, 2))\nyoung_bad_p = str(round(young_bad_p, 2))\nyoung_adult_good_p = str(round(young_adult_good_p, 2))\nyoung_adult_bad_p = str(round(young_adult_bad_p, 2))\nsenior_good_p = str(round(senior_good_p, 2))\nsenior_bad_p = str(round(senior_bad_p, 2))\nelder_good_p = str(round(elder_good_p, 2))\nelder_bad_p = str(round(elder_bad_p, 2))\n\n\n\nx = [\"Young\", \"Young Adults\", \"Senior\", \"Elder\"]\n\ngood_loans = go.Bar(\n    x=x,\n    y=[young_good, young_adult_good, senior_good, elder_good],\n    name=\"Good Loans\",\n    text=[young_good_p + '%', young_adult_good_p + '%', senior_good_p + '%', elder_good_p + '%'],\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(111, 235, 146)',\n        line=dict(\n            color='rgb(60, 199, 100)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbad_loans =  go.Bar(\n    x=x,\n    y=[young_bad, young_adult_bad, senior_bad, elder_bad],\n    name=\"Bad Loans\",\n    text=[young_bad_p + '%', young_adult_bad_p + '%', senior_bad_p + '%', elder_bad_p + '%'],\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(247, 98, 98)',\n        line=dict(\n            color='rgb(225, 56, 56)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\ndata = [good_loans, bad_loans]\n\nlayout = dict(\n    title=\"Type of Loan by Age Group\", \n    xaxis = dict(title=\"Age Group\"),\n    yaxis= dict(title=\"Credit Amount\")\n)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig, filename='grouped-bar-direct-labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b893b50092cf4c54f6ac4a01d528574bbfe47573","_cell_guid":"840d8ace-8100-4128-9286-ce453e15fa36"},"cell_type":"markdown","source":"## Wealth Analysis:\n<a id=\"wealth_analysis\"></a>\nIn this section we will analyse the amount of wealth our clients have by analyzing their checking accounts and whether the wealth status of our clients contribute to the risk of the loans Lending Club is issuing to customers.\n\n## Summary: \n<ul>\n<li> Individuals belonging to the <b>\"little wealth\"</b>  group, had a higher probability of being bad risk loans than other types fo groups.</li>\n<li> The <b>higher the wealth</b>, the lower the probability of being a bad risk loan. </li>\n</ul>"},{"metadata":{"_uuid":"691139cfb49ae95acfe374ece4a8cb34e1210c29","_cell_guid":"16a3f5c9-71fa-4563-9266-9f6d128dc0a5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# We have some missing value so we will just ignore the missing values in this analysis.\ndf[\"Checking account\"].unique()\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3fcb4d7dea84fe9953161b3f9fb2667c91f8385","_cell_guid":"5aa6d9fe-436f-4afc-b5a0-c0a00bac14c0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cross_checking = pd.crosstab(df['Risk'], df['Checking account']).apply(lambda x: x/x.sum() * 100)\ndecimals = pd.Series([2,2,2], index=['little', 'moderate', 'rich'])\n\ncross_checking = cross_checking.round(decimals)\ncross_checking","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d30dac529d7e8ef84c69e580753c86ab0e67d5f","_cell_guid":"4300b0ff-4726-49bc-b995-d8fc3db99cfc","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\n\nbad_little = cross_checking['little'][0]\ngood_little = cross_checking['little'][1]\nbad_moderate = cross_checking['moderate'][0]\ngood_moderate = cross_checking['moderate'][1]\nbad_rich = cross_checking['rich'][0]\ngood_rich = cross_checking['rich'][1]\n\nlittle = go.Bar(\n    x=['Bad Risk', 'Good Risk'],\n    y=[bad_little, good_little],\n    name=\"Little Wealth\",\n    text='%',\n     marker=dict(\n        color='#FF5050',\n         line=dict(\n            color='#E13232',\n            width=1.5),\n    ),\n)\n\nmoderate = go.Bar(\n    x=['Bad Risk', 'Good Risk'],\n    y=[bad_moderate, good_moderate],\n    name=\"Moderate Wealth\",\n    text='%',\n    xaxis='x2',\n    yaxis='y2',\n    marker=dict(\n        color='#FFB232',\n         line=dict(\n            color='#CD8000',\n            width=1.5),\n    ),\n)\nrich = go.Bar(\n    x=['Bad Risk', 'Good Risk'],\n    y=[bad_rich, good_rich],\n    name=\"Rich Wealth\",\n    text=\"%\",\n    marker=dict(\n        color='#8DFF83',\n         line=dict(\n            color='#3DAF33',\n            width=1.5),\n    ),\n    xaxis='x3',\n    yaxis='y3'\n)\n\ndata=[little, moderate, rich]\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=('Little Wealth', 'Moderate Wealth', 'Rich Wealth'))\n\nfig.append_trace(little, 1, 1)\nfig.append_trace(moderate, 1, 2)\nfig.append_trace(rich, 1, 3)\n\nfig['layout'].update(height=400, width=800, title='Levels of Risk' +\n                                                  ' by Wealth')\n\n\niplot(fig, filename='make-subplots-multiple-with-titles')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57b3e88984794ca89c941d4e98195865fa041098","_cell_guid":"cf5dcb9e-33e3-496b-9c80-f220765899aa"},"cell_type":"markdown","source":"## High Risk Loans vs Low Risk Loans:\nIn this section we will analyze both high and low risk loans. The most important thing is to find patters that could describe the some sort of correlation with these output values.\n\n## Correlation (Our intent):\n<a id=\"correlations\"></a>\nIn this part of the analysis, we want to look as to what feature affect directly the risk of the loan. In order to see these patterns, the first thing we have to do is to create a new column named \"Risk_int\" (Stands for risk in integer form) and involve this column in the correlation heatmap plot. \"0\" will stand for \"bad risk\" loans and \"1\" will stand for \"good risk\" loans.\n\n## Summary:\n<ul>\n<li> The higher the <b>credit amount</b> borrowed, the most likely the loan will end up <b>bad</b>.</li>\n<li> The higher the <b>duration</b> of the loan, the most likely the loan will turn out to be <b> bad</b></li>\n<li><b>Senior</b> and <b> Elders </b> that asked for loans over 12k, have a high chance of becoming <b>bad loans </b> </li>\n<li>If the credit amount borrowed is <b>equivalent to 11,000 or more</b>, the probability for the loan to be a bad one increases drastically. (Observe the Correlation of Risk with Credit Amount Borrowed.) </li>\n\n</ul>"},{"metadata":{"_uuid":"5af96da09914219ebe65a1cf834e3ae042f2b7c2","_cell_guid":"421cae3b-ebdd-40f6-b37c-a3d78f2d95f7","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['Risk_int'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[df['Risk'] == 'bad', 'Risk_int'] = 0 \n    col.loc[df['Risk'] == 'good', 'Risk_int'] = 1\n    \n    \ndf['Risk_int'] = df['Risk_int'].astype(int)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e13e80f522ed44b47d654a9f7c8a7783543bba98","_cell_guid":"89ceead0-7bcc-4924-9253-7fdb5c979c6c","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.plotly as py\nimport plotly.figure_factory as ff\n\n\ncorr = df.corr()\n\narr_corr = corr.values\narr_corr = np.around(arr_corr, decimals=2)\n\ncolumns = corr.columns.values.tolist()\n\nfig = ff.create_annotated_heatmap(arr_corr, x=columns, y=columns, colorscale='Reds')\n\nfig.layout.title = 'Pearson Correlation <br> (What Determines Risk in Loans?)'\niplot(fig, filename='annotated_heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb586d599e9bacb61efba3b915ab1bf8e37723bc","_cell_guid":"c5f58ffd-c7d2-4be6-bbfc-17c450cd796d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# ---> Looking at correlations in the form of Scatterplots.\n\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\n\nimport numpy as np\nimport pandas as pd\n\nnumeric_vals = df.select_dtypes(exclude=['object'])\n\n\nrisks = df['Risk'].values.tolist()\n\ncolormap = ['#b22222', '#4169e1']\n\nnumeric_matrix = numeric_vals.as_matrix()\n\n\ndataframe = pd.DataFrame(numeric_matrix,\n                         columns=['Credit_amount', 'Duration', 'Job', 'Age', 'Risk'])\n\ndataframe['Risk'] = pd.Series(risks)\n\n\nfig = ff.create_scatterplotmatrix(dataframe, diag='histogram', index='Risk', colormap=colormap,\n                                  height=800, width=800)\niplot(fig, filename='Histograms along Diagonal Subplots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eaf8f188638e018c636fd6582ed70d06031bfb9","_kg_hide-input":true,"_cell_guid":"e9425b4a-51f5-4629-a9d8-1e15106a41a5","trusted":true},"cell_type":"code","source":"# The higher the credit amount the higher the risk of the loan. Scatter plot?\n# The higher the duration of the loan the higher the risk of the loan?\n\nbad_credit_amount = df[\"Credit_amount\"].loc[df['Risk'] == 'bad'].values.tolist()\ngood_credit_amount = df[\"Credit_amount\"].loc[df['Risk'] == 'good'].values.tolist()\nbad_duration = df['Duration'].loc[df['Risk'] == 'bad'].values.tolist()\ngood_duration = df['Duration'].loc[df['Risk'] == 'good'].values.tolist()\n\n\nbad_loans = go.Scatter(\n    x = bad_duration,\n    y = bad_credit_amount,\n    name = 'Bad Loans',\n    mode = 'markers',\n    marker = dict(\n        size = 10,\n        color = 'rgba(152, 0, 0, .8)',\n        line = dict(\n            width = 2,\n            color = 'rgb(0, 0, 0)'\n        )\n    )\n)\n\ngood_loans = go.Scatter(\n    x = good_duration,\n    y = good_credit_amount,\n    name = 'Good Loans',\n    mode = 'markers',\n    marker = dict(\n        size = 10,\n        color = 'rgba(34, 139, 34, .9)',\n        line = dict(\n            width = 2,\n        )\n    )\n)\n\ndata = [bad_loans, good_loans]\n\nlayout = dict(title = 'Correlation of Risk with <br> Credit Amount Borrowed',\n              yaxis = dict(zeroline = False),\n              xaxis = dict(zeroline = False)\n             )\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='styled-scatter')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16d6fb8c011479c603a35e25154d0d3338a0421a","_cell_guid":"4e1e259d-bac1-4f66-92cc-2091393fa489"},"cell_type":"markdown","source":"## Exploring Purposes of Loans:\n<a id=\"purpose_loans\"></a>\nIn this section my main aim is to see what purposes where most likely to bring most risk, in other words which of these pruposes were more likely to be considered high risk loans. Also, I would like to explore the operative side of the business, by determining which purposes where the ones that contributed the most towards loans issued.\n\n## Summary: \n<ul> \n<li><b>Cars</b>, <b> Radio/TV</b> and <b> Furniture and Equipment</b> made more than 50 % of the total risk and has the <b>highest distribution</b> of credit issued</li>\n<li>The <b> rest of the purposes</b> were not frequent purposes in applying for a loan. </li>\n<li> <b>Cars</b> and <b>Radio/TV</b> purposes were the less risky from the operative perspective since it had the widest gap between good and bad risk. </li>\n</ul>"},{"metadata":{"_uuid":"84d33c436534663b2a6f0257f30453f1887d503b","_cell_guid":"375aafcb-b1e9-40bf-8239-13007132c16a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\n\nradio_tv = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'radio/TV'].values)\neducation = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'education'].values)\nfurniture = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'furniture/equipment'].values)\ncar = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'car'].values)\nbusiness = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'business'].values)\ndomestic_app = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'domestic appliances'].values)\nrepairs = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'repairs'].values)\nvacation_others = np.sum(df['Credit_amount'].loc[df['Purpose'] == 'vacation/others'].values)\n\ndf_purposes = pd.DataFrame(data=[[radio_tv, education, furniture, car, business, domestic_app, repairs, vacation_others]],\n                          columns=df[\"Purpose\"].unique())\n\nprint(df_purposes)\n\ndata = [\n    go.Scatterpolar(\n        r = [radio_tv, education, furniture,car, business, domestic_app, repairs, vacation_others],\n        theta = df['Purpose'].unique().tolist(),\n        line = dict(\n            color = \"#ff7f50\"\n        ),\n        mode = 'lines+markers',\n        fill='toself',\n        marker = dict(\n            color = '#8090c7',\n            symbol='square',\n            size=6\n        )\n    )\n]\n\nlayout = go.Layout(\n    showlegend = False,\n    title= \"Credit Distribution by Purpose\",\n    paper_bgcolor = \"rgb(255, 245, 250)\"\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename = 'polar/basic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8aef8e9052453a894ee288e08dfeb70691c5aec","_cell_guid":"9169e845-ee89-43fd-954f-dd3e2e50446d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['Purpose'].unique()\n\ncross_purpose = pd.crosstab(df['Purpose'], df['Risk']).apply(lambda x: x/x.sum() * 100)\ncross_purpose = cross_purpose.round(decimals=2)\ncross_purpose.sort_values(by=['bad'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b1108b12f7759d7a52dfbcb19f260d78532c722","_cell_guid":"c5daf0ca-5075-4c7f-9cd6-fae182ef4d79","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This will be x\npurposes = df['Purpose'].unique().tolist()\npurposes = sorted(purposes)\n\ngood_purposes = cross_purpose[\"good\"].values.tolist()\nbad_purposes = cross_purpose[\"bad\"].values.tolist()\n\n\n\n\ngood = go.Bar(\n    x=purposes,\n    y=good_purposes,\n    name='Good Risk',\n    text='%',\n    marker=dict(\n        color='rgb(123, 255, 255)',\n        line=dict(\n            color='rgb(0, 213, 213)',\n            width=1.5\n        )\n    ),\n)\nbad = go.Bar(\n    x=purposes,\n    y=bad_purposes,\n    name='Bad Risk',\n    text = '%',\n    marker=dict(\n        color='rgb(255, 123, 123)',\n        line=dict(\n            color='rgb(213, 0, 0)',\n            width=1.5\n        )\n    ),\n)\ndata = [good, bad]\nlayout = go.Layout(\n    title='Which Purpose Carried the Highest Risk?',\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Percentage(%)',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='style-bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d1873c77406e2e835cab64ba3dbdaef0e49d75f","_cell_guid":"0e50e6b1-4c31-4fca-93d3-191ba8caac48","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cross_credit = df.groupby(['Risk', 'Purpose'], as_index=False).Credit_amount.sum()\n\ngood_amnt = cross_credit[\"Credit_amount\"].loc[cross_credit[\"Risk\"] == \"good\"].values.tolist()\nbad_amnt = cross_credit[\"Credit_amount\"].loc[cross_credit[\"Risk\"] == \"bad\"].values.tolist()\n\ngood_risk = {\"x\": good_amnt, \n          \"y\": purposes, \n          \"marker\": {\"color\": \"rgb(28, 98, 98)\", \"size\": 12}, \n          \"mode\": \"markers\", \n          \"name\": \"Good Risk\", \n          \"type\": \"scatter\"\n}\n\nbad_risk = {\"x\": bad_amnt, \n          \"y\": purposes, \n          \"marker\": {\"color\": \"rgb(98, 28, 28)\", \"size\": 12}, \n          \"mode\": \"markers\", \n          \"name\": \"Bad Risk\", \n          \"type\": \"scatter\", \n}\n\ndata = [good_risk, bad_risk]\nlayout = {\"title\": \"Credit Loan Applications by Purpose\", \n          \"xaxis\": {\"title\": \"Credit Amount Borrowed\", }, \n          \"yaxis\": {\"title\": \"\"},\n         \"width\": 800,\n         \"height\": 700}\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='basic_dot-plot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfcf39ea583e6c22ec36f926a269b2759dbf6b0c","_cell_guid":"4b41a30b-6ba2-4f0b-a7d4-1b215da7e5dc"},"cell_type":"markdown","source":"## Predictive Modelling:\n<a id=\"predictive_modelling\"></a>\n"},{"metadata":{"_uuid":"8d0141b8552ceffe9bdbe79b7167709c57534505","_cell_guid":"b3dfb863-09d8-47ea-b0d3-5d08c2bf2b03","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check missing values in our dataframe\noriginal_df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b51b458bc510e9a70f8b2572193b806fe20d85d5","_cell_guid":"ebe192d1-6d3a-427a-9435-a350cc1d021f","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# We will drop the columns that have missing values although we will be loosing some information. Hopefully this does not cause\n# the model to underfit in the future.\noriginal_df.drop(['Checking account', 'Saving accounts'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f5036d3329f393948546f91d45a97d9ce9df638","_cell_guid":"3dde4894-9326-49b0-a647-11b234ad36ec","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"original_df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8206bcbe93d4507fb3bc4a0f81b80e551541bf29","_cell_guid":"03af2937-f096-4b70-9b07-2293053a66d0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Feature Engineering (We cannot delete the missing values because we have too litle information)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n\noriginal_df[\"Risk\"].value_counts() # 70% is good risk and 30% is bad risk.\n\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in stratified.split(original_df, original_df[\"Risk\"]):\n    strat_train = original_df.loc[train]\n    strat_test = original_df.loc[test]\n    \n\n# The main purpose of this code is to have an approximate ratio\n# of 70% good risk and 30% bad risk in both training and testing sets.\nstrat_train[\"Risk\"].value_counts() / len(df) \nstrat_test[\"Risk\"].value_counts() / len(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"627cec745125a6a2453df47f66c87df475802f2a","_cell_guid":"4cf81fb3-2fa4-4c8e-88c2-c8cb77f82711","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Have our new train and test data\ntrain = strat_train\ntest = strat_test\n\n\n# Our features\nX_train = train.drop('Risk', axis=1)\nX_test = test.drop('Risk', axis=1)\n\n# Our Labels we will use them later\ny_train = train[\"Risk\"]\ny_test = test[\"Risk\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44cf6ca2a60fa5ab62569cdcaabe4b208d0d28d3","_cell_guid":"b31b9dfc-924f-442a-ac2c-a3eda6871acb","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfb7eb415eca8f20dedfcd5b747c3c8e784f11cc","_cell_guid":"310dd36b-c8ff-4930-b81d-49b17e1863f7","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Scikit-Learn does not handle dataframes in pipeline so we will create our own class.\n# Reference: Hands-On Machine Learning\nfrom sklearn.base import BaseEstimator, TransformerMixin\n# Create a class to select numerical or cateogrical columns.\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit (self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ae9aef2f3fe79660a155da986cc5d2f6db3b057","_cell_guid":"d44b4943-0e33-400a-8a6f-9fc2ce1ce8cb","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\n\nnumeric_train_df = X_train.select_dtypes(exclude=['object'])\nnumeric_test_df = X_test.select_dtypes(exclude=['object'])\n\ncategorical_train_df = X_train.select_dtypes(['object'])\ncategorical_test_df = X_test.select_dtypes(['object'])\n\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector(numeric_train_df.columns.values.tolist())),\n    (\"std_scaler\", StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    ('select_categoric', DataFrameSelector(categorical_train_df.columns.values.tolist())),\n    ('encoding', CategoricalEncoder(encoding='onehot-dense'))\n])\n\n# Combine both pipelines\nmain_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', numerical_pipeline),\n    ('cat_pipeline', categorical_pipeline)\n])\n\nX_train_scaled = main_pipeline.fit_transform(X_train)\nX_test_scaled = main_pipeline.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57e9b86fda77c16e008336b6a073981abb5eafec","_cell_guid":"9c186137-6564-4579-8880-a28f3b75e573","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train_scaled = encode.fit_transform(y_train)\ny_test_scaled = encode.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03df2b50ece666194e1de1b81ddd3892f66e45b1","_cell_guid":"63805737-7b16-4ead-a62d-89fe8d3ceb38","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n# Implement gridsearchcv to see which are our best p\n\nparams = {'C': [0.75, 0.85, 0.95, 1], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [3, 4, 5]}\n\nsvc_clf = SVC(random_state=42)\n\ngrid_search_cv = GridSearchCV(svc_clf, params)\ngrid_search_cv.fit(X_train_scaled, y_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a452473502d3d2fcdd8bf112815b8ecdeba58255","_cell_guid":"8c641b1a-e154-4883-bc72-feaa233dc8d2","trusted":true},"cell_type":"code","source":"grid_search_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"205a4189070e4c99ba42ad34ad75e897edf5eb6f","_cell_guid":"d9951397-7326-46b9-ab2c-2c3df8ee4a27","trusted":true},"cell_type":"code","source":"grid_search_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6a013f452a144167fbae85877fa214670d35e98","_cell_guid":"fd3e765c-bc53-41ec-bea4-f930264f259c","trusted":true},"cell_type":"code","source":"svc_clf = grid_search_cv.best_estimator_\nsvc_clf.fit(X_train_scaled, y_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bc96e8702471bccc690d092d20b2634b37e6bf5","_cell_guid":"92c8ee6a-aadc-40c6-bc57-c393b9c11b1c","trusted":true},"cell_type":"code","source":"svc_clf.score(X_train_scaled, y_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21432e10eb36480dc95a6529b18f8890e567b0d1","_cell_guid":"8218ceb4-8c32-4f82-83c2-8213736ec5a3","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Let's make sure the data is not overfitting\nsvc_clf = SVC(kernel='rbf', C=1, random_state=42)\nscores = cross_val_score(svc_clf, X_train_scaled, y_train_scaled)\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea6c6d025c10f57f9b14f8afec162f6b935e6f04","_cell_guid":"cb852bfd-e350-46a5-a014-8171aea8209b","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nsvc_clf.fit(X_train_scaled, y_train_scaled)\ny_pred = svc_clf.predict(X_test_scaled)\n\n# Accuracy score 71%\naccuracy_score(y_test_scaled, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db1470c93a640e5615b12e5d3b155e5550ee27ad","_cell_guid":"9e505192-87ea-4707-96f9-a2361be0d799"},"cell_type":"markdown","source":"## References:\n<ul> \n<li> <a href=\"https://www.kaggle.com/kabure/german-credit-risk-financial-eda\"> German Credit risk [FinancialEDA]</a> by Leonardo Ferreira</li>\n\n</ul>\n\n### Note: This Kernel will be subjected to further updates."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}