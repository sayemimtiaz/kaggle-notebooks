{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem-level Performace Prediction Notebook\n\n## Dataset\nWe have a dataset of online learning activity from Junyi Academy. Junyi Academy Foundation is a non-profit organization based in Taiwan that aims to provide all children equitable quality education by technology. The dataset is divided into the following files - \n\n1. Log_Problem.csv - This has data about 16,217,311 problem attempts of 72,630 selected students for a year from 2018/08 to 2019/07.\n2. Info_Content.csv - This describes the metadata of the exercises, each exercise is a basic unit of learning consisted of many problems.\n3. Info_UserData.csv - This describes the metadata of the selected registered students in Junyi Academy.\n\nThe entire dataset can be downloaded from Kaggle - [Link](https://www.kaggle.com/junyiacademy/learning-activity-public-dataset-by-junyi-academy).\n\n## Features\n\nThe files have the following columns - \n\n### Log_Problem\n\n1. timestamp_TW - The timestamp of the first behavior, answered the problem or used a hint. It is in UTC+8 timezone.\n2. uuid - The unique ID of the user. It can be used to join with Info_UserData.\n3. ucid - The unique ID of the content. It can be used to join with Info_Content.\n4. upid - The unique ID of the problem.\n5. problem_number - The number of problems this user had encountered, including this problem, in this exercise.\n6. exercise_problem_repeat_session - The number of times the user encounters this problem in this exercise.\n7. is_correct - Whether the answer is considered correct or not. Only if the student answered the correct answer for the first time.\n8. total_sec_taken - How many seconds the user use for this problem encounter.\n9. total_attempt_cnt - How many times have the user submitted an answer for this problem encounter.\n10. used_hint_cnt - How many hints the user have used for this problem encounter.\n11. is_hint_used - Whether the user use a hint or not..\n12. is_downgrade - After this attempt, is the user upgraded to the next level.\n13. is_upgrade - After this attempt, is the user downgraded to the next level.\n14. level - After this attempt, which level does this user belong to in this exercise? There are five possible levels. All users start from level 0 and declare Proficient at level 4.\n\n***\n\n### Info_Content\n1. ucid - The hashed unique ID of the content.\n2. content_pretty_name - The Chinese display name of this content.\n3. content_kind - The kind of this content. The current dataset release only includes `Exercise`.\n4. difficulty - The difficulty of this content. There are four possible values: `Easy`, `Normal`, `Hard` and `Unset`. Unset means that this content has not been set to any difficulty yet.\n5. subject - The subject of this content. The current dataset release only includes `math`.\n6. learning_stage - The learning stage of this content. There are three possible values: `Elementary`, `Junior` and `Senior`.\n7. level1_id - The hashed level 1 layer ID of this content. The levels form the tree-like hierarchy structure of contents in Junyi Academy. The current dataset release has four levels in the hierarchy.\n8. level2_id - The hashed level 2 layer ID of this content. The levels form the tree-like hierarchy structure of contents in Junyi Academy. The current dataset release has four levels in the hierarchy.\n9. level3_id - The hashed level 3 layer ID of this content. The levels form the tree-like hierarchy structure of contents in Junyi Academy. The current dataset release has four levels in the hierarchy.\n10. level4_id - The hashed level 4 layer ID of this content. The levels form the tree-like hierarchy structure of contents in Junyi Academy. The current dataset release has four levels in the hierarchy.\n\n***\n\n### Info_UserData\n1. uuid - The unique ID of this user.\n2. gender - The gender of this user. There are four possible values: `male`, `female`, `unspecified` and `null`.\n3. points - The user will receive energy points from the Junyi Academy after completing exercises, watching videos, and when the user receives a badge.\n4. badges_cnt - Badges are awarded to the users when the user achieves certain conditions.\n5. first_login_date_TW - The first login date after the user registers to Junyi Academy.\n6. user_grade - The grade of the user. The possible values are between 1 and 12.\n7. user_city - The resident city of the user.\n8. has_teacher_cnt - The number of teachers this user has in the Junyi Academy.\n9. is_self_coach - Does the user add himself/herself as a teacher of their own?\n10. has_student_cnt - The number of students this user has in the Junyi Academy. Despite the user role of this user is a student, this user can still add another user as a student.\n11. belongs_to_class_cnt - The number of classes this user belongs to.\n12. has_class_cnt - The number of classes this user created to add other users. Despite the user role of this user is a student, this user can still create a class to add other users in.\n\n***\n\nUsing the above features, the goal is to predict whether a student will answer a problem correct given the details of the problem and the student's performance history. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom collections import Counter, defaultdict\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initialize the constants\n\nIn the following cell, we initialize the various constants used throughout the notebook for various tasks. We have variables which control which parts of the notebook to run and variables which contains the paths to raw data (original data) and pre-processed data. \n\nThe pre-processed data has the following files -\n\n1. FILE_LOG_PROCESSED - This contains the data from the Log_Problem table sorted by timestamp. \n2. FILE_USER_PROCESSED - This contains the data from Info_UserData in parquet format for quick read from disk into memory. \n3. FILE_CONTENT_PROCESSED - This contains the data from Info_Content in parquet format for quick read from disk into memory.\n4. FILE_M_PROFICIENCY_LEVEL4 - This file has the one-hot encoding of all level_4 ids per problem. We use this to capture the learning history of a student. We will see more on this later.\n5. FILE_M_PROFICIENCY_CONCEPT = This file has the one-hot encoding of all concept Ids per problem. Again, we use this to capture the learning history of a student. We will see more on this later.\n6. FILE_V_UCID_ACC - This file has the accuracy per content id. (ie) the number of right answers received so far for the content id.\n7. FILE_V_UPID_ACC - This file has the accuracy per problem id. (ie) the number of right answers received so far for the problem id. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# - we shouldn't use this info before the user takes the exercise\nVARS_REDUNDANT = ['total_sec_taken','is_hint_used','is_downgrade','is_upgrade']\nVARS_LOG_CATEGORY = ['uuid', 'ucid', 'upid']\nVARS_CONTENT_CATEGORY = ['level3_id','level4_id']\n\n# - par\n# ORDER_MONTH = ['2018-08','2018-09','2018-10','2018-11','2018-12',\n#                '2019-01','2019-02','2019-03','2019-04','2019-05','2019-06','2019-07','2019-08']\n\n# - control which parts of the notebook to run\n# -- false if want to read the preprocessed files to save time\nRUN_PREPRECESS = False\nRUN_LEVEL4 = True\n\n# whether to compute the one-hot encoding vector for level4\nRUN_M_LEVEL4 = True\n\n# -- whether to compute the upid accuracy vector (False: read from input file)\nRUN_V_UPID_ACC = False\n\n# -- whether to compute the proficiency matrix (False: read from input file)\nRUN_M_PROFICIENCY = False\n\n# -- whether to compute the concept proficiency matrix.\nRUN_M_CONCEPT = False\n\nPLOT = True\n# MONTH_EXCLUDED = ['2018-08','2019-08']\n\n# -- False for reading only the top 1000 rows in the df_log\nRUN_FULL = True\n\n# - path\nPATH_INPUT = '/kaggle/input/learning-activity-public-dataset-by-junyi-academy/'\nPATH_OUTPUT = '/kaggle/working/'\nPATH_PREPROCESSED_INPUT = '../input/junyi-preprocessed/'\n\n# - file\n# -- raw timestamp\nFILE_LOG_FULL = os.path.join(PATH_PREPROCESSED_INPUT ,'Log_Problem_raw_timestamp.parquet.gzip')\n\nFILE_USER = os.path.join(PATH_INPUT,'Info_UserData.csv')\nFILE_CONTENT = os.path.join(PATH_INPUT,'Info_Content.csv')\n\n# -- read the preprocessed files to save time\n# --- raw timestamp\nFILE_LOG_PROCESSED = os.path.join(PATH_PREPROCESSED_INPUT ,'Processed_Log_Problem_raw_timestamp.parquet.gzip')\n\n# --- rounded timestamp\n# FILE_LOG_PROCESSED = os.path.join(PATH_PREPROCESSED_INPUT ,'Processed_Log_Problem.parquet.gzip')\nFILE_USER_PROCESSED = os.path.join(PATH_PREPROCESSED_INPUT ,'Processed_Info_UserData.parquet.gzip')\nFILE_CONTENT_PROCESSED = os.path.join(PATH_PREPROCESSED_INPUT ,'Processed_Info_Content.parquet.gzip')\n\n# FILE_M_HISTORY_LEVEL4 = os.path.join(PATH_PREPROCESSED_INPUT ,'m_history_level4.npz')\nFILE_M_PROFICIENCY_LEVEL4 = os.path.join(PATH_PREPROCESSED_INPUT ,'m_proficiency_level4.npz')\nFILE_M_PROFICIENCY_CONCEPT = os.path.join(PATH_PREPROCESSED_INPUT ,'m_concept_proficiency.npz')\nFILE_V_UCID_ACC = os.path.join(PATH_PREPROCESSED_INPUT ,'v_ucid_acc.npz')\nFILE_V_UPID_ACC = os.path.join(PATH_PREPROCESSED_INPUT ,'v_upid_acc.npz')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing\n\nThe following section contains the code for preprocessing the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the preprecessed file (1.5GB)\n# - should first run the preprocessing, save the files to output, and then download the files. \n# -- After that, \"Add data->upload a data set\".\nif not RUN_PREPRECESS:\n    df_log = pd.read_parquet(FILE_LOG_PROCESSED)\n    df_user = pd.read_parquet(FILE_USER_PROCESSED)\n    df_content = pd.read_parquet(FILE_CONTENT_PROCESSED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    '''\n    -----------------------------------------------------------------------------\n    Read in the log file\n    -----------------------------------------------------------------------------\n    '''\n\n    '''\n    -----------------------------------------------------------------------------\n    #         Un-comment this block of code to read from csv. \n    #         log_dtypes = {\n    #                     'timestamp_TW':'object',\n    #                     'uuid':'category',\n    #                     'ucid':'category',\n    #                     'upid':'category',\n    #                     #int16: -32768 to 32767,\n    #                     'problem_number':'int16',\n    #                     'exercise_problem_repeat_session':'int16',\n    #                     'is_correct':'boolean',\n    #                     'total_sec_taken':'int16',\n    #                     'total_attempt_cnt':'int16',\n    #                     'used_hint_cnt':'int16',\n    #                     'is_hint_used':'boolean',\n    #                     'is_downgrade':'boolean',\n    #                     'is_upgrade':'boolean',\n    #                     #int8: -256 to 256                    \n    #                     'level':'int8'                            \n    #                       }\n    #         df_log = pd.read_csv(FILE_LOG_FULL,dtype=log_dtypes)  \n    #                 # 545.9+ MB\n    -----------------------------------------------------------------------------\n    '''\n\n    # read from parquet         \n    df_log = pd.read_parquet(FILE_LOG_FULL)\n \n    \n    '''\n    -----------------------------------------------------------------------------\n    Read in the user file\n    -----------------------------------------------------------------------------\n    '''\n    user_dtype = {'uuid':'category',\n                  'gender':'category',\n                  #int8: -256 to 256                                      \n                  'user_grade':'int8'\n                 }\n    df_user = pd.read_csv(FILE_USER,dtype=user_dtype)\n    \n    '''\n    -----------------------------------------------------------------------------\n    Read in the content file\n    -----------------------------------------------------------------------------\n    '''\n    content_dtype = {'ucid':'category',\n                     'level4_id':'category',\n                     'level4_id':'category',\n                     'difficulty':'category',\n                     'learning_stage':'category'\n                     }    \n    df_content = pd.read_csv(FILE_CONTENT,dtype=content_dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    # join the \"user_grade\" and \"gender\" info\n    df_log = pd.merge(df_log,df_user[['uuid','user_grade','gender']],on='uuid',how='left')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    # sort the df by time stamp in ascending order\n    # - to faciliate the derivation of the history vectors\n    df_log = df_log.sort_values(by = 'timestamp_TW')   \n    # reset the row index\n    df_log = df_log.reset_index(drop=True)\n\n    # NOTE: If using the log file with rounded timestamp:\n    # one critical limitation: the timestamp was rounded to the closest 15 mins, \n    # so the order of the row does not reflect the actual order of a student's activity\n    # print(df_log.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    # - Convert gender to one-hot encoding to handle \"unspecified\"\n    # set NaN as \"unspecified\"\n    df_log.fillna(value = {'gender':'unspecified'},inplace=True)\n    df_log = pd.concat([df_log,pd.get_dummies(df_log.gender)],axis=1).drop(columns='gender')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    # Redefine \"level\" as the 'uuid' level of this exercise right before the attempt\n    # - Should offset the change due to this attempt\n    df_log['level'] = (df_log['level']+df_log['is_downgrade'].fillna(0).astype(int)-df_log['is_upgrade'].fillna(0).astype(int)).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_PREPRECESS:\n    # Preprocessing\n    # - drop redundant columns\n    df_log = df_log.drop(columns = VARS_REDUNDANT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Uncomment the following cell to save the output of pre-processed data if you are running pre-processing. This saves the data to output directory for future use."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # save the preprocessed data\n# df_log.to_parquet(os.path.join(PATH_OUTPUT ,'Processed_Log_Problem_raw_timestamp.parquet.gzip'))\n# df_user.to_parquet(os.path.join(PATH_OUTPUT ,'Processed_Info_UserData.parquet.gzip'))\n# df_content.to_parquet(os.path.join(PATH_OUTPUT ,'Processed_Info_Content.parquet.gzip'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Pick only a subset of records for quick testing. \nif not RUN_FULL:\n    df_log = df_log.head(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters used by the following cells for various tasks. \n# - used by the cells below\n# list of problem id in order\nlist_upid = df_log.upid.unique().to_numpy()\n# list of concept id in order\nlist_concept_id = df_content.ucid.to_numpy()\n# list of the level4_id in order\nlist_level4_id = df_content.level4_id.unique().to_numpy()\n# list of user id in order\nlist_user_id = df_user['uuid'].unique()\n\n# dict of list_upid {id: order}\ndict_upid = {id:order for order, id in enumerate(list_upid)}\n# dict of list_concept_id {id: order}\ndict_concept_id = {id:order for order, id in enumerate(list_concept_id)}\n# dict of list_level4_id {id: order}\ndict_level4_id = {id:order for order, id in enumerate(list_level4_id)}\n# dict of list_user_id {id: order}\ndict_user_id = {id:order for order, id in enumerate(list_user_id)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n\nThis section contains the code for creating concept proficiency, level_4 proficiency, upid_accuracy vector."},{"metadata":{},"cell_type":"markdown","source":"#### Creation of UPID Accuracy Matrix\n\n- m = 25785"},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_V_UPID_ACC:\n    ACC_GRAND_AVG = df_log.is_correct.mean()\n    # create the accuracy vector (# logs, 1) which encodes the upid accuracy of each log so far (based only on past data)\n    v_upid_acc = np.zeros((len(df_log),1),dtype = 'float16')\n    # - initialize the helper vector (# upid, 1) to keep track of the sum of correct response\n    v_sum_correct = np.zeros((len(list_upid),1),dtype='int')\n    # - initialize the helper matrix (# upid, 1) to keep track of the count per upid\n    v_count = np.zeros((len(list_upid),1),dtype='int')    \n\n    # update the matrices while iterating over df_log\n    for i_r,log in df_log.iterrows():\n        if i_r % 10000 == 0:                \n            print(i_r)    \n        # update v_acc (should update this before processing the response of this current log)\n        if v_count[dict_upid[log['upid']],0] == 0:            \n            v_upid_acc[i_r,0] = ACC_GRAND_AVG\n        else:\n            v_upid_acc[i_r,0] = v_sum_correct[dict_upid[log['upid']],0]/v_count[dict_upid[log['upid']],0]\n        # update v_sum_correct\n        v_sum_correct[dict_upid[log['upid']],0] += log['is_correct']   \n        \n        # update v_count\n        v_count[dict_upid[log['upid']],0] += 1\n            \n    # save the v_acc\n    np.savez_compressed(os.path.join(PATH_OUTPUT,'v_upid_acc'), v_upid_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not RUN_V_UPID_ACC:\n    v_upid_acc = np.load(FILE_V_UPID_ACC)['arr_0']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creation of Concept proficiency matrix\n\n- m = 1326"},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_M_CONCEPT:\n    # create the \"proficiency matrix\" (# logs, # concept id) which encodes the most recent level per concept id\n    m_concept_proficiency = np.empty((len(df_log),len(list_concept_id)),dtype = 'float16')\n    m_concept_proficiency[:] = np.nan     \n\n    # update the matrices while iterating over df_log\n    for i_r,log in df_log.iterrows():\n        if i_r % 1000000 == 0:                \n            print(i_r)\n\n        # update the \"proficiency matrix\" with the average concept level within the level 4 id\n        # - only update the relevant cell\n        m_concept_proficiency[i_r,dict_concept_id[log['ucid']]] = log['level']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not RUN_M_CONCEPT:\n    m_concept_proficiency = np.load(FILE_M_PROFICIENCY_CONCEPT)['m_concept_proficiency']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creation of Level-4 proficiency matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_LEVEL4:\n    # initialize some useful variables for section below\n    # - the map for looking up the dummy vector given a ucid\n    df_content_level4_dummies = pd.get_dummies(list_level4_id)\n    \n    # - the map for looking up the list of ucid given a level4 id\n    dict_level4_to_ucid = defaultdict(list)\n    for i_r, row in df_content.iterrows():\n        dict_level4_to_ucid[dict_level4_id[row['level4_id']]].append(dict_concept_id[row['ucid']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_LEVEL4:\n    # join the level 4 info\n    df_log = df_log.merge(df_content[[\"ucid\",\"level4_id\"]],how =\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_LEVEL4:\n    if RUN_M_LEVEL4:\n        # Problem vector: one-hot encoding (one row vector for one log, i.e., one row in df_log)\n        # - create the 2d numpy matrix of problem vectors: avoid joining to `df_log` (RAM expensive)\n        # - (# logs = df_log.shape[0], # level4 id = df_content_level4_dummies.shape[1]-1)\n        m_level4_id = df_content_level4_dummies[df_log.level4_id]\n        m_level4_id = np.transpose(m_level4_id.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create the matrix of level-4 proficiency vectors\n\n- m = 171\n- matrix: (# logs, # level4 id)\n- For each cell: the student's most recent \"level\" of a level-4 category, which is derived by averaging across the most recent levels of all concepts within one \"level-4\" category."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will take a long while (1~2 hours...)\nif RUN_LEVEL4:\n    if RUN_M_PROFICIENCY:\n        # create the \"proficiency matrix\" (# logs, # level 4 id) which encodes the most recent level per level-4 category (averaged across ucid)\n        # - note: unseen concept is encoded as NaN. Therefore, when computing the level of level-4 sum, one should use np.nansum().\n        m_proficiency = np.empty((len(df_log),len(list_level4_id)),dtype = 'float16')\n        m_proficiency[:] = np.nan     \n        \n        # create the helper \"concept level matrix\" (# users, # concept id) which encodes the most recent level per concept of each student\n        # - note: unseen concept is encoded as NaN. Therefore, when computing the level of level-4 sum, one should use np.nansum().        \n        m_concept_level = np.empty((len(list_user_id),len(list_concept_id)),dtype = 'int8')\n        m_concept_level[:] = np.nan     \n        # update the matrices while iterating over df_log\n        for i_r,log in df_log.iterrows():\n            if i_r % 10000 == 0:                \n                print(i_r)\n            # update the \"concept level matrix\"\n            m_concept_level[dict_user_id[log['uuid']],dict_concept_id[log['ucid']]] = log['level']\n                            \n            # update the \"proficiency matrix\" with the average concept level within the level 4 id\n            # - only update the relevant cell\n            m_proficiency[i_r,dict_level4_id[log['level4_id']]] =\\\n            np.nansum(m_concept_level[dict_user_id[log['uuid']],dict_level4_to_ucid[dict_level4_id[log['level4_id']]]])                              \n\n        # save the m_proficiency matrix\n        np.savez_compressed(os.path.join(PATH_OUTPUT,'m_proficiency_level4'), m_proficiency)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_LEVEL4:\n    if not RUN_M_PROFICIENCY:\n        m_proficiency = np.load(FILE_M_PROFICIENCY_LEVEL4)['arr_0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not RUN_FULL:\n    m_proficiency = m_proficiency[:1000,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n\n#### The following cell is very important. We need to convert nan values present in proficiency matrix to 0. Otherwise, we will get an exception when training the model.\n\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_proficiency[np.isnan(m_proficiency)] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\n\nThe following section contains few data exploration tasks which were done to understand the correlation between input variables and the output. This section is entirely optional and you can skip to the next section if needed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"user_data = pd.read_csv(FILE_USER)\nlog_problem = pd.read_csv(FILE_LOG_FULL)\ncontent = pd.read_csv(FILE_CONTENT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join tables based on uuid and ucid\ndf1 = pd.merge(df_log_problem, user_data, on='uuid')\ndf2 = pd.merge(df1, content, on='ucid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    -----------------------------------------------------------------------------\n    Select only required columns\n    -----------------------------------------------------------------------------\n'''\n\nrequired_columns = ['is_correct', 'total_sec_taken', 'total_attempt_cnt', 'used_hint_cnt', 'is_hint_used', 'level', 'difficulty', 'learning_stage', 'gender', 'user_grade', 'has_teacher_cnt', 'is_self_coach', 'has_student_cnt', 'belongs_to_class_cnt', 'has_class_cnt']\n\ndf = df2[required_columns]\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr = df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f = plt.figure(figsize=(15, 15))\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=45)\nplt.yticks(range(df.shape[1]), df.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (13, 13))\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\n-----------------------------------------------------------------------------\nThe above correlation matrix does not include gender, difficulty, learning_stage because they are non-numeric values.\nChanging them to numeric values so that we can use them for training.\n-----------------------------------------------------------------------------\n'''\n\nprint('Unique Gender values = ', df.gender.unique())\nprint('Unique Difficulty values = ', df.difficulty.unique())\nprint('Unique Learning Stage values = ', df.learning_stage.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\n-----------------------------------------------------------------------------\nAssigning category labels to Gender, Difficulty and Learning Columns.\n-----------------------------------------------------------------------------\n'''\n\ndf['gender'].replace({'unspecified': 0, 'male': 1, 'female': 2},inplace=True)\ndf['difficulty'].replace({'unset': 0, 'easy': 1, 'normal': 2, 'hard': 3}, inplace=True)\ndf['learning_stage'].replace({'elementary': 0, 'junior': 1, 'senior': 2}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Dropping Nan values\n\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\n-----------------------------------------------------------------------------\nPlotting correlation matrix after updating gender, difficulty and learning stage with numerical values.\n-----------------------------------------------------------------------------\n'''\n\ncorr = df.corr()\n\n# Print correlation matrix\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training and Evaluation\n\nThe following sections contain the code for training and evaluation of several different models. For each model, we used a combination of following features - \n\n- level\n- difficulty\n- learning_stage\n- gender\n- user_grade\n- has_teacher_cnt\n- is_self_coach\n- has_student_cnt\n- belongs_to_class_cnt\n- has_class_cnt\n- m_level4_proficiency matrix\n- m_concept_proficiency matrix\n- v_upid_acc matrix\n- v_ucid_acc matrix\n\nIn each model, our output variable was `is_correct` (i.e.), whether the student got the particular problem right / wrong. Each subsection contains the code for creating the training and testing data and we have reported accuracy of training and testing sets of different sizes. In most cases, we were unable to train the model on the entire dataset due to memory constraints. Hence, we used sizes of `10K`, `100K`, and `1MM` as our data set sizes for training and evaluation purposes.\n\n***\n"},{"metadata":{},"cell_type":"markdown","source":"### Model 1: Benchmark model\n\n#### Input Features\n- level\n- difficulty\n- learning_stage\n- gender\n- user_grade\n- has_teacher_cnt\n- is_self_coach\n- has_student_cnt\n- belongs_to_class_cnt\n- has_class_cnt\n\n#### Output Feature\n- is_correct"},{"metadata":{"trusted":false},"cell_type":"code","source":"## Note, the df used here is from the data exploration section. Only this Logistic model uses this dataframe as input.\n## All the other following model use a different data frame as input. \n\ninput_data = df.to_numpy()\nn = input_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\n-------------------\n    Split the data into 80 - 20% split for training and testing\n-------------------\n'''\nnum_samples = int(n * 0.8)\n\nsamples = np.random.choice(range(n), num_samples, replace=False)\n\nmask = np.ones(n, dtype=bool)\nmask[samples] = False\n\nX_train = input_data[samples, 5:]\ny_train = input_data[samples, 0]\n\n#y_train = np.reshape(y_train, (num_samples, 1))\ny_train = y_train.astype('int')\n\nX_eval = input_data[mask, 5:]\ny_eval = input_data[mask, 0]\n\n#y_eval = np.reshape(y_eval, (n - num_samples, 1))\ny_eval = y_eval.astype('int')\n\nprint('X_train shape is = ', np.shape(X_train))\nprint('y_train shape is = ', np.shape(y_train))\n\n\nprint('X_eval shape is = ', np.shape(X_eval))\nprint('y_eval shape is = ', np.shape(y_eval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_scaled = preprocessing.MinMaxScaler().fit_transform(X_train)\n\nmodel = LogisticRegression(random_state=0).fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_eval_scaled = preprocessing.MinMaxScaler().fit_transform(X_eval)\n\nmodel.score(X_eval_scaled, y_eval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n\n- Accuracy (n = 1MM) = 70.9 %\n- Accuracy (n = 16MM) (Entire Dataset) = 71.1 %\n\n***"},{"metadata":{},"cell_type":"markdown","source":"### Model 2 - Full model\n\n\n- For how the features were engineered, see the section [Feature Engineering](#Feature-Engineering)\n- Labels (y) [# logs x 1]:\n    - Correct or not of the new problem (problem-level)\n- Features (X):\n    - Demographics [#logs x 4] [From **df_user**]\n        - grade (#logs x 1)\n        - gender (#logs x 3)\n    - Difficulty features  [#logs x 1]:\n        - upid accuracy [**From v_upid_acc**]\n        - ~difficulty (only 3 levels, not quite informative)~\n        - ~learning_stage (only elementary vs. junior, not quite informative)~\n    - History features [#logs x 3]: \n        - most recent 'Level' of this ucid [From **df_log**]\n        - 'problem_number' of this 'ucid' [From **df_log**]\n        - 'exercise_problem_repeat_session' of this 'upid' [From **df_log**]        \n    - One-hot encoding matrix [#logs x #level4 id]:  [**m_level4_id**]\n        - one-hot encoding of the content ID of the new \n    - Proficiency matrix [#logs x #level4 id]: [**m_proficiency**]\n        - encodes the studentâ€™s performance of each content (i.e.,level)    \n- Model:\n    - Decision Tree\n    - Logistic Regression\n        - With L2 penalty\n        - With L1 penalty\n    - SVM\n        - With rbf kernal\n        - With linear kernal\n- Evaludate Accuracy:\n    - Hold-out 20% test set\n\n***\n"},{"metadata":{},"cell_type":"markdown","source":"#### Split the data into 80 - 20% split for training and testing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# set to `num_samples` for using full data. set to a small number for quick testing\n# n_subset = 10000000 will overflow the RAM limit (this step `np.concatenate()`)\nn_subset = 10000\n# n_subset = df_log.shape[0]\n\nnum_samples = int(df_log.head(n_subset).shape[0])\nnum_train_samples = int(num_samples * 0.8)\n\nnp.random.seed(760)\nsamples_train = np.random.choice(range(num_samples), num_train_samples, replace=False)\n\n# True: training set/ False: test set\nmask_train = np.zeros(num_samples, dtype=bool)\nmask_train[samples_train] = True\n\nX_train = np.concatenate((\n        # grade\n        df_log.head(n_subset).loc[mask_train,\"user_grade\"].to_numpy()[:,np.newaxis],\n        # gender\n        df_log.head(n_subset).loc[mask_train,[\"female\",\"male\",\"unspecified\"]].to_numpy(),\n        # Difficulty features \n        v_upid_acc[:n_subset,:][mask_train,:],\n        # History features\n        df_log.head(n_subset).loc[mask_train,\"level\"].to_numpy()[:,np.newaxis],    \n        df_log.head(n_subset).loc[mask_train,\"problem_number\"].to_numpy()[:,np.newaxis],\n        df_log.head(n_subset).loc[mask_train,\"exercise_problem_repeat_session\"].to_numpy()[:,np.newaxis],    \n        # one-hot matrix\n        m_level4_id[:n_subset,:][mask_train,:],\n        # proficiency matrix\n        m_proficiency[:n_subset,:][mask_train,:]\n#         # interaction between one-hot matrix and proficiency matrix\n#         m_inter_level4_proficiency[:n_subset,:][mask_train,:]\n    ),axis=1)\n\ny_train = df_log.head(n_subset).loc[mask_train,\"is_correct\"].to_numpy(dtype = bool)\n\nX_test = np.concatenate((\n        # grade    \n        df_log.head(n_subset).loc[~mask_train,\"user_grade\"].to_numpy()[:,np.newaxis],\n        # gender\n        df_log.head(n_subset).loc[~mask_train,[\"female\",\"male\",\"unspecified\"]].to_numpy(),\n        # Difficulty features \n        v_upid_acc[:n_subset,:][~mask_train,:],    \n        # History features\n        df_log.head(n_subset).loc[~mask_train,\"level\"].to_numpy()[:,np.newaxis],        \n        df_log.head(n_subset).loc[~mask_train,\"problem_number\"].to_numpy()[:,np.newaxis],\n        df_log.head(n_subset).loc[~mask_train,\"exercise_problem_repeat_session\"].to_numpy()[:,np.newaxis],    \n        # one-hot matrix\n        m_level4_id[:n_subset,:][~mask_train,:],\n        # proficiency matrix\n        m_proficiency[:n_subset,:][~mask_train,:]\n#         # interaction between one-hot matrix and proficiency matrix\n#         m_inter_level4_proficiency[:n_subset,:][~mask_train,:]    \n    ),axis=1)\ny_test = df_log.head(n_subset).loc[~mask_train,\"is_correct\"].to_numpy(dtype = bool)\n\n\nprint('X_train shape is = ', np.shape(X_train))\nprint('y_train shape is = ', np.shape(y_train))\n\nprint('X_test shape is = ', np.shape(X_test))\nprint('y_test shape is = ', np.shape(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Min-max transformation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Overwrite the raw data matrix to reduce RAM usage\nX_train = preprocessing.MinMaxScaler().fit_transform(X_train)\nX_test = preprocessing.MinMaxScaler().fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"dc_full = DecisionTreeClassifier(criterion=\"entropy\",random_state=0).fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(dc_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(dc_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.901875/ test = 0.6715\n# n_subset = 100000 : train = 0.92925/ test = 0.6881\n# n_subset = 1000000 : train = 0.9574675/ test = 0.67654\n\n# [Grade + Gender model + concent history + difficulty + one-hot matrix]\n# n_subset = 10000 : train = 0.892375/ test = 0.6775\n# n_subset = 100000 : train = 0.9190875/ test = 0.69365\n# n_subset = 1000000 : train = 0.94633875/ test = 0.67734\n\n# [Grade + Gender model + concent history + difficulty]\n# n_subset = 10000 : train = 0.82225/ test = 0.683\n# n_subset = 100000 : train = 0.8305875/ test = 0.71545\n# n_subset = 1000000 : train = 0.86713875/ test = 0.69857\n\n# Without difficulty ----------------\n# [Grade+Gender model]\n# n_subset = 10000: train = 0.7225/ test = 0.721 (Best)\n# n_subset = 100000: train = 0.7413/ test = 0.74865 (Best)\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355 (Best)\n\n# [Grade only model] \n# n_subset = 10000: train = 0.7225 / test = 0.721 (Best)\n# n_subset = 100000: train = 0.7411875 / test = 0.7486 (Best)\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355 (Best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting\n\n- https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_full = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(gb_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(gb_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.760875/ test = 0.744\n# n_subset = 100000 : train = 0.763675/ test = 0.7665\n# n_subset = 1000000 : train = 0.76075/ test = 0.761455 (best)\n\n# [Grade + Gender model + concent history + difficulty + one-hot matrix]\n# n_subset = 10000 : train = 0.755125/ test = 0.745 (best)\n# n_subset = 100000 : train = 0.7642125/ test = 0.7678 (best)\n# n_subset = 1000000 : train = 0.7605875/ test = 0.76125\n\n# [Grade + Gender model + concent history + difficulty]\n# n_subset = 10000 : train = 0.74675/ test = 0.732\n# n_subset = 100000 : train = 0.7594375/ test = 0.7638\n# n_subset = 1000000 : train = 0.7604625/ test = 0.76084\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355\n\n# [Grade only model] \n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L2 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"logit_full = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(logit_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(logit_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.74325/ test = 0.734 (Best)\n# n_subset = 100000 : train = 0.7613375/ test = 0.7638\n# n_subset = 1000000 : train = 0.7580275/ test = 0.75891 (Best)\n\n# [Grade + Gender model + concent history + difficulty + one-hot matrix] \n# n_subset = 10000 : train = 0.741375/ test = 0.7325\n# n_subset = 100000 : train = 0.760325/ test = 0.7641 (Best)\n# n_subset = 1000000 : train = 0.7577325/ test = 0.75869\n\n# [Grade + Gender model + concent history + difficulty]\n# n_subset = 10000 : train = 0.73125/ test = 0.728\n# n_subset = 100000 : train = 0.75265/ test = 0.7574\n# n_subset = 1000000 : train = 0.75702875/ test = 0.758\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355\n\n# [Grade only model]\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L1 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso_full = LogisticRegression(penalty='l1', solver='saga',random_state=0,max_iter=1000).fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(lasso_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(lasso_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.7435/ test = 0.7335 (Best)\n# n_subset = 100000 : train = 0.761575/ test = 0.7641\n# n_subset = 1000000 : train = 0.75812375/ test = 0.758975 (Best)\n\n# [Grade + Gender model + concent history + difficulty + one-hot matrix] \n# n_subset = 10000 : train = 0.741/ test = 0.7335 (Best)\n# n_subset = 100000 : train = 0.760275/ test = 0.7643 (Best)\n# n_subset = 1000000 : train = 0.75778875/ test = 0.758735\n\n# [Grade + Gender model + concent history + difficulty]\n# n_subset = 10000 : train = 0.729875/ test = 0.7285\n# n_subset = 100000 : train = 0.7526625/ test = 0.7576\n# n_subset = 1000000 : train = 0.75708/ test = 0.7581\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000: train = 0.7225 / test = 0.721\n# n_subset = 100000: train = 0.7411875 / test = 0.7486\n# n_subset = 1000000: train = 0.74041625 / test = 0.741355\n\n# [Grade only model]\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with rbf kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_full = SVC().fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(svc_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(svc_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.748375/ test = 0.7395 (Best)\n# n_subset = 100000: (exceeds the 6 hour time limit...)\n# n_subset = 1000000:\n\n# [Grade + Gender model + concent level + difficulty + one-hot matrix]\n# n_subset = 10000 : train = 0.7455/ test = 0.738\n# n_subset = 100000: \n# n_subset = 1000000:\n\n# [Grade + Gender model + concent level + difficulty]\n# n_subset = 10000 : train = 0.736625/ test = 0.7395 (Best)\n# n_subset = 100000: \n# n_subset = 1000000:\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000:\n# n_subset = 1000000:\n\n# [Grade only model]\n# n_subset = 10000: train = 0.7225/test = 0.721\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with polynomial kernal, degree = 3)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_poly_full = SVC(kernel='poly').fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(svc_poly_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(svc_poly_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.75025/ test = 0.74 (Best)\n# n_subset = 100000: (exceeds the 6 hour time limit...)\n# n_subset = 1000000:\n\n# [Grade + Gender model + concent level + difficulty + one-hot matrix]\n# n_subset = 10000 : train = 0.74875/ test = 0.74 (Best)\n# n_subset = 100000: \n# n_subset = 1000000:\n\n# [Grade + Gender model + concent level + difficulty]\n# n_subset = 10000 : train = 0.731625/ test = 0.726\n# n_subset = 100000: \n# n_subset = 1000000:\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000:\n# n_subset = 1000000:\n\n# [Grade only model]\n# n_subset = 10000: train = 0.7225/test = 0.721\n# n_subset = 100000:\n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with linear kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_linear_full = LinearSVC(random_state=0,max_iter=10000,dual=False).fit(X_train, y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(svc_linear_full.score(X_train, y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(svc_linear_full.score(X_test, y_test)))\n\n# With difficulty ----------------\n# [Grade + Gender model + concent history + difficulty + one-hot matrix + proficiency matrix]\n# n_subset = 10000 : train = 0.745375/ test = 0.732\n# n_subset = 100000 : train = 0.7615375/ test = 0.76335 (Best)\n# n_subset = 1000000 : train = 0.75712/ test = 0.75817 (Best)\n\n# [Grade + Gender model + concent level + difficulty + one-hot matrix]\n# n_subset = 10000 : train = 0.741625/ test = 0.7325\n# n_subset = 100000 : train = 0.760325/ test = 0.76335 (Best)\n# n_subset = 1000000 : train = 0.7567025/ test = 0.757925\n\n# [Grade + Gender model + concent level + difficulty]\n# n_subset = 10000 : train = 0.73225/ test = 0.7285\n# n_subset = 100000 : train = 0.7518875/ test = 0.7568\n# n_subset = 1000000 : train = 0.75613625/ test = 0.75743\n\n# Without difficulty ----------------\n# [Grade + Gender model]\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355\n\n# [Grade only model]\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 3:  Model without the proficiency matrix\n\n- Features (X):\n    - Demographics [#logs x 4] [From **df_user**]\n        - grade (#logs x 1)\n        - gender (#logs x 3)\n    - Difficulty features  [#logs x 1]:\n        - upid accuracy [**From v_upid_acc**]\n        - ~difficulty (only 3 levels, not quite informative)~\n        - ~learning_stage (only elementary vs. junior, not quite informative)~\n    - History features [#logs x 3]: \n        - most recent 'Level' of this ucid [From **df_log**]\n        - 'problem_number' of this 'ucid' [From **df_log**]\n        - 'exercise_problem_repeat_session' of this 'upid' [From **df_log**]        \n    - One-hot encoding matrix [#logs x #level4 id]:  [**m_level4_id**]\n        - one-hot encoding of the content ID of the new "},{"metadata":{"trusted":false},"cell_type":"code","source":"# all features excluding the proficiency matrix\nslice_no_prof = slice(None, -len(list_level4_id), None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"dc_no_prof = DecisionTreeClassifier(criterion=\"entropy\",random_state=0).fit(X_train[:,slice_no_prof], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(dc_no_prof.score(X_train[:,slice_no_prof], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(dc_no_prof.score(X_test[:,slice_no_prof], y_test)))\n\n# n_subset = 10000 : train = 0.892375/ test = 0.6775 *\n# n_subset = 100000 : train = 0.9190875/ test = 0.69365 *\n# n_subset = 1000000 : train = 0.94633875/ test = 0.67734 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_no_prof = GradientBoostingClassifier(random_state=0).fit(X_train[:,slice_no_prof], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(gb_no_prof.score(X_train[:,slice_no_prof], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(gb_no_prof.score(X_test[:,slice_no_prof], y_test)))\n# n_subset = 10000 : train = 0.755125/ test = 0.745 *\n# n_subset = 100000 : train = 0.7642125/ test = 0.7678 *\n# n_subset = 1000000 : train = 0.7605875/ test = 0.76125 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L2 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"logit_no_prof = LogisticRegression(random_state=0,max_iter=1000).fit(X_train[:,slice_no_prof], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(logit_no_prof.score(X_train[:,slice_no_prof], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(logit_no_prof.score(X_test[:,slice_no_prof], y_test)))\n\n# n_subset = 10000 : train = 0.741375/ test = 0.7325 *\n# n_subset = 100000 : train = 0.760325/ test = 0.7641 *\n# n_subset = 1000000 : train = 0.7577325/ test = 0.75869 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L1 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso_no_prof = LogisticRegression(penalty='l1', solver='saga',random_state=0,max_iter=1000).fit(X_train[:,slice_no_prof], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(lasso_no_prof.score(X_train[:,slice_no_prof], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(lasso_no_prof.score(X_test[:,slice_no_prof], y_test)))\n\n# n_subset = 10000 : train = 0.741/ test = 0.7335 *\n# n_subset = 100000 : train = 0.760275/ test = 0.7643 *\n# n_subset = 1000000 : train = 0.75778875/ test = 0.758735 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with rbf kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_no_prof = SVC().fit(X_train[:,slice_no_prof], y_train)\nacc_train = svc_no_prof.score(X_train[:,slice_no_prof], y_train)\nacc_test = svc_no_prof.score(X_test[:,slice_no_prof], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n# n_subset = 10000 : train = 0.7455/ test = 0.738 *\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with polynomial kernal, degree = 3)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_poly_no_prof = SVC(kernel = 'poly').fit(X_train[:,slice_no_prof], y_train)\nacc_train = svc_poly_no_prof.score(X_train[:,slice_no_prof], y_train)\nacc_test = svc_poly_no_prof.score(X_test[:,slice_no_prof], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n# n_subset = 10000 : train = 0.74875/ test = 0.74 *\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with linear kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_linear_no_prof = LinearSVC(random_state=0,max_iter=10000,dual=False).fit(X_train[:,slice_no_prof], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(svc_linear_no_prof.score(X_train[:,slice_no_prof], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(svc_linear_no_prof.score(X_test[:,slice_no_prof], y_test)))\n\n# n_subset = 10000 : train = 0.741625/ test = 0.7325 *\n# n_subset = 100000 : train = 0.760325/ test = 0.76335 *\n# n_subset = 1000000 : train = 0.7567025/ test = 0.757925 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 4: Model without proficiency matrix and one-hot encoding matrix\n\n- Features (X):\n    - Demographics [#logs x 4] [From **df_user**]\n        - grade (#logs x 1)\n        - gender (#logs x 3)\n    - Difficulty features  [#logs x 1]:\n        - upid accuracy [**From v_upid_acc**]\n        - ~difficulty (only 3 levels, not quite informative)~\n        - ~learning_stage (only elementary vs. junior, not quite informative)~\n    - History features [#logs x 3]: \n        - most recent 'Level' of this ucid [From **df_log**]\n        - 'problem_number' of this 'ucid' [From **df_log**]\n        - 'exercise_problem_repeat_session' of this 'upid' [From **df_log**]\n\n***\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# all features excluding the proficiency matrix\nslice_no_prof_onehot = slice(None, -(2*len(list_level4_id)), None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"dc_no_prof_onehot = DecisionTreeClassifier(criterion=\"entropy\",random_state=0).fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = dc_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = dc_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n\n# n_subset = 10000 : train = 0.82225/ test = 0.683 *\n# n_subset = 100000 : train = 0.8305875/ test = 0.71545 *\n# n_subset = 1000000 : train = 0.86713875/ test = 0.69857 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_no_prof_onehot = GradientBoostingClassifier(random_state=0).fit(X_train[:,slice_no_prof_onehot], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(gb_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(gb_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)))\n# n_subset = 10000 : train = 0.74675/ test = 0.732 *\n# n_subset = 100000 : train = 0.7594375/ test = 0.7638 *\n# n_subset = 1000000 : train = 0.7604625/ test = 0.76084 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L2 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"logit_no_prof_onehot = LogisticRegression(random_state=0,max_iter=1000).fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = logit_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = logit_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n\n# n_subset = 10000 : train = 0.73125/ test = 0.728 *\n# n_subset = 100000 : train = 0.75265/ test = 0.7574 *\n# n_subset = 1000000 : train = 0.75702875/ test = 0.758 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L1 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso_no_prof_onehot = LogisticRegression(penalty='l1', solver='saga',random_state=0,max_iter=1000).fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = lasso_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = lasso_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n\n# n_subset = 10000 : train = 0.729875/ test = 0.7285 *\n# n_subset = 100000 : train = 0.7526625/ test = 0.7576 *\n# n_subset = 1000000 : train = 0.75708/ test = 0.7581 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with rbf kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_no_prof_onehot = SVC().fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = svc_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = svc_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n# n_subset = 10000 : train = 0.736625/ test = 0.7395\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with polynomial kernal, degree = 3)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_poly_no_prof_onehot = SVC(kernel = 'poly').fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = svc_poly_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = svc_poly_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n# n_subset = 10000 : train = 0.731625/ test = 0.726\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with linear kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_linear_no_prof_onehot = LinearSVC(random_state=0,max_iter=10000,dual=False).fit(X_train[:,slice_no_prof_onehot], y_train)\nacc_train = svc_linear_no_prof_onehot.score(X_train[:,slice_no_prof_onehot], y_train)\nacc_test = svc_linear_no_prof_onehot.score(X_test[:,slice_no_prof_onehot], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n\n# n_subset = 10000 : train = 0.73225/ test = 0.7285 *\n# n_subset = 100000 : train = 0.7518875/ test = 0.7568 *\n# n_subset = 1000000 : train = 0.75613625/ test = 0.75743 *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 5: Demographics only model\n\n- Features (X):\n    - Demographics [#logs x 4] [From **df_user**]\n        - grade (#logs x 1)\n        - gender (#logs x 3)\n\n***\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"slice_demo = slice(None, 4, None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"dc_demo= DecisionTreeClassifier(criterion=\"entropy\",random_state=0).fit(X_train[:,slice_demo], y_train)\nprint(dc_demo.score(X_train[:,slice_demo], y_train))\nprint(dc_demo.score(X_test[:,slice_demo], y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7413/ test = 0.74865\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_demo = GradientBoostingClassifier(random_state=0).fit(X_train[:,slice_demo], y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(gb_demo.score(X_train[:,slice_demo], y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(gb_demo.score(X_test[:,slice_demo], y_test)))\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L2 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"logit_demo = LogisticRegression(random_state=0,max_iter=1000).fit(X_train[:,slice_demo], y_train) \nprint(logit_demo.score(X_train[:,slice_demo], y_train))\nprint(logit_demo.score(X_test[:,slice_demo], y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L1 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso_demo = LogisticRegression(penalty='l1', solver='saga',random_state=0,max_iter=1000).fit(X_train[:,slice_demo], y_train)\nprint(lasso_demo.score(X_train[:,slice_demo], y_train))\nprint(lasso_demo.score(X_test[:,slice_demo], y_test))\n# n_subset = 10000: train = 0.7225 / test = 0.721\n# n_subset = 100000: train = 0.7411875 / test = 0.7486\n# n_subset = 1000000: train = 0.74041625 / test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with rbf kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_demo = SVC().fit(X_train[:,slice_demo], y_train)\nprint(svc_demo.score(X_train[:,slice_demo], y_train))\nprint(svc_demo.score(X_test[:,slice_demo], y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with polynomial kernal, degree = 3)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_poly_demo = SVC(kernel= 'poly').fit(X_train[:,slice_demo], y_train)\nprint(svc_poly_demo.score(X_train[:,slice_demo], y_train))\nprint(svc_poly_demo.score(X_test[:,slice_demo], y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with linear kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_demo = LinearSVC(random_state=0,max_iter=10000,dual=False).fit(X_train[:,slice_demo], y_train)\nacc_train = svc_demo.score(X_train[:,slice_demo], y_train)\nacc_test = svc_demo.score(X_test[:,slice_demo], y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 6: Grade only model\n\n\n- Features (X):\n    - Demographics [#logs x 1] [From **df_user**]\n        - grade (#logs x 1)"},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"dc_grade= DecisionTreeClassifier(criterion=\"entropy\",random_state=0).fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(dc_grade.score(X_train[:,0].reshape(-1, 1), y_train))\nprint(dc_grade.score(X_test[:,0].reshape(-1, 1), y_test))\n# n_subset = 10000: train = 0.7225 / test = 0.721\n# n_subset = 100000: train = 0.7411875 / test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_grade = GradientBoostingClassifier(random_state=0).fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(gb_grade.score(X_train[:,0].reshape(-1, 1), y_train))+\"/ \",end = \"\")\nprint(\"test = \" + str(gb_grade.score(X_test[:,0].reshape(-1, 1), y_test)))\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L2 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"logit_grade= LogisticRegression(random_state=0,max_iter=1000).fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(logit_grade.score(X_train[:,0].reshape(-1, 1), y_train))\nprint(logit_grade.score(X_test[:,0].reshape(-1, 1), y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Model (with L1 penalty)"},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso_grade = LogisticRegression(penalty='l1', solver='saga',random_state=0,max_iter=1000).fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(lasso_grade.score(X_train[:,0].reshape(-1, 1), y_train))\nprint(lasso_grade.score(X_test[:,0].reshape(-1, 1), y_test))\n# n_subset = 10000: train = 0.7225/ test = 0.721\n# n_subset = 100000: train = 0.7411875/ test = 0.7486\n# n_subset = 1000000: train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with rbf kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_grade = SVC().fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(svc_grade.score(X_train[:,0].reshape(-1, 1), y_train))\nprint(svc_grade.score(X_test[:,0].reshape(-1, 1), y_test))\n# n_subset = 10000: train = 0.7225/test = 0.721\n# n_subset = 100000: 0.7486 \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with polynomial kernal, degree = 3)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_poly_grade = SVC(kernel='poly').fit(X_train[:,0].reshape(-1, 1), y_train)\nprint(svc_poly_grade.score(X_train[:,0].reshape(-1, 1), y_train))\nprint(svc_poly_grade.score(X_test[:,0].reshape(-1, 1), y_test))\n# n_subset = 10000: train = 0.7225/test = 0.721\n# n_subset = 100000: \n# n_subset = 1000000:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM (with linear kernal)"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_grade = LinearSVC(random_state=0,max_iter=10000,dual=False).fit(X_train[:,0].reshape(-1, 1), y_train)\nacc_train = svc_grade.score(X_train[:,0].reshape(-1, 1), y_train)\nacc_test = svc_grade.score(X_test[:,0].reshape(-1, 1), y_test)\nprint(\"# n_subset = \" + str(n_subset),\": \",end = \"\")\nprint(\"train = \" + str(acc_train)+\"/ \",end = \"\")\nprint(\"test = \" + str(acc_test))\n# n_subset = 10000 : train = 0.7225/ test = 0.721\n# n_subset = 100000 : train = 0.7411875/ test = 0.7486\n# n_subset = 1000000 : train = 0.74041625/ test = 0.741355","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}