{"cells":[{"metadata":{},"cell_type":"markdown","source":"Literal english meanings of names of Indian states.\n![](https://preview.redd.it/vmmbl5uml4151.jpg?width=960&crop=smart&auto=webp&s=d94c0167566189047eee8ae919eca988087c6f04)\nhttps://preview.redd.it/vmmbl5uml4151.jpg?width=960&crop=smart&auto=webp&s=d94c0167566189047eee8ae919eca988087c6f04","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pandas-bokeh/dataset_tk.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes from AmritGrg https://www.kaggle.com/amritgrg/high-accuracy-with-detailed-eda-feature-engineer","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Numerical features\nNumerical_feat = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint('Total numerical features: ', len(Numerical_feat))\nprint('\\nNumerical Features: ', Numerical_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zoomed heatmap, correlation matrix\nsns.set(rc={'figure.figsize':(12,8)})\ncorrelation_matrix = df.corr()\n\nk = 10             #number of variables for heatmap\ncols = correlation_matrix.nlargest(k, 'Rajasthan')['Rajasthan'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"## Lets Find the realtionship between discrete features and SalePrice\n\n#plt.figure(figsize=(8,6))\n\nfor feature in Numerical_feat:\n    data=df.copy()\n    plt.figure(figsize=(8,6))\n    data.groupby(feature)['Rajasthan'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Rajasthan')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[Numerical_feat].hist(bins=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let us now examine the relationship between continuous features and SalePrice\n## Before that lets find continous features that donot contain zero values\n\ncontinuous_nozero = [feature for feature in Numerical_feat if 0 not in data[feature].unique() and feature not in ['Tripura', 'Nagaland']]\n\nfor feature in continuous_nozero:\n    plt.figure(figsize=(8,6))\n    data = df.copy()\n    data[feature] = np.log(data[feature])\n    data['Rajasthan'] = np.log(data['Rajasthan'])\n    plt.scatter(data[feature], data['Rajasthan'])\n    plt.xlabel(feature)\n    plt.ylabel('Rajasthan')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Normality and distribution checking for continous features\nfor feature in continuous_nozero:\n    plt.figure(figsize=(6,6))\n    data = df.copy()\n    sns.distplot(data[feature])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Categorical features","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# categorical features\ncategorical_feat = [feature for feature in df.columns if df[feature].dtypes=='O']\nprint('Total categorical features: ', len(categorical_feat))\nprint('\\n',categorical_feat)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# lets find unique values in each categorical features\nfor feature in categorical_feat:\n    print('{} has {} categories. They are:'.format(feature,len(df[feature].unique())))\n    print(df[feature].unique())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# let us find relationship of categorical with target variable\n\nfor feature in categorical_feat:\n    data=df.copy()\n    data.groupby(feature)['Rajasthan'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Rajasthan')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are selected features from EDA section\nfeatures = ['Rajasthan', 'Punjab', 'Uttarakhand', 'UP', 'J&K', 'Meghalaya', 'DNH', 'Unnamed: 0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot bivariate distribution (above given features with saleprice(target feature))\nfor feature in features:\n    if feature!='Rajasthan':\n        plt.scatter(df[feature], df['Rajasthan'])\n        plt.xlabel(feature)\n        plt.ylabel('Rajasthan')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Outliers\n\nWe can see a clear otliers in Uttarakhand. I mean it just doesn't make sense for larger values Uttarakhand to have low value of Rajasthan (SalePrice). There might be some reason for this but we'll consider them as outliers and drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting outliers for GrLivArea\ndf = df.drop(df[(df['Uttarakhand']>4000) & (df['Rajasthan']<300000)].index)\n\nplt.scatter(df['Uttarakhand'], df['Rajasthan'])\nplt.xlabel('Uttarakhand')\nplt.ylabel('Rajasthan')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Normalizing some numerical data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are selected features from EDA section\nfeatures = ['Rajasthan', 'Punjab', 'Uttarakhand', 'UP', 'J&K', 'Meghalaya', 'DNH']\n\n# selecting continuous features from above\ncontinuous_features = ['Rajasthan', 'Uttarakhand']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train = train_df.shape[0]\n#Test = test_df.shape[0]\n#target_feature = train_df.SalePrice.values\n#combined_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n#combined_data.drop(['SalePrice','Id'], axis=1, inplace=True)\n#print(\"all_data size is : {}\".format(combined_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since I have no train, test files, Id, I adapted the code above for just 1 line, so that I could plot the distplot.  \ncombined_data = pd.concat((df, df)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\n\n# checking distribution of continuous features(histogram plot)\nfor feature in continuous_features:\n    if feature!='Rajasthan':\n        sns.distplot(combined_data[feature], fit=norm)\n        plt.show()\n    else:\n        sns.distplot(df['Rajasthan'], fit=norm)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Label encoding, One-Hot-Encoding/dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# so let's label encode above ordinal features\nfrom sklearn.preprocessing import LabelEncoder\nfor feature in features:\n    encoder = LabelEncoder()\n    combined_data[feature] = encoder.fit_transform(list(combined_data[feature].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets see label encoded data\ncombined_data[features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## One hot encoding or getting dummies \n\ndummy_ordinals = pd.get_dummies(features) \ndummy_ordinals.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# creating dummy variables\n\ncombined_data = pd.get_dummies(combined_data)\nprint(combined_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first see descriptive stat info \ncombined_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Above, data range differs so much. We need to scale them to same range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## we willtake all features from combined_dummy_data \nfeatures_to_scale = [feature for feature in combined_data]\nprint(len(features_to_scale))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now here is where we will scale our data using sklearn module.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = combined_data.columns  # columns of combined_dummy_data\n\nscaler = MinMaxScaler()\ncombined_data = scaler.fit_transform(combined_data[features_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after scaling combined_data it is now in ndarray datypes\n# so we will create DataFrame from it\ncombined_scaled_data = pd.DataFrame(combined_data, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_scaled_data.head() # this is the same combined_dummy_data in scaled form.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see descriptive stat info \ncombined_scaled_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above two dataframe tables that datas are now scaled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#That's the code. Though we don't have train nor test, then I adapted once more. \n#train_df.shape, test_df.shape, combined_scaled_data.shape, combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape, df.shape, combined_scaled_data.shape, combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially, train data had 504 observations but I had droped 1 (oo 3 )in outlier handling section so now I have ??? observations.\nI simply didn't understand that countability. The original said: \"Initially, train data had 1460 observations but we had droped 2 oo 3 in outlier handling section so now we have 4581 observations.\" And the numbers are: ((1458, 81), (1459, 80), (2919, 225), (2919, 225)). I'll read it later and try to get it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate train data and test data \ntrain_data = combined_scaled_data.iloc[:504,:]\ntest_data = combined_scaled_data.iloc[504:,:]\n\ntrain_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets add target feature to train_data\n#train_data['Rajasthan']= train_data['Rajasthan']  # This saleprice is normalized. Its very impportant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.reset_index()\ntest_data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = train_data.copy()  # copy train_data to dataset variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"## lets create dependent and target feature vectors\n\nX = dataset.drop(['Rajasthan'],axis=1)\nY = dataset[['Rajasthan']]\n\nX.shape, Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# lets do feature selection here\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\n# define feature selection\nfs = SelectKBest(score_func=f_regression, k=27)\n# apply feature selection\nX_selected = fs.fit_transform(X, Y)\nprint(X_selected.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 28 (27+1 outlier?) best/important features have been selected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(range(1,28))\n\n## create dataframe of selected features\n\nselected_feat = pd.DataFrame(data=X_selected,columns=[cols])\nselected_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(selected_feat,Y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Model Building","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LINEAR REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlr = LinearRegression()\nlr.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(x_test) # predicting test data\ny_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model\nprint('R squared score',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R square score is prety high almost 80% score which is prety good. MAE, MSE and RMSE values also shows pretty good result. It was written that in AmritGrg's Notebook. I don't know if my result is good or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for underfitting and overfitting\nprint('Train Score: ', lr.score(x_train,y_train))\nprint('Test Score: ', lr.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above train score and test score comparable which is good. Even though it shows a slight case of underfitting but thats fine here. Is it? I don't know.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#RandomForestRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators=100)\nrf_reg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf_reg.predict(x_test)\nprint(y_pred[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluating the model\n\nprint('R squared error',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check score\nprint('Train Score: ', rf_reg.score(x_train,y_train))\nprint('Test Score: ', rf_reg.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"We have train set accuracy of 0.9792356047371404 and test set accuracy of 0.8848913083086402. Here we can see overfitting issue but for now we'll leave it alone. They are still pretty good score.\"  I didn't understand again that words and that numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Such a nice error plot? We can see the errors are normally distributed. If you say so.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Code from Ria https://www.kaggle.com/rai555/google-landmark-retrieval-eda-outlier-analysis/comments","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\nsns.set_style('whitegrid')\nfig, (ax1,ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 6)\n\nsns.countplot(df['Punjab'], order=df['Punjab'].value_counts().index[:20],palette='viridis', ax=ax1)\nsns.countplot(df['Rajasthan'], order=df['Rajasthan'].value_counts().index[:20],palette='viridis', ax=ax2)\n\nax1.tick_params(axis='x', labelrotation=45)\nax2.tick_params(axis='x', labelrotation=45)\nax1.set_title('Punjab')\nax2.set_title('Tripura')\nax2.set(ylim=(0, 100))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i1.wp.com/gyanshare.in/wp-content/uploads/2019/11/List-of-Indian-states-and-their-languages.jpg?w=800&ssl=1)\nhttps://gyanshare.in/list-of-indian-states-and-their-languages-indian-states-and-their-languages/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Das War's, Kaggle Notebook Runner: MarÃ­lia Prata  @mpwolke ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}