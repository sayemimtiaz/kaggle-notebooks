{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BREAST CANCER WISCONSIN ANALYSIS\n\n## Purpose\n\nThe purpose of this notebook is to use the breast cancer database from the wisconsin in order to determine if according to ****some characterstics**** of a cell nuclei, the diagnosis is **malignant** or **belign**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comprehension of the data\n\nA Fine Needle Aspiration (FNA) Biopsy is a simple procedure that involves passing a thin needle through the skin to sample fluid or tissue from a cyst or solid mass, as can be seen in the picture below. \n\n<img src=\"https://healthengine.com.au/info/uploads/VMC/DiseaseImages/1270_Breast_FNA.jpg\">\n\n### Then, for they look to cell nucleis of the breast and analyse its characteristics :\n\n**a)** radius (mean of distances from center to points on the perimeter)\n\n**b)** texture (standard deviation of gray-scale values)\n\n**c)** perimeter\n\n**d)** area\n\n**e)** smoothness (local variation in radius lengths)\n\n**f)** compactness (perimeter^2 / area - 1.0)\n\n**g)** concavity (severity of concave portions of the contour)\n\n**h)** concave points (number of concave portions of the contour)\n\n**i)** symmetry\n\n**j)** fractal dimension (\"coastline approximation\" - 1)\n\n### And for each of these characteristics, there is 3 values :\n\n**1)** the mean of all cells nucleis \n\n**2)** the standard deviation of all cell nucleis\n\n**3)** finally, the \"worst\" which means the mean of the three largest values\n\n### Some examples : \n\n- smoothness_mean : local variation for each cells are calculated, and we take the mean\n- symmetry_worst : symmetry is calculated for each cells according their way of calcul, then the 3 worst are selected and finally they do the mean of t\n\n### Here is the first line of the dataframe : "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndisplay(data.head())\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shape of this dataframe is (569, 33). It has 569 rows (= examples) and 33 columns. Furthermore, there\nIn these 33 columns, we have :\n- the id column : id of an analysed breast\n- an unnamed columns at the end, which contains only NaN values, so we will drop it\n- All the characteristics : it takes 30 columns because we have 10 differents characteristics and 3 differents values for each of these characteristics so 10*3 = 30\n- Finally, the most important columns : the **\"label\"**. This is what we want to predict given an example of cell nucleis. \n"},{"metadata":{},"cell_type":"markdown","source":"## Let's the correlation between the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, mask=mask, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's understand te label\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['diagnosis'].copy()\n\nprint('There is only {} differents label which are {}'.format(len(np.unique(y)), np.unique(y)))\nprint('----------------------------------------------')\nproportion_B = (y == 'M').sum()/len(y) *100\nprint('''Proportion of label = 'B': {:0.2f} %'''.format(proportion_B))\nprint('''Proportion of label = 'M': {:0.2f} %'''.format(100 - proportion_B))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## But what means M and B ?\n\nM stands for **Malignant** and B for **Benign**\n\n## What is that ? \n\nThe samples taken by the FNA realized are examined by a pathologist under a microscope. A detailed report will then be provided about the type of cells that were seen, including any suggestion that the cells might be cancer. It is important to remember that having a lump or mass does not necessarily mean that it is cancerous; many fine needle aspiration biopsies reveal that suspicious lumps or masses are benign (non-cancerous) or cysts. Aspirate samples may be described as one of the following types:\n\n* **Inadequate/insufficient:** The sample taken was not adequate to exclude or confirm a diagnosis.\n* **Benign:** There are no cancerous cells present. The lump or growth is under control and has no spread to other areas of the body.\n* **Atypical/indeterminate, or suspicious of malignancy:** The results are unclear. Some cells appear abnormal but are not definitely cancerous. A surgical biopsy may be required to adequately sample the cells.\n* **Malignant:** The cells are cancerous, uncontrolled and have the potential or have spread to other areas of the body.\n\nIn our dataset, only aspirate samples which are **only** Benign and Malignant are described\n\nNow we have understand the dataset, let's do some **preprocessing** ! (This technique allows the dataset to be in the best form possible to feed the machine learning model)\n\n## Let's change M to 1 and B to 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef our_binary_encoder(y):\n    le = LabelEncoder()\n    y_enc = le.fit_transform(y)\n    return y_enc\n\ny_enc = our_binary_encoder(y)\ny_enc.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### y_enc is a list where 1 means Malignant (cancerous) and 0 means Benign (no cancerous)"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nHere the main tasks will be to create separate X and y (our example and the label) in order to find a **mapping** between both. \n\nWe will also **rescale** the data because some data may have more variations than others so the dataset may be biaised. Each columns will have a mean of 0 and a variance of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    df = df.copy()\n    \n    ##drop useless columns (id and last) and the label\n    df = df.drop(['id', 'Unnamed: 32','diagnosis'], axis=1)\n    \n    return df\n\nX = preprocessing(data)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the data is totally clean, no missing features and we suppose there is no outliers. Thus, preprocessing is very fast.\n\nNow we split our data with train_test_split from sklearn.model_selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size = 0.2, random_state = 42)\nprint(f'The shape of X_train is {X_train.shape}')\nprint(f'The shape of X_test is {X_test.shape}')\nprint(f'The shape of y_train is {y_train.shape}')\nprint(f'The shape of y_test is {y_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, it's time to train our algorithms in 2 differents way\n\n## 1. Let's use a Pipeline to show its strengh\n\nA **pipeline** combines a **transformer** and a **estimator** in a same bloc !\n\nWe will do the following pipeline : \n\n<img src=\"https://tse4.mm.bing.net/th?id=OIP.n6CHRAuypD7Qw5cWFBpwsgHaFa&pid=Api\">\n\n* Scaling : We will scale every feature in order to have mean(column) = 0 and std(columns) = 1\n* Dimensionality Reduction : We will use PCA (Principal Component Analysis). It allows to decrease the dimensionality while keeping the maximum of information.\n* We will choose a learning algorithm that best fit with our training set and ALSO with the test set (to avoid overfitting)"},{"metadata":{},"cell_type":"markdown","source":"# Librairies"},{"metadata":{},"cell_type":"markdown","source":"### We define our librairies first"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import precision_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Then we call them with simplier name"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\npca = PCA(n_components = 3) # n_components is the number of dimension after PCA\n\nlog_reg = LogisticRegression(solver ='lbfgs', C = 10**10)\nsvc = LinearSVC(C = 1, loss = 'hinge', max_iter=1000)\ntree_clf = DecisionTreeClassifier(max_depth = 5)\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\nliste_estimator = [log_reg, svc, tree_clf, forest_clf]\nliste_estimator_str = ['log_reg', 'svc', 'tree_clf', 'forest_clf']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate the precision for each estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_classification_report_with_pipeline(X_train, y_train, X_test, y_test, liste_estimator):\n\n    for estimator in liste_estimator:\n        \n        ## Define and train the pipeline on the trainset\n        model = make_pipeline(scaler, pca, estimator)\n        model.fit(X_train, y_train)\n        \n        ## Predict the label from the testset\n        y_pred = model.predict(X_test)\n        \n        ## Do some report for each estimator (4 in our example)\n        report = classification_report(y_test, y_pred)\n        \n        ## Print it\n        print('Estimator : ',estimator)\n        print(report)\n        \nshow_classification_report_with_pipeline(X_train, y_train, X_test, y_test, liste_estimator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Without Pipeline\n\n## Let's do exactly the same thing but without a Pipeline, and we will plot confusion_matrix\n"},{"metadata":{},"cell_type":"markdown","source":"### Here it is what the pipeline do automatically "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = 8 # FIX THE FINAL NBR OF DIMENSION\n\ndef our_transformator_X(X_train, X_test, n_components):\n    scaler = StandardScaler()\n    pca = PCA(n_components = n_components)\n    \n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    pca_train = pca.fit_transform(X_train_scaled)\n    pca_train_df = pd.DataFrame(pca_train, columns=[\"PCA\" + str(i) for i in range(n_components)])\n\n    pca_test = pca.transform(X_test_scaled)\n    pca_test_df = pd.DataFrame(pca_test, columns=[\"PCA\" + str(i) for i in range(n_components)])\n    \n    return pca, pca_train_df, pca_test_df\n\npca, pca_train_df, pca_test_df = our_transformator_X(X_train, X_test, n_components = n_components)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's plot them"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\ndef confusion_matrix(X_train, y_train, X_test, y_test, liste_estimator, cv = 10):\n    for estimator in liste_estimator:\n        estimator.fit(X_train, y_train)\n        plot_confusion_matrix(estimator, X_test, y_test)\n\nprint(liste_estimator)\nconfusion_matrix(pca_train_df, y_train, pca_test_df, y_test, liste_estimator, cv=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We will skip the part of hyperparameter optimization and keep logistic regression which seems th be the best performant\n\n#### Let's do some plot to understand how PCA and LogisticRegression works"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x = pca.explained_variance_ratio_, y = [\"PCA\" + str(i) for i in range(n_components)])\nplt.xlim(0,1)\nplt.xlabel('proportion de variance des donnÃ©es originales')\nplt.ylabel('PCA')\nplt.show()\nprint(f'{pca.explained_variance_ratio_.sum()*100}% of the variance is preserved when 30=>{n_components} dimension')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the reduction of dimensionality is good when the number of features is large ! However, it decreases the variance of the data ( = less information)"},{"metadata":{},"cell_type":"markdown","source":"# Let's do a 2-D plot with boundaries with the best estimator = LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define X and y as : X is X_train after scaling and PCA(2) and y is y_train\npca, X, pca_test_df = our_transformator_X(X_train, X_test, n_components = 2)\ny = y_train\n\n## We use LogisticRegression and fit it\nlog_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\nlog_reg.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's do a nice plot in 2D (n_components = 2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x0, x1 = np.meshgrid(\n        np.linspace(-7, 17, 500).reshape(-1, 1),\n        np.linspace(-9, 13, 200).reshape(-1, 1),\n    )\n\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = log_reg.predict_proba(X_new)\n\nplt.figure(figsize=(12, 6))\nplt.plot(X[y==0].iloc[:,0], X[y==0].iloc[:,1], \"bs\")\nplt.plot(X[y==1].iloc[:,0], X[y==1].iloc[:,1], \"g^\")\n\nzz = y_proba[:, 1].reshape(x0.shape)\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n\nleft_right = np.array([-7, 17])\nboundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n\nplt.clabel(contour, inline=0.5, fontsize=12)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(-5, 7.5, \"Malignant\", fontsize=14, color=\"b\", ha=\"center\")\nplt.text(10, 7.5, \"Belign\", fontsize=14, color=\"g\", ha=\"center\")\nplt.xlabel(\"PCA0\", fontsize=14)\nplt.ylabel(\"PCA1\", fontsize=14)\nplt.axis([-7, 17, -9, 13])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}