{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from os import walk\nfor (dirpath, dirnames, filenames) in walk(\"../input\"):\n    print(\"Directory path: \", dirpath)\n    print(\"Folder name: \", dirnames)\n    print(\"File name: \", filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd, matplotlib.pyplot as plt, numpy as np, datetime as dt, seaborn as sbrn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Testers\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MSE metric, as stated in Kaggle Documentation.\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Estimators/models.\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection Functions\nfrom sklearn.feature_selection import SelectKBest, RFE, RFECV\nfrom sklearn.feature_selection import f_regression, mutual_info_regression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Funciton Definitions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scope_names = dir()\nscope_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_name_from_estimator(obj_estimator):\n    \n    name = str(obj_estimator).split('(')\n    name = name[0].strip('(')\n    return name\n\ndef return_repeatedstratifiedkfold(splits = 5, repeats = 5, rand_state = 88):        \n    from sklearn.model_selection import RepeatedStratifiedKFold\n    \n    cv_splitter = RepeatedStratifiedKFold(\n        n_splits = splits,\n        n_repeats = repeats,\n        random_state = rand_state\n        )\n    \n    return cv_splitter\n\ndef run_rfecv(df, lst_X, y, estimator, scoring_method, cv_splitter = None):\n    \n    if \"RFECV\" not in scope_names:\n        from sklearn.feature_selection import RFECV    \n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    rfecv = RFECV(\n        estimator = estimator,\n        cv = cv_splitter,\n        scoring = scoring_method,\n        n_jobs = -1\n        )\n    \n    rfecv.fit(X = df[lst_X], y = df[y])\n    \n    results = {\n        \"features\": df[lst_X].columns[rfecv.support_],\n        \"split_scores\": rfecv.grid_scores_,\n        \"fit_estimator\": rfecv.estimator_,\n        \"rankings\": rfecv.ranking_\n        }\n    \n    return results\n\ndef run_kbest(lst_X, y, df, n_passers = \"all\", score_func = None):\n    if (score_func == None) and (\"mutual_info_regression\" not in scope_names):\n        from skelarn.feature_selection import mutual_info_regression\n        score_func = mutual_info_regression\n    \n    if \"SelectKBest\" not in scope_names:\n        from sklearn.feature_selection import SelectKBest\n    \n    kbest = SelectKBest(score_func = score_func, k = n_passers)\n    kbest.fit(X = df[lst_X], y = df[y])\n        \n    results = {\n        \"kbest_scores\": kbest.scores_,\n        \"kbest_pvalues\": kbest.pvalues_,\n        \"kbest_params\": kbest.get_params(),\n        \"passing_features\": kbest.get_support()\n        }\n    \n    return results\n\ndef run_randsearch(estimator, X_data, y_data, dict_params,\n                   scoring_method = None,\n                   n_combinations = 128,\n                   cv_splitter = None,\n                   get_trainset_scores = False\n                   ):\n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    randomized_search = RandomizedSearchCV(\n        estimator = estimator,\n        param_distributions = dict_params,\n        n_iter = n_combinations,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )        \n    \n    randomized_search.fit(X = X_data, y = y_data)\n    dict_cv_results = randomized_search.cv_results_\n    \n    results = {\n        \"cv_results\": dict_cv_results,\n        \"cv_results_best\": dict_cv_results['params'][randomized_search.best_index_],\n        \"best_params\": randomized_search.best_params_,\n        \"best_score\": randomized_search.best_score_,\n        \"best_estimator\": randomized_search.best_estimator_,\n        \"refit_time\": randomized_search.refit_time_\n        }\n    \n    return results\n\ndef manymodel_manyfeatureset_randsearch(\n        dict_estimators_params,\n        dict_feature_sets,\n        dict_target_features,\n        scoring_method = None,\n        n_combinations = 128,\n        cv_splitter = None,\n        get_trainset_scores = False        \n        ):\n    \n    model_results = {}\n    \n    for estimator, param_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(estimator)        \n        featureset_results = {}\n        \n        for alias, feature_data in dict_feature_sets.items():\n            featureset_results[alias] = run_randsearch(\n                estimator = estimator,\n                X_data = feature_data,\n                y_data = dict_target_features[alias],\n                dict_params = param_grid)\n        \n        model_results[name] = featureset_results\n    \n    return model_results\n\ndef run_gridsearch(\n        estimator, df, lst_X, y, dict_params,\n        scoring_method = None,\n        cv_splitter = None,\n        get_trainset_scores = False\n        ):\n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    grid_search = GridSearchCV(\n        estimator = estimator,\n        param_grid = dict_params,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )\n    \n    grid_search.fit(X = df[lst_X], y = df[y])\n    cv_results = grid_search.cv_results_\n    \n    results = {\n        \"best_params\": grid_search.best_params_,\n        \"best_score\": grid_search.best_score_,\n        \"best_estimator\": grid_search.best_estimator_,\n        \"cv_results\": cv_results,\n        \"best_of_cv_results\": cv_results['params'][grid_search.best_index_]\n        }\n    \n    return results\n\ndef manymodel_manyfeatureset_hparam_gridsearch(\n    dict_estimators_params,\n    dict_X_combinations,    \n    df,\n    y,\n    scoring_method,\n    cv_splitter = return_repeatedstratifiedkfold(),\n    get_trainset_scores = False):\n    \n    dict_manymodel_gridsearch = {}\n    \n    for model, hparam_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(model)\n        rslt = {}\n        \n        for alias, feature_combination in dict_X_combinations.items():\n            rslt[alias] = run_gridsearch(\n                estimator = model,\n                dict_params = hparam_grid,\n                df = df,\n                y = y,\n                lst_X = feature_combination,\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter,\n                get_trainset_scores = get_trainset_scores\n            )            \n            \n        dict_manymodel_gridsearch[name] = rslt\n        \n    return dict_manymodel_gridsearch\n    \ndef run_crossvalscore(\n        estimator, train_data, target_feature,\n        scoring_method = None,\n        cv_splitter = None\n        ):\n    \n    if (cv_splitter == None) and (\"RepeatedStratifiedKFold\" not in scope_names):\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    cvscores = cross_val_score(\n        estimator = estimator,\n        X = train_data,\n        y = target_feature,\n        scoring = scoring_method,\n        cv = cv_splitter,\n        n_jobs = -1\n        )    \n    \n    mean_of_scores = np.nanmean(cvscores)\n    mean_of_scores_withnan = np.mean(cvscores)\n    standard_deviation = np.std(cvscores)\n    variance = np.var(cvscores)    \n    \n    results = {\n        \"mean_score\": mean_of_scores,\n        \"nanmean_score\": mean_of_scores_withnan,\n        \"std\": standard_deviation,\n        \"var\": variance\n        }    \n    \n    return results\n\ndef manymodel_manyfeatureset_cvs(\n        lst_estimators,\n        dict_featuresets,\n        dict_target_features,\n        scoring_method,\n        cv_splitter = return_repeatedstratifiedkfold(),\n        ):\n    \"\"\"\n    Parameters\n    ----------\n    lst_stimators: List-like of Estimator Objects/Models\n        A list with properly instantiated estimators or algorithm models, ex:\n            MLPRegressor(hidden_layer_sizes = (64, 64)).\n    \n    dict_featuresets: Dictionary\n        A dictionary. The keys are arbitrary names/aliases used to identify the\n        different sets of training data. The values are the corresponding set of\n        feature data to be passed \"as-is\" to an estimator. Examples are\n        dataframes, series, or NumPy n-dimensional arrays.\n        \n    dict_target_features: Dictionary\n        A Python dictionary whose key-value pairs are a name and target feature data set\n        stored as a series or dataframe. The keys, i.e. names/aliases, must be the same\n        as the corresponding target feature's learning input data. For example,\n            `dict_featuresets` contains the values [\"df_classif\": df_classify,\"df_reg\":df_regress],\n            `dict_target_features` must contain [\"df_classif\":Beats_Marketprice, \"df_reg\":Price]\n    \n    scoring_method: String\n        A string giving the scoring metric or criteria, such as root mean squared error\n        or accuracy score.\n    \n    cv_splitter: CV Splitter Object, default RepeatedStratifiedKFold(n_repeats = 5, n_splits = 5)\n        An instance of an SKLearn cross-validation splitter.\n    \n    Returns\n    -------\n    dict_results: Dictionary\n        A dictionary whose keys are the estimator names with parenthesis and parameters removed.\n        The values are also dictionaries.\n        The \"second layer\" dictionaries corresponding to the estimator names\n        have the feature set aliases as keys and\n        dictionaries with summary statistics about the cross-validation scores.\n        Sample Structure:\n        {\n            \"LinearRegression\":\n                {\n                    \"df_raw\":\n                        {\"mean_score\":NaN, \"nanmean_score\":0.53, \"std\":3, \"var\":9},\n                    \"df_processed\":\n                        {\"mean_score\": 0.75, \"nanmean_score\":0.75, \"std\":0.5, \"var\":0.70710}\n             },\n            \"MLPRegression\":\n                {\n                    \"df_raw\":\n                        { \"mean_score\":NaN, \"nanmean_score\":0.83, \"std\":3, \"var\":9 },\n                    \"df_processed\":\n                        {\"mean_score\": 0.85, \"nanmean_score\":0.95, \"std\":0.1, \"var\":0.31622}\n                }    \n        }\n\n    \"\"\"\n    \n    dict_results = dict()\n    \n    for estimator in lst_estimators:\n        name = make_name_from_estimator(estimator)        \n        featureset_rslt = dict()\n        \n        for alias, X in dict_featuresets.items():\n            featureset_rslt[alias] = run_crossvalscore(\n                estimator = estimator,\n                train_data = X,\n                target_feature = dict_target_features[alias],\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter\n                )\n        \n    dict_results[name] = featureset_rslt\n    \n    return dict_results\n\n\ndef make_traintest(df, train_fraction = 0.7, random_state_val = 88):\n    df = df.copy()\n    df_train = df.sample(frac = train_fraction, random_state = random_state_val)    \n    bmask_istrain = df.index.isin(df_train.index.values)\n    df_test = df.loc[ ~bmask_istrain ]\n    \n    return {\n        \"train\":df_train,\n        \"test\":df_test\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing_tsv = pd.read_csv(filepath_or_buffer = \"../input/ames-iowa-housing-tsv/AmesHousingTabSep.tsv\",\n                         sep = '\\t',\n                         index_col = \"Order\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing_tsv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing = pd.read_csv(filepath_or_buffer = \"../input/housing/AmesHousing.csv\",  \n                         index_col = \"Order\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_shape = df_housing.shape\noriginal_shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Data Formatting and Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Removing Columns that Leak Future Information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features below will not be available for unsold houses. They might also \"leak\" information that would not be present in\n# actual data sets of unsold houses.\nleakers = [\n    \"Mo Sold\",\n    \"Yr Sold\",\n    \"Sale Type\",\n    \"Sale Condition\"\n]\ndf_housing.drop(columns = leakers, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df_housing.columns.values\ncols.sort()\ncols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting Columns to Appropriate Data Types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_true_integers = [    \n    \"Year Built\",\n    \"Year Remod/Add\",    \n    \"Bsmt Full Bath\",\n    \"Bsmt Half Bath\",\n    \"Full Bath\",\n    \"Half Bath\",\n    \"Bedroom AbvGr\",\n    \"Kitchen AbvGr\",\n    \"TotRms AbvGrd\",\n    \"Fireplaces\",\n    \"Garage Yr Blt\",\n    \"Garage Cars\",    \n]\nlst_true_integers.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_true_floats = [\n    \"Lot Frontage\",\n    \"Lot Area\",\n    \"Mas Vnr Area\",\n    \"BsmtFin SF 1\",\n    \"BsmtFin SF 2\",\n    \"Bsmt Unf SF\",\n    \"Total Bsmt SF\",\n    \"1st Flr SF\",\n    \"2nd Flr SF\",\n    \"Low Qual Fin SF\",\n    \"Gr Liv Area\",\n    \"Garage Area\",\n    \"Wood Deck SF\",\n    \"Open Porch SF\",\n    \"Pool Area\",\n    \"Enclosed Porch\",\n    \"3Ssn Porch\",\n    \"Screen Porch\",\n    \"Misc Val\"\n]\nlst_true_floats.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Study of a Dangerous Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns with suspiciously low number of non-null values/too many null values.\n#When run, columns that do have values return several non-null values in the resulting dictionary.\n#To be studied later.\n\n# null_counts = df_housing.isnull().sum()\n# print(\"Null Counts from `df.isnull().sum()`:\\n\", type(null_counts), null_counts, \"\\n=====\")\n# dict_null_counts = dict(zip(null_counts.index.values, null_counts.values))\n# print(\"Results of `null_counts.index.values:\\n\", type(null_counts.index.values), null_counts, \"\\n=====\")\n# print(\"Results of `null_counts.values:\\n\", type(null_counts.index.values), null_counts, \"\\n=====\")\n# for column, null_count in dict_null_counts.items():\n#     if null_count > 0:\n#         print(column, \":\\t\", null_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns with suspiciously low number of non-null values/too many null values.\nnull_counts = df_housing.isnull().sum()\ndict_nulls = dict(zip(null_counts.keys(), null_counts.values))\nfor feature, null_count in dict_nulls.items():\n    print(feature, ':\\t', null_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Null Counts in the Data Frame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_have_nulls = []\nfor feature in df_housing.columns.values.tolist():\n    nullcount = df_housing[feature].isnull().sum()\n    if nullcount > 0:\n        lst_have_nulls.append(feature)\n        print(feature, \"\\n=====\\nNull Count:\\t\", nullcount,'\\n*****')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing With Columns That Have Many Null Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The main sources for the decision are two questions on Analytics Vidhya (slugs `missing-value-threshold` and `what-should-be-the-allowed-percentage-of-missing-vales`) and an article on Statistics Solutions (*Missing Values in Data*).\n\nThe two Analytics Vidhya articles stated that normally a column with five percent missing values should be imputed. Any hiigher and the column may be dropped. However, \"in practice\", it will depend wildly on two things. One is model performance on different data sets with different features removed. The other and seemingly more important one is \"significance\" or \"information\" contained in a feature. If a feature is very important or has plenty information, you should impute it.\n\nEqually siginificant, if you have a reliable way of guessing or anticipating values, such as in time series analysis where a certain column follows a pattern, you may also impute more values. The answer suggested using `pd.Series.interpolate`.\nHowever, since it would be time consuming to do significant research into the significance of each variable in house sale price, the standard \"by the book\" theoritical thresholds will be used to decide whether or not to impute a feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### To Split: Before or After Cleaning? Before or After Analysis?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"An email was sent to Jason Brownlee asking whether a data set should be split before data cleaning or not, among other questions. For now, it is assumed that splitting the data set is best because it simulates production procedure: splitting on data to predict using data that could not possible be used for model training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_traintest = make_traintest(df = df_housing)\ndf_train = dict_traintest[\"train\"]\ndf_test = dict_traintest[\"test\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df_housing_tsv, train_size = 0.70, test_size = 0.30, random_state = 88)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping Features with 5% or More Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def impute_series(sr_values, feature_name = ''):\n    sr_out = sr_values.copy()\n    try:        \n        sr_out.fillna(value = sr_values.mean())\n        print(\"Feature\", feature_name, \"imputed with mean:\", sr_values.mean())\n    except Exception as e:\n        print(\"Filling NaN values with mean of feature\", feature_name, \"caused an error:\\n\", e)\n        try:\n            sr_out.fillna(value = sr_values.median())\n            print(\"Feature\", feature_name, \"imputed with median:\", sr_values.median())\n        except Exception as e:\n            print(\"Filling NaN values with median for feature\", feature_name, \"caused an error:\\n\", e)\n            sr_out.fillna(value = sr_values.mode())\n            print(\"Feature\", feature_name, \"imputed with mode:\", sr_values.mode())            \n    \n    return sr_out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_columns = df_train.columns[df_train.isna().sum() / df_train.shape[0] > 0.05]\ndf_train.drop(drop_columns, axis=1)\n\nnum_columns = []\ncat_columns = []\n\nfor col, dtype in df_train.dtypes.iteritems():\n    if dtype in [np.int64, np.float64]:\n        num_columns.append(col)\n    else:\n        cat_columns.append(col)\n        \ndf_train[num_columns] = df_train[num_columns].fillna(df_train[num_columns].mean())\ndf_train[cat_columns] = df_train[cat_columns].fillna(df_train[cat_columns].mode())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_count = df_train.shape[0]\nsr_null_counts = df_train.isnull().sum()\nlst_drop = []\nend_decorator = \"\\n******\"\nfor feature in lst_have_nulls:\n    threshold = 0.05\n    null_percent = (sr_null_counts[feature] / row_count)\n    \n    if null_percent > 0.05:\n        print(\"To Drop: \", feature, type(df_train[feature]), type(df_train[feature].iloc[0]) )\n        lst_drop.append(feature)\n    else:\n        print(\"Imputed:\", str(type(df_train[feature]) ), type(df_train[feature].iloc[0]) )\n        data = df_train[feature]\n        df_train[feature] = impute_series(sr_values = data, feature_name = feature)\n        \n    print(end_decorator)\n    \ndf_train.drop(columns = lst_drop, inplace = True)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_have_nulls.sort()\nlst_have_nulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}