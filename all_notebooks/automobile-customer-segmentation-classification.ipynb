{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction: Business Goal & Problem Definition\n\nThe goal of this project is to study and predict the right group of new customers for an automotive company, so the company can adopt the specific proven marketing strategy to each of them and be more succesful in the business. For that we´ll use the Customer Segmentation Classification Dataset available in Kaggle, containing 10,695 observations, each with the following attributes:\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\n* ID\tUnique ID\n* Gender\tGender of the customer\n* Ever_Married\tMarital status of the customer\n* Age\tAge of the customer\n* Graduated\tIs the customer a graduate?\n* Profession\tProfession of the customer\n* Work_Experience\tWork Experience in years\n* Spending_Score\tSpending score of the customer\n* Family_Size\tNumber of family members for the customer (including the customer)\n* Var_1\tAnonymised Category for the customer\n* Segmentation\t(target) Customer Segment of the customer"},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing Basic Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_train_ds = pd.read_csv(\"../input/customer-segmentation/Train.csv\", sep=\",\")\nauto_test_ds = pd.read_csv(\"../input/customer-segmentation/Test.csv\", sep=\",\")\nauto_ds = pd.concat([auto_train_ds, auto_test_ds])\n\nauto_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preliminary Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nauto_ds.sample(n=10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking dataset info by feature\n\nauto_ds.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of zeros in rows\n\n(auto_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(auto_ds==0).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of duplicated rows\n\nauto_ds.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking data balancing (for classification)\n\nauto_balancing = pd.DataFrame()\nauto_balancing[\"Count\"] = auto_ds[\"Segmentation\"].value_counts()\nauto_balancing[\"Count%\"] = auto_ds[\"Segmentation\"].value_counts()/auto_ds.shape[0]*100\n\nauto_balancing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking basic statistical data by feature\n\nauto_ds.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Cleaning\n\n    We´ll perform the following:\n\n    1. Remove duplicated rows (38 in total)\n\n\n    2. Create a calculated column (Work_Experience_to_Age_Ratio) that could potentialy be a relevant feature to the model (to be tested later)\n\n\n    3. Treat missing values:\n        *   3.1 Ever_Married: consider missing values as No\n        *   3.2 Graduated: consider missing values as No\n        *   3.3 Profession: delete (it´s not representative the amount of blanks)\n        *   3.4 Family_Size: mean\n        *   3.5 Var_1: delete (it´s not representative the amount of blanks)\n        *   3.6 Work_Experience: estimate based on Age\n         \n    \n    4. Remove column ID as it´s not important to the model\n           \n        \n    5. Convert categorical variables (Gender, Ever_Married, Graduated, Profession, Spending_Score, Var_1, Segmentation) to dummies\n    \n    \n    6. Convert all numerical variables (Age, Work_Experience, Family_Size, Work_Experience_to_Age_Ratio) to categorical ranges (to be used in next step when analyzing correlations)\n    \n\n    * No outliers found\n    * The entire dataset will be taken"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\nauto_ds.drop_duplicates(inplace=True)\n\n#2\nauto_ds[\"Work_Experience_to_Age_Ratio\"] = auto_ds[\"Work_Experience\"] / auto_ds[\"Age\"]\n\n#3\nauto_ds[\"Ever_Married\"].fillna(\"No\", inplace=True)\nauto_ds[\"Graduated\"].fillna(\"No\", inplace=True)\nauto_ds.dropna(subset=[\"Profession\"], inplace=True)\nauto_ds[\"Family_Size\"].fillna(auto_ds[\"Family_Size\"].mean(), inplace=True)\nauto_ds.dropna(subset=[\"Var_1\"], inplace=True)\nauto_ds[\"Work_Experience\"].replace(0, np.nan, inplace=True)\nauto_ds[\"Work_Experience\"].fillna(auto_ds[\"Work_Experience\"].sum() / auto_ds[\"Age\"].sum() * auto_ds[\"Age\"], inplace=True)\nauto_ds[\"Work_Experience_to_Age_Ratio\"].replace(0, np.nan, inplace=True)\nauto_ds[\"Work_Experience_to_Age_Ratio\"].fillna(auto_ds[\"Work_Experience\"] / auto_ds[\"Age\"], inplace=True)\n\n#4\nauto_ds.drop([\"ID\"], axis=1, inplace=True)\n\n#5\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Gender\"], prefix=\"Gender\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Ever_Married\"], prefix=\"Ever_Married\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Graduated\"], prefix=\"Graduated\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Profession\"], prefix=\"Profession\")], axis=1)\nauto_ds[\"Spending_Score_Level\"] = auto_ds[\"Spending_Score\"].apply(lambda x: [\"Low\", \"Average\", \"High\"].index(x))+1 #Label encoding\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Var_1\"], prefix=\"Var_1\")], axis=1)\n#target variable for the ML model (label encoding)\nauto_ds[\"Segmentation_Level\"] = auto_ds[\"Segmentation\"].apply(lambda x: [\"A\", \"B\", \"C\", \"D\"].index(x))+1 #Label encoding\n#target variable for the DL model (one-hot encoding)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Segmentation\"], prefix=\"Segmentation\")], axis=1)\nSegmentation_encoding_dl = np.asarray(auto_ds[[\"Segmentation_A\", \"Segmentation_B\", \"Segmentation_C\", \"Segmentation_D\"]]) #creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\n\n#6\nauto_ds[\"Age_Range\"] = np.where(auto_ds.Age>=60, \"60+\", np.where(auto_ds.Age>=50, \"50-60\", np.where(auto_ds.Age>=40, \"40-50\", np.where(auto_ds.Age>=30, \"30-40\", np.where(auto_ds.Age>=18, \"18-30\", \"18-\")))))\nauto_ds[\"Work_Experience_Range\"] = np.where(auto_ds.Work_Experience>=10, \"10+\", np.where(auto_ds.Work_Experience>=5, \"5-10\", \"0-5\"))\nauto_ds[\"Family_Size_Range\"] = np.where(auto_ds.Family_Size>=6, \"6+\", np.where(auto_ds.Family_Size>=3, \"3-6\", \"0-3\"))\nauto_ds[\"Work_Experience_to_Age_Ratio_Range\"] = np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.5, \"0.5+\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.4, \"0.4-0.5\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.3, \"0.3-0.4\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.2, \"0.2-0.3\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.1, \"0.1-0.2\", \"0+\")))))\n\nauto_ds.to_excel(\"auto_ds_clean.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.\tData Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Segmentation\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Segmentation\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Segmentation Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Gender\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Ever_Married\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Ever_Married\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Marriage Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Graduated\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Graduated\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Graduation Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Profession\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Profession\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Profession Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Spending_Score\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Spending_Score\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Spending Score Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Var_1\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Var_1\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Var 1 Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Age\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Age\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Age\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Work Experience Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Work_Experience\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Work_Experience\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Work_Experience\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Family Size Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Family_Size\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Family_Size\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Family_Size\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Work Experience to Age Ratio Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alternatively using Profile Report to see variables statistics and correlations\n\n# from pandas_profiling import ProfileReport\n# profile = ProfileReport(auto_ds, title=\"Automobile Customer Segmentation Classification\")\n# profile.to_file(output_file=\"Automobile_Customer_Segmentation_Classification.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Correlations Analysis & Features Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Bar Charts, also considering all numerical to categorical variables created at the step before\n\nfig, axarr = plt.subplots(4, 3, figsize=(30, 18))\nsns.countplot(x=\"Gender\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][0])\nsns.countplot(x=\"Ever_Married\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][1])\nsns.countplot(x=\"Graduated\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][2])\nsns.countplot(x=\"Profession\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][0])\nsns.countplot(x=\"Spending_Score\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][1])\nsns.countplot(x=\"Var_1\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][2])\nsns.countplot(x=\"Age_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][0])\nsns.countplot(x=\"Work_Experience_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][1])\nsns.countplot(x=\"Family_Size_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][2])\nsns.countplot(x=\"Work_Experience_to_Age_Ratio_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[3][0])\n\n#Deleting original categorical columns\n\nauto_ds.drop([\"Segmentation\", \"Gender\", \"Ever_Married\", \"Graduated\", \"Profession\", \"Spending_Score\", \"Var_1\", \"Age_Range\",\n              \"Work_Experience_Range\", \"Family_Size_Range\", \"Work_Experience_to_Age_Ratio_Range\"], axis=1, inplace=True)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(25,25))\nsns.heatmap(auto_ds.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\n# sns.pairplot(auto_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a Feature Importance\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n#Defining Xs and y\nX = auto_ds.drop([\"Segmentation_Level\"], axis=1)\ny = auto_ds[\"Segmentation_Level\"]\n#Defining the model\nmodel = RandomForestClassifier().fit(X, y)\n#Getting importance\nimportance = model.feature_importances_\n#Summarizing feature importance\nfor i,v in enumerate(importance):\n    print(\"Feature:{0:}, Score:{1:,.4f}\".format(X.columns[i], v))\n#Plotting feature importance\npd.Series(model.feature_importances_[::-1], index=X.columns[::-1]).plot(kind=\"barh\", figsize=(25,25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Data Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Xs and y\n\nX = auto_ds[[\"Age\", \"Family_Size\", \"Profession_Healthcare\", \"Profession_Entertainment\", \"Profession_Artist\", \"Ever_Married_Yes\",\n             \"Graduated_No\", \"Spending_Score_Level\"]]\ny = auto_ds[\"Segmentation_Level\"]\ny_dl = Segmentation_encoding_dl #for the DL model\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Setting train/test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\nX_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_scaled, y_dl, random_state=0) #for the DL model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Machine Learning Algorithms Implementation & Assessment"},{"metadata":{},"cell_type":"markdown","source":"# 9.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Logistic Regression model and checking its Metrics\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\n    y_preds_train = model_lr.predict(X_train_degree)\n    y_preds_test = model_lr.predict(X_test_degree)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 2\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the model\nmodel_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\nprint(f\"Linear Regression Intercept: {model_lr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_lr.predict(X_train_degree)\ny_preds_test = model_lr.predict(X_test_degree)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_scaled)\ny_preds_all = model_lr.predict(X_degree)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_lr.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.2 Logistic Regression CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Creating a Logistic Regression CV model and checking its Metrics (for this exercise we´re skipping this algorithm due to the execution time since we´d need a more powerful machine)\n\n# #Trying different polynomial degrees\n# degrees = [1, 2]\n# print(\"Testing degrees:\")\n# for a in degrees:\n#     poly = PolynomialFeatures(degree=a)\n#     X_train_degree = poly.fit_transform(X_train)\n#     X_test_degree = poly.fit_transform(X_test)\n#     model_lr_cv = linear_model.LogisticRegressionCV(max_iter=1000000000).fit(X_train_degree, y_train)\n#     y_preds_train = model_lr_cv.predict(X_train_degree)\n#     y_preds_test = model_lr_cv.predict(X_test_degree)\n#     accuracy_train = accuracy_score(y_train, y_preds_train)\n#     accuracy_test = accuracy_score(y_test, y_preds_test)\n#     precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n#     precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n#     recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n#     recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n#     f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n#     f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n#     print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n#     print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\")\n\n# #Choosing the best polynomial degree\n# chosen_degree = 2\n# poly = PolynomialFeatures(degree=chosen_degree)\n\n# #Working on X_train & X_test in the polynomial chosen degree\n# X_train_degree = poly.fit_transform(X_train)\n# X_test_degree = poly.fit_transform(X_test)\n\n# #Fitting to the model\n# model_lr_cv = linear_model.LogisticRegressionCV(1000000000).fit(X_train_degree, y_train)\n# print(f\"Linear Regression Intercept: {model_lr_cv.intercept_}\")\n# print(f\"Linear Regression Coefficients: {model_lr_cv.coef_}, \\n\")\n\n# #Getting the predictions & Metrics\n# y_preds_train = model_lr_cv.predict(X_train_degree)\n# y_preds_test = model_lr_cv.predict(X_test_degree)\n# accuracy_train = accuracy_score(y_train, y_preds_train)\n# accuracy_test = accuracy_score(y_test, y_preds_test)\n# precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n# precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n# recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n# recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n# f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n# f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n# print(\"Chosen degree:\")\n# print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\n# print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n# #Visualizing y_pred in the dataset\n# X_degree = poly.fit_transform(X)\n# y_preds_all = model_lr_cv.predict(X_scaled)\n# auto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\n# auto_ds.to_excel(\"model_lr_cv.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.3 SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a SVM model and checking its Metrics\n\nfrom sklearn import svm\n\n#Fitting to the model\nmodel_svm = svm.SVC().fit(X_train, y_train)\n\n#Getting the predictions & Metrics\ny_preds_train = model_svm.predict(X_train)\ny_preds_test = model_svm.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_svm.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_svm.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.4 Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Naive Bayes model and checking its Metrics\n\nfrom sklearn import naive_bayes\n\n#Fitting to the model\nmodel_nb = naive_bayes.MultinomialNB().fit(X_train, y_train)\n\n#Getting the predictions & Metrics\ny_preds_train = model_nb.predict(X_train)\ny_preds_test = model_nb.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_nb.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_nb.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.5 KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a KNN model and checking its Metrics\n\nfrom sklearn import neighbors\n\n#Trying different neighbors\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nprint(\"Testing neighbors:\")\nfor a in n_neighbors:\n    model_knn = neighbors.KNeighborsClassifier(n_neighbors=a).fit(X_train, y_train)\n    y_preds_train = model_knn.predict(X_train)\n    y_preds_test = model_knn.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best neighbor\nchosen_neighbor = 10\nmodel_knn = neighbors.KNeighborsClassifier(n_neighbors=chosen_neighbor).fit(X_train, y_train)\ny_preds_train = model_knn.predict(X_train)\ny_preds_test = model_knn.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen neighbors:\")\nprint(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_knn.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_knn.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.6 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Random Forest model and checking its Metrics\n\nfrom sklearn import ensemble\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_rf = ensemble.RandomForestClassifier(max_depth=a, random_state=0).fit(X_train, y_train)\n    y_preds_train = model_rf.predict(X_train)\n    y_preds_test = model_rf.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 5\nmodel_rf = ensemble.RandomForestClassifier(max_depth=chosen_depth, random_state=0).fit(X_train, y_train)\ny_preds_train = model_rf.predict(X_train)\ny_preds_test = model_rf.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_rf.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_rf.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.7 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a XGBoost model and checking its Metrics\n\nfrom xgboost import XGBClassifier\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_xgbc = XGBClassifier(max_depth=a, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train)\n    y_preds_train = model_xgbc.predict(X_train)\n    y_preds_test = model_xgbc.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 2\nmodel_xgbc = XGBClassifier(max_depth=chosen_depth, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train)\ny_preds_train = model_xgbc.predict(X_train)\ny_preds_test = model_xgbc.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_xgbc.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_xgbc.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.8 Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Deep Learning model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\npd.options.mode.chained_assignment = None\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[1]))\n\n#Output Layer\nmodel_dl.add(Dense(units=4, activation=\"softmax\"))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"Adam\", loss=\"CategoricalCrossentropy\", metrics=[\"CategoricalAccuracy\"])\n\n#Fitting to the model\nmodel_dl.fit(X_train_dl, y_train_dl, epochs=100)\n\n#Getting the predictions & Metrics\n#Training set\ny_preds_train = model_dl.predict(X_train_dl)\ny_preds_train = pd.DataFrame(y_preds_train)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds_train = pd.DataFrame()\nauto_ds_train[\"Segmentation_A_predicted_train\"] = y_preds_train[0]\nauto_ds_train[\"Segmentation_B_predicted_train\"] = y_preds_train[1]\nauto_ds_train[\"Segmentation_C_predicted_train\"] = y_preds_train[2]\nauto_ds_train[\"Segmentation_D_predicted_train\"] = y_preds_train[3]\n#Finding the highest probability per row\nmax_prob_per_row_train_ds = auto_ds_train[[\"Segmentation_A_predicted_train\", \"Segmentation_B_predicted_train\", \"Segmentation_C_predicted_train\", \"Segmentation_D_predicted_train\"]].idxmax(axis=1)\nmax_prob_per_row_train_ds = pd.DataFrame(max_prob_per_row_train_ds)\nmax_prob_per_row_train_ds = max_prob_per_row_train_ds.reset_index(drop=True)\nauto_ds_train[\"Segmentation_predicted_train\"] = max_prob_per_row_train_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_train_ds)):\n    if  max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0    \n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 1\n    else:\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n#Creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\ny_preds_train_encoded = np.asarray(auto_ds_train[[\"Segmentation_A_predicted_train\", \"Segmentation_B_predicted_train\", \"Segmentation_C_predicted_train\", \"Segmentation_D_predicted_train\"]])\n\n#Test set\ny_preds_test = model_dl.predict(X_test_dl)\ny_preds_test = pd.DataFrame(y_preds_test)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds_test = pd.DataFrame()\nauto_ds_test[\"Segmentation_A_predicted_test\"] = y_preds_test[0]\nauto_ds_test[\"Segmentation_B_predicted_test\"] = y_preds_test[1]\nauto_ds_test[\"Segmentation_C_predicted_test\"] = y_preds_test[2]\nauto_ds_test[\"Segmentation_D_predicted_test\"] = y_preds_test[3]\n#Finding the highest probability per row\nmax_prob_per_row_test_ds = auto_ds_test[[\"Segmentation_A_predicted_test\", \"Segmentation_B_predicted_test\", \"Segmentation_C_predicted_test\", \"Segmentation_D_predicted_test\"]].idxmax(axis=1)\nmax_prob_per_row_test_ds = pd.DataFrame(max_prob_per_row_test_ds)\nmax_prob_per_row_test_ds = max_prob_per_row_test_ds.reset_index(drop=True)\nauto_ds_test[\"Segmentation_predicted_test\"] = max_prob_per_row_test_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_test_ds)):\n    if  max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0    \n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 1\n    else:\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n#Creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\ny_preds_test_encoded = np.asarray(auto_ds_test[[\"Segmentation_A_predicted_test\", \"Segmentation_B_predicted_test\", \"Segmentation_C_predicted_test\", \"Segmentation_D_predicted_test\"]])\n\naccuracy_train = accuracy_score(y_train_dl, y_preds_train_encoded)\naccuracy_test = accuracy_score(y_test_dl, y_preds_test_encoded)\nprecision_train = precision_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nprecision_test = precision_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nrecall_train = recall_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nrecall_test = recall_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nf1_train = f1_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nf1_test = f1_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# from sklearn.metrics import confusion_matrix\n# confusion_matrix = confusion_matrix(y_test_dl, y_preds_test_encoded)\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='.0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_dl.predict(X_scaled)\ny_preds_all = pd.DataFrame(y_preds_all)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds[\"Segmentation_A_predicted\"] = y_preds_all[0]\nauto_ds[\"Segmentation_B_predicted\"] = y_preds_all[1]\nauto_ds[\"Segmentation_C_predicted\"] = y_preds_all[2]\nauto_ds[\"Segmentation_D_predicted\"] = y_preds_all[3]\n#Finding the highest probability per row\nmax_prob_per_row_all_ds = auto_ds[[\"Segmentation_A_predicted\", \"Segmentation_B_predicted\", \"Segmentation_C_predicted\", \"Segmentation_D_predicted\"]].idxmax(axis=1)\nmax_prob_per_row_all_ds = pd.DataFrame(max_prob_per_row_all_ds)\nmax_prob_per_row_all_ds = max_prob_per_row_all_ds.reset_index(drop=True)\nauto_ds[\"Segmentation_predicted\"] = max_prob_per_row_all_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_all_ds)):\n    if  max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 1\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0    \n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 1\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 1\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 1\n    else:\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\nauto_ds.to_excel(\"model_dl.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Model Deployment"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Entering Xs\n\n# age_input = float(input(\"Enter the client´s age: \"))\n# fam_input = int(input(\"Enter the client´s family size: \"))\n# prof_hc_input = str(input(\"Is the client´s profession Healthcare (Yes/No)? \"))\n# if prof_hc_input == \"Yes\":\n#     prof_hc_input = 1\n# else:\n#     prof_hc_input = 0\n# prof_e_input = str(input(\"Is the client´s profession Entertainment (Yes/No)? \"))\n# if prof_e_input == \"Yes\":\n#     prof_e_input = 1\n# else:\n#     prof_e_input = 0\n# prof_a_input = str(input(\"Is the client´s profession Artist (Yes/No)? \"))\n# if prof_a_input == \"Yes\":\n#     prof_a_input = 1\n# else:\n#     prof_a_input = 0\n# married_input = str(input(\"Has the client ever married (Yes/No)? \"))\n# if married_input == \"Yes\":\n#     married_input = 1\n# else:\n#     married_input = 0\n# grad_n_input = str(input(\"Is the client graduated (Yes/No)? \"))\n# if grad_n_input == \"Yes\":\n#     grad_n_input = 0\n# else:\n#     grad_n_input = 1\n# spend_input = int(input(\"Enter the client´s score level: \"))\n\n#Defining Xs\n# X_mod_dep = pd.DataFrame({\"Age\":[age_input], \"Family_Size\":[fam_input], \"Profession_Healthcare\":[prof_hc_input], \n#                           \"Profession_Entertainment\":[prof_e_input], \"Profession_Artist\":[prof_a_input], \n#                           \"Ever_Married_Yes\":[married_input], \"Graduated_No\":[grad_n_input], \"Spending_Score_Level\":[spend_input]})\n\n#Choosing an specific client for testing:\nX_mod_dep = pd.DataFrame({\"Age\":[79], \"Family_Size\":[1], \"Profession_Healthcare\":[0], \n                          \"Profession_Entertainment\":[0], \"Profession_Artist\":[1], \n                          \"Ever_Married_Yes\":[1], \"Graduated_No\":[0], \"Spending_Score_Level\":[3]})\n\n#Appending X_mod_dep to original X dataframe, so we can scale it all together next\n\nX_with_X_mode_dep = X.append(X_mod_dep)\nX_with_X_mode_dep = X_with_X_mode_dep.reset_index(drop=True)\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X_with_X_mode_dep)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Recovering X_mod_dep row in dataframe after scaling\n\nX_mod_dep = X_scaled.tail(1)\n\n#Predicting results\n\nprediction = model_xgbc.predict(X_mod_dep)\nif prediction == 1:\n    prediction_answer = \"A\"\nif prediction == 2:\n    prediction_answer = \"B\"\nif prediction == 3:\n    prediction_answer = \"C\"\nif prediction == 4:\n    prediction_answer = \"D\"\n\nprint(\"\")\nprint(f\"This client´s predicted Segmentation is: {prediction_answer}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this project we went through all the process from defining the business objective, collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting 7 different algorithms with metrics to select the best to predict the Customer Segmentation, what will help the business adopt the best marketing strategies to each of them and bring more market share and revenue to the company. The chosen model was XGBoost since it´s the most accurate, although it has limitations and doesn’t present a high accuracy. We could reach a more robust model having more data about customers, it´s something to explore and go deeper in the organization with the business team and the data engineer in order to explore if more relevant features are available."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}