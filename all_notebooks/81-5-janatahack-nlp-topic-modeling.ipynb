{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Topic Modeling(Using HashingVectorizer + SVC)\n\n## Problem statement\n\nThe problem statement is from Analytics Vidhya JanataHack hackathon. \n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n\n2. Physics\n\n3. Mathematics\n\n4. Statistics\n\n5. Quantitative Biology\n\n6. Quantitative Finance\n\n## Dataset\n\nThe dataset consists of three files `train.csv`, `test.csv` and `sample_submission.csv`.\n\n|Fields| Description|\n|-------|-----------|\n|ID |Unique ID for each article|\n|TITLE|Title of the research article|\n|ABSTRACT|Abstract of the research article|\n|Computer Science|Whether article belongs to topic computer science (1/0)|\n|Physics\t|Whether article belongs to topic physics (1/0)|\n|Mathematics\t|Whether article belongs to topic Mathematics (1/0)|\n|Statistics\t|Whether article belongs to topic Statistics (1/0)|\n|Quantitative Biology\t|Whether article belongs to topic Quantitative Biology (1/0)|\n|Quantitative Finance|Whether article belongs to topic Quantitative Finance (1/0)|\n\n## Approach\n\nIn this notebook, there are two approaches followed,\n1. Consider each binary column as a target variable\n2. Perform HashVectorizer for the feature extraction operation\n3. Construct model for each target variable and combine the results. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n    \n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nimport re\n\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, remove_stopwords, strip_numeric, stem_text\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer, HashingVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, Normalizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC, SVC\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:blue\">Loading Dataset</span>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ntrain_df = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/train.csv')\ntest_df = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/test.csv')\n\nsubmission_df = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/sample_submission_UVKGLZE.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for missing values and data columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.isnull().sum())\nprint(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:blue\">Explore Data</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting binary column to category\ntarget_cols = ['Computer Science', 'Physics', 'Mathematics','Statistics', 'Quantitative Biology', 'Quantitative Finance']\ny_data = train_df[target_cols]\n\n# Plot category data\nplt.figure(figsize=(10,6))\ny_data.sum(axis=0).plot.bar()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemmer object\nporter = PorterStemmer()\nwnl = WordNetLemmatizer()\n\nclass DataPreprocess:\n    \n    def __init__(self):\n        self.filters = [strip_tags,\n                       strip_numeric,\n                       strip_punctuation,\n                       lambda x: x.lower(),\n                       lambda x: re.sub(r'\\s+\\w{1}\\s+', '', x),\n                       remove_stopwords]\n    def __call__(self, doc):\n        clean_words = self.__apply_filter(doc)\n        return clean_words\n    \n    def __apply_filter(self, doc):\n        try:\n            cleanse_words = set(preprocess_string(doc, self.filters))\n#             filtered_words = set(wnl.lemmatize(w) if w.endswith('e') else porter.stem(w) for w in cleanse_words)\n            filtered_words = set(wnl.lemmatize(word, 'v') for word in cleanse_words)\n            return ' '.join(filtered_words)\n        except TypeError as te:\n            raise(TypeError(\"Not a valid data {}\".format(te)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combine Train and Test Data\n\nHere, we are combining both train and test data into one single DataFrame. This will help us to perform the data preprocessing steps for both train and test data corpus at once. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['train_or_test'] = 0\ntest_df['train_or_test'] = 1\n\nfeature_col = ['ID', 'TITLE', 'ABSTRACT', 'train_or_test']\n\n# Concat train and test data\ncombined_set = pd.concat([train_df[feature_col], test_df[feature_col]])\n\n# Combine the Title and Abstract data\ncombined_set['TEXT'] = combined_set['TITLE'] + combined_set['ABSTRACT']\n\n# Drop unwanted columns\ncombined_set = combined_set.drop(['TITLE', 'ABSTRACT'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pre-process the text data** \n\nWe have combined the train and test dataset before applying the pre-processing steps. It will make us to execute the preprocessing pipeline only once for the entire dataset, otherwise we will have to run it separately for test dataset as well. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Invoke data preprocess operation on the text data\ncombined_set['Processed'] = combined_set['TEXT'].apply(DataPreprocess())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = combined_set.loc[combined_set['train_or_test'] == 0]\ntest_set = combined_set.loc[combined_set['train_or_test'] == 1]\n\n# Drop key reference column\ntrain_set = train_set.drop('train_or_test', axis=1)\ntest_set = test_set.drop('train_or_test', axis=1)\n# View just 2 row value\ntrain_set[0:2].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n\n**HashingVectorizer**\n<pre>\nsklearn.feature_extraction.text.HashingVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', n_features=1048576, binary=False, norm='l2', alternate_sign=True)\n</pre>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lsa_reduction(X_train, X_test, n_comp=120):\n    svd = TruncatedSVD(n_components=n_comp)\n    normalizer = Normalizer()\n    \n    lsa_pipe = Pipeline([('svd', svd),\n                        ('normalize', normalizer)]).fit(X_train)\n    \n    train_reduced = lsa_pipe.transform(X_train)\n    test_reduced = lsa_pipe.transform(X_test)\n    return train_reduced, test_reduced\n\ndef vectorize(vector, X_train, X_test):\n    vector_fit = vector.fit(X_train)\n    \n    X_train_vec = vector_fit.transform(X_train)\n    X_test_vec = vector_fit.transform(X_test)\n    \n    print(\"Vectorization is completed.\")\n    return X_train_vec, X_test_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hashing Vectorizer calculates the hash value for each term thus keep only the unique words in the vector\ndef hash_vectorizer(X_train, X_test):\n    hasher = HashingVectorizer(ngram_range=(1,2), n_features=25000)\n    tfidf_transformer = TfidfTransformer(use_idf=True)\n    feature_extractor = Pipeline([('hash', hasher),\n                             ('tfidf', tfidf_transformer)]).fit(X_train)\n    \n    x_train_tf = feature_extractor.transform(X_train)\n    x_test_tf = feature_extractor.transform(X_test)\n    \n    return x_train_tf, x_test_tf\n\n\n# Hashing Vectorizer performs better than TFIDF\nX_train_hashed, X_test_hashed = hash_vectorizer(train_set['Processed'], test_set['Processed'])\n\n# X_train_hashed, X_test_hashed = vectorize(tfidf_vector, train_set['Processed'], test_set['Processed'])\n\n# Dimension reduction\n# --------------------------------------------\n# Result is not very good after feature reduction\n# ---------------------------------------------\n# x_train_svd, x_test_svd = lsa_reduction(X_train_hashed, X_test_hashed, 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_hashed.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build a Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = LogisticRegression(C=1.0,class_weight='balanced', \n#                         l1_ratio=0.9, \n#                         solver='saga', \n#                         penalty='l1')\nsvc = LinearSVC()\n\n# One vs Restclassifier\norc_clf = OneVsRestClassifier(estimator=svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for target in target_cols:\n    y = train_df[target]\n#     print(y)\n    \n    # Split from the loaded dataset\n    X_train, X_valid, y_train, y_test = train_test_split(X_train_hashed, y, test_size=0.2, shuffle=True, random_state=0)\n    \n    orc_clf.fit(X_train, y_train)\n    \n    y_pred = orc_clf.predict(X_valid)\n    \n    print(\"Label: %s \\n Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (target, \n                                                                                    accuracy_score(y_test, y_pred),\n                                                                                     precision_score(y_test, y_pred, average='micro'),\n                                                                                     recall_score(y_test, y_pred, average='micro'),\n                                                                                     f1_score(y_test, y_pred, average='micro')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict the Test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy of submission dataframe\noutput_df = submission_df.copy()\n\n# Iterate over the target variables\nfor target in target_cols:\n    y = train_df[target]\n    \n    orc_clf.fit(X_train_hashed, y)\n    \n    # Predict the values for test data\n    y_pred = orc_clf.predict(X_test_hashed)\n    # Assign the predicted vector to each column\n    output_df[target] = y_pred\n\n\n# Submission dataframe\noutput_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission file.\noutput_df.to_csv(\"ovr_svc_hash_tfidf_07.csv\", index=False)\n# output_df.to_csv(\"ovr_lr_hash_tfidf_06.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}