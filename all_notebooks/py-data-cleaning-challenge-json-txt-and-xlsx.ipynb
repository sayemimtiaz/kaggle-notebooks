{"cells":[{"metadata":{"_cell_guid":"7cf862d9-81ee-4933-b7f3-69750d9852b7","_uuid":"f54643419b1fe02262fd42ebe77ca5845212a7a4"},"cell_type":"markdown","source":"Welcome to Day 1 of the 5-Day Data Challenge! Today, we're going to be looking at how to read different file formats into R. ([If you're new to R, you might want to look through these introductory lessons.](https://www.kaggle.com/rtatman/getting-started-in-r-first-steps)) Specifically, we're going to:\n\n* Learn about different ways of storing structured data\n* Read in .json files\n* Read in .txt files\n* Read in .xls & .xlsx files\n\nI'll start by introducing each concept or technique, and then you'll get a chance to apply it with an exercise (look for the **Your turn!** section). Ready? Let's get started!\n\n___\n\n**Kernel FAQs:**\n\n* **How do I get started?**   To get started, click the blue \"Fork Notebook\" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)\n\n* **How do I run the code in this notebook?** Once you fork the notebook, it will open in the notebook editor. From there you can write code in any code cell (the ones with the grey background) and run the code by either 1) clicking in the code cell and then hitting CTRL + ENTER or 2) clicking in the code cell and the clicking on the white \"play\" arrow to the left of the cell. If you want to run all the code in your notebook, you can use the double, \"fast forward\" arrows at the bottom of the notebook editor.\n\n* **How do I save my work?** Any changes you make are saved automatically as you work. You can run all the code in your notebook and save a static version by hitting the blue \"Commit & Run\" button in the upper right hand corner of the editor. \n\n* **How can I find my notebook again later?** The easiest way is to go to your user profile (https://www.kaggle.com/replace-this-with-your-username), then click on the \"Kernels\" tab. All of your kernels will be under the \"Your Work\" tab, and all the kernels you've upvoted will be under the \"Favorites\" tab.\n\n___"},{"metadata":{"_cell_guid":"7e21caca-f2db-4ef6-b0e6-6178ebbf6200","_uuid":"c0ae05c161322eed13be49eecdd9fc9b03048156"},"cell_type":"markdown","source":"# Setting up our environment\n___\n\nFirst, let's read in all the packages we're going to use in this kernel. Usually, I'd read in the data here as well, but reading data is the whole point of this kernel, so we'll wait on that for now.\n\n>**Important:** make sure you run this cell first. Otherwise, the libraries I'll be using won't be loaded into your local R session and you'll get errors when you try to run the other cells. "},{"metadata":{"_cell_guid":"157ac262-4d72-4d66-a550-451e3b9c2f8f","_kg_hide-output":true,"_uuid":"678f90f7fe33a6d3d3bdfb4614880de6fc207075","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"# libraries we'll need\nlibrary(tidyverse) # handy utility functions\nlibrary(readxl) # for reading in xl files\nlibrary(jsonlite) # for reading in json","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0f609f7-8fc0-42f4-b656-7361739737dd","_uuid":"c00ad484b0f94b5efac69c21341895bfe0e85781"},"cell_type":"markdown","source":"# Tabular vs. Hierarchical vs. Raw Text Data\n____\n\nIf you've used R in the past, or Pandas in Python, you've probably mostly worked with .csv (or \"comma separated values\") files. This are pretty easy to deal with in R: you can read them into a dataframe using the `read.csv()` function (or `read_csv()` if you're using the tidyverse). But other data formats, in particular .json, .txt and .xls(x), can be a little trickier to read into R. \n\nBefore we dive in to how to handle these different formats, let's quickly talk about some of the different types of data you might encounter. In general, different types of data are stored in different file types because they have different structures. This also means that they are best represented with different data structures once you do read them into R. \n\n* **Tabular data** is basically a spreadsheet format like what you'd see in Excel or Google Sheets. It has rows and columns. Often, each row will be a single observation and each column will be a specific variable. The same variables will be recorded for each observation (or else you have empty columns in some rows).\n    * File formats: .csv, .tsv, .xls, .xlsx\n    * R data structure(s): dataframe, matrix (if all numeric)\n* **Hierarchical data** is data format where values can be nested within each other. With hierarchical data structures, you can have different information about each observation. You generally want to try to avoid trying to \"flatten\" hierarchical data into a tabular data structure because it's often not space efficient. For example, if you have a dataset of food products and clothes products, you probably want to know the expiration date for the food and the clothing size for the clothes. In a hierarchical structure you don't need to specify that you don't know the size for the food or the expiration date for the clothes, but in a tabular data structure you would.  As a result, you'd end up with a lot of NA cells that aren't very informative and waste space.\n    * File formats: .json, .xml\n    * R data structure(s): list\n* **Raw text data** is just that: raw text that doesn't have a specific data structure specified in the format of the file. This type of data is also called \"unstructured\". \n    * File format: .txt\n    * R data structure(s): character string\n\n![](https://i.imgur.com/6XIfG0o.png)\n\n**OK, so why does this matter?** The reason I started by talking about the differences between these types of data is because I want to make it clear that there's a bigger difference between these file formats than just the letters at the end. Different file formats also reflect differences in how the underlying data is structured. In some cases you can convert between the different types of files (for example, if you have the same fields for each object in a .json file and no nested values you can safely convert it to a .csv) but generally it's more space efficient to use a data structure that makes sense given the way the original data was organized.\n\nIn other words **the first step to reading a dataset into R is to understand how it's organized and what type of data structure you should use to store it**. Fortunately, the functions we'll be working with today are pretty smart about how they handle data and can do a lot of the guessing for us.\n\n\n## Your turn!\n\nCheck out each of these three datasets (hint: look in the Data tab so that you can see a quick preview of each dataset) and decide whether the dataset is tabular, hierarchical or raw text data. \n\n* [Reddit Memes Dataset: A collection of the latest memes from the various meme subreddits](https://www.kaggle.com/sayangoswami/reddit-memes-dataset/data)\n* [Emoji sentiment: Are people that use emoji happier?](https://www.kaggle.com/harriken/emoji-sentiment)\n* [Aristo MINI Corpus: 1,197,377 science-relevant sentences drawn from public data](https://www.kaggle.com/allenai/aristo-mini-corpus)"},{"metadata":{"_cell_guid":"07ca03d1-0218-4743-ac1a-248691c6868d","_uuid":"253c83f0d002be6b7834f896337ad3b9d74150d5"},"cell_type":"markdown","source":"# Json Files\n___\n\nJSON files generally represent hierarchical  data structures. Let's take a look at an example file to see what that looks like when we're working with them in R.\n\nThis dataset I'm working with here contains .json files with information on different movies, including their ratings across different platforms. Let's read it in using the `read_json` function from the `jsonlite` package. "},{"metadata":{"_cell_guid":"39e02f7c-5530-400f-aa13-6d426b1f15d6","_uuid":"e720de88a217bbf7a4c870c8d8445097a454d429","collapsed":true,"trusted":true},"cell_type":"code","source":"movie_ratings <- read_json(\"../input/rating-vs-gross-collector/2018-2-4.json\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89ecd149-111b-46fc-a30e-b2832045ecc5","_uuid":"4e8970be534aa4b53f6f2b25b560ad79f7f0951e"},"cell_type":"markdown","source":"Now that we've read it in, let's look at part of the structure of the data:"},{"metadata":{"_cell_guid":"d5fc4640-335f-43d1-a12a-24b3f36f975a","_uuid":"f35334db9f97f378cf0a2a72807503d0be774382","collapsed":true,"trusted":true},"cell_type":"code","source":"# Since when we look at the structure of a list we see the whole list, I'm \n# going to save that output to a variable and print just a few rows of it.\n# You can see the whole structure by just running str(movie_ratings).\njson_structure <- capture.output(str(movie_ratings))\nprint(json_structure[1:16])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61c24664-f9fc-4bde-b225-63db3a1c1766","_uuid":"f6a50060fe6938327b23853cadfa1b39060804fd"},"cell_type":"markdown","source":"By looking at the very first line, we can see that this is a list with 1 item in it. The second line tells us that this one item is itself a list, with 38 items (in this case, one for each movie). We can pull out individual entries from a list by using double bracket notation.\n\n> **What are double brackets?** Double brackets (they look like this: [[]]) let you pull out a single item from an object, either by its name or its index. You'll see them most often used with lists, but you can also use them with dataframes instead of the \\$ notation. `dataframe[['column']]`  is the same things as `dataframe$column`. \n\nSince in this data structure we have a list inside another list, we're going to need to use two sets of double brackets to get information on a single movie. In the example below, the first double bracket says that we're looking at the first object in the outer list, and the second double bracket says that we want to look at the information in the fifth object (in this case, the fifth movie)."},{"metadata":{"_cell_guid":"d64fa38f-98c7-4634-83a3-6239af5949d5","_uuid":"50e3c306c95c0dc2b06b689ca92085c05dab0347","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get the information from the fifth movie in the list\nmovie_ratings[[1]][[5]]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cb2db14c-45fc-4a8e-88fd-1db40e57eaf1","_uuid":"d1d815515db92e08bcc780e1ac9d38b60d1de4a5"},"cell_type":"markdown","source":"From there, we can use the dollar sign notation to get the value for each key in our inner list. So this next bit of code will get us the information on the Gross for the fifth movie from our list:"},{"metadata":{"_cell_guid":"a54548a6-1f42-4b7e-8e2f-9d2682abe8d7","_uuid":"54fea5a7312f2ae7105d1b35738966cc78b0a64b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get just the Gross from the fifth movie\nmovie_ratings[[1]][[5]]$Gross","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1cde150-37e9-4725-b801-cbef89b1018a","_uuid":"fa3a00c534561a5f96994b5f7b5be3380c3368af"},"cell_type":"markdown","source":"At this point, you may be wondering *why can't we just get the Gross value for all the movies in our list and then convert it to a column in a dataframe*? The reason is that, in this case, not every movie has a Gross value recorded for it. (Remember that, for hierarchical data, unlike tabular data, there's no requirement that every value is recorded for every observation, so it's possible that you might have different variables for different observations.) We can see this by looping through the first three movies in our list and trying to get the value for \"Gross\" for each of them."},{"metadata":{"_cell_guid":"cb02cf5b-7387-4584-bb2b-2d6ae73d0cf7","_uuid":"3edd51973985e03e29d0ef7a8fb76fc2895845ed","collapsed":true,"trusted":true},"cell_type":"code","source":"# print the value for the Gross key for the first 3 movies\nfor(i in 1:3){\n    print(movie_ratings[[1]][[i]]$Gross)\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2231b34-c49e-42e2-868c-a29652df9909","_uuid":"b80e739b7e40f9a30703b839ace1b46db55f2225"},"cell_type":"markdown","source":"We can see from the output that there were recorded values for \"Gross\" for the first and second movie (even though the value was \"unknown\" for the second movie). The NULL result, however, tells us that there was no \"Gross\" information for the third movie at all! If you look through some of the other movies, you can see that there is a lot of information that isn't recorded for every movie, and it's not entirely clear why this is the case. (We'll talk about some strategies for handling missing information tomorrow!)\n\nIf we did want to convert this to a tabular data structure, we'd need to have a column for every single different value recorded for each observation. This might lead to having a very large data frame that's mostly empty! We'd also need to figure out how to handle the observations where no value was recorded. It's much simpler to represent this dataset with a hierarchical data structure of nested lists instead."},{"metadata":{"_cell_guid":"f471d437-413f-4db3-b59c-b3e2e61803c4","_uuid":"aaef91ceb88d1b082f9eee1113de39770528b7a1"},"cell_type":"markdown","source":"## Your turn!\n\nRead in the file `2018-2-8.json` from the \"Ratings vs Gross\" dataset . Check out its structure and see if the same information is recorded for each observation.\n\nFor an extra challenge, you can try reading in the .json file using the `read_json` file with the argument `simplifyVector = T`. This will attempt to change your list into a dataframe. What is the resulting data structure? Does it look the way that you expected? "},{"metadata":{"_cell_guid":"51651421-c8e7-4c49-bb4f-5fe42fa9e621","_uuid":"19394f6261470f9f35918adfbebd38293e96d84c","scrolled":true,"trusted":true},"cell_type":"code","source":"# your code goes here :)\nimport json\n\n\nwith open(\"../input/rating-vs-gross-collector/2018-2-4.json\",'r') as load_f:\n    load_dict = json.load(load_f)\n    \n    print(load_dict)\n    type(load_dict)\n    load_dict\n    #print(load_dict[[1]])\n    #print(load_dict[['12 Strong']])","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"812e8560-3ff5-43c1-985f-51093012ee33","_uuid":"6a1820707be59f211abd8260b13bc1cb2a20706a"},"cell_type":"markdown","source":"# Txt files\n___\n\nThe first thing I'd recommend doing with a .txt file is to open it to make sure it's actually raw text. If the file's on Kaggle, you can just look at the Data tab for that dataset ([like this one](https://www.kaggle.com/rtatman/character-encoding-examples/data)) to see what it looks like. If you're working locally, you can try opening it in a text editor ([like Gedit](https://github.com/GNOME/gedit)) or printing a few lines of the file to your console using `head`. Looking at part of your file will tell you if you're really working with raw text, or something else that's just been saved with a .txt file extension.\n\nOnce you've verified that, yes, you actually do have raw text, the next step is to read it into R. I personally prefer to read text files in line-by-line, partly because this is usually what I do in Python and I like to keep my workflows in the two language as parallel as possible. To read in files by lines, I use the `read_lines`function from the tidyverse collection of packages. When I'm reading in a file for the first time, I generally just read in the first few lines to make sure everything works. (This makes sure I don't have to wait for a huge file to load into memory only to find out that the first line is corrupted!)\n\nFor this example, I'm reading in a text file of stop words (short, common words commonly removed during text preprocessing) from Afrikaans. "},{"metadata":{"_cell_guid":"449c89bc-d0c3-41df-8937-a24195bdcf04","_uuid":"20dc957707daf400d9dc322e08dfd4025347e0fb","collapsed":true,"trusted":true},"cell_type":"code","source":"# Read in the first 10 lines from the file af.txt\naf_stopwords <- read_lines(\"../input/stopword-lists-for-african-languages/af.txt\", n_max = 10)\n\n# check out the structure of the data we've read in\nstr(af_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73e03e17-f28c-4f12-a48d-0481123f64f6","_uuid":"df09252c4dc8272a8bd4d995be6f443917ef4d0e"},"cell_type":"markdown","source":"As you can see, our data has been read in as a character vector, with each line as a separate item in that vector. We can double check that our data has read in correctly by printing it out:"},{"metadata":{"_cell_guid":"6e43a5e8-4038-4498-aa29-6d90ed60ce6e","_uuid":"486dc65f41065fb1cf1ed932d1c8aa16293ad19b","collapsed":true,"trusted":true},"cell_type":"code","source":"# print out all our words\naf_stopwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0dbebe97-81ce-4ead-9944-bc5bc3d5311c","_uuid":"7dda0287f0990f0fccd22535801f10a22140f3e0"},"cell_type":"markdown","source":"And that's it for reading in raw text data! As you can see, its less fiddly tham .json data because you don't need to worry about maintaining the structure of the data you're reading in."},{"metadata":{"_cell_guid":"8c04c6a2-b31e-4c91-b3c5-03e2f22bec85","_uuid":"267629245162387ab17af67ad44377af748ff6d8"},"cell_type":"markdown","source":"## Your turn!\n\nRead in all the words from the isiZulu stoplist (zu.txt) and make sure it looks good to you. \n\nFor an extra challenge, you can try alphabetizing the list of stop words. (They're currently sorted by how frequently they occur in the language.)"},{"metadata":{"_cell_guid":"47e4d327-7aa2-4dfd-9f16-f74d6aec973e","_uuid":"f29b41ee8920f47b2e5c7c6d1f72d25f74dce72c","trusted":true},"cell_type":"code","source":"# your code goes here :)\nf = open(\"../input/stopword-lists-for-african-languages/zu.txt\")\nprint(f.read())\nf.close()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"30927541-54cb-44ad-aa3d-6bef0f1a0067","_uuid":"199b4557bda65a7fa1bf543f9152791161de3c70"},"cell_type":"markdown","source":"# Xlsx files\n___\n\n\nFinally, we come to .xlsx or .xls files. These are proprietary file formats generated by Microsoft Excel, the popular spreadsheet editing software. Of the file formats we've talked about so far, these can be the easiest to read into R. Why? Because they store data in a tabular way, and R is really excellent at handling tabular data.\n\n> **When will you run into trouble with .xls files?** In general, the prettier the original spreadsheet looks, the more information you'll lose when you read it into R. In particular, files with multiple spreadsheets in different tabs, lots of color coding and empty cells separating different subtables are more difficult to read into R. \n\nTo read in .xslx files, I like the `read_excel` function from the `readxl` package. It works just like `read.csv` or `read_csv`. To test it out, let's read in an .xls file from a dataset of air quality measures:"},{"metadata":{"_cell_guid":"ed097b0a-eb3f-4250-8634-6c1caa57a2c7","_uuid":"bfe6c8085e0449dd66049437bf9751fa77ceba31","collapsed":true,"trusted":true},"cell_type":"code","source":"# read in our .xls file\nair_quality_data <- read_excel(\"../input/air-quality-data-earlwood-nsw-australia/Earlwood_Air_Data_17_18.xls\")\n\n# check out its structure\nstr(air_quality_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3e8631b-e4cc-4274-b3ff-357febcabc52","_uuid":"ec762d45ee3be9abd9dad4bafdf9de8722e9bd14"},"cell_type":"markdown","source":"Looks good so far! Let's just double-check that our data looks reasonable by checking out the first few rows of the dataframe. "},{"metadata":{"_cell_guid":"caea4074-063e-46c1-a5a0-fba8d83c13e6","_uuid":"73e8fcd5eff432a0ea3e548ddd2e4f3ae65c6bec","collapsed":true,"trusted":true},"cell_type":"code","source":"head(air_quality_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e9fbc75-c0ef-4009-a5e3-aa817153f2b0","_uuid":"684a1450c064927c4a6da28262abf31d8c3dc7b4"},"cell_type":"markdown","source":"Yep, that still looks good to me! Now it's  your turn to try reading in an Excel file. "},{"metadata":{"_cell_guid":"41d5ac3a-3ba2-4869-b81f-e72e11bf1918","_uuid":"1c94b3037df42687db8193628964e8c032a23f64"},"cell_type":"markdown","source":"## Your turn!\n\n`read_excel` works for both .xlsx & .xls files. Read in the `njs2016_dd_en.xlsx` file from the Canada National Justice Survey 2016 dataset using read_excel. Check its structure and some of the data to make sure it looks the way you expect."},{"metadata":{"_cell_guid":"332c9a88-8606-4b46-b65a-133ea68604fe","_uuid":"5368dbcf12e54ba31bb89b5cee656a266028a38e","trusted":true},"cell_type":"code","source":"# your code goes here\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport openpyxl\nwb=openpyxl.load_workbook('../input/national-justice-survey-2016/njs2016_dd_en.xlsx')  #打开excel文件\nprint(wb.get_sheet_names())  #获取工作簿所有工作表名\n\nsheet=wb.get_sheet_by_name('Sheet1')  #获取工作表\nprint(sheet.title) \n\nsheet02=wb.get_active_sheet()  #获取活动的工作表\nprint(sheet02.title)\nsheets = wb.get_sheet_names()         #从名称获取sheet  \nbooksheet = wb.get_sheet_by_name(sheets[0])  \n  \nrows = booksheet.rows  \ncolumns = booksheet.columns  \n#迭代所有的行  \nfor row in rows:  \n    line = [col.value for col in row] \n    print(line)\n  \n#通过坐标读取值  \ncell_11 = booksheet.cell('A1').value  \ncell_11 = booksheet.cell(row=1, column=1).value  \n'''\n\nimport numpy as np\nimport pandas as pd\nimport os\n# helpful character encoding module\nimport chardet\n\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/national-justice-survey-2016\"))\n# look at the first ten thousand bytes to guess the character encoding\nwith open(\"../input/national-justice-survey-2016/njs2016_dd_en.xlsx\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(720))\n\n# check what the character encoding might be\nprint(result)\n#train_df = pd.read_csv(\"../input/national-justice-survey-2016/njs2016_dd_en.xlsx\",encoding='ISO-8859-1')\n#train_df = pd.read_csv(\"../input/national-justice-survey-2016/njs2016_dd_en.xlsx\",encoding='Windows-1253')\n#train_df = pd.read_csv(\"../input/national-justice-survey-2016/njs2016_dd_en.xlsx\", encoding=\"ISO-8859-1\")\n#train_df = pd.read_csv(\"../input/national-justice-survey-2016/njs2016_dd_en.xlsx\", encoding=\"ISO-8859-1\")\n'''","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"d16db609-710b-43d4-8fcd-d0495ec4e9a7","_uuid":"c443db5e96c9904078d21c31aaee522a137c4718"},"cell_type":"markdown","source":"# And that's it for Day 1!\n\n___\n\nAnd that's it for today! If you have any questions, be sure to post them in the comments below or [on the forums](https://www.kaggle.com/questions-and-answers).\n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also lets you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\"."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}