{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport os # accessing directory structure\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, concatenate\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras import regularizers\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! conda install -y gdown\nimport gdown\n!gdown https://drive.google.com/file/d/1ozuMHtM1PXj-csgcr8dh6p_oiRhyLcjO/view?usp=sharing\n\nurl = 'https://drive.google.com/file/d/1ozuMHtM1PXj-csgcr8dh6p_oiRhyLcjO/view?usp=sharing'\n\n# output = 'glove.6B.100d.txt'\n\ngdown.download(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df1.columns)\ndf1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df1[df1.columns[0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all string to lower case\ndf1 = df1.apply(lambda x: x.astype(str).str.lower())\n\n# Removing Punctuations\ndf1.tweet = df1.tweet.str.replace('[^\\s\\w]','')\n\n# Removing HTML Tags\ndf1.tweet = df1.tweet.str.replace('[^\\s\\w]','')\n\n# Tokenizing\nnltk.download('punkt')\n\ndf1['tweet_token'] = df1['tweet'].apply(lambda x: word_tokenize(x))\n\n# Stemming\nps = PorterStemmer() \n\ndf1.tweet = df1.tweet_token.apply(lambda x: list(ps.stem(i) for i in x))\n\n# Removing the stop words and Rejoining \nnltk.download('stopwords')\nstops = set(stopwords.words(\"english\"))                  \n\ndf1.tweet = df1.tweet.apply(lambda x: ' '.join(list(i for i in x if i not in stops)))\n\n# Lammatizing\nnltk.download('wordnet')\nlamatizer = WordNetLemmatizer()\n\ndf1.tweet.apply(lambda x: lamatizer.lemmatize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 4500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = df1.tweet)\nX1 = tokenizer.texts_to_sequences(texts = df1.hate_speech)\nX2 = tokenizer.texts_to_sequences(texts = df1.offensive_language)\nX3 = tokenizer.texts_to_sequences(texts = df1.neither)\nX4 = tokenizer.texts_to_sequences(texts = df1.tweet)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pad_sequences(sequences= X1 , maxlen = 1000)\nclass1 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata2 = pad_sequences(sequences= X2 , maxlen = 1000)\nclass2 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata3 = pad_sequences(sequences= X3 , maxlen = 1000)\nclass3 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\ndata4 = pad_sequences(sequences= X4 , maxlen = 1000)\nclass4 = to_categorical(np.asarray(df1['class']), num_classes = 3)\n\nprint('Length of data1 tensor:', data1.shape)\nprint('Length of labels1 tensor:', class1.shape)\nprint('Length of data1 tensor:', data2.shape)\nprint('Length of labels1 tensor:', class2.shape)\nprint('Length of data1 tensor:', data3.shape)\nprint('Length of labels1 tensor:', class3.shape)\nprint('Length of data1 tensor:', data4.shape)\nprint('Length of labels1 tensor:', class4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices1 = np.arange(df1.shape[0])\nnp.random.shuffle(indices1)\ndata1 = data1[indices1]\nclass1 = class1[indices1]\nx_train1, x_test1, y_train1, y_test1 = train_test_split(data1, class1, test_size=0.2, random_state=42)\nx_test1, x_val1, y_test1, y_val1 = train_test_split(data1, class1, test_size=0.4, random_state=42)\n\n# data2 = data2[indices1]\n# class2 = class2[indices1]\n# x_train2, x_test2, y_train2, y_test2 = train_test_split(data2, class2, test_size=0.2, random_state=42)\n# x_test2, x_val2, y_test2, y_val2 = train_test_split(data2, class2, test_size=0.4, random_state=42)\n\n# dat3 = data3[indices1]\n# class3 = class3[indices1]\n# x_train3, x_test3, y_train3, y_test3 = train_test_split(data3, class3, test_size=0.2, random_state=42)\n# x_test3, x_val3, y_test3, y_val3 = train_test_split(data3, class3, test_size=0.4, random_state=42)\n\n# data3 = data3[indices1]\n# class3 = class3[indices1]\n# x_train3, x_test3, y_train3, y_test3 = train_test_split(data3, class3, test_size=0.2, random_state=42)\n# x_test3, x_val1, y_test3, y_val3 = train_test_split(data3, class3, test_size=0.4, random_state=42)\n\nprint('1:')\nprint(x_train1.shape)\nprint(y_train1.shape)\nprint(x_test1.shape)\nprint(y_test1.shape)\nprint(x_val1.shape)\nprint(y_val1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pre-trained word embeddings\nMAX_SEQUENCE_LENGTH = 1000\nGLOVE_DIR = \"../input/glove-global-vectors-for-word-representation/\" \nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Total %s word vectors in Glove.' % len(embeddings_index))\n\nembedding_matrix = np.random.random((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nembedding_layer = Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep CNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Model for 1st data set (test_data1)\ndef DCNN_model(n, x_train, y_train, x_val, y_val, x_test, y_test):\n    sequence_input = Input(shape=(1000,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    l_cov1= Conv1D(8, 5, activation='relu')(embedded_sequences)\n    l_pool1 = MaxPooling1D(5)(l_cov1)\n    l_cov2 = Conv1D(8, 5, activation='relu')(l_pool1)\n    l_pool2 = MaxPooling1D(5)(l_cov2)\n    l_cov3 = Conv1D(8, 5, activation='relu')(l_pool2)\n    l_pool3 = MaxPooling1D(35)(l_cov3)\n    l_flat = Flatten()(l_pool3)\n    l_dense = Dense(8, activation='relu')(l_flat)\n    l_dense1 = Dense(8, activation='relu')(l_dense)\n    l_dense2 = Dense(8, activation='relu')(l_dense1)\n    preds = Dense(3, activation='softmax')(l_dense2)\n\n    dcnn_model = Model(sequence_input, preds)\n    dcnn_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['acc'])\n    print(\"Fitting the simple convolutional neural network model\")\n    dcnn_model.summary()\n    history = dcnn_model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=8)\n    print('\\nModel Training Completed !')\n\n    ### PREDICTING \n    y_preds = dcnn_model.predict(x_test)\n    y_pred = np.round(y_preds)\n    cpred = float(sum(y_pred == y_test)[0])\n    cm = confusion_matrix(y_test.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test))\n    print(\"\\n-> Accuracy of model: \", cpred/float(len(y_test)))\n    print(\"\\n-> Confusion matrix for Dataset\",n,\": \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return history\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"history = DCNN_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1,\n                     x_train2, y_train2, x_val2, y_val2, x_test2, y_test2,\n                     x_train3, y_train3, x_val3, y_val3, x_test3, y_test3,\n                     x_train4, y_train4, x_val4, y_val4, x_test4, y_test4)\nprint(' ')\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi Channel CNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1):\n    \n    sequence_input1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences1 = embedding_layer(sequence_input1)\n    cov1= Conv1D(32, 5, activation='relu')(embedded_sequences1)\n    pool1 = MaxPooling1D(5)(cov1)\n    flat1 = Flatten()(pool1)\n\n    sequence_input2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences2 = embedding_layer(sequence_input2)\n    cov2 = Conv1D(32, 5, activation='relu')(embedded_sequences2)\n    pool2 = MaxPooling1D(5)(cov2)\n    flat2 = Flatten()(pool2)\n\n    sequence_input3 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences3 = embedding_layer(sequence_input3)\n    cov3 = Conv1D(32, 5, activation='relu')(embedded_sequences3)\n    pool3 = MaxPooling1D(35)(cov3)\n    flat3 = Flatten()(pool3)\n    \n#     sequence_input4 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n#     embedded_sequences4 = embedding_layer(sequence_input4)\n#     cov4 = Conv1D(32, 5, activation='relu')(embedded_sequences4)\n#     pool4 = MaxPooling1D(35)(cov4)\n#     flat4 = Flatten()(pool4)\n\n    merge = concatenate([flat1, flat2, flat3])\n\n    # flat4 = Flatten()(merge)\n    dense = Dense(32, activation='relu')(merge)\n    preds = Dense(3, activation='softmax')(dense)\n\n    model = Model(inputs = [sequence_input1, sequence_input2, sequence_input3], outputs = preds)\n    model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['acc'])\n    print(\"Fitting the simple convolutional neural network model\")\n    model.summary()\n    # history = model.fit([x_train, x_train, x_train], y_train, epochs=2, batch_size=32)\n    history = model.fit([x_train1, x_train2, x_train3], y_train1,validation_data = ([x_val1, x_val1, x_val1], y_val1), epochs=10, batch_size=32)\n    print('\\nModel Training Completed !')\n\n    ### PREDICTION\n    y_pred = np.round(model.predict([x_test1, x_test1, x_test1]))\n    cpred = float(sum(y_pred == y_test1)[0])\n    cm = confusion_matrix(y_test1.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test1))\n    print(\"\\n-> Accuracy of model: \", cpred/float(len(y_test1)))\n    print(\"\\n-> Confusion Matrix: \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = define_model(x_train1, y_train1, x_val1, y_val1, x_test1, y_test1)\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Model for 1st data set (test_data1)\ndef LSTM_model(n, x_train, y_train, x_val, y_val, x_test, y_test):\n    lstm_model = Sequential()\n    lstm_model.add(Embedding(len(word_index) + 1, 100, weights = [embedding_matrix], input_length = MAX_SEQUENCE_LENGTH, trainable = False))\n    lstm_model.add(LSTM(128))\n    lstm_model.add(Dense(128, activation = 'relu'))\n    lstm_model.add(Dense(64, activation = 'relu'))\n    lstm_model.add(Dense(32, activation = 'relu'))\n    lstm_model.add(Dense(3, activation = 'softmax'))\n    lstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    lstm_model.summary()\n    history = lstm_model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 15, batch_size = 128)\n    print('\\nModel Training Complete !')\n    \n    ### PREDICTION\n    y_preds = lstm_model.predict(x_test)\n    y_pred = np.round(y_preds)\n    cpred = float(sum(y_pred == y_test)[0])\n    cm = confusion_matrix(y_test.argmax(1), y_pred.argmax(1))\n    print(\"\\n-> Correct predictions:\", cpred)\n    print(\"\\n-> Total number of test examples:\", len(y_test))\n    print(\"\\n-> Accuracy of model: \", cpred/float(len(y_test)))\n    print(\"\\n-> Confusion for Dataset\",n,\": \", cm)\n\n    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n    plt.title('Confusion matrix - CNN Model 1')\n    plt.colorbar()\n    plt.ylabel('Expected label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = LSTM_model(1, x_train1, y_train1, x_val1, y_val1, x_test1, y_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}