{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom math import ceil\nfrom sklearn.preprocessing import scale\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51beb95d548f1a2028681925b665ca2f4fe1dac4"},"cell_type":"code","source":"!mkdir logdir\n!mkdir ckpts","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def tf_namespace(namespace):\n    def wrapper(f):\n        def wrapped_f(*args, **kwargs):\n            with tf.name_scope(namespace):\n                return f(*args, **kwargs)\n\n        return wrapped_f\n\n    return wrapper\n\n\nclass ImgSaver:\n\n    def __init__(self, n, w, h, prefix=None):\n        self.w = w\n        self.h = h\n        self.n = n\n        self.prefix = prefix or 'img'\n\n    def set_vae(self, vae):\n        self.vae = vae\n\n    def __call__(self, epoch=None):\n        samples = self.vae.generate(self.n).reshape(-1, self.w, self.h)\n        suffix = '' if epoch is None else f'_epoch_{epoch}'\n        for i in range(self.n):\n            sample = samples[i, :, :]\n            plt.imsave(f'{self.prefix}_{i}' + suffix + '.jpg', sample, cmap='gray')\n\n@tf_namespace('psigmoid')\ndef psigmoid(x, beta=None, alpha=None, beta_start=1., alpha_start=0.):\n    if beta is None:\n        beta = tf.Variable(beta_start, name='beta', dtype=tf.float32)\n    if alpha is None:\n        alpha = tf.Variable(alpha_start, name='alpha', dtype=tf.float32)\n    exponent = (x + alpha) * beta\n    e_x = tf.exp(exponent)\n    return tf.divide(e_x,(e_x+1.), 'psigmoid')\n\n\nclass VAE:\n\n    def __init__(self, input_shape, encode_sizes, latent_size, decode_sizes=None, mu_prior=None, sigma_prior=None, lr=10e-4,\n                 momentum=0.9, n_saver=20, prefix_imsaver=None, model_name=None):\n        self.encode_sizes = encode_sizes\n        self.latent_size = latent_size\n        self.decode_sizes = decode_sizes or encode_sizes[::-1]\n        self.model_name = model_name or 'mikedev_vae'\n        self.mu_prior = mu_prior or np.zeros([latent_size], dtype='float32')\n        self.sigma_prior = sigma_prior or np.ones([latent_size], 'float32')\n        self.lr = lr\n        self.momentum = momentum\n        self.input_shape = input_shape\n        self._build_graph(input_shape, latent_size)\n        self.imsaver = ImgSaver(20, 28, 28, prefix_imsaver)\n\n    def _build_graph(self, input_shape, latent_size):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self._create_placeholders(input_shape)\n            self._create_encoder(self.X)\n            self._create_latent_distribution(self.encoder, latent_size)\n            self._create_decoder(self.z)\n            self.loss = - self.elbo(self.X, self.decoder_logits, self.mu, self.log_sigma_square, self.sigma_square,\n                                    tf.constant(self.mu_prior), tf.constant(self.sigma_prior))\n            self.opt = tf.train.AdamOptimizer(self.lr, self.momentum)\n            self.opt_op = self.opt.minimize(self.loss)\n            self.session = tf.InteractiveSession(graph=self.graph)\n        writer = tf.summary.FileWriter(logdir='logdir', graph=self.graph)\n        writer.flush()\n\n    @property\n    def k_init(self):\n        return {'kernel_initializer': tf.glorot_uniform_initializer()}\n\n    def elbo(self, X_true, X_logits, mu, log_sigma, sigma, mu_prior, sigma_prior):\n        epsilon = tf.constant(0.000001)\n        mae = tf.reduce_sum(tf.losses.sigmoid_cross_entropy(X_true, X_logits, reduction=tf.losses.Reduction.NONE), axis=1)\n        log_sigma_prior = tf.log(sigma_prior + epsilon)\n        mu_diff = mu - mu_prior\n        self.kl = 0.5 * tf.reduce_sum(log_sigma_prior - log_sigma - 1 +\n                                      (sigma + tf.multiply(mu_diff, mu_diff)) / sigma_prior, axis=1)\n        return  tf.reduce_mean( - mae - self.kl)\n\n    @tf_namespace('placeholders')\n    def _create_placeholders(self, input_shape):\n        self.X = tf.placeholder(tf.float32, shape=[None, *input_shape], name='X')\n\n    @tf_namespace('encoder')\n    def _create_encoder(self, X):\n        self.encode_layers = []\n        self.encoder = X\n        self.encoder = tf.reshape(self.encoder, [tf.shape(self.encoder)[0], 28, 28, 1])\n        for i, conv_size in enumerate(self.encode_sizes):\n            self.encoder = tf.layers.conv2d(self.encoder, *conv_size, **self.k_init,\n                                           activation=tf.nn.relu, name=f'encoder_{i+1}')\n            self.encode_layers.append(self.encoder)\n            setattr(self, f'encoder_{i + 1}', self.encoder)\n\n    @tf_namespace('latent')\n    def _create_latent_distribution(self, encoder, latent_dim):\n        self.flatter_layer = tf.layers.flatten(encoder)\n        self.mu = tf.layers.dense(self.flatter_layer, latent_dim, **self.k_init, name='mu')\n        self.log_sigma_square = tf.layers.dense(self.flatter_layer, latent_dim,\n                                                **self.k_init, name='log_sigma_square')\n        self.sigma_square = tf.exp(self.log_sigma_square, 'sigma_square')\n        self.z = tf.add(self.mu, self.sigma_square * tf.random.normal(tf.shape(self.sigma_square)), 'z')\n\n    @tf_namespace('decoder')\n    def _create_decoder(self, z):\n        self.decoder = z\n        self.decode_layers = []\n        for i, conv_size in enumerate(self.decode_sizes):\n            self.decoder = tf.layers.dense(self.decoder, conv_size, **self.k_init,\n                                           activation=tf.nn.relu, name=f'decoder_{i+1}')\n            setattr(self, f'decoder_{i + 1}', self.decoder)\n            self.decode_layers.append(self.decoder)\n            if i == len(self.decode_sizes)-1:\n                self.decoder = tf.layers.dense(self.decoder, self.input_shape[0],\n                                               **self.k_init, name=f'decoder_{i+2}')\n                self.decoder_logits = self.decoder\n                self.decoder = psigmoid(self.decoder, beta=3., alpha=0.)\n                setattr(self, f'decoder_{i+2}', self.decoder)\n                self.decode_layers.append(self.decoder)\n        return self.decoder\n\n    @property\n    def layers(self):\n        return [(f'encoder_{i}', getattr(self, f'encoder_{i}')) for i in range(1, len(self.encode_layers) + 1)] + \\\n               [('flatten', self.flatter_layer), ('mu', self.mu), ('sigma', self.log_sigma_square), ('z', self.z)] + \\\n               [(f'decoder_{i}', getattr(self, f'decoder_{i}')) for i in range(1, len(self.decode_layers) + 1)]\n\n    def fit(self, X, epochs, batch_size, print_every=50, save_every=10):\n        if self.imsaver is not None:\n            self.imsaver.set_vae(self)\n        n_batch = ceil(X.shape[0] / batch_size)\n        saver = tf.train.Saver()\n        self.session.run(tf.global_variables_initializer())\n        self.history = []\n        for epoch in range(1, epochs + 1):\n            np.random.shuffle(X)\n            acc_loss = 0\n            counter = 0\n            epoch_hs = []\n            for i in range(n_batch):\n                slice_batch = slice(i * batch_size, (i + 1) * batch_size) if i != n_batch - 1 else slice(\n                    i * batch_size,\n                    None)\n                X_batch = X[slice_batch, :]\n                batch_loss, _ = self.session.run([self.loss, self.opt_op], {self.X: X_batch})\n                acc_loss += batch_loss\n                epoch_hs.append(batch_loss)\n                if counter % print_every == 0:\n                    print(f\" Epoch {epoch} - batch {i} - neg_ELBO = {batch_loss}\")\n                counter += 1\n            print(f'\\nEpoch {epoch} - Avg loss = {acc_loss / n_batch}')\n            print('\\n' + ('-' * 70))\n            self.history.append(epoch_hs)\n            saver.save(self.session, f\"ckpts/{self.model_name}.ckpt\")\n            if self.imsaver is not None and epoch % save_every == 0:\n                self.imsaver(epoch)\n\n    def generate(self, n=1, mu_prior=None, sigma_prior=None):\n        if mu_prior is None:\n            mu_prior = self.mu_prior\n        if sigma_prior is None:\n            sigma_prior = self.sigma_prior\n        z = np.random.multivariate_normal(mu_prior, np.diag(sigma_prior), [n])\n        return self.session.run(self.decoder, feed_dict={self.z: z})\n\n    def reconstruct(self, X):\n        return self.session.run(self.decoder, feed_dict={self.X: X})\n\n    def open(self):\n        if not hasattr(self, 'session') or self.session is None:\n            if self.graph is None:\n                self._build_graph(self.input_shape, self.latent_size)\n            else:\n                self.session = tf.InteractiveSession(graph=self.graph)\n\n    def close(self):\n        if hasattr(VAE, 'session') and VAE.session is not None:\n            VAE.session.close()\n            VAE.session = None\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def __delete__(self, instance):\n        self.close()\n\n    def __setattr__(self, key, value):\n        if key == 'session':\n            if hasattr(self, 'session') and self.session is not None:\n                self.session.close()\n            elif hasattr(VAE, 'session') and VAE.session is not None:\n                VAE.session.close()\n            VAE.session = value\n        else:\n            self.__dict__[key] = value\n\n    def __delattr__(self, item):\n        if item == 'session':\n            self.close()\n            del VAE.__dict__['session']\n        else:\n            del self.__dict__[item]\n\n    def __enter__(self):\n        self.open()\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ca6d92b9a1993beb018f7d853767df593dfcc25"},"cell_type":"code","source":"def plot_generated_images(mikedev_vae, n = 15):\n    X_new = mikedev_vae.generate(n).reshape((n, 28, 28))\n    for i in range(n):\n        plt.imshow(X_new[i, :, :], 'gray')\n        plt.imsave(f'pic_{i}_epoch_100.jpg', X_new[i, :, :], cmap='gray')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"555c367ac664ac9defab9617a3b3b2d694c4ce64"},"cell_type":"code","source":"def plot_reconstructed_images(X_train, mikedev_vae, n=15):\n    ix = np.random.randint(0, X_train.shape[0], n)\n    X_orig = X_train[ix, :]\n    X_new = mikedev_vae.reconstruct(X_orig).reshape((n, 28, 28))\n    for i in range(n):\n        fig, axs= plt.subplots(1, 2)\n        ax1, ax2 = axs\n        ax1.imshow(X_orig[i, :].reshape((28, 28)), 'gray')\n        ax2.imshow(X_new[i, :, :], 'gray')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57270652b2e7bd431ed0c5a0ea84881a6d330997"},"cell_type":"markdown","source":"## Character mnist"},{"metadata":{"trusted":true,"_uuid":"baaad6311771ce3e390d86c7e32990b065c4c580"},"cell_type":"code","source":"train = pd.read_csv('../input/handwritten-az/handwritten_data_785.csv')\ntrain.columns = ['label'] + [f'pixel_{i}' for i in range(1, 785)]\ntrain = train.sample(100_000, random_state=13)\nX_train, y_train = train.drop(columns='label').values, train['label'].values\nX_train = X_train.astype('float32')\nX_train = X_train / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a27d0afaca9e0197d2ebf0f296c07fee9c8f525"},"cell_type":"code","source":"encoder = [(8, 3), (16,5), (32,5), (32,5)]\ndecoder = [57, 124, 353, 674]\nwith VAE((784,), encoder, 2, decoder, prefix_imsaver='char', model_name='char_vae') as mikedev_vae:\n    mikedev_vae.fit(X_train, epochs=100, batch_size=256, print_every=100000)\n    with open('char_history_training', 'w') as f:\n        f.write(str(mikedev_vae.history))\n    plot_generated_images(mikedev_vae)\n    plot_reconstructed_images(X_train, mikedev_vae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a9d37419674f4c493b005c62e042187d1a2baf3"},"cell_type":"markdown","source":"## Fashion MNIST"},{"metadata":{"trusted":true,"_uuid":"6d757ea36e8a432424ea31146f4099463121907f"},"cell_type":"code","source":"train = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\nX_train, y_train = train.drop(columns='label').values, train['label'].values\nX_train = X_train.astype('float32')\nX_train = X_train / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"005ba83092cb2b43631f50184f4ec8fbca67643a"},"cell_type":"code","source":"encoder = [(8, 3), (16,5), (32,5), (32,5)]\ndecoder = [57, 124, 353, 674]\nwith VAE((784,), encoder, 2, decoder, prefix_imsaver='fashion', model_name='fashion_vae') as mikedev_vae:\n    mikedev_vae.fit(X_train, epochs=100, batch_size=256, print_every=100000)\n    with open('fashion_history_training', 'w') as f:\n        f.write(str(mikedev_vae.history))\n    plot_generated_images(mikedev_vae)\n    plot_reconstructed_images(X_train, mikedev_vae)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}