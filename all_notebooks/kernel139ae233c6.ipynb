{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center\">CLASSIFICATION OF BREAST CANCER CELLS</h1>\n\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">Cancer is one of the most dangerous diseases in today’s society. Worldwide, 18.1 million people is diagnosed with having cancer and 9.6 million died because of cancer in 2018. One of every 5 men and one of every 6 women are have been diagnosed with having cancer during their lives. One of every 8 men and one of every 11 women die from cancer. There are various types of cancer and female breast cancer is a well-known type of cancer. It caused 627.000 deaths in 2018 which is approximately 6.6% of the total number of deaths caused by any type of cancer in 2018. However, it is relatively easier to detect breast cancer compared to other types of cancers, especially in more developed countries. https://www.who.int/cancer/PRGlobocanFinal.pdf</div>\n<br>\n<div style=\"text-align:justify;font-size:17px;\">This project aims to increase the prediction accuracy of the classification of the cancer cells for the corresponding patients, as benign or malignant. In order to detect the sickness of the cells various Machine Learning algorithms are used. In order to train and test the program a dataset is used. For obtaining the dataset, cells taken from the patients are observed by using the digitized images of the cells. Then, the features for data training are created based the different properties of the cell. In this project, the dataset is taken from Kaggle. https://www.kaggle.com/uciml/breast-cancer-wisconsin-data. In Kaggle, the data is preprocessed so there are suitable values for every label for each cell. This dataset has 569 cells and each cell has 30 features. 70% of the dataset is used for training the algorithms in order to get estimations and the 30% of the dataset are used for testing the accuracy of the program. </div>\n<br>\n<div style=\"text-align:justify;font-size:17px;\">The features of the cells are based on the grayscale images of them. There are 30 features and they are based on the properties of the cell; such as size of the cell where radius, perimeter and area are observed or the patterns on the cell where standard deviations in gray-scale values, variation is radius lengths and the general symmetry of the cell are measured or the shape of the cell where the concave points of the cell and proportions of radius and perimeter to each other are observed. The high number of the used features and the variance in the type of the features makes the data classification and detection of the cancer cells a lot easier.</div>\n<br>\n<div style=\"text-align:justify;font-size:17px;\">In this project, various concepts are used for evaluating the cells and increasing the accuracy of cancer detection. These concepts are Principal Component Analysis (PCA), Logistic Regression Classification, K-Nearest Neighbors (KNN) Classification, Support Vector Machine (SVM) Classification, Decision Tree Classification and Naïve Bayes Classification. Algorithms of these concepts will be explained in detail in the other parts of the code.</div>"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">Pre-processing</h3>\n<hr>\n<div style=\"text-align:justify;font-size:17px;\">First we import the dataset and then shuffle it as standard practice. Then the next step is to identify the labels and the features in the dataset. The labels are labelled as either 1 for malignant or 0 for benign. Similarly the features are extracted and their values are normalized from 0 to 1. </div>\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"######################## FATIH SEVBAN UYANIK ################################3\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.utils.fixes import signature\nimport seaborn as sns\nimport warnings\nimport torch.nn as nn\nimport torch\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n# getting the dataset and shuffling data.\ndfFeatures = pd.read_csv('../input/data.csv');\n\n# dividing the dataset to labels and features\ndfLabels = pd.DataFrame([1 if each == 'M' else 0 for each in dfFeatures.diagnosis], columns=[\"label\"])\ndfFeatures.drop(['id','diagnosis','Unnamed: 32'], axis = 1 ,inplace=True)\n\n# normalizing the dataset.\ndfFeatures = (dfFeatures - dfFeatures.min()) / (dfFeatures.max() - dfFeatures.min())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d66bcaf83293f1cc983862ca0fcda461d053f976"},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">Principal Component Analysis (PCA)</h3>\n<hr>\n<div style=\"text-align:justify;font-size:17px;\">The second part of the pre-processing step is to apply the PCA (principle component analysis) algorithm. This is a very powerful algorithm and is widely used to reduce the dimension size and variance of features in a dataset. It ranks the eigenvalues of a feature vector of the dataset, the corresponding eigenvector to the largest eigenvalue becomes the first principle component and so on. Are purpose was to construct one 2D and one 3D model so we used drew out the first, second and third principle components. To check the performance of these two models, we printed out the variances of each dimension and the sum of these variances. The variance sum for the 2D model was 0.7038117901347686 while for the 3D model it was 0.7749562100792776. The dimensional variances were [0.53097689 0.1728349 ] and [0.53097689 0.1728349  0.07114442]. Here we make the observation that the 3 principle component has very little variance so it is not as useful to us as the first two. Therefore, the 2D model should suffice  normally however we will also use the 3D model for added complexity. </div>"},{"metadata":{"trusted":true,"_uuid":"192cc09690b4950f0277ff5813ef1c6b4efd0faf"},"cell_type":"code","source":"# importing PCA\nfrom sklearn.decomposition import PCA\n\n# constructing 3D model.\npca_3D_model = PCA(n_components = 3, whiten = True)\npca_3D_model.fit(dfFeatures)\nnpFeatures_3D = pca_3D_model.transform(dfFeatures)\ndfFeatures_3D = pd.DataFrame(npFeatures_3D, columns=['param1', 'param2', 'param3'])\ndfLabels_3D = dfLabels.copy()\n\n# constructing 2D model.\npca_2D_model = PCA(n_components = 2, whiten = True)\npca_2D_model.fit(dfFeatures)\nnpFeatures_2D = pca_2D_model.transform(dfFeatures)\ndfFeatures_2D = pd.DataFrame(npFeatures_2D, columns=['param1', 'param2'])\ndfLabels_2D = dfLabels.copy()\n\n# printing out the variance ratio and sum for 3D\nprint('-------------------------------------------------------------')\nprint('Variance Ratio: ' + str(pca_3D_model.explained_variance_ratio_))\nprint('Variance Sum  : ' + str(sum(pca_3D_model.explained_variance_ratio_)))\nprint('-------------------------------------------------------------')\n\n# printing out the variance ratio and sum for 2D\nprint('-------------------------------------------------------------')\nprint('Variance Ratio for 2D: ' + str(pca_2D_model.explained_variance_ratio_))\nprint('Variance Sum for 2D : ' + str(sum(pca_2D_model.explained_variance_ratio_)))\nprint('-------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ecd00b481e3ec5021f7315cc4e6d443f7c22654"},"cell_type":"code","source":"# Scatter Plot \ndfTemp_2D = pd.concat([dfFeatures_2D, dfLabels_2D], axis=1)\nplt.figure(figsize=(15,15))\nplt.scatter(dfTemp_2D.param1[dfTemp_2D.label == 0], dfTemp_2D.param2[dfTemp_2D.label == 0], color='red')\nplt.scatter(dfTemp_2D.param1[dfTemp_2D.label == 1], dfTemp_2D.param2[dfTemp_2D.label == 1], color='green')\nplt.xlabel('Param1')\nplt.ylabel('Param2')\nplt.title('PCA for 2D')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"280c54cddccfd63d00c71c0fc96010f1af44474b"},"cell_type":"code","source":"from mpl_toolkits.mplot3d import axes3d\ndfTemp_3D = pd.concat([dfFeatures_3D, dfLabels_3D], axis=1)\n\nfig = plt.figure(figsize=(20,20))\nax = fig.add_subplot(111, projection='3d')\nx = dfTemp_3D.param1;\ny = dfTemp_3D.param2;\nz = dfTemp_3D.param3;\n\nax.scatter(x[dfTemp_3D.label == 0], y[dfTemp_3D.label == 0], z[dfTemp_3D.label == 0], c = 'g', marker = 'o', s=30)\nax.scatter(x[dfTemp_3D.label == 1], y[dfTemp_3D.label == 1], z[dfTemp_3D.label == 1], c = 'r', marker = 'o', s=30)\nax.set_xlabel('param1')\nax.set_ylabel('param2')\nax.set_zlabel('param3')\nplt.title('PCA for 3D')\nplt.show()\n######################## FATIH SEVBAN UYANIK ################################3\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">CORRELATION MATRIX</h3>\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"######################## HALIL ################################3\n\n# showing the correlations through a heatmap\ncorrelation_matrix = dfFeatures.corr()\ntop_correlated_features = correlation_matrix.index\nplt.figure(figsize=(20, 20))\ng=sns.heatmap(dfFeatures[top_correlated_features].corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">SELECTING BEST 10 FEATURES</h3>\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# Selecting best features with chi square statistical method\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(dfFeatures,dfLabels)\nx_train_selected2 = fit.transform(dfFeatures)\ndf_scores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(dfFeatures.columns)\n#concat two dataframes for better visualization\nfScores = pd.concat([dfcolumns,df_scores],axis=1)\nfScores.columns = ['Features','Score']\nprint(fScores.nlargest(10,'Score'))\n\n# getting the best 10 features as a dataframe.\ndfFeatures_10B = dfFeatures.iloc[:, [0, 2, 3, 6, 7, 20, 22, 23, 26, 27]].copy()\ndfLabels_10B = dfLabels.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6759c34eb19fc86fd685367c1fcf90c37eade631"},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">Splitting the Data to Train and Test Data</h3>\n<hr>\n<div style=\"text-align:justify;font-size:17px;\">For this part, the sklearn.model_selection library was imported for its train_test_split method to split the datasets into train features, train labels, test features and test labels. It has been applied to the the raw dataset, 2D PCA and 3D PCA. </div>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing the data.\nfrom sklearn.model_selection import train_test_split\n\n# splitting for initial data with 30 features\ndfTrainFeatures, dfTestFeatures, dfTrainLabels, dfTestLabels = train_test_split(dfFeatures, dfLabels, test_size = 0.3, random_state = 42) \n\n# splitting for PCA data that has 3 dimensions.\nfrom sklearn.model_selection import train_test_split\ndfTrainFeatures_3D, dfTestFeatures_3D, dfTrainLabels_3D, dfTestLabels_3D = train_test_split(dfFeatures_3D, dfLabels_3D, test_size = 0.3, random_state = 42)\n\n# splitting for PCA data that has 2 dimensions.\nfrom sklearn.model_selection import train_test_split\ndfTrainFeatures_2D, dfTestFeatures_2D, dfTrainLabels_2D, dfTestLabels_2D = train_test_split(dfFeatures_2D, dfLabels_2D, test_size = 0.3, random_state = 42)\n\n# splitting for PCA data that has the best 10 features\nfrom sklearn.model_selection import train_test_split\ndfTrainFeatures_10B, dfTestFeatures_10B, dfTrainLabels_10B, dfTestLabels_10B = train_test_split(dfFeatures_10B, dfLabels_10B, test_size = 0.3, random_state = 42)\n######################## HALIL ################################3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################## FATIH SEVBAN UYANIK ################################3\n\n# converting pandas data frames also to numpy arays\nnpTrainFeatures = dfTrainFeatures.values\nnpTestFeatures  = dfTestFeatures.values\nnpTrainLabels   = dfTrainLabels.values\nnpTestLabels    = dfTestLabels.values\n\nnpTrainFeatures_2D = dfTrainFeatures_2D.values\nnpTestFeatures_2D  = dfTestFeatures_2D.values\nnpTrainLabels_2D   = dfTrainLabels_2D.values\nnpTestLabels_2D    = dfTestLabels_2D.values\n\nnpTrainFeatures_3D = dfTrainFeatures_3D.values\nnpTestFeatures_3D  = dfTestFeatures_3D.values\nnpTrainLabels_3D   = dfTrainLabels_3D.values\nnpTestLabels_3D    = dfTestLabels_3D.values\n\nnpTrainFeatures_10B = dfTrainFeatures_10B.values\nnpTestFeatures_10B  = dfTestFeatures_10B.values\nnpTrainLabels_10B   = dfTrainLabels_10B.values\nnpTestLabels_10B    = dfTestLabels_10B.values\n\n# converting numpy arrays also to tensors\ntensorTrainFeatures = torch.tensor( npTrainFeatures )\ntensorTestFeatures  = torch.tensor( npTestFeatures  )\ntensorTrainLabels   = torch.tensor( npTrainLabels   )\ntensorTestLabels    = torch.tensor( npTestLabels    )\n\ntensorTrainFeatures_2D = torch.tensor( npTrainFeatures_2D )\ntensorTestFeatures_2D  = torch.tensor( npTestFeatures_2D  )\ntensorTrainLabels_2D   = torch.tensor( npTrainLabels_2D   )\ntensorTestLabels_2D    = torch.tensor( npTestLabels_2D    )\n\ntensorTrainFeatures_3D = torch.tensor( npTrainFeatures_3D )\ntensorTestFeatures_3D  = torch.tensor( npTestFeatures_3D  )\ntensorTrainLabels_3D   = torch.tensor( npTrainLabels_3D   )\ntensorTestLabels_3D    = torch.tensor( npTestLabels_3D    )\n\ntensorTrainFeatures_10B = torch.tensor( npTrainFeatures_10B )\ntensorTestFeatures_10B  = torch.tensor( npTestFeatures_10B  )\ntensorTrainLabels_10B   = torch.tensor( npTrainLabels_10B   )\ntensorTestLabels_10B    = torch.tensor( npTestLabels_10B    )\n\n######################## FATIH SEVBAN UYANIK ################################3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee73ae4cb73d479a5a1056f09f8ecd8b5ebab893"},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">LOGISTIC REGRESSION CLASSIFICATION</h3>\n<hr>\n<div style=\"text-align:justify;font-size:17px;\">Logistic regression (a misnomer) is a classification algorithm that fits data to a sigmoid function of the form  where  is the linear regression function  where  is the set of parameters and  is the feature set. This algorithm works best when the labels exhibit the most probability for a set of features above a certain threshold and vice versa. We used the LogisticRegression library from sklearn.linear_model. Using the just the initials data, we fitted the train features and labels to the model. And then used that same model to calculate the prediction accuracy on our test labels and features. We repeated this process for 2D PCA and 3D PCA.</div>"},{"metadata":{"trusted":true,"_uuid":"f4f69e894782ae8b45f9aad0d654a95dc38d124f"},"cell_type":"code","source":"################################################### AHMAD KHAN ###################################################\n\n# importing the model.\nfrom sklearn.linear_model import LogisticRegression\n\n# applying logistic regression to initial data.\nlogistic_regression_model = LogisticRegression()\nlogistic_regression_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_lr = logistic_regression_model.score(dfTestFeatures, dfTestLabels)\npredictions_lr = logistic_regression_model.predict(dfTestFeatures)\npredictions_lr_prob = logistic_regression_model.predict_proba(dfTestFeatures)[:,1]\nmacro_precision_lr, macro_recall_lr, macro_fscore_lr, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr, average='macro')\nmicro_precision_lr, micro_recall_lr, micro_fscore_lr, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr, average='micro')\n\n\n# applying logistic regression to PCA data that has 3 dimensions.\nlogistic_regression_model_3D = LogisticRegression()\nlogistic_regression_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacurracy_3D_lr = logistic_regression_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_lr_3D = logistic_regression_model_3D.predict(dfTestFeatures_3D)\npredictions_lr_3D_prob = logistic_regression_model_3D.predict_proba(dfTestFeatures_3D)[:,1]\nmacro_precision_lr_3D, macro_recall_lr_3D, macro_fscore_lr_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_3D, average='macro')\nmicro_precision_lr_3D, micro_recall_lr_3D, micro_fscore_lr_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_3D, average='micro')\n\n\n# applying logistic regression to PCA data that has 2 dimensions.\nlogistic_regression_model_2D = LogisticRegression()\nlogistic_regression_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacurracy_2D_lr = logistic_regression_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_lr_2D = logistic_regression_model_2D.predict(dfTestFeatures_2D)\npredictions_lr_2D_prob = logistic_regression_model_2D.predict_proba(dfTestFeatures_2D)[:,1]\nmacro_precision_lr_2D, macro_recall_lr_2D, macro_fscore_lr_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_2D, average='macro')\nmicro_precision_lr_2D, micro_recall_lr_2D, micro_fscore_lr_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_2D, average='micro')\n\n\n# applying logistic regression to PCA data that has 10 best features.\nlogistic_regression_model_10B = LogisticRegression()\nlogistic_regression_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_10B_lr = logistic_regression_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_lr_10B = logistic_regression_model_10B.predict(dfTestFeatures_10B)\npredictions_lr_10B_prob = logistic_regression_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\nmacro_precision_lr_10B, macro_recall_lr_10B, macro_fscore_lr_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_10B, average='macro')\nmicro_precision_lr_10B, micro_recall_lr_10B, micro_fscore_lr_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_lr_10B, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_lr    ))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_2D_lr ))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_3D_lr ))\nprint('ACURRACY FOR (10 BEST FEA.)  : ' + str(acurracy_10B_lr))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_lr))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_lr_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_lr_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_lr_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_lr))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_lr_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_lr_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_lr_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_lr))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_lr_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_lr_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_lr_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_lr))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_lr_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_lr_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_lr_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_lr))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_lr_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_lr_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_lr_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_lr))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_lr_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_lr_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_lr_10B))\nprint('------------------------------------------------------')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm_logistic_regression     = confusion_matrix(dfTestLabels, predictions_lr)\ncm_logistic_regression_3D  = confusion_matrix(dfTestLabels, predictions_lr_3D)\ncm_logistic_regression_2D  = confusion_matrix(dfTestLabels, predictions_lr_2D)\ncm_logistic_regression_10b = confusion_matrix(dfTestLabels, predictions_lr_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_logistic_regression, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Logistic Regression predictions for initial data\")\nplt.ylabel(\"Logistic Regression test labels for initial data\")\nplt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_logistic_regression_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Logistic Regression predictions for PCA 3D\")\nplt.ylabel(\"Logistic Regression test labels for PCA 3D\")\nplt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_logistic_regression_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Logistic Regression predictions for PCA 2D\")\nplt.ylabel(\"Logistic Regression test labels for PCA 2D\")\nplt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_logistic_regression_10b, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Logistic Regression predictions for 10 Best\")\nplt.ylabel(\"Logistic Regression test labels for 10 Best\")\nplt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX FOR 10 Best\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_lr,     truePositiveRate_lr,     thresholds_lr     = roc_curve(dfTestLabels, predictions_lr_prob)\nfalsePositiveRate_lr_3D,  truePositiveRate_lr_3D,  thresholds_lr_3D  = roc_curve(dfTestLabels, predictions_lr_3D_prob)\nfalsePositiveRate_lr_2D,  truePositiveRate_lr_2D,  thresholds_lr_2D  = roc_curve(dfTestLabels, predictions_lr_2D_prob)\nfalsePositiveRate_lr_10B, truePositiveRate_lr_10B, thresholds_lr_10B = roc_curve(dfTestLabels, predictions_lr_10B_prob)\n\n# drawing the graph\nplt.plot(falsePositiveRate_lr, truePositiveRate_lr, color='red')\nplt.plot(falsePositiveRate_lr_3D,  truePositiveRate_lr_3D, color='green')\nplt.plot(falsePositiveRate_lr_2D,  truePositiveRate_lr_2D, color='blue')\nplt.plot(falsePositiveRate_lr_10B, truePositiveRate_lr_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Logistic Regression')\nplt.grid()\n################################################### AHMAD KHAN ###################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_lr,     recall_lr,     _ = precision_recall_curve(npTestLabels,     predictions_lr_prob)\nprecision_lr_3D,  recall_lr_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_lr_3D_prob)\nprecision_lr_2D,  recall_lr_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_lr_2D_prob)\nprecision_lr_10B, recall_lr_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_lr_10B_prob)\n\nplt.plot(recall_lr,     precision_lr,     marker='.', color=\"red\")\nplt.plot(recall_lr_3D,  precision_lr_3D,  marker='.', color=\"green\")\nplt.plot(recall_lr_2D,  precision_lr_2D,  marker='.', color=\"blue\")\nplt.plot(recall_lr_10B, precision_lr_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b33bcfe65d3f16ea8dff009959f7d2845d898e56"},"cell_type":"markdown","source":"<h1 style=\"text-align:center\">K-NEAREST NEIGHBOUR (KNN) CLASSIFICATION</h1>\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">K-Nearest Neighbors (KNN) Classification is used in this project. Same K-NN classification algorithm is used for initial data, 2-dimensional PCA, 3-dimensional PCA and the initial data with 10 best features. In order to estimate the best value for K, values from 1 to 100 are used for initial data, 2-dimensional PCA, 3-dimensional PCA and data with 10 best features and their accuracies and mean squared error are compared. When the average of the mean squared error is taken for the 4 different data types it is seen that when K is equal to 7 it gives the smallest error and gives a very high accuracy so K is chosen as 7. Accuracy for 4 different data types when K is between 1 and 10 are printed out for better demonstration.\n</div>\n\n<br>\n\n<div style=\"text-align:justify;font-size:17px;\">K is chosen as 7, so 7-NN is used for this algorithm and very high accuracies are found for initial data, 2-dimensional PCA, 3-dimensional PCA and the initial data with 10 best features. When the accuracies are compared, it is observed that 3-dimensional PCA has the highest accuracy, initial data has the second highest, data with 10 best labels has the third highest and the 2-dimensional PCA has the fourth highest. In addition to accuracy; precision, recall and fscore values for both macro and micro are also calculated. Accuracy is very high so all of them also have very high values. </div>\n\n<br>\n\n<div style=\"text-align:justify;font-size:17px;\">The estimated values for the 4 different data types are displayed on a confusion matrix. For this confusion matrix 171 different cells are used. 108 of them are healthy and 63 of them have disease. When accuracy is calculated for all them they give the same value found in the previous step.</div>\n\n<br>\n\n<div style=\"text-align:justify;font-size:17px;\">By using the true positive, true negative, false positive and false negative values in the confusion matrix, the results are shown in a ROC curve and a precision-recall curve. Initial values are shown by red, 3-dimensional PCA is shown by green, 2-dimensional PCA is shown by blue and the initial data with the 10 best features is shown by black. By examining the graph, it can be seen that all four of the results have very high accuracy.</div>\n\n"},{"metadata":{"trusted":true,"_uuid":"e36cb303924f250032447aa6192436f35102fac4"},"cell_type":"code","source":"####################################### ÇINAR YALÇINDURAN ###################################################\n\n# importing the model.\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error\n\naccurracies_knn     = []\naccurracies_3D_knn  = []\naccurracies_2D_knn  = []\naccurracies_10B_knn = []\nmean_squared_error_knn = []\nmean_squared_error_2D_knn = []\nmean_squared_error_3D_knn = []\nmean_squared_error_10B_knn = []\n\nfor n_neighbor in range(1,101):\n    knn_model     = KNeighborsClassifier(n_neighbors = n_neighbor)\n    knn_model_3D  = KNeighborsClassifier(n_neighbors = n_neighbor)\n    knn_model_2D  = KNeighborsClassifier(n_neighbors = n_neighbor)\n    knn_model_10B = KNeighborsClassifier(n_neighbors = n_neighbor)\n    \n    knn_model.fit(dfTrainFeatures, dfTrainLabels)\n    knn_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\n    knn_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\n    knn_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\n    \n    pred_knn = knn_model.predict(dfTestFeatures) \n    pred_2D_knn = knn_model_2D.predict(dfTestFeatures_2D) \n    pred_3D_knn = knn_model_3D.predict(dfTestFeatures_3D) \n    pred_10B_knn = knn_model_10B.predict(dfTestFeatures_10B) \n    \n    acc     = knn_model.score(dfTestFeatures, dfTestLabels)\n    acc_3D  = knn_model_3D.score(dfTestFeatures_3D,  dfTestLabels_3D)\n    acc_2D  = knn_model_2D.score(dfTestFeatures_2D,  dfTestLabels_2D)\n    acc_10B = knn_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\n    \n    mse = mean_squared_error(dfTestLabels,pred_knn,multioutput='raw_values')    \n    mse_2D = mean_squared_error(dfTestLabels,pred_2D_knn,multioutput='raw_values')    \n    mse_3D = mean_squared_error(dfTestLabels,pred_3D_knn,multioutput='raw_values')\n    mse_10B = mean_squared_error(dfTestLabels,pred_10B_knn,multioutput='raw_values')\n    \n    mean_squared_error_knn.append(mse)\n    mean_squared_error_2D_knn.append(mse_2D)\n    mean_squared_error_3D_knn.append(mse_3D)\n    mean_squared_error_10B_knn.append(mse_10B)\n    \n    accurracies_knn.append(acc)\n    accurracies_3D_knn.append(acc_3D)\n    accurracies_2D_knn.append(acc_2D)\n    accurracies_10B_knn.append(acc_10B)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9975a4add7ea7b65aa0021c76e80f1fe35bedbf"},"cell_type":"code","source":"print('-----------------------------------')\nprint('-ACURRACY FOR DIFFERENT DATA TYPES-')\nprint('---ONLY K FROM 1 TO 10 ARE SHOWN---')\nprint('')\n\nprint('-----------------------------------')\nprint('-----ACURRACY FOR INITIAL DATA-----')\nprint('-----------------------------------')\n\nfor i in range(10):\n        print(str(i+1) + ' nn acurracy for initial data: ' + str(accurracies_knn[i]))\n    \nprint('-----------------------------------')\nprint('------ACURRACY FOR PCA 3D DATA-----')\nprint('-----------------------------------')\n\nfor i in range(10):\n        print(str(i+1) + ' nn acurracy for pca 3D data: ' + str(accurracies_3D_knn[i]))\n        \nprint('-----------------------------------')\nprint('------ACURRACY FOR PCA 2D DATA-----')\nprint('-----------------------------------')\n\nfor i in range(10):\n        print(str(i+1) + ' nn acurracy for pca 2D data: ' + str(accurracies_2D_knn[i]))\n        \nprint('------------------------------------')\nprint('------ACURRACY FOR PCA 10B DATA-----')\nprint('------------------------------------')\n\nfor i in range(10):\n        print(str(i+1) + ' nn acurracy for pca 10B data: ' + str(accurracies_10B_knn[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting errors for different k values.\nk = [i for i in range(1,101)]\nplt.plot(k,mean_squared_error_knn,color='red')\nplt.plot(k,mean_squared_error_2D_knn,color='blue')\nplt.plot(k,mean_squared_error_3D_knn,color='green')\nplt.plot(k,mean_squared_error_10B_knn,color='black')\nplt.xlabel('K Values')\nplt.ylabel('Mean Squared Error')\nplt.title('Mean Squared Error for 4 Different Data Types')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_mse = []\nfor i in range(0,100):\n    avg = (mean_squared_error_knn[i] + mean_squared_error_2D_knn[i] + mean_squared_error_3D_knn[i] + mean_squared_error_10B_knn[i]) / 4 \n    average_mse.append(avg)\n\n#KNN is equal to 7\nbest_knn = average_mse.index(min(average_mse)) + 1\nprint('Best K value for K-NN is: ' + str(best_knn))\nplt.plot(average_mse)\nplt.xlabel('K Values')\nplt.ylabel('Mean Squared Error')\nplt.title('Average of the Mean Squared Errors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying 7 nn to initial data.\nknn_model = KNeighborsClassifier(n_neighbors = best_knn)\nknn_model.fit(dfTrainFeatures, dfTrainLabels)\nacc_knn = knn_model.score(dfTestFeatures, dfTestLabels)\npredictions_knn = knn_model.predict(dfTestFeatures)\npredictions_knn_prob = knn_model.predict_proba(dfTestFeatures)[:,1]\nmacro_precision_knn, macro_recall_knn, macro_fscore_knn, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn, average='macro')\nmicro_precision_knn, micro_recall_knn, micro_fscore_knn, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn, average='micro')\n\n\n# applying 7 nn to PCA 3D data.\nknn_model_3D = KNeighborsClassifier(n_neighbors = best_knn)\nknn_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacc_knn_3D = knn_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_knn_3D = knn_model_3D.predict(dfTestFeatures_3D)\npredictions_knn_3D_prob = knn_model_3D.predict_proba(dfTestFeatures_3D)[:,1]\nmacro_precision_knn_3D, macro_recall_knn_3D, macro_fscore_knn_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_3D, average='macro')\nmicro_precision_knn_3D, micro_recall_knn_3D, micro_fscore_knn_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_3D, average='micro')\n\n\n# applying 7 nn to PCA 2D data.\nknn_model_2D = KNeighborsClassifier(n_neighbors = best_knn)\nknn_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacc_knn_2D = knn_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_knn_2D = knn_model_2D.predict(dfTestFeatures_2D)\npredictions_knn_2D_prob = knn_model_2D.predict_proba(dfTestFeatures_2D)[:,1]\nmacro_precision_knn_2D, macro_recall_knn_2D, macro_fscore_knn_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_2D, average='macro')\nmicro_precision_knn_2D, micro_recall_knn_2D, micro_fscore_knn_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_2D, average='micro')\n\n\n# applying 7 nn to PCA 10 Best data.\nknn_model_10B = KNeighborsClassifier(n_neighbors = best_knn)\nknn_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacc_knn_10B = knn_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_knn_10B = knn_model_10B.predict(dfTestFeatures_10B)\npredictions_knn_10B_prob = knn_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\nmacro_precision_knn_10B, macro_recall_knn_10B, macro_fscore_knn_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_10B, average='macro')\nmicro_precision_knn_10B, micro_recall_knn_10B, micro_fscore_knn_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_knn_10B, average='micro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA     : ' + str(acc_knn))\nprint('ACURRACY FOR PCA (2DIMENSION) : ' + str(acc_knn_3D))\nprint('ACURRACY FOR PCA (3DIMENSION) : ' + str(acc_knn_2D))\nprint('ACURRACY FOR (10 BEST FEAT.)  : ' + str(acc_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_knn))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_knn_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_knn_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_knn))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_knn_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_knn_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_knn))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_knn_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_knn_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_knn))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_knn_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_knn_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_knn))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_knn_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_knn_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_knn_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_knn))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_knn_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_knn_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_knn_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_7nn     = confusion_matrix(dfTestLabels, predictions_knn)\ncm_7nn_3D  = confusion_matrix(dfTestLabels, predictions_knn_3D)\ncm_7nn_2D  = confusion_matrix(dfTestLabels, predictions_knn_2D)\ncm_7nn_10B = confusion_matrix(dfTestLabels, predictions_knn_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_7nn, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"7nn predictions for initial data\")\nplt.ylabel(\"7nn test labels for initial data\")\nplt.title(\"7NN CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_7nn_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"7nn predictions for PCA 3D\")\nplt.ylabel(\"7nn test labels for PCA 3D\")\nplt.title(\"7NN CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_7nn_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"7nn predictions for PCA 2D\")\nplt.ylabel(\"7nn test labels for PCA 2D\")\nplt.title(\"7NN CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_7nn_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"7nn predictions for 10 Best\")\nplt.ylabel(\"7nn test labels for 10 Best\")\nplt.title(\"7NN CONFUSION MATRIX FOR 10 BEST\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_knn,     truePositiveRate_knn,     thresholds_knn     = roc_curve(dfTestLabels, predictions_knn_prob)\nfalsePositiveRate_knn_3D,  truePositiveRate_knn_3D,  thresholds_knn_3D  = roc_curve(dfTestLabels, predictions_knn_3D_prob)\nfalsePositiveRate_knn_2D,  truePositiveRate_knn_2D,  thresholds_knn_2D  = roc_curve(dfTestLabels, predictions_knn_2D_prob)\nfalsePositiveRate_knn_10B, truePositiveRate_knn_10B, thresholds_knn_10B = roc_curve(dfTestLabels, predictions_knn_10B_prob)\n\n# drawing the graph\nplt.plot(falsePositiveRate_knn,     truePositiveRate_knn,     color='red')\nplt.plot(falsePositiveRate_knn_3D,  truePositiveRate_knn_3D,  color='green')\nplt.plot(falsePositiveRate_knn_2D,  truePositiveRate_knn_2D,  color='blue')\nplt.plot(falsePositiveRate_knn_10B, truePositiveRate_knn_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for 7NN')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_knn,     recall_knn,     _ = precision_recall_curve(npTestLabels,     predictions_knn_prob)\nprecision_knn_3D,  recall_knn_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_knn_3D_prob)\nprecision_knn_2D,  recall_knn_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_knn_2D_prob)\nprecision_knn_10B, recall_knn_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_knn_10B_prob)\n\nplt.plot(recall_knn,     precision_knn,     marker='.', color=\"red\")\nplt.plot(recall_knn_3D,  precision_knn_3D,  marker='.', color=\"green\")\nplt.plot(recall_knn_2D,  precision_knn_2D,  marker='.', color=\"blue\")\nplt.plot(recall_knn_10B, precision_knn_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad9778893bc88c9ec73a1f47b479d08277d208e0"},"cell_type":"markdown","source":"<h3 style=\"text-align:center\">SUPPORT VECTOR MACHINE (SVM) CLASSIFICATION</h3>\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">We first import the SVC library sklearn.svm. SVM is a classifier that separates clusters clusters of data via a plane (or a hyperplane for higher dimensions). As in the aforementioned algorithms, we fit initial dataset to the model. And then use that model to perform predictions on our test set. This process is repeated a further two times on 2D PCA and 3D PCA data. Then there accuracies, confusion matrices and ROC curves are calculated as below:</div>\n\n"},{"metadata":{"trusted":true,"_uuid":"23f9dfa0d4b6204f231c06dbbd66bc98faccef98","_kg_hide-input":false},"cell_type":"code","source":"# importing the model.\nfrom sklearn.svm import SVC\n\n# applying svm to initial data.\nsvm_model = SVC(random_state = 1)\nsvm_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_svm = svm_model.score(dfTestFeatures, dfTestLabels)\npredictions_svm = svm_model.predict(dfTestFeatures)\nmacro_precision_svm, macro_recall_svm, macro_fscore_svm, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm, average='macro')\nmicro_precision_svm, micro_recall_svm, micro_fscore_svm, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm, average='micro')\n\n# applying svm to PCA data that has 3 dimensions.\nsvm_model_3D = SVC(random_state = 1)\nsvm_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacurracy_3D_svm = svm_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_svm_3D = svm_model_3D.predict(dfTestFeatures_3D)\nmacro_precision_svm_3D, macro_recall_svm_3D, macro_fscore_svm_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_3D, average='macro')\nmicro_precision_svm_3D, micro_recall_svm_3D, micro_fscore_svm_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_3D, average='micro')\n\n# applying svm to PCA data that has 2 dimensions.\nsvm_model_2D = SVC(random_state = 1)\nsvm_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacurracy_2D_svm = svm_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_svm_2D = svm_model_2D.predict(dfTestFeatures_2D)\nmacro_precision_svm_2D, macro_recall_svm_2D, macro_fscore_svm_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_2D, average='macro')\nmicro_precision_svm_2D, micro_recall_svm_2D, micro_fscore_svm_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_2D, average='micro')\n\n# applying svm to data that has 10 Best Features\nsvm_model_10B = SVC(random_state = 1)\nsvm_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_10B_svm = svm_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_svm_10B = svm_model_10B.predict(dfTestFeatures_10B)\nmacro_precision_svm_10B, macro_recall_svm_10B, macro_fscore_svm_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_10B, average='macro')\nmicro_precision_svm_10B, micro_recall_svm_10B, micro_fscore_svm_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_svm_10B, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_svm))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_3D_svm))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_2D_svm))\nprint('ACURRACY FOR (10 BEST FEAT.) : ' + str(acurracy_10B_svm))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_svm))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_svm_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_svm_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_svm_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_svm))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_svm_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_svm_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_svm_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_svm))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_svm_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_svm_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_svm_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_svm))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_svm_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_svm_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_svm_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_svm))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_svm_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_svm_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_svm_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_svm))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_svm_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_svm_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_svm_10B))\nprint('------------------------------------------------------')\n\n\n####################################### ÇINAR YALÇINDURAN ###################################################\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################### AHMAD KAAN ###################################################\n# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_svm     = confusion_matrix(dfTestLabels, predictions_svm)\ncm_svm_3D  = confusion_matrix(dfTestLabels, predictions_svm_3D)\ncm_svm_2D  = confusion_matrix(dfTestLabels, predictions_svm_2D)\ncm_svm_10B = confusion_matrix(dfTestLabels, predictions_svm_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_svm, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"svm predictions for initial data\")\nplt.ylabel(\"svm test labels for initial data\")\nplt.title(\"SVM CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_svm_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"svm predictions for PCA 3D\")\nplt.ylabel(\"svm test labels for PCA 3D\")\nplt.title(\"SVM CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_svm_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"svm predictions for PCA 2D\")\nplt.ylabel(\"svm test labels for PCA 2D\")\nplt.title(\"SVM CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_svm_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"svm predictions for 10B\")\nplt.ylabel(\"svm test labels for 10B\")\nplt.title(\"SVM CONFUSION MATRIX FOR 10BEST\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_svm,     truePositiveRate_svm,     thresholds_svm     = roc_curve(dfTestLabels, predictions_svm)\nfalsePositiveRate_svm_3D,  truePositiveRate_svm_3D,  thresholds_svm_3D  = roc_curve(dfTestLabels, predictions_svm_3D)\nfalsePositiveRate_svm_2D,  truePositiveRate_svm_2D,  thresholds_svm_2D  = roc_curve(dfTestLabels, predictions_svm_2D)\nfalsePositiveRate_svm_10B, truePositiveRate_svm_10B, thresholds_svm_10B = roc_curve(dfTestLabels, predictions_svm_10B)\n\n# drawing the graph\nplt.plot(falsePositiveRate_svm,     truePositiveRate_svm,     color='red')\nplt.plot(falsePositiveRate_svm_3D,  truePositiveRate_svm_3D,  color='green')\nplt.plot(falsePositiveRate_svm_2D,  truePositiveRate_svm_2D,  color='blue')\nplt.plot(falsePositiveRate_svm_10B, truePositiveRate_svm_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for SVM')\nplt.grid()\n####################################### AHMAD KAAN ###################################################\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_svm,     recall_svm,     _ = precision_recall_curve(npTestLabels,     predictions_svm)\nprecision_svm_3D,  recall_svm_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_svm_3D)\nprecision_svm_2D,  recall_svm_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_svm_2D)\nprecision_svm_10B, recall_svm_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_svm_10B)\n\nplt.plot(recall_svm,     precision_svm,     marker='.', color=\"red\")\nplt.plot(recall_svm_3D,  precision_svm_3D,  marker='.', color=\"green\")\nplt.plot(recall_svm_2D,  precision_svm_2D,  marker='.', color=\"blue\")\nplt.plot(recall_svm_10B, precision_svm_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center\">DECISSION TREE CLASSIFICATION</h2>\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">We import the DecisionTreeClassifier library from sklearn.tree. That classifies the label for each feature and then continues doing so in a linear and consequent manner for other features. Then the final classification for the final feature is our prediction. Same as for other algorithms, we fit our data to the model and then perform our predictions using this model on our test labels. The results for these are given below:</div> \n<br>\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################### HALİL ŞİRİN ###################################################\n\n# importing the model.\nfrom sklearn.tree import DecisionTreeClassifier\n\n# applying Decision Tree Classification to initial data.\ndecision_tree_model = DecisionTreeClassifier()\ndecision_tree_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_dt = decision_tree_model.score(dfTestFeatures, dfTestLabels)\npredictions_dt = decision_tree_model.predict(dfTestFeatures)\npredictions_dt_prob = decision_tree_model.predict_proba(dfTestFeatures)[:,1]\nmacro_precision_dt, macro_recall_dt, macro_fscore_dt, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt, average='macro')\nmicro_precision_dt, micro_recall_dt, micro_fscore_dt, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt, average='micro')\n\n# applying Decision Tree Classification to PCA data that has 3 dimensions.\ndecision_tree_model_3D = DecisionTreeClassifier()\ndecision_tree_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacurracy_dt_3D = decision_tree_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_dt_3D = decision_tree_model_3D.predict(dfTestFeatures_3D)\npredictions_dt_3D_prob = decision_tree_model_3D.predict_proba(dfTestFeatures_3D)[:,1]\nmacro_precision_dt_3D, macro_recall_dt_3D, macro_fscore_dt_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_3D, average='macro')\nmicro_precision_dt_3D, micro_recall_dt_3D, micro_fscore_dt_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_3D, average='micro')\n\n# applying Decision Tree Classification to PCA data that has 2 dimensions.\ndecision_tree_model_2D = DecisionTreeClassifier()\ndecision_tree_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacurracy_dt_2D = decision_tree_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_dt_2D = decision_tree_model_2D.predict(dfTestFeatures_2D)\npredictions_dt_2D_prob = decision_tree_model_2D.predict_proba(dfTestFeatures_2D)[:,1]\nmacro_precision_dt_2D, macro_recall_dt_2D, macro_fscore_dt_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_2D, average='macro')\nmicro_precision_dt_2D, micro_recall_dt_2D, micro_fscore_dt_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_2D, average='micro')\n\n# applying Decision Tree Classification to data that has 10 Best Features.\ndecision_tree_model_10B = DecisionTreeClassifier()\ndecision_tree_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_dt_10B = decision_tree_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_dt_10B = decision_tree_model_2D.predict(dfTestFeatures_2D)\npredictions_dt_10B_prob = decision_tree_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\nmacro_precision_dt_10B, macro_recall_dt_10B, macro_fscore_dt_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_10B, average='macro')\nmicro_precision_dt_10B, micro_recall_dt_10B, micro_fscore_dt_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_dt_10B, average='micro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_dt))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_dt_3D))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_dt_2D))\nprint('ACURRACY FOR (10 BEST FEAT.) : ' + str(acurracy_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_dt))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_dt_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_dt_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_dt))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_dt_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_dt_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_dt))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_dt_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_dt_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_dt))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_dt_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_dt_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_dt))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_dt_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_dt_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_dt_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_dt))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_dt_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_dt_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_dt_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_decision_tree     = confusion_matrix(dfTestLabels, predictions_dt)\ncm_decision_tree_3D  = confusion_matrix(dfTestLabels, predictions_dt_3D)\ncm_decision_tree_2D  = confusion_matrix(dfTestLabels, predictions_dt_2D)\ncm_decision_tree_10B = confusion_matrix(dfTestLabels, predictions_dt_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_decision_tree, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Decision Tree predictions for initial data\")\nplt.ylabel(\"Decision Tree test labels for initial data\")\nplt.title(\"DECISION TREE CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_decision_tree_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Decision Tree predictions for PCA 3D\")\nplt.ylabel(\"Decision Tree test labels for PCA 3D\")\nplt.title(\"DECISION TREE CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_decision_tree_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Decision Tree predictions for PCA 2D\")\nplt.ylabel(\"Decision Tree test labels for PCA 2D\")\nplt.title(\"DECISION TREE CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_decision_tree_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Decision Tree predictions for 10 Best\")\nplt.ylabel(\"Decision Tree test labels for 10 Best\")\nplt.title(\"DECISION TREE CONFUSION MATRIX FOR 10 BEST\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_dt,     truePositiveRate_dt,     thresholds_dt     = roc_curve(dfTestLabels, predictions_dt_prob)\nfalsePositiveRate_dt_3D,  truePositiveRate_dt_3D,  thresholds_dt_3D  = roc_curve(dfTestLabels, predictions_dt_3D_prob)\nfalsePositiveRate_dt_2D,  truePositiveRate_dt_2D,  thresholds_dt_2D  = roc_curve(dfTestLabels, predictions_dt_2D_prob)\nfalsePositiveRate_dt_10B, truePositiveRate_dt_10B, thresholds_dt_10B = roc_curve(dfTestLabels, predictions_dt_10B_prob)\n\n# drawing the graph\nplt.plot(falsePositiveRate_dt, truePositiveRate_dt, color='red')\nplt.plot(falsePositiveRate_dt_3D, truePositiveRate_dt_3D, color='green')\nplt.plot(falsePositiveRate_dt_2D, truePositiveRate_dt_2D, color='blue')\nplt.plot(falsePositiveRate_dt_10B, truePositiveRate_dt_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Decision Tree')\nplt.grid()\n\n####################################### HALİL ŞİRİN ###################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_dt,     recall_dt,     _ = precision_recall_curve(npTestLabels,     predictions_dt_prob)\nprecision_dt_3D,  recall_dt_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_dt_3D_prob)\nprecision_dt_2D,  recall_dt_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_dt_2D_prob)\nprecision_dt_10B, recall_dt_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_dt_10B_prob)\n\nplt.plot(recall_dt,     precision_dt,     marker='.', color=\"red\")\nplt.plot(recall_dt_3D,  precision_dt_3D,  marker='.', color=\"green\")\nplt.plot(recall_dt_2D,  precision_dt_2D,  marker='.', color=\"blue\")\nplt.plot(recall_dt_10B, precision_dt_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center\">RANDOM FOREST CLASSIFICATION</h2>\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the model.\nfrom sklearn.ensemble import RandomForestClassifier\n\n# applying Random Forest Classification to initial data.\nrf_model = RandomForestClassifier(n_estimators=100, random_state=1)\nrf_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_rf = rf_model.score(dfTestFeatures, dfTestLabels)\npredictions_rf = rf_model.predict(dfTestFeatures)\npredictions_rf_prob = rf_model.predict_proba(dfTestFeatures)[:,1]\nmacro_precision_rf, macro_recall_rf, macro_fscore_rf, _ = precision_recall_fscore_support(dfTestLabels, predictions_rf, average='macro')\nmicro_precision_rf, micro_recall_rf, micro_fscore_rf, _ = precision_recall_fscore_support(dfTestLabels, predictions_rf, average='micro')\n\n# applying Random Forest Classification to PCA data that has 3 dimensions.\nrf_model_3D = RandomForestClassifier(n_estimators=100, random_state=1)\nrf_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacurracy_rf_3D = rf_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_rf_3D = rf_model_3D.predict(dfTestFeatures_3D)\npredictions_rf_3D_prob = rf_model_3D.predict_proba(dfTestFeatures_3D)[:,1]\nmacro_precision_rf_3D, macro_recall_rf_3D, macro_fscore_rf_3D, _ = precision_recall_fscore_support(dfTestLabels_3D, predictions_rf_3D, average='macro')\nmicro_precision_rf_3D, micro_recall_rf_3D, micro_fscore_rf_3D, _ = precision_recall_fscore_support(dfTestLabels_3D, predictions_rf_3D, average='micro')\n\n# applying Random Forest Classification to PCA data that has 2 dimensions.\nrf_model_2D = RandomForestClassifier(n_estimators=100, random_state=1)\nrf_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacurracy_rf_2D = rf_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_rf_2D = rf_model_2D.predict(dfTestFeatures_2D)\npredictions_rf_2D_prob = rf_model_2D.predict_proba(dfTestFeatures_2D)[:,1]\nmacro_precision_rf_2D, macro_recall_rf_2D, macro_fscore_rf_2D, _ = precision_recall_fscore_support(dfTestLabels_2D, predictions_rf_2D, average='macro')\nmicro_precision_rf_2D, micro_recall_rf_2D, micro_fscore_rf_2D, _ = precision_recall_fscore_support(dfTestLabels_2D, predictions_rf_2D, average='micro')\n\n# applying Random Forest Classification to data that has 10 Best Features.\nrf_model_10B = RandomForestClassifier(n_estimators=100, random_state=1)\nrf_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_rf_10B = rf_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_rf_10B = rf_model_10B.predict(dfTestFeatures_10B)\npredictions_rf_10B_prob = rf_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\nmacro_precision_rf_10B, macro_recall_rf_10B, macro_fscore_rf_10B, _ = precision_recall_fscore_support(dfTestLabels_10B, predictions_rf_10B, average='macro')\nmicro_precision_rf_10B, micro_recall_rf_10B, micro_fscore_rf_10B, _ = precision_recall_fscore_support(dfTestLabels_10B, predictions_rf_10B, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_rf))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_rf_3D))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_rf_2D))\nprint('ACURRACY FOR (10 BEST FEAT.) : ' + str(acurracy_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_rf))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_rf_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_rf_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_rf))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_rf_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_rf_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_rf))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_rf_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_rf_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_rf))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_rf_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_rf_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_rf))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_rf_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_rf_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_rf_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_rf))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_rf_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_rf_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_rf_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_rf     = confusion_matrix(dfTestLabels, predictions_rf)\ncm_rf_3D  = confusion_matrix(dfTestLabels, predictions_rf_3D)\ncm_rf_2D  = confusion_matrix(dfTestLabels, predictions_rf_2D)\ncm_rf_10B = confusion_matrix(dfTestLabels, predictions_rf_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_rf, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Random Forest Classification predictions for initial data\")\nplt.ylabel(\"Random Forest Classification test labels for initial data\")\nplt.title(\"RANDOM FOREST CLASSIFICATION CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_rf_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Random Forest Classification predictions for PCA 3D\")\nplt.ylabel(\"Random Forest Classification test labels for PCA 3D\")\nplt.title(\"RANDOM FOREST CLASSIFICATION CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_rf_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Random Forest Classification predictions for PCA 2D\")\nplt.ylabel(\"Random Forest Classification test labels for PCA 2D\")\nplt.title(\"RANDOM FOREST CLASSIFICATION CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_rf_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Random Forest Classification predictions for 10 Best\")\nplt.ylabel(\"Random Forest Classification test labels for 10 Best\")\nplt.title(\"RANDOM FOREST CLASSIFICATION CONFUSION MATRIX FOR 10 BEST\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_rf,     truePositiveRate_rf,     thresholds_rf     = roc_curve(dfTestLabels, predictions_rf_prob)\nfalsePositiveRate_rf_3D,  truePositiveRate_rf_3D,  thresholds_rf_3D  = roc_curve(dfTestLabels, predictions_rf_3D_prob)\nfalsePositiveRate_rf_2D,  truePositiveRate_rf_2D,  thresholds_rf_2D  = roc_curve(dfTestLabels, predictions_rf_2D_prob)\nfalsePositiveRate_rf_10B, truePositiveRate_rf_10B, thresholds_rf_10B = roc_curve(dfTestLabels, predictions_rf_10B_prob)\n\n# drawing the graph\nplt.plot(falsePositiveRate_rf, truePositiveRate_rf, color='red')\nplt.plot(falsePositiveRate_rf_3D, truePositiveRate_rf_3D, color='green')\nplt.plot(falsePositiveRate_rf_2D, truePositiveRate_rf_2D, color='blue')\nplt.plot(falsePositiveRate_rf_10B, truePositiveRate_rf_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Random Forest Classification')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_rf,     recall_rf,     _ = precision_recall_curve(npTestLabels,     predictions_rf_prob)\nprecision_rf_3D,  recall_rf_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_rf_3D_prob)\nprecision_rf_2D,  recall_rf_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_rf_2D_prob)\nprecision_rf_10B, recall_rf_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_rf_10B_prob)\n\nplt.plot(recall_rf,     precision_rf,     marker='.', color=\"red\")\nplt.plot(recall_rf_3D,  precision_rf_3D,  marker='.', color=\"green\")\nplt.plot(recall_rf_2D,  precision_rf_2D,  marker='.', color=\"blue\")\nplt.plot(recall_rf_10B, precision_rf_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">NAIVE BAYES CLASSIFICATION</h3>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"<h4 >MULTINOMIAL MODEL</h4>\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">We import the MultinomialNB library from sklearn.naive_bayes. Naive bayes is a classifier that treats all instances of feature to be independent from each other. Which in nature is “Naive” assumption. But despite this fact, it is still commonly used classifier due to its accuracy. For this algorithm, we only use our initial data and best 10 features. The results are given below: </div> \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"######################## FATIH SEVBAN UYANIK ################################\nfrom sklearn.naive_bayes import MultinomialNB\n\n# applying Naive Bayes Classification to initial data.\nnaive_bayes_mult_model = MultinomialNB()\nnaive_bayes_mult_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_nb_mult = naive_bayes_mult_model.score(dfTestFeatures, dfTestLabels)\npredictions_nb_mult = naive_bayes_mult_model.predict(dfTestFeatures)\npredictions_nb_mult_prob = naive_bayes_mult_model.predict_proba(dfTestFeatures)[:,1]\n\n# applying Naive Bayes Classification to 10 Best Data.\nnaive_bayes_mult_model_10B = MultinomialNB()\nnaive_bayes_mult_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_nb_mult_10B = naive_bayes_mult_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_nb_mult_10B = naive_bayes_mult_model_10B.predict(dfTestFeatures_10B)\npredictions_nb_mult_prob_10B = naive_bayes_mult_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\n\n# printing the results.\nprint('ACURRACY FOR INITIAL DATA : ' + str(acurracy_nb_mult))\nprint('ACURRACY FOR 10 BEST DATA : ' + str(acurracy_nb_mult_10B))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_nb_mult     = confusion_matrix(dfTestLabels, predictions_nb_mult)\ncm_nb_mult_10B = confusion_matrix(dfTestLabels, predictions_nb_mult_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_mult, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Multinomial naive bayes predictions for initial data\")\nplt.ylabel(\"Multinomial naive bayes test labels for initial data\")\nplt.title(\"MULTINOMIAL NAIVE BAYES CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_mult_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Multinomial naive bayes predictions for 10 Best data\")\nplt.ylabel(\"Multinomial naive bayes test labels for 10 Best data\")\nplt.title(\"MULTINOMIAL NAIVE BAYES CONFUSION MATRIX FOR 10 BEST DATA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_nb_mult, truePositiveRate_nb_mult, thresholds_nb_mult = roc_curve(dfTestLabels, predictions_nb_mult_prob)\nfalsePositiveRate_nb_mult_10B, truePositiveRate_nb_mult_10B, thresholds_nb_mult_10B = roc_curve(dfTestLabels, predictions_nb_mult_prob_10B)\n\n# drawing the graph \nplt.plot(falsePositiveRate_nb_mult, truePositiveRate_nb_mult, color='red')\nplt.plot(falsePositiveRate_nb_mult_10B, truePositiveRate_nb_mult_10B, color='blue')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Multinomial Naive Bayes Classification')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_nb_mult,     recall_nb_mult,     _ = precision_recall_curve(npTestLabels,     predictions_nb_mult_prob)\nprecision_nb_mult_10B, recall_nb_mult_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_nb_mult_prob_10B)\n\nplt.plot(recall_nb_mult,     precision_nb_mult,     marker='.', color=\"red\")\nplt.plot(recall_nb_mult_10B, precision_nb_mult_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>GAUSSIAN MODEL</h4>\n<hr>\n<div style=\"text-align:justify;font-size:17px;\">However, the negative instances in the PCA data did not allow a multinomial classifier to fit to it. Therefore, we used a gaussian model for them. The results using the gaussian model are given below: </div>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# applying Naive Bayes Gaussian Classification to initial data.\nnaive_bayes_gaus_model = GaussianNB()\nnaive_bayes_gaus_model.fit(dfTrainFeatures, dfTrainLabels)\nacurracy_nb_gaus = naive_bayes_gaus_model.score(dfTestFeatures, dfTestLabels)\npredictions_nb_gaus = naive_bayes_gaus_model.predict(dfTestFeatures)\npredictions_nb_gaus_prob = naive_bayes_gaus_model.predict_proba(dfTestFeatures)[:,1]\nmacro_precision_nb_gaus, macro_recall_nb_gaus, macro_fscore_nb_gaus, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus, average='macro')\nmicro_precision_nb_gaus, micro_recall_nb_gaus, micro_fscore_nb_gaus, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus, average='micro')\n\n\n# applying Naive Bayes Gaussian Classification to PCA 3D data.\nnaive_bayes_gaus_model_3D = GaussianNB()\nnaive_bayes_gaus_model_3D.fit(dfTrainFeatures_3D, dfTrainLabels_3D)\nacurracy_nb_gaus_3D = naive_bayes_gaus_model_3D.score(dfTestFeatures_3D, dfTestLabels_3D)\npredictions_nb_gaus_3D = naive_bayes_gaus_model_3D.predict(dfTestFeatures_3D)\npredictions_nb_gaus_prob_3D = naive_bayes_gaus_model_3D.predict_proba(dfTestFeatures_3D)[:,1]\nmacro_precision_nb_gaus_3D, macro_recall_nb_gaus_3D, macro_fscore_nb_gaus_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_3D, average='macro')\nmicro_precision_nb_gaus_3D, micro_recall_nb_gaus_3D, micro_fscore_nb_gaus_3D, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_3D, average='micro')\n\n\n# applying Naive Bayes Gaussian Classification to PCA 2D data.\nnaive_bayes_gaus_model_2D = GaussianNB()\nnaive_bayes_gaus_model_2D.fit(dfTrainFeatures_2D, dfTrainLabels_2D)\nacurracy_nb_gaus_2D = naive_bayes_gaus_model_2D.score(dfTestFeatures_2D, dfTestLabels_2D)\npredictions_nb_gaus_2D = naive_bayes_gaus_model_2D.predict(dfTestFeatures_2D)\npredictions_nb_gaus_prob_2D = naive_bayes_gaus_model_2D.predict_proba(dfTestFeatures_2D)[:,1]\nmacro_precision_nb_gaus_2D, macro_recall_nb_gaus_2D, macro_fscore_nb_gaus_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_2D, average='macro')\nmicro_precision_nb_gaus_2D, micro_recall_nb_gaus_2D, micro_fscore_nb_gaus_2D, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_2D, average='micro')\n\n\n# applying Naive Bayes Gaussian Classification to 10 Best Data.\nnaive_bayes_gaus_model_10B = GaussianNB()\nnaive_bayes_gaus_model_10B.fit(dfTrainFeatures_10B, dfTrainLabels_10B)\nacurracy_nb_gaus_10B = naive_bayes_gaus_model_10B.score(dfTestFeatures_10B, dfTestLabels_10B)\npredictions_nb_gaus_10B = naive_bayes_gaus_model_10B.predict(dfTestFeatures_10B)\npredictions_nb_gaus_prob_10B = naive_bayes_gaus_model_10B.predict_proba(dfTestFeatures_10B)[:,1]\nmacro_precision_nb_gaus_10B, macro_recall_nb_gaus_10B, macro_fscore_nb_gaus_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_10B, average='macro')\nmicro_precision_nb_gaus_10B, micro_recall_nb_gaus_10B, micro_fscore_nb_gaus_10B, _ = precision_recall_fscore_support(dfTestLabels, predictions_nb_gaus_10B, average='micro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_nb_gaus))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_nb_gaus_3D))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_nb_gaus_2D))\nprint('ACURRACY FOR 10 BEST FEATURES: ' + str(acurracy_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_nb_gaus))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_nb_gaus_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_nb_gaus_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_nb_gaus))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_nb_gaus_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_nb_gaus_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_nb_gaus))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_nb_gaus_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_nb_gaus_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_nb_gaus))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_nb_gaus_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_nb_gaus_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_nb_gaus))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_nb_gaus_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_nb_gaus_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_nb_gaus_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_nb_gaus))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_nb_gaus_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_nb_gaus_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_nb_gaus_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_nb_gaussian     = confusion_matrix(dfTestLabels, predictions_nb_gaus)\ncm_nb_gaussian_3D  = confusion_matrix(dfTestLabels, predictions_nb_gaus_3D)\ncm_nb_gaussian_2D  = confusion_matrix(dfTestLabels, predictions_nb_gaus_2D)\ncm_nb_gaussian_10B = confusion_matrix(dfTestLabels, predictions_nb_gaus_2D)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_gaussian, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Gaussian naive bayes predictions for initial data\")\nplt.ylabel(\"Gaussian naive bayes test labels for initial data\")\nplt.title(\"GAUSSIAN NAIVE BAYES CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_gaussian_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Gaussian naive bayes predictions for PCA 3D\")\nplt.ylabel(\"Gaussian naive bayes test labels for PCA 3D\")\nplt.title(\"GAUSSIAN NAIVE BAYES CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_gaussian_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Gaussian naive bayes predictions for PCA 2D\")\nplt.ylabel(\"Gaussian naive bayes test labels for PCA 2D\")\nplt.title(\"GAUSSIAN NAIVE BAYES CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_nb_gaussian_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"Gaussian naive bayes predictions for 10 Best Data\")\nplt.ylabel(\"Gaussian naive bayes test labels for 10 Best Data\")\nplt.title(\"GAUSSIAN NAIVE BAYES CONFUSION MATRIX FOR 10 BEST DATA\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_nb_gaus,     truePositiveRate_nb_gaus,     thresholds_nb_gaus     = roc_curve(dfTestLabels, predictions_nb_gaus_prob)\nfalsePositiveRate_nb_gaus_3D,  truePositiveRate_nb_gaus_3D,  thresholds_nb_gaus_3D  = roc_curve(dfTestLabels, predictions_nb_gaus_prob_3D)\nfalsePositiveRate_nb_gaus_2D,  truePositiveRate_nb_gaus_2D,  thresholds_nb_gaus_2D  = roc_curve(dfTestLabels, predictions_nb_gaus_prob_2D)\nfalsePositiveRate_nb_gaus_10B, truePositiveRate_nb_gaus_10B, thresholds_nb_gaus_10B = roc_curve(dfTestLabels, predictions_nb_gaus_prob_10B)\n\n# drawing the graph\nplt.plot(falsePositiveRate_nb_gaus, truePositiveRate_nb_gaus, color='red')\nplt.plot(falsePositiveRate_nb_gaus_3D,  truePositiveRate_nb_gaus_3D, color='green')\nplt.plot(falsePositiveRate_nb_gaus_2D,  truePositiveRate_nb_gaus_2D, color='blue')\nplt.plot(falsePositiveRate_nb_gaus_10B, truePositiveRate_nb_gaus_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Gaussian Naive Bayes Classification')\nplt.grid()\n\n######################## FATIH SEVBAN UYANIK ################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_nb_gaus,     recall_nb_gaus,     _ = precision_recall_curve(npTestLabels,     predictions_nb_gaus_prob)\nprecision_nb_gaus_3D,  recall_nb_gaus_3D,  _ = precision_recall_curve(npTestLabels_3D,  predictions_nb_gaus_prob_3D)\nprecision_nb_gaus_2D,  recall_nb_gaus_2D,  _ = precision_recall_curve(npTestLabels_2D,  predictions_nb_gaus_prob_2D)\nprecision_nb_gaus_10B, recall_nb_gaus_10B, _ = precision_recall_curve(npTestLabels_10B, predictions_nb_gaus_prob_10B)\n\nplt.plot(recall_nb_gaus,     precision_nb_gaus,     marker='.', color=\"red\")\nplt.plot(recall_nb_gaus_3D,  precision_nb_gaus_3D,  marker='.', color=\"green\")\nplt.plot(recall_nb_gaus_2D,  precision_nb_gaus_2D,  marker='.', color=\"blue\")\nplt.plot(recall_nb_gaus_10B, precision_nb_gaus_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align:center\">ANN CLASSIFICATION</h2>\n<hr>\n\n<div style=\"text-align:justify;font-size:17px;\">Artificial neural networks is a mthod in Data Science that enables to make classification by having multiple nodes. These nodes are single neural nets and they are constructing the layers of the network. Each single neural net has its own weights and by doing forward and backward propagation, the weights are adjusting themselves to the training dataset. We used pytorch and created two models. One of them has 5 nodes with 1 hidden layer and the other has 10 nodes with 2 hidden layer. By doing 1000 epochs, our weights adjusted to the dataset and predictions were made. Predictions were compared with the real dataset and their acurracy, precisions, recalls and fscores were printed out and reported. We repeated this process for 2D PCA and 3D PCA and 10 best features training dataset.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAcurracy(predictions, labels):\n    truePositives = np.sum( np.logical_and( labels, predictions) )\n    falsePositives = np.sum( np.logical_and( np.logical_not(labels), predictions) ) \n    falseNegatives = np.sum( np.logical_and( labels, np.logical_not(predictions)) ) \n    trueNegatives  = np.sum( np.logical_and( np.logical_not(labels), np.logical_not(predictions)) )\n    acurracy = (truePositives + trueNegatives) / (truePositives + falsePositives + falseNegatives + trueNegatives)\n    return acurracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the model for \n# ANN with 1 hidden layer\nclass ANNModel_1Hidden(nn.Module):\n    def __init__(self, inputSize, hiddenSize, outputSize):\n        super().__init__()\n        self.model1 = nn.Linear(inputSize, hiddenSize)\n        self.model2 = nn.Linear(hiddenSize, outputSize)\n  \n    def propagateForward(self, x):\n        y_head1 = torch.sigmoid( self.model1(x) )\n        y_head2 = torch.sigmoid( self.model2(y_head1) )\n        return y_head2      \n  \n    def predictTests(self, xTest):\n        predictions = self.propagateForward(xTest)\n    \n        for i in range(predictions.shape[0]):\n            if (predictions[i] > 0.5):\n                predictions[i] = 1\n            else:\n                predictions[i] = 0\n\n        return predictions\n    \n    def predictTestsProba(self, xTest):\n        return self.propagateForward(xTest)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model_1hidden     = ANNModel_1Hidden(30, 5, 1)\nann_model_1hidden_2D  = ANNModel_1Hidden( 2, 5, 1)\nann_model_1hidden_3D  = ANNModel_1Hidden( 3, 5, 1)\nann_model_1hidden_10B = ANNModel_1Hidden(10, 5, 1)\ncriterion = nn.BCELoss()\noptimizer_1hidden = torch.optim.Adam(ann_model_1hidden.parameters(), lr=0.01)\noptimizer_1hidden_3D  = torch.optim.Adam(ann_model_1hidden_3D.parameters(),  lr=0.01)\noptimizer_1hidden_2D  = torch.optim.Adam(ann_model_1hidden_2D.parameters(),  lr=0.01)\noptimizer_1hidden_10B = torch.optim.Adam(ann_model_1hidden_10B.parameters(), lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1000):\n    y_head = ann_model_1hidden.propagateForward(tensorTrainFeatures.float())    \n    y_head_2D = ann_model_1hidden_2D.propagateForward(tensorTrainFeatures_2D.float())    \n    y_head_3D = ann_model_1hidden_3D.propagateForward(tensorTrainFeatures_3D.float())    \n    y_head_10B = ann_model_1hidden_10B.propagateForward(tensorTrainFeatures_10B.float())    \n    \n    loss_1hidden = criterion(y_head, tensorTrainLabels.float())\n    loss_1hidden_2D  = criterion(y_head_2D, tensorTrainLabels_2D.float())\n    loss_1hidden_3D  = criterion(y_head_3D, tensorTrainLabels_3D.float())\n    loss_1hidden_10B = criterion(y_head_10B, tensorTrainLabels_10B.float())\n\n    optimizer_1hidden.zero_grad()\n    optimizer_1hidden_2D.zero_grad()\n    optimizer_1hidden_3D.zero_grad()\n    optimizer_1hidden_10B.zero_grad()\n    \n    loss_1hidden.backward()\n    loss_1hidden_2D.backward()\n    loss_1hidden_3D.backward()\n    loss_1hidden_10B.backward()  \n    \n    optimizer_1hidden.step()\n    optimizer_1hidden_2D.step()\n    optimizer_1hidden_3D.step()\n    optimizer_1hidden_10B.step()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensorPredictions_1hidden     = ann_model_1hidden.predictTests( tensorTestFeatures.float() )\ntensorPredictions_1hidden_2D  = ann_model_1hidden_2D.predictTests( tensorTestFeatures_2D.float() )\ntensorPredictions_1hidden_3D  = ann_model_1hidden_3D.predictTests( tensorTestFeatures_3D.float() )\ntensorPredictions_1hidden_10B = ann_model_1hidden_10B.predictTests( tensorTestFeatures_10B.float() )\n\nnpPredictions_1hidden = tensorPredictions_1hidden.detach().numpy()\nnpPredictions_1hidden_2D  = tensorPredictions_1hidden_2D.detach().numpy()\nnpPredictions_1hidden_3D  = tensorPredictions_1hidden_3D.detach().numpy()\nnpPredictions_1hidden_10B = tensorPredictions_1hidden_10B.detach().numpy()\n\ntensorPredictions_1hidden_proba     = ann_model_1hidden.predictTestsProba( tensorTestFeatures.float() )\ntensorPredictions_1hidden_2D_proba  = ann_model_1hidden_2D.predictTestsProba( tensorTestFeatures_2D.float() )\ntensorPredictions_1hidden_3D_proba  = ann_model_1hidden_3D.predictTestsProba( tensorTestFeatures_3D.float() )\ntensorPredictions_1hidden_10B_proba = ann_model_1hidden_10B.predictTestsProba( tensorTestFeatures_10B.float() )\n\nnpPredictions_1hidden_proba = tensorPredictions_1hidden_proba.detach().numpy()\nnpPredictions_1hidden_2D_proba  = tensorPredictions_1hidden_2D_proba.detach().numpy()\nnpPredictions_1hidden_3D_proba  = tensorPredictions_1hidden_3D_proba.detach().numpy()\nnpPredictions_1hidden_10B_proba = tensorPredictions_1hidden_10B_proba.detach().numpy()\n\nacurracy_1hidden     = getAcurracy(npPredictions_1hidden, npTestLabels)\nacurracy_1hidden_2D  = getAcurracy(npPredictions_1hidden_2D,  npTestLabels_2D)\nacurracy_1hidden_3D  = getAcurracy(npPredictions_1hidden_3D,  npTestLabels_3D)\nacurracy_1hidden_10B = getAcurracy(npPredictions_1hidden_10B, npTestLabels_10B)\n\nmacro_precision_ann1, macro_recall_ann1, macro_fscore_ann1, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden, average='macro')\nmacro_precision_ann1_2D, macro_recall_ann1_2D, macro_fscore_ann1_2D, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_2D, average='macro')\nmacro_precision_ann1_3D, macro_recall_ann1_3D, macro_fscore_ann1_3D, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_3D, average='macro')\nmacro_precision_ann1_10B, macro_recall_ann1_10B, macro_fscore_ann1_10B, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_10B, average='macro')\n\nmicro_precision_ann1, micro_recall_ann1, micro_fscore_ann1, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden, average='micro')\nmicro_precision_ann1_2D,  micro_recall_ann1_2D,  micro_fscore_ann1_2D, _  = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_2D,  average='micro')\nmicro_precision_ann1_3D,  micro_recall_ann1_3D,  micro_fscore_ann1_3D, _  = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_3D,  average='micro')\nmicro_precision_ann1_10B, micro_recall_ann1_10B, micro_fscore_ann1_10B, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_1hidden_10B, average='micro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_1hidden))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_1hidden_3D))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_1hidden_2D))\nprint('ACURRACY FOR 10 BEST FEATURES: ' + str(acurracy_1hidden_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_ann1))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_ann1_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_ann1_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_ann1_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_ann1))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_ann1_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_ann1_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_ann1_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_ann1))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_ann1_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_ann1_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_ann1_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_ann1))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_ann1_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_ann1_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_ann1_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_ann1))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_ann1_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_ann1_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_ann1_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_ann1))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_ann1_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_ann1_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_ann1_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_ann1     = confusion_matrix(dfTestLabels, npPredictions_1hidden)\ncm_ann1_3D  = confusion_matrix(dfTestLabels, npPredictions_1hidden_3D)\ncm_ann1_2D  = confusion_matrix(dfTestLabels, npPredictions_1hidden_2D)\ncm_ann1_10B = confusion_matrix(dfTestLabels, npPredictions_1hidden_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann1, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for initial data\")\nplt.ylabel(\"ANN test labels for initial data\")\nplt.title(\"ANN CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann1_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for PCA 3D\")\nplt.ylabel(\"ANN test labels for PCA 3D\")\nplt.title(\"ANN CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann1_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for PCA 2D\")\nplt.ylabel(\"ANN test labels for PCA 2D\")\nplt.title(\"ANN CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann1_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for 10 Best Data\")\nplt.ylabel(\"ANN test labels for 10 Best Data\")\nplt.title(\"ANN CONFUSION MATRIX FOR 10 BEST DATA\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_ANN1,     truePositiveRate_ANN1,     thresholds_ANN1     = roc_curve(dfTestLabels, npPredictions_1hidden_proba)\nfalsePositiveRate_ANN1_3D,  truePositiveRate_ANN1_3D,  thresholds_ANN1_3D  = roc_curve(dfTestLabels, npPredictions_1hidden_3D_proba)\nfalsePositiveRate_ANN1_2D,  truePositiveRate_ANN1_2D,  thresholds_ANN1_2D  = roc_curve(dfTestLabels, npPredictions_1hidden_2D_proba)\nfalsePositiveRate_ANN1_10B, truePositiveRate_ANN1_10B, thresholds_ANN1_10B = roc_curve(dfTestLabels, npPredictions_1hidden_10B_proba)\n\n# drawing the graph\nplt.plot(falsePositiveRate_ANN1, truePositiveRate_ANN1, color='red')\nplt.plot(falsePositiveRate_ANN1_3D,  truePositiveRate_ANN1_3D,  color='green')\nplt.plot(falsePositiveRate_ANN1_2D,  truePositiveRate_ANN1_2D,  color='blue')\nplt.plot(falsePositiveRate_ANN1_10B, truePositiveRate_ANN1_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for ANN with 1 Hidden Layer Classification')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_ann_1hidden,     recall_ann_1hidden,     _ = precision_recall_curve(npTestLabels,     npPredictions_1hidden_proba)\nprecision_ann_1hidden_3D,  recall_ann_1hidden_3D,  _ = precision_recall_curve(npTestLabels_3D,  npPredictions_1hidden_3D_proba)\nprecision_ann_1hidden_2D,  recall_ann_1hidden_2D,  _ = precision_recall_curve(npTestLabels_2D,  npPredictions_1hidden_2D_proba)\nprecision_ann_1hidden_10B, recall_ann_1hidden_10B, _ = precision_recall_curve(npTestLabels_10B, npPredictions_1hidden_10B_proba)\n\nplt.plot(recall_ann_1hidden,     precision_ann_1hidden,     marker='.', color=\"red\")\nplt.plot(recall_ann_1hidden_3D,  precision_ann_1hidden_3D,  marker='.', color=\"green\")\nplt.plot(recall_ann_1hidden_2D,  precision_ann_1hidden_2D,  marker='.', color=\"blue\")\nplt.plot(recall_ann_1hidden_10B, precision_ann_1hidden_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the model for \n# ANN with 2 hidden layer\nclass ANNModel_2Hidden(nn.Module):\n    def __init__(self, inputSize, hiddenSize1, hiddenSize2, outputSize):\n        super().__init__()\n        self.model1 = nn.Linear(inputSize, hiddenSize1)\n        self.model2 = nn.Linear(hiddenSize1, hiddenSize2)\n        self.model3 = nn.Linear(hiddenSize2, outputSize)\n  \n    def propagateForward(self, x):\n        y_head1 = torch.sigmoid( self.model1(x) )\n        y_head2 = torch.sigmoid( self.model2(y_head1) )\n        y_head3 = torch.sigmoid( self.model3(y_head2) )\n        return y_head3      \n  \n    def predictTests(self, xTest):\n        predictions = self.propagateForward(xTest)\n    \n        for i in range(predictions.shape[0]):\n            if (predictions[i] > 0.5):\n                predictions[i] = 1\n            else:\n                predictions[i] = 0\n\n        return predictions\n    \n    def predictTestsProba(self, xTest):\n        return self.propagateForward(xTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model_2hidden     = ANNModel_2Hidden(30, 5, 5, 1)\nann_model_2hidden_2D  = ANNModel_2Hidden( 2, 5, 5, 1)\nann_model_2hidden_3D  = ANNModel_2Hidden( 3, 5, 5, 1)\nann_model_2hidden_10B = ANNModel_2Hidden(10, 5, 5, 1)\ncriterion = nn.BCELoss()\noptimizer_2hidden = torch.optim.Adam(ann_model_2hidden.parameters(), lr=0.01)\noptimizer_2hidden_3D  = torch.optim.Adam(ann_model_2hidden_3D.parameters(),  lr=0.01)\noptimizer_2hidden_2D  = torch.optim.Adam(ann_model_2hidden_2D.parameters(),  lr=0.01)\noptimizer_2hidden_10B = torch.optim.Adam(ann_model_2hidden_10B.parameters(), lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1000):\n    y_head = ann_model_2hidden.propagateForward(tensorTrainFeatures.float())    \n    y_head_2D = ann_model_2hidden_2D.propagateForward(tensorTrainFeatures_2D.float())    \n    y_head_3D = ann_model_2hidden_3D.propagateForward(tensorTrainFeatures_3D.float())    \n    y_head_10B = ann_model_2hidden_10B.propagateForward(tensorTrainFeatures_10B.float())    \n    \n    loss_2hidden = criterion(y_head, tensorTrainLabels.float())\n    loss_2hidden_2D  = criterion(y_head_2D, tensorTrainLabels_2D.float())\n    loss_2hidden_3D  = criterion(y_head_3D, tensorTrainLabels_3D.float())\n    loss_2hidden_10B = criterion(y_head_10B, tensorTrainLabels_10B.float())\n\n    optimizer_2hidden.zero_grad()\n    optimizer_2hidden_2D.zero_grad()\n    optimizer_2hidden_3D.zero_grad()\n    optimizer_2hidden_10B.zero_grad()\n    \n    loss_2hidden.backward()\n    loss_2hidden_2D.backward()\n    loss_2hidden_3D.backward()\n    loss_2hidden_10B.backward()  \n    \n    optimizer_2hidden.step()\n    optimizer_2hidden_2D.step()\n    optimizer_2hidden_3D.step()\n    optimizer_2hidden_10B.step()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensorPredictions_2hidden     = ann_model_2hidden.predictTests( tensorTestFeatures.float() )\ntensorPredictions_2hidden_2D  = ann_model_2hidden_2D.predictTests( tensorTestFeatures_2D.float() )\ntensorPredictions_2hidden_3D  = ann_model_2hidden_3D.predictTests( tensorTestFeatures_3D.float() )\ntensorPredictions_2hidden_10B = ann_model_2hidden_10B.predictTests( tensorTestFeatures_10B.float() )\n\nnpPredictions_2hidden = tensorPredictions_2hidden.detach().numpy()\nnpPredictions_2hidden_2D  = tensorPredictions_2hidden_2D.detach().numpy()\nnpPredictions_2hidden_3D  = tensorPredictions_2hidden_3D.detach().numpy()\nnpPredictions_2hidden_10B = tensorPredictions_2hidden_10B.detach().numpy()\n\ntensorPredictions_2hidden_proba     = ann_model_2hidden.predictTestsProba( tensorTestFeatures.float() )\ntensorPredictions_2hidden_2D_proba  = ann_model_2hidden_2D.predictTestsProba( tensorTestFeatures_2D.float() )\ntensorPredictions_2hidden_3D_proba  = ann_model_2hidden_3D.predictTestsProba( tensorTestFeatures_3D.float() )\ntensorPredictions_2hidden_10B_proba = ann_model_2hidden_10B.predictTestsProba( tensorTestFeatures_10B.float() )\n\nnpPredictions_2hidden_proba = tensorPredictions_2hidden_proba.detach().numpy()\nnpPredictions_2hidden_2D_proba  = tensorPredictions_2hidden_2D_proba.detach().numpy()\nnpPredictions_2hidden_3D_proba  = tensorPredictions_2hidden_3D_proba.detach().numpy()\nnpPredictions_2hidden_10B_proba = tensorPredictions_2hidden_10B_proba.detach().numpy()\n\nacurracy_2hidden     = getAcurracy(npPredictions_2hidden, npTestLabels)\nacurracy_2hidden_2D  = getAcurracy(npPredictions_2hidden_2D,  npTestLabels_2D)\nacurracy_2hidden_3D  = getAcurracy(npPredictions_2hidden_3D,  npTestLabels_3D)\nacurracy_2hidden_10B = getAcurracy(npPredictions_2hidden_10B, npTestLabels_10B)\n\nmacro_precision_ann2, macro_recall_ann2, macro_fscore_ann2, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden, average='macro')\nmacro_precision_ann2_2D, macro_recall_ann2_2D, macro_fscore_ann2_2D, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_2D, average='macro')\nmacro_precision_ann2_3D, macro_recall_ann2_3D, macro_fscore_ann2_3D, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_3D, average='macro')\nmacro_precision_ann2_10B, macro_recall_ann2_10B, macro_fscore_ann2_10B, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_10B, average='macro')\n\nmicro_precision_ann2, micro_recall_ann2, micro_fscore_ann2, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden, average='micro')\nmicro_precision_ann2_2D,  micro_recall_ann2_2D,  micro_fscore_ann2_2D, _  = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_2D,  average='micro')\nmicro_precision_ann2_3D,  micro_recall_ann2_3D,  micro_fscore_ann2_3D, _  = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_3D,  average='micro')\nmicro_precision_ann2_10B, micro_recall_ann2_10B, micro_fscore_ann2_10B, _ = precision_recall_fscore_support(dfTestLabels, npPredictions_2hidden_10B, average='micro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the results.\nprint('------------------------------------------------------')\nprint('ACURRACY FOR INITIAL DATA    : ' + str(acurracy_2hidden))\nprint('ACURRACY FOR PCA (3DIMENSION): ' + str(acurracy_2hidden_3D))\nprint('ACURRACY FOR PCA (2DIMENSION): ' + str(acurracy_2hidden_2D))\nprint('ACURRACY FOR 10 BEST FEATURES: ' + str(acurracy_2hidden_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO PRECISION FOR INITIAL DATA: \" + str(macro_precision_ann2))\nprint(\"MACRO PRECISION PCA (2DIMENSION): \" + str(macro_precision_ann2_2D))\nprint(\"MACRO PRECISION PCA (3DIMENSION): \" + str(macro_precision_ann2_3D))\nprint(\"MACRO PRECISION (10 BEST FEA.)  : \" + str(macro_precision_ann2_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO RECALL FOR INITIAL DATA: \" + str(macro_recall_ann2))\nprint(\"MACRO RECALL PCA (2DIMENSION): \" + str(macro_recall_ann2_2D))\nprint(\"MACRO RECALL PCA (3DIMENSION): \" + str(macro_recall_ann2_3D))\nprint(\"MACRO RECALL (10 BEST FEA.)  : \" + str(macro_recall_ann2_10B))\nprint('------------------------------------------------------')\nprint(\"MACRO FSCORE FOR INITIAL DATA: \" + str(macro_fscore_ann2))\nprint(\"MACRO FSCORE PCA (2DIMENSION): \" + str(macro_fscore_ann2_2D))\nprint(\"MACRO FSCORE PCA (3DIMENSION): \" + str(macro_fscore_ann2_3D))\nprint(\"MACRO FSCORE (10 BEST FEA.)  : \" + str(macro_fscore_ann2_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO PRECISION FOR INITIAL DATA: \" + str(micro_precision_ann2))\nprint(\"MICRO PRECISION PCA (2DIMENSION): \" + str(micro_precision_ann2_2D))\nprint(\"MICRO PRECISION PCA (3DIMENSION): \" + str(micro_precision_ann2_3D))\nprint(\"MICRO PRECISION (10 BEST FEA.)  : \" + str(micro_precision_ann2_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO RECALL FOR INITIAL DATA: \" + str(micro_recall_ann2))\nprint(\"MICRO RECALL PCA (2DIMENSION): \" + str(micro_recall_ann2_2D))\nprint(\"MICRO RECALL PCA (3DIMENSION): \" + str(micro_recall_ann2_3D))\nprint(\"MICRO RECALL (10 BEST FEA.)  : \" + str(micro_recall_ann2_10B))\nprint('------------------------------------------------------')\nprint(\"MICRO FSCORE FOR INITIAL DATA: \" + str(micro_fscore_ann2))\nprint(\"MICRO FSCORE PCA (2DIMENSION): \" + str(micro_fscore_ann2_2D))\nprint(\"MICRO FSCORE PCA (3DIMENSION): \" + str(micro_fscore_ann2_3D))\nprint(\"MICRO FSCORE (10 BEST FEA.)  : \" + str(micro_fscore_ann2_10B))\nprint('------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing out the confusion matrix.\nfrom sklearn.metrics import confusion_matrix\ncm_ann2     = confusion_matrix(dfTestLabels, npPredictions_2hidden)\ncm_ann2_3D  = confusion_matrix(dfTestLabels, npPredictions_2hidden_3D)\ncm_ann2_2D  = confusion_matrix(dfTestLabels, npPredictions_2hidden_2D)\ncm_ann2_10B = confusion_matrix(dfTestLabels, npPredictions_2hidden_10B)\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann2, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for initial data\")\nplt.ylabel(\"ANN test labels for initial data\")\nplt.title(\"ANN CONFUSION MATRIX FOR INITIAL DATA\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann2_3D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for PCA 3D\")\nplt.ylabel(\"ANN test labels for PCA 3D\")\nplt.title(\"ANN CONFUSION MATRIX FOR PCA 3D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann2_2D, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for PCA 2D\")\nplt.ylabel(\"ANN test labels for PCA 2D\")\nplt.title(\"ANN CONFUSION MATRIX FOR PCA 2D\")\nplt.show()\n\nf, ax = plt.subplots(figsize = (6, 6))\nsns.heatmap(cm_ann2_10B, annot = True, linewidths=1, linecolor='black', fmt='.0f', ax=ax)\nplt.xlabel(\"ANN predictions for 10 Best Data\")\nplt.ylabel(\"ANN test labels for 10 Best Data\")\nplt.title(\"ANN CONFUSION MATRIX FOR 10 BEST DATA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing ROC library and drawing ROC curve.\nfrom sklearn.metrics import roc_curve\n\n# finding out false positive rate and true positive rate\nfalsePositiveRate_ANN2,     truePositiveRate_ANN2,     thresholds_ANN2     = roc_curve(dfTestLabels, npPredictions_2hidden_proba)\nfalsePositiveRate_ANN2_3D,  truePositiveRate_ANN2_3D,  thresholds_ANN2_3D  = roc_curve(dfTestLabels, npPredictions_2hidden_3D_proba)\nfalsePositiveRate_ANN2_2D,  truePositiveRate_ANN2_2D,  thresholds_ANN2_2D  = roc_curve(dfTestLabels, npPredictions_2hidden_2D_proba)\nfalsePositiveRate_ANN2_10B, truePositiveRate_ANN2_10B, thresholds_ANN2_10B = roc_curve(dfTestLabels, npPredictions_2hidden_10B_proba)\n\n# drawing the graph\nplt.plot(falsePositiveRate_ANN2, truePositiveRate_ANN2, color='red')\nplt.plot(falsePositiveRate_ANN2_3D,  truePositiveRate_ANN2_3D,  color='green')\nplt.plot(falsePositiveRate_ANN2_2D,  truePositiveRate_ANN2_2D,  color='blue')\nplt.plot(falsePositiveRate_ANN2_10B, truePositiveRate_ANN2_10B, color='black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for ANN with 2 Hidden Layer Classification')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_ann_2hidden,     recall_ann_2hidden,     _ = precision_recall_curve(npTestLabels,     npPredictions_2hidden_proba)\nprecision_ann_2hidden_3D,  recall_ann_2hidden_3D,  _ = precision_recall_curve(npTestLabels_3D,  npPredictions_2hidden_3D_proba)\nprecision_ann_2hidden_2D,  recall_ann_2hidden_2D,  _ = precision_recall_curve(npTestLabels_2D,  npPredictions_2hidden_2D_proba)\nprecision_ann_2hidden_10B, recall_ann_2hidden_10B, _ = precision_recall_curve(npTestLabels_10B, npPredictions_2hidden_10B_proba)\n\nplt.plot(recall_ann_2hidden,     precision_ann_2hidden,     marker='.', color=\"red\")\nplt.plot(recall_ann_2hidden_3D,  precision_ann_2hidden_3D,  marker='.', color=\"green\")\nplt.plot(recall_ann_2hidden_2D,  precision_ann_2hidden_2D,  marker='.', color=\"blue\")\nplt.plot(recall_ann_2hidden_10B, precision_ann_2hidden_10B, marker='.', color=\"black\")\n\nplt.title(\"PRECISION - RECALL CURVE\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>FEATURE SELECTION</h3>\n<div style=\"text-align:justify;font-size:17px;\"><strong>This part is not fully done. After the progress, we are gong to integrate this part with the machine learning algorithms.</strong></div>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import SelectFromModel\n\n#Select percentile method to choose most important features\nselect = SelectPercentile(percentile=90)\n\nselect.fit(dfTrainFeatures,dfTrainLabels)\nx_train_selected = select.transform(dfTrainFeatures)\n\nselect.fit(dfTestFeatures,dfTestLabels)\nx_test_selected = select.transform(dfTestFeatures)\n\nprint(\"------------------------------------------------------------------------------------------------\")\n#-----Selecting best features with chi square statistical method\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(dfTrainFeatures,dfTrainLabels)\nx_train_selected2 = fit.transform(dfTrainFeatures)\ndf_scores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(dfTrainFeatures.columns)\n#concat two dataframes for better visualization\nfScores = pd.concat([dfcolumns,df_scores],axis=1)\nfScores.columns = ['Features','Score']\nprint(fScores.nlargest(10,'Score'))\nprint(\"------------------------------------------------------------------------------------------------\")\n\n\n#Selecting best features with desicion tree model\n\n# importing the model.\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(dfTrainFeatures,dfTrainLabels)\nprint(model.feature_importances_)\nprint(dfTrainFeatures.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.Series(model.feature_importances_, index=dfTrainFeatures.columns)\nplt.figure(figsize=(14,14))\nfeature_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}