{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_is_fork":false,"_change_revision":0,"language_info":{"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"}},"cells":[{"metadata":{"_uuid":"b069b133f76dfbf204347424204d149726f08626","_cell_guid":"d428a6a9-bd36-b30a-e491-1047634888f9"},"execution_count":null,"source":"In this kernel, I am creating a simple quote generator using the \"South Park dialogue\" corpus. Quote generators are often used as a just for fun tool - for instance in Twitter parody accounts, and indeed they can yield some funny things. ","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"9295b3db239a5389680e35d12fc4e126ae6575c5","_cell_guid":"1def0d65-d32f-d4e6-3368-ce0b5a14e13d"},"execution_count":null,"source":"# Imports","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c7b5fe7e3b2baad2884def35da51890ccfb4d27d","_cell_guid":"d0448059-c6d4-03fa-7fdc-8b087717f9d2"},"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom math import log\nfrom collections import defaultdict\nimport random","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"4025d1dd4c4769f0b6fa3b32b3f1931a047d1bd0","_cell_guid":"e84f373d-6e4d-b9aa-630a-564de978b957"},"execution_count":null,"source":"# Load the Data","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"129d2ff83efdca11614c2604a684e7e4757ecb07","_cell_guid":"e1a789cd-7c67-72e5-ab8b-613dc2c9f3d8"},"execution_count":null,"source":"quotes = pd.read_csv('../input/All-seasons.csv')","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"fd3d99da05b6d1cc5bb37ee5db5b2cad08138b56","_cell_guid":"8cc0b9b0-d229-f0bc-0ac0-793488e0515e"},"execution_count":null,"source":"# Get the Characters to Quote","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"8cd11ca6b91fbe6985198601c7da184bd687d44e","_cell_guid":"00387dd2-0abd-60b1-f7d8-be8df79a5921"},"execution_count":null,"source":"The dataset contains many characters, but most of them don't have enough entries that would enable us to create something funny. So we will just stick to those who have at least 1000 words.","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"59ae2d9079f754116394288be8d1c071da91a1ce","_cell_guid":"1303eb58-95a8-7347-8f19-5b9c08f287a4"},"execution_count":null,"source":"quotes_by_character = quotes.groupby('Character')\nquotes_by_character.count()[quotes_by_character.count().Line > 1000]","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"57267887c1c0e9779713c80917bba6210e87fa43","_cell_guid":"34e3a337-0b69-6bc5-88a2-e0d8919e4680"},"execution_count":null,"source":"We have six entries, I will not work on all of them in this kernel, I will just pick one for demonstration. Let's go with Kyle, the 3rd with most entries.","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"59aa085390f74f4c4ba8df7942d47de729c511db","_cell_guid":"343fbe8d-2c7c-850d-13cf-cfdd447703c2"},"execution_count":null,"source":"kyle_quotes = quotes[quotes.Character == \"Kyle\"].Line","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"043cf97fe835d28a5d7f8b1f20839b4c6872606c","_cell_guid":"bffb93e2-ce5b-3b43-1d5c-83f5522dc6a2"},"execution_count":null,"source":"kyle_quotes.head()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"47a6ef335e3c7a559b926122eb00f12b3782954a","_cell_guid":"2b786c50-47ce-54cc-1b68-e7fee96a5867"},"execution_count":null,"source":"## Preprocessing","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"7db6c4c8501a0a8f16858a426636620580114080","_cell_guid":"2bc044c4-e515-302b-ddec-7fe566124cc8"},"execution_count":null,"source":"It's often a good idea to put the words into lower case before starting any word counting task in NLP. Let's do that. Also, let's strip the new line characters!","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c6aa1c847ed14ce52ba82958011fdac0d57d4533","_cell_guid":"2c7b0589-e2ee-5394-c445-9e6e417d8c05"},"execution_count":null,"source":"kyle_quotes_lower = kyle_quotes.apply(str.lower).apply(str.rstrip, '\\n')","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"085130dd03b3768688ca907c84d835a547bd9bd9","_cell_guid":"8e69550f-cee6-696b-8663-53052e5b74e2"},"execution_count":null,"source":"Next thing to do is to tokenize the entries. Tokenizing means turning the sentences into a list of its constituent words. This is not an easy task as it may seem at first: for example, in the head() display of Kyle's quotes above, you can see that the question marks are written right after its preceeding words, so the task is more demanding than a mere string splitting over spaces. We will use the NLTK's built-in tokenizer to do the job:","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"66cbea3dacd2cc5e3a4b15b0f85f6c7c320008f1","_cell_guid":"787b1ea1-9815-4e8c-3f76-eff585974389"},"execution_count":null,"source":"kyle_tokens = kyle_quotes_lower.apply(nltk.word_tokenize)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"a7890853344f89249b0ed7fb9aa94bbb97cb3b1e","_cell_guid":"1729960f-dad0-1fd2-7af8-d57f7f25690f"},"execution_count":null,"source":"kyle_tokens.head()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"666db2fd486a2f2ecb8ffe60f97e3e35e9ef4eea","_cell_guid":"5ef29ff9-b453-b508-0afe-02247c391410"},"execution_count":null,"source":"As you can see, it is not perfect (For example, it made \"can't\" as two words, \"ca\" and \"'nt\". But it's good enough for our purposes.","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"7ecdf24e8400aa4d1a6c8b644f1a527abb71f0c6","_cell_guid":"834d7605-43cc-8555-c61e-c0bbe5a0c1d9"},"execution_count":null,"source":"The last thing I want to do is to transform this Pandas series into a Python list, to make life easier for us and NLTK","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"43d77c6693146443b4d9f8b1d4e6d01ce1c429eb","_cell_guid":"2ca959a6-4976-c54d-8ad6-8a17dfd1b1a3"},"execution_count":null,"source":"kyle_tokens_list =  [ word for inner_list in list(kyle_tokens) for word in inner_list]","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"851beaf8fe568bd2ffde35cf3370c21735e2caee","_cell_guid":"99a2454f-0f52-fd10-0017-efda30cac9ba"},"execution_count":null,"source":"### A Random Question: How Articulate is Kyle?","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"a13b387f96f52bd129359bdd291a081832103d02","_cell_guid":"046dc9f3-50de-3083-f056-2e357fe57de8"},"execution_count":null,"source":"I want to see his lexical diversity, does he use a lot of different words?","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"43cd703aeb5a0cea253d964d5016eb94e555b16b","_cell_guid":"b578f5c1-8d0d-efbc-4f9c-3cc0f2a43698"},"execution_count":null,"source":"kyle_lexical_diversity = len(set(kyle_tokens_list)) / len(kyle_tokens_list)\nprint(kyle_lexical_diversity)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"786de12288791195246ce28e5b0fa533d11e3626","_cell_guid":"9c180f27-9ffa-5e4b-54b6-5d57d8b0a02f"},"execution_count":null,"source":"The higher the number, the more different words he uses. But of course the number is dependent over the length of the corpus, a corpus of 5 million words would yield can different range than this one.","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"27cb6242d408a068d678b6943937d3b78c65179c","_cell_guid":"5bf721dc-a1f5-28d3-0d8b-ad5055f13d5c"},"execution_count":null,"source":"### How long are his sentences, on average?","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"7e4a82705dc33910156ffb904f1a7540f94ee5b7","_cell_guid":"54f7f7a4-b166-dae4-8ad4-82dd32ff094b"},"execution_count":null,"source":"len(kyle_tokens_list)/len(kyle_tokens)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"b749cde44d78ac32089486fb2d0e0c93c353d7f3","_cell_guid":"f15ad4bd-d808-bcca-9f1c-70f6686fe564"},"execution_count":null,"source":"On average, his sentences are 12 words long.","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"223a87fc69544031e73e541ad74ad7c61cc75bcb","_cell_guid":"e9b6179a-62a3-4373-9919-3d150ce18d53"},"execution_count":null,"source":"### How Does He Compare to the Other Characters?","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"48ad47ff6bd05a1fc20fe48b281719bf0d4c4b77","_cell_guid":"7e22c024-df72-434c-92ca-dfe2a175b47b"},"execution_count":null,"source":"I will get the data for those who have more than a 100 words, but will display only the top six","cell_type":"markdown","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"8c13e526e3b5182f15d9536e872047370b62f636","_cell_guid":"6bcdeaf7-826f-4b66-8283-b6fd2f769f32"},"execution_count":null,"source":"top_characters = quotes_by_character.count()[quotes_by_character.count().Line > 100].index","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"7952132bde3b74f117c6067f9791f4d6c27c848a","_cell_guid":"0cfa47c2-bc13-4edb-883c-1ce216423ada"},"execution_count":null,"source":"#This function will redo all the computation explained above to compute the lexical diversity and the average sentence length\ndef get_character_params(data, character):\n    character_quotes = data[data.Character == character].Line\n    character_quotes_lower = character_quotes.apply(str.lower).apply(str.rstrip, '\\n')\n    character_tokens = character_quotes_lower.apply(nltk.word_tokenize)\n    character_tokens_list =  [ word for inner_list in list(character_tokens) for word in inner_list]\n    number_of_unique_words = len(set(character_tokens_list))\n    character_lexical_diversity = number_of_unique_words / len(character_tokens_list)\n    character_avg_sentence_length = len(character_tokens_list)/len(character_tokens)\n    \n    return [len(character_tokens), len(character_tokens_list),  character_avg_sentence_length, number_of_unique_words, character_lexical_diversity]","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"05d58d282b75160a3013faa6f2678f6276ccf54f","_cell_guid":"bedf7c8d-08d7-436f-8385-9fa6a09d3a24"},"execution_count":null,"source":"top_characters_tokens = []\n\ncolumns = ['Name', 'Number of Lines','Total Word Count', \"Average Sentence Length\", 'Unique Words', \"Lexical Diversity\"]\ncharacter_quotes_parameters_df = pd.DataFrame(columns=columns)\n\nfor speaker in top_characters:\n    temp_entry_dict = {'Name':\"\", 'Number of Lines':\"\",'Total Word Count':\"\", \n                       \"Average Sentence Length\":\"\", 'Unique Words':\"\", \"Lexical Diversity\":\"\"}\n    \n    character_params = get_character_params(quotes, speaker)\n    \n    temp_entry_dict['Name'] = speaker\n    temp_entry_dict['Number of Lines'] = character_params[0]\n    temp_entry_dict['Total Word Count'] = character_params[1]\n    temp_entry_dict['Average Sentence Length'] = character_params[2]\n    temp_entry_dict['Unique Words'] = character_params[3]\n    temp_entry_dict['Lexical Diversity'] = character_params[4]\n    \n    character_quotes_parameters_df = character_quotes_parameters_df.append(temp_entry_dict, ignore_index=True)\n    \ncharacter_quotes_parameters_df.head()","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"74bbf1fa10892271da3805c588eadb294750e988","_cell_guid":"45f1fbe1-df3a-4353-a509-ba4afa2b4b2e"},"execution_count":null,"source":"Somehow the numbers seems to have a relationship of some sort, let's visualize them!","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f79e87b68d2ceb2f12aeba34fb33a48bea39c8a8","_cell_guid":"4ffe69ee-07b8-4174-b53f-7fe157a78656"},"execution_count":null,"source":"### Visualising the Result","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"0d888ad55caed7e873e2d56855472aef3115e446","_cell_guid":"ae54dbe6-1a0e-4418-857a-3a1cd90524d8"},"execution_count":null,"source":"### Lexical Diversity vs Total Word Count","cell_type":"markdown","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"e9564ef8f6c376c62ed158bb5672dfea43ab3326","_cell_guid":"e35b3a9a-9910-4329-a792-60fa716c6d93"},"execution_count":null,"source":"import matplotlib.pyplot  as plt\n\nplt.plot(character_quotes_parameters_df['Total Word Count'], character_quotes_parameters_df['Lexical Diversity'],'ro')\n\nplt.xlabel('Total Word Count', fontsize=16)\nplt.ylabel('Lexical Diversity', fontsize=16)\n\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e1aa7dc0fab96a26bf1978e26795032af83bf2ed","_cell_guid":"84b5b4a7-dede-4dec-9d5a-2f8c8a6c20fd"},"execution_count":null,"source":"An exponential decay maybe? Let's take the log of the variables:","cell_type":"markdown","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"2287c9308cb0d3fdb52b6af91d54f564929ae1e1","_cell_guid":"663c0934-9bf1-45a9-b271-fc67ba0252c3"},"execution_count":null,"source":"import matplotlib.pyplot  as plt\nimport math\nplt.plot(np.log(character_quotes_parameters_df['Total Word Count'].astype(float)), np.log(character_quotes_parameters_df['Lexical Diversity'].astype(float)),'ro')\n\nplt.xlabel('Ln Total Word Count', fontsize=16)\nplt.ylabel('Ln Lexical Diversity', fontsize=16)\n\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"15710d75b74f333b124c08db7a26db12e0963803","_cell_guid":"c14edb15-c3ca-4f07-b8c8-2d68642c10ae"},"execution_count":null,"source":"### Unique Word Count vs Total Word Count","cell_type":"markdown","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"a7f73869d8ad78f7b7c848983ba264ff87b1cb13","_cell_guid":"6d2a1b92-ded4-42d1-9da3-561d23391ac0"},"execution_count":null,"source":"import matplotlib.pyplot  as plt\n\nplt.plot(character_quotes_parameters_df['Total Word Count'], character_quotes_parameters_df['Unique Words'],'ro')\n\nplt.xlabel('Total Word Count', fontsize=16)\nplt.ylabel('Unique Word Count', fontsize=16)\n\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"9c942968e989d0a1aeffa6dfff36e749744aeb9f","_cell_guid":"305d4735-2237-4841-8981-a5f246d6246e"},"execution_count":null,"source":"import matplotlib.pyplot  as plt\n\nplt.plot(np.log(character_quotes_parameters_df['Total Word Count'].astype(float)), np.log(character_quotes_parameters_df['Unique Words'].astype(float)),'ro')\n\nplt.xlabel('Ln Total Word Count', fontsize=16)\nplt.ylabel('Ln Unique Word Count', fontsize=16)\n\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"356ed86cd01bc0b1e012776a98402cb55ec59092","_cell_guid":"6f2e0c90-b5e8-48e0-a601-1323d0f49f52"},"execution_count":null,"source":"Note: The log function in numpy is actually the natural log (ln), not the log to the base 10 or 2. I think it is amazing how this number can approximate growth in real life. So the relationship in here is that of a natural exponential growth.","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"6c0f0986278b8ed8981e76f7ae874a1d459a3450","_cell_guid":"344286d9-9be9-e366-36de-290a502a5825"},"execution_count":null,"source":"## Word Frequencies","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"a66e8a1d173faa59f1b6858ad934032b83518046","_cell_guid":"68e6a689-3b7e-9805-6efe-0c92b2a851ee"},"execution_count":null,"source":"kyle_word_freq = nltk.FreqDist(kyle_tokens_list)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"3e0d9e74037c0f1413605a71237a7edc93db5aed","_cell_guid":"4caf4dd4-1207-980b-62f2-e8fe767d850f"},"execution_count":null,"source":"kyle_word_log_probability = [ {word: -log(float(count)/ len(kyle_tokens_list))} for word, count in kyle_word_freq.items() ]","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c454d2346911c00d7928e0e13e97c306c13a7ae4","_cell_guid":"f9274b53-5b66-b426-ad94-cdd2bb14f193"},"execution_count":null,"source":"kyle_word_log_probability[:10]","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"986153826565cd3543100a662e91473cb7a332bf","_cell_guid":"95fed883-06eb-6a4b-b26b-988f50bb32d1"},"execution_count":null,"source":"## N-Grams","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"e0196b88d0860df6a66337fd7001b079359a56f4","_cell_guid":"258ecebd-0ea1-5427-8386-a17669ecd3bd"},"execution_count":null,"source":"Grams are **successive** words that appear in the text. The N in N-grams can be substituted by any number starting from one, so 1-gram is also called unigram, 2-gram = bigram, 3 = trigram ..etc. When we take a bigram for example and apply it for the following sentence:\n\n\"The quick brown fox jumps over the lazy dog\"\n\nthe bigrams that we have are:<br>\n - The quick\n - quick brown\n - brown fox\n - fox jumps ..etc\n \nThe trigrams would be:\n - The quick brown\n - quick brown fox\n - brown fox jumps ..etc\n\nA unigram is just each individual word, same as the word list we created above.\n\nSo why would we want to use that? If we create a list of bigrams with the probabilities of the following word, this can be used to mimic a person's style. So for example, if we create the bigrams of the sentence above, and see the word \"the\": it appeared twice, once followed by quick and another time followed by lazy. So if we want to create a sentence similar to this one (Which is totally boring, I know), when we encounter the word \"the\", we know that it is followed 50% of the time by \"quick\" and another 50% of the time by \"lazy\". When the corpus of text is much larger, we have a lot of different possibilities and interesting patterns of speech may be found.","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"80bd69ad32df6cef8fc9d9ea72d00ac46305937a","_cell_guid":"d5124dd9-e814-8c50-e051-8042bbc8423d"},"execution_count":null,"source":"Although NLTK offers a function to get bigrams, we want to get more to generate nicer sentences, so let's build our own n-gram builder function:","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"4ce6334e04688858bc15b9e02abbf2909fba8cd3","_cell_guid":"224333f1-9061-d36a-3f7b-296c6919608e"},"execution_count":null,"source":"#Build n-gram builder\ndef build_ngram(text, n):\n    #sanity check, but generally speaking we want the text to be much much longer than the sentence length to get\n    #interesting\\funny results\n    if len(text) < n:\n        print( \"Text length is less than n\")\n        return text\n    index = 0\n    tokenized_text = nltk.word_tokenize(text)#Try it with lower case, i.e. text.lower()\n    \n    ngram = defaultdict()\n    \n    #Loop over all text, except the last n words, since they cannot have n words after\n    for index in range(len(tokenized_text) - n):\n        #Get current word from the corpus\n        current_word = tokenized_text[index]\n        \n        #Get the next n words, so that we can push them into the current word's entry in the ngram dictionary\n        ngram_tail = \" \".join(tokenized_text[index + 1 : index+n])\n        \n        #The general structure of an entry is as follows: the beginning of the ngram is the key. Its contents is a dictionary\n        #that contains the total number of grams that are started by this first word, plus another dictionary of all the grams\n        #and their counts. To save a little space, only the tail is stored in that last dictionary. That way, we can compute \n        #easily the probability since everything needed is already stored inside\n        \n        #If this is a new entry, create a new one\n        if current_word not in ngram.keys():\n            ngram[current_word] = {\n                                     'total_grams_start' : 1, \n                                     'grams':  { ngram_tail : 1  } \n                                   }\n        else:\n            #increase the total count of grams starting with this word\n            ngram[current_word]['total_grams_start'] += 1\n            #If this ngram tail is new, create a new sub-entry with this ngram\n            if ngram_tail not in ngram[current_word]['grams'].keys():\n                ngram[current_word]['grams'][ngram_tail] = 1\n            #else, increment the entry count by one\n            else:\n                ngram[current_word]['grams'][ngram_tail] += 1\n\n    return ngram","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"1d8921ebc226391fb451393f7df85f2c5920a76c","_cell_guid":"dcd0a112-71c2-7db3-420e-ae34438679dd"},"execution_count":null,"source":"# Let the Fun Begin: Quote Generation","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"52f7e36712a19c994ba738aa316ec1c45cbfd702","_cell_guid":"b4b60f7a-1319-87df-13bf-67727e6f3e41"},"execution_count":null,"source":"def Generate_quote(grammed_input, gram_size, start_word, quote_length):\n    \n    output_str = start_word\n    \n    #This is like the seed based on which we will pick the next word.\n    current_word = start_word.lower()\n    \n    next_word = \"\"\n    \n    #iterate length by gram size times + 1. We want to iterate as much as needed to build a sentence of n size\n    for i in range(quote_length//gram_size + 1):\n        #We want some randomness in picking the next word, not just pick the highest probable next word. So we are going to\n        #set a minimum probability under which the gram is not going to get picked.\n        random_num = random.random()\n        \n        #cumulative probability\n        cum_prob = 0\n        for potential_next_word, count in grammed_input[current_word]['grams'].items():\n            #The cumulative probability is the count of this gram-tail divided by how many time the see word appeared\n            cum_prob += float(count)/grammed_input[current_word]['total_grams_start']\n            #print cum_prob, random_num\n            #If the cumulative probability has reached the minimum probability threshold, then this is the gram to use\n            if cum_prob > random_num:\n                output_str += ( \" \" + potential_next_word) \n                current_word = potential_next_word.split()[-1]\n                break\n            #else, i.e. this gram's probability is lower than our random threshold, get the next gram\n            else:\n                continue\n    # finish with an end of sentence. For now, a sentence ends with a full stop, no question\\exclamation marks.\n    # The code will continue to generate text until we encounter a gram that ends with a full stop.\n    if output_str[-1] != '.':\n        #eos = end of sentence\n        no_eos = True\n        while no_eos:\n            cum_prob = 0\n            random_num = random.random()\n            \n            for potential_next_word, count in grammed_input[current_word]['grams'].items():\n                cum_prob += float(count)/grammed_input[current_word]['total_grams_start']\n                #print cum_prob, random_num\n                if cum_prob > random_num:\n                    if '.' in potential_next_word:\n                        potential_next_word = potential_next_word.split('.')[0]\n                        output_str += ( \" \" + potential_next_word + \".\") \n                        no_eos = False\n                    else:\n                        output_str += ( \" \" + potential_next_word) \n                        current_word = potential_next_word.split()[-1]\n                    break\n                else:\n                    continue\n        \n    return output_str","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"759467da0d303ceea4ecb26b8b102b720e02fb9a","_cell_guid":"e822e7cd-66a0-aee9-68c2-fd0915dc5b03"},"execution_count":null,"source":"So now, let's generate some sentences based on different grams. Let's start with the bigram:","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"cb69b6928bdeb7511957123577a55c66937ea80a","_cell_guid":"b932a51a-d1c8-a457-7bf4-e6117be10b82"},"execution_count":null,"source":"### Bigram","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c5ae56b5c92757228d37237251c2566a205f0bb9","_cell_guid":"7e9a34f4-a5ae-92e3-03af-5d2cbc36ea04"},"execution_count":null,"source":"kyle_bigram = build_ngram(' '.join(kyle_tokens_list), 2)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"ff75ddc1839aa521190256fc648ccaa9f2aa9b32","_cell_guid":"f508b00f-4c6d-699e-c25d-8ea714c8a16f"},"execution_count":null,"source":"Generate_quote(kyle_bigram, 2, 'i', 12)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"e1082a18b61a829f4e8efc838fb394f777da2b1c","_cell_guid":"8111b5cc-bd57-6b77-6598-928f58996d22"},"execution_count":null,"source":"Some bigram sentences:\n\n - \"i know what 's illegal for being our technology really old world 's office is it ! friends were no ! aaaaah ! i 'm pretty dead .\"<br>\n - \"i 'll get him your farts in warm water ? ten ! he even be okay , he went really wrong .\"<br>\n - 'i mean , but now ! stan ? cartman ? dude okay , we would feel our hands beneath a way ! look alike .'<br>\n - 'i saw this stupid and look , and- yeah .'<br>\n - \"i 'm gon na go on ? ! nonono , we need to die ! you ! please , i 'm finally taught me .\"<br>\n - 'i saw that it pissed you sleep , there ! no jews ! aahhh .'","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"93557a150671dbe933086331b8865c636e9150f6","_cell_guid":"a4e99eaf-4baf-c755-2773-23a180ad8e28"},"execution_count":null,"source":"Sounds funny, but it doesn't make a lot of sense. Let's try the trigram and see how this would improve:","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"d8ed8d37288ea7059745a991ae160297e81ef315","_cell_guid":"16e816e2-f79f-429e-9f4e-ec96a89199ed"},"execution_count":null,"source":"### Trigrams","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"1eb5c5d4eeb0e7445ce267cec534d94e1f938c8d","_cell_guid":"5a8aca52-0299-e6f0-7518-793f18b6f522"},"execution_count":null,"source":"kyle_trigram = build_ngram(' '.join(kyle_tokens_list), 3)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"a2e9820df996e7a7dec21a085b96a4512f90aa37","_cell_guid":"6a54de43-3e4a-afcd-2771-74ea91d703f0"},"execution_count":null,"source":"Generate_quote(kyle_trigram, 3, 'i', 12)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"e899b3b98ff1768d8c592895bcf03429a836d70f","_cell_guid":"fa3ef155-b4b3-2e12-faf7-ad70f6e37329"},"execution_count":null,"source":" -  \"i guess it 's dying on our pig has used somalia .\"<br>\n - \"i 've been able to see it 's supposed to care for it is n't the same laws and you know how i said i do n't know , do n't think i say he got all right ? ! oh this is the coolest guy in there .\"<br>\n - \"i would have sex with water ! mom , dreidel , chef ! now ! '' `` giant douche .\"<br>\n - \"i 'm kyle broflovski . nothing 's in a long time ago .\"\n - \"i 'm sorry dude . i 'm getting a big performance in denver tomorrow.\"<br>\n\nNote that last one, sounds like a normal statement.","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"f167ece75b632fdb18d6b6dd7faf0e5f14dba729","_cell_guid":"0a4361b4-3b28-065a-e635-c8fa9c41a9f6"},"execution_count":null,"source":"### Quadgrams:","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"3ccd5b7eecabe119080d120bda21b0fc09bf59be","_cell_guid":"11b9fab7-5795-3325-b468-514d006d533e"},"execution_count":null,"source":"kyle_quadgram = build_ngram(' '.join(kyle_tokens_list), 4)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"28cc64c04079f8de55ab80e9b3c88ae490b1a6b2","_cell_guid":"671295ff-d2dd-e018-311d-62af7fdd0ee3"},"execution_count":null,"source":"Generate_quote(kyle_quadgram, 4, 'i', 12)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"1d0bb26804bc7f8c7377546cbdf5794039ce6609","_cell_guid":"8c6d00d1-8b76-5ed5-90e0-ea7681a2be2e"},"execution_count":null,"source":" - 'i had the same thing ; that ms. ellen was such Ã¼ber pwnage oh yeah .'<br>\n - \"i gut dragged for christ 's sake of all humanity than peace in its head ! see ? i have this strange .\"<br>\n - \"i thought ... why ? like what 's so important part of our language ! aw , come on ! hang on the left side , stan ? change our name .\"<br>\n - \"i swear to god , i hate crime ? but i i did n't eat so much better about what you 're in detention ! twenty dollars .\"<br>\n - \"i do n't care . so it either . kenny ! kenny , molestered , bad guy after all right ! yeah .\"","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"a64f63297f414ec39f03d38f5647dadd9b438cdd","_cell_guid":"ec480692-2e06-4e3b-d546-f1cdeb441540"},"execution_count":null,"source":"### Pentgrams:","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"9c1062f6629efb70e4e21dcf621de1e903b6841c","_cell_guid":"e24c7fc0-2068-6f35-33af-c13f1cfaaf0c"},"execution_count":null,"source":"kyle_pentgram = build_ngram(' '.join(kyle_tokens_list), 5)","cell_type":"code","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"8392bd20e7085738d482df4af8736c543afdc6fa","_cell_guid":"ab4d0da0-89b0-4ec1-54c0-765499ec33dd"},"execution_count":null,"source":"Generate_quote(kyle_pentgram, 5, 'i', 12)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"e50d0635529dac8ee149c92acf9ca26301c391e8","_cell_guid":"8a79707a-86e5-f5a9-5956-2abddb0bb727"},"execution_count":null,"source":" - \"i am never sucking your wiener in his mouth , and i 've got the doll ! they got it working on our school project the movies on cartman ! goddammit cartman , check it out .\"<br>\n - \"i ca n't stop feeling this might be the fuck is wrong with a pig .\"<br>\n - \"i do n't believe it , that was n't gon na work now go fund yourself .\"\n - \"i guess . look , it 's true . i wan na be cool .\"<br>\n - \"i am not , not scary , fatass ! so that 's why bad ? because they doing thiiis ? ! and what the hell is wrong with them ! hello , everybodyyy .\"","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"68138577b90e5de8cda4f60fdb59681b34b6dcc5","_cell_guid":"86e4a628-3259-2dac-3c44-c559dc445c13"},"execution_count":null,"source":"A Richer Corpus can create nicer sentences. Here is an example of sentences generated offline using the same code based on Shakespeare's work, using a quadgram:","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"e1f8e4ba5643f7044e46e0ca595604534a3a6c3a","_cell_guid":"3ca759fe-da0f-00e9-529a-0bf6b2cf6500"},"execution_count":null,"source":"\"I ' faith , yet needful 't is gone , Achilles . Then call them to the alehouse with us . Come , kiss ; And on the place , it concluded , No , sir , thou art overthrown by noble Brutus .\"","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"224bf998aa334b3d0dffae1cad0ae3541a8ccc78","_cell_guid":"0126c107-2c33-6a9a-2b44-54da09bf70b4"},"execution_count":null,"source":"# Conclusion","cell_type":"markdown","outputs":[]},{"metadata":{"_uuid":"e43827ae860bc51ffa0fcc4a1cf53dd2bf425bf2","_cell_guid":"a8c0f6c5-552a-0e38-6ed9-9cd371f4c0c2"},"execution_count":null,"source":"There is something that I want to point to before wrapping-up this kernel:<br>\n\nThe higher the number of grams you will use:<br>\n  - The longer the time it will take to generate the probabilities, so beware of that. Doing a pentgram on Shakespeare's work is going to take a while to finish<br>\n  - The more the sentences are going to be exact copies from the original text. So using a 12-gram to generate a sentence of length of 12 can give back an original sentence, unchanged (The only variation is from the random number generated to pick the next word).\n","cell_type":"markdown","outputs":[]}],"nbformat":4,"nbformat_minor":0}