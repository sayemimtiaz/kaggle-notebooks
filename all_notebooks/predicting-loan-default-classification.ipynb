{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=+4 color=\"Black\"><center><b>Vehicle Loan Default Prediction</b></center></font>\n<font size=-1 color=\"Black\"><center><b>Classification Models </b></right></font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work</b></font>\n"},{"metadata":{},"cell_type":"markdown","source":"### About this notebook\n\n## Banking industry is using classification models to predict the default or raise red flags on the accounts which are likely to default. One major problem is of Imbalanced Distribution of classes i.e. the available training data consists more of non-default accounts vs. default accounts. Here is my take on tackling such a problem.\n## This notebook covers various EDA, Outlier treatment, PCA, Variable Inspection, Classification models etc.\n\n\n![](https://media3.giphy.com/media/mnptBuEshROyk/giphy.gif)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Plenty of times we get data where there is imbalanced class distribution. This notebooks covers the existing methodologies to tackle such problems.[](http://)"},{"metadata":{},"cell_type":"markdown","source":"Contents:\n\n* [1. Importing data & libraries](#1)\n* [2. Variable Inspection](#2)\n* [3. EDA](#3)\n    * [3.1 Class Distribution](#4)\n    * [3.2 Default Timeline](#5)\n    * [3.3 Univariate analysis](#6)  \n    * [3.4 Outlier Treatment](#7)\n    * [3.5 Feature Selection](#8)\n        * [3.5.1 Standardization of data](#9)\n        * [3.5.2 Dummy insertion](#10)\n* [4. Models](#11)\n    * [4.1 Logistic Regression](#12)\n    * [4.2 Random Forest](#13)\n    * [4.3 Naive Bayes](#14)\n    * [4.4 Stochastic Gradient Descent](#15)\n    * [4.5 Decision Tree Classifier](#16)\n    * [4.6 XG Boost](#17)\n* [5. Dealing with Imbalanced Data](#18)\n    * [5.1 SMOTE](#19)\n    * [5.2 Upsampling](#20)\n    * [5.3 Downsampling](#21)\n    * [5.4 PCA](#22)\n    * [5.1 Resampling](#23)"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"1\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Importing data & libraries</b></font><br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport itertools\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"        \ntest = pd.read_csv('/kaggle/input/vehicle-loan-default-prediction/test.csv')\ntrain = pd.read_csv('/kaggle/input/vehicle-loan-default-prediction/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df= pd.concat ([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Shape of dataframe\nprint(\" Shape of training dataframe: \", train.shape)\nprint(\" Shape of testing dataframe: \", test.shape)\n# Drop duplicates\ntrain.drop_duplicates()\ntest.drop_duplicates()\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"2\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Variable Inspection</b></font><br>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Names of columns \", list(train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Null values in training dataset\n\nnull= train.isnull().sum().sort_values(ascending=False)\ntotal =train.shape[0]\npercent_missing= (train.isnull().sum()/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data.sort_values(by ='Total missing', ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Null values in test dataset\n\nnull= test.isnull().sum().sort_values(ascending=False)\ntotal =test.shape[0]\npercent_missing= (test.isnull().sum()/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data.sort_values(by ='Total missing', ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Flag 1: 3443 missing values in employment type"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_null_unique= train.EMPLOYMENT_TYPE .unique()\ntest_null_unique= test.EMPLOYMENT_TYPE .unique()\nprint(train_null_unique)\nprint (test_null_unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.EMPLOYMENT_TYPE= train.EMPLOYMENT_TYPE.fillna(\"Missing\")\ntest.EMPLOYMENT_TYPE= test.EMPLOYMENT_TYPE .fillna(\"Missing\")\ntrain_null_unique= train.EMPLOYMENT_TYPE .unique()\ntest_null_unique= test.EMPLOYMENT_TYPE .unique()\nprint(train_null_unique)\nprint (test_null_unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Flag 2:\n#### AVERAGE_ACCT_AGE, CREDIT_HISTORY_LENGTH are object, but they should be int. \n#### DATE_OF_BIRTH & DISBURSAL_DATE should be datetime type\n\n#### Lets' check..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing AVERAGE_ACCT_AGE & CREDIT_HISTORY_LENGTH\n\ntrain[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = train['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntrain[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = train['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntrain[\"AVERAGE_ACCT_AGE\"]= train[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((train[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))/12)\ntrain= train.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntest[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = test['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntest[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = test['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntest[\"AVERAGE_ACCT_AGE\"]= test[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((test[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))/12)\ntest= test.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntrain[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = train['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntrain[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = train['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntrain[\"CREDIT_HISTORY_LENGTH\"]= train[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((train[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))/12)\ntrain= train.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])\n\ntest[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = test['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntest[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = test['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntest[\"CREDIT_HISTORY_LENGTH\"]= test[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((test[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))/12)\ntest= test.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['DATE_OF_BIRTH'] =  pd.to_datetime(train['DATE_OF_BIRTH'], format='%d-%m-%Y')\n#format='%d%b%Y:%H:%M:%S.%f'\ntest['DATE_OF_BIRTH'] =  pd.to_datetime(test['DATE_OF_BIRTH'], format='%d-%m-%Y')\ntrain['DISBURSAL_DATE'] =  pd.to_datetime(train['DISBURSAL_DATE'], format='%d-%m-%Y')\ntest['DISBURSAL_DATE'] =  pd.to_datetime(test['DISBURSAL_DATE'], format='%d-%m-%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#remove_n = 128000\n#temp= train[train['LOAN_DEFAULT']==0]\n#drop_indices = np.random.choice(temp.index, remove_n, replace=False)\n#df_temp = temp.drop(drop_indices)\n#temp2= train[train['LOAN_DEFAULT']==1]\n#del train\n#train= pd.concat ([df_temp, temp2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"3\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. EDA</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"4\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.1 Class Distribution</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df = train.groupby('LOAN_DEFAULT').count()['UNIQUEID'].reset_index().sort_values(by='UNIQUEID',ascending=False)\nclass_df.style.background_gradient(cmap='winter')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Graph\nmy_pal = {0: 'deepskyblue', 1: 'deeppink'}\n\nplt.figure(figsize = (12, 6))\nax = sns.countplot(x = 'LOAN_DEFAULT', data = train, palette = my_pal)\nplt.title('Class Distribution')\nplt.show()\n\n# Count and %\nCount_Normal_transacation = len(train[train['LOAN_DEFAULT']==0])\nCount_Fraud_transacation = len(train[train['LOAN_DEFAULT']==1]) \nPercentage_of_Normal_transacation = Count_Normal_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of no defaults       :', Percentage_of_Normal_transacation*100)\nprint('Number of no defaults     :', Count_Normal_transacation)\nPercentage_of_Fraud_transacation= Count_Fraud_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of defaults         :',Percentage_of_Fraud_transacation*100)\nprint('Number of defaults    :', Count_Fraud_transacation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Flag : Uneven class"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(\"Employment type\\n\")\nprint(train.groupby([\"EMPLOYMENT_TYPE\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Mobile Flag\\n\")\nprint(train.groupby([\"MOBILENO_AVL_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Aadhar Flag\\n\")\nprint(train.groupby([\"AADHAR_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Pan Flag\\n\")\nprint(train.groupby([\"PAN_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Voter ID Flag\\n\")\nprint(train.groupby([\"VOTERID_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Driving L Flag\\n\")\nprint(train.groupby([\"DRIVING_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Passport\\n\")\nprint(train.groupby([\"PASSPORT_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nprint(train.groupby([\"LOAN_DEFAULT\",\"EMPLOYMENT_TYPE\",\"AADHAR_FLAG\",\"PAN_FLAG\",\"DRIVING_FLAG\",\"PASSPORT_FLAG\"]).VOTERID_FLAG.value_counts(normalize=False))\nprint(\"##############\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_0 = train[train[\"LOAN_DEFAULT\"]==0]\ntrain_1 = train[train[\"LOAN_DEFAULT\"]==1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"5\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.2 Default vs Disbursal date</b></font><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))\n\nbins = 50\n\nax1.hist(train.DISBURSAL_DATE[train.LOAN_DEFAULT == 1], bins = bins, color = 'deeppink')\nax1.set_title('Default')\n\nax2.hist(train.DISBURSAL_DATE[train.LOAN_DEFAULT == 0], bins = bins, color = 'deepskyblue')\nax2.set_title('No default')\n\nplt.xlabel('DISBURSAL DATE')\nplt.ylabel('Number of Loans')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"6\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.3 Univariate analysis</b></font><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot distribution of one feature\ndef plot_distribution(feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(train[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()\n    \n# Plot distribution of multiple features, with TARGET = 1/0 on the same graph\ndef plot_distribution_comp(var,nrow=2):\n    \n    i = 0\n    t1 = train.loc[train['LOAN_DEFAULT'] != 0]\n    t0 = train.loc[train['LOAN_DEFAULT'] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"LOAN_DEFAULT = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"LOAN_DEFAULT = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_bar_comp(var,nrow=2):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.countplot(train[feature])\n        sns.countplot(train[feature])\n        plt.ylabel('Count plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Box Plot for one feature\ndef plot_box(feature, color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Box Plot of %s\" % feature)\n    sns.boxplot(train[feature].dropna(),color=color)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bar Plot for one feature\ndef plot_bar(feature):\n    plt.figure(figsize=(10,50))\n    sns.catplot(y=feature, hue=\"LOAN_DEFAULT\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look into variables with high importance\n### Loan information"},{"metadata":{},"cell_type":"markdown","source":"###  'DISBURSED_AMOUNT' : Amount of Loan disbursed"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.DISBURSED_AMOUNT.describe())\nplot_distribution('DISBURSED_AMOUNT','green')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_box(\"DISBURSED_AMOUNT\", \"green\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since there are outliers - either we can treat outliers or do the binning\n#### Here, we have the liberty of performing both and comparing their results"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"7\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.4 Outlier Treatment </b></font><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Number of observations in column\nobs = len(train.DISBURSED_AMOUNT)\nprint(\"No. of observations in column: \",obs)\n\n# calculate summary statistics\ndata_mean, data_std = mean(train.DISBURSED_AMOUNT), std(train.DISBURSED_AMOUNT)\nprint('Statistics: Mean=%.3f, Std dev=%.3f' % (data_mean, data_std))\n# identify outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n# identify outliers\noutliers = [x for x in train.DISBURSED_AMOUNT if x < lower or x > upper]\nprint('Identified outliers: %d' % len(outliers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_outlier(x):\n    if x <= lower:\n        return(data_mean)\n    elif x>= (upper):\n        return(data_mean)\n    else:\n        return(x)\ntrain[\"DISBURSED_AMOUNT_new\"]= train[\"DISBURSED_AMOUNT\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.DISBURSED_AMOUNT_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binning\n#### mean      54356.993528\n#### std       12971.314171\n#### min       13320.000000\n#### 25%       47145.000000\n#### 50%       53803.000000\n#### 75%       60413.000000\n#### max      990572.000000"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['DISBURSED_AMOUNT_bins'] = pd.qcut(train['DISBURSED_AMOUNT'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['DISBURSED_AMOUNT_bins'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_bar(\"DISBURSED_AMOUNT_bins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'ASSET_COST' : Payment default in the first EMI on due date"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.ASSET_COST.describe().astype(str))\nplot_distribution('ASSET_COST','tomato')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_box(\"ASSET_COST\", \"tomato\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Number of observations in column\ndef outlier_data(df, feature):\n    #Number of observations in column\n    obs = len(df[feature])\n    print(\"No. of observations in column: \",obs)\n\n    # calculate summary statistics\n    data_mean, data_std = mean(df[feature]), std(df[feature])\n    print('Statistics: Mean=%.3f, Std dev=%.3f' % (data_mean, data_std))\n    # identify outliers\n    cut_off = data_std * 3\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    # identify outliers\n    outliers = [x for x in df[feature] if x < lower or x > upper]\n    print('Identified outliers: %d' % len(outliers))\n\ndef impute_outlier(x):\n    if x <= lower:\n        return(data_mean)\n    elif x>= (upper):\n        return(data_mean)\n    else:\n        return(x)\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_data(train,\"ASSET_COST\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"ASSET_COST_new\"]= train[\"ASSET_COST\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.DISBURSED_AMOUNT_new))\noutlier_data(train,\"ASSET_COST_new\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binning\n#### mean     75865.06814380195\n#### std      18944.78128866517\n#### min                37000.0\n#### 25%                65717.0\n#### 50%                70946.0\n#### 75%               79201.75\n#### max              1628992.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['ASSET_COST_bins'] = pd.qcut(train['ASSET_COST'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['ASSET_COST_bins'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(\"ASSET_COST_bins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LTV"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.LTV.describe().astype(str))\nplot_distribution('LTV','blue')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nplot_box(\"LTV\", \"blue\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_data(train,\"LTV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"LTV_new\"]= train[\"LTV\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.LTV_new))\noutlier_data(train,\"LTV_new\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binning\n#### mean      74.74653001879038\n#### std      11.456635738792304\n#### min                   10.03\n#### 25%                   68.88\n#### 50%                    76.8\n#### 75%                   83.67\n#### max                    95.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['LTV_bins'] = pd.qcut(train['LTV'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['LTV_bins'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(\"LTV_bins\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'PERFORM_CNS_SCORE': Bureau Score\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.PERFORM_CNS_SCORE.describe().astype(str))\nplot_distribution('PERFORM_CNS_SCORE','blue')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box(\"PERFORM_CNS_SCORE\", \"blue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_data(train,\"PERFORM_CNS_SCORE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbin_labels = [\"No History\",'Very Low', \"Low\" ,'Medium', 'High']\ncut_bins = [-1,10,150, 350, 650, 1000]\n\ntrain['PERFORM_CNS_SCORE_bins'] = pd.cut(train['PERFORM_CNS_SCORE'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PERFORM_CNS_SCORE_bins'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(\"PERFORM_CNS_SCORE_bins\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train.groupby([\"PERFORM_CNS_SCORE_DESCRIPTION\"]).PERFORM_CNS_SCORE_bins.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PERFORM_CNS_SCORE_DESCRIPTION"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train.PERFORM_CNS_SCORE_DESCRIPTION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"g = train.groupby(\"PERFORM_CNS_SCORE_DESCRIPTION\")['LOAN_DEFAULT']\ngg = pd.concat([g.value_counts(), \n                g.value_counts(normalize=True).mul(100)],axis=1, keys=('counts','percentage'))\nprint (gg)\n\n#train.groupby(\"PERFORM_CNS_SCORE_DESCRIPTION\").LOAN_DEFAULT.value_counts(normalize=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PRI_NO_OF_ACCTS : count of total loans taken by the customer at the time of disbursement\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"print(train.PRI_NO_OF_ACCTS .describe().astype(str))\nplot_distribution('PRI_NO_OF_ACCTS','blue')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_box(\"PRI_NO_OF_ACCTS\", \"blue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_data(train,\"PRI_NO_OF_ACCTS\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"PRI_NO_OF_ACCTS_new\"]= train[\"PRI_NO_OF_ACCTS\"].apply(impute_outlier)\noutlier_data(train,\"PRI_NO_OF_ACCTS_new\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbin_labels = [\"One\",'More than One']\ncut_bins = [-1,1, 1000]\n\ntrain['PRI_NO_OF_ACCTS_bins'] = pd.cut(train['PRI_NO_OF_ACCTS'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PRI_NO_OF_ACCTS_bins'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(\"PRI_NO_OF_ACCTS_bins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PRI_OVERDUE_ACCTS: count of default accounts at the time of disbursement\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.PRI_OVERDUE_ACCTS.describe().astype(str))\nplot_box(\"PRI_OVERDUE_ACCTS\", \"blue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_data(train,\"PRI_OVERDUE_ACCTS\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"PRI_OVERDUE_ACCTS_new\"]= train[\"PRI_OVERDUE_ACCTS\"].apply(impute_outlier)\noutlier_data(train,\"PRI_OVERDUE_ACCTS_new\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_labels = [\"None\",'One (or more)']\ncut_bins = [-1,0, 1000]\n\ntrain['PRI_OVERDUE_ACCTS_bins'] = pd.cut(train['PRI_OVERDUE_ACCTS'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PRI_OVERDUE_ACCTS_bins'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(\"PRI_OVERDUE_ACCTS_bins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's look into data with lesser importance\n#### MOBILENO_AVL_FLAG : if Mobile no. was shared by the customer then flagged as 1\n#### AADHAR_FLAG : if aadhar was shared by the customer then flagged as 1\n#### PAN_FLAG : if pan was shared by the customer then flagged as 1\n#### VOTERID_FLAG : if voter  was shared by the customer then flagged as 1\n#### PASSPORT_FLAG : if DL was shared by the customer then flagged as 1\n#### DRIVING_FLAG : if passport was shared by the customer then flagged as 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = ['MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'PASSPORT_FLAG', 'DRIVING_FLAG']\nplot_bar_comp(var,nrow=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Employment Type\nsns.catplot(data=train,kind='count',x='EMPLOYMENT_TYPE',hue='LOAN_DEFAULT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age is in days"},{"metadata":{"trusted":true},"cell_type":"code","source":"now = pd.Timestamp('now')\n#train['DATE_OF_BIRTH'] = train['DATE_OF_BIRTH'].where(train['DATE_OF_BIRTH'] < now, train['DATE_OF_BIRTH'] -  np.timedelta64(100, 'Y'))   \ntrain['age'] = (now - train['DATE_OF_BIRTH'])  \n\ntrain['age']= train['age'].astype(str)\ntrain[['age','age_waste']] = train['age'].str.split(\"days\",expand=True)\ntrain['age']= train['age'].astype(str).astype(int)\ntrain= train.drop(columns= ['age_waste'])\n\nprint(train['age'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['disbursal_time'] = (now - train['DISBURSAL_DATE'])  \n\ntrain['disbursal_time']= train['disbursal_time'].astype(str)\ntrain[['disbursal_time','disbursal_time_waste']] = train['disbursal_time'].str.split(\"days\",expand=True)\ntrain['disbursal_time']= train['disbursal_time'].astype(str).astype(int)\ntrain= train.drop(columns= ['disbursal_time_waste'])\n\nprint(train['disbursal_time'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MANUFACTURER_ID\nsns.catplot(data=train,kind='count',x='MANUFACTURER_ID',hue='LOAN_DEFAULT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(data=train,kind='count',x='BRANCH_ID',hue='LOAN_DEFAULT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see the new columns along with the less important continous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = ['PRI_NO_OF_ACCTS_new', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS_new', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT']\nplot_distribution_comp(var,nrow=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = ['SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT']\nplot_distribution_comp(var,nrow=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"8\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5 Feature Selection</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Useless features\n#train = train.drop(['DISBURSED_AMOUNT','ASSET_COST', 'LTV', 'PRI_NO_OF_ACCTS','PRI_OVERDUE_ACCTS', 'DATE_OF_BIRTH', 'DISBURSAL_DATE', 'STATE_ID', 'EMPLOYEE_CODE_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID','BRANCH_ID'],axis=1)\ntrain = train.drop([ 'DATE_OF_BIRTH' , 'STATE_ID', 'EMPLOYEE_CODE_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID','BRANCH_ID'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Highly Correlated\nsns.set()\n\ncols = train[['PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT', \n              'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n              'SEC_DISBURSED_AMOUNT',  'PRI_NO_OF_ACCTS_new', 'PRI_OVERDUE_ACCTS_new']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, annot=True, vmax=.8, square=True,cmap = 'YlGnBu');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Not highly correlated with anyone: 'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE','PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT','SEC_OVERDUE_ACCTS'\n#### 'PRI_NO_OF_ACCTS_new', 'PRI_OVERDUE_ACCTS_new'are perfectly positively correlated and hence keeping one \n#### 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS' are highly positively correlated, hence keeping one\n####  'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT' are highly positively correlated, hence keeping one\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train.drop(['PRI_OVERDUE_ACCTS_new','SEC_ACTIVE_ACCTS','SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT' ],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Highly Correlated\nsns.set()\n\ncols = train[['SEC_INSTAL_AMT', 'PERFORM_CNS_SCORE','NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n              'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES','age', 'disbursal_time']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, annot=True, vmax=.8, square=True,cmap = 'YlGnBu');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH'are highly positively correlated and hence keeping one"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train.drop(['AVERAGE_ACCT_AGE' ],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Highly Correlated\nsns.set()\n\ncols = train[['PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PERFORM_CNS_SCORE',\n              'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',\n              'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', \n              'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES',\n              'DISBURSED_AMOUNT_new','ASSET_COST_new','LTV_new','PRI_NO_OF_ACCTS_new','age', 'disbursal_time']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(13, 10))\nsns.heatmap(corr, annot=True, vmax=.7, square=True,cmap = 'YlGnBu');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One out of 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT' \n#### One out of 'LTV_new', 'PRI_NO_OF_ACCTS_new'\n#### Eliminate 'NEW_ACCTS_IN_LAST_SIX_MONTHS'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train.drop(['PRI_SANCTIONED_AMOUNT','PRI_NO_OF_ACCTS_new','NEW_ACCTS_IN_LAST_SIX_MONTHS'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing Datasets 1) Binned Variables 2) Continous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_con = train[['EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',\n                   'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE', 'PERFORM_CNS_SCORE_DESCRIPTION', \n                   'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', \n                   'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', \n                   'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES',\n                   'LOAN_DEFAULT', 'DISBURSED_AMOUNT_new', 'ASSET_COST_new', \n                   'LTV_new', 'age', 'disbursal_time']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bin = train [['UNIQUEID', 'EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', \n                    'VOTERID_FLAG', 'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE', \n                    'PERFORM_CNS_SCORE_DESCRIPTION', 'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE',\n                    'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',\n                    'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n                    'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES', 'LOAN_DEFAULT',\n                    'DISBURSED_AMOUNT_bins', 'ASSET_COST_bins', 'LTV_bins',\n                    'PERFORM_CNS_SCORE_bins', 'PRI_NO_OF_ACCTS_bins', 'PRI_OVERDUE_ACCTS_bins', 'age', 'disbursal_time']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Confusion Matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Precision, Recall, F1 Score\ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n    plt.xlim([0.0,0.001])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#feature importance plot\ndef plot_feature_importance(model):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (15,8))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"9\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5.1 Standardization of data</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_data = StandardScaler() \ndef scaleColumns(df, cols_to_scale):\n\n    for col in cols_to_scale:\n\n        df[col] = pd.DataFrame(scaler_data.fit_transform(pd.DataFrame(train_con[col])),columns=[col])\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_df = scaleColumns(train_con,['PERFORM_CNS_SCORE','PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', \n                                 'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', \n                                 'SEC_CURRENT_BALANCE', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n                                 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', \n                                 'NO_OF_INQUIRIES', 'DISBURSED_AMOUNT_new',\n                                 'ASSET_COST_new', 'LTV_new', 'age', 'disbursal_time'])\n\nscaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"10\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5.2 Dummy insertion</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummy = pd.get_dummies(scaled_df, prefix_sep='_', drop_first=True)\ntrain_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"np.any(np.isnan(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X = X.fillna(0)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"11\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Base Line Models</b></font><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"12\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.1 Logistic Regression</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression() \nlogmodel.fit(X_train,y_train)\nlogpred = logmodel.predict(X_test)\n\n\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, logpred))\nprint(\"F1 Score \",f1_score(y_test, logpred))\nprint(\"Recall Score \",recall_score(y_test, logpred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, logpred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is good, however the model is not predicting the Defaults well"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"13\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.2 Random Forest</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is good, however the model is predicting the Defaults better than Logistic reg\n"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"14\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.3 Naive Bayes</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# train model\nnb = GaussianNB().fit(X_train, y_train)\n\n# predict on test set\nnb_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test, nb_pred))\nprint(round(accuracy_score(y_test, nb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, nb_pred))\nprint(\"F1 Score \",f1_score(y_test, nb_pred))\nprint(\"Recall Score \",recall_score(y_test, nb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, nb_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model accuracy is very poor"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"15\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.4 Stochastic Gradient Descent</b></font><br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is good, however the model is not predicting the Defaults well"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"16\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.5 Decision Tree Classifier</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, dtree_pred))\nprint(\"F1 Score \",f1_score(y_test, dtree_pred))\nprint(\"Recall Score \",recall_score(y_test, dtree_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, dtree_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is good, however the model is not predicting the Defaults well"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"17\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.6 XG Boost</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# XG Boost\n\nfrom xgboost import XGBClassifier\n\n# train model\nxgb = XGBClassifier().fit(X_train, y_train)\n\n# predict on test set\nxgb_pred = xgb.predict(X_test)\nprint(confusion_matrix(y_test, xgb_pred))\nprint(round(accuracy_score(y_test, xgb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, xgb_pred))\nprint(\"F1 Score \",f1_score(y_test, xgb_pred))\nprint(\"Recall Score \",recall_score(y_test, xgb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, xgb_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is good, however the model is not predicting the Defaults well"},{"metadata":{},"cell_type":"markdown","source":"## Best model is Random Forest till now"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"18\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Dealing with Imbalanced data</b></font><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":" <a id=\"19\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.1 SMOTE</b></font><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"SMOTE or Synthetic Minority Oversampling Technique is used to create synthetic data. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_classification\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, dtree_pred))\nprint(\"F1 Score \",f1_score(y_test, dtree_pred))\nprint(\"Recall Score \",recall_score(y_test, dtree_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, dtree_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The accuracy of RF might have fone down by 7% but is predicting defaults better now. "},{"metadata":{},"cell_type":"markdown","source":" <a id=\"20\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.2 Upsampling</b></font><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Upsampling can be defined as adding more copies of the minority class. Upsampling can be a good choice when you dont have a ton of data to work with. (Not a good choice here though)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.LOAN_DEFAULT.value_counts()\n\ny_train = upsampled.LOAN_DEFAULT\nX_train = upsampled.drop('LOAN_DEFAULT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"21\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.3 Downsampling</b></font><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downsample\nfrom sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]\n\n# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.LOAN_DEFAULT.value_counts()\n\ny_train = downsampled.LOAN_DEFAULT\nX_train = downsampled.drop('LOAN_DEFAULT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"22\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.4 PCA</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (12,9))\nplt.plot(range(1,45), pca.explained_variance_ratio_.cumsum(), marker= 'o', linestyle='--')\nplt.title(\"Explained variance by components\")\nplt.xlabel(\"No. of components\")\nplt.ylabel(\"Cumulative variance explained\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 17)\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"23\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.5 Resampling</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check new class counts\nupsampled.LOAN_DEFAULT.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = upsampled.LOAN_DEFAULT\nX_train = upsampled.drop('LOAN_DEFAULT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Downsample"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.LOAN_DEFAULT.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = downsampled.LOAN_DEFAULT\nX_train = downsampled.drop('LOAN_DEFAULT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best result is obtained by Random Forest after deploying [ SMOTE](#19)."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Green\"><b>Related Work:</b></font>\n\n1. Deep diving into more methods to tackle the problem of Oversampling\n2. Stacking of algos\n3. Using deep learning models"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work</b></font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}