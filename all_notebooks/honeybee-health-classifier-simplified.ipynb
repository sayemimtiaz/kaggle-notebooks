{"cells":[{"metadata":{"_uuid":"fdbd53d4550fb7525004ddb8e151cb065cf5f6d1"},"cell_type":"markdown","source":"Welcome to the Honeybee Health Classifier - Simplified. This code builds and trains a neural network to classify bees in distress. After a little more than 150 epochs it achieved better than 99.5% test accuracy.\n\nThe dataset for this project consists of 5,100+ photos of bees, labeled by professional beekeepers according to the category of distress each bee is in.\n\nThe model is based on various homework solutions I submitted for an applied AI class at NCSU. It is a an example of partially-trainable knowledge transfer (KT) using VGG16, with a single, 256-node relu layer between the pretrained convolutional and the final softmax layers.\n\nBecause the source data is staged differently than for that homework, I first had to rearrange the input files into train/test/validate categorical subfolders, so that the training data can be augmented by ImageDataGenerator and fed into the network via flow_from_directory.\n\nThe source data was simplified in two ways: 1) two similar health categories (\"varrao\") were collapsed into one category; 2) a rare category (\"missing queen\") was eliminated. See \"Prep the data\" below.\n\nThe preprocessed images were unzipped into ./input/honeybees-simplified/bees-simple/bees, beforehand.\n"},{"metadata":{"id":"honpaUwhuVh3","colab_type":"text","_uuid":"4a5e03389f66eba1c016bb7e504554ee1f6c6f6c"},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"id":"9DUUUAywuM9U","colab_type":"text","_uuid":"0a731fe713f4c756b6415f4be3f54cd8f3efdca7"},"cell_type":"markdown","source":"The credit for collecting and preparing the honeybee dataset goes to Jenny Yang from Kaggle: https://www.kaggle.com/jenny18/honey-bee-annotated-images/.  Thanks Jenny!\n"},{"metadata":{"_uuid":"c4017edd9c2fdf55d57d96813ebe036a16b24aab"},"cell_type":"markdown","source":"## Prep the data\nThe kaggle data is organized as a single csv file for the labels and a single folder containing all the images. So here I reorganize the images into categorical subfolders, so that I can use ImageDataGenerator and flow_from_directory to feed the files into keras during training, validation and testing.\n\nThe original data has the following categories:\n* hive being robbed \n* healthy\n* few varrao, hive beetles \n* ant problems\n* missing queen\n* Varroa, Small Hive Beetles\n\nI decided to collapse Varroa... into few varrao, because it seems a minute, fuzzy distinction, and because I do not know anything about beekeeping.\n\nAlso I eliminated missing queen, because there are too few instances."},{"metadata":{"trusted":true,"_uuid":"e6b0471c231f2fe226a2f7e57cee326357586c6d"},"cell_type":"code","source":"# The following was run on colab.  It would need minor modifications to run it here on kaggle:\ndef prep_data():\n    df=pd.read_csv('bee_data.csv', \n                    index_col=False,  \n                    parse_dates={'datetime':[1,2]},\n                    dtype={'subspecies':'category', 'health':'category','caste':'category'})\n\n    out_dir = '/content/bees'\n    shuffle = np.random.randint(df.shape[0], size=(df.shape[0]))\n    test_size = 500\n    test_data = shuffle[:test_size]\n    validate_data = shuffle[test_size:(test_size*2)]\n    train_data = shuffle[(test_size*2):]\n\n    logprint(\"input sizes:\")\n    logprint(\"  test = \"+str(test_data.shape[0]))\n    logprint(\"  valid = \"+str(validate_data.shape[0]))\n    logprint(\"  train = \"+str(train_data.shape[0]))\n    subfolders = ['test','validate','train']\n    labels = []\n    categories = {}\n\n    #convert Varroa to few varrao:\n    def convert(rec):\n      if rec['health'] == 'Varroa, Small Hive Beetles':\n        return 'few varrao, hive beetles' \n      else:\n        return rec['health']\n    df['health'] = df.apply(lambda x: convert(x), axis=1)\n\n    #delete the missing queens:\n    missing_queens = df[df.health == 'missing queen'].index\n    logprint(\"num queens: \" + str(missing_queens.shape[0]))\n    df = df.drop(missing_queens)\n\n    #create the subfolders and build the categories dictionary:\n    count = 0\n    for health in df.health.unique():\n      category = \"category\"+str(count)\n      categories[health] = category\n      labels.append(health)\n      for subfolder in subfolders:\n        dirname = os.path.join(os.path.join(out_dir, subfolder), category)\n        if not os.path.exists(dirname):\n          os.makedirs(dirname)\n      count += 1\n\n    for key, value in categories.items():\n      logprint(\"categories[\"+key+\"] = \"+value)\n\n    #initialize a dictionary to store the counts of each sub-subfolder:\n    counts = {}\n    for s in range(len(subfolders)):\n      counts[subfolders[s]] = {}\n      for health, category in categories.items():\n        counts[subfolders[s]][category] = 0\n\n    count = 0\n    for filename in os.listdir(base_dir):\n      try:\n        #for the missing queens the dataframe lookup will throw IndexError:\n        health = df[df.file == filename].health.iloc[0]\n        category = categories[health]\n\n        fromfile = os.path.join(base_dir, filename)\n        if count < test_size:\n          subfolder = subfolders[0]\n        elif count < 2*test_size:\n          subfolder = subfolders[1]\n        else:\n          subfolder = subfolders[2]\n        counts[subfolder][category] += 1\n        todir = os.path.join(out_dir, os.path.join(subfolder, category))\n        tofile = os.path.join(todir, filename)\n        shutil.copyfile(fromfile, tofile)\n        count += 1\n      except IndexError:\n        None\n\n    totals = {}\n    for subfolder, cats in counts.items():\n      if subfolder not in totals.keys():\n        totals[subfolder] = 0\n      for category, num in cats.items():\n        logprint(\"counts[\"+subfolder+\"][\"+category+\"]: \"+str(num))\n        totals[subfolder] += counts[subfolder][category]\n    logprint(\"totals:\")\n    logprint(totals)","execution_count":null,"outputs":[]},{"metadata":{"id":"kkO8qSPKCPRJ","colab_type":"code","outputId":"d21561a6-9bd9-41dd-9e56-e5d06c7f6b8f","colab":{"base_uri":"https://localhost:8080/","height":35},"trusted":true,"_uuid":"07321df0c2fad1fc608a57bf003f17360dd32bfd"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random, datetime, os, shutil, math\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nimport os\n\nimage_size = (150, 150)\n\nbase_filename = '../input/modelcache/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nval_acc_filename = 'hbhc-simple-val_acc.h5'\nval_loss_filename = 'hbhc-simple-val_loss.h5'\nsave_filename = 'hbhc-simple-model.h5'\nhist_filename = 'hbhc-simple-hist.csv'\n\n#images in:\ninput_dir = '../input/honeybees-simplified/bees-simple/bees/'\ntrain_dir = input_dir + \"train\"\ntest_dir = input_dir + \"test\"\nvalidate_dir = input_dir + \"validate\"\n\nlog_filename = \"hbhc_log.txt\"\nlog_file = open(log_filename, \"a\")\n  \n# timestamp and then write the msg to both the console and the log file:\ndef logprint(msg):\n  msg_str = \"[\"+str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))+\"] \"+str(msg)\n  print(msg_str)\n  log_file = open(log_filename, \"a\")\n  log_file.write(msg_str+\"\\n\")\n  log_file.close()\n\nlogprint(\"Reopened log file \"+log_filename)\n\n#display a sample of bee photos in an auto-sized grid:\ndef show_bees(bzz):\n  numbees = len(bzz)\n  if numbees == 0:\n    return None\n  rows = int(math.sqrt(numbees))\n  cols = (numbees+1)//rows\n  f, axs = plt.subplots(rows, cols)\n  fig = 0\n  for b in bzz:\n    img = image.load_img(b)\n    row = fig // cols\n    col = fig % cols\n    axs[row, col].imshow(img)\n    fig += 1\n  plt.show()\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"1pJz7d-AsJs5","colab_type":"text","_uuid":"15ab42cdbc1e0b127da2c76223c4bd90b383b18f"},"cell_type":"markdown","source":"## Sample Images"},{"metadata":{"id":"H9WpeP7FjgIE","colab_type":"code","outputId":"c73cacb9-52f1-4c59-dd20-993d6bd04008","colab":{"base_uri":"https://localhost:8080/","height":351},"trusted":true,"_uuid":"1cce8fbfa042c2246b6e1e1214e4e26b0008d673"},"cell_type":"code","source":"#show some sample images:\ndir_name = os.path.join(test_dir,\"category0\")\nall_images = [os.path.join(dir_name, fname) for fname in os.listdir(dir_name)]\nshow_bees(all_images[:6])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"gu310ijJuQ1w","colab_type":"text","_uuid":"94d8f20021648bd055841446eb9578dd9afe19ef"},"cell_type":"markdown","source":"## Create and train the model"},{"metadata":{"id":"n_1ailBjYWD4","colab_type":"code","colab":{},"trusted":true,"_uuid":"c55debf33dfd84bef862d689fdf3d4f714e2ca0e"},"cell_type":"code","source":"# This is F. Chollet's VGG16 pretrained convnet, slightly modified by yours truly to adapt it to kaggle.\n\n# -*- coding: utf-8 -*-\n'''VGG16 model for Keras.\n# Reference:\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n'''\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\nfrom keras.applications.imagenet_utils import decode_predictions\nfrom keras.applications.imagenet_utils import preprocess_input\n#from keras.applications.imagenet_utils import _obtain_input_shape\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras.engine.topology import get_source_inputs\n\n\n#WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n#WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nWEIGHTS_PATH_NO_TOP = base_filename\n\ndef VGG16(weights='imagenet',\n          input_tensor=None, input_shape=None,\n          pooling=None,\n          classes=1000):\n    \"\"\"Instantiates the VGG16 architecture.\n    Optionally loads weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_data_format=\"channels_last\"` in your Keras config\n    at ~/.keras/keras.json.\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The data format\n    convention used by the model is the one\n    specified in your Keras config file.\n    # Arguments\n\n        weights: one of `None` (random initialization)\n            or \"imagenet\" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` data format)\n            or `(3, 224, 244)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 48.\n            E.g. `(200, 200, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    \"\"\"\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=48,\n                                      require_flatten=True,\n                                      data_format=K.image_data_format())\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n\n    if pooling == 'avg':\n        x = GlobalAveragePooling2D()(x)\n    elif pooling == 'max':\n        x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='vgg16')\n\n    # load weights\n\n    if weights == 'imagenet':\n        model.load_weights(WEIGHTS_PATH_NO_TOP)\n            \n        if K.backend() == 'theano':\n            layer_utils.convert_all_kernels_in_model(model)\n\n        if K.image_data_format() == 'channels_first':\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image data format convention '\n                              '(`image_data_format=\"channels_first\"`). '\n                              'For best performance, set '\n                              '`image_data_format=\"channels_last\"` in '\n                              'your Keras config '\n                              'at ~/.keras/keras.json.')\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XZzVj-V7kMHk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":18914},"outputId":"170d44b7-b572-4e95-f2b7-9009c7c9e1a1","trusted":true,"_uuid":"0285ce259324dd1821b71a1d5acee0a80df85e51"},"cell_type":"code","source":"def build_model():\n  conv_base = VGG16(weights='imagenet', \\\n                    input_shape=(image_size[0], image_size[1], 3))\n  model = models.Sequential()\n  model.add(conv_base)\n  model.add(layers.Flatten())\n  model.add(layers.Dense(256, activation='relu'))\n  model.add(layers.Dropout(rate=0.5))\n  model.add(layers.Dense(4, activation='softmax'))\n\n  conv_base.trainable = True\n  set_trainable = False\n  for layer in conv_base.layers:\n      if layer.name == 'block4_conv1':\n          set_trainable = True\n      if set_trainable:\n          layer.trainable = True\n      else:\n          layer.trainable = False\n\n  model.compile(loss='categorical_crossentropy',\n                optimizer=optimizers.RMSprop(lr=1e-5),\n                metrics=['acc'])\n  conv_base.summary()\n  return model\n  \n# return a list of callbacks for training:\ndef callbacks():\n  ckpt_val_acc = ModelCheckpoint(val_acc_filename, monitor='val_acc', save_best_only=True)\n  ckpt_val_loss = ModelCheckpoint(val_loss_filename, monitor='val_loss', save_best_only=True)\n  ckpt_model = ModelCheckpoint(save_filename, save_best_only=False)\n  save_hist = CSVLogger(hist_filename, separator=',', append=True)\n  return [ckpt_val_acc,ckpt_val_loss,ckpt_model,save_hist]\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\ntrain_flow = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=image_size,\n        batch_size=20,\n        class_mode='categorical')\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\nvalidate_flow = validation_datagen.flow_from_directory(\n        validate_dir,\n        target_size=image_size,\n        batch_size=20,\n        class_mode='categorical')\n\nif os.path.isfile(save_filename):\n    # restore saved model from prior run:\n    model = models.load_model(save_filename)\n    logprint(\"loaded model from cache: \"+save_filename)\nelse:\n    model = build_model()\n    logprint(\"no cache found: \"+save_filename)\n    \nmodel.summary()\nlogprint(\"training start (epochs limited to 1 for demo only)\")\nhistory = model.fit_generator(\n      train_flow,\n      steps_per_epoch=100,\n      epochs=1,\n      validation_data=validate_flow,\n      validation_steps=50,\n      callbacks=callbacks())\nlogprint(\"training complete\")\n","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Honey Bee Health Classifier - Simplified","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}