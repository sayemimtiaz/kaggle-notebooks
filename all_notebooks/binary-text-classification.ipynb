{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\nimport pandas as pd\nimport random, time\nfrom babel.dates import format_date, format_datetime, format_time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\n\nimport transformers, os\nfrom transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check device \n# Get the GPU device name if available.\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n\n# If we dont have GPU but a CPU, training will take place on CPU instead\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \ntorch.cuda.empty_cache()\n    \n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_fake =\"../input/fake-and-real-news-dataset/Fake.csv\"\npath_true =\"../input/fake-and-real-news-dataset/True.csv\"\n\n# Read both files\ndf_fake = pd.read_csv(path_fake)\ndf_true = pd.read_csv(path_true)\n\n# Set value  0 to fake news and value 1 to true news\ndf_fake['label'] = 0\ndf_true['label'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df_fake, df_true])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get to know the data","metadata":{}},{"cell_type":"code","source":"# get length of all the titles in the dataframe\nseq_len_premise = [len(i.split()) for i in df['title']]\n\npd.Series(seq_len_premise).hist(bins = 25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of all the text in the dataframe\nseq_len_premise = [len(i.split()) for i in df['text']]\n\npd.Series(seq_len_premise).hist(bins = 25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the count of fake and true news \nsns.countplot(df['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordclouds","metadata":{}},{"cell_type":"code","source":"# Wordcloud of text\n\n# Get stopwords\n# Define nltk stopwords in english\nstop_words = stopwords.words('english')\nstop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n\n# Get a string of all the texts available\ndata_text = \",\".join(txt.lower() for txt in df.text)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all texts',fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a string of the true texts only\ndata_text_true = \",\".join(txt.lower() for txt in df.text[df.label == 1])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text_true)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all true texts',fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a string of the fake news text only\ndata_text_fake = \",\".join(txt.lower() for txt in df.text[df.label==0])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100,\n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_text_fake)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all true texts',fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wordcloud of titles\n\n# Get a string of all the titles available\ndata_title = \",\".join(t_title.lower() for t_title in df.title)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(data_title)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in all titles',fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Term frequencies","metadata":{}},{"cell_type":"code","source":"## Check the word frequency in texts\n#\n## lemmatize text column by using a lemmatize function\n#def lemmatize_text(text):\n#    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n#\n#\n## Initialize the Lemmatizer and Whitespace Tokenizer\n#w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n#lemmatizer = nltk.stem.WordNetLemmatizer()\n#\n## Lemmatize words\n#df['text_lemmatized'] = df.text.apply(lemmatize_text)\n#df['text_lemmatized'] = df['text_lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n#\n## use explode to expand the lists into separate rows\n#wf_text = df.text_lemmatized.explode().to_frame().reset_index(drop=True)\n#\n## plot\n#sns.countplot(x='text_lemmatized', data=wf_text, order=wf_text.text_lemmatized.value_counts().iloc[:10].index)\n#plt.xlabel('Most common used words in all texts')\n#plt.ylabel('Frequency [%]')\n#plt.xticks(rotation=70)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check the word frequency in titles\n#\n## Lemmatize words\n#df['title_lemmatized'] = df.title.apply(lemmatize_text)\n#df['title_lemmatized'] = df['title_lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n#\n## use explode to expand the lists into separate rows\n#wf_title = df.title_lemmatized.explode().to_frame().reset_index(drop=True)\n#\n## plot dfe\n#sns.countplot(x='title_lemmatized', data=wf_title, order=wf_title.title_lemmatized.value_counts().iloc[:10].index)\n#plt.xlabel('Most common used words in all titles')\n#plt.ylabel('Frequency [%]')\n#plt.xticks(rotation=70)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the data","metadata":{}},{"cell_type":"code","source":"#  Preprocess train dataset\n# remove special characters from text column\ndf.text = df.text.str.replace('[#,@,&]', '')\n# Remove digits\ndf.text = df.text.str.replace('\\d*','')\n#Remove www\ndf.text = df.text.str.replace('w{3}','')\n# remove urls\ndf.text = df.text.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ndf.text = df.text.str.replace('\\s+', ' ')\n#remove all single characters\ndf.text = df.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')\n\n# Remove english stopwords\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split test and train data using 25% of the dataset for validation purposes\nx_train, x_test, y_train, y_test = train_test_split(df['text'], \n                                                      df['label'], test_size=0.25, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try transformer model","metadata":{}},{"cell_type":"code","source":"# Obtain a 10% test set from train set\nX_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n                                                    x_train, y_train, test_size=0.20, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-uncased'\nSEQ_LEN = 200\nbatch_size = 16\nepochs = 5\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nsteps_per_epoch = 50\nnum_workers = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_split(text1):\n    '''Get split of the text with 200 char lenght'''\n    l_total = []\n    l_parcial = []\n    if len(text1.split())//150 >0:\n        n = len(text1.split())//150\n    else: \n        n = 1\n    for w in range(n):\n        if w == 0:\n            l_parcial = text1.split()[:200]\n            l_total.append(\" \".join(l_parcial))\n        else:\n            l_parcial = text1.split()[w*150:w*150 + 200]\n            l_total.append(\" \".join(l_parcial))\n    return str(l_total)\n\n# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\nsplit_train_text = [get_split(t) for t in X_train_Transformer]\nsplit_valid_text = [get_split(t) for t in X_val_Transformer]\nsplit_test_text = [get_split(t) for t in x_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split_valid_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the RoBERTa tokenizer and tokenize the data\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trencoding = tokenizer.batch_encode_plus(\n  list(split_train_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\nvalencoding = tokenizer.batch_encode_plus(\n  list(split_valid_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\n\ntestencoding = tokenizer.batch_encode_plus(\n  list(split_test_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.special_tokens_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trencoding.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Class Weights","metadata":{}},{"cell_type":"code","source":"#compute the class weights\nclass_wts = compute_class_weight('balanced', np.unique(df['label'].values.tolist()), \n                                 df['label'])\n\n#print(class_wts)\n\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\n#cross_entropy  = nn.NLLLoss(weight=weights) \ncross_entropy  = nn.CrossEntropyLoss(weight=weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loadData(prep_df, batch_size, num_workers, sampler):\n    \n    return  DataLoader(\n            prep_df,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            sampler=sampler,\n            pin_memory=True\n        )\n\n## convert lists to tensors\ntrain_seq = torch.tensor(trencoding['input_ids'])\ntrain_mask = torch.tensor(trencoding['attention_mask'])\ntrain_token_ids = torch.tensor(trencoding['token_type_ids'])\ntrain_y = torch.tensor(y_train_Transformer.tolist())\n\nval_seq = torch.tensor(valencoding['input_ids'])\nval_mask = torch.tensor(valencoding['attention_mask'])\nval_token_ids = torch.tensor(valencoding['token_type_ids'])\nval_y = torch.tensor(y_val_Transformer.tolist())\n\ntest_seq = torch.tensor(testencoding['input_ids'])\ntest_mask = torch.tensor(testencoding['attention_mask'])\ntest_token_ids = torch.tensor(testencoding['token_type_ids'])\ntest_y = torch.tensor(y_test.tolist())\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n# Train Data Loader\ntraindata = loadData(train_data, batch_size, num_workers, train_sampler)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n# Val Data Loader\nvaldata = loadData(val_data, batch_size, num_workers, val_sampler)\n\n# wrap tensors\ntest_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n# sampler for sampling the data during training\ntest_sampler = SequentialSampler(test_data)\n# Val Data Loader\ntestdata = loadData(test_data, batch_size, num_workers, test_sampler)\n\n\nprint('Number of data in the train set', len(traindata))\nprint('Number of data in the validation set', len(valdata))\nprint('Number of data in the test set', len(testdata))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load BERT model","metadata":{}},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n    \n    def __init__(self, n_classes, freeze_bert=False):\n        \n        super(BERT_Arch,self).__init__()\n        # Instantiating BERT model object\n        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n        \n        # Freeze bert layers\n        if freeze_bert:\n            for p in self.bert.parameters():\n                p.requires_grad = False\n                \n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n        self.bn = nn.BatchNorm1d(768) # (768)\n        self.bert_drop_2 = nn.Dropout(0.25)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )\n        output = self.bert_drop_1(output)\n        output = self.fc(output)\n        output = self.bn(output)\n        output = self.bert_drop_2(output)\n        output = self.out(output)        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = np.unique(df['label'])\nprint('Downloading the BERT custom model...')\nmodel = BERT_Arch(len(class_names))\nmodel.to(device) # Model to GPU.\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer \n                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer \n                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\nprint('Preparing the optimizer...')\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=learning_rate)\nsteps = steps_per_epoch\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the BERT model","metadata":{}},{"cell_type":"code","source":"# function to train the bert model\ndef trainBERT():\n  \n    print('Training...')\n    model.train()\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step, batch in enumerate(traindata):\n    \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n        # clear previously calculated gradients \n        model.zero_grad()        \n        # get model predictions for the current batch\n        preds = model(sent_id, mask, token_type_ids)\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        # backward pass to calculate the gradients\n        loss.backward()\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # update parameters\n        optimizer.step()\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n        # append the model predictions\n        total_preds.append(preds)\n        \n        torch.cuda.empty_cache()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(traindata)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n    t0 = time.time()\n    \n    model.eval() # deactivate dropout layers\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(valdata):\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [t.to(device) for t in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n\n            # model predictions\n            preds = model(sent_id, mask, token_type_ids)\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds, labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n        torch.cuda.empty_cache()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(valdata) \n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# Empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n# for each epoch perform training and evaluation\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = trainBERT()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    print('Evaluation done for epoch {}'.format(epoch + 1))\n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        print('Saving model...')\n        torch.save(model.state_dict(), 'bert_weights.pt') # Save model weight's (you can also save it in .bin format)\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nTest Set...')\n\ntest_preds = []\n\nprint('Total batches:', len(testdata))\n\nfor fold_index in range(0, 3):\n    \n    print('\\nFold Model', fold_index)\n    \n    # Load the fold model\n    path_model = 'bert_weights.pt'\n    model.load_state_dict(torch.load(path_model))\n\n    # Send the model to the GPU\n    model.to(device)\n\n    stacked_val_labels = []\n    \n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n\n\n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n\n    for j, test_batch in enumerate(testdata):\n\n        inference_status = 'Batch ' + str(j + 1)\n\n        print(inference_status, end='\\r')\n\n        b_input_ids = test_batch[0].to(device)\n        b_input_mask = test_batch[1].to(device)\n        b_token_type_ids = test_batch[2].to(device)\n        b_test_y = test_batch[3].to(device)\n\n\n        outputs = model(b_input_ids, \n                        attention_mask=b_input_mask,\n                        token_type_ids=b_token_type_ids)\n\n        # Get the preds\n        preds = outputs[0]\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n        \n        # Stack the predictions.\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n            \n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n            \n    test_preds.append(stacked_val_preds)\n    \n            \nprint('\\nPrediction complete.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_preds))\nprint(test_preds[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sum the predictions of all fold models\nfor i, item in enumerate(test_preds):\n    if i == 0:\n        preds = item\n    else:\n        # Sum the matrices\n        preds = item + preds\n\n# Average the predictions\navg_preds = preds/(len(test_preds))\n\n#print(preds)\n#print()\n#print(avg_preds)\n\n# Take the argmax. \n# This returns the column index of the max value in each row.\ntest_predictions = np.argmax(avg_preds, axis=1)\n\n# Take a look of the output\nprint(type(test_predictions))\nprint(len(test_predictions))\nprint()\nprint(test_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_y = []\nfor j, test_batch in enumerate(testdata):\n    true_y.append(int(test_batch[3][0].numpy().flatten()))\nprint(true_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy and classification report \ntarget_names = ['true_y', 'predicted_y']\n\ndata = {'true_y': true_y,\n       'predicted_y': test_predictions}\n\ndf_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n\nconfusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy of BERT model', accuracy_score(true_y, test_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(true_y, test_predictions, target_names=target_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try Logistic regression","metadata":{}},{"cell_type":"markdown","source":"In this section I am going to train a Logistic Regression model by using a Pipeline containing the TfidfVectorizer and LogisticRegression. Also, I am going to apply a GridSearchCV to the Pipeline to find the best parameters for the model. This is going to find the optimal parameters, however, it's a bit time consuming.","metadata":{}},{"cell_type":"code","source":"# Create a Pipeline with the TfidfVectorizer and LogisticRegression model\nLR_pipeline = Pipeline(steps = [('tf', TfidfVectorizer()), \n                                ('lgrg', LogisticRegression())]) # initialize TfidfVectorizer and LogisticRegression\n\n\n# Create Parameter Grid\npgrid_lgrg = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n 'lgrg__class_weight' : ['balanced', None]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_lgrg = GridSearchCV(LR_pipeline, pgrid_lgrg, cv=2, n_jobs=-1, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_lgrg.fit(x_train, y_train) # Train LR model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_lgrg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Score of train set', gs_lgrg.score(x_train, y_train))\nprint('Score of test set',gs_lgrg.score(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_pred = gs_lgrg.predict(x_test) # Predict on validation data\n\ndata = {'true_y': y_test,\n       'predicted_y': LR_pred}\ndf_pred = pd.DataFrame(data, columns=['true_y','predicted_y'])\nconfusion_matrix = pd.crosstab(df_pred['true_y'], df_pred['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy of LR model', accuracy_score(y_test, LR_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, LR_pred, target_names=target_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we are obtaining a 0.99 acc by just training a Logistic Regression model. Sometimes the simplest solution is the best choice to solve a certain task if it can save us computation time. In any case, the results are very similar.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}