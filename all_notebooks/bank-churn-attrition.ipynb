{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing the libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"bank_data=pd.read_csv(\"/kaggle/input/predicting-churn-for-bank-customers/Churn_Modelling.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Entire EDA at a single go by using Pandas-Profiling ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pandas-profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nreport=ProfileReport(bank_data,title=\"profile report\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report.to_file(\"report.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_report = sweetviz.analyze([bank_data, \"data\"],target_feat='Exited')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_report.show_html('my_report.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note the following.\n* No Missing values. \n* Number of variables is 14.\n* Number of observations is 10000\n* Numerical variables are 7, categorical variables are 4, and boolean variables are 3.\n* Number of missing values is zero for all variables.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We also note the following.\n* Row number is not required for our analysis.\n* Customer Id is not required\n* Surname has high cardinality and i will remove it from the analysis part.\n* Credit score has minimal left skewed and almost normally distributed\n* Geography has 3 categories. Germany,Spain, France. so we need to encode this \n* Gender is categorical and has two categories and we need to encode this.\n* Age is slightly right skewed.\n* Tenure has zeros but no ned of any opeartion there.\n* Balance has zeros but no need of any operation there.\n* Number of products is categorical.\n* Has credit card is boolean which will explain whether the customer has credit card or not.\n* Is Active member is boolean and it says that the customer is an active member or not.\n* Estimated Salary is having its importance in the analysis.\n* Exited is boolean which is our target variable and we have whether the customer is exited or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* No Strong correlation can be seen on either sides.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"genderwise=bank_data.groupby('Gender')['Exited'].sum()\ngenderwise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=genderwise.tolist()\nlabel=['Female','Male']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['blue','orange'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Female is churning more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"geographywise=bank_data.groupby('Geography')['Exited'].sum()\ngeographywise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=geographywise.tolist()\nlabel=['France','Germany','Spain']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['blue','orange','green'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Germany the most churned.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nofproductswise=bank_data.groupby('NumOfProducts')['Exited'].sum()\nnofproductswise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=nofproductswise.tolist()\nlabel=['1','2','3','4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['orange','indigo','blue','green'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* customer using only one product are the most churned","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hascreditwise=bank_data.groupby('HasCrCard')['Exited'].sum()\nhascreditwise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(hascreditwise.tolist(), labels=['0','1'],shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['orange','yellow'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The people who are having credit card churned more.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# We first review for categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.countplot(x='Geography', hue = 'Exited',data = bank_data, palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Majority of the data is from persons from France. \n* The proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem in the areas where it has fewer clients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.countplot(x='Gender', hue = 'Exited',data = bank_data, palette=\"Set1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The proportion of female customers churning is also greater than that of male customers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.countplot(x='HasCrCard', hue = 'Exited',data = bank_data, palette=\"Set2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Majority of the customers that churned are those with credit cards.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.countplot(x='IsActiveMember', hue = 'Exited',data = bank_data, palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Relations based on the continuous data attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set3\",dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is not much difference in the credit score distribution between retained and churned customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = bank_data,dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The older customers are churning at more than the younger ones hinting to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(bank_data, col = \"Exited\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It seems younger customers tend to stick with the company more compared to older customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set2\",dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set1\",dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The bank is losing customers with significant bank balances which is likely to hit their available capital for lending.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set3\",dodge=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The number of product has no significant effect on the likelihood to churn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The salary has no significant effect on the likelihood to churn.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Target variable count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count = bank_data.Exited.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* Lets see the proportion of the imbalance of the classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class count\ncount_class_0, count_class_1 = bank_data.Exited.value_counts()\n\n# Divide by class\ndf_class_0 = bank_data[bank_data['Exited'] == 0]\ndf_class_1 = bank_data[bank_data['Exited'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\nbank_data_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(bank_data_over.Exited.value_counts())\n\nbank_data_over.Exited.value_counts().plot(kind='bar', title='Count (target)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Row number, Customer ID, Surname are not part of our analysis, So, I dont include them .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=bank_data_over.iloc[:,3:13]\ny=bank_data_over.iloc[:,13]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Geography and gender features are categorical and we need to create dumies for them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"geography=pd.get_dummies(X['Geography'], drop_first= True)\ngender=pd.get_dummies(X['Gender'], drop_first= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets concatenate the data frames","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X,geography,gender],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dropping the unwanted columns now which are Geography and Gender","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X.drop(['Geography','Gender'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets split the data to train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets do the feature scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets dive into Machine learning models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Training my models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to get the best algorithm which is giving us the best output as predictability. So, we need to try out with various ML algorithms and then we can select the best ones which wll give us the best performance. Then we can choose the baseline models and then we can proceed further by doing hyper parametr tuning to that baseline models to get the better accuracy.\n#The algorithms we are implementing is as follows\n1. Logistic Regression\n2. Naive Bayes Model.\n3. Decision tree\n4. Random forest\n5. XG Boost\n6. Ada Boost\n7. Gradient Boosting.\n\n#So, we will be comparing the accuracies and ROC AUC scores and finalise the baseline models.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(C=100,random_state=0)\nlr.fit(X_train,y_train)\ny_pred_logistic=lr.predict(X_test)\ncorrect = (y_test == y_pred_logistic).sum()\nincorrect = (y_test != y_pred_logistic).sum()\naccuracy = correct / (correct + incorrect) * 100\n\nprint('\\nPercent Accuracy: %0.1f' %accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pd.DataFrame()\nprediction['actual'] = y_test\nprediction['predicted'] = y_pred_logistic\nprediction['correct'] = prediction['actual'] == prediction['predicted']\n\nprint ('\\nDetailed results for first 20 tests:')\nprint (prediction.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nc_logistic=confusion_matrix(y_test,y_pred_logistic)\nprint(c_logistic)\n\n#Accuracy of our model.\nAccuracy_logistic=sum(np.diag(c_logistic))/(np.sum(c_logistic))\nAccuracy_logistic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred_logistic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC_AUC \nfrom sklearn.metrics import roc_auc_score, roc_curve\nlogistic_roc_auc = roc_auc_score(y_test, y_pred_logistic , average = 'macro', sample_weight = None)\nlogistic_roc_auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier_naive=GaussianNB()\n\n# Fitting the model with training data\nclassifier_naive.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_naive=classifier_naive.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_naive=confusion_matrix(y_test,y_predict_naive)\nprint(c_naive)\n\n#Accuracy of our model.\nAccuracy_naive=sum(np.diag(c_naive))/(np.sum(c_naive))\nAccuracy_naive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_naive))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC AUC\nnaive_roc_auc = roc_auc_score(y_test, y_predict_naive , average = 'macro')\nnaive_roc_auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\n# Fitting the model with training data\nclassifier_tree=classifier_tree.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_tree = classifier_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_tree=confusion_matrix(y_test,y_predict_tree)\nprint(c_tree)\n\n#Accuracy of our model.\nAccuracy_tree=sum(np.diag(c_tree))/(np.sum(c_tree))\nAccuracy_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_tree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC AUC\ntree_roc_auc = roc_auc_score(y_test, y_predict_tree , average = 'macro')\ntree_roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_label = X.columns\nimportances = classifier_tree.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"magenta\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from sklearn import tree\nfrom sklearn.tree import export_text\nplt.figure(figsize=(60,50))\ntree.plot_tree(classifier_tree,filled=True)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\n# Fitting the model with training data\nclassifier_ensemble.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_ensemble = classifier_ensemble.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_ensemble=confusion_matrix(y_test,y_predict_ensemble)\nprint(c_ensemble)\n\n#Accuracy of our model.\nAccuracy_ensemble=sum(np.diag(c_ensemble))/(np.sum(c_ensemble))\nAccuracy_ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_ensemble))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC AUC\nensemble_roc_auc = roc_auc_score(y_test, y_predict_ensemble , average = 'macro')\nensemble_roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_label = X.columns\nimportances = classifier_ensemble.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"red\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XG Boost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the XGBoost to the training set\nfrom xgboost import XGBClassifier\nclassifier_xg=XGBClassifier()\n\n# Fitting the model with training data\nclassifier_xg.fit(X_train,y_train)\n\n# Predicting the test results\ny_predict_xg= classifier_xg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_xg=confusion_matrix(y_test,y_predict_xg)\nprint(c_xg)\n\n#Accuracy of our model.\nAccuracy_xg=sum(np.diag(c_xg))/(np.sum(c_xg))\nAccuracy_xg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_xg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC AUC\nxg_roc_auc = roc_auc_score(y_test, y_predict_xg , average = 'macro')\nxg_roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_label = X.columns\nimportances = classifier_xg.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"yellow\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adaboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nclassifier_ada = AdaBoostClassifier(base_estimator = None, n_estimators = 200, learning_rate = 1.0)\n\n# Fitting the model with training data \nclassifier_ada.fit(X_train, y_train)\n\n\n# Predicting the test results\ny_predict_ada= classifier_ada.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_ada=confusion_matrix(y_test,y_predict_ada)\nprint(c_ada)\n\n#Accuracy of our model.\nAccuracy_ada=sum(np.diag(c_ada))/(np.sum(c_ada))\nAccuracy_ada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_ada))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC AUC\nada_roc_auc = roc_auc_score(y_test, y_predict_ada , average = 'macro')\nada_roc_auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GradientBoosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nclassifier_GB = GradientBoostingClassifier(loss = 'deviance', n_estimators = 200)\n\n# Fitting the model with training data \nclassifier_GB.fit(X_train, y_train)\n\n# Predicting the test results\ny_predict_GB= classifier_GB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_GB=confusion_matrix(y_test,y_predict_GB)\nprint(c_GB)\n\n#Accuracy of our model\nAccuracy_GB=sum(np.diag(c_GB))/(np.sum(c_GB))\nAccuracy_GB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_GB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC AUC\nGB_roc_auc = roc_auc_score(y_test, y_predict_GB , average = 'macro')\nGB_roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'XGBoost', 'AdaBoost', 'GradientBoosting']\naccuracy = [Accuracy_logistic, Accuracy_naive, Accuracy_tree, Accuracy_ensemble, Accuracy_xg, Accuracy_ada, Accuracy_GB]\nroc_auc = [logistic_roc_auc, naive_roc_auc, tree_roc_auc, ensemble_roc_auc, xg_roc_auc, ada_roc_auc, GB_roc_auc]\n\nmetrics = {'accuracy': accuracy, 'roc_auc': roc_auc}\ntable_metrics = pd.DataFrame(metrics, index = models)\ntable_metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets us do the cross validation approach and then move on to hyper parametr tuning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will go for the best ones to go for hyper parameter tuning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Implementing a cross-validation based approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Function that will track the mean value and the standard deviation of the metric\n\ndef cvDictGen(functions, score, X_train = X, y_train = y, cv = 5):\n    cvDict = {}\n    for func in functions:\n        cvScore = cross_val_score(func, X_train, y_train, cv = cv, scoring = score)\n        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n    \n    return cvDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [lr, classifier_naive, classifier_tree, classifier_ensemble, classifier_xg, classifier_ada, classifier_GB]\ncvD = cvDictGen(model, score = 'roc_auc')\ncvD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n1. Decision Tree\n2. Random forest\n3. xgboost\n\nAre performing well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Hyper parameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\nmin_samples_split = [5, 10]\nmin_samples_leaf = [4, 6, 8, 10]\nccp_alpha= [0.001,0.005,0.010,0.015,0.020,0.025,0.030]\n\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion':[\"entropy\", \"gini\"],\n               'ccp_alpha' : ccp_alpha\n               }\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Setting refit='AUC', refits an estimator on the whole dataset with the\n* parameter setting that has the best cross-validated AUC score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator = classifier_ensemble, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2,scoring='roc_auc',refit='AUC', random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\nrf_random.best_params_,rf_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator = classifier_ensemble, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2,scoring='accuracy', random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\nrf_random.best_params_,rf_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xgboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_child_weight = [1,3, 5, 7, 10]\ngamma = [0.1,0.2,0.3,0.4, 0.5, 1, 1.5, 2, 5]\nsubsample = [0.6, 0.8, 1.0]\ncolsample_bytree = [0.3,0.4,0.5,0.6, 0.8, 1.0]\nmax_depth = [int(x) for x in np.linspace(3, 50, num = 5)]\nmax_depth.append(None)\nsampling_method = ['uniform','gradient_based']\nlearning_rate = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ]\n\nxg_grid = {\n        'min_child_weight': min_child_weight,\n        'gamma': gamma,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'max_depth': max_depth,\n        'sampling_method' : sampling_method,\n        'learning_rate' : learning_rate\n        }\n\nprint(xg_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_random = RandomizedSearchCV(estimator = classifier_xg, param_distributions = xg_grid, n_iter = 100, cv = 3, verbose=2, random_state=42,scoring='roc_auc', n_jobs = -1)\nxg_random.fit(X_train, y_train)\nxg_random.best_params_,xg_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_random = RandomizedSearchCV(estimator = classifier_xg, param_distributions = xg_grid, n_iter = 100, cv = 3, verbose=2, random_state=42,scoring='accuracy', n_jobs = -1)\nxg_random.fit(X_train, y_train)\nxg_random.best_params_,xg_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(3, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [5, 10]\nmin_samples_leaf = [4,6,8,10]\nccp_alpha = [0.001,0.005,0.010,0.015,0.020,0.025,0.030]\n\ndecision_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion':[\"entropy\", \"gini\"],\n               'ccp_alpha' : ccp_alpha\n                }\n\nprint(decision_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_random = RandomizedSearchCV(estimator = classifier_tree, param_distributions = decision_grid, n_iter = 100, cv = 3, verbose=2,scoring='roc_auc', random_state=42, n_jobs = -1)\ndecision_random.fit(X_train, y_train)\ndecision_random.best_params_,decision_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_random = RandomizedSearchCV(estimator = classifier_tree, param_distributions = decision_grid, n_iter = 100, cv = 3, verbose=2,scoring='accuracy', random_state=42, n_jobs = -1)\ndecision_random.fit(X_train, y_train)\ndecision_random.best_params_,decision_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So, now after this XGBoost,RANDOM FOREST has taken the complete edge over the Decision Tree.","execution_count":null},{"metadata":{},"cell_type":"raw","source":"* The accuracy of the train set is 83.09% and the ROC_AUC score is 91% for random forest.\n* The accuracy of the train set is 92.58% and the ROC_AUC score is 97% for xg boost.\n* The accuracy of the train set is 77.7% and the ROC_AUC score is 85.9% for decision tree.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* After looking at the scores, we can clearly see that randome forest accuracy and ROC_AUC score is ggod to go with.\n* accuracy is 91% and roc-auc score is 97% for train data and lets check for the test data as well","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final XGBoost Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_xg=XGBClassifier(subsample= 1.0, min_child_weight= 1, max_depth= 38, gamma= 0.3, colsample_bytree= 0.3, sampling_method= 'uniform',learning_rate= 0.15)\nfinal_model_xg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = final_model_xg.predict(X_test)\nprint(final_model_xg.__class__.__name__, accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is performing well for test dataset. The accuracy turns out to be 95%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final Random Forest Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_rf=RandomForestClassifier(n_estimators= 2000, min_samples_split= 5,criterion=\"entropy\", min_samples_leaf= 6, max_features= 'log2', max_depth= 20,ccp_alpha = 0.001)\nfinal_model_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = final_model_rf.predict(X_test)\nprint(final_model_rf.__class__.__name__, accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The acuuracy is 82.29% for test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_rf.estimators_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_model_rf.estimators_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nplt.figure(figsize=(50,20))\ntree.plot_tree(final_model_rf.estimators_[2],filled= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look for the explaianbility of 2000 decision trees.\n#for i in range(len(final_model_rf.estimators_)):\n    #print(tree.export_text(final_model_rf.estimators_[i]))\n\n    \n\n##Look for the explaianability of which ever tree you wants by entering the number between 0-1999    \nprint(tree.export_text(final_model_rf.estimators_[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets concentrate on ANN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialising the ANN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the Input layer and the first hidden layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu',input_dim=11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the second hidden layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the third hidden layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the output layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform',activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compiling the ANN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the ANN to training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_history=classifier.fit(X_train,y_train,validation_split=0.33,batch_size=10,nb_epoch=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy and validation accuracy are almost similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Predicting the test set results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# confusion matrix and the metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nc=confusion_matrix(y_test,y_pred)\n\nfrom sklearn.metrics import accuracy_score\nscore=accuracy_score(y_pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(c,score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 78% accuracy is obtained.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets do the hyper parameter tuning to get the best hyper parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Embedding,Flatten,LeakyReLU,BatchNormalization\nfrom keras.activations import relu, sigmoid\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(layers,activation):\n    model=Sequential()\n    for i,nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n            \n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n    model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\nmodel=KerasClassifier(build_fn=create_model, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layers=[(20,),(40,20),(20,10),(30,20),(25,20)]\nactivations=['sigmoid','relu']\nparam_grid = dict(layers= layers,activation= activations,batch_size= [128,256],epochs= [30])\n\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_result=grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result.best_score_,grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y=grid.predict(X_test)\ny_pred=(pred_y>0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\nc1=confusion_matrix(y_pred,y_test)\nscore=accuracy_score(y_pred,y_test)\nprint(c1,score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a slight increase in the accuracy even after some hyper parameter tuning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Interpretability of the  ML models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Initialize JS For Plot\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_explainer = shap.TreeExplainer(final_model_rf)\nrf_shap_values = rf_explainer.shap_values(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Variable importance plot.â€” Global Interpretability","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* lists the most significant variables in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(rf_shap_values, X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the effects of all the features\nshap.summary_plot(rf_shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * The collective force plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(rf_explainer.expected_value[0], rf_shap_values[0], X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP Summary Plot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The SHAP values for RF explain the margin output of the model.\n\nThis summary plot replaces the typical bar chart of feature importance. It tells which features are most important, and also their range of effects over the dataset. The color allows us match how changes in the value of a feature effect the change in risk.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}