{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Linear and logistic regression modeling, a case study with the NHANES data\n\nThis notebook introduces two statistical modeling techniques: linear\nregression and logistic regression.  The focus here will be on fitting\nthese two types of models to data, using Python statistical modeling\nlibraries in the Jupyter notebook environment.  As with several previous\ncase studies in this course, here we will be analyzing the\n[NHANES](https://www.cdc.gov/nchs/nhanes/index.htm) data, allowing us\nto illustrate the use of these two regression methods for addressing\nmeaningful questions with actual data.\n\nNote that the NHANES data were collected as a designed survey, and in\ngeneral should be analyzed as such.  This means that survey design\ninformation such as weights, strata, and clusters should be accounted\nfor in any analysis using NHANES.  But to introduce how linear and\nlogistic regression are used with independent data samples, or with convenience\nsamples, we will not incorporate the survey structure of the NHANES sample\ninto the analyses conducted here.\n\nAs with our previous work, we will be using the\n[Pandas](http://pandas.pydata.org) library for data management, the\n[Numpy](http://www.numpy.org) library for numerical calculations, and\nthe [Statsmodels](http://www.statsmodels.org) library for statistical\nmodeling.\n\nWe begin by importing the libraries that we will be using.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will load the data.  The NHANES study encompasses multiple\nwaves of data collection.  Here we will only use the\n2015-2016 data.  As with most data sets, there are some missing values\nin the NHANES files.  While many of the methods demonstrated below would handle\nmissing values automatically (at least in a crude way), here we drop\nup-front all rows with missing values in any of the key variables that\nwe will use in this notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the 2015-2016 wave of NHANES data\nda = pd.read_csv(\"../input/nhanes-2015-2016/NHANES.csv\")\n\n# Drop unused columns, and drop rows with any missing values.\nvars = [\"BPXSY1\", \"RIDAGEYR\", \"RIAGENDR\", \"RIDRETH1\", \"DMDEDUC2\", \"BMXBMI\", \"SMQ020\"]\nda = da[vars].dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear regression\n\nWe will focus initially on regression models in which systolic [blood\npressure](https://en.wikipedia.org/wiki/Blood_pressure) (SBP)\nis the outcome (dependent) variable.  That is, we will\npredict SBP from other variables.  SBP is an important indicator of\ncardiovascular health.  It tends to increase with age, is greater for\noverweight people (i.e. people with greater body mass index or BMI),\nand also differs among demographic groups, for example among gender\nand ethnic groups.\n\nSince SBP is a quantitative variable, we will model it using linear\nregression.  Linear regression is the most widely-utilized form of\nstatistical regression.  While linear regression is commonly used with\nquantitative outcome variables, it is not the only type of regression\nmodel that can be used with quantitative outcomes, nor is it the case\nthat linear regression can only be used with quantitative outcomes.\nHowever, linear regression is a good default starting point for any\nregression analysis using a quantitative outcome variable.\n\n### Interpreting regression parameters in a basic model\n\nWe start with a simple linear regression model with only one\ncovariate, age, predicting SBP.  In the NHANES data, the variable\n[BPXSY1](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BPX_I.htm#BPXSY1)\ncontains the first recorded measurement of SBP for a subject, and\n[RIDAGEYR](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm#RIDAGEYR)\nis the subject's age in years.  The model that is fit in the next cell\nexpresses the expected SBP as a linear function of age:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.OLS.from_formula(\"BPXSY1 ~ RIDAGEYR\", data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much of the output above is not relevant for us, so focus on\nthe center section of the output where the header begins with\n__coef__.  This section contains the estimated\nvalues of the parameters of the regression model, their standard\nerrors, and other values that are used to quantify the uncertainty\nin the regression parameter estimates.  Note that the parameters\nof a regression model, which appear in the column labeled\n__coef__ in the table above,\nmay also be referred to as *slopes* or *effects*.\n\nThis fitted model implies that when\ncomparing two people whose ages differ by one year, the older person\nwill on average have 0.48 units higher SBP than the younger person.\nThis difference is statistically significant, based on the p-value\nshown under the column labeled __`P>|t|`__.  This means that there\nis strong evidence that there is a real association between between systolic blood\npressure and age in this population.\n\nSBP is measured in units of *millimeters of mercury*, expressed\n*mm/Hg*.  In order to better understand the meaning of the estimated\nregression parameter 0.48, we can look at the standard deviation of SBP:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"da.BPXSY1.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard deviation of around 18.5 is large compared to the\nregression slope of 0.48.  However the regression slope corresponds to\nthe average change in SBP for a single year of age, and this effect\naccumulates with age.  Comparing a 40 year-old person to a 60 year-old\nperson, there is a 20 year difference in age, which translates into a\n`20 * 0.48 = 9.6` unit difference in average SBP between these two\npeople.  This difference is around half of one standard deviation, and\nwould generally be considered to be an important and meaningful shift.\n\n### R-squared and correlation\n\nIn the case of regression with a\nsingle independent variable, as we have here, there is a very close\ncorrespondence between the regression analysis and a Pearson\ncorrelation analysis, which we have discussed earlier in course 2.\nThe primary summary statistic for assessing the strength of a\npredictive relationship in a regression model is the *R-squared*, which is\nshown to be 0.207 in the regression output above.  This means that 21%\nof the variation in SBP is explained by age.  Note that this value is\nexactly the same as the squared Pearson correlation coefficient\nbetween SBP and age, as shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cc = da[[\"BPXSY1\", \"RIDAGEYR\"]].corr()\nprint(cc.BPXSY1.RIDAGEYR**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a second way to interpret the R-squared, which makes use\nof the *fitted values* of the regression.  The fitted values are\npredictions of the blood pressure for each person in the data\nset, based on their covariate values.  In this case, the only\ncovariate is age, so we are predicting each NHANES subject's\nblood pressure as a function of their age.  If we calculate\nthe Pearson correlation coefficient between the fitted values\nfrom the regression, and the actual SBP values, and then square\nthis correlation coefficient, we see\nthat we also get the R-squared from the regression:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cc = np.corrcoef(da.BPXSY1, result.fittedvalues)\nprint(cc[0, 1]**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, we see that in a linear model fit with only one covariate,\nthe regression R-squared is equal to the squared Pearson\ncorrelation between the covariate and the outcome, and is also\nequal to the squared Pearson correlation between the fitted\nvalues and the outcome.\n\n### Adding a second variable\n\nAbove we considered a simple linear regression analysis with only one\ncovariate (age) predicting systolic blood pressure (SBP).  The real\npower of regression analysis arises when we have more than one\ncovariate predicting an outcome.  As noted above, SBP is expected to\nbe related to gender as well as to age, so we next add gender to the\nmodel.  The NHANES variable for gender is named\n[RIAGENDR](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm#RIAGENDR)\n\nWe begin by creating a relabeled version of the gender variable:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a labeled version of the gender variable\nda[\"RIAGENDRx\"] = da.RIAGENDR.replace({1: \"Male\", 2: \"Female\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to fit the linear model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.OLS.from_formula(\"BPXSY1 ~ RIDAGEYR + RIAGENDRx\", data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The syntax `RIDAGEYR + RIAGENDRx` in the cell above does not mean\nthat these two variables are literally added together.  Instead,\nit means that these variables are both included in the model as\npredictors of blood pressure (`BPXSY1`).\n\nThe model that was fit above uses both age and gender to explain the\nvariation in SBP.  It finds that two people with the same gender whose\nages differ by one year tend to have blood pressure values differing\nby 0.47 units, which is essentially the same age parameter that we found above in\nthe model based on age alone.  This model also shows us that comparing\na man and a woman of the same age, the man will on average have 3.23 units\ngreater SBP.\n\nIt is very important to emphasize that the age coefficient of 0.47 is\nonly meaningful when comparing two people of the same gender, and the\ngender coefficient of 3.23 is only meaningful when comparing two\npeople of the same age.\nMoreover, these effects are additive, meaning that if we compare, say, a 50 year\nold man to a 40 year old woman, the man's blood pressure will on\naverage be around 3.23 + 10*0.47 = 7.93 units higher, with the first\nterm in this sum being attributable to gender, and the second term\nbeing attributable to age.\n\nWe noted above that the regression coefficient for age did not change\nby much when we added gender to the model.  It is important to note\nhowever that in general, the estimated coefficient of a variable in a\nregression model will change when other variables are added or\nremoved.  The only circumstance in which a regresion parameters is unchanged\nwhen other variables are added or removed from the model is when those\nvariables are uncorrelated with the variables that remain in the model.\n\nBelow we confirm that gender and age are nearly uncorrelated in this\ndata set (the correlation of around -0.02 is negligible).  Thus, it is\nexpected that when we add gender to the model, the age coefficient\nis unaffected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to use the original, numerical version of the gender\n# variable to calculate the correlation coefficient.\nda[[\"RIDAGEYR\", \"RIAGENDR\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observe that in the regression output shown above, an R-squared value of 0.215 is\nlisted.  Earlier we saw that for a model with only one covariate,\nthe R-squared from the regression could be defined in two different\nways, either as the squared correlation coefficient between the covariate and the outcome,\nor as the squared correlation coefficient between the fitted values and the outcome.\nWhen more than one covariate is in the model, only the second of these\ntwo definitions continues to hold:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cc = np.corrcoef(da.BPXSY1, result.fittedvalues)\nprint(cc[0, 1]**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical variables and reference levels\n\nIn the model fit above, gender is a categorical variable, and only a\ncoefficient for males is included in the regression output (i.e. there\nis no coefficient for females in the tables above).  Whenever a categorical variable is\nused as a covariate in a regression model, one level of the variable\nis omitted and is automatically given a coefficient of zero.  This\nlevel is called the *reference level* of the covariate.  Here, the\nfemale level of the gender variable is the reference level.  This does\nnot mean that being a woman has no impact on blood pressure.  It\nsimply means that we have written the model so that female blood\npressure is the default, and the coefficient for males (3.23) shifts\nthe blood pressure by that amount for males only.\n\nWe could alternatively have set 'male' to be the reference level, in\nwhich case males would be the default, and the female coefficient\nwould have been around -3.23 (meaning that female blood pressure is\n3.23 units lower than the male blood pressure).\n\nWhen using a categorical variable as a predictor in a regression\nmodel, it is recoded into \"dummy variables\" (also known as \"indicator\nvariables\").  A dummy variable for a single level, say `a`, of a\nvariable `x`, is a variable that is equal to `1` when `x=a` and is\nequal to `0` when `x` is not equal to `a`.  These dummy variables are\nall included in the regression model, to represent the variable that they\nare derived from.\n\nStatsmodels, like most software, will automatically recode a\ncategorical variable into dummy variables, and will select a reference\nlevel (it is possible to override this choice, but we do not cover that\nhere).  When interpreting the regression output, the level that is\nomitted should be seen as having a coefficient of 0, with a standard\nerror of 0.  It is important to note that the selection of a reference\nlevel is arbitrary and does not imply an assumption or constraint\nabout the model, or about the population that it is intended to capture.\n\n### A model with three variables\n\nNext we add a third variable, body mass index (BMI), to the model predicting SBP.\n[BMI](https://en.wikipedia.org/wiki/Body_mass_index) is a measure that is used\nto assess if a person has healthy weight given their height.\n[BMXBMI](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BMX_I.htm#BMXBMI)\nis the NHANES variable containing the BMI value for each subject.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.OLS.from_formula(\"BPXSY1 ~ RIDAGEYR + BMXBMI + RIAGENDRx\", data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not surprisingly, BMI is positively associated with SBP.  Given two\nsubjects with the same gender and age, and whose BMI differs by 1\nunit, the person with greater BMI will have, on average, 0.31 units\ngreater systolic blood pressure (SBP).  Also note that after adding\nBMI to the model, the coefficient for gender became somewhat greater.\nThis is due to the fact that the three covariates in the model, age,\ngender, and BMI, are mutually correlated, as shown next:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"da[[\"RIDAGEYR\", \"RIAGENDR\", \"BMXBMI\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the correlations among these three variables are not strong,\nthey are sufficient to induce fairly substantial differences in the\nregression coefficients (e.g. the gender coefficient changes from 3.23\nto 3.58).  In this example, the gender effect becomes larger after we\ncontrol for BMI - we can take this to mean that BMI was masking part\nof the association between gender and blood pressure.  In other settings, including\nadditional covariates can reduce the association between a covariate\nand an outcome.\n\n### Visualization of the fitted models\n\nIn this section we demonstrate some graphing techniques that can be\nused to gain a better understanding of a regression model that has\nbeen fit to data.\n\nWe start with plots that allow us to visualize the fitted regression\nfunction, that is, the mean systolic blood pressure expressed as a\nfunction of the covariates.  These plots help to show the estimated\nrole of one variable when the other variables are held fixed.  We will\nalso plot 95% *simultaneous confidence bands* around these fitted\nlines.  Although the estimated mean curve is never exact based on a\nfinite sample of data, we can be 95% confident that the true mean\ncurve falls somewhere within the shaded regions of the plots below.\n\nThis type of plot requires us to fix the values of all variables\nother than the independent variable (SBP here), and one independent\nvariable that we call the *focus variable* (which is age here).\nBelow we fix the gender as \"female\" and the BMI as 25.  Thus,\nthe graphs below show the relationship between expected SBP\nand age for women with BMI equal to 25.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.sandbox.predict_functional import predict_functional\n\n# Fix certain variables at reference values.  Not all of these\n# variables are used here, but we provide them with a value anyway\n# to prevent a warning message from appearing.\nvalues = {\"RIAGENDRx\": \"Female\", \"RIAGENDR\": 1, \"BMXBMI\": 25,\n          \"DMDEDUC2\": 1, \"RIDRETH1\": 1, \"SMQ020\": 1}\n\n# The returned values are the predicted values (pr), the confidence bands (cb),\n# and the function values (fv).\npr, cb, fv = predict_functional(result, \"RIDAGEYR\",\n                values=values, ci_method=\"simultaneous\")\n\nax = sns.lineplot(fv, pr, lw=4)\nax.fill_between(fv, cb[:, 0], cb[:, 1], color='grey', alpha=0.4)\nax.set_xlabel(\"Age\")\n_ = ax.set_ylabel(\"SBP\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The analogous plot for BMI is shown next.  Here we fix the\ngender as \"female\" and the age at 50, so we are looking\nat the relationship between expected SBP and age for women\nof age 50.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del values[\"BMXBMI\"] # Delete this as it is now the focus variable\nvalues[\"RIDAGEYR\"] = 50\npr, cb, fv = predict_functional(result, \"BMXBMI\",\n                values=values, ci_method=\"simultaneous\")\n\nax = sns.lineplot(fv, pr, lw=4)\nax.fill_between(fv, cb[:, 0], cb[:, 1], color='grey', alpha=0.4)\nax.set_xlabel(\"BMI\")\n_ = ax.set_ylabel(\"SBP\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error band for BMI is notably wider than the error band for age,\nindicating that there is less certainty about the relationship between\nBMI and SBP compared to the relationship between age and SBP.\n\nThe discussion so far has primarily focused on the mean structure of\nthe population, that is, the model for the average SBP of a person\nwith a given age, gender, and BMI.  A regression model can also be\nused to assess the *variance structure* of the population, that is,\nhow much and in what manner the observations deviate from their mean.\nWe will focus on informal, graphical methods for assessing this.\n\nTo begin with, we plot the residuals against the fitted values.\nRecall that the fitted values are the estimated means for each\nobservation, and the residuals are the difference between an\nobservation and its fitted mean.  For example, the model may estimate\nthat a 50 year old female will have on average an SBP of 125.  But a\nspecific 50 year old female may have a blood pressure of 110 or 150,\nfor example.  The fitted values for both of these women are 125, and\ntheir residuals are -15, and 25, respectively.\n\nThe simplest variance pattern that we can see in a linear regression\noccurs when the points are scattered around the mean, with the same\ndegree of scatter throughout the range of the covariates.  When there\nare multiple covariates, it is hard to assess whether the variance is\nuniform throughout this range, but we can easily check for a\n\"mean/variance relationship\", in which there is a systematic relationship\nbetween the variance and the mean, i.e. the variance either increases\nor decreases systematically with the mean.  The plot of residuals on fitted values is\nused to assess whether such a mean/variance relationship is present.\n\nBelow we show the plot of residuals on fitted values for the NHANES\ndata.  It appears that we have a modestly increasing mean/variance\nrelationship.  That is, the scatter around the mean blood pressure is\ngreater when the mean blood pressure itself is greater.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = sns.scatterplot(result.fittedvalues, result.resid)\npp.set_xlabel(\"Fitted values\")\n_ = pp.set_ylabel(\"Residuals\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A \"component plus residual plot\" or \"partial residual plot\" is\nintended to show how the data would look if all but one covariate\ncould be fixed at reference values.  By controlling the values of\nthese covariates, all remaining variation is due either to the \"focus\nvariable\" (the one variable that is left unfixed, and is plotted on\nthe horizontal axis), or to sources of variation that are unexplained\nby any of the covariates.\n\nFor example, the partial residual plot below shows how age (horizontal\naxis) and SBP (vertical axis) would be related if gender and BMI were\nfixed.  Note that the origin of the vertical axis in these plots is\nnot meaningful (we are not implying that anyone's blood pressure would\nbe negative), but the differences along the vertical axis are\nmeaningful.  This plot implies that when BMI and gender are held\nfixed, the average blood pressures of an 80 and 18 year old differ by\naround 30 mm/Hg.  This plot also shows, as discussed above,\nthat the deviations from the\nmean are somewhat smaller at the low end of the range compared to the\nhigh end of the range.  We also see that at the high end of the range, the\ndeviations from the mean are somewhat right-skewed, with\nexceptionally high SBP values being more common than exceptionally low SBP values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is not part of the main Statsmodels API, so needs to be imported separately\nfrom statsmodels.graphics.regressionplots import plot_ccpr\n\nax = plt.axes()\nplot_ccpr(result, \"RIDAGEYR\", ax)\nax.lines[0].set_alpha(0.2) # Reduce overplotting with transparency\n_ = ax.lines[1].set_color('orange')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we have a partial residual plot that shows how BMI (horizontal\naxis) and SBP (vertical axis) would be related if gender and age were\nfixed.  Compared to the plot above, we see here that age is more\nuniformly distributed than BMI.  Also, it appears that there is more\nscatter in the partial residuals for BMI compared to what we saw above\nfor age. Thus there seems to be less information about SBP in BMI,\nalthough a trend certainly exists.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.axes()\nplot_ccpr(result, \"BMXBMI\", ax)\nax.lines[0].set_alpha(0.2)\nax.lines[1].set_color(\"orange\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another important plot used for understanding a regression model is an \"added variable plot\".  This is a plot\nthat may reveal nonlinearity in the relationship between one covariate and the outcome.  Below, we create\nan added variable plot for age as a predictor of SBP.  Note that the two variables being plotted (age and blood pressure) have been centered.  The scale of the variables is unchanged, but the origin has been translated to zero.  The red line is an estimte of the relationship between age and blood pressure.  Unlike the relationship in the model, it is not forced to be linear, and there is in fact a hint that the shape is slightly flatter for the first 15 years or so of age.  This would imply that blood pressure increases slightly more slowly for people in theie 20s and early 30s, then begins increasing faster after that point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is not part of the main Statsmodels API, so needs to be imported separately\nfrom statsmodels.graphics.regressionplots import add_lowess\n\n# This is an equivalent way to fit a linear regression model, it needs to be\n# done this way to be able to make the added variable plot\nmodel = sm.GLM.from_formula(\"BPXSY1 ~ RIDAGEYR + BMXBMI + RIAGENDRx\", data=da)\nresult = model.fit()\nresult.summary()\n\nfig = result.plot_added_variable(\"RIDAGEYR\")\nax = fig.get_axes()[0]\nax.lines[0].set_alpha(0.2)\n_ = add_lowess(ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression\n\nWe now turn to regression models for *binary* outcome variables,\nmeaning an outcome that can take on only two distinct values.  For\nillustration, we will work with the NHANES variable\n[SMQ020](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/SMQ_I.htm#SMQ020),\nwhich asks whether a person has smoked at least 100 cigarettes in\ntheir lifetime (if this is the case, we say that the person has a\n\"smoking history\").  Below we create a version of this variable in\nwhich smoking and non-smoking are coded as 1 and 0, respectively, and\nrare responses like *don't know* and *refused to answer* are coded as\nmissing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"da[\"smq\"] = da.SMQ020.replace({2: 0, 7: np.nan, 9: np.nan})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Odds and log odds\n\nLogistic regression provides a model for the *odds* of an event\nhappening.  Recall that if an event has probability `p`, then the odds\nfor this event is `p/(1-p)`.  The odds is a mathematical\ntransformation of the probability onto a different scale.  For\nexample, if the probability is 1/2, then the odds is 1.\n\nTo begin, we look at the odds of alcohol use for women and men separately.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = pd.crosstab(da.RIAGENDRx, da.smq).apply(lambda x: x/x.sum(), axis=1)\nc[\"odds\"] = c.loc[:, 1] / c.loc[:, 0]\nc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the probability that a woman has ever smoked is\nsubstantially lower than the probability that a man has ever smoked\n(30% versus 51%).  This is reflected in the odds for a woman smoking\nbeing much less than 1 (around 0.47), while the odds for a man smoking\nis around 1.14.\n\nIt is common to work with *odds ratios* when comparing two groups.\nThis is simply the odds for one group divided by the odds for the\nother group.  The odds ratio for smoking, comparing males to females,\nis around 2.4.  In other words, a man has around 2.4 times greater\nodds of smoking than a woman (in the population represented by these\ndata).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c.odds.Male / c.odds.Female","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is conventional to work with odds on the logarithmic scale.  To\nunderstand the motivation for doing this, first note that the neutral\npoint for a probability is 0.5, which is equivalent to an odds of 1\nand a log odds of 0.  Populations where men smoke more than women will\nhave odds between 1 and infinity, with the exact value depending on\nthe magnitude of the relationship between the male and female smoking\nrates.  Populations where women smoke more than men would have odds\nfalling between 0 and 1.\n\nWe see that the scale of the odds statistic is not symmetric.  It is\nusually arbitrary in which order we compare two groups -- we could\ncompare men to women, or compare women to men.  An odds of 2 (men have\ntwice the odds of smoking as women) is equivalent in strength to an\nodds of 1/2 (women have twice the odds of smoking as men).  Taking the\nlog of the odds centers the scale at zero, and symmetrizes the\ninterpretation of the scale.\n\nTo interpret the log odds when comparing two groups,\nit is important to remember the following facts:\n\n* A probability of 1/2, an odds of 1, and a log odds\nof 0 are all equivalent.\n\n* A positive log odds indicates that the first\ngroup being compared has greater odds (and greater probability) than\nthe second group.\n\n* A negative log odds indicates that the second\ngroup being compared has greater odds (and greater probability) than\nthe first group.\n\n* The scale of the log odds statistic is symmetric in\nthe sense that a log odds of, say, 2, is equivalent in strength to a\nlog odds of -2 (but with the groups swapped in terms of which has the\ngreater probability).\n\nIf you know that the log odds when comparing two groups is a given\nvalue, say 2, and you want to report the odds, you simply exponentiate\nthe log odds to get the odds, e.g. `exp(2)` is around 7.4. Note\nhowever that you cannot recover the individual probabilities (or their\nratio) from an odds ratio.`\n\nBelow we show the log odds for smoking history status of females and\nmales in the NHANES data.  The fact that the log odds for females is\nnegative reflects that fact that substantially less than 50% of\nfemales have a history of smoking.  The log odds for males is closer to\n0, consistent with around half of males having a history of smoking.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c[\"logodds\"] = np.log(c.odds)\nc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A basic logistic regression model\n\nNow that we have a clear understanding of log odds statistics, we will\nfit a logistic regression.  The dependent variable (outcome) of this\ninitial model is smoking status, and the only covariate is gender.\nThus, we are looking at gender as a predictor of smoking status.  We\nfit the model using the `GLM` function, where `GLM` stands for\n*Generalized Linear Model*.  Logistic regression is one type of GLM,\na class which also includes many other regression methods such as Poisson\nregression that we do not discuss further here.  As with linear\nregression, logistic models also include an intercept parameter, but\nwe are not focusing on that parameter now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.GLM.from_formula(\"smq ~ RIAGENDRx\", family=sm.families.Binomial(), data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see the connection between logistic regression and the log odds\nstatistic, note that the logistic regression coefficient for male\ngender is exactly equal to the difference between the log odds\nstatistics for males and females:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c.logodds.Male - c.logodds.Female","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This relationship will always hold when conducting a logistic\nregression with a single binary covariate.\n\nIn general, a logistic regression model will have multiple covariates\nthat may not be binary, but there is still an important connection\nbetween logistic regression and odds ratios.  In this more general\nsetting, we will use a more general type of odds ratio, which we will\nexplore further next.\n\n### Adding additional covariates\n\nAs with linear regression, we can include multiple covariates in a\nlogistic regression.  Below we fit a logistic regression for smoking\nstatus using age (RIDAGEYR) and gender as covariates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.GLM.from_formula(\"smq ~ RIDAGEYR + RIAGENDRx\", family=sm.families.Binomial(), data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding age to the model leads to a very small shift in the gender\nparameter (it changed from 0.885 to 0.892).  In general, regression\ncoefficients can change a lot when adding or removing other variables\nfrom a model.  But in this case the change is quite minimal.  This\nfitted model suggests that older people are more likely to have a\nhistory of smoking than younger people.  The log odds for smoking\nincreases by 0.017 for each year of age.  This effect is additive, so\nthat comparing two people whose ages differ by 20 years, the log odds\nof the older person smoking will be around 0.34 units greater than the\nlog odds for the younger person smoking, adn the odds for the older\nperson smoking will be around `exp(0.34) = 1.4` times greater than\nthe odds for the younger person smoking.\n\nThe greater prevalence of smoking history among older people could be\npartly due to the definition of smoking status that we are using here -- an older person has\nhad more time to smoke 99 cigarettes than a younger person.  However\nmost people who smoke begin when they are young, and the smoking rate\nin the US has been slowly declining for several decades.  Thus, it is\nlikely that the increased smoking levels in older people are driven\nprimarily by real shifts in behavior.\n\nAs with linear regression, the roles of age and gender in the logistic\nregression model can be seen as being additive, but here the additivity\nis on the scale of log odds, not odds or probabilities.  If we compare\na 30 year old female to a 50 year old male, the log odds for the male\nbeing a smoker are `0.89 + 0.34 = 1.23` units greater than the log odds\nfor the female being a smoker.  The value of 0.89 in this expression is\nthe change attributable to gender, and the value of 0.34 is the change\nattributable to age.  Again, we can exponentiate to convert these\neffects from the log odds scale to the odds scale.  Since\n`exp(0.89 + 0.34) = exp(0.89)*exp(0.34) = 2.44*1.41` we can state\nthat male gender is associated with a 2.44 fold increase in the odds\nof smoking, and 20 years of age is associated with a 1.41 fold increase\nin the odds for smoking.  These two effects are multiplied when\ndiscussing the odds, so a 50 year old man has `exp(1.23) = 3.42` fold\ngreater odds of smoking than a 30 year old woman.\n\nIn this logistic regression model with two covariates, the\ncoefficients for age and gender both have interpretations in terms of\n*conditional log odds*.  This generalizes the interpretation of a\nlogistic regression coefficient in terms of marginal log odds that we\ndiscussed above.  When there are two or more covariates in a logistic\nregression model, we always need to think in terms of conditional, not\nmarginal log odds.\n\nSpecifically, the coefficient of around 0.89 for male gender impacts the conditional\nlog odds in the sense that when comparing a male to a female at a\nfixed age, the male will have 0.89 units greater log odds for smoking than\nthe female.  This relationship holds within any age (i.e. it holds\namong all people of age 30, and among all people of age 70).  In this\nsense, it is a *conditional* coefficient because it is only\ninterpretable when holding the other variables in the model fixed.\nSimilarly, the coefficient of around 0.02 for age holds within a\ngender.  Comparing two females whose ages differ by one year, the\nolder female has 0.02 units greater log odds for smoking than the\nyounger female.  This same contrast holds for males.\n\n### A logistic regression model with three predictors\n\nNext we fit a logistic regression model, again for smoking, including\neducational attainment as a predictor.  The educational attainment in\nNHANES is called\n[DMDEDUC2](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm#DMDEDUC2),\nand we will recode it so that the meaning of the levels becomes more\nclear.  We will call the recoded variable `DMDEDUC2x`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a labeled version of the educational attainment variable\nda[\"DMDEDUC2x\"] = da.DMDEDUC2.replace({1: \"lt9\", 2: \"x9_11\", 3: \"HS\", 4: \"SomeCollege\",\n                                       5: \"College\", 7: np.nan, 9: np.nan})\n\nmodel = sm.GLM.from_formula(\"smq ~ RIDAGEYR + RIAGENDRx + DMDEDUC2x\", family=sm.families.Binomial(), data=da)\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the \"Female\" level of the gender variable, and the\n\"College\" level of the educational attainment variable are the\nreference levels, as they are not shown in the output above.  We have\ndiscussed the gender and age variables above, but the educational\nattainment variable is new for us.  All non-reference coefficients for\nthe educational attainment are positive, while the `College`\ncoefficient, as the reference coefficient, is exactly zero.  Thus, we\nsee that people with a college degree have the lowest rate of smoking,\nfollowed by people with less than 9 years of schooling, then\n(after a large gap) people\nwith some college, then people with a high school degree (and no\ncollege), and finally (with the greatest rate of smoking), people with\n9-11 years of schooling.  The overall story here is that smoking rates\nare much lower for people who graduated from college or did not start\nhigh school, presumably for very different reasons.  On the other\nhand, people with some high school, people who completed high school,\nand people who began but did not complete college have much higher\nrates of smoking.  The odds ratio between the former and the latter\ngroup depends on the specific subgroups being compared, but can be\nalmost `3 = exp(1.09)`.\n\nAs noted above when we were discussing linear regression, it is\nimportant to remember that a coefficient in a logistic regression are\n\"conditional\" on the other variables being held fixed.  For example,\nthe log odds ratio of 1.09 between people with 9-11 years of schooling\nand people who completed college applies only when comparing people with\nthe same age and gender.\n\n### Visualization of the fitted models\n\nVisualization of fitted logistic regression models is more challenging\nthan visualization of fitted linear models, but is still worth\npursuing.  We can begin by plotting the fitted proportion of the\npopulation that smokes, for various subpopulations defined by the\nregression model.  We will focus here on how the smoking rate varies\nwith age, so we restrict the population to female college graduates.\n\nThe following plot shows the fitted log odds (or logit) probability\nfor the smoking outcome as a function of age.  The grey band is a\nsimultaneous 95% simultaneous confidence band, as discussed above in\nthe case of a linear model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"values = {\"RIAGENDRx\": \"Female\", \"RIAGENDR\": 1, \"BMXBMI\": 25,\n          \"DMDEDUC2\": 1, \"RIDRETH1\": 1, \"SMQ020\": 1,\n          \"DMDEDUC2x\": \"College\", \"BPXSY1\": 120}\n\npr, cb, fv = predict_functional(result, \"RIDAGEYR\",\n                values=values, ci_method=\"simultaneous\")\n\nax = sns.lineplot(fv, pr, lw=4)\nax.fill_between(fv, cb[:, 0], cb[:, 1], color='grey', alpha=0.4)\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Smoking\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can display the same plot in terms of probabilities instead of in\nterms of log odds.  The probability can be obtained from the log odds\nusing the relationship `p = 1 / (1 + exp(-o))` where `o` is the log\nodds.  Note that while the age and log odds are linearly related, age\nhas a curved relationship with probability.  This is necessary since\nprobabilities must remain between 0 and 1, a linear relationship would\neventually exit this interval.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pr1 = 1 / (1 + np.exp(-pr))\ncb1 = 1 / (1 + np.exp(-cb))\nax = sns.lineplot(fv, pr1, lw=4)\nax.fill_between(fv, cb1[:, 0], cb1[:, 1], color='grey', alpha=0.4)\nax.set_xlabel(\"Age\", size=15)\nax.set_ylabel(\"Smoking\", size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we turn to diagnostic plots that are intended to reveal certain\naspects of the data that may not be correctly captured by the model.\nThe three plots below are intended to reveal any curvature in the mean\nrelationship between the outcome and one of the covariates. We used\nthe partial regression plotting technique above for this same purpose\nwhen working with linear models.\n\nIn the case of logistic regression, the three techniques\ndemonstrated below can identify\nmajor discrepancies between the fitted model and the population, but\nevidence for small discrepancies is not reliable unless the sample\nsize is very large.  The CERES technique has the strongest theoretical\nsupport.  Taken at face value, the plots below suggest that smoking\nrates may rise slightly faster for people between the ages of 20 and 35,\nand again for people between the ages of 50 and 60, with a period of minimal\nincrease between these age intervals.  This would contradict the\nperfectly linear model for age (on the log odds scale)\nthat we have specified in our model.\nThese plotting techniques can be useful at identifying\npossible opportunities for future analysis with additional data, but\ndo not identify features that can be claimed with high confidence\nusing the present data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = result.plot_partial_residuals(\"RIDAGEYR\")\nax = fig.get_axes()[0]\nax.lines[0].set_alpha(0.2)\n\n_ = add_lowess(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = result.plot_added_variable(\"RIDAGEYR\")\nax = fig.get_axes()[0]\nax.lines[0].set_alpha(0.2)\n_ = add_lowess(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = result.plot_ceres_residuals(\"RIDAGEYR\")\nax = fig.get_axes()[0]\nax.lines[0].set_alpha(0.2)\n_ = add_lowess(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}