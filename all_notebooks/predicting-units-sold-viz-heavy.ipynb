{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Packages Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport os\nimport sys\nsys.path.append(os.path.realpath('..')) #note to self: this works, only when notebook is alrdy saved in directory. So, first save notebook and then use this line of code.\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Loading application records into a pandas dataframe\nsales = pd.read_csv( '../input/summer-products-and-sales-in-ecommerce-wish/summer-products-with-rating-and-performance_2020-08.csv') ","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print( sales.shape )\nprint()\nsales.info()\nprint()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So...  \nWe have:  \n* NA's  \n* probably a single value in currency_buyer  \n* probably some useless fields like: product_url, merchant_profile_picture and product_picture fields","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start by removing the useless columns we saw previously:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean = sales.drop(\n    columns=['product_url', 'merchant_profile_picture', 'product_picture','currency_buyer', \\\n    'theme', 'crawl_month', 'merchant_info_subtitle', 'merchant_name', 'merchant_title', \\\n        'urgency_text'\n    ])\n\nsales_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing With Missing Data and Duplicates","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check again how many uniques and nulls we got:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_unique(dataframe):\n    \"\"\"\n    Receives a pandas dataframe and  produces a new dataframe with\n    nr. of uiques count and nr. of nulls count.\n    \"\"\"\n    uniques = pd.DataFrame( dataframe.nunique(), columns= ['nr_uniques'] )\n    nulls = pd.DataFrame( dataframe.isnull().sum(), columns= ['nr_nulls'] )\n    _a = pd.concat( [uniques, nulls], axis = 1 )\n    _a['nr_observations'] = dataframe.shape[0]\n    \n    if _a['nr_nulls'].sum() > 0:\n        _a = _a[['nr_observations','nr_uniques', 'nr_nulls']].sort_values(by=['nr_nulls'], ascending = False)\n    elif _a['nr_uniques'].sum() != _a['nr_observations'].sum():\n        _a = _a[['nr_observations','nr_uniques', 'nr_nulls']].sort_values(by=['nr_uniques'], ascending = True)\n    else:\n        _a = _a[['nr_observations','nr_uniques', 'nr_nulls']].sort_index()\n\n    return _a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_unique(sales_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are has_urgency_banner is lacking alot of values.  \nThis is probably, because there simply isn't a urgency flag on it, so we'll fillna with 0 in this case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.has_urgency_banner.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the below code, we can easily check that rating counts columns are null always when rating_count equals 0.  ","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"for number in ['one', 'two', 'three', 'four', 'five']:\n    col = 'rating_'+ number +'_count'\n    print( col + ' has ' + str( sales_clean[sales_clean[col].isnull()].rating_count.value_counts()[0]) +' null values when rating_count=0' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this cases I guess it's safe to fillna with 0's aswell, because if we feed this information into a given algorithm, it will recognize that a certain product that had rating_count=0 it also had 0 counts in other rating columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for number in ['one', 'two', 'three', 'four', 'five']:\n    col = 'rating_'+ number +'_count'\n    sales_clean[col].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check our uniques and nulls grid again, this time only for the cases where we still have NA's:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_check = null_unique(sales_clean)\ngrid_check[grid_check.nr_nulls > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect:  \nLet's first check if there are any duplicated prodcuts with colors NA's in some cases, and in other cases with colors assigned.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# counting \nmask = sales_clean.product_id.value_counts() \nmask = mask[mask>1]\nsales_dup_prod = sales_clean.iloc[mask.to_list()].sort_values(by=['product_id'])\nsales_dup_prod.isnull().sum().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means we have no NA's whitin the duplicated products subset.  \nSo, there is no way we can know what is the main color of the product.  \nFor this reason we can fill NA's for product_color and origin_country with 'unknown' category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.product_color.fillna('unknown', inplace=True)\nsales_clean.origin_country.fillna('unknown', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets, finally, check product_variation_size_id for NA's:  \nI suspect that this field is NA whenever product_variation_inventory = 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean[['product_id', 'product_variation_inventory', 'product_variation_size_id']][sales_clean.product_variation_size_id.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ooopss...! I guess I was wrong =)  \nI guess we won't be able to know what is one of the available size variation for this products, so we better fillna with 'unknown' category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.product_variation_size_id.fillna('unknown', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check our unique and null grid again:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_unique(sales_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This time we got no missing data anymore. Hurray to that! =)  \nHowever, we still have about 200 duplicated product ID's.  \nLet's get rid of those, as they won't help in prodictive task we got ahead.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean = sales_clean.drop_duplicates(subset='product_id').reset_index(drop=True)\n\nnull_unique(sales_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Organizing our Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"merchant_id and product_id are univocal identifiers of each row of the dataset now.  \nLet's pass those as indexes:  ","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"sales_clean = sales_clean.set_index(['merchant_id', 'product_id']).sort_index()\n\nsales_clean.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now build a couple lists of dataset columns that might prove usefull ahead.  \nWe can divide columns by: Flags or binary features, numerical features and categorical features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"_b = null_unique(sales_clean)\nflag_cols = _b[_b['nr_uniques'] == 2].index.to_list()\nflag_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [col for col in sales_clean.columns if col not in flag_cols and sales_clean[col].dtype != 'O' ]\nnum_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [col for col in sales_clean.columns if sales_clean[col].dtype == 'O' ]\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check-Zone:\n# len(sales_clean.columns) == 31\n# len(flag_cols) == 7\n# len(num_cols) == 17\n# len(cat_cols) == 7\n\nlen(flag_cols) + len(num_cols)  + len(cat_cols)  == len(sales_clean.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Continuing With Data Cleaning Tasks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features  Cleaning","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    print( 'There are ' + str(len( sales_clean[col].unique() )) + ' distinct categories for '+ col +' feature.' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, our categorical features have alot of categories (and i'm not referring to title or tags features right now, we''l deal with those later on).  \nWe should try to reduce the size of categrories in each of them, or the predictive model might be poorly fed.  \nBelow we'll clean any trailing and/or leading spaces from each feature aswell as lower case of the strings:  ","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# passing all categorical features to lower case, and striping trailing and/or leading spaces\nfor col in cat_cols:\n    sales_clean[col] = sales_clean[col].str.lower()\n    sales_clean[col] = sales_clean[col].str.strip()\nsales_clean[cat_cols].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok!  \nNow that we have uniformized the strings characters, let's check what we got using the help of some plotting:","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    if 'title' not in col and 'tags' not in col:\n        #print('ok')\n        pd.DataFrame( sales_clean[col].value_counts(ascending=True) ).plot(kind='barh', figsize=(10,22), legend = False)\n        plt.title( col + ' | Categories Distribution.' )\n        ;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, although there are alot of categories, in each categorical feature, we can try to reduce them in some cases:  \nFor instance, in ``` shipping_option_name ``` we can see that the categories are expressed in several languages.  \nOne thing we could try, is to use googletrans to detect language and translate to english. However, there is a flag column   \nthat identifies if the shipping is express or not (which, was the primary objective of the shipping_option_name if we fed it to any model - the model would retain if the product is shipped in express mode or not), so this column end up being redundant thus we can drop it.  \nIn any case i'll leave a cell below with an implementation of googletrans application (yeah! I tried the implementation, because I forgot about the flag column mention previously).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from googletrans import Translator\n# def translate_strings( pandas_series ):\n#     \"\"\"\n#     Translate the unique values from a pandas series, in order to minimize the calls to google services.\n#     Even thought google service is free it has a limit, therefore the use fo this function.\n# \n#     This functions returns a dataframe with 2 columns:\n#     real_cats = original pandas series categories\n#     cats = translated cateories\n#     \"\"\"\n#     translator = Translator()\n#     _b = pd.DataFrame( pandas_series.unique(), columns = ['real_cats'] )\n#     _b['cats'] = _b['real_cats'].apply(translator.translate, dest='en').apply(getattr, args=('text',)).str.lower()\n# \n#     return _b\n# \n# ship_options = translate_strings( sales_clean['shipping_option_name'] )\n# \n# \n# replacing_cats = {\n#     'standard delivery': 'standard shipping',\n#     'standard post': 'standard shipping',\n#     'normal delivery': 'standard shipping',\n#     'express delivery': 'express shipping'\n# }\n# ship_options['cats'].replace(replacing_cats, inplace = True)\n# ship_options.set_index('real_cats', inplace = True)\n# \n# sales_clean['shipping_option_name'] = sales_clean.shipping_option_name.replace( ship_options.cats.to_dict() )\n# sales_clean.shipping_option_name.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.drop(columns = ['shipping_option_name'], inplace = True)\n\ncat_cols.remove('shipping_option_name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In ```origin_country```, we can try to transform the feature to have 2 categories, since the majority of the records has 'cn' value:  \nlet's say 'cn' and 'others' will be the new categories:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['origin_country'] = sales_clean.origin_country.where(sales_clean.origin_country=='cn', 'other country')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In ```product_color```, we can try to transform the feature to have a couple categories with some equilized distribution values:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['product_color'] = \\\nsales_clean.product_color.where( \n    sales_clean.product_color.isin(['black','white','pink','blue','yellow', 'red', 'green', 'grey', 'purple', 'unknown']),\n    'other'\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.product_color.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In ``` product_variation_size_id ``` we have some more data cleaning to deal with","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.replace('.*[^x*]xl', 'xl')\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.replace('size', '')\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.replace('--', '')\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.replace('-', '')\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.replace('\\.', '')\nsales_clean['product_variation_size_id'] = sales_clean.product_variation_size_id.str.strip()\n\nsales_clean.product_variation_size_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['product_variation_size_id'] = \\\nsales_clean.product_variation_size_id.where(\n    sales_clean.product_variation_size_id.isin(['xxs','xs','s','m','l','xl','xxl']),\n    'other size'\n    )\nsales_clean.product_variation_size_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    if 'title' not in col and 'tags' not in col:\n        #print('ok')\n        pd.DataFrame( sales_clean[col].value_counts(ascending=True) ).plot(kind='barh', legend = False)\n        plt.title( col + ' | Categories Distribution.' )\n        ;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Title columns can be used to build a new flag feature, that will indicate to an eventual predictive model that it was made availabe a translation for the product.  \nPerhaps consumers buy more if the product has a translation for theyr own language, because this fact alone can cause a better perception, on the consumer, of what the product is.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['flag_has_transl'] =   np.where( sales_clean.title == sales_clean.title_orig , 1, 0 )\nsales_clean.drop(columns=['title','title_orig'], inplace=True)\n\ncat_cols.remove('title')\ncat_cols.remove('title_orig')\nflag_cols.append('flag_has_transl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Flag Features  Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For this set of features we will only change the data type in order to optimize memory usage a little bit","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"sales_clean[flag_cols].info()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"for col in flag_cols:\n    sales_clean[col] = sales_clean[col].astype('uint8')\n\nsales_clean[flag_cols].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in flag_cols:\n    pd.DataFrame( sales_clean[col].value_counts(ascending=True) ).plot(kind='barh', legend = False)\n    plt.title( col + ' | Categories Distribution.' );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numeric Features  Cleaning","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"sales_clean[num_cols].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nothing to clean in this subset of features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before diving deep into analytics, lets build a new numeric feature that will represent the user perception of the advantage that is buying the product via Wish platform when comparing with other retailers.  \nFor that we will simply calculate the difference between the retail price and \"wish\" price:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['user_discount'] = sales_clean['retail_price'] - sales_clean['price']\nnum_cols.append('user_discount')\n\nsales_clean[['retail_price','price','user_discount']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please note:  \n  \n* If the ```user_discount``` < 0 => negative user perception: he will spend that extra money if buying product via Wish platform;  \n* If the ```user_discount``` > 0 => positive user perception: he will save that amount of money if buying product via Wish platform;  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At this point, we can also build another numeric feature, that will basically count the number of tags a product have","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean['tags_nr'] = sales_clean.tags.str.count(',')\nnum_cols.append('tags_nr')\n\nsales_clean.drop(columns = ['tags'], inplace =True)\ncat_cols.remove('tags')\n\nsales_clean[['retail_price','price','user_discount', 'tags_nr', 'units_sold']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive Analytics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Numeric Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check numeric features distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def distplot_matrix(total_cols, dataframe):\n    \"\"\"\n    Builds a facet with mutiple sns distribution plots - one for eachfeature in the dataframe.\n    This function works for numeric features.\n\n    Input elements:\n    total_cols - total number of columns the user wants to have in the matrix where the plots will be in the end\n    dataframe - pandas dataframe with the features we want to plot\n    \"\"\"\n    # Subplots are organized in a Rows x Cols Grid\n    # Tot and Cols are known\n    # https://stackoverflow.com/questions/12319796/dynamically-add-create-subplots-in-matplotlib\n\n    Tot = len( dataframe.columns )\n    Cols = total_cols\n\n    # Compute Rows required\n    Rows = Tot // Cols \n    Rows += Tot % Cols\n\n    # Create a Position index\n    Position = range(1, Tot + 1)\n\n    # Create main figure\n    fig = plt.figure(1, figsize=(30,35)) \n    # optimization needed: dynamically adjust figure size given the total columns the user wants to see in final matrix and the total charts to display = len(dataframe.columns)\n\n    for k, col in zip( range(Tot), dataframe.columns ):\n        # add every single subplot to the figure with a for loop\n        ax = fig.add_subplot(Rows, Cols, Position[k])\n        \n        sns.distplot( \n            dataframe[col], \n            hist_kws={\"histtype\": \"bar\", \"rwidth\":0.7,  \"alpha\": 1, \"color\": \"#f1c80f\"}, \n            kde_kws={\"color\": \"black\", \"lw\": 2, \"bw\":0.6},\n            ax=ax\n            )                \n        #Removes frame but keep axis\n        plt.gca().spines['right'].set_color('none')\n        plt.gca().spines['top'].set_color('none');\n        plt.title(col + ' Distribution', fontweight=\"bold\")\n\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distplot_matrix( 4, sales_clean[num_cols] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above matrix of distribution plots we can retain some conclusions:  \n* If we compare the price and retail_price plots, we can see that retail_price has a 5 times larger scale, when compared with prices. Maybe this is why consumers come to this platform to buy. Perhaps the price differences are really this big between Wish and other selling spots and consumers realize that when shopping.  \n  \n* Another interesting observation, is that the majority of the mean product ratings concentrate arround 4 (scale 1 to 5), which means that the Whish platform users/consumers prespective over the products are, overall, pretty decent. The same can be observed for the seller ratings. This leads to conclude that Wish consumers perception about products and sellers is relatively good and, probably, this fact alone will generate more confidence amongst the consumers to keep buying or, eventually, to buy, more.  \n  \n* Rating counts features have all very simillar distributions, although with different frequencies.  \n  \n* Aparently products/sellers do not own many badges, since most observations concentrate themselfs arround 0 unit.  \n  \n* Most of the products are well stocked (perhaps too well stocked?), since most of the observations concentrate themselfs on 50 units. However, there is a quite relevant part of products that concentrate on 10 or less units. Perhaps those are the products that are selling well and need to be replenished in stock more frequently?  \n  \n* The shipping prices are, in the majority of the cases, very low (<4â‚¬). This fact alone is another sales booster since the consumers won't have to spend alot more money in shiping costs.  \n  \n* There is a significant proportion of poducts where the user discount is null or negative, although in the majority of the products the users can actually get some decent discounts.\n  \n* A final observation to talk about the countries where a given product is shipped: most products are shipped to arround 40 distinct countries. This leads to conclude and again reinforce the conclusions we made earlier about consumer perception on a global level.  \n  \n  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Numeric Features Correlations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's plot Pearson correlations:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,9))\n\ncorr_df = sales_clean[num_cols].corr().round(2)\nmask_ut= np.zeros_like(corr_df)\nmask_ut[np.triu_indices_from(mask_ut)] = True\nax = sns.heatmap( corr_df, mask = mask_ut, annot=True )\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\nplt.title('Numerical features Correlations', fontweight=\"bold\")\n;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that:  \n* rating_count is highly correlated with  'rating_five_count', 'rating_four_count', 'rating_three_count', 'rating_two_count'and 'rating_one_count'  \n* all the previously mentioned features are correlated with units sold  \n* shipping_option_price is highly correlated with price\n* user_discount is highly correlated with retail_price\n\nFor this reason, it's safe to drop 'retail_price','shipping_option_price', 'rating_five_count', 'rating_four_count', 'rating_three_count', 'rating_two_count', 'rating_one_count' and 'rating_count' features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_clean.drop(columns=['retail_price','shipping_option_price','rating_five_count', 'rating_four_count', 'rating_three_count', 'rating_two_count', 'rating_one_count', 'rating_count'], inplace = True)\n\nnum_cols_to_drop = ['retail_price','shipping_option_price', 'rating_five_count', 'rating_four_count', 'rating_three_count', 'rating_two_count', 'rating_one_count', 'rating_count']\n\nnum_cols = [col for col in num_cols if col not in num_cols_to_drop ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features Distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_matrix(total_cols, dataframe):\n    \"\"\"\n    Builds a facet with mutiple sns distribution plots - one for eachfeature in the dataframe.\n    This function works categorical features only.\n\n    Input elements:\n    total_cols - total number of columns the user wants to have in the matrix where the plots will be in the end\n    dataframe - pandas dataframe with the features we want to plot\n    \"\"\"\n    # Subplots are organized in a Rows x Cols Grid\n    # Tot and Cols are known\n    # https://stackoverflow.com/questions/12319796/dynamically-add-create-subplots-in-matplotlib\n\n    Tot = len( dataframe.columns )\n    Cols = total_cols\n\n    # Compute Rows required\n    Rows = Tot // Cols \n    Rows += Tot % Cols\n\n    # Create a Position index\n    Position = range(1, Tot + 1)\n\n    # Create main figure\n    fig = plt.figure(1, figsize=(25,5)) \n    # optimization needed: dynamically adjust figure size given the total columns the user wants to see in final matrix and the total charts to display = len(dataframe.columns)\n\n    #sns.set_palette(sns.cubehelix_palette(8))\n    sns.set_palette(sns.color_palette(\"BrBG\", 7))\n    for k, col in zip( range(Tot), dataframe.columns ):\n        # add every single subplot to the figure with a for loop\n        ax = fig.add_subplot(Rows, Cols, Position[k])\n        _data = pd.DataFrame( dataframe[col].value_counts() ).rename(columns={col:'freq'})\n        \n\n        sns.barplot( \n            data = _data,\n            y = list(_data.index), \n            x=_data.freq,\n            ax=ax\n            )                \n        #Removes frame but keep axis\n        plt.gca().spines['right'].set_color('none')\n        plt.gca().spines['top'].set_color('none')\n        plt.title(col + ' Distribution', fontweight=\"bold\")\n        ;\n\n\n    plt.show();\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_matrix(3, sales_clean[cat_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With those plots we can conclude that:  \n* Most products in the dataset have colors, or combinations of colors, other then black, white, pink, etc. However, there is a big proportion of products whose color is black or white;  \n* Most products on the data set have size s or xs;  \n* Most products on the dataset are originary from china (which, probably justifies the lower price ranges of Whish platform, when comparing with prices other retailers).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Binary features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def flag_matrix(total_cols, dataframe):\n    \"\"\"\n    Builds a facet with mutiple sns distribution plots - one for eachfeature in the dataframe.\n    This function works categorical features only.\n\n    Input elements:\n    total_cols - total number of columns the user wants to have in the matrix where the plots will be in the end\n    dataframe - pandas dataframe with the features we want to plot\n    \"\"\"\n    # Subplots are organized in a Rows x Cols Grid\n    # Tot and Cols are known\n    # https://stackoverflow.com/questions/12319796/dynamically-add-create-subplots-in-matplotlib\n\n    Tot = len( dataframe.columns )\n    Cols = total_cols\n\n    # Compute Rows required\n    Rows = Tot // Cols \n    Rows += Tot % Cols\n\n    # Create a Position index\n    Position = range(1, Tot + 1)\n\n    # Create main figure\n    fig = plt.figure(1, figsize=(25,22)) \n    # optimization needed: dynamically adjust figure size given the total columns the user wants to see in final matrix and the total charts to display = len(dataframe.columns)\n\n    for k, col in zip( range(Tot), dataframe.columns ):\n        # add every single subplot to the figure with a for loop\n        ax = fig.add_subplot(Rows, Cols, Position[k])\n        _data = pd.DataFrame( dataframe[col].value_counts() ).rename(columns={col:'freq'})\n        \n        sns.barplot( \n            data = _data,\n            y = list(_data.index), \n            x=_data.freq,\n            orient = 'h',\n            palette = ['#4341ab', '#85738f'],\n            ax=ax\n            )                \n        #Removes frame but keep axis\n        plt.gca().spines['right'].set_color('none')\n        plt.gca().spines['top'].set_color('none')\n        plt.title(col + ' Distribution', fontweight=\"bold\")\n        ;\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make some plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"flag_matrix(3, sales_clean[flag_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above mosaic we can conclude:  \n* Most products do not own a quality badge;  \n  \n* Most products are not classified with an urgency banner ( maybe because, as we seen previously, most products are well stocked?);  \n  \n* Most products are not shipped in express mode;  \n  \n* Most products do not own a fast shipping badge, meaning that most products do not get shipped rapidly (or at least, users are not reporting them as so);  \n  \n* Most products are not produced locally;  \n  \n* Most merchants do not own a picture (I don't know how much this feature, ```merchant_has_profile_picture``` , can be a decent input for a machine learning model. However, I, as a consumer, would feel alot more confortable buying from a known/trusted merchant and in that sense, I belive a picture would help me to gain some trust in the merchant if I dind't know him already);  \n \n* In our dataset, there is a balance between products where theyr merchants use advertisement boosts and products where theyr merchants do not use advertisement boosts;  \n  \n* In most cases, merchants do not offer a translation for the products","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Joint Features Analytics | Outlier Detection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets start by building a funtion that will allow us for some massive box plotting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot_matrix(total_cols, num_cols, cat_col, dataframe):\n    \"\"\"\n    Builds a facet with mutiple sns box plots.\n    \n    Input elements:\n    total_cols - total number of columns the user wants to have in the matrix where the plots will be in the end\n    num_cols - list with numeric columns in the dataframe to use \n    cat_col - name of the categorical columns to be used\n    dataframe - pandas dataframe with the features we want to plot\n    \"\"\"\n    # Subplots are organized in a Rows x Cols Grid\n    # Tot and Cols are known\n    # https://stackoverflow.com/questions/12319796/dynamically-add-create-subplots-in-matplotlib\n\n    Tot = len( dataframe.columns )\n    Cols = total_cols\n\n    # Compute Rows required\n    Rows = Tot // Cols \n    Rows += Tot % Cols\n\n    # Create a Position index\n    Position = range(1, Tot + 1)\n\n    # Create main figure\n    fig = plt.figure(1, figsize=(23,70)) \n    # optimization needed: dynamically adjust figure size given the total columns the user wants to see in final matrix and the total charts to display = len(dataframe.columns)\n\n    sns.set_palette(sns.color_palette(\"BrBG\", 7))\n    for k, num_col in zip( range(Tot), dataframe[num_cols].columns ):\n        # add every single subplot to the figure with a for loop\n        ax = fig.add_subplot(Rows, Cols, Position[k])\n        order_df = pd.DataFrame( dataframe.groupby(cat_col)[num_col].mean()).sort_values(by= num_col, ascending = False)\n        \n        sns.boxplot( \n            data = dataframe,\n            x = num_col,\n            y = cat_col,\n            order = order_df.index,\n            linewidth= 1,\n            orient = 'h',\n            ax=ax\n            )                \n        #Removes frame but keep axis\n        plt.gca().spines['right'].set_color('none')\n        plt.gca().spines['top'].set_color('none')\n        plt.title( cat_col +  ' Vs. ' + num_col, fontweight=\"bold\" )        \n        ;\n\nplt.tight_layout();\nplt.show();\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cat_cols == ['product_color', 'product_variation_size_id', 'origin_country'] <-- used to switch variibale column in cat_col argument in below function\nboxplot_matrix(2, num_cols = num_cols, cat_col = 'product_color', dataframe = sales_clean);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above mosaic of boxplots we can see that there are alot of outliers.  \nSo, next step is to scale data. Scalling will be beneficial, not only to minimize/remove outliers, but also to uniformize all features unit measure.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = sales_clean['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = sales_clean.drop(columns = ['units_sold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_seed = 123\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = .3, random_state = rand_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting Dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pd.get_dummies( x_train, columns = cat_cols, prefix_sep = '==' , drop_first = True)\nx_test = pd.get_dummies( x_test, columns = cat_cols, prefix_sep = '==' , drop_first = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# perform a robust scaler transform of the dataset\ntrans = RobustScaler()\n\n#We are interested in scaling the input feature and not the output feature, so we take this extra step to create a list with numeric input features\nnum_cols.remove('units_sold')\n\nsales_num_scaled = pd.DataFrame( trans.fit_transform(x_train[num_cols]), columns = num_cols, index =  x_train.index )\n\nx_train.drop(columns = num_cols, inplace = True)\n\nx_train = pd.concat( [x_train, sales_num_scaled] , axis = 1 )\n\nnum_cols.append('units_sold')\n\n#keep test set with same column order then train set\nx_train.sort_index(axis=1, inplace=True)\nx_test.sort_index(axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Units sold","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring_gridsearchcv = pd.DataFrame( metrics.SCORERS.keys(), columns = ['metrics_name_gridsearchcv'] )\n#scoring_gridsearchcv\nscoring_gridsearchcv[scoring_gridsearchcv.metrics_name_gridsearchcv.str.contains('error')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding baseline predictions MAE","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"baseline_predictions = np.ones(y_test.shape) * y_train.mean()\nmae_baseline = mean_absolute_error(y_test, baseline_predictions)\n\nprint( 'Baseline MAE is {:.2f}'.format(mae_baseline) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding more advanced solutions: XGBoost","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"param_grid = {\n    'nthread':[2], #when use hyperthread, xgboost may become slower\n    'learning_rate': [0.001, 0.005, 0.01], #so called `eta` value\n    'max_depth': range(3,5,1),\n    #'importance_type': ['weight', 'gain', 'cover'],\n    #'min_child_weight' : [ 1 ],\n    #'gamma': [ 0.0],\n    'colsample_bytree' : [0.2],\n    'verbosity': [0],\n    'n_estimators': [800, 900, 1000], #number of trees\n    'seed': [rand_seed]\n    }\n\nscoring_func = 'neg_mean_absolute_error'\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=rand_seed)\noptimal_xgb_model = GridSearchCV(estimator=XGBRegressor(), param_grid = param_grid, scoring = scoring_func, cv= cv, error_score='raise', verbose = 1)\noptimal_xgb_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print( 'Optimal xgb model configuration found:' )\nprint()\nprint(optimal_xgb_model.best_estimator_)\nprint()\nprint()\nprint( 'Optimal ' + scoring_func + ' :' )\nprint(  int( optimal_xgb_model.best_score_.round() ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pd.Series( optimal_xgb_model.predict(x_test ).round().astype('int'), index = x_test.index )\ndf_final_comp = pd.concat([y_pred,y_test], axis=1).rename(columns={0:'units_sold_pred_xgb'})\n\ndf_final_comp.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding more advanced solutions: AdaBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://machinelearningmastery.com/adaboost-ensemble-in-python/","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"param_grid = {\n    'n_estimators':[5, 10, 20, 50, 100],\n    'learning_rate':[0.00001, 0.00002, 0.0001]\n    }\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=rand_seed)\noptimal_ada_model = GridSearchCV(estimator= AdaBoostRegressor(), param_grid = param_grid, scoring = scoring_func, cv= cv, error_score='raise', verbose = 1)\noptimal_ada_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print( 'Optimal AdaBoost model configuration found:' )\nprint()\nprint(optimal_ada_model.best_estimator_)\nprint()\nprint()\nprint( 'Optimal ' + scoring_func + ' :' )\nprint(  int( optimal_ada_model.best_score_.round() ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_ada = pd.Series( optimal_ada_model.predict(x_test).round().astype('int'), index = x_test.index )\ndf_final_comp = pd.concat([df_final_comp,y_pred_ada], axis=1).rename(columns={0:'units_sold_pred_ada'})\ndf_final_comp = df_final_comp[['units_sold_pred_xgb','units_sold_pred_ada','units_sold']]\n\ndf_final_comp.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A couple comments:  \nOverall I'm not happy with both models found.  \nThey defenetly can be improved.  \nI'm not sure if I tunned the models correctly.  \nIt's defenetly possible to build more predictable features out of the dataset, but I have no idea how to do it, so far.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}