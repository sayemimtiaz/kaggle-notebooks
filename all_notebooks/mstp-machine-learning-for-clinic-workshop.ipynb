{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this tutorial, we are going to analyze a data set using a logistic regression  and a random forest model. To do so, we are first (1) load the python packages we will be using, (2) load and complete simple manipulations of the data set, and (3) split our data into a train and test set for both models.\n\nThe data set we are using is from the Framingham Hearts study which began in 1948 and has uncovered numerous associations or risk factors relating to coronary artery disease. This portion of the data set includes 4,238 subjects, each with 15 descriptive measures (features) such as cholesterol, blood pressure, and heart rate  in addition to an output label-- if a diagnosis of CHD was made over 10years. More information can be found at the following link:\n\nhttps://www.framinghamheartstudy.org/fhs-risk-functions/hard-coronary-heart-disease-10-year-risk/\n\n\nTo explore the data set manually, you can double click the 'framingham_heart_disease.csv' under 'Workspace' on the right hand-side pane."},{"metadata":{},"cell_type":"markdown","source":"### 1. To begin, we will import the relevant python packages we will be using:\n- **numpy** allows for basic math functions and data manipulation\n- **pandas** is a data analysis package that helps with preprocessing and manipulation of your data set.\n- **sklearn** provides a large set of preprocessing and machine learning functions "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#General packages.\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns #more plotting\n\nimport os #for interacting with the operating system. \n\nfrom subprocess import call\nfrom IPython.display import Image\n\nimport shap #for SHAP values\nfrom pdpbox import pdp, info_plots #for partial plots\nnp.random.seed(123) #ensure reproducibility\n\npd.options.mode.chained_assignment = None  #hide any pandas warnings\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#ML related packages:\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import accuracy_score as score\n\nfrom sklearn.ensemble import RandomForestClassifier #for the model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz #plot tree\nfrom sklearn.metrics import roc_curve, auc #for model evaluation\nimport eli5 #for purmutation importance\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2a) Preparing the data for analysis: Load Framingham Heart Study data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting the data path and file name:\ndata_path = \"../input/logistic-regression-heart-disease-prediction\" \nfname = os.listdir(data_path)[0] #file name\n\n#loading and examination of data set.\ndataset = pd.read_csv((data_path +'/'+ fname))\ndataset.rename(columns={'male':'sex'}, inplace=True)\n\n#remove entries with NANs\ndataset.dropna(axis=0,inplace=True)\n\ndataset.head() #show first few entries.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2b) Preparing the data for analysis: Separate data set into X (input features) and y (output label)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separate data set into X (input features) and y (output label).\nX = dataset.drop('TenYearCHD', 1) #dataset.iloc[:,:-1].values\ny = dataset['TenYearCHD'] #dataset.iloc[:,-1].values\n\nfeatures = dataset.drop('TenYearCHD', 1).columns\n# print(features.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The following provides a visual description of our data set:"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def plot_hists(dataframe, features, rows, cols):\n    ''' for plotting histogram of each feature'''\n    fig=plt.figure(figsize=(25,25))\n    \n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title('Distribution: '+ feature,color='DarkRed')\n        \n    fig.tight_layout()  \n    plt.show()\n\nplot_hists(dataset,dataset.columns,5,4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3a) Preparing data for training models: Split data set into train and test sets. \n- We will train our models on 75% of the data set and subsequently test our model performance on the remaining 25%.\n- The aim is to get a reasonable measure of how well our model will work on a new set of data. This is the simplest form of cross-validation!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data set into train and test sets. Our models will be trained \n# using the former and evaluated on the test set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0, stratify=y)\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4) Train model A: Logistic regression\nHere we will train a logistic regression model to learn a relationship between the input features in our training data set and the subjects' diagnosis of CHD. We then use this model to predict the diagnosis of CHD on a new set of patients (X_test) and evaluate performance (% accuracy).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression to predict ten year risk of CHD.\nLR_clf = LogisticRegression(solver='liblinear')#random_state=0, solver='lbfgs')\nLR_clf.fit(X_train, y_train)\ny_LR_pred = LR_clf.predict(X_test)\ny_LR_pred_quant = LR_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### We will be looking at accuracy, sensitivity, specificity, and AUC using ROC curve plot.\n##### ROC Curve with Logistic Regression \n* logistic regression output is probabilities\n* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n* By default logistic regression threshold is 0.5\n* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n* If the curve in plot is closer to left-top corner, test is more accurate.\n* Roc curve score is auc that is computation area under the curve from prediction scores\n* We want auc to closer 1\n* fpr = False Positive Rate\n* tpr = True Positive Rate\n\n\\begin{align}\nSensitivity = \\frac{True\\:Positives}{True\\:Positives + False\\:Negatives}\n\\end{align}\n\\begin{align}\nSpecificity = \\frac{True\\:Negatives}{True\\:Negatives + False\\:Positives}\n\\end{align}"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression result analysis::\ncm_LR=cm(y_test,y_LR_pred)\n# print('Confusion matrix: ',cm_LR)\nsns.heatmap(cm_LR,annot=True)\nplt.show()\n\nfpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_LR_pred_quant)\nfig, ax = plt.subplots()\nax.plot(fpr_LR, tpr_LR)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for CHD: Logistic Regression classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.show()\nprint('Accuracy:',score(y_test,y_LR_pred)*100)\nsensitivity = cm_LR[0,0]/(cm_LR[0,0]+cm_LR[1,0])\nprint('Sensitivity : ', sensitivity )\nspecificity = cm_LR[1,1]/(cm_LR[1,1]+cm_LR[0,1])\nprint('Specificity : ', specificity)\nprint('AUC:',auc(fpr_LR, tpr_LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression to predict ten year risk of CHD.\nLR_clf = LogisticRegression(solver='liblinear', class_weight='balanced')#random_state=0, solver='lbfgs')\nLR_clf.fit(X_train, y_train)\ny_LR_pred = LR_clf.predict(X_test)\ny_LR_pred_quant = LR_clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5) Train model B: Random Forest classifier.\n*Here we will train a random forest model to learn the relationship between the input features in our training data set and the subjects' diagnosis of CHD. We then use this model to predict the diagnosis of CHD on a new set of patients (X_test) and measure performance (% accuracy).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_names = [i for i in X.columns]\ny_train_str = y_train.astype('str')\ny_train_str[y_train == 0] = 'no disease'\ny_train_str[y_train == 1] = 'CHD'\ny_train_str = y_train_str.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random forest model of ten year risk of CHD.\nmodel_RF = RandomForestClassifier(max_depth=4, class_weight='balanced', criterion='entropy')#, n_estimators=4)\nmodel_RF.fit(X_train, y_train)\nestimator = model_RF.estimators_[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.get_dummies(dataset, drop_first=True)\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = features,\n                class_names = ['no disease', 'CHD'],\n                rounded = True, proportion = True, \n                label='all',\n                precision = 7, filled = True, impurity=False)\n\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\nImage(filename = 'tree.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_RF_predict = model_RF.predict(X_test)\ny_RF_pred_quant = model_RF.predict_proba(X_test)[:, 1]\ny_RF_pred_bin = model_RF.predict(X_test)\nprint('Accuracy:',score(y_test,y_RF_predict)*100)\nconfusion_matrix_ = cm(y_test, y_RF_pred_bin)\nconfusion_matrix_\ntotal=sum(sum(confusion_matrix_))\nprint(confusion_matrix_)\nsensitivity = confusion_matrix_[0,0]/(confusion_matrix_[0,0]+confusion_matrix_[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix_[1,1]/(confusion_matrix_[1,1]+confusion_matrix_[0,1])\nprint('Specificity : ', specificity)\nfpr_RF, tpr_RF, thresholds = roc_curve(y_test, y_RF_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr_RF, tpr_RF)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for CHD: Random Forest classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nprint('AUC:',auc(fpr_RF, tpr_RF))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nax.plot(fpr_LR, tpr_LR, color='green', label='Logistic Reg')\nax.plot(fpr_RF, tpr_RF, color='blue', label='Random Forest')\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for CHD: Model Comparison')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend()\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(model_RF, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nexplainer = shap.TreeExplainer(model_RF)\nshap_values = explainer.shap_values(X_test)\n\n# shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\nshap_values = explainer.shap_values(X_train[:50])\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test[:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}