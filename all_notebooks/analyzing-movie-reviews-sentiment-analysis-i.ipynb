{"cells":[{"metadata":{"_uuid":"2a8f7168e29eb6c697a85584d55999ff2d294524"},"cell_type":"markdown","source":"# Analyzing Movie Reviews - Sentiment Analysis\nIn this notebook, we focus on trying to analyze a large corpus of movie reviews and derive the sentiment.\n\n[![image](http://www.moviereviewworld.com/wp-content/uploads/2013/06/movie-review-world-homepage-image.jpg)](http://www.moviereviewworld.com/)\n\nIn this first part, we cover a wide variety of techniques for analyzing sentiment, which include the following.\n- Unsupervised lexicon-based models\n- Traditional supervised Machine Learning models\n\nBesides looking at various approaches and models, we also focus on important aspects in the Machine Learning pipeline including text pre-processing, normalization, and in-depth analysis of models, including model interpretation and topic models. The key idea here is to understand how we tackle a problem like sentiment analysis on unstructured text, learn various techniques, models and understand how to interpret the results. This will enable you to use these methodologies in the future on your own datasets. Let's get started!"},{"metadata":{"toc":true,"_uuid":"3dcbd68028680603976553dff3cef525718fd642"},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-classify-Sentiment?\" data-toc-modified-id=\"How-to-classify-Sentiment?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to classify Sentiment?</a></span></li></ul></li><li><span><a href=\"#Preparing-environment-and-data\" data-toc-modified-id=\"Preparing-environment-and-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparing environment and data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-and-Setting-Up-Dependencies\" data-toc-modified-id=\"Import-and-Setting-Up-Dependencies-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import and Setting Up Dependencies</a></span></li><li><span><a href=\"#Text-Pre-Processing-and-Normalization\" data-toc-modified-id=\"Text-Pre-Processing-and-Normalization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Text Pre-Processing and Normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Text---strip-HTML\" data-toc-modified-id=\"Cleaning-Text---strip-HTML-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Cleaning Text - strip HTML</a></span></li><li><span><a href=\"#Removing-accented-characters\" data-toc-modified-id=\"Removing-accented-characters-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Removing accented characters</a></span></li><li><span><a href=\"#Expanding-Contractions\" data-toc-modified-id=\"Expanding-Contractions-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Expanding Contractions</a></span></li><li><span><a href=\"#Removing-Special-Characters\" data-toc-modified-id=\"Removing-Special-Characters-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Removing Special Characters</a></span></li><li><span><a href=\"#Lemmatizing-text\" data-toc-modified-id=\"Lemmatizing-text-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Lemmatizing text</a></span></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Removing Stopwords</a></span></li><li><span><a href=\"#Normalize-text-corpus---tying-it-all-together\" data-toc-modified-id=\"Normalize-text-corpus---tying-it-all-together-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Normalize text corpus - tying it all together</a></span></li></ul></li><li><span><a href=\"#Topics-Help-Functions\" data-toc-modified-id=\"Topics-Help-Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Topics Help Functions</a></span></li><li><span><a href=\"#Simplify-Get-Results\" data-toc-modified-id=\"Simplify-Get-Results-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Simplify Get Results</a></span></li><li><span><a href=\"#Load-and-normalize-data\" data-toc-modified-id=\"Load-and-normalize-data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Load and normalize data</a></span></li></ul></li><li><span><a href=\"#Sentiment-Analysis---Unsupervised-Lexical\" data-toc-modified-id=\"Sentiment-Analysis---Unsupervised-Lexical-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sentiment Analysis - Unsupervised Lexical</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-with-AFINN\" data-toc-modified-id=\"Sentiment-Analysis-with-AFINN-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Sentiment Analysis with AFINN</a></span></li><li><span><a href=\"#Sentiment-Analysis-with-SentiWordNet\" data-toc-modified-id=\"Sentiment-Analysis-with-SentiWordNet-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Sentiment Analysis with SentiWordNet</a></span></li><li><span><a href=\"#Sentiment-Analysis-with-VADER\" data-toc-modified-id=\"Sentiment-Analysis-with-VADER-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Sentiment Analysis with VADER</a></span></li></ul></li><li><span><a href=\"#Classifying-Sentiment-with-Supervised-Learning\" data-toc-modified-id=\"Classifying-Sentiment-with-Supervised-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Classifying Sentiment with Supervised Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Traditional-Supervised-Machine-Learning-Models\" data-toc-modified-id=\"Traditional-Supervised-Machine-Learning-Models-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Traditional Supervised Machine Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Prediction-and-Performance-Evaluation\" data-toc-modified-id=\"Prediction-and-Performance-Evaluation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Prediction and Performance Evaluation</a></span></li></ul></li></ul></li><li><span><a href=\"#Analyzing-Sentiment-Causation\" data-toc-modified-id=\"Analyzing-Sentiment-Causation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Analyzing Sentiment Causation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Text-Classification-Pipeline-with-The-Best-Model\" data-toc-modified-id=\"Build-Text-Classification-Pipeline-with-The-Best-Model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Build Text Classification Pipeline with The Best Model</a></span></li><li><span><a href=\"#Interpreting-Predictive-Models\" data-toc-modified-id=\"Interpreting-Predictive-Models-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Interpreting Predictive Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analyze-Model-Prediction-Probabilities\" data-toc-modified-id=\"Analyze-Model-Prediction-Probabilities-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Analyze Model Prediction Probabilities</a></span></li><li><span><a href=\"#Interpreting-Model-Decisions\" data-toc-modified-id=\"Interpreting-Model-Decisions-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Interpreting Model Decisions</a></span></li></ul></li><li><span><a href=\"#Analyzing-Topic-Models\" data-toc-modified-id=\"Analyzing-Topic-Models-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Analyzing Topic Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-features-from-positive-and-negative-reviews\" data-toc-modified-id=\"Extract-features-from-positive-and-negative-reviews-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Extract features from positive and negative reviews</a></span></li><li><span><a href=\"#Topic-Modeling-on-Reviews\" data-toc-modified-id=\"Topic-Modeling-on-Reviews-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Topic Modeling on Reviews</a></span></li><li><span><a href=\"#Visualize-topics-for-positive-reviews\" data-toc-modified-id=\"Visualize-topics-for-positive-reviews-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Visualize topics for positive reviews</a></span></li><li><span><a href=\"#Display-and-visualize-topics-for-negative-reviews\" data-toc-modified-id=\"Display-and-visualize-topics-for-negative-reviews-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Display and visualize topics for negative reviews</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"},{"metadata":{"_uuid":"e95ac4c6d6527080343b524f7a9626cf2c6e8c05"},"cell_type":"markdown","source":"## Introduction\n\nThe problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents.\n\nA text corpus consists of multiple text documents and each document can be as simple as a single sentence to a complete document with multiple paragraphs. Textual data, in spite of being highly unstructured, can be classified into two major types of documents:\n- ***Factual/objective documents***: typically depict some form of statements or facts with no specific feelings or emotion attached to them. \n- ***Subjective documents***: text that expresses feelings, moods, emotions, and opinions.\n\nTypically sentiment analysis seems to work best on subjective text, where people express opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.\n\n![image](https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg)\n\n**Sentiment analysis** is also popularly known as **opinion analysis** or **opinion mining**. The key idea is to use techniques from text analytics, NLP, Machine Learning, and linguistics to extract important information or data points from unstructured text. This in turn can help us derive ***qualitative outputs*** like the overall sentiment being on a ***positive***, ***neutral***, or ***negative*** scale and ***quantitative outputs*** like the sentiment ***polarity***, ***subjectivity***, and ***objectivity*** proportions. \n\n**Sentiment polarity** is typically a numeric score that's assigned to both the positive and negative aspects of a text document based on subjective parameters like specific words and phrases expressing feelings and emotion. Neutral sentiment typically has 0 polarity since it does not express and specific sentiment, positive sentiment will have polarity > 0, and negative < 0. Of course, you can always change these thresholds based on the type of text you are dealing with.\n\n### How to classify Sentiment?\n![image](https://www.kdnuggets.com/images/sentiment-fig-2-532.jpg)\n__Machine Learning__:\n\nThis approach, employes a machine-learning technique and diverse features to construct a classifier that can identify text that expresses sentiment. Nowadays, deep-learning methods are popular because they fit on data learning representations.\n\n__Lexicon-Based__:\n\nThis method uses a variety of words annotated by polarity score, to decide the general assessment score of a given content. The strongest asset of this technique is that it does not require any training data, while its weakest point is that a large number of words and expressions are not included in sentiment lexicons.\n\n__Hybrid__:\n\nThe combination of machine learning and lexicon-based approaches to address Sentiment Analysis is called Hybrid. Though not commonly used, this method usually produces more promising results than the approaches mentioned above.\n"},{"metadata":{"_uuid":"60620c098d158c01abad9f0133432832c92cde10"},"cell_type":"markdown","source":"## Preparing environment and data\n### Import and Setting Up Dependencies\n\nLet’s load the necessary dependencies and settings before getting started."},{"metadata":{"trusted":true,"_uuid":"684eda5d87532d11b200c40262f9a8eab7144f9a","_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\n\nnlp = spacy.load('en', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()\n\nimport datetime\nfrom datetime import timedelta\n \ndatetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n\nfrom sklearn.preprocessing import LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import NMF\nfrom sklearn.base import clone\n\nfrom scipy import interp\n\nfrom afinn import Afinn\nafn = Afinn(emoticons=True) \n\nimport nltk\nnltk.download('all', halt_on_error=False)\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport gensim\n\nfrom collections import Counter\n\nfrom IPython.display import SVG\n\n#from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\nfrom lime.lime_text import LimeTextExplainer\n\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\nnp.set_printoptions(precision=2, linewidth=80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb9a2832f4a75eed136d01b525968a57dfdf5234"},"cell_type":"markdown","source":"**Notes**: NLP libraries which will be used include spacy, nltk, and gensim. Do remember to check that your installed nltk version is at least >= 3.2.4, otherwise, the ToktokTokenizer class may not be present. If you want to use a lower nltk version for some reason, you can use any other tokenizer like the default word_tokenize() based on the TreebankWordTokenizer. The version for gensim should be at least 2.3.0 and for spacy, the version used was 1.9.0. We recommend using the latest version of spacy which was recently released (version 2.x) as this has fixed several bugs and added several improvements.\n\n### Text Pre-Processing and Normalization\n\nAn initial step in text and sentiment classification is pre-processing. A significant amount of techniques is applied to data in order to improvement of classification effectiveness. This enables standardization across a document corpus, which helps build meaningful features, to reduce dimensionality and reduce noise that can be introduced due to many factors like irrelevant symbols, special characters, XML and HTML tags, and so on.\n\nThe main components in our text normalization pipeline are:"},{"metadata":{"_uuid":"dbe19a042c3bfe4cc705a10ed0641f16bee02729"},"cell_type":"markdown","source":"#### Cleaning Text - strip HTML\nOur text often contains unnecessary content like HTML tags, which do not add much value when analyzing sentiment. Hence we need to make sure we remove them before extracting features. The BeautifulSoup library does an excellent job in providing necessary functions for this. Our strip_html_tags(...) function enables in cleaning and stripping out HTML code."},{"metadata":{"trusted":true,"_uuid":"ad69ec0f9c932cfa40586e08ad1e30fd158dce2f"},"cell_type":"code","source":"def strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ada07fcff515a1b479456c628031b205ce6cbf1"},"cell_type":"markdown","source":"#### Removing accented characters\nIn our dataset, we are dealing with reviews in the English language so we need to make sure that characters with any other format, especially accented characters are converted and standardized into ASCII characters. A simple example would be converting é to e. Our remove_accented_chars(...) function helps us in this respect."},{"metadata":{"trusted":true,"_uuid":"a66111cd86afbf71afb381c07fd29aad2fb71510"},"cell_type":"code","source":"def remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e028ca43e7fa37449e66dc91a07d1be695616cd9"},"cell_type":"markdown","source":"#### Expanding Contractions\nIn the English language, contractions are basically shortened versions of words or syllables. Contractions pose a problem in text normalization because we have to deal with special characters like the apostrophe and we also have to convert each contraction to its expanded, original form. Our expand_contractions(...) function uses regular expressions and various contractions mapped to expand all contractions in our text corpus."},{"metadata":{"trusted":true,"_uuid":"d17dd783d10ddb29b4bd2b92c14c6de55b461bb4"},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# Contraction Map\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32f01ef28221b2ac7c3292aaa7020241e6018cc9"},"cell_type":"markdown","source":"#### Removing Special Characters\nSimple regexes can be used to achieve this. Our function remove_special_characters(...) helps us remove special characters. In our code, we have retained numbers but you can also remove numbers if you do not want them in your normalized corpus."},{"metadata":{"trusted":true,"_uuid":"b1e9733ea613c748ef985aceec6432e4d38b05e5"},"cell_type":"code","source":"def remove_special_characters(text):\n    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d615e8f3f7ae86d29dc0511e81c7aef6135cb072"},"cell_type":"markdown","source":"#### Lemmatizing text\n**Word stems** are usually the base form of possible words that can be created by ***attaching affixes*** like prefixes and suffixes ***to the stem*** to create new words. This is known as **inflection**. The **reverse process** of obtaining the base form of a word is known as **stemming**. The nltk package offers a wide range of stemmers like the PorterStemmer and LancasterStemmer. **Lemmatization** is very similar to stemming, where we remove word affixes to get to the base form of a word. However the base form in this case is known as the **root word** but not the root stem. The difference being that ***the root word is always a lexicographically correct word***, present in the dictionary, but the root stem may not be so. We will be using lemmatization only in our normalization pipeline to retain lexicographically correct words. The function lemmatize_text(...) helps us with this aspect."},{"metadata":{"trusted":true,"_uuid":"17730ba345b559e93ba0c54197505f57114a20e0"},"cell_type":"code","source":"def lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2e6a21537291231317c521eb7eeaff4ccbb52b"},"cell_type":"markdown","source":"#### Removing Stopwords\nWords which have little or no significance especially when constructing meaningful features from text are also known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a document corpus. Words like a, an, the, and so on are considered to be stopwords. There is no universal stopword list but we use a standard English language stopwords list from nltk. You can also add your own domain specific stopwords if needed. The function remove_stopwords(...) helps us remove stopwords and retain words having the most significance and context in a corpus."},{"metadata":{"trusted":true,"_uuid":"e5e0e1bec9a1e48407d3f6ee89336e6de577073a"},"cell_type":"code","source":"stopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f2d964cd0258402be87ddb2b623273cc89c4696"},"cell_type":"markdown","source":"#### Normalize text corpus - tying it all together\n\nWe use all these components and tie them together in the following function called normalize_corpus(...), which can be used to take a document corpus as input and return the same corpus with cleaned and normalized text documents."},{"metadata":{"trusted":true,"_uuid":"9b502d950565af91ded35353125d0e805c279bdc"},"cell_type":"code","source":"def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n        # expand contractions    \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        # remove special characters    \n        if special_char_removal:\n            doc = remove_special_characters(doc)  \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc6f849db45552bcc4e4ef9870a8c0c92d8cd53a"},"cell_type":"markdown","source":"### Topics Help Functions\n\nWe will also leverage some utility functions to support get and display topics from a corpus with their terms and weights."},{"metadata":{"trusted":true,"_uuid":"aeeeab503fffd2e6edaaa428b5e0d8423c6079a8"},"cell_type":"code","source":"# Prints components of all the topics obtained from topic modeling\ndef print_topics_udf(topics, total_topics=1,\n                     weight_threshold=0.0001,\n                     display_weights=False,\n                     num_terms=None):\n    \n    for index in range(total_topics):\n        topic = topics[index]\n        topic = [(term, float(wt))\n                 for term, wt in topic]\n        topic = [(word, round(wt,2)) \n                 for word, wt in topic \n                 if abs(wt) >= weight_threshold]\n                     \n        if display_weights:\n            print('Topic #'+str(index+1)+' with weights')\n            print(topic[:num_terms]) if num_terms else topic\n        else:\n            print('Topic #'+str(index+1)+' without weights')\n            tw = [term for term, wt in topic]\n            print(tw[:num_terms]) if num_terms else tw\n        print()\n        \n\n# Extracts topics with their terms and weights \n# Format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \ndef get_topics_terms_weights(weights, feature_names):\n    feature_names = np.array(feature_names)\n    sorted_indices = np.array([list(row[::-1]) \n                           for row \n                           in np.argsort(np.abs(weights))])\n    sorted_weights = np.array([list(wt[index]) \n                               for wt, index \n                               in zip(weights,sorted_indices)])\n    sorted_terms = np.array([list(feature_names[row]) \n                             for row \n                             in sorted_indices])\n    \n    topics = [np.vstack((terms.T, term_weights.T)).T \n              for terms, term_weights \n              in zip(sorted_terms, sorted_weights)]     \n    \n    return topics         ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d642b10d39068d6350e5dfdccd7a41b72b4143fa"},"cell_type":"markdown","source":"### Simplify Get Results\nLet's build a function to standardize the capture and exposure of the results of our models.\n\nAs a classification problem, Sentiment Analysis uses the evaluation metrics of Precision, Recall, F-score, and Accuracy. Also, average measures like macro, micro, and weighted F1-scores are useful for multi-class problems. "},{"metadata":{"trusted":true,"_uuid":"390d2827bfb7c7a0c9ab88d831db490d9a90018a"},"cell_type":"code","source":"def get_results(model, name, data, true_labels, target_names = ['positive', 'negative'], results=None, reasume=False):\n\n    if hasattr(model, 'layers'):\n        param = wtp_dnn_model.history.params\n        best = np.mean(wtp_dnn_model.history.history['val_acc'])\n        predicted_labels = model.predict_classes(data) \n        im_model = InMemoryModel(model.predict, examples=data, target_names=target_names)\n\n    else:\n        param = gs.best_params_\n        best = gs.best_score_\n        predicted_labels = model.predict(data).ravel()\n        if hasattr(model, 'predict_proba'):\n            im_model = InMemoryModel(model.predict_proba, examples=data, target_names=target_names)\n        elif hasattr(clf, 'decision_function'):\n            im_model = InMemoryModel(model.decision_function, examples=data, target_names=target_names)\n        \n    print('Mean Best Accuracy: {:2.2%}'.format(best))\n    print('-'*60)\n    print('Best Parameters:')\n    print(param)\n    print('-'*60)\n    \n    y_pred = model.predict(data).ravel()\n    \n    display_model_performance_metrics(true_labels, predicted_labels = predicted_labels, target_names = target_names)\n    if len(target_names)==2:\n        ras = roc_auc_score(y_true=true_labels, y_score=y_pred)\n    else:\n        roc_auc_multiclass, ras = roc_auc_score_multiclass(y_true=true_labels, y_score=y_pred, target_names=target_names)\n        print('\\nROC AUC Score by Classes:\\n',roc_auc_multiclass)\n        print('-'*60)\n\n    print('\\n\\n              ROC AUC Score: {:2.2%}'.format(ras))\n    prob, score_roc, roc_auc = plot_model_roc_curve(model, data, true_labels, label_encoder=None, class_names=target_names)\n    \n    interpreter = Interpretation(data, feature_names=cols)\n    plots = interpreter.feature_importance.plot_feature_importance(im_model, progressbar=False, n_jobs=1, ascending=True)\n    \n    r1 = pd.DataFrame([(prob, best, np.round(accuracy_score(true_labels, predicted_labels), 4), \n                         ras, roc_auc)], index = [name],\n                         columns = ['Prob', 'CV Accuracy', 'Accuracy', 'ROC AUC Score', 'ROC Area'])\n    if reasume:\n        results = r1\n    elif (name in results.index):        \n        results.loc[[name], :] = r1\n    else: \n        results = results.append(r1)\n        \n    return results\n\ndef roc_auc_score_multiclass(y_true, y_score, target_names, average = \"macro\"):\n\n  #creating a set of all the unique classes using the actual class list\n  unique_class = set(y_true)\n  roc_auc_dict = {}\n  mean_roc_auc = 0\n  for per_class in unique_class:\n    #creating a list of all the classes except the current class \n    other_class = [x for x in unique_class if x != per_class]\n\n    #marking the current class as 1 and all other classes as 0\n    new_y_true = [0 if x in other_class else 1 for x in y_true]\n    new_y_score = [0 if x in other_class else 1 for x in y_score]\n    num_new_y_true = sum(new_y_true)\n\n    #using the sklearn metrics method to calculate the roc_auc_score\n    roc_auc = roc_auc_score(new_y_true, new_y_score, average = average)\n    roc_auc_dict[target_names[per_class]] = np.round(roc_auc, 4)\n    mean_roc_auc += num_new_y_true * np.round(roc_auc, 4)\n    \n  mean_roc_auc = mean_roc_auc/len(y_true)  \n  return roc_auc_dict, mean_roc_auc\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print('Accuracy:  {:2.2%} '.format(metrics.accuracy_score(true_labels, predicted_labels)))\n    print('Precision: {:2.2%} '.format(metrics.precision_score(true_labels, predicted_labels, average='weighted')))\n    print('Recall:    {:2.2%} '.format(metrics.recall_score(true_labels, predicted_labels, average='weighted')))\n    print('F1 Score:  {:2.2%} '.format(metrics.f1_score(true_labels, predicted_labels, average='weighted')))\n                        \n\ndef train_predict_model(classifier,  train_features, train_labels,  test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, target_names):\n    \n    total_classes = len(target_names)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[['Predicted:'], target_names], labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], target_names], labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, target_names):\n\n    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=target_names) \n    print(report)\n    \ndef display_model_performance_metrics(true_labels, predicted_labels, target_names):\n    print('Model Performance metrics:')\n    print('-'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print('\\nModel Classification report:')\n    print('-'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n    print('\\nPrediction Confusion Matrix:')\n    print('-'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, 'classes_'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n    n_classes = len(class_labels)\n   \n    if n_classes == 2:\n        if hasattr(clf, 'predict_proba'):\n            prb = clf.predict_proba(features)\n            if prb.shape[1] > 1:\n                y_score = prb[:, prb.shape[1]-1] \n            else:\n                y_score = clf.predict(features).ravel()\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n        \n        fpr, tpr, _ = roc_curve(true_labels, y_score)      \n        roc_auc = auc(fpr, tpr)\n\n        plt.plot(fpr, tpr, label='ROC curve (area = {0:3.2%})'.format(roc_auc), linewidth=2.5)\n        \n    elif n_classes > 2:\n        if  hasattr(clf, 'clfs_'):\n            y_labels = label_binarize(true_labels, classes=list(range(len(class_labels))))\n        else:\n            y_labels = label_binarize(true_labels, classes=class_labels)\n        if hasattr(clf, 'predict_proba'):\n            y_score = clf.predict_proba(features)\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n            \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_labels[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_labels.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n\n        for i, label in enumerate(class_names):\n            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:2.2%})'\n                                           ''.format(label, roc_auc[i]), linewidth=2, linestyle=':')\n        roc_auc = roc_auc[\"macro\"]   \n    else:\n        raise ValueError('Number of classes should be atleast 2 or more')\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.01])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return prob, y_score, roc_auc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"736d541e760a6e5061cf9a6d45497022ad18db62"},"cell_type":"markdown","source":"### Load and normalize data\nWe can now load our IMDb movie reviews dataset, use the first 40,000 reviews for training models and the remaining 10,000 reviews as the test dataset to evaluate model performance."},{"metadata":{"trusted":true,"_uuid":"df68c4b6691ddc95ce72af60d66afda8aa52f2b6"},"cell_type":"code","source":"dataset = pd.read_csv(r'../input/movie_reviews.csv')\nreviews = np.array(dataset['review'])\nsentiments = np.array(dataset['sentiment'])\n\n# take a peek at the data\ndisplay(dataset.head())\n\n# build train and test datasets\ntrain_reviews, test_reviews, train_sentiments, test_sentiments =\\\n    train_test_split(reviews, sentiments , test_size=0.20,  random_state=101)\n\nsample_review_ids = [7626, 3533, 9010]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34355b3ab36bf4ef1ae4fa7abf3624a278c2fee2"},"cell_type":"markdown","source":" Now, we will also use our normalization module to normalize our review datasets. This is a time-consuming operation."},{"metadata":{"trusted":true,"_uuid":"79208bdaa098b439e29936c4bd22e9a2451660a1"},"cell_type":"code","source":"now = datetime.datetime.now()\nprint('Current date and time: {}'.format(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n\n# normalize training dataset\nprint('-'*60)\nprint('Normalize training dataset:')\nnorm_train_reviews = normalize_corpus(train_reviews)\ndiff = (datetime.datetime.now() - now)\nnow = datetime.datetime.now()\nprint('Elapsed time: {}\\n'.format(diff))\n\n# normalize test dataset\nprint('-'*60)\nprint('Normalize test dataset:')\nnorm_test_reviews = normalize_corpus(test_reviews)\ndiff = (datetime.datetime.now() - now)\nprint('Elapsed time: {}\\n'.format(diff))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ac98f20112167bd7e1ab062d4042e46d165acc"},"cell_type":"markdown","source":"## Sentiment Analysis - Unsupervised Lexical\n\nEven though we have labeled data, this section should give you a good idea of how lexicon based models work and you can apply the same in your own datasets when you do not have labeled data.\n\nUnsupervised sentiment analysis models use well curated knowledgebases, ontologies, lexicons, and databases that have detailed information pertaining to subjective words, phrases including sentiment, mood, polarity, objectivity, subjectivity, and so on. A lexicon model typically uses a lexicon, also known as a dictionary or vocabulary of words specifically aligned toward sentiment analysis. Usually these lexicons contain a list of words associated with positive and negative sentiment, polarity (magnitude of negative or positive score), parts of speech (POS) tags, subjectivity classifiers (strong, weak, neutral), mood, modality, and so on. You can use these lexicons and compute sentiment of a text document by matching the presence of specific words from the lexicon, look at other additional factors like presence of negation parameters, surrounding words, overall context and phrases and aggregate overall sentiment polarity scores to decide the final sentiment score. \n\n![image](https://image.slidesharecdn.com/iccbr-12-main-121111094448-phpapp01/95/sentiment-classification-with-casebased-reasoning-10-638.jpg)\n\nThere are several popular lexicon models used for sentiment analysis. Some of them are mentioned as follows.\n- Bing Liu’s Lexicon\n- MPQA Subjectivity Lexicon\n- Pattern Lexicon\n- AFINN Lexicon\n- SentiWordNet Lexicon\n- VADER Lexicon\n\nThis is not an exhaustive list of lexicon models, but definitely lists among the most popular ones available today. Since we have labeled data, it will be easy for us to see how well our actual sentiment values for these movie reviews match our lexiconmodel based predicted sentiment values. We will be covering the last three lexicon models in more detail and predict their sentiment and see how well our model performs based on model evaluation metrics like accuracy, precision, recall, and F1-score.\n\n\n### Sentiment Analysis with AFINN\n\nThe [AFINN lexicon](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis. It is a list of words rated for valence with an integer between minus five (negative) and plus five (positive).  The current version of the lexicon is [AFINN-en-165.txt](https://github.com/fnielsen/afinn/blob/master/afinn/data/) and it contains over 3,300+ words with a polarity score associated with each word. The author has also created a nice wrapper library on top of this in Python called afinn which we will be using for our analysis needs. AFINN takes into account other aspects like emoticons and exclamations.\n![image](https://image.slidesharecdn.com/phpbnl18-machine-learning-180126163450/95/learning-machine-learning-31-638.jpg)\n\nWe can now use this object and compute the polarity of our chosen four sample reviews. The results permit you compare the actual sentiment label for each review and also check out the predicted sentiment polarity score. A negative polarity typically denotes negative sentiment. "},{"metadata":{"trusted":true,"_uuid":"20dd660b16893f8b7da5964d654f13f3ab459f78"},"cell_type":"code","source":"sample_review_ids = [7626, 3533, 9010]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1e24c1e6cb034bd1fe03d703e4a383fc1f05c2"},"cell_type":"code","source":"for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    print('Predicted Sentiment polarity:', afn.score(review))\n    print('-'*60)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee8493c3b4cdb7f8bbf1349ac5f6efd5282e777"},"cell_type":"markdown","source":"Below we used a threshold of >= 2.0 to determine if the overall sentiment is positive else negative. You can choose your own threshold based on analyzing your own corpora in the future."},{"metadata":{"trusted":true,"_uuid":"4062a7c125450ff6b00aa8d3ee8b5e56ce4413d4"},"cell_type":"code","source":"sentiment_polarity = [afn.score(review) for review in test_reviews]\npredicted_sentiments = ['positive' if score >= 2.0 else 'negative' for score in sentiment_polarity]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05685f440efe5338cf66c7d6ff75f133fe087cb8"},"cell_type":"markdown","source":"Now that we have our predicted sentiment labels, we can evaluate our model performance based on standard performance metrics using our utility function."},{"metadata":{"trusted":true,"_uuid":"03ac7d475aa593b64c003c084034091c710c397b"},"cell_type":"code","source":"display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa051af5b3157f5dcb698578c3887155465443fa"},"cell_type":"markdown","source":"We get an overall F1-Score of 72%, which is quite decent considering it's an unsupervised model. Looking at the confusion matrix we can clearly see that quite a number of positive sentiment based reviews have been misclassified as negative (1,848) and this leads to the lower recall of 63% for the positive sentiment class. Performance for negative class is better with regard to recall or f1-score, where we correctly predicted 4,131 out of 5,041 negative reviews, but precision is 69% because of the many wrong negative predictions made in case of positive sentiment reviews.\n\n### Sentiment Analysis with SentiWordNet\nThe WordNet corpus is definitely one of the most popular corpora for the English language used extensively in natural language processing and semantic analysis. WordNet gave us the concept of ***synsets*** or ***synonym sets***. The SentiWordNet lexicon is based on WordNet synsets and can be used for sentiment analysis and opinion mining. The [SentiWordNet](http://sentiwordnet.isti.cnr.it) lexicon typically assigns three sentiment scores for each WordNet synset. These include a positive polarity score, a negative polarity score and an objectivity score. We will be using the nltk library, which provides a Pythonic interface into [SentiWordNet](https://pt.coursera.org/lecture/text-mining-analytics/5-6-how-to-do-sentiment-analysis-with-sentiwordnet-5RwtX). Consider we have the adjective awesome. \n![image](https://player.slideplayer.com/11/3238511/data/images/img18.png)"},{"metadata":{"trusted":true,"_uuid":"2ccda1714c5705fdf6bf8e90ada754f6f7dbe6a0"},"cell_type":"code","source":"awesome = list(swn.senti_synsets('awesome', 'a'))[0]\nprint('Positive Polarity Score:', awesome.pos_score())\nprint('Negative Polarity Score:', awesome.neg_score())\nprint('Objective Score:', awesome.obj_score())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1916fc9b018866443601c9447e07fde2c1de6784"},"cell_type":"markdown","source":"Let's now build a generic function to extract and aggregate sentiment scores for a complete textual document based on matched synsets in that document. Our function basically takes in a movie review, tags each word with its corresponding POS tag, extracts out sentiment scores for any matched synset token based on its POS tag, and finally aggregates the scores. We can clearly see the predicted sentiment along with sentiment polarity scores and an objectivity score for each sample movie review depicted in formatted dataframes. "},{"metadata":{"trusted":true,"_uuid":"9e208955ee7ec33cbb68e96cc1bb94918bf42be8"},"cell_type":"code","source":"def analyze_sentiment_sentiwordnet_lexicon(review, verbose=False):\n\n    # tokenize and POS tag text tokens\n    tagged_text = [(token.text, token.tag_) for token in nlp(review)]\n    pos_score = neg_score = token_count = obj_score = 0\n    # get wordnet synsets based on POS tags\n    # get sentiment scores if synsets are found\n    for word, tag in tagged_text:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        # if senti-synset is found        \n        if ss_set:\n            # add scores for all found synsets\n            pos_score += ss_set.pos_score()\n            neg_score += ss_set.neg_score()\n            obj_score += ss_set.obj_score()\n            token_count += 1\n    \n    # aggregate final scores\n    final_score = pos_score - neg_score\n    norm_final_score = round(float(final_score) / token_count, 2)\n    final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n    if verbose:\n        norm_obj_score = round(float(obj_score) / token_count, 2)\n        norm_pos_score = round(float(pos_score) / token_count, 2)\n        norm_neg_score = round(float(neg_score) / token_count, 2)\n        # to display results in a nice table\n        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n                                         norm_neg_score, norm_final_score]],\n                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                             ['Predicted Sentiment', 'Objectivity',\n                                                              'Positive', 'Negative', 'Overall']], \n                                                             labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        display(sentiment_frame)\n        \n    return final_sentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffcd32dc8350bd139d9367f2b010e2f318de7a90"},"cell_type":"markdown","source":"Let's use this model now to predict the sentiment of samples reviews and compare their results with its actual values."},{"metadata":{"trusted":true,"_uuid":"4a8234af4c33e48fed3fd3eb4c654d2655af218b"},"cell_type":"code","source":"for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:\\n', review)\n    print('\\nActual Sentiment:', sentiment)\n    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae38f6f0ac1363beb79de84c45b516011636a363"},"cell_type":"markdown","source":"Let's use this model now to predict the sentiment of all our test reviews and evaluate its performance. A threshold of >=0 has been used for the overall sentiment polarity to be classified as positive and < 0 for negative sentiment."},{"metadata":{"trusted":true,"_uuid":"d54fa4f65e4a972364a6a73446ddb85e9d37396f"},"cell_type":"code","source":"predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]\n\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ba4124a960bbe671e0e1421e0040332af5fbf5d"},"cell_type":"markdown","source":"We get an overall F1-Score of 55%, which is definitely a step down from our AFINN based model. While we have lesser number of negative sentiment based reviews being misclassified as positive, the other aspects of the model performance have been affected.\n\n### Sentiment Analysis with VADER\nThe [VADER lexicon](https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text), developed by C.J. Hutto, is a lexicon that is based on a rule-based sentiment analysis framework, specifically tuned to analyze sentiments in social media. VADER stands for Valence Aware Dictionary and Sentiment Reasoner. You can use the library based on nltk's interface under the nltk.sentiment.vader module. Besides this, you can also [download the actual lexicon or install the framework](https://github.com/cjhutto/\nvaderSentiment). The file titled vader_lexicon.txt contains necessary sentiment scores associated with words, emoticons and slangs (like wtf, lol, nah, and so on). There were a total of over 9,000 lexical features from which over 7,500 curated lexical features were finally selected in the lexicon with proper validated valence scores. Each feature was rated on a scale from \"[-4] Extremely Negative\" to \"[4] Extremely Positive\", with allowance for \"[0] Neutral (or Neither, N/A)\". The process of selecting lexical features was done by keeping all features that had a non-zero mean rating and whose standard deviation was less than 2.5, which was determined by the aggregate of ten independent raters. \n![image](https://image.slidesharecdn.com/capstoneprojectgadatasciencelinm-160512021206/95/sentiment-analysis-of-airline-tweets-15-638.jpg)\n\nNow let's use VADER to analyze our movie reviews! We build our own modeling function as follows. In our modeling function, we do some basic pre-processing but keep the punctuations and emoticons intact. Besides this, we use VADER to get the sentiment polarity and also proportion of the review text with regard to positive, neutral and negative sentiment. We also predict the final sentiment based on a user-input threshold for the aggregated sentiment polarity."},{"metadata":{"trusted":true,"_uuid":"6b4d27dd2c09fc7769d2bd332843cb19bd7d46c5"},"cell_type":"code","source":"def analyze_sentiment_vader_lexicon(review, threshold=0.1, verbose=False):\n    # pre-process text\n    review = strip_html_tags(review)\n    review = remove_accented_chars(review)\n    review = expand_contractions(review)\n    \n    # analyze the sentiment for review\n    analyzer = SentimentIntensityAnalyzer()\n    scores = analyzer.polarity_scores(review)\n    # get aggregate scores and final sentiment\n    agg_score = scores['compound']\n    final_sentiment = 'positive' if agg_score >= threshold\\\n                                   else 'negative'\n    if verbose:\n        # display detailed sentiment statistics\n        positive = str(round(scores['pos'], 2)*100)+'%'\n        final = round(agg_score, 2)\n        negative = str(round(scores['neg'], 2)*100)+'%'\n        neutral = str(round(scores['neu'], 2)*100)+'%'\n        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, negative, neutral]],\n                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                                      ['Predicted Sentiment', 'Polarity Score',\n                                                                       'Positive', 'Negative', 'Neutral']], \n                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        display(sentiment_frame)\n    \n    return final_sentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4306d59009132ce92100842d721955b09a8f3c"},"cell_type":"markdown","source":"Let's see how our model classify our samples and compare with their actual values. Typically, VADER recommends using positive sentiment for aggregated polarity >= 0.5, neutral between [-0.5, 0.5], and negative for polarity < -0.5. We use a threshold of >= 0.4 for positive and < 0.4 for negative in our corpus. The following is the analysis of our sample reviews."},{"metadata":{"trusted":true,"_uuid":"2193860494255c60779241aa02e7550c19b110b1"},"cell_type":"code","source":"for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9b06948526c9073badb0b3c458c12ff68fb0251"},"cell_type":"markdown","source":"Let's try out our model on the complete test movie review corpus now and evaluate the model performance."},{"metadata":{"trusted":true,"_uuid":"5753eaa44492ed3fdce08f043e0f1a2b03ac88b7"},"cell_type":"code","source":"predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.5, verbose=False) for review in test_reviews]\n\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38917b22908f0d2bddc8c36663ab4547e3f33c09"},"cell_type":"markdown","source":"We get an overall F1-Score and model accuracy of 72%, which is quite similar to the AFINN based model. The AFINN based model only wins for very little, both models have a similar performance."},{"metadata":{"_uuid":"9fb3216788e64eba1d87e4bd8f991bea01926b1e"},"cell_type":"markdown","source":"## Classifying Sentiment with Supervised Learning\n\n__Introduction:__\n\nWe will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n1. Prepare train and test datasets (optionally a validation dataset)\n2. Pre-process and normalize text documents\n3. Feature engineering\n4. Model training\n5. Model prediction and evaluation\n\n![image](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-1-4842-2388-8_4/MediaObjects/427287_1_En_4_Fig2_HTML.jpg)<center>Blueprint for building an automated text classification system (Source: Text Analytics with Python, Apress 2016)</center>\n\nIn our scenario, documents indicate the movie reviews and classes indicate the review sentiments that can either be positive or negative, making it a binary classification problem. \n\n### Feature Engineering\nOur feature engineering techniques will be based on the Bag of Words model and the TF-IDF model.\n\nThe ***bag-of-words model*** is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. The core principle is to convert text documents into numeric vectors. The dimension or size of each vector is N where N indicates all possible distinct words across the corpus of documents. Each document once transformed is a numeric vector of size N where the values or weights in the vector indicate the frequency of each word in that specific document. Hence the name bag of words because this model represents unstructured text into a bag of words without taking into account word positions, syntax, or semantics.\n![image](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg?resize=800%2C203)\n\nThere are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these will tend to overshadow other terms in the feature set. The ***[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) model*** tries to combat this issue by using a scaling or normalizing factor in its computation. TF-IDF stands for Term Frequency-Inverse Document Frequency, which uses a combination of two metrics in its computation, namely: term frequency (tf) and inverse document frequency (idf). This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of information retrieval and text analytics.\n![image](https://skymind.ai/images/wiki/tfidf.png?resize=40%2C20)"},{"metadata":{"trusted":true,"_uuid":"e6eb8daecf8f1dbb9d6504b9a9130579b1df4291"},"cell_type":"code","source":"# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n\n# build TFIDF features on train reviews\ntv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2), sublinear_tf=True)\ntv_train_features = tv.fit_transform(norm_train_reviews)\n\n\n# transform test reviews into features\ncv_test_features = cv.transform(norm_test_reviews)\ntv_test_features = tv.transform(norm_test_reviews)\n\nprint('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\nprint('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0810ca86f1cb75dadd2b608ae44c3ecad317c5b4"},"cell_type":"markdown","source":"### Traditional Supervised Machine Learning Models\n\nWe can now use some traditional supervised Machine Learning algorithms which work very well on text classification. We recommend using logistic regression, support vector machines, and multinomial Naïve Bayes models\n\n#### Model Training\nThe logistic regression is intended for binary (two-class) classification problems, where it will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification. In this case, we try to predict the probability that a given movie review will belong to one of the discrete classes.\n\n**<center>P(X) = P(Y=1|X)</center>**"},{"metadata":{"trusted":true,"_uuid":"ecb7bc06e67f5ec5fa7af45ce14c98d7525516cc"},"cell_type":"code","source":"lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\nsvm = SGDClassifier(loss='hinge', l1_ratio=0.15, max_iter=300, n_jobs=4, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9d3ac8a432a8dae99976428e22bdf0d14b27031"},"cell_type":"markdown","source":"#### Prediction and Performance Evaluation\nWe will now use our utility function train_predict_model(...) to build a logistic regression model on our training features and evaluate the model performance on our test features."},{"metadata":{"trusted":true,"_uuid":"90deceaf556064d66aff23e0b19eaf4631ad86d8"},"cell_type":"code","source":"# Logistic Regression model on BOW features\nlr_bow_predictions = train_predict_model(classifier=lr, \n                                         train_features=cv_train_features, train_labels=train_sentiments,\n                                         test_features=cv_test_features, test_labels=test_sentiments)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2debfa653fd399dec8a68101938cbac66f48c0e1"},"cell_type":"markdown","source":"We get all metrics as **91%**, which is really excellent! \n\nWe can now build a logistic regression model similarly on our TF-IDF features and see if we can get better results."},{"metadata":{"trusted":true,"_uuid":"2e5a5bb8ce926a6fd7733ce1cbaa290ef5e48c6f"},"cell_type":"code","source":"# Logistic Regression model on TF-IDF features\nlr_tfidf_predictions = train_predict_model(classifier=lr, \n                                           train_features=tv_train_features, train_labels=train_sentiments,\n                                           test_features=tv_test_features, test_labels=test_sentiments)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef351db31a16bfe0590dfb871ddfb99d2ad1cdd"},"cell_type":"markdown","source":"As you can see we get all metrics close to 90%, which which is great but our previous model is still slightly better.\n\nLet's see if we can do better with SVM:"},{"metadata":{"trusted":true,"_uuid":"84646992bb226f061ac3e5eca4ab8147b5447388"},"cell_type":"code","source":"svm_bow_predictions = train_predict_model(classifier=svm, \n                                          train_features=cv_train_features, train_labels=train_sentiments,\n                                          test_features=cv_test_features, test_labels=test_sentiments)\nprint('SVM results with Bow:')\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n                                 target_names=['positive', 'negative'])\n\n\nsvm_tfidf_predictions = train_predict_model(classifier=svm, \n                                            train_features=tv_train_features, train_labels=train_sentiments,\n                                            test_features=tv_test_features, test_labels=test_sentiments)\nprint('-'*60)\nprint('\\nSVM results with TF-IDF:')\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n                                  target_names=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2ab240299853685ff263444078780dcf2e23e01"},"cell_type":"markdown","source":"As you can see, again we obtened all scores close to 90%. Let's see if we can improve with apply a DNN model.\n\nLet's try to understanding our best model."},{"metadata":{"_uuid":"d10a366e5100af3998b27a2b2c1931f39b4ea587"},"cell_type":"markdown","source":"## Analyzing Sentiment Causation\n\nBusiness and key stakeholders often perceive Machine Learning models as complex black boxes and poses the question, why should I trust your model? Explaining to them complex mathematical or theoretical concepts doesn't serve the purpose. Is there some way in which we can explain these models in an easy-to-interpret manner?\n\n[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/video_screenshot.png)](https://www.youtube.com/watch?v=hUnRCxnydCc)\n\nIn the analyze sentiment causation, the main idea is to determine the root cause or key factors causing positive or negative sentiment. The first area of focus will be model interpretation, where we will try to understand, interpret, and explain the mechanics behind predictions made by our classification models. The second area of focus is to apply topic modeling and extract key topics from positive and negative sentiment reviews.\n\n### Build Text Classification Pipeline with The Best Model\nLet's first build a basic text classification pipeline for the model that worked best for us so far. This is the Logistic Regression model based on the Bag of Words feature model. We will leverage the pipeline module from scikit-learn to build this Machine Learning pipeline using the following code."},{"metadata":{"trusted":true,"_uuid":"f62d90ce0a35cab2ee621fa53c3a397de463daa6"},"cell_type":"code","source":"# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n\n# build Logistic Regression model\nlr = LogisticRegression()\nlr.fit(cv_train_features, train_sentiments)\n\n# Build Text Classification Pipeline\nlr_pipeline = make_pipeline(cv, lr)\n\n# save the list of prediction classes (positive, negative)\nclasses = list(lr_pipeline.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51de91487af5ab0c42445e5e99bb0510e865856d"},"cell_type":"markdown","source":"We build our model based on norm_train_reviews, which contains the normalized training reviews that we have used in all our earlier analyses. Now that we have our classification pipeline ready, you can actually deploy the model by using pickle or joblib to save the classifier and feature objects \n\n### Interpreting Predictive Models\nThere are various ways to interpret the predictions made by our predictive sentiment classification models. We want to understand more into why a positive review was correctly predicted as having positive sentiment or a negative review having negative sentiment. Besides this, no model is a 100% accurate always, so we would also want to understand the reason for mis-classifications or wrong predictions. \n\n#### Analyze Model Prediction Probabilities\nAssuming our pipeline is in production, how do we use it for new movie reviews? Let's try to predict the sentiment for two new sample reviews, which were not used in training the model:"},{"metadata":{"trusted":true,"_uuid":"8c05a4ee61b25db5bdbaf70fe212c603a005f13f"},"cell_type":"code","source":"lr_pipeline.predict(['the lord of the rings is an excellent movie', 'i hated the recent movie on tv, it was so bad'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf5449cc17bdac5456968813efbba07fba91595"},"cell_type":"markdown","source":"Our classification pipeline predicts the sentiment of both the reviews correctly! This is a good start, but how do we interpret the model predictions? One way is to typically use the model prediction class probabilities as a measure of confidence. You can use the following code to get the prediction probabilities for our sample reviews."},{"metadata":{"trusted":true,"_uuid":"6dc20a22012fee8064a9f332da54559c634a6bd0"},"cell_type":"code","source":"pd.DataFrame(lr_pipeline.predict_proba(['the lord of the rings is an excellent movie', \n                     'i hated the recent movie on tv, it was so bad']), columns=classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4480548252cd12b4d67d54ac5ed40dd2d8e883e9"},"cell_type":"markdown","source":"Thus we can say that the first movie review has a prediction confidence or probability of 83% to have positive sentiment as compared to the second movie review with a 73% probability to have negative sentiment. \n\n#### Interpreting Model Decisions\nBesides prediction probabilities, we will be using the [skater framework](https://github.com/marcotcr/lime) for easy interpretation of the model decisions. First, to do this we define a helper function which takes in a document index, a corpus, its response predictions, and an explainer object and helps us with the our model interpretation analysis."},{"metadata":{"trusted":true,"_uuid":"392cc0dfd514451a772d9e304d422221d6039768"},"cell_type":"code","source":"explainer = LimeTextExplainer(class_names=classes)\ndef interpret_classification_model_prediction(doc_index, norm_corpus, corpus, prediction_labels, explainer_obj):\n    # display model prediction and actual sentiments\n    print(\"Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}\"\n      .format(index=doc_index, actual=prediction_labels[doc_index],\n              predicted=lr_pipeline.predict([norm_corpus[doc_index]])))\n    # display actual review content\n    print(\"\\nReview:\", corpus[doc_index])\n    # display prediction probabilities\n    print(\"\\nModel Prediction Probabilities:\")\n    for probs in zip(classes, lr_pipeline.predict_proba([norm_corpus[doc_index]])[0]):\n        print(probs)\n    # display model prediction interpretation\n    exp = explainer.explain_instance(norm_corpus[doc_index], \n                                     lr_pipeline.predict_proba, num_features=10, \n                                     labels=[1])\n    exp.show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f3f0211b4736b4530d8307d00edefb38f22065"},"cell_type":"markdown","source":"The preceding snippet leverages skater to explain our text classifier to analyze its decision-making process in a global perspective. This is done by learning the model around the vicinity of the data point of interest X by sampling instances around X and assigning weightages based on their proximity to X. Thus, these locally learned linear models help in explaining complex models in a more easy to interpret way with class probabilities, contribution of top features to the class probabilities that aid in the decision making process. \n[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png)](https://arxiv.org/pdf/1602.04938.pdf)"},{"metadata":{"trusted":true,"_uuid":"1e21799894ecaf47c105cbc4f893da5048bd07ce"},"cell_type":"code","source":"doc_index = 100 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b879a4d302f4b8a9996a1a2d3e2269a894a10ff8"},"cell_type":"markdown","source":"The results show us the top 10 features and we can notice that our model performs quite well in this scenario.\n\nLet's see a positive classification case:"},{"metadata":{"trusted":true,"_uuid":"c3f7827ae310c2002179ea600ce00a397d911d44"},"cell_type":"code","source":"doc_index = 2000\ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ec1eb17f86cdcb4b0893a93d520c2a58abf4fe9"},"cell_type":"markdown","source":"Based on the content, the reviewer really liked this model. In our final analysis, we will look at the model interpretation of an example where the model makes a wrong prediction."},{"metadata":{"trusted":true,"_uuid":"e7b430ec1de71869b36a400cb13616b44e2d0c8d"},"cell_type":"code","source":"doc_index = 20 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcb519212871f95cccf0bc422f9d2f3fc240e00c"},"cell_type":"markdown","source":"The results tell us that the reviewer in fact shows signs of positive sentiment in the movie review The model interpretation also correctly identifies the aspects of the review contributing to negative sentiment like. Hence, this is one of the more complex reviews which indicate both positive and negative sentiment and the final interpretation would be in the reader's hands. \n\n### Analyzing Topic Models\nThe main aim of topic models is to extract and depict key topics or concepts which are otherwise latent and not very prominent in huge corpora of text documents. \n\nFor do this we can use some topic modeling technique like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix\nfactorization. let's proceed with the second one.\n\n#### Extract features from positive and negative reviews\nThe first step in this analysis is to combine all our normalized train and test reviews and separate out these reviews into positive and negative sentiment reviews. Once we do this, we will extract features from these two datasets using the TF-IDF feature vectorizer. "},{"metadata":{"trusted":true,"_uuid":"8c21950bbb48d362219c0a3d3779405c74fd34ae"},"cell_type":"code","source":"# consolidate all normalized reviews\nnorm_reviews = norm_train_reviews+norm_test_reviews\n\n# get tf-idf features for only positive reviews\npositive_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == 'positive']\nptvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nptvf_features = ptvf.fit_transform(positive_reviews)\n\n# get tf-idf features for only negative reviews\nnegative_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == 'negative']\nntvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nntvf_features = ntvf.fit_transform(negative_reviews)\n\n# view feature set dimensions\nprint(ptvf_features.shape, ntvf_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd443ce1fd9cd5e53cc27b110766a62a12964635"},"cell_type":"markdown","source":"From the preceding output dimensions, you can see that we have filtered out a lot of the features we used previously when building our classification models by making min_df to be 0.05 and max_df to be 0.95. This is to speed up the topic modeling process and remove features that either occur too much or too rarely.\n\n#### Topic Modeling on Reviews\n\nThe NMF class from scikit-learn will help us with topic modeling. We also use pyLDAvis for building interactive visualizations of topic models. The core principle behind Non-Negative Matrix Factorization (NNMF) is to apply matrix decomposition (similar to SVD) to a non-negative feature matrix X such that the decomposition can be represented as X ≈ WH where W & H are both non-negative matrices which if multiplied should approximately re-construct the feature matrix X. A cost function like L2 norm can be used for getting this approximation. Let’s now apply NNMF to get 15 topics from our positive sentiment reviews. We will also leverage the functions to display the results by topics in a clean format."},{"metadata":{"trusted":true,"_uuid":"3f15a62989db00766bf509499f6170318d610d4f"},"cell_type":"code","source":"pyLDAvis.enable_notebook()\ntotal_topics = 10\n\n# build topic model on positive sentiment review features\npos_nmf = NMF(n_components=total_topics, random_state=101, alpha=0.1, l1_ratio=0.2)\npos_nmf.fit(ptvf_features)\n\n# extract features and component weights\npos_feature_names = ptvf.get_feature_names()\npos_weights = pos_nmf.components_\n\n# extract and display topics and their components\npos_topics = get_topics_terms_weights(pos_weights, pos_feature_names)\nprint_topics_udf(topics=pos_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d969db521d32a8b647a556b12e7952f4b7b0bb10"},"cell_type":"markdown","source":"#### Visualize topics for positive reviews\n\nYou can leverage [pyLDAvis](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf) now to visualize these topics in an interactive visualization."},{"metadata":{"trusted":true,"_uuid":"a66eb0a18a6f33769e5f98b99371c1128a029711"},"cell_type":"code","source":"pyLDAvis.sklearn.prepare(pos_nmf, ptvf_features, ptvf, R=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9160b39578bc43de4212ec48d592b379c03cb1c2"},"cell_type":"markdown","source":"From the topics and the terms, we can see terms like movie cast, actors, performance, play, characters, music, wonderful, good, and so on have contributed toward positive sentiment in various topics. This is quite interesting and gives you a good insight into the components of the reviews that contribute toward positive sentiment of the reviews. \n\nThis visualization is completely interactive if you are using the jupyter notebook and you can click on any of the bubbles representing topics in the Intertopic Distance Map on the left and see the most relevant terms in each of the topics in the right bar chart.\n\nThe plot on the left is rendered using Multi-dimensional Scaling (MDS). Similar topics should be close to one another and dissimilar topics should be far apart. The size of each topic bubble is based on the frequency of that topic and its components in the overall corpus.\n\nThe visualization on the right shows the top terms. When no topic it selected, it shows the top 15 most salient topics in the corpus. A term's saliency is defined as a measure of how frequently the term appears the corpus and its distinguishing factor when used to distinguish between topics. When some topic is selected, the chart changes to shows the top 15 most relevant terms for that topic. The relevancy metric is controlled by λ, which can be changed based on a slider on top of the bar chart.\n\n#### Display and visualize topics for negative reviews\n\nFrom the topics and the terms, we can see terms like waste, time, money, crap, plot, terrible, acting, and so on have contributed toward negative sentiment in various topics. Of course, there are high chances of overlap between topics from positive and negative sentiment reviews, but there will be distinguishable, distinct topics that further help us with interpretation and causal analysis."},{"metadata":{"trusted":true,"_uuid":"b2829cf0fc614a47044a9160ff8fef91077b686e"},"cell_type":"code","source":"# build topic model on negative sentiment review features\nneg_nmf = NMF(n_components=total_topics, random_state=101, alpha=0.1, l1_ratio=0.2)\nneg_nmf.fit(ntvf_features)      \n\n# extract features and component weights\nneg_feature_names = ntvf.get_feature_names()\nneg_weights = neg_nmf.components_\n\n# extract and display topics and their components\nneg_topics = get_topics_terms_weights(neg_weights, neg_feature_names)\nprint_topics_udf(topics=neg_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=True) \n\npyLDAvis.sklearn.prepare(neg_nmf, ntvf_features, ntvf, R=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c6d7936c502ffd9cfc08c9b595a6cb6942d5ca4"},"cell_type":"markdown","source":"## Conclusion\n\nAs you see, traditional methods has good performance. In fact, normalize a greater number of reviews has the most time consuming process.\n\nIn the next kernel, let's look at how neural networks and more advanced methods can be applied to sentiment analysis, as you will see, these models will have initial performance similar to those obtained here, but the training process becomes more intense, however allowing better results if you get more data to train them. It is strongly recommended to apply this training process to a cluster or GPUs."}],"metadata":{"anaconda-cloud":{},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"338.496px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}