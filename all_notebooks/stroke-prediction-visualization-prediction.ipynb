{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stroke Prediction Dataset\n\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n**Dataset Information:**\n\nhealthcare-data-stroke-data.csv: The csv contains data related to patients who may have heart disease and various attributes which determine that :\n\n* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n* age: age of the patient\n* hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* ever_married: \"No\" or \"Yes\"\n* work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* Residence_type: \"Rural\" or \"Urban\"\n* avg_glucose_level: average glucose level in blood\n* bmi: body mass index\n* smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\n**Objective**\n\n* Visualize the relationships between various Healthy and Unhealthy habits to Heart Strokes, and there by predict the stroke probability with best model and hypertuned parameters.\n\n**Assumptions**\n\n* Smoking can induce Stroke, is it true?\n* Heart with a Heart Disease is prone to Stroke, is it true?\n* Workload results in high blood pressure and that could lead to Stroke, is it true?\n* Males are most susceptible to strokes due to high work related stress, is it true?\n\n**Questions to be answered**\n\n* Does age has impact on strokes? and How is this parameter distributed?\n* Does body mass index and glucose levels in a person, propel a heart stroke?\n* Is there a difference in the rate of heart stroke for smokers and non smokers?\n* Does the type of job, whether stressdul or not, contribute to heart stroke?","metadata":{}},{"cell_type":"markdown","source":"**Import necessary packages**","metadata":{}},{"cell_type":"code","source":"# Data manipulation libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Avoid Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n#Common model helpers\nfrom sklearn.preprocessing import (StandardScaler,\n                                   LabelEncoder,\n                                   OneHotEncoder)\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (classification_report, accuracy_score, \n                             auc, \n                             precision_score,\n                             recall_score,\n                             f1_score, \n                             roc_auc_score,\n                             confusion_matrix)\nfrom sklearn.model_selection import (GridSearchCV,\n                                     StratifiedKFold,\n                                     cross_val_score)\n\n\n# dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom umap import UMAP\nimport pylab as pl\n\n# imbalance dataset handling\n\nfrom imblearn.datasets import make_imbalance\nfrom imblearn.under_sampling import (RandomUnderSampler, \n                                     ClusterCentroids,\n                                     TomekLinks,\n                                     NeighbourhoodCleaningRule,\n                                     EditedNearestNeighbours,\n                                     NearMiss)\n\n\nfrom imblearn.over_sampling import (SMOTE,\n                                    ADASYN)\n# model algorithams\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the csv file in variable \n\ndf = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the size of dataframe\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** There are 5110 rows and 12 columns in the dataset.","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n* id, hypertension, heart_disease and stroke are of int datatype.\n* gender, ever_married, work_type, Residence_type and smoking_status are of object datatype.\n* age, avg_glucose_level and bmi are of float datatype.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** Only bmi column has certain missing values.","metadata":{}},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\n\nmsno.matrix(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(df, sort = 'descending')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** We can thus validate our previous assumption that there are null values in bmi column. Let's try filling out those null values.","metadata":{}},{"cell_type":"code","source":"df['bmi'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handling missing values\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(4,2,figsize = (16,16))\nsns.set_style('whitegrid')\nfig.suptitle(\"Count plot for various categorical features\")\n\nsns.countplot(ax=axes[0,0],data=df,x='gender')\nsns.countplot(ax=axes[0,1],data=df,x='hypertension')\nsns.countplot(ax=axes[1,0],data=df,x='heart_disease')\nsns.countplot(ax=axes[1,1],data=df,x='ever_married')\nsns.countplot(ax=axes[2,0],data=df,x='work_type')\nsns.countplot(ax=axes[2,1],data=df,x='Residence_type')\nsns.countplot(ax=axes[3,0],data=df,x='smoking_status')\nsns.countplot(ax=axes[3,1],data=df,x='stroke')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is to look at what all unique values have . Just trying to use python\nlist_col=['smoking_status','work_type','Residence_type','gender']\n\n#What are the different types of smking status?\n#What are the different work types?\n#What are the residence types?\n#How many models we have?\n\nfor col in list_col: \n    print('{} :{} ' . format(col.upper(),df[col].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n* There are 4 different types of people on the basis of smoking category.\n* People can be categorised into 5 types on the basis of type of work.\n* There are only 2 residences - 'Urban' and 'Rural'\n* There are 3 different gender entries.","metadata":{}},{"cell_type":"code","source":"fig = px.box(data_frame = df,\n            x = \"avg_glucose_level\",\n            width = 800,\n            height = 300)\nfig.update_layout({\"template\":\"plotly_dark\"})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n* avg_glucose_level has large number of outliers present in the dataset.\n\n","metadata":{}},{"cell_type":"code","source":"## binning of numerical variables\n\ndf['bmi_cat'] = pd.cut(df['bmi'], bins = [0, 19, 25,30,10000], labels = ['Underweight', 'Ideal', 'Overweight', 'Obesity'])\ndf['age_cat'] = pd.cut(df['age'], bins = [0,13,18, 45,60,200], labels = ['Children', 'Teens', 'Adults','Mid Adults','Elderly'])\ndf['glucose_cat'] = pd.cut(df['avg_glucose_level'], bins = [0,90,160,230,500], labels = ['Low', 'Normal', 'High', 'Very High'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The above section of the code has been taken from Bhuvan Chennoju's notebook [https://www.kaggle.com/bhuvanchennoju/data-stroytelling-auc-focus-on-strokes](http://)","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='stroke', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** \n\n* The number of people have heart stroke is actually negligible as compared to the ones not having it.\n* The data is higly unbalanced.\n* So, while modeling and training data, either over sampling or under sampling has to be done to obtain best results.","metadata":{}},{"cell_type":"code","source":"!pip install pywaffle\nfrom pywaffle import Waffle\n\nstroke_gen = df[df['stroke'] == 1]['gender'].value_counts()\nhealthy_gen = df[df['stroke'] == 0]['gender'].value_counts()\n\nfemale = df['gender'].value_counts().values[0]\nmale =  df['gender'].value_counts().values[1]\n\nstroke_female = int(round (stroke_gen.values[0] / female * 100, 0))\nstroke_male = int(round( stroke_gen.values[1] / male *100, 0))\nhealthy_female = int(round(healthy_gen.values[0] / female * 100, 0))\nhealthy_male = int(round(healthy_gen.values[1] / male *100, 0))\n\n\nfemale_per = int(round(female/(female+male) * 100, 0))\nmale_per = int(round(male/(female+male)* 100, 0))\n\n\n\nfig = plt.figure(FigureClass = Waffle, \n                 constrained_layout = True,\n                 figsize = (7,7),\n                 facecolor = '#f6f5f5',dpi = 100,\n                 \n                 plots = {'121':\n                          {     \n                           'rows':7,\n                           'columns': 7,\n                           'values' : [healthy_male,stroke_male],\n                            'colors' : ['#512b58','#fe346e'],\n                              'vertical' : True,\n                              'interval_ratio_y': 0.1,\n                              'interval_ratio_x': 0.1,\n                              'icons' : 'male',\n                              'icon_legend': False,\n                               'icon_size':20,\n                              'plot_anchor':'C',\n                              'alpha':0.1\n                          },\n                          \n                          '122' :\n                          { \n                            'rows': 7,\n                            'columns':7,\n                            'values':[healthy_female,stroke_female],         \n                              'colors' : ['#512b58','#fe346e'],\n                              'vertical': True,\n                              'interval_ratio_y': 0.1,\n                              'interval_ratio_x': 0.1,\n                              'icons' : 'female',\n                              'icon_legend' :False,\n                              'icon_size':20,\n                              'plot_anchor':'C',\n                              'alpha':0.1\n                                                      \n                           }\n                         },\n                   \n)\n#fig.text ('asdfasdfasd0', {'font':'Serif', 'size':35, 'color':'black'} )\n\n\nfig.text(0., 0.8, 'Gender Risk for Stroke - effect of gender on strokes?', {'font':'Serif', 'size':20, 'color':'black', 'weight':'bold'})\nfig.text(0., 0.73, 'Risk of stroke in both male and female are same,\\nprove our initial assumption is wrong. ', {'font':'Serif', 'size':13, 'color':'black', 'weight':'normal'}, alpha = 0.7)\nfig.text(0.24, 0.22, 'ooo', {'font':'Serif', 'size':16,'weight':'bold' ,'color':'#f6f5f5'})\nfig.text(0.65, 0.22, 'ooo', {'font':'Serif', 'size':16,'weight':'bold', 'color':'#f6f5f5'})\nfig.text(0.23, 0.28, '{}%'.format(healthy_male), {'font':'Serif', 'size':20,'weight':'bold' ,'color':'#512b58'},alpha = 1,)\nfig.text(0.65, 0.28, '{}%'.format(healthy_female), {'font':'Serif', 'size':20,'weight':'bold', 'color':'#512b58'}, alpha = 1)\nfig.text(0.21, 0.67, 'Male ({}%)'.format(male_per), {'font':'Serif', 'size':14,'weight':'bold' ,'color':'black'},alpha = 0.5,)\nfig.text(0.61, 0.67, 'Female({}%)'.format(female_per), {'font':'Serif', 'size':14,'weight':'bold', 'color':'black'}, alpha = 0.5)\n#fig.text(0., 0.8, 'Assumption was proven wrong', {'font':'Serif', 'size':24, 'color':'black', 'weight':'bold'})\n\nfig.text(0.9,0.73, 'Stroke ', {'font': 'Serif','weight':'bold','Size': '16','weight':'bold','style':'normal', 'color':'#fe346e'})\nfig.text(1.02,0.73, '|', {'color':'black' , 'size':'16', 'weight': 'bold'})\nfig.text(1.035,0.73, 'No Stroke', {'font': 'Serif','weight':'bold', 'Size': '16','style':'normal', 'weight':'bold','color':'#512b58'},alpha = 1)\n\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n* It is interesting to  note that although the number of males and females are different in the dataset, but, both of them are at equal risk to heart stroke. \n* Hence proving that our assumption that males are more susciptible to stroke due to work load, as wrong.\n\n\n> Please note that the above figure has been taken from Bhuvan's notebook. [https://www.kaggle.com/bhuvanchennoju/data-stroytelling-auc-focus-on-strokes](http://)","metadata":{}},{"cell_type":"code","source":"cat_cols = [\"gender\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\",\"stroke\"]\ncont_cols = [\"age\",\"avg_glucose_level\",\"bmi\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cr = df[cont_cols].corr()\nplt.figure(figsize = (10,10))\nsns.heatmap(cr,cmap=\"viridis\", annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** We cannot see any noteworthy correlation between the given features!","metadata":{}},{"cell_type":"code","source":"bmi = list(df['bmi'].values)\nhist_data = [bmi]\ngroup_labels = [\"bmi\"]\ncolors = ['Red']\nfig = ff.create_distplot(hist_data,group_labels,show_hist = True,colors=colors)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n* There are a lot of outliers in bmi\n* The outliers make the distribution curve highly skewed towards right\n* Either the outliers can be removed or the distribution curve can be made less-skewed by mapping the values with a log but both cases will lead to loss of the number of datapoints with Stroke = 1","metadata":{}},{"cell_type":"code","source":"df['gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** Since there is only one 'Other' category in gender, we should remove it from our data.","metadata":{}},{"cell_type":"code","source":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)\ndf['gender'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The shape before removing the BMI outliers : \",df.shape)\ndf.drop(df[df['bmi'] > 47].index, inplace = True)\nprint(\"The shape after removing the BMI outliers : \",df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The outliers of BMI index have been removed. Let us plot the distribution to see if it is still skewed.","metadata":{}},{"cell_type":"code","source":"bmi = list(df['bmi'].values)\nhist_data = [bmi]\ngroup_labels = [\"bmi\"]\ncolors = ['Red']\nfig = ff.create_distplot(hist_data,group_labels,show_hist = True,colors=colors)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since **STROKE** is highly imbalanced, there are two ways to deal with it. We can either undersample the majority class or we could oversample the minority class.\n\n**We will be using oversampling technique for this project.**\n\nThe simplest approach involves duplicating examples in the minority class, although these examples donâ€™t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or **SMOTE** for short.","metadata":{}},{"cell_type":"code","source":"# Label Encoding the categorical variables\n\nfrom sklearn.preprocessing import LabelEncoder\nobject_cols = [\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"]\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_encoder.fit(df[col])\n    df[col] = label_encoder.transform(df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see that our data has been completely transformed into numerical dataset.","metadata":{}},{"cell_type":"code","source":"df.drop(['bmi_cat', 'age_cat', 'glucose_cat'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using SMOTE\nfrom imblearn.over_sampling import SMOTE\nsampler = SMOTE(random_state = 42)\nX = df.drop(['stroke'],axis=1)\ny = df[['stroke']]\nX,y= sampler.fit_resample(X,y['stroke'].values.ravel())\ny = pd.DataFrame({'stroke':y})\nsns.countplot(data = y, x = 'stroke', y= None)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** It's quite clear that the data has been completely balanced.","metadata":{}},{"cell_type":"code","source":"# Joining back dataset\ndf = pd.concat([X,y],axis = 1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(frac = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = [\"gender\",\"hypertension\",\"heart_disease\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"]\ncont_cols = [\"age\",\"avg_glucose_level\",\"bmi\"]\ny_col = [\"stroke\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cat in cat_cols:\n    df[cat] = df[cat].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stacking the categorical columns\ncats = np.stack([df[col].cat.codes.values for col in cat_cols], 1)\ncats[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the stack into tensor\ncats = torch.tensor(cats, dtype = torch.int64)\ncats[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stacking the continuous columns & converting to tensor\nconts = np.stack([df[col].values for col in cont_cols], 1)\nconts = torch.tensor(conts, dtype=torch.float)\nconts[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting target variable to tensor and flattening since CrossEntropyLoss expects a 1-d tensor\ny = torch.tensor(df[y_col].values).flatten()\ny[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cats.shape)\nprint(conts.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_szs = [len(df[col].cat.categories) for col in cat_cols]\nemb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\nemb_szs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TabularModel(nn.Module):\n\n    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist = []\n        n_emb = sum((nf for ni,nf in emb_szs))\n        n_in = n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i)) \n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in = i\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n            \n        self.layers = nn.Sequential(*layerlist)\n        \n    def forward(self, x_cat, x_cont):\n        embeddings = []\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings, 1)\n        x = self.emb_drop(x)\n        \n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x, x_cont], 1)\n        x = self.layers(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\nmodel = TabularModel(emb_szs, conts.shape[1], 2, [400,200,100], p=0.2)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 9000\ntest_size = 492\n\ncat_train = cats[:batch_size-test_size]\ncat_test = cats[batch_size-test_size:batch_size]\ncon_train = conts[:batch_size-test_size]\ncon_test = conts[batch_size-test_size:batch_size]\ny_train = y[:batch_size-test_size]\ny_test = y[batch_size-test_size:batch_size]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\nepochs = 320\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model(cat_train, con_train)\n    loss = criterion(y_pred, y_train)\n    losses.append(loss)\n    \n    if i%25 == 1:\n        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'epoch: {i:3}  loss: {loss.item():10.8f}') \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(epochs), losses)\nplt.ylabel('Cross Entropy Loss')\nplt.xlabel('epoch');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    y_val = model(cat_test, con_test)\n    loss = criterion(y_val, y_test)\nprint(f'CE Loss: {loss:.8f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 200\ncorrect = 0\ngroundTruth = []\npredictedValues = []\nprint(f'{\"MODEL OUTPUT\":26} ARGMAX  Y_TEST')\nfor i in range(rows):\n    print(f'{str(y_val[i]):26} {y_val[i].argmax():^7}{y_test[i]:^7}')\n    predictedValues.append(y_val[i].argmax().item())\n    groundTruth.append(y_test[i])\n    if y_val[i].argmax().item() == y_test[i]:\n        correct += 1\nprint(f'\\n{correct} out of {rows} = {100*correct/rows:.2f}% correct')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint(\"The F1-score is :\", f1_score(groundTruth, predictedValues))\nprint(\"\\n\")\nprint(confusion_matrix(groundTruth, predictedValues))\nprint(\"\\n\")\nprint(classification_report(groundTruth, predictedValues))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The prediction part of this notebook has been referred from Naman Manchanda's notebook [https://www.kaggle.com/namanmanchanda/stroke-eda-and-ann-prediction](http://)\n\nThank you for making such a wonderful notebook! Really helped me learn a lot!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}