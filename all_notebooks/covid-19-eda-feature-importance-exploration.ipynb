{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Which populations assessed should stay home and which should see an HCP?**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since this question only has a few submissions, I think this notebook will be able to add a new perspective. The question we are trying to answer is \"Which populations assessed should stay home and which should see an HCP?\" When viewing this research question this sparked the question which what would be a metric to measure which communities should see a HCP? I decided to use death rate to determine which populations should stay home versus see an HCP. <br/>\nDisclaimer: This analysis is only using data from the United States. However, I think some of the conclusions we can make can be generalized to other countries. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports for the notebook\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n# import plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import preprocessing\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing the correct datasets**<br/>\nThis analysis uses three main datasets:\n* The USA Facts Confirmed Cases dataset \n* The USA Facts Deaths dataset\n* The us-county-health-rankings-2020.csv dataset provided by Kaggle\n\nThese USA Facts datasets are similar to the confirmed-covid-19-cases-in-us-by-state-and-county.csv and the confirmed-covid-19-deaths-in-us-by-state-and-county.csv dataset provided by Kaggle. However, the datasets used are from the usafacts.org website. These datasets were used because they contained the most up to date information. <br/>\nURL to USAFacts website with datasets: https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"us_cases_county = pd.read_csv('/kaggle/input/usafacts-updated/covid_confirmed_usafacts_June1.csv')\nus_deaths_county = pd.read_csv('/kaggle/input/usafacts-updated/covid_deaths_usafacts_June1.csv')\n\n#Importing data on each county\nhealth_by_county = pd.read_csv('/kaggle/input/county-health-rankings/us-county-health-rankings-2020.csv')\nhealth_by_county.rename(columns={'fips': \"countyFIPS\"}, inplace=True)\nhealth_by_county.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Once we read in the data, we want to get what is the latest date. This is because the data is the cumulative number of confirmed cases or deaths per county in the United States. That way we will be able to conduct our analysis with the most up to date data. We will filter out all the data except the data most recently available death and confirm cases number along with the county and state information. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cum_cases_county = us_cases_county.filter(['countyFIPS', 'County Name', 'State', 'stateFIPS', '6/1/20'], axis=1)\ncum_deaths_county = us_deaths_county.filter(['countyFIPS', 'County Name', 'State', 'stateFIPS', '6/1/20'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will need to filter the data by an arbitrary number of deaths, since death rates can be inaccurate when there have only been only a few cases or deaths in a given county. These counties will not be able to provide useful information and can mess up any models we are using. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cum_deaths_county[cum_deaths_county['6/1/20'] >= 20].sort_values(by='6/1/20', ascending=False).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will create a new dataframe and create the new column for the death rate which to reiterate will be the metric used to determine in which populations people should visit an HCP if they have symptoms. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changed names for some columns\ncum_cases_county.rename(columns={'6/1/20': \"Confirmed Cases\"}, inplace=True)\ncum_deaths_county.rename(columns={'6/1/20': \"Deaths\"}, inplace=True)\n#Building new Dataframe \ncum_cases_deaths_county = pd.DataFrame(cum_cases_county)\ncum_cases_deaths_county['Deaths']= cum_deaths_county['Deaths']\ncum_cases_deaths_county['Death Rate'] = cum_cases_deaths_county['Deaths'] / cum_cases_deaths_county['Confirmed Cases']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can clean up the data and check to see where in the country, there are teh highest death rates. Below the cell will output the top ten highest death rates for counties with 20 or more deaths. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cum_cases_deaths_county = cum_cases_deaths_county[(cum_cases_deaths_county['countyFIPS'] != 0) \n                                                  & (cum_cases_deaths_county['Death Rate'] <= 1)]\ncum_cases_deaths_county[cum_cases_deaths_county['Deaths'] >= 20].sort_values(by='Death Rate', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the death rate by state.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"death_rate_state = cum_cases_deaths_county.groupby(['State']).agg(\n    {'Death Rate': 'mean'}).reset_index().sort_values(by='Death Rate', ascending=False)\n\ndeath_rate_state.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a map of the death rate by state.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Geographical map representation death rates\nfig = go.Figure(data=go.Choropleth(locations=death_rate_state['State'],\n                                   z=death_rate_state['Death Rate'].astype(float),\n                                  locationmode='USA-states', \n                                  colorscale='Reds',\n                                  colorbar_title='Death Rate of Covid-19'))\nfig.update_layout(title_text='Average Death Rate of Covid-19 by State', \n                  geo_scope='usa')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data from the us-county-health-rankings-2020 dataset is incredibly extensive so some of the features would be needed to be filtered out. It originally had over 500 features, and after the feature selection process there were only a little over 100 features. Below is the process used to chose the features that will be used in the model. An example of features that were removed were something like teen birth rates. Some features didn't seem to have any obvious connection to the death rate. Features such as obesity, health, and smoking were of course kept for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting filtered county data by index\nindex_lst = []\nindex_lst.extend(range(7))\nindex_lst.extend(range(23,31))\nindex_lst.extend(range(55,71))\nindex_lst.extend(range(103,112))\nindex_lst.extend(range(120,122))\nindex_lst.extend(range(134,141))\nindex_lst.extend(range(163,167)) #Income\nindex_lst.extend(range(203,215)) #Housing\nindex_lst.extend(range(326,330)) #Food\nindex_lst.extend(range(371,382))\nindex_lst.extend(range(394,397))\nindex_lst.append(412) #reduced lunch\nindex_lst.extend(range(485,507))\n\n#Total 106 features (columns)\nsimp_health_county = health_by_county.iloc[:, index_lst]\n#Outputs our newly filtered dataframe\nsimp_health_county.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the us-county-health-rankings-2020 dataset will be joined with the cumulative death rates dataframe where the county FIPS is the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now do a join on simp_health_county and death rate\ncounty_data = pd.merge(simp_health_county, cum_cases_deaths_county, on='countyFIPS')\ncounty_data.drop(['County Name', 'State'], axis=1, inplace=True)\ncounty_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter the data for more than 15 deaths (Gives us 448 samples (not much :( )\n#10 or more gives us 561 samples \nmin_deaths = 20\nfiltered_county_data = county_data[county_data['Deaths'] >= min_deaths]\n\n#Add random column (used in feature importance)\nfiltered_county_data['random'] = np.random.random(size=len(filtered_county_data))\n\n#Ouput Data\nlabels = pd.DataFrame(filtered_county_data['Death Rate'])\n\n#Get input data\nx_data = filtered_county_data.drop(['state',\n                                    'county', 'Death Rate',\n                                    'primary_care_physicians_ratio',\n                                    'other_primary_care_provider_ratio',\n                                    'Deaths','Confirmed Cases'], axis=1)\n#Dealing with the NANS\n#average of that column by state \nx_data.fillna(x_data.groupby(['stateFIPS']).transform('mean'), inplace=True)\n\n#Only effective if the state has no value (Put in average for the entire column)\nx_data.fillna(x_data.mean(), inplace=True)\n\n\n#Train Test split \nx_train, x_test, y_train, y_test = train_test_split(x_data, labels, \n                                                    test_size=0.2, random_state=101)\n\n#View our training input data\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the x training data\nscaler = preprocessing.MinMaxScaler()\nx_train_norm = scaler.fit_transform(x_train)\nx_train_norm_df = pd.DataFrame(x_train_norm, columns=x_train.columns)\n\n#From Towards Data Science Article\n#Normalizing x test data\nscaler = preprocessing.MinMaxScaler()\nx_test_norm = scaler.fit_transform(x_test)\nx_test_norm_df = pd.DataFrame(x_test_norm, columns=x_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Regression Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To model the data, the input features will be the 104 columns from us-county-health-rankings-2020 dataset and the labels will be the death rates from each county calculated from the USAFacts datasets. The regression model used is the random forest regression model from SKLearn. \n<br/>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Will use Random Forests\nrf = RandomForestRegressor(n_estimators=400, max_features='sqrt', n_jobs=1, oob_score=True,\n                           bootstrap=True, random_state=101)\nmodel = rf.fit(x_train_norm_df, y_train.values.ravel())\nprint('R^2 Training Score: {:.2f}'.format(rf.score(x_train_norm_df, y_train)))\nprint('OOB Score: {:.2f}'.format(rf.oob_score_))\nprint('Validation Score: {:.2f}'.format(rf.score(x_test_norm_df, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section of the notebook, I will use three seperate methods to determine feature importance. \n1. Impurity-based feature importance (built into SKLearn)\n2. Permutation feature importance (built into SKLearn)\n3. Linear regression coeficients for each feature and the labels\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Impurity-Based Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using SKLearn feature importance\n#Need feature names to be with feature_importances \nfeature_importances_init = rf.feature_importances_\nfeature_importances = []\nfeat_cols = []\nfor i in range(feature_importances_init.shape[0]):\n    if feature_importances_init[i] >= 0.01:\n        feature_importances.append(feature_importances_init[i])\n        feat_cols.append(x_train.columns[i])\n\n#Convert lists to numpy arrays \nfeature_importances = np.asarray(feature_importances)\nfeat_cols = np.asarray(feat_cols)\n\nnum_features = len(feature_importances)\n\n#We want to sort the importances and in order to plot them\nsorted_importances_indices = feature_importances.argsort()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the feature importances in a horizontal bar graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_figheight(15)\nax.barh(range(num_features), feature_importances[sorted_importances_indices], color='b', align='center')\nax.set_yticks(range(num_features))\nax.set_yticklabels(feat_cols)\nax.invert_yaxis()\nax.set_title(\"Covid-19 Death Rate Feature Importances\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot we can get from this graph. However, it is important to note that the impurity-based feature importance may be deceptive when there are there are large amounts of unique categorical data. The other thing to keep in mind about the horizontal bar graph above is that these are the most important features in the random forest regression model. The model right now has an R squared, validation value of 0.41 which still needs to be substantially improved before the features in the graph can be trusted more. <br/>\nOn the other hand, if we look at our most important features based on the graph, these features do not seem to far off of reality. Many of the counties with the highest death rates are from counties in more rural states such as Ohio, Louisiana, and Indiana. The next four highest rated features all have to do with the whether or not the county has proficient English speakers. This once again is not too far off of reality where immigrants often do not have the same accessability to relief options. Interestingly, it appears that the percentage of females in a county has a important role in the prediction of our model. After the English proficiency, it appears that race/ ethnicity, age distributions, insurance, and overcrowding all have an impact.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Permutation Feature Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Permutation feature importance is the decrease in the score of the model when a specific feature is shuffled randomly. If there is a change in the model, this means that the feature that was randomly shuffled had an impact on the outcome of the model. There is the n_repeats parameter in the permutation_imporance function which determines how many times each feature is randomly shuffled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r = permutation_importance(model, x_test, y_test, n_repeats=30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"permutation_df = pd.DataFrame(columns=['Feature', 'Importance Mean', 'Importance'])\n\nfor i in r.importances_mean.argsort()[::-1]:\n    #Checking if it is within two standard deviations of the mean\n    if (r.importances_mean[i] - 2 * r.importances_std[i]) > 0:\n        importance_val = str(r.importances_mean[i]) + \" +/- \" + str(r.importances_std[i])\n        permutation_df = permutation_df.append({'Feature': x_train.columns[i], 'Importance Mean': r.importances_mean[i],\n                                                'Importance': importance_val}, ignore_index=True)\n\n#Sorts the features in permutation_df from largest to smallest importance\npermutation_df.sort_values(by='Importance Mean', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This table above shows some of the similar information to the impurity-based feature importance analysis. Once again the number of non proficient English speakers is at the top. 95percent_ci_low_39 is the lower confidence interval for the number of nonproficient English speakers. This makes sense because people who are not proficient in the language are less likely to want to seek medical attention because of the language barrier. This goes along with many what has been in the news where immigrants have been hit hard by the pandemic. We also see that once again the percent of native hawaiian or other pacific islanders seems to have a large impact on the death rate. The percentage of native Americans also appears to be of some importance. Once again, this makes sense since native American reservations have been ravaged by Covid-19. However, this approach does not show the importance of a county being rural compared to the impurity-based approach. \n<br/>\nQuick note: <br/>\nThe features that appeared in this approach changed after normalizing the data with the MinMaxScaler approach. Before the normalization, one of the other important features was whether or not someone was insured. I wanted to include this because this feature made a lot of intuitive sense. These individuals who do not have health insurance may only seek the guidance of a health care professionals after their symptoms have worsened. Also, the other features pertaining to the percentage of speakers not proficient in English were also percieved to be important before I applied data normalization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_lin_correlation(x_data, y_data):\n    correlation_df = pd.DataFrame(columns=['Feature', 'Correlation'])\n    for cols in x_data.columns:\n        reg = LinearRegression()\n        #Input and output data for linear regression\n        x = x_data[cols].to_numpy()\n        y = y_data.to_numpy()\n        \n        #Reshaping data\n        x = x.reshape(-1, 1)\n        y = y.reshape(-1, 1)\n        \n        fitReg = reg.fit(x, y) \n\n        #Adds feature and its correlation value to the correlation dataframe\n        correlation_df = correlation_df.append({'Feature': str(cols), 'Correlation': float(fitReg.coef_)}, \n                                               ignore_index=True)\n    return correlation_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_df = feature_lin_correlation(x_data, labels)\n\n#Orders the correlation dataframe from highest postive correlation to highest negative correlation\ncorrelation_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This output seems to be somewhat less helpful than the previous two feature importance methods as a result of the random feature. It still has a stronger correlation than many other values, but it is formed of all values randomly generated from zero to one. This means any value with a lower absolute value of the correlation value should not be included in important features. <br/>\nThe average number of physically healthy days, 95percent_ci_high_3 (upper confidence interval for average number of physically healthy days), 95percent_ci_low_3 (lower confidence interval for the average number of physically healthy days) were all in the top five positive correlations. This shows that previous unhealthiness seems to be an important part of determining the death rate. Quartile 10 is the percent of people with access to exercise opportunities while quartile 9 is the percent of physically inactive people. The percentage of physically inactive people having a positive correlation makes sense because inactive people are more likely to have preexisting conditions.<br/>\nIn terms of the negative correlation the only feature that seems to be important is the percentage of native hawaiian or other pacific islanders. It seems that this feature has a large impact on the death rate given its large negative coefficient.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is important to note that the impurity based and permutation feature based feature importance analysis is based on the random forest regressor model. These approaches only tell us the importance of each feature in the model based off of that model. Since the random forest regressor model does not necesarrily fit the data super well, it is important to know that the features that are the most important are not necessarily the most important features in determining the death rate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the feature extraction analysis, we are able to see some features that may have a higher correlation with death rate than others. These are the features that will be most important for policy makers to keep in mind when trying to make decisions in the United States. If there are attributes of these communities that are leading to higher death rates, it is up to the others to realize that these are the populations that need to go to the hospital if they have any symptoms at all. Hopefully, by trying to focus on these features of counties in the United Sates, we will be able to help mitigate the impact on Covid-19 on communities that are yet to be ravaged by the virus. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Next Steps","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With the information we gathered from the feature importance methods, we can use other data from the us county health rankings in order to try to make more accurate models. The model that I used had 106 features. It would be interesting to try using fewer features since our data is not very large. We can also experiment with using different minimum death filters. For example in my model, I made an arbitrary decision to use the information with counties with 20 or more deaths. However, it may be that a filter of 10 will lead to more enlightening information about the impact of death rates, and thus the populations who should seek a HCP. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}