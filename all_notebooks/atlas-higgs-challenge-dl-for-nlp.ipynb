{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport numpy as np\nimport scipy\nimport matplotlib\nimport pandas\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom time import time\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import manifold, metrics, preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import cross_val_score, learning_curve, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file = '../input/higgs-boson-data/higgs_train_10k.csv'\ntest_file = '../input/higgs-boson-data/higgs_test_5k.csv'\n\nnames = [\n    'response',\n    'x1',\n    'x2',\n    'x3',\n    'x4',\n    'x5',\n    'x6',\n    'x7',\n    'x8',\n    'x9',\n    'x10',\n    'x11',\n    'x12',\n    'x13',\n    'x14',\n    'x15',\n    'x16',\n    'x17',\n    'x18',\n    'x19',\n    'x20',\n    'x21',\n    'x22',\n    'x23',\n    'x24',\n    'x25',\n    'x26',\n    'x27',\n    'x28']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = read_csv(train_file, names=names)\ntest_csv = read_csv(test_file, names=names)\nprint(train_csv.shape)\nprint(test_csv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring The Data"},{"metadata":{},"cell_type":"markdown","source":"### Data Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"types = train_csv.dtypes\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"set_option('display.width', 100)\nset_option('precision', 5)\ndescription = train_csv.describe()\nprint(description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Distrubibution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# class distribution for train and test\ntrain_data_class = train_csv.groupby('response').size()\nprint(train_data_class)\ntest_data_class = test_csv.groupby('response').size()\nprint(test_data_class)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation of Classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = train_csv.corr(method='pearson')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,29,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names, size = 25)\nax.set_yticklabels(names, size = 25)\nplt.rcParams['figure.figsize'] = (40,40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Box Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12,12)\ntrain_csv.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\nplt.show()\ntest_csv.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\nplt.rcParams['figure.figsize'] = (12,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Train & Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_arr = train_csv.values\nX_train = train_arr[:,1:28]\nY_train = train_arr[:,0]\ntest_arr = test_csv.values\nX_test = test_arr[:,1:28]\nY_test = test_arr[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manifold Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"methods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nn_neighbors = 10\nn_components = 2\ncolor=Y_train\n\nfor i, method in enumerate(methods):\n    Ytransformed = manifold.Isomap(n_neighbors, n_components).fit_transform(X_train)\n    ax = fig.add_subplot(257)\n    plt.scatter(Ytransformed[:, 0], Ytransformed[:, 1],c=color, cmap=plt.cm.Spectral)\n    plt.title(labels[i])\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n    plt.show()\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nYtransformed = mds.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(258)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nYtransformed = se.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(259)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nYtransformed = tsne.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA Feature Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npca = PCA(n_components=2)\nfit = pca.fit(X_train)\nprojected = pca.fit_transform(X_train)\n\nplt.scatter(projected[:, 0], projected[:, 1],\n               c=Y_train, edgecolor='none', alpha=0.5)\nplt.xlabel('PCA component 1')\nplt.ylabel('PCA component 2')\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.colorbar()\nplt.show()\npca = PCA(n_components=25)\nfit = pca.fit(X_train)\nplt.plot(numpy.cumsum(fit.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()\n# summarize components\nprint(\"Explained Variance: %s\" % fit.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Creation, Tuning Hyperparameters and Validation using Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using roc AUC as scoring\nscoring = 'accuracy'\n\n# Naive Bayes\nnaiveBayes = GaussianNB()\nnbscore = cross_val_score(naiveBayes, X_train, Y_train, cv=3, scoring=scoring)\nprint('Naive Bayes CV score =', np.mean(nbscore))\n\n\n# penalty\npenalties = numpy.array(['l2'])\n# C for logistic regression\nc_values = numpy.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n# max iteration\niters = numpy.array([100, 150])\nLR_param_grid = {'penalty': penalties, 'C': c_values, 'max_iter': iters}\n\n# logistic regression as algorithm\ngridLogisticRegression = LogisticRegression()\n# Using GridSearchCV on Training Data for LR\ngrid = GridSearchCV(\n    estimator=gridLogisticRegression,\n    param_grid=LR_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('LR CVScore ', grid.best_score_)\nprint('LR Penalty', grid.best_estimator_.penalty)\nprint('LR C', grid.best_estimator_.C)\nprint('LR Max Iterations', grid.best_estimator_.max_iter)\n\n\n# Perceptron\n# Using GridSearchCV on Training Data for perceptron\n# alphas\nalphas = numpy.array([0.001, 0.0001, 0.00001, 0.000001])\n# iterations\npereptorn_param_grid = {'alpha': alphas, 'max_iter': iters}\ngrid = GridSearchCV(\n    estimator=Perceptron(),\n    param_grid=pereptorn_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('Perceptron CVScore ', grid.best_score_)\nprint('Perceptron alpha', grid.best_estimator_.alpha)\nprint('Perceptron Max Iterations', grid.best_estimator_.max_iter)\n\n# LDA\ntols = numpy.array([0.001, 0.00001, 0.001])\nlda_param_grid = {'tol': tols}\ngrid = GridSearchCV(\n    estimator=LinearDiscriminantAnalysis(),\n    param_grid=lda_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('LDA CVScore ', grid.best_score_)\nprint('LDA tol', grid.best_estimator_.tol)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# gamma parameter in SVM\ngammas = numpy.array([1, 0.1, 0.01, 0.001])\n# C for logistic regression\nc_values = numpy.array([100, 1, 0.1, 0.01])\nsvm_param_grid = {'gamma': gammas, 'C': c_values}\nsvm = SVC(kernel='rbf')\nscoring = 'accuracy'\ngrid = GridSearchCV(estimator=svm, param_grid=svm_param_grid, scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.gamma)\nprint(grid.best_estimator_.C)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipeline with Feature Reduction Selection, Logistic Regression using Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler = preprocessing.MinMaxScaler()\nscaler = min_max_scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\npipe = Pipeline([\n    ('reduce_dim', PCA()),\n    ('classify', LogisticRegression())\n])\n\nN_FEATURES_OPTIONS = [10, 15, 20]\nC_OPTIONS = [0.001, 0.1, 1, 10, 100, 1000]\nmax_iter_OPTIONS = [100, 150]\nparam_grid = [\n    {\n        'reduce_dim': [PCA(iterated_power=10)],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n    {\n        'reduce_dim': [SelectKBest(chi2)],\n        'reduce_dim__k': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n]\nreducer_labels = ['PCA', 'KBest(chi2)']\n\ngrid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\ngrid.fit(X_train_scaled, Y_train)\n\nmean_scores = np.array(grid.cv_results_['mean_test_score'])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\nbar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n               (len(reducer_labels) + 1) + .5)\n\nplt.figure()\nCOLORS = ['tomato', 'darkolivegreen', 'lightsteelblue']\nfor i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n\nplt.title(\"Comparing feature reduction techniques\")\nplt.xlabel('Reduced number of features')\nplt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\nplt.ylabel('Classification accuracy')\nplt.ylim((0, 1))\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learning Curvers on Training and Validation Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, name, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title('Learning Curves for ' + name)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"No. Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\nestimator = LogisticRegression(C=0.1, penalty='l2', max_iter=100)\nplot_learning_curve(estimator, 'Tuned Logistic Regression', X_train, Y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()\nestimator = SVC(C=100, gamma=0.01, kernel='rbf')\nplot_learning_curve(estimator, 'Tuned SVM', X_train, Y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection on Whole Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_and_test(clf):\n    print('_' * 80)\n    print(\"Training: \")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, Y_train)\n    train_time = time() - t0\n    print(\"train time: %0.3fs\" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print(\"test time:  %0.3fs\" % test_time)\n\n    score = metrics.accuracy_score(Y_test, pred)\n    print(\"accuracy:   %0.3f\" % score)\n    print(\"classification report:\")\n    print(metrics.classification_report(Y_test, pred))\n    print()\n    clf_descr = str(clf).split('(')[0]\n    return clf_descr, score, train_time, test_time\n\n\nresults = []\nfor classifier, name in (\n    (LogisticRegression(\n        C=0.1, penalty='l2', max_iter=100), \"Logistic Regressin\"), (Perceptron(\n            alpha=0.001, max_iter=100), \"Perceptron\"), (LinearDiscriminantAnalysis(\n                tol=0.001), \"LDA\"), (GaussianNB(), \"Naive Bayes\"), (SVC(\n                    C=100, gamma=0.01, kernel='rbf'), \"SVM\")):\n    print('=' * 80)\n    print(name)\n    results.append(train_and_test(classifier))\n\n    \nindices = np.arange(len(results))\nresults = [[x[i] for x in results] for i in range(4)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots of ROC Curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.1, penalty='l2', max_iter=150)\nlr.fit(X_train, Y_train)\nlrpreds = lr.predict_proba(X_test)[:,1]\nlr_fpr, lr_tpr, _ = metrics.roc_curve(Y_test, lrpreds)\nplt.figure()\nlw = 2\nplt.plot(lr_fpr, lr_tpr, color='darkorange',\n         lw=lw)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.01])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.rcParams['figure.figsize'] = (5,5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other Models & Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('Bag', BaggingClassifier(DecisionTreeClassifier(),100, random_state=7)))\nmodels.append(('RF', RandomForestClassifier(100, max_features=5)))\nmodels.append(('Bo', AdaBoostClassifier(DecisionTreeClassifier(),100, random_state=7)))\n# create a voting estimation \nestimators = []\nestimators.append(('logistic',LogisticRegression()))\nestimators.append(('NB',  GaussianNB()))\nmodels.append(('ELE',VotingClassifier(estimators, voting='soft')))\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n# replace with 'roc_auc', 'neg_log_loss',.. based on the need\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure() \nfig.suptitle('Linear and Non-Linear Algorithm Comparison on Cross-Validation') \nax = fig.add_subplot(111) \nplt.boxplot(results) \nax.set_xticklabels(names) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}