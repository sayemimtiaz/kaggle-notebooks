{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"Loading data which is in csv data form."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.model_selection import train_test_split,cross_val_score\nimport seaborn as sns\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image \nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.preprocessing import StandardScaler\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"Telco_data = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see,our data set have some categorical columns and some numerical columns. So  our first step would be to check if there is some data that missing or if some incorrect data is there in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dataframe.info() function is used to get a concise summary of the dataframe. Also, info() is used to check if there is any null values in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for NAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are no NAN in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After considering min,max and unique values of data, there is some inconsistancy in the data. For example, the min value of TotalCharges is blank space. MultipleLines have 3 unique values but its min and max just show yes and No. Thus, these columns might have some ambigous values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(Telco_data[(Telco_data['TotalCharges']==' ')]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of blank spaces in TotalCharges is just 11, so deleteing those 11 rows will not impact much to data set. Hence, we will deleting those 11 rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space=Telco_data[(Telco_data['TotalCharges']!=' ')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting TotalCharges into float"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space['TotalCharges'] = pd.to_numeric(Telco_No_Space['TotalCharges'])\nTelco_No_Space.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding unique values of the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingMovies']\nfor c in check_cols:\n    print( c + '=', Telco_No_Space[c].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing values of SeniorCitizen to yes/no"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[\"SeniorCitizen\"] = Telco_No_Space[\"SeniorCitizen\"].replace(to_replace=[0, 1], value=['No', 'Yes'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Replacing ambiguous values"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space.replace(['No internet service','No phone service'],'No', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in Telco_No_Space:\n    print(i+\"=\",Telco_No_Space[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert all categorial variables to dummy variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_dummies=pd.get_dummies(Telco_No_Space)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_dummies.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Each Predictor Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.set(style=\"ticks\", color_codes=True)\n\nfig, axes = plt.subplots(nrows = 3,ncols = 5,figsize = (25,15))\nsns.countplot(x = \"gender\", data = Telco_No_Space, ax=axes[0][0])\nsns.countplot(x = \"Partner\", data = Telco_No_Space, ax=axes[0][1])\nsns.countplot(x = \"Dependents\", data = Telco_No_Space, ax=axes[0][2])\nsns.countplot(x = \"PhoneService\", data = Telco_No_Space, ax=axes[0][3])\nsns.countplot(x = \"MultipleLines\", data = Telco_No_Space, ax=axes[0][4])\nsns.countplot(x = \"InternetService\", data = Telco_No_Space, ax=axes[1][0])\nsns.countplot(x = \"OnlineSecurity\", data = Telco_No_Space, ax=axes[1][1])\nsns.countplot(x = \"OnlineBackup\", data = Telco_No_Space, ax=axes[1][2])\nsns.countplot(x = \"DeviceProtection\", data = Telco_No_Space, ax=axes[1][3])\nsns.countplot(x = \"TechSupport\", data = Telco_No_Space, ax=axes[1][4])\nsns.countplot(x = \"StreamingTV\", data = Telco_No_Space, ax=axes[2][0])\nsns.countplot(x = \"StreamingMovies\", data = Telco_No_Space, ax=axes[2][1])\nsns.countplot(x = \"Contract\", data = Telco_No_Space, ax=axes[2][2])\nsns.countplot(x = \"PaperlessBilling\", data = Telco_No_Space, ax=axes[2][3])\nax = sns.countplot(x = \"PaymentMethod\", data = Telco_No_Space, ax=axes[2][4])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\n* Gender are almost equal in dataset.\n* Almost 50% have partners.\n* Around 35% has dependents.\n* 85% of the customers have phine service.\n* Around 45% of customers have multiple lines.\n* People prefer Fiber Optics over DSL for Internet\n* Around 30% have taken online security.Majority of Customer don't have Online security or backup\n* Close 35% prefer device protection\n* Majority of Customer don't have Tech Support\n* Around 37% have registered for Streaming TV & MOvie\n* Contract - Majority of customers are subscribed for Month to Month contract (55%)\n* Majority of customers have opted Paperless billing\n* Majority of customers pay eletronic check."},{"metadata":{},"cell_type":"markdown","source":"# Target Variable Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space['Churn'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that there is slight imbalance in the dataset but it is not very big. So moving forward with this. lets plot the churn "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\", color_codes=True)\n\nsns.countplot(x = \"Churn\", data = Telco_No_Space)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have imblanced traget variable i.e. the \"Churn\" have imblance distribution. The positive Churn is appoximately 25%."},{"metadata":{},"cell_type":"markdown","source":"## Vizualizing the Numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vizualizing the Numeric variables')\nfig, axes = plt.subplots(1, 3, figsize=(20,5))\n#Ploting the histogram\nTelco_No_Space[\"tenure\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='Tenure',ax=axes[0])\nTelco_No_Space[\"MonthlyCharges\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='MonthlyCharges',ax=axes[1])\nTelco_No_Space[\"TotalCharges\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='TotalCharges',ax=axes[2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency Distribution of Numeric Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Frequency Distribution of Numeric Variable')\nfig, axes = plt.subplots(1, 3, figsize=(20,5))\nsns.distplot( Telco_No_Space[\"tenure\"] , kde=True, rug=False, color=\"skyblue\", ax=axes[0])\nsns.distplot( Telco_No_Space[\"MonthlyCharges\"] , kde=True, rug=False, color=\"olive\", ax=axes[1])\nsns.distplot( Telco_No_Space[\"TotalCharges\"] , kde=True, rug=False, color=\"gold\", ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tenure:\n\n1. There is no normal distribution.\n2. It seems like most of the  customers (0 to 1 month) joined the network for one month and then left.\n3. Between 10 months to 65 months, we can see flat distribution of data.\n4. There are lot customers in 69-72 months range. They are the loyal customers.\n\nMonthly Charges:\n\n1. There is no normal distribution.\n2. Most of the customers are paying around 20$. We can say those customers are using basic packages.\n3. There are customers who are paying around 70 to 100 dollars, we can consider those customers are using multiple lines.\n\nTotal Charges:\n\n1. We can see data is skewed to one side.\n2. Majority of the population have spent close to $1,100 dollars\n"},{"metadata":{},"cell_type":"markdown","source":"# Checking Outliers using boxplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(20,5))\nsns.boxplot(x=Telco_No_Space[\"tenure\"], orient=\"v\", color=\"salmon\",ax=axes[0])\nsns.boxplot(x=Telco_No_Space[\"MonthlyCharges\"], orient=\"v\", color=\"skyblue\",ax=axes[1])\nsns.boxplot(x=Telco_No_Space[\"TotalCharges\"] , orient=\"v\", color=\"green\",ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are no outliers in the data."},{"metadata":{},"cell_type":"markdown","source":"# Corelation "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(10,10))\nCorrelation=Telco_No_Space.select_dtypes(include=[np.number]).corr()\nsns.heatmap(Correlation,annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seperating numeric and categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_data =Telco_No_Space.select_dtypes(include=[np.number])\nnumeric_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_data=Telco_No_Space.select_dtypes(exclude=[np.number])\ncategorical_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relation of Predictors with Target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','gender','customerID']].groupby(['Churn','gender']).agg('count')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen, count of gender is almost same for customer who churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','SeniorCitizen','customerID']].groupby(['Churn','SeniorCitizen']).agg('count')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can say that, majority of customers who churn are not senior citizens."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','Partner','customerID']].groupby(['Churn','Partner']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of customers who are single, churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','Dependents','customerID']].groupby(['Churn','Dependents']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customer with dependants have lower chance of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','PhoneService','customerID']].groupby(['Churn','PhoneService']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers with phone service, have higher chance of leaving."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','MultipleLines','customerID']].groupby(['Churn','MultipleLines']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','InternetService','customerID']].groupby(['Churn','InternetService']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customer with Fiber Optic Interner Service have higher chances of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','OnlineSecurity','customerID']].groupby(['Churn','OnlineSecurity']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers with online security, are less likely to leave."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','OnlineBackup','customerID']].groupby(['Churn','OnlineBackup']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers with online backup, are less likely to leave."},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','DeviceProtection','customerID']].groupby(['Churn','DeviceProtection']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Customers without device protection have likely higher chances of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','TechSupport','customerID']].groupby(['Churn','TechSupport']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customer not opting for TechSupport have higher chances of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','StreamingTV','customerID']].groupby(['Churn','StreamingTV']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','StreamingMovies','customerID']].groupby(['Churn','StreamingMovies']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','Contract','customerID']].groupby(['Churn','Contract']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Month to Month customers have likely higher chances to leave"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','PaperlessBilling','customerID']].groupby(['Churn','PaperlessBilling']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers with paperless billing have higher chances of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','PaymentMethod','customerID']].groupby(['Churn','PaymentMethod']).agg('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People paying with electronic check have higher chances of leaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','MonthlyCharges']].groupby(['Churn']).agg('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','MonthlyCharges']].groupby(['Churn']).agg('mean').unstack(1).plot(kind='bar', subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(Telco_No_Space, col=\"Churn\", col_order=[\"Yes\", \"No\"])\ng = g.map(plt.hist, 'MonthlyCharges', color=\"m\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation :\n\n* Majority of customers who leave, pays monthly charges around 75-120$.\n\nDecisions :\n\n* We will use 'MonthlyCharges' column in our model training.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space[['Churn','tenure']].groupby(['Churn']).agg('mean').unstack(1).plot(kind='bar', subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(Telco_No_Space, col=\"Churn\", col_order=[\"Yes\", \"No\"])\ng = g.map(plt.hist, 'tenure', color=\"m\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n* Customers mostly left Telco within 1st month.\n* Churn declines with time steadly.\n* If customer can be retained between 10-20 months, there are high chances, customer will stay very long.\n\nDecisions:\n\n* We should definitely use 'Tenure' column in our model training.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(Telco_No_Space, col=\"Churn\", col_order=[\"Yes\", \"No\"])\ng = g.map(plt.hist, 'TotalCharges', color=\"m\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\n* Data is positively skewed.\n* Total charges are Tenure * MonthlyCharges . \n* Tenur might me high and Monthly charges may be low and vice-versa. \n\nDecision:\n\n* We will use this column."},{"metadata":{},"cell_type":"markdown","source":"# Variable Improtance"},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space_1 = Telco_No_Space.drop(['customerID'], axis = 1)\nTelco_No_Space_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space_1.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data=Telco_No_Space_1.select_dtypes(exclude=[np.number]).astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space_1[cat_data.columns]=cat_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_No_Space_1.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1=Telco_No_Space_1.drop([\"Churn\"],axis=1)\ny_1=Telco_No_Space_1[[\"Churn\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Telco_dummies=pd.get_dummies(Telco_No_Space_1)\n#cols=['tenure','MonthlyCharges','TotalCharges']\n#num_data = Telco_dummies.loc[:,cols]\n#norm_data = (num_data-num_data.mean())/num_data.std()\n#Telco_dummies.drop(cols,axis=1, inplace=True)\n#Telco_dummies=pd.concat([Telco_dummies,norm_data],axis=1)\n#Telco_dummies.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Telco_dummies=pd.get_dummies(Telco_No_Space_1)\nX=Telco_dummies.drop([\"Churn_Yes\",\"Churn_No\"],axis=1)\ny=Telco_dummies[[\"Churn_Yes\"]]\n\n# Create decision tree classifer object\nclf = RandomForestClassifier(random_state=0, n_jobs=-1)\n\n# Train model\nmodel = clf.fit(X, y)\n\n# Calculate feature importances\nimportances = model.feature_importances_\n\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [X.columns[i] for i in indices]\n\n# Create plot\nplt.figure(figsize=(20,10))\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(X.shape[1]),names, rotation=90)\n\n# Show plot\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n* As we can see above, all the numeric variables are very important to consider for our model training.\n* Phone Service is the least important variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Telco_dummies=pd.get_dummies(Telco_No_Space_1)\nX_1=Telco_dummies.drop([\"Churn_Yes\",\"Churn_No\",\"TotalCharges\",\"MonthlyCharges\",\"tenure\"],axis=1)\ny=Telco_dummies[[\"Churn_Yes\"]]\n\n# Create decision tree classifer object\nclf = RandomForestClassifier(random_state=0, n_jobs=-1)\n\n# Train model\nmodel = clf.fit(X_1, y)\n\n# Calculate feature importances\nimportances = model.feature_importances_\n\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_1.columns[i] for i in indices]\n\n# Create plot\nplt.figure(figsize=(20,10))\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(X_1.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(X_1.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n* Here, variable importance is plotted without numeric variable, to see the impact of other variables with Churn.\n* We can say that, numeric variable are most important.\n* Excluding numeric variables doesn't change the impact of other variables over Churn."},{"metadata":{},"cell_type":"markdown","source":"# Model Building\n\nWe are going to choose different machine algorithms to train our model using all features, then select the one that perform best in order to have better accuracy.\n\nSelecting machine learning algorithms:\n\nThis is a classification problem, we want to predict whether or not a customer will churn. Here are the classifications that we will explore:\n\n1. Regularized Logistic Regression (using scaled data)\n2. KNN Model with Unscaled Data\n3. KNN (using scaled data)\n4. Decision Tree (using unscaled data)\n5. Random Forest (using unscaled data)"},{"metadata":{},"cell_type":"markdown","source":"# Approach 1"},{"metadata":{},"cell_type":"markdown","source":"# Spliting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\n#print length of X_train, X_test, y_train, y_test\nprint (\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regularized Logistic Regression (using scaled data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_logistic_pipe = Pipeline(steps = [('sc', StandardScaler()),('classifier', LogisticRegression())])\n\n#scaled_logistic_param_grid = { \"classifier__penalty\": ['l2','l1'], \"classifier__C\": np.logspace(0, 4, 10)}\n\nC = np.logspace(-4, 4, 50)\n# Create a list of options for the regularization penalty\npenalty = ['l2']\n# Create a dictionary of all the parameter options \n# Note has you can access the parameters of steps of a pipeline by using '__â€™\nscaled_logistic_param_grid = dict(classifier__C=C,\n                  classifier__penalty=penalty)\nnp.random.seed(1)\n\nscaled_logistic_grid_search = GridSearchCV(scaled_logistic_pipe, scaled_logistic_param_grid, cv=10)\n\nscaled_logistic_grid_search.fit(X_train, y_train.values.ravel())\n\nscaled_logistic_model = scaled_logistic_grid_search.best_estimator_\n\n \n\nprint('Cross Validation Score:', scaled_logistic_grid_search.best_score_)\n\nprint('Best Hyperparameters:  ', scaled_logistic_grid_search.best_params_)\n\nprint('Training Accuracy:     ', scaled_logistic_model.score(X_train, y_train))\n\ny_pred_logistic=scaled_logistic_model.predict(X_test)\n\nprint(\"Logistic Accuracy:\",metrics.accuracy_score(y_test, y_pred_logistic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_logistic))\npreds_logistic = scaled_logistic_model.predict_proba(X_test)\npreds_logistic = preds_logistic[:,1]\nfpr_logistic, tpr_logistic, _ = metrics.roc_curve(y_test, preds_logistic)\nauc_score_logistic = metrics.auc(fpr_logistic, tpr_logistic)\nplt.title('ROC Curve')\nplt.plot(fpr_logistic, tpr_logistic, label='AUC = {:.2f}'.format(auc_score_logistic))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Model with Unscaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_knn_pipe = Pipeline(steps = [('classifier', KNeighborsClassifier())])\n\nunscaled_knn_param_grid = {'classifier__n_neighbors': range(1,10),'classifier__p': [1,2,3]}\n\nnp.random.seed(1)\n\nunscaled_knn_grid_search = GridSearchCV(unscaled_knn_pipe, unscaled_knn_param_grid, cv=10, refit='True')\n\nunscaled_knn_grid_search.fit(X_train, y_train.values.ravel())\n\nunscaled_knn_model = unscaled_knn_grid_search.best_estimator_\n\n \n\nprint('Cross Validation Score:', unscaled_knn_grid_search.best_score_)\n\nprint('Best Hyperparameters:  ', unscaled_knn_grid_search.best_params_)\n\nprint('Training Accuracy:     ', unscaled_knn_model.score(X_train, y_train))\n\ny_pred_knn=unscaled_knn_model.predict(X_test)\n\nprint(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_knn))\npreds_knn = unscaled_knn_model.predict_proba(X_test)\npreds_knn = preds_knn[:,1]\nfpr_knn, tpr_knn, _ = metrics.roc_curve(y_test, preds_knn)\nauc_score_knn = metrics.auc(fpr_knn, tpr_knn)\nplt.title('ROC Curve')\nplt.plot(fpr_knn, tpr_knn, label='AUC = {:.2f}'.format(auc_score_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN (using scaled data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_knn_pipe = Pipeline(steps = [('sc', StandardScaler()),('classifier', KNeighborsClassifier())])\n\nscaled_knn_param_grid = {'classifier__n_neighbors': range(1,10),'classifier__p': [1,2,3]}\n\nnp.random.seed(1)\n\nscaled_knn_grid_search = GridSearchCV(scaled_knn_pipe, scaled_knn_param_grid, cv=10, refit='True')\n\nscaled_knn_grid_search.fit(X_train, y_train.values.ravel())\n\nscaled_knn_model = scaled_knn_grid_search.best_estimator_\n\n \n\nprint('Cross Validation Score:', scaled_knn_grid_search.best_score_)\n\nprint('Best Hyperparameters:  ', scaled_knn_grid_search.best_params_)\n\nprint('Training Accuracy:     ', scaled_knn_model.score(X_train, y_train))\n\ny_pred_knn=scaled_knn_model.predict(X_test)\n\nprint(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_knn))\npreds_knn = scaled_knn_model.predict_proba(X_test)\npreds_knn = preds_knn[:,1]\nfpr_knn, tpr_knn, _ = metrics.roc_curve(y_test, preds_knn)\nauc_score_knn = metrics.auc(fpr_knn, tpr_knn)\nplt.title('ROC Curve')\nplt.plot(fpr_knn, tpr_knn, label='AUC = {:.2f}'.format(auc_score_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree (using unscaled data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_tree_pipe = Pipeline(steps = [('decisiontree', DecisionTreeClassifier())])\n\n#Create lists of parameter for Decision Tree Classifier\ncriterion = ['gini', 'entropy']\nmax_depth = [1,2,3,4,5,6,7,8,9,10,11,12]\n    \nunscaled_tree_param_grid = dict(decisiontree__criterion=criterion,decisiontree__max_depth=max_depth)\n\nnp.random.seed(1)\n\nunscaled_tree_grid_search = GridSearchCV(unscaled_tree_pipe, unscaled_tree_param_grid, cv=10)\n\nunscaled_tree_grid_search.fit(X_train, y_train)\n\nunscaled_tree_model = unscaled_tree_grid_search.best_estimator_\n\n \n\nprint('Cross Validation Score:', unscaled_tree_grid_search.best_score_)\n\nprint('Best Hyperparameters:  ', unscaled_tree_grid_search.best_params_)\n\nprint('Training Accuracy:     ', unscaled_tree_model.score(X_train, y_train))\n\ny_pred_tree=unscaled_tree_model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_tree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_tree))\npreds_tree = unscaled_tree_model.predict_proba(X_test)\npreds_tree = preds_tree[:,1]\nfpr_tree, tpr_tree, _ = metrics.roc_curve(y_test, preds_tree)\nauc_score_tree = metrics.auc(fpr_tree, tpr_tree)\nplt.title('ROC Curve')\nplt.plot(fpr_tree, tpr_tree, label='AUC = {:.2f}'.format(auc_score_tree))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest (using unscaled data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_rf_pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\nunscaled_rf_param_grid = {\n                 \"classifier__n_estimators\": [10, 100, 1000],\n                 \"classifier__max_depth\":[5,8,15,25,30,None],\n                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"classifier__max_leaf_nodes\": [2, 5,10]}\nunscaled_rf_grid_search = GridSearchCV(unscaled_rf_pipe, unscaled_rf_param_grid, cv=5, verbose=0,n_jobs=-1)\n\nunscaled_rf_grid_search.fit(X_train, y_train.values.ravel())\n\nunscaled_rf_model = unscaled_tree_grid_search.best_estimator_\n\n \n\nprint('Cross Validation Score:', unscaled_rf_grid_search.best_score_)\n\nprint('Best Hyperparameters:  ', unscaled_rf_grid_search.best_params_)\n\nprint('Training Accuracy:     ', unscaled_rf_model.score(X_train, y_train))\n\ny_pred_rf=unscaled_rf_model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_rf))\npreds_rf = unscaled_rf_model.predict_proba(X_test)\npreds_rf = preds_rf[:,1]\nfpr_rf, tpr_rf, _ = metrics.roc_curve(y_test, preds_rf)\nauc_score_rf = metrics.auc(fpr_rf, tpr_rf)\nplt.title('ROC Curve')\nplt.plot(fpr_rf, tpr_rf, label='AUC = {:.2f}'.format(auc_score_rf))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = categorical_data.columns[1:len(categorical_data.columns)-1].tolist()\nnumerical_features = numeric_data.columns.tolist()\ntarget = \"Churn\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=Telco_No_Space\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    \"\"\"For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {'a': [1, 5, 2, 5, 2, 8],\n               'b': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key='a')\n    >> data['a'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\npipeline_tree = Pipeline([\n   \n\n    # Use FeatureUnion to combine the features from subject and body\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from the post's subject line\n            (\"categorical_features\", Pipeline([\n                ('selector', ItemSelector(key=categorical_features)),\n                                (\"onehot\",OneHotEncoder()),\n                \n            ])),\n\n            # Pipeline for standard bag-of-words model for body\n            (\"numerical_features\", Pipeline([\n                ('selector', ItemSelector(key=numerical_features)),\n                                (\"scaler\",StandardScaler()),\n             \n            ])),    \n\n        ], \n    )),\n\n    # Use a SVC classifier on the combined features\n     (\"classifier\",tree.DecisionTreeClassifier(max_depth = 5,random_state=42)),\n])\n\npipeline_tree.fit(df_train, df_train[target])\npred_tree = pipeline_tree.predict(df_test)\n\n\nprint(classification_report(df_test[target], pred_tree))\n\npreds_tree = pipeline.predict_proba(df_test)\npreds_tree = preds_tree[:,1]\nfpr_tree, tpr_tree, _ = metrics.roc_curve(df_test[target].replace(to_replace=['No', 'Yes'], value=[0, 1]), preds_tree)\nauc_score_tree = metrics.auc(fpr_tree, tpr_tree)\nplt.title('ROC Curve')\nplt.plot(fpr_tree, tpr_tree, label='AUC = {:.2f}'.format(auc_score_tree))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_rf = Pipeline([\n   \n\n    # Use FeatureUnion to combine the features from subject and body\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from the post's subject line\n            (\"categorical_features\", Pipeline([\n                ('selector', ItemSelector(key=categorical_features)),\n                                (\"onehot\",OneHotEncoder()),\n                \n            ])),\n\n            # Pipeline for standard bag-of-words model for body\n            (\"numerical_features\", Pipeline([\n                ('selector', ItemSelector(key=numerical_features)),\n                                (\"scaler\",StandardScaler()),\n             \n            ])),    \n\n        ], \n    )),\n\n    # Use a SVC classifier on the combined features\n     (\"classifier\",RandomForestClassifier(max_depth = 8, max_leaf_nodes = 10, min_samples_leaf = 10, n_estimators=10)),\n])\n\npipeline_rf.fit(df_train, df_train[target])\npred_rf = pipeline_rf.predict(df_test)\n\n\nprint(classification_report(df_test[target], pred_rf))\n\n\n\npreds_rf = pipeline_rf.predict_proba(df_test)\npreds_rf = preds_rf[:,1]\nfpr_rf, tpr_rf, _ = metrics.roc_curve(df_test[target].replace(to_replace=['No', 'Yes'], value=[0, 1]), preds_rf)\nauc_score_rf = metrics.auc(fpr_rf, tpr_rf)\nplt.title('ROC Curve')\nplt.plot(fpr_rf, tpr_rf, label='AUC = {:.2f}'.format(auc_score_rf))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\npipeline_svm = Pipeline([\n   \n\n    # Use FeatureUnion to combine the features from subject and body\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from the post's subject line\n            (\"categorical_features\", Pipeline([\n                ('selector', ItemSelector(key=categorical_features)),\n                                (\"onehot\",OneHotEncoder()),\n                \n            ])),\n\n            # Pipeline for standard bag-of-words model for body\n            (\"numerical_features\", Pipeline([\n                ('selector', ItemSelector(key=numerical_features)),\n                                (\"scaler\",StandardScaler()),\n             \n            ])),    \n\n        ], \n    )),\n\n    # Use a SVC classifier on the combined features\n     (\"classifier\",svm.SVC(probability=True)),\n])\n\n\nparam_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ngrid_params_svm = [{'classifier__kernel': ['linear', 'rbf'], 'classifier__C': param_range}]\njobs = -1\n\ngs_svm = GridSearchCV(estimator=pipeline_svm,\n                      param_grid=grid_params_svm,\n                      scoring='accuracy',\n                      cv=10,\n                      n_jobs=jobs)\n\n\n\n#gs_svm.fit(X_train, y_train)\n\ngs_svm.fit(df_train, df_train[target])\ngs_svm_model = gs_svm.best_estimator_\npred_svm = gs_svm_model.predict(df_test)\n\nprint(classification_report(df_test[target], pred_svm))\n\n\n\npreds_svm = gs_svm_model.predict_proba(df_test)\npreds_svm = preds_svm[:,1]\nfpr_svm, tpr_svm, _ = metrics.roc_curve(df_test[target].replace(to_replace=['No', 'Yes'], value=[0, 1]), preds_svm)\nauc_score_svm = metrics.auc(fpr_svm, tpr_svm)\nplt.title('ROC Curve')\nplt.plot(fpr_svm, tpr_svm, label='AUC = {:.2f}'.format(auc_score_svm))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}