{"cells":[{"metadata":{"papermill":{"duration":0.004766,"end_time":"2020-12-21T22:41:22.523203","exception":false,"start_time":"2020-12-21T22:41:22.518437","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n<h1><center><strong>Ensemble Learning in a quick Time üëç</strong></center></h1>\n    <p>\nIn machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance \n        than could be obtained from any of the constituent learning algorithms alone.\n\n</p>\n</div>"},{"metadata":{"papermill":{"duration":0.003264,"end_time":"2020-12-21T22:41:22.53041","exception":false,"start_time":"2020-12-21T22:41:22.527146","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<div>\n<img src=\"https://d2ueix13hy5h3i.cloudfront.net/wp-content/uploads/2019/02/ensemble-learning.png\">\n</div>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-12-21T22:41:22.555996Z","iopub.status.busy":"2020-12-21T22:41:22.545517Z","iopub.status.idle":"2020-12-21T22:41:24.340596Z","shell.execute_reply":"2020-12-21T22:41:24.339871Z"},"papermill":{"duration":1.807066,"end_time":"2020-12-21T22:41:24.340727","exception":false,"start_time":"2020-12-21T22:41:22.533661","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Calling dataset\nimport pandas as pd\ndata=pd.read_csv('../input/seed-from-uci/Seed_Data.csv')\nX=data.drop('target',axis=1)\ny=data['target']\n\n#dataset splitting into training and testing\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3,random_state=3)\n\n## We call the algorithms like this way\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nknn=KNeighborsClassifier()\nnab=GaussianNB()\nsvc=LinearSVC()\ndt=DecisionTreeClassifier()\n\n## Then we use Voting Classifier for ensemble learning:\nfrom sklearn.ensemble import VotingClassifier\nEns = VotingClassifier( estimators= [('KNN',knn),('NB',nab),('SVM',svc),('DT',dt)], voting = 'hard')\n\n## Training the Ensemble learning:\nEns= Ens.fit(X_train , y_train)\n\n#Accuracy of Ensemble learning:\nprint('Accuracy score of Ensemble Learning is = {:.2f}'.format(Ens.score(X_test, y_test)),'%')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.003494,"end_time":"2020-12-21T22:41:24.348516","exception":false,"start_time":"2020-12-21T22:41:24.345022","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<div class=\"alert alert-info\">  \n<h3><strong>Conclusion</strong></h3>\n    <p>\n       The goal of any machine learning problem is to find a single model that will best predict our wanted outcome.   \n        Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble \n        methods take a myriad of models into account, and average those models to produce one final model. \n        \n</p>\n</div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}