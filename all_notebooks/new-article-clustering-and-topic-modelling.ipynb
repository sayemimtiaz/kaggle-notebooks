{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sklearn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.neighbors import KDTree\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport gensim\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('punkt')\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnp.random.seed(2018)\nnltk.download('wordnet')\nimport matplotlib.pyplot as plt\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/india-headlines-news-dataset/india-news-headlines.csv')\ndata['cat']=data['headline_category'].str.split('.').map(lambda x : x[0])\ndata=data.loc[(data['cat']=='entertainment') | (data['cat']=='business') | (data['cat']=='sports') | (data['cat']=='india') ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=data.loc[data['cat']=='sports'].head(15000)\ndata2=data.loc[data['cat']=='business'].head(15000)\ndata3=data.loc[data['cat']=='entertainment'].head(15000)\ndata4=data.loc[data['cat']=='india'].head(15000)\ndata=data1.append(data2).append(data3).append(data4)\ndata=data.sample(frac=1)\ndata=data.set_index(pd.Series([i for i in range(0,len(data))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data=pd.DataFrame({\"headline_text\":np.array(data['headline_text'].sample(frac=0.1))})\n#data['index']=data.index\n#documents=data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def lemmatize_stemming(text):\n#    stemmer = PorterStemmer()\n#    return WordNetLemmatizer().lemmatize(text, pos='v')\n#def preprocess(text):\n#    result = []\n#    for token in gensim.utils.simple_preprocess(text):\n#        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n#            result.append(lemmatize_stemming(token))\n#    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doc_sample = documents[documents['index'] == 4310].values[0][0]\n#print('original document: ')\n#corp=[]\n#for u in range(0,len(documents)):\n#    words = []\n#    for word in documents['headline_text'][u].split(' '):\n#        words.append(word)\n#    corp.append(words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#processed_docs = documents['headline_text'].map(preprocess)\n#processed_docs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_func(sen):\n    filter_list=['CC','RB','IN','TO','WRB','JJ','PRP','DT',':',')','(','POS','CD','.','MD']\n    text=nltk.word_tokenize(sen)\n    tagged=nltk.pos_tag(text)\n    for g in tagged:\n        if(g[1] in filter_list):\n            text.remove(g[0])\n    text1=[word.lower() for word in text]\n    return(text1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags=[]\nfor i in range(0,len(data)):\n    tags.append(filter_func(data['headline_text'][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#corp=[]\n#for i in list(data['headline_text']): \n#    corp.append(nltk.word_tokenize(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import nltk\n#nltk.download('stopwords')\n#from nltk.tokenize import word_tokenize \n#from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stop_words = set(stopwords.words('english')) \n#filtered=[]\n#for i in range(0,len(corp)):\n#    filtered.append([w for w in corp[i] if not w in stop_words]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic exploration using Word embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(tags, min_count=1,size=1000)\nmodel.wv.vectors.shape\nwordvecs=model.wv.vectors   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(wordvecs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clustering_on_wordvecs(word_vectors, num_clusters):\n    # Initalize a k-means object and use it to extract centroids\n    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n    idx = kmeans_clustering.fit_predict(word_vectors);\n    \n    return kmeans_clustering.cluster_centers_, idx;\ncenters, clusters = clustering_on_wordvecs(wordvecs, 7);\ncentroid_map = dict(zip(model.wv.index2word, clusters));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_words(index2word, k, centers, wordvecs):\n    tree = KDTree(wordvecs);\n#Closest points for each Cluster center is used to query the closest 20 points to it.\n    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n    closest_words_idxs = [x[1] for x in closest_points];\n#Word Index is queried for each position in the above array, and added to a Dictionary.\n    closest_words = {};\n    for i in range(0, len(closest_words_idxs)):\n        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n#A DataFrame is generated from the dictionary.\n    df = pd.DataFrame(closest_words);\n    df.index = df.index+1\n    return(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words = get_top_words(model.wv.index2word, 5000, centers, wordvecs);\ntop_words.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import TfidfVectorizer\n\n#sns.set_style('whitegrid')\n#%matplotlib inline\n## Helper function\n#def plot_20_most_common_words(count_data, count_vectorizer):\n#    import matplotlib.pyplot as plt\n#    words = count_vectorizer.get_feature_names()\n#    total_counts = np.zeros(len(words))\n#    for t in count_data:\n#        total_counts+=t.toarray()[0]\n#    \n#    count_dict = (zip(words, total_counts))\n#    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n#    words = [w[0] for w in count_dict]\n#    counts = [w[1] for w in count_dict]\n#    x_pos = np.arange(len(words)) \n#    \n#    plt.figure(2, figsize=(12, 12/1.6180))\n#    plt.subplot(title='30 most common words')\n#    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n#    sns.barplot(x_pos, counts, palette='husl')\n#    plt.xticks(x_pos, words, rotation=90) \n#    plt.xlabel('words')\n#    plt.ylabel('counts')\n#    plt.show()\n## Initialise the count vectorizer with the English stop words\n#count_vectorizer = TfidfVectorizer(stop_words='english')\n## Fit and transform the processed titles\n#count_data = count_vectorizer.fit_transform(data['headline_text'])\n## Visualise the 20 most common words\n#plot_20_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import warnings\n#warnings.simplefilter(\"ignore\", DeprecationWarning)\n## Load the LDA model from sk-learn\n#from sklearn.decomposition import LatentDirichletAllocation as LDA\n# \n## Helper function\n#def print_topics(model, count_vectorizer, n_top_words):\n#    words = count_vectorizer.get_feature_names()\n#    for topic_idx, topic in enumerate(model.components_):\n#        print(\"\\nTopic #%d:\" % topic_idx)\n#        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n#        \n## Tweak the two parameters below\n#number_topics = 6\n#number_words = 20\n## Create and fit the LDA model\n#lda = LDA(n_components=number_topics, n_jobs=-1)\n#lda.fit(count_data)\n## Print the topics found by the LDA model\n#print(\"Topics found via LDA:\")\n#print_topics(lda, count_vectorizer, number_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Latent dirichlet alloacation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora, models\ndef bow(tags):\n    dictionary = gensim.corpora.Dictionary(tags)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in tags]\n    return(bow_corpus)\n\ndef bow_tfidf(tags):\n    dictionary = gensim.corpora.Dictionary(tags)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in tags]\n    tfidf = models.TfidfModel(bow_corpus)\n    corpus_tfidf = tfidf[bow_corpus]\n    return(corpus_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_corpus =bow(tags)\ncorpus_tfidf=bow_tfidf(tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bow_doc_4310 = bow_corpus[4310]\n#for i in range(len(bow_doc_4310)):\n#    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],dictionary[bow_doc_4310[i][0]],bow_doc_4310[i][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=gensim.corpora.Dictionary(tags), passes=2, workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=6, id2word=gensim.corpora.Dictionary(tags), passes=2, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for idx, topic in lda_model.print_topics(-1):\n#    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for idx, topic in lda_model_tfidf.print_topics(-1):\n#    print('Topic: {} Word: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#unseen_document = 'Two suspects killed by police'\n#unseen2='One more lok sabha constituency added'\n#unseen3='Modi ji doubts over ballot papers'\n#bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n#bow_vector3=dictionary.doc2bow(preprocess(unseen2))\n#bow_vector4=dictionary.doc2bow(preprocess(unseen3))\n\n#for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n#    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flag=[]\nfor i in range(0,len(bow_corpus)):\n    vec=[0]*6\n    for j in lda_model_tfidf[corpus_tfidf[i]]:\n        vec[j[0]]=j[1]\n    vec=np.array(vec)\n    flag.append(vec)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flag2=[]\nfor i in range(0,len(bow_corpus)):\n    vec=[0]*6\n    for j in lda_model[bow_corpus[i]]:\n        vec[j[0]]=j[1]\n    vec=np.array(vec)\n    flag2.append(vec)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_topic(c):\n    return(sorted(lda_model_tfidf[bow_corpus[c]], key=lambda tup: -1*tup[1])[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sents(num,a):\n    sents=[]\n    for i in range(0,num):\n        p=get_topic(i)\n        if(p==a):\n            sents.append(data['cat'][i])\n    return(sents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_cat(f):\n    cats=['business','sports','entertainment','india']\n    cout=[]\n    for i in range(0,4):\n        cout.append(get_sents(len(bow_corpus),f).count(cats[i]))\n    plt.bar(cats,cout)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_cat(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering the vectors generated by LDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import mixture\nfrom sklearn.cluster import AffinityPropagation\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\n#g = mixture.GaussianMixture(n_components=6)\ng=KMeans(n_clusters=6)\n#g=AffinityPropagation()\ng.fit(flag)\nlabels=g.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_label(a):\n    cats=['business','sports','entertainment','india']\n    lb=[]\n    cout=[]\n    for i in range(0,len(labels)):\n        if(labels[i]==a):\n            lb.append(data['cat'][i])\n    for i in range(0,4):\n        cout.append(lb.count(cats[i]))\n    plt.bar(cats,cout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(1)  #sports","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(2)  #business","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(3)  #sports","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(4)    #entertainment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_label(5)   #india","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_func(sent):\n    sef=filter_func(sent)\n    b=bow_tfidf([sef])[0]\n    lvec=[0]*6\n    for j in lda_model_tfidf[b]:\n        lvec[j[0]]=j[1]\n    lvec=np.array(lvec)\n    print(lvec)\n    pred=g.predict([lvec])[0]\n    print(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_func(data['headline_text'][9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}