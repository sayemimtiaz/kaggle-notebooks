{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <h1>Analisis Faktor yang Mempengaruhi Kebahagiaan Masyarakat di setiap Negara di Dunia</h1>"},{"metadata":{},"cell_type":"markdown","source":"<p>World Happiness Report adalah survei tengara dari kondisi kebahagiaan global. Laporan pertama diterbitkan pada 2012, yang kedua di 2013, yang ketiga di 2015, dan yang keempat di Pembaruan 2016. World Happiness 2017, yang berada di peringkat 155 negara berdasarkan tingkat kebahagiaannya, dirilis di PBB pada acara yang merayakan Hari Kebahagiaan Internasional pada 20 Maret. Laporan ini terus mendapatkan pengakuan global karena pemerintah, organisasi dan masyarakat sipil semakin menggunakan indikator kebahagiaan untuk menginformasikan keputusan pembuatan kebijakan mereka. Pakar terkemuka di berbagai bidang - ekonomi, psikologi, analisis survei, statistik nasional, kesehatan, kebijakan publik, dan lainnya - menggambarkan bagaimana pengukuran kesejahteraan dapat digunakan secara efektif untuk menilai kemajuan negara. Laporan tersebut meninjau keadaan kebahagiaan di dunia saat ini dan menunjukkan bagaimana ilmu kebahagiaan yang baru menjelaskan variasi kebahagiaan pribadi dan nasional.</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import feature_selection, linear_model\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import cluster, mixture \nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation #For clustering\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy.optimize import curve_fit\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/world-happiness/2017.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Memahami Variabel dalam Dataset</h3>\n<p>Ekonomi PDB per Kapita (Economy..GDP.per.Capita.): Sejauh mana kontribusi PDB terhadap perhitungan Skor Kebahagiaan.</p>\n\n<p>Keluarga (Family): Sejauh mana Keluarga berkontribusi dalam perhitungan Skor Kebahagiaan</p>\n\n<p>Kesehatan,Harapan Hidup(Health..Life.Expectancy.): Sejauh mana Harapan Hidup berkontribusi pada perhitungan Skor Kebahagiaan.</p>\n\n<p>Kebebasan(Freedom): Sejauh mana Kebebasan berkontribusi pada perhitungan Skor Kebahagiaan.</p>\n\n<p>Kepercayaan,Korupsi Pemerintah(Trust..Government.Corruption.): Sejauh mana Persepsi Korupsi berkontribusi pada Skor Kebahagiaan.</p>\n\n<p>Kedermawanan(Generosity): Sejauh mana Kedermawanan berkontribusi pada perhitungan Skor Kebahagiaan.</p>\n\n<p>Dystopia Residual: Sejauh mana Dystopia Residual berkontribusi pada perhitungan Skor Kebahagiaan.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Terdiri dari {:,} baris \".format(df.shape[0]) + \"dan {} kolom dalam data\".format(df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Menghapus kolom yang tidak terpakai/duplikat\ndf2 = df.drop(['Whisker.high','Whisker.low'], axis=1)\n\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Scatterplot untuk masing masing variabel</h2>\n<p>Setelah dataframe selesai di preparation, langkah selanjutnya adalah melihat korelasi beberapa data terhadap kolom point \"Happiness Score\", untuk memudahkan dalam visualisasi dalam kasus ini saya menggunakan library dari seaborns agar lebih efektif dan efisien</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Dugaan korelasi terkuat terhadap Happiness Score</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df2.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Setelah melihat heatmap diatas, yang memungkinkan untuk berkorelasi adalah warna cerah, karena semakin cerah akan mendekati 1, yang artinya sangat berkorelasi. Terdapat 2 korelasi yang lumayan cerah dengan tingkat korelasi 0,81 dan 0,78 yaitu</p>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sns.pairplot(data=df2, height = 5,\n                  x_vars=['Happiness.Score'],\n                  y_vars=['Economy..GDP.per.Capita.', 'Health..Life.Expectancy.'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Didapatkan korelasi antara Happiness Score dan masing-masing variabel lainnya. Variabel mana yang memiliki korelasi tertinggi dengan Happiness Score?</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.corr(method=\"pearson\", min_periods=20)[\"Happiness.Score\"].abs().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Kesimpulan</h3>\n<p>Saya putuskan untuk menggunakan data GDP dikarenakan pada GDP memiliki korelasi yang kuat terhadap Happiness Score, dengan begitu GDP sangat berpengaruh dalam pengambilan Score(abaikan score dan rank)</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=df2, height = 5,\n                  x_vars=['Happiness.Score'],\n                  y_vars=['Economy..GDP.per.Capita.'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Statistik Deskriptif 2 fitur terpilih</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"mina = np.min(df2['Happiness.Score'])\nmaxa = np.max(df2['Happiness.Score'])\nmeana = np.mean(df2['Happiness.Score'])\nmediana = np.median(df2['Happiness.Score'])\nstda = np.std(df2['Happiness.Score'])\nvara = np.var(df2['Happiness.Score'])\n\nminb = np.min(df2['Economy..GDP.per.Capita.'])\nmaxb = np.max(df2['Economy..GDP.per.Capita.'])\nmeanb = np.mean(df2['Economy..GDP.per.Capita.'])\nmedianb = np.median(df2['Economy..GDP.per.Capita.'])\nstdb = np.std(df2['Economy..GDP.per.Capita.'])\nvarb = np.var(df2['Economy..GDP.per.Capita.'])\n\nprint(\"Min Happiness Score :\",mina, '\\t\\t\\t\\tdan Min GDP : ',minb )\nprint(\"Max Happiness Score :\",maxa, '\\t\\t\\tdan Max GDP : ',maxb)\nprint(\"Mean Happiness Score :\",meana, '\\t\\t\\tdan Mean GDP : ',meanb)\nprint(\"Median Happiness Score :\",mediana, '\\t\\t\\tdan Median GDP : ',medianb)\nprint(\"Standar Deviation Happiness Score :\",stda, '\\t\\tdan Standar Deviation GDP : ',stdb)\nprint(\"Variance Happiness Score :\",vara, '\\t\\t\\tdan Variance GDP : ',varb)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Analisisi Model Regersi (linear/nonlinier)</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Analisis Model Regresi\nmsk = np.random.rand(len(df2)) < 0.8\ntrain = df2[msk]\ntest = df2[~msk]\n\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['Happiness.Score']])\ntrain_y = np.asanyarray(train[['Economy..GDP.per.Capita.']])\nregr.fit (train_x, train_y)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Regressi Linear</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Regresi Linear\nplt.scatter(train[['Happiness.Score']], train[['Economy..GDP.per.Capita.']],  color='blue')\nplt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')\nplt.xlabel(\"Happiness.Score\")\nplt.ylabel(\"Health..Life.Expectancy.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Model Evaluasi</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = np.asanyarray(test[['Happiness.Score']])\ntest_y = np.asanyarray(test[['Economy..GDP.per.Capita.']])\ntest_y_ = regr.predict(test_x)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y_ , test_y) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Regressi NonLiniear</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_data, y_data = (df2[\"Happiness.Score\"].values, df2[\"Economy..GDP.per.Capita.\"].values)\nplt.plot(x_data, y_data, 'mo' ,alpha=0.5)\nplt.xlabel('Happiness.Score')\nplt.ylabel('Economy..GDP.per.Capita.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp (-beta_1*(x-Beta_2)) )\n     return y\n\ndef expo (x, Beta_0, Beta_1):\n     y = Beta_0*np.exp(Beta_1*x)\n     return y\n\ndef qubic (x, Beta_0, Beta_1, Beta_2, Beta_3):\n     y = Beta_0+Beta_1*x+Beta_2*x**2+Beta_3*x**3\n     return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"beta_1 = 1.0\nbeta_2 = 1\nbeta_3= 1\nbeta_4=0.1\n\nY_preds =sigmoid (x_data, beta_1, beta_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xdata =x_data/max(x_data)\nydata =y_data/max(y_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popt1, pcov1 = curve_fit(sigmoid, xdata, ydata, maxfev = 10000)\npopt2, pcov2 = curve_fit (expo, xdata, ydata, maxfev = 10000) \npopt3, pcov3 = curve_fit (qubic, xdata, ydata, maxfev = 10000) \n\nprint(\" Exponensial\",\"B1 = %f, B2=%f\"%(popt2[0], popt2[1]))\nprint(\" Sigmoid\",\"B1 = %f, B2=%f\"%(popt1[0], popt1[1]))\nprint(\" Qubic\",\"B1 = %f, B2=%f\"%(popt3[0], popt3[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.linspace(10, 200, 1000)\nx = x/max(x)\nplt.figure(figsize=(13,5))\n\ny1 = expo(x, *popt1)\ny2 = sigmoid(x, *popt2)\ny3 = qubic(x, *popt3)\n\nplt.plot(xdata, ydata, 'mo',alpha=0.5 ,label='data')\nplt.plot(x,y1, linewidth=3.0, label='Eksponensial')\nplt.plot(x,y2, linewidth=3.0, label='Sigmoid')\nplt.plot(x,y3, linewidth=3.0, label='Qubic')\nplt.legend(loc='best')\nplt.xlabel('Efisiensi Pemain')\nplt.ylabel('Points')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# msk = np.random.rand(len(df2)) < 0.8\n\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt1, pcov1 = curve_fit(sigmoid, train_x, train_y, maxfev = 100000)\npopt2, pcov2 = curve_fit(expo, train_x, train_y, maxfev = 100000)\npopt3, pcov3 = curve_fit(qubic, xdata, ydata, maxfev = 10000)\n\ny_hat1 = sigmoid(test_x, *popt1)\ny_hat2 = expo(test_x, *popt2)\ny_hat3 = qubic(test_x, *popt3)\n\nprint (\"Sigmoid\")\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat1 - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat1 - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat1 , test_y) )\n\nprint (\"\\nExponens\")\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat2 - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat2 - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat2 , test_y) )\n\nprint (\"\\nQubic\")\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat3 - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat3 - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat3 , test_y) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kesimpulan : Setelah melakukan pembandingan Linier dan NonLinier, Error terkecil berada pada Regresi Non Linier milik Qubic memiliki r2Score 0.69 dibandingkan dengan yang lainnya. Prediksi dapat dilakukan menggunakan hasil B0 dan B1 dari regresi Qubic"},{"metadata":{},"cell_type":"markdown","source":"<h1>Analisis menggunakan clustering</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#hapus country dan happiness rank\ndf2 = df2.iloc[:,2:]\n\nprint(\"\\n \\n Dimenstion of dataset  : df2.shape\")\ndf2.shape\n\ndf2.dtypes\n\nss = StandardScaler()\nss.fit_transform(df2)\ndf2.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tentukan kelas CluserMethod: yang mengembalikan hasil pengelompokan berdasarkan input\nclass ClusterMethodList(object) :\n    def get_cluster_instance(self, argument,input_data,X):\n        method_name = str(argument).lower()+ '_cluster'\n        method = getattr(self,method_name,lambda : \"Invalid Clustering method\")\n        return method(input_data,X)\n    \n    def kmeans_cluster(self,input_data,X):\n        km = cluster.KMeans(n_clusters =input_data['n_clusters'] )\n        return km.fit_predict(X)\n   \n    def meanshift_cluster(self,input_data,X):\n        ms = cluster.MeanShift(bandwidth=input_data['bandwidth'])\n        return  ms.fit_predict(X)\n    \n    def minibatchkmeans_cluster(self,input_data,X):\n        two_means = cluster.MiniBatchKMeans(n_clusters=input_data['n_clusters'])\n        return two_means.fit_predict(X)\n   \n    def dbscan_cluster(self,input_data,X):\n        db = cluster.DBSCAN(eps=input_data['eps'])\n        return db.fit_predict(X)\n    \n    def spectral_cluster(self,input_data,X):\n        sp = cluster.SpectralClustering(n_clusters=input_data['n_clusters'])\n        return sp.fit_predict(X)\n   \n    def affinitypropagation_cluster(self,input_data,X):\n        affinity_propagation =  cluster.AffinityPropagation(damping=input_data['damping'], preference=input_data['preference'])\n        affinity_propagation.fit(X)\n        return affinity_propagation.predict(X)\n       \n    \n    def birch_cluster(self,input_data,X):\n        birch = cluster.Birch(n_clusters=input_data['n_clusters'])\n        return birch.fit_predict(X)\n   \n    def gaussian_mixture_cluster(self,input_data,X):\n        gmm = mixture.GaussianMixture( n_components=input_data['n_clusters'], covariance_type='full')\n        gmm.fit(X)\n        return  gmm.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# menentukan clustering process\n\ndef startClusteringProcess(list_cluster_method,input_data,no_columns,data_set):\n    fig,ax = plt.subplots(no_rows,no_columns, figsize=(10,10)) \n    cluster_list = ClusterMethodList()\n    i = 0\n    j=0\n    for cl in list_cluster_method :\n        cluster_result = cluster_list.get_cluster_instance(cl,input_data,data_set)\n        #convert cluster result array to DataFrame\n        data_set[cl] = pd.DataFrame(cluster_result)\n        ax[i,j].scatter(data_set.iloc[:, 0], data_set.iloc[:, 1],  c=cluster_result)\n        ax[i,j].set_title(cl+\" Cluster Result\")\n        j=j+1\n        if( j % no_columns == 0) :\n            j= 0\n            i=i+1\n    plt.subplots_adjust(bottom=-0.5, top=1.5)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_cluster_method = ['KMeans',\"MeanShift\",\"MiniBatchKmeans\",\"DBScan\",\"Spectral\",\"AffinityPropagation\",\"Birch\",\"Gaussian_Mixture\"]\n# untuk menampilkan graph\nno_columns = 2\nno_rows = 4\n# tdk smw algo make ini\nn_clusters= 3\nbandwidth = 0.1\n# eps untuk DBSCAN\neps = 0.3\n## Damping and perference untuk Affinity Propagation clustering method\ndamping = 0.9\npreference = -200\ninput_data = {'n_clusters' :  n_clusters, 'eps' : eps,'bandwidth' : bandwidth, 'damping' : damping, 'preference' : preference}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"startClusteringProcess(list_cluster_method,input_data,no_columns,df2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}