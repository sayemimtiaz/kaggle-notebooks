{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Cleaning**","metadata":{}},{"cell_type":"markdown","source":"### Hey everyone, \nWe will focus on the famous wine-reviews dataset. It contains information about both, the wines and the tasters. \n\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment</span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote</span> if you like this project.</h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***\n<h1 style=\"background-color:darkorange; color:black\" >-> Topics:</h1>\n\n## 0. [**What is Data Cleaning?**](#sec0)\n## 1. [**Find and visualize Missing Values**](#sec1)\n## 2. [**Handle Missing Values**](#sec2)\n#### 2.1. [Drop rows](#sec3)\n#### 2.2. [Drop columns](#sec4)\n#### 2.3. [Replace Null values with 'Unknown' or 0](#sec5)\n#### 2.4. [Replace missing values with predicted values](#sec6)\n#### 2.5. [Attention! Hidden Missing Values](#sec61)\n## 3. [**Handle Measurng Errors and Outliers**](#sec8)\n## 4. [**Change Measuring Units**](#sec8)\n## 5. [**Delete Duplicates**](#sec9)\n***","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec0\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 0. What is Data Cleaning?</h1>\n\nData Cleaning is the collective term for all methods which improve the [quality of data](https://en.wikipedia.org/wiki/Data_cleansing#Data_quality).\nData Quality has huge impacts on the strength of machine learning models.\n\nHigh-quality data has the following properties:\n\n\n### <b><mark style=\"background-color: darkorange\"><font color=\"black\">Completeness:</font></mark></b>\n* There should be no [missing values](#sec1) in our dataset.\n\n\n### <b><mark style=\"background-color: darkorange\"><font color=\"black\">Consistency:</font></mark></b>\n* No inconsistencies within the data.\n* Inconsistencies may occur in one dataset or between multiple datasets.\n* Example 1: One data set contains the customers' places of residence. An inconsistency would occur, when the zip code and the address don't match within the same row\n* Example 2: One dataset contains information about the status of your companies cargo ships (damaged, under repair, scrapped). The second dataset contains information about the position of the cargo ships. An inconsistency would occur if one cargo ship is said to be scrapped but on the open sea.\n\n\n### <b><mark style=\"background-color: darkorange\"><font color=\"black\">Uniformity:</font></mark></b>\n* Variables should be measured on the same scale.\n* Example: Some income might be measured in EUR, some might be measured in USD\n* Assure uniformity even across multiple features (E.g. measure both, the income and the expenses using the same unit)\n\n\n### <b><mark style=\"background-color: darkorange\"><font color=\"black\">Accuracy:</font></mark></b>\n* The data should be as close to the ground truth as possible\n* Example: Since the exact height of people depends, among other things, on the daytime, it's impossible to determine 100% accurate measurements. Errors of a few millimeters are legit, but using inaccurate measurements (like 1.80 instead of 1.87) should be avoided.\n\n\n### <b><mark style=\"background-color: darkorange\"><font color=\"black\">Validity:</font></mark></b>\n* The data fulfills the necessary constraints. These constraints could either be defined for functionality reasons (e.g. the uniqueness of identifiers) or domain and business-related reasons (e.g. the pattern of customer identifiers).","metadata":{}},{"cell_type":"markdown","source":"Let us first of all load the dataset, import everything needed, and take a look at the top rows of this dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndf = pd.read_csv('../input/wine-reviews/winemag-data-130k-v2.csv')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 1. Find and visualize Missing Values</h1>\n\nFirst of all, we should clarify, what exactly missing values are. Missing data occurs, when a specific value wasn't measured, can't be measured, or got lost due to any mistakes. In numpy and pandas, we distinguish between the two following types of missing values (aka null values):\n\n* **None:** numpy arrays and pandas series with *dtype: 'object'* contain **None** values, if missing values occur.\n* **NaN:** numpy arrays and pandas series with *dtype: 'float64'* (or any float in general) contain **NaN** values, if missing values occur. The advantage of NaNs is, that they don't lead to a crash when used in aggregations:\n","metadata":{}},{"cell_type":"code","source":"with_None = np.array([1, None, 2])\nwith_NaN = np.array([1, np.nan, 2])\nprint(f'with_None dtype: {with_None.dtype}\\n' + \n      f'with_NaN dtype: {with_NaN.dtype}')","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with_None.sum()  throws an error, due to None","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with_NaN.sum() # throws no error, but provides a nan","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with_NaN.sum() * 10 # provides nan","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.nansum(with_NaN) # sum, which ignores NaN (throws error with None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values lead to a lack of [Completeness](#sec0). \n\nIt is not only important to know whether a dataset contains missing values, but also do we need to know which columns contain which amount of missing values. ","metadata":{}},{"cell_type":"code","source":"# NOTE: somehow, the heatmap of my dataset doesn't show the null value sin the columns \n# 'country', 'province' and 'variety'. Please help me to fix this.\ndef nullscan(df_check, save=False):\n    '''\n    df: a dataframe on which we want to perofrm the nullscan\n    save: determines, whether you want to save the .png of the plot or not\n    \n    plots the rate of null values per column in a dataframe using \n    a seaborn heatmap and a barplot.\n    '''    \n    # a df with the same size of the original dataframe, containing True in cells containing NUll values.\n    # and False in all the other cells.\n    df_nulls = df_check.isna()\n    # a series containing the sum of all values within a column having the column names as indices.\n    # True is interpreted as 1 and False is interpreted as 0 \n    nulls_per_col = df_nulls.sum(axis=0)\n    # the rate makes it way more interpretable:\n    nulls_per_col /= len(df_check.index)\n\n    with plt.style.context('dark_background'):\n        fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(8, 10))\n    \n        # ax1 is losely based on: https://www.kaggle.com/ipshitagh/wine-dataset-data-cleaning\n        # NOTE: I could have used the cmap viridis or anything else instead, \n        # but I want to make clear that you can use any customized cmap as well.\n        vir = matplotlib.cm.get_cmap('viridis')\n        colormap = matplotlib.colors.ListedColormap([vir(0), 'darkorange'])\n        sns.heatmap(df_check.isnull(), cmap=colormap, cbar=False, yticklabels=False, ax=ax1)\n    \n        nulls_per_col.plot(kind='bar', color='darkorange', x=nulls_per_col.values, \n                           y=nulls_per_col.index, ax=ax2, width=1, linewidth=1, \n                           edgecolor='black', align='edge', label='Null value rate')\n        \n        ax2.set_ylim((0,1))\n        # centered labels\n        labels=df_check.columns\n        ticks = np.arange(0.5, len(labels))\n        ax2.xaxis.set(ticks=ticks, ticklabels=labels)\n    \n        # hide spines:\n        # NOTE: I could have used ax2.set_frameon(False), \n        # but I wanted the bottom and the left spine to stay white.\n        ax2.spines['top'].set_color('black')\n        ax2.spines['right'].set_color('black')\n        \n        \n        \n        # workaround to visualize very small amounts of null values per col\n        na_ticks = ticks[(nulls_per_col > 0) & (nulls_per_col < 0.05)]\n        if (len(na_ticks) > 0):\n            ax2.plot(na_ticks, [0,]*len(na_ticks), 's', c='darkorange', markersize=10, \n                     label='Very few missing values')\n    \n        fig.suptitle('Null Value Rate per Column', fontsize=30, y=1.05)\n        ax2.legend()\n        fig.tight_layout() \n        if(save):\n            plt.savefig('nullscan.png')\n        plt.show()\nnullscan(df)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we know, that the dataset contains missing values, we have to decide how to handle them.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2. Handle Missing Values</h1>\n\n\n**There are some standard ways of handling these Null Values:**\n\n* Drop rows containing a specified amount of Null Values. Use this method only, if the number of missing values within the respective row is very high. \n\n\n* Drop columns containing a specified amount of Null Values if the column isn't too relevant and the number of missing values is very high.\n\n\n* Replace missing values with the column average. The most common value of a categorical feature and the median or the mean of a numerical feature can be interpreted as the average of the respective column.\n\n\n* Replace the missing value with a predicted value. Therefore, one could use any Machine Learning Algorithm.\n\n\n* Replace the missing values with zeros or 'Unknown' and treat it just like any other value if possible\n\nNote: Replacing missing values is often called [imputation](https://scikit-learn.org/stable/modules/impute.html). If you impute many missing values in one column, it might improve the performance of your model if you create a binary flag column containing ones in all rows where you imputed a missing value.\n\n\nBesides these standard methods, insights into the dataset and its context can reveal further suitable methods. Consider we have a dataset containing the email and the name of people. The *name* column contains missing values and the *email* column only contains values in rows having null values in the *name* column. It might be useful to fill the emails into the corresponding *name* cells in order to obtain an identifier for each person containing the *name* or at least the *email*.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2.1. Drop rows</h1>\n","metadata":{}},{"cell_type":"markdown","source":"As we can see in the plot above, the columns 'country' and 'province' contain very few missing values. I want to show you how to drop the rows containing Null values in these columns. This method of handling Null values shouldn't be used too much, because we lose costly collected data. Hence, we should always consider if any other suitable methods could be applied instead if we are solving real-world problems.","metadata":{}},{"cell_type":"code","source":"# drop all rows with Null values in 'country', 'province' OR 'variety':\ndf = df.dropna(subset=['country', 'province', 'variety'])\n\n# one could even drop all rows containing 'too many' Null values:\n# nrows_before = len(df.index)\n# na_allowed = int(len(df.columns)/3)\n# thresh = int(len(df.columns)) - na_allowed\n# df = df.dropna(axis=0, thresh=thresh)\n# nrows_afterwards = len(df.index)\nnullscan(df, save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2.2. Drop Columns</h1>","metadata":{}},{"cell_type":"markdown","source":"Since the column 'region_2' contains many Null Values and it doesn't contain important information, we can simply drop the column.","metadata":{}},{"cell_type":"code","source":"df = df.drop('region_2', axis=1)\n\nnullscan(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec5\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2.3. Replace Null values with 'Unknown' or 0</h1>\n","metadata":{}},{"cell_type":"markdown","source":"The column 'designation' has the second biggest amount of Missing Values. Let's take a look at the column.","metadata":{}},{"cell_type":"code","source":"designation = df['designation'].value_counts().head(20) / len(df.index)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,10), sharey=True)\nax.barh(y=designation.index, width=designation.values, color='darkorange', edgecolor='black')\nax.set_title('Occurance of Designations in the Dataset', fontsize=20)\nax.set_xlabel('Occurance in the Dataset')\nax.set_ylabel('Designation')\nax.set_facecolor('black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some values appear pretty often and it might be interesting to reveal any relationships between this feature and the other features. We shouldn't drop the column but replace the missing values with anything suitable. Unfortunately, the most common designation 'Reserve' is merely occurring in slightly more than 1% of the rows. Hence, we shouldn't replace the missing values with this most common value of the column.\n\nAlternatively, I suggest replacing missing values with 'Unknown' and treat it like any other value. The same method can be applied to the column 'region_1'.","metadata":{}},{"cell_type":"code","source":"df[['designation', 'region_1']] = df[['designation', 'region_1']].fillna('Unknown')\nnullscan(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To handle 'taster_name' and 'taster_twitter_handle', we have to get some insights into the relationship of the two variables. \n\nWe know that both represent a reviewer and it might happen, that some reviewers are solely known by their name, and that some others are solely known by their Twitter handle. If so, we would have to find a way to merge these two columns into one column containing an identifier for the taster. To that end, we would prioritize the 'taster_name' to distinguish between the tasters from each other and fill the missing values in that column with the Twitter handle in the respective rows. Furthermore, it might happen that a single Twitter handle is used to review wines by multiple people or a single taster can have multiple Twitter handles.\n\n**Let's see how often a taster name is given but a Twitter handle isn't:**","metadata":{}},{"cell_type":"code","source":"only_name = df.loc[df['taster_twitter_handle'].isnull() & df['taster_name'].notna(), \n                   ['taster_name', 'taster_twitter_handle']]\nnum_only_name = len(only_name.index)\n\nonly_twitter = df.loc[df['taster_name'].isnull() & df['taster_twitter_handle'].notna(), \n                      ['taster_name', 'taster_twitter_handle']]\nnum_only_twitter = len(only_twitter.index)\n\nprint(f'rows containing a name but no twitter handle: {num_only_name}'\n      + f'\\nrows containing a twitter handle but no taster name: {num_only_twitter}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since tasters might review wines using multiple Twitter handles, and review differently using differing Twitter handles (e.g. if they delete their Twitter account and start reviewing wine on Twitter using a new account), we should scan if there are any tasters using multiple Twitter handles.","metadata":{}},{"cell_type":"code","source":"twitter_per_name = df.groupby('taster_name')['taster_twitter_handle'].nunique()\nlabels = twitter_per_name.index\n\n\nfig, ax = plt.subplots(figsize=(8,10))\ntwitter_per_name.plot(kind='barh', ax=ax, color='darkorange', edgecolor='black')\nax.set_xticks([0,1])\nax.set_xlabel('Number of Twitter Handles')\nax.set_ylabel('Taster')\nax.set_title('Twitter handles per taster', fontsize=20)\nax.set_facecolor('black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, each taster uses either one Twitter Handle or no Twitter handle at all. Thus, we can simply drop the column 'taster_twitter_handle'. It doesn't provide any  further information for our models. Moreover, we can replace the missing values in 'taster_name' with Unknown. ","metadata":{}},{"cell_type":"code","source":"df = df.drop('taster_twitter_handle', axis=1)\ndf['taster_name'] = df['taster_name'].fillna('Unknown')\nnullscan(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec6\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2.4. Replace missing values with predicted values</h1>","metadata":{}},{"cell_type":"markdown","source":"The last column containing Null values is 'price'. We could easily replace missing values with the column mean. The advantage of this method would be, that the column means would stay the same, which might be important in later analysis steps. On the other hand, this method ( just like replacing missing values with the median of the column) is very unprecise and doesn't preserve the relationship between the respective feature and the other features.\n\nI will give you a simple example of predicting the missing values using KNN. It will replace the missing price value with the median of the price values of the K nearest neighbors. Obviously, this method will most likely not reveal the real prices in all cases but will contain some residuals. At least, this method preserves the relationship between the used features. For simplicity, I will solely use the features 'points', 'country' and 'taster_name' to predict the missing values.\n\nSince we want to predict prices, i.e. numerical values, we use a regression model (KNeighboursRegressor).\n\nOne advantage of using a KNN Regressor instead of any other regression model is that it (by definition) doesn't predict values that are out of the range from the already collected data observations (i.e. the training data). \n\nInfo: The sklearn KNeighboursRegressor predicts the mean of the y-values of the K nearest neighbors, but in some cases, it might be more suitable to predict the median. Unfortunately, predicting the mean is hardcoded in sklearns KNeighboursRegressor (and for simplicity I will rely on sklearn in this tutorial).","metadata":{}},{"cell_type":"markdown","source":"Firstly, convert the categorical columns 'country' and 'taster_name' to indicator variables and store the training data and the data on which we want to predict the prices individually.","metadata":{}},{"cell_type":"code","source":"df_cleanup = df.loc[:, ['price', 'points', 'country', 'taster_name']]\nencoded = pd.get_dummies(df_cleanup[['country', 'taster_name']], prefix=['country', 'taster_name'])\ndf_cleanup = pd.concat([df_cleanup.drop(['country', 'taster_name'], axis=1), encoded], axis=1)\n\n# training data\ndf_cleanup_known = df_cleanup.loc[df_cleanup['price'].notnull(), :]\nX_known = df_cleanup_known.drop('price', axis=1)\ny_known = df_cleanup_known['price']\n\n# prediction data\ndf_cleanup_unknown = df_cleanup.loc[df_cleanup['price'].isnull(), :]\nX_unknown = df_cleanup_unknown.drop('price', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create the Regressor.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nknn_cleanup = KNeighborsRegressor()\n\n# simple finetuning:\nparams = {'n_neighbors':[1,5,9]}\ngs = GridSearchCV(knn_cleanup, params)\ngs.fit(X_known, y_known)\nknn_cleanup = gs.best_estimator_","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict the prices and concatenate the data with already known prices and the data with predicted prices. \n\nShuffle the result in order to ensure that the rows with predicted prices are not stored in a huge block.","metadata":{}},{"cell_type":"code","source":"df_known = df.loc[df['price'].notnull(),:]\ndf_predicted = df.loc[df['price'].isnull(),:]\n\n# to evade SettingWithCopyWarning\ndf_predicted = df_predicted.drop('price', axis=1)\ndf_predicted['price'] = knn_cleanup.predict(X_unknown)\ndf = pd.concat([df_known, df_predicted], axis=0, ignore_index=True)\n\n# shuflle the dataset along rows\ndf = df.sample(frac=1).reset_index(drop=True)\n\n\n\nnullscan(df)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values left!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec61\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 2.5. Attention! Hidden Missing Values</h1>","metadata":{}},{"cell_type":"markdown","source":"We got rid of all Null values, so we can already train a machine learning model on the data. But sometimes, missing values are not given in form of `None` or `NaN`. Let me give you an example. Let's take a look at the pima-indians diabetes dataset!","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\ndata_diabetes = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')[['SkinThickness', 'Glucose']]\nscatter_matrix(data_diabetes, figsize=(8,8));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can you see these streaks at 0 ? They occure in more than just these two columns of this dataset. They most likely represent missing values in this dataset.\nWhat I want to tell you is that you definitely have to take a look at your dataset. Plotting variables against each other may reveal interesting residuals from questionable preprocessing decisions which affected the data before you are abled to access the data.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec7\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 3. Handle Measurng Errors and Outliers</h1>\n","metadata":{}},{"cell_type":"markdown","source":"Measuring Errors and Outliers have a huge impact on the [Data Quality](#sec0):\n* Consistency: A measuring error might lead to contradictory values (as described in [Section 0](#sec0))\n* Accuracy: For obvious reasons, measuring errors might lead to values far away from the ground truth. Outliers distort \"the big picture\" and thus lead to a weakened accuracy concerning the whole population.\n\nSometimes, we can detect implicitely detect measuring errors by investigating outliers. E.g. if we have a dataset of student grades and each student can achieve between 0 and 10 points. If the dataset tells us that one student achieved more than 10 points, our dataset contains a measuring error and we have to determine how to handle this issue. In this case, the measuring error would most likely be an outlier as well, and we could detect it via outlier detection. \n\nNevertheless, there might be some more measuring errors within our dataset, which aren't outliers, but in most cases, there is no **simple** way to detect these, as long as they match into the pattern of the data. We would have to treat them as noise. \n\nData-points which are both, measuring errors and outliers, might be very harmful and have a huge impact on our model. \nBesides handling these harmful measuring errors, handling outliers might help our model to be even more powerful and we may want to handle them during [Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering/edit/run/45767292). Hence, we should definitely detect the outliers for multiple reasons.\n\nBut how do we detect outliers automatically? \nAnd how do we decide if these outliers are harmful?\nLet us find out whether the numerical column 'price' contains outliers.","metadata":{}},{"cell_type":"markdown","source":"The probably most famous ways of detecting outliers are using the *z score* and visualizing the data using a *boxplot*.","metadata":{}},{"cell_type":"code","source":"fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(8, 10))\ndf.boxplot(column='price', ax=ax1)\nax.set_title('Outliers in Price?');","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I prefer using z-scores to detect `extreme` outliers only. Usually, outliers are defined as values having z-scores having an absolute value of 3. I prefer using a higher threshold of about 5 or 6:","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n# display all rows having outliers in 'price'\ndata = pd.DataFrame(df['price'])\ndata_zscore = data.apply(stats.zscore, axis=0)\nmask_outliers = ((data_zscore>6).any(axis=1)).values + ((data_zscore<-6).any(axis=1)).values\ndata.loc[mask_outliers,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are outliers in the respective column. Should we handle these as measuring errors?\n\nThis example shows us, that it isn't as easy to detect measuring errors as it is to detect missing values. One can simply determine whether a value is missing or not, but we need further information to determine whether a value is valid or not. \n\nI guess most of us wouldn't even think about buying a wine bottle that costs more than 200$, but every one of us knows that there are wines costing way more than a couple of hundred dollars per bottle, even though these wine bottles might not be found in the supermarket. Although expensive wines are rare, they still exist and thus we can't simply handle these data points as measuring errors. \n\nAs you can see, boxplots might be useful, but in most cases, one needs to have some *domain knowledge* to determine whether one has to handle specific outliers separately as measuring errors or not. \n\nAnyways, if you are sure that you found some data-points which are measuring errors, you can treat them just like [Missing values](#sec1). And what about the outliers that are probably no errors? We should try to reduce their impact on our machine learning model during [Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering). if that's not working, we can still handle them as missing values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec8\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 4. Change Measuring Units</h1>\n\nTo obtain [Uniformity](#sec0), we might have to bring values to a different unit. \n\nUnfortunately, it isn't always possible to determine, if one feature contains values, which were measured on the same scale. Further informations like notes from the data measuring process might be helpful. However, it might help to use a clustering algorithm to find out, if there are clusters with (context-based) extremely dissimilar mean values.\n\nLet me give you an easy example using [k-means clustering](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1):","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nheight = np.array([1.74, 1.67, 1.87, 176, 184, 159, 163, 1.71, 165, 1.62, 191, 1.82])\nnp.size(height)\n\n# use kmeans; try different number of clusters, if needed\ncluster_kmeans = KMeans(n_clusters=2)\ncluster_kmeans.fit(height.reshape((-1,1)))\n# plot\ncolors = np.array(['darkorange', 'firebrick'])\nplt.scatter(x=range(np.size(height)), y=height, c=colors[cluster_kmeans.labels_])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the plotted clusters, it might be obvious, that the data isn't measured on the same scale. If it is obvious or not might depend on the context.\nAfterward, we can easily separate the clusters and bring them on the same scale:","metadata":{}},{"cell_type":"code","source":"# separate the clusters\nheight_meter = height[cluster_kmeans.labels_ == 1]\nheight_cm = height[cluster_kmeans.labels_ == 0]\n# bring them on a mutually equal scale\nheight_meter = height_meter * 100\n# plot\nheight_transformed = np.append(height_cm, height_meter)\nplt.scatter(x=range(np.size(height)), y=height_transformed, c='firebrick')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In simple cases like this, it might even be sufficient to take a look at the variance, since the variance of this feature should is pretty big, which indicates the data to contain very versatile values. \n\nHowever, in most cases, the best way to determine whether the data has differing scales within one feature is to refer to domain knowledge or notes from the data measuring procedure.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec9\"></a>\n<h1 style=\"background-color:darkorange; color:black\" >-> 5. Delete Duplicates</h1>\n\nTo obtain [Validity](#sec0), we have to ensure that the data fuflills a range of specifications. One very basic specification which has to be fulfilled in almost every case is that the data shouldn't contain any duplicates. \n\nLet's get ridd of all duplicates, if there are any..","metadata":{}},{"cell_type":"code","source":"# our data doesn't contain any duplicates\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So let#s create some data to give you an example..","metadata":{}},{"cell_type":"code","source":"dupli_values = np.array([[1,2],\n                        [1,2],\n                        [3,4],\n                        [5,6],\n                        [3,4]])\n\ndupli_data = pd.DataFrame(data = dupli_values, columns=['a', 'b'])\ndupli_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicates(data):\n    dupli_con = pd.Series(range(len(data)))[data.duplicated() == False]\n    return data.loc[dupli_con,:]\n    \nremove_duplicates(dupli_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## That's it, thank you for reading my Notebook =)\n\nI hope you learned a lot about cleaning your data and thereby improving its quality. \n\nFeel free to suggest further methods and ideas in the commetn section below!","metadata":{}}]}