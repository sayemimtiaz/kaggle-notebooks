{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Credits to [Utility Script Winners](https://www.kaggle.com/general/113366#652563)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport pandas as pd\nimport numpy as np\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<img src=\"https://www.911metallurgist.com/blog/wp-content/uploads/2013/09/flotation-separators.jpg\" width=\"450\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%writefile utilityfunctions.py\n# Utility Functions\n\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import NuSVR\nfrom sklearn.linear_model import HuberRegressor\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef summary(df):\n    \"\"\"\n    Display data summary\n    \"\"\"\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name', 'dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    display(summary)\n    return summary\n\n\ndef stats(data_frame):\n    \"\"\"\n    Collect stats about all columns of a dataframe, their types\n    and descriptive statistics, return them in a Pandas DataFrame\n\n    :param data_frame: the Pandas DataFrame to show statistics for\n    :return: a new Pandas DataFrame with the statistics data for the\n    given DataFrame.\n    \"\"\"\n    stats_column_names = ('column', 'dtype', 'nan_cts', 'val_cts',\n                          'min', 'max', 'mean', 'stdev', 'skew', 'kurtosis')\n    stats_array = []\n    for column_name in sorted(data_frame.columns):\n        col = data_frame[column_name]\n        if is_numeric_column(col):\n            stats_array.append(\n                [column_name, col.dtype, col.isna().sum(), len(col.value_counts()),\n                 col.min(), col.max(), col.mean(), col.std(), col.skew(),\n                 col.kurtosis()])\n        else:\n            stats_array.append(\n                [column_name, col.dtype, col.isna().sum(), len(col.value_counts()),\n                 0, 0, 0, 0, 0, 0])\n    stats_df = pd.DataFrame(data=stats_array, columns=stats_column_names)\n    return stats_df\n\n\ndef of_type(stats_data_frame, column_dtype):\n    \"\"\"\n    Filter on columns of a given dtype ('object', 'int64', 'float64', etc)\n\n    :param stats_data_frame: a DataFrame produced by the stats() function (above)\n    :param column_dtype: a valid column dtype string ('object', 'int64', 'float64', ...)\n    :return: the stats_data_frame that was passed in\n    \"\"\"\n    return stats_data_frame[stats_data_frame['dtype'] == column_dtype]\n\n\ndef sort(data_frame, column_name, ascending=False):\n    \"\"\"\n    Shorthand for sorting a data frame by one column's values.\n    Useful with the status dataframe columns.\n\n    :param data_frame: data_frame whose contents are to be sorted\n    :param column_name: String name of the column to sort by\n    :param ascending: if True, sort in ascending order (default, False)\n    :return: a copy of the data_frame, sorted as specified\n    \"\"\"\n    return data_frame.sort_values(column_name, ascending=ascending)\n\n\ndef is_numeric_column(df_column):\n    \"\"\"\n    Answer whether a column of a data_frame is numeric\n\n    :param df_column: Any column from a Pandas DataFrame\n    :return: True if it's in one of the standard numeric types\n    \"\"\"\n    numeric_types = (np.int16, np.float16, np.int32, np.float32,\n                     np.int64, np.float64)\n    return df_column.dtype in numeric_types\n\n# Auto Regression\ndef log_transform(x):\n    return np.log1p(x)\n\ndef inverse_log_transform(x):\n    return np.expm1(x)\n\n\ndef getClassifiers():\n\n    \"\"\"\n    Provide lists of regression classifiers and their names.\n    \"\"\"\n    n_jobs       = -1\n    random_state =  42\n\n    classifiers = [\n                   DummyRegressor(),\n                   #IsotonicRegression(random_state=random_state),\n                   KNeighborsRegressor(n_neighbors=7),\n                   LinearRegression(n_jobs=n_jobs), \n                   Ridge(random_state=random_state), \n                   Lasso(random_state=random_state), \n                   ElasticNet(random_state=random_state),\n                   KernelRidge(),\n                   HuberRegressor(),\n                   SGDRegressor(random_state=random_state),\n                   SVR(kernel=\"linear\"),\n                   LinearSVR(random_state=1),\n                   NuSVR(C=1.0, nu=0.1),\n                   DecisionTreeRegressor(random_state=random_state),\n                   RandomForestRegressor(n_jobs=n_jobs, random_state=random_state),\n                   GradientBoostingRegressor(random_state=random_state),\n                   lgb.LGBMRegressor(n_jobs=n_jobs, random_state=random_state),\n                   xgb.XGBRegressor(objective=\"reg:squarederror\", n_jobs=n_jobs, random_state=random_state),\n    ]\n\n    clf_names = [\n                \"DummyRegressor       \",\n                #\"IsotonicRegression   \",\n                \"KNeighborsRegressor  \",\n                \"LinearRegression     \", \n                \"Ridge                \",\n                \"Lasso                \",\n                \"ElasticNet           \",\n                \"KernelRidge          \",\n                \"HuberRegressor       \",\n                \"SGDRegressor         \",\n                \"SVR                  \",\n                \"LinearSVR            \",\n                \"NuSVR                \",\n                \"DecisionTreeRegressor\",\n                \"RandomForest         \", \n                \"GBMRegressor         \", \n                \"LGBMRegressor        \", \n                \"XGBoostRegressor     \",\n    ]\n\n    return clf_names, classifiers\n\n\n\ndef prepareData(df, target_name):\n\n    \"\"\"\n    Separate descriptive variables and target variable.\n    Separate numerical and categorical columns.\n    \"\"\"\n\n    if target_name is not None:\n        X = df.drop(target_name, axis=1)\n        y = df[target_name]\n    else:\n        X = df\n        y = None\n\n    # get list of numerical & categorical columns in order to process these separately in the pipeline \n    num_cols = X.select_dtypes(\"number\").columns\n    cat_cols = X.select_dtypes(\"object\").columns\n    \n    return X, y, num_cols, cat_cols\n\n\ndef getPipeline(classifier, num_cols, cat_cols, impute_strategy, log_x, log_y):\n\n    \"\"\"\n    Create Pipeline with a separate pipe for categorical and numerical data.\n    Automatically impute missing values, scale and then one hot encode.\n    \"\"\"\n\n    # the numeric transformer gets the numerical data acording to num_cols\n    # first step: the imputer imputes all missing values to the provided strategy argument\n    # second step: all numerical data gets stanadard scaled \n    if log_x == False:\n        numeric_transformer = Pipeline(steps=[\n            ('imputer', make_pipeline(SimpleImputer(strategy=impute_strategy))),\n            ('scaler', StandardScaler())])\n    # if log_x is \"True\" than log transform feature values\n    else:\n        numeric_transformer = Pipeline(steps=[\n            ('imputer', make_pipeline(SimpleImputer(strategy=impute_strategy))),\n            ('log_transform', FunctionTransformer(np.log1p)),\n            ('scaler', StandardScaler()),\n            ])\n    \n    # the categorical transformer gets all categorical data according to cat_cols\n    # first step: imputing missing values\n    # second step: one hot encoding all categoricals\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    # the column transformer creates one Pipeline for categorical and numerical data each\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n    \n    # return the whole pipeline for the classifier provided in the function call\n    if log_y == False:\n        return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n    # if log_y is \"True\" than use a TransformedTargetRegressor with log and inverse log functions for \"y\"\n    else:\n        transformed_classifier = TransformedTargetRegressor(regressor=classifier, \n            func=log_transform, inverse_func=inverse_log_transform)\n        return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', transformed_classifier)])\n\n\ndef scoreModels(df, target_name, sample_size=None, \n    impute_strategy=\"mean\", scoring_metric=\"r2\", log_x=False, log_y=False, verbose=True):\n\n    \"\"\"\n    This function yields error scores for a large variety of common regression classifiers on provided training data. \n\n    \"\"\"\n\n    \n    if sample_size is not None:\n        df = df.sample(sample_size)\n  \n    # retrieve X, y and separated columns names for numerical and categorical data\n    X, y, num_cols, cat_cols = prepareData(df, target_name)\n\n    scores = []\n\n    clf_names, classifiers = getClassifiers()\n    if verbose == True:\n        print(f\"Classifier             Metric ({scoring_metric})\")\n        print(\"-\"*30)\n    for clf_name, classifier in zip(clf_names, classifiers):\n        start_time = time.time()\n        \n        # create a pipeline for each classifier\n        clf = getPipeline(classifier, num_cols, cat_cols, impute_strategy, log_x, log_y)\n                \n        # crossvalidate classifiers on training data\n        cv_score = cross_val_score(clf, X, y, cv=3, scoring=scoring_metric)\n        \n        if verbose == True:\n            print(f\"{clf_name} {cv_score.mean(): .4f}  |  {(time.time() - start_time):.2f} secs\")\n        \n        scores.append([clf_name.strip(), cv_score.mean()])\n\n    scores = pd.DataFrame(scores, columns=[\"Classifier\", scoring_metric]).sort_values(scoring_metric, ascending=False)\n    \n    # just for good measure: add the mean of all scores to dataframe\n    scores.loc[len(scores) + 1, :] = [\"mean_all\", scores[scoring_metric].mean()]\n\n    return scores.reset_index(drop=True)\n    \n\n\ndef trainModels(df, target_name, \n    impute_strategy=\"mean\", log_x=False, log_y=False, verbose=True): \n\n    \"\"\"\n    This function trains a large variety of common regression classifiers on provided training data. \n    \n    \"\"\"\n\n    X, y, num_cols, cat_cols = prepareData(df, target_name)\n\n    pipelines = []\n\n    if verbose == True:\n        print(f\"Classifier            Training time\")\n        print(\"-\"*35)\n    \n    clf_names, classifiers = getClassifiers()\n    for clf_name, classifier in zip(clf_names, classifiers):\n        start_time = time.time()\n        clf = getPipeline(classifier, num_cols, cat_cols, impute_strategy, log_x, log_y)\n        clf.fit(X, y)\n        if verbose == True:\n            print(f\"{clf_name}     {(time.time() - start_time):.2f} secs\")\n        pipelines.append(clf)\n    \n    return pipelines\n\n\n\ndef predictFromModels(df_test, pipelines):\n\n    \"\"\"\n    This function makes predictions with a list of pipelines. \n    \n    \"\"\"\n    \n    X_test, _ , _, _ = prepareData(df_test, None)\n    predictions = []\n    \n    for pipeline in pipelines:\n        preds = pipeline.predict(X_test)\n        predictions.append(preds)\n        \n    df_predictions = pd.DataFrame(predictions).T\n    clf_names, _ = getClassifiers()\n    df_predictions.columns = [clf_name.strip() for clf_name in clf_names]\n\n    return df_predictions\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feed Data\")\nfeed = pd.read_csv(\"../input/froth-flotation/feed.csv\")\nfeed.shape\nfeed.head()\n#summary(feed)\n\nprint(\"Floatation Data\")\nfloatation = pd.read_csv(\"../input/froth-flotation/flotation.csv\")\nfloatation.shape\nfloatation.head()\n#summary(floatation)\n\nprint(\"Scoring Dataset\")\ntest = pd.read_csv(\"../input/froth-flotation/scoringdataset.csv\")\ntest.shape\ntest.head()\n#summary(test)\n\nprint(\"Submission File\")\nsubmission = pd.read_csv(\"../input/froth-flotation/SubmissionFormat.csv\")\nsubmission.shape\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = pd.merge(feed, floatation, on='date')\n\ndata.head()\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns\n\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.drop(['Unnamed: 0_x', 'X1_x', 'Unnamed: 0_y', 'X1_y'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Interpretability"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tabulate import tabulate\n\nscores = scoreModels(df, \"% Silica Concentrate\", sample_size=1000, verbose=False)\nprint()\nprint(tabulate(scores, showindex=False, floatfmt=\".3f\", headers=\"keys\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":4}