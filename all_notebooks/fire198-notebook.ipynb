{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Smit Patel\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#New Version with all of the data added to input\nimport os\nos.chdir('/kaggle/input/alldata/Project')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install Cython\n!pip install git+https://github.com/philferriere/cocoapi.git#egg=pycocotools^&subdirectory=PythonAPI","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('detr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DETR Imports\n!pwd\n\nimport argparse\nimport random\nimport cv2\nfrom pathlib import Path\nimport torch\nimport torchvision.transforms as T\nimport PIL.Image\nfrom models import build_model\nfrom main import get_args_parser\nimport sys\nimport json\nimport shutil\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer\n\nfrom models_lfw.face_recognition.model_small import create_model\nfrom models_lfw.face_recognition.align import AlignDlib\nfrom models_lfw.triplet_loss import TripletLossLayer\nfrom coml_preprocessor import ComlFaceDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##WE NEED TO ADD IN A DATSET FOR THE \nimport csv\nmapping = {}\nwith open('/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Face Photos Dataset/person_name_id_mapping.csv', newline='') as csvfile:\n     reader = csv.DictReader(csvfile)\n     for row in reader:\n         print(row[\"Name\"], int(row[\"ID\"]))\n         mapping[row[\"Name\"]] = int(row[\"ID\"]) # This stores Id in a key-value pair format similar to a java map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Configurations\nparser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = '/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Team 3/Project/models_lfw/checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args) \n\n# Buuld Model\nmodel, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)\n\n# Load Weights\noutput_dir = Path(args.output_dir)\nif args.resume:\n    # the model will download the weights and model state from the https link provided\n    if args.resume.startswith('https'):\n      checkpoint = torch.hub.load_state_dict_from_url(\n          args.resume, map_location='cpu', check_hash = True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n\n    # this load the weights and model state into the model\n    model.load_state_dict(checkpoint['model'], strict=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COCO classes\nCLASSES = [\n   'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n   'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n   'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef detect(im, model, transform):\n    # mean-std normalize the input image (batch-size: 1)\n    img = transform(im).unsqueeze(0)\n\n    assert img.shape[-2] <= 1600 and img.shape[-1] <=1600, 'demo model only supports images up to 1600 pixels on each side'\n\n    outputs = model(img)\n\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled\n\ndef plot_results(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        \n        text= f'Persons name'\n        #f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()\n\n# Stores coordinates of the bounding box\ndef bbox_dims(pil_img, prob, boxes, classes):\n    bbox_list = []\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        bbox_list.append([xmin, ymin, xmax, ymax])\n    return bbox_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_image = PIL.Image.open('/content/a.jpg')\n\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\n#resized_im = the_image.resize((round(the_image.size[0]*0.5), round(the_image.size[1]*0.5)))\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\nplot_results(the_image, scores, boxes, plot_classes)\n\n# Obtains bbox coordinates for each bbox and adds them to the list\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n#for bbox in bbox_dim_list:\n#  print(bbox)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom\nprint(cropped_images)\n\ni = 1\nfor bbox in bbox_dim_list:\n  plt.subplot(1, len(cropped_images), i)\n  plt.imshow(cropped_images[i - 1])\n  i += 1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LFW","metadata":{}},{"cell_type":"code","source":"# Input for anchor, positive, and negative images\nin_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\n# create the base model from model_small\nmodel_sm = create_model()\n\n# Output the embedding vectors from anchor, positive, and negative images\n# The model weights are shared (Triplet network)\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\n# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\n# Model that can be trained with anchor, positive, and negative images\nmodel = Model([in_a, in_p, in_n], triplet_loss_layer)\n\n# Load weights\nmodel.load_weights(\"/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Team 3/Project/ckpts(lfw)/epoch024_loss1.440.hdf5\")\n\nbase_model = model.layers[3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path, upper_limit):\n    metadata = []\n    count = 0\n    for i in sorted(os.listdir(path)):\n        if count == upper_limit: \n            break\n            \n        for f in sorted(os.listdir(os.path.join(path, i))):\n            if count == upper_limit: \n                break\n                \n            count += 1\n            # Check file extension. Allow only jpg/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n    \ndef align_image(img):\n        alignment = AlignDlib('models_lfw/landmarks.dat')\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = load_metadata('/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Team 3/Project/data/fire', 100)\nalignment = AlignDlib('models_lfw/landmarks.dat')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img / 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)#m.image_path().split('/')[11].replace('_', ' ')\n  # base_model.summary()\n  myEmbeddings[i] = base_model.predict(img)\n    # for displaying the progress of creating embeddings\n  if i%1 == 0: print(str(i) + \"\\n\", end=\" \")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = np.empty((metadata.shape[0], 128))\n\nfor i, m in enumerate(metadata):\n    img = load_image(m.image_path())\n    img = align_image(img)\n    img = img.astype('float32')\n    img = img / 255.0\n    img = np.expand_dims(img, axis=0)\n    person = m.image_path().split('/')[11].replace('_', ' ')\n   # base_model.summary()\n    embeddings[i] = base_model.predict(img)\n    \n    # for displaying the progress of creating embeddings\n    if i%1 == 0: print(str(i) + \" \" + person + \"\\n\", end=\" \")\n    if i == len(metadata)-1: print('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person = {}\ndef show_pair_m(myEmbeddings, embeddings, idMy, idE):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(myEmbeddings[idMy], embeddings[idE]):.2f}') \n\n    plt.subplot(1,2,1)\n    plt.imshow(align_image(np.asarray(cropped_images[idMy])))\n    plt.subplot(1,2,2)\n    plt.imshow(align_image(load_image(metadata[idE].image_path())))\n\n    if (idMy == 0):\n      person[idMy] = [metadata[idE].image_path().split('/')[11].replace('_', ' ')]\n    if (idMy != 0):\n      dist = distance(myEmbeddings[idMy], embeddings[idE])\n      if dist < distance(myEmbeddings[idMy - 1], embeddings[idE - 1]):\n        #p.append(metadata[idE].image_path().split('/')[11].replace('_', ' '))\n        personidMy = [metadata[idE].image_path().split('/')[11].replace('_', ' ')]\n\n    plt.show()\n    \n# show positive pair (change this to what you want)\n\nfor i in range(len(myEmbeddings)):\n  for b in range(len(embeddings)):\n    show_pair_m(myEmbeddings, embeddings, i, b)\n\nprint(person)\n\n#def show_pair_m(myEmbeddings, embeddings, idMy, idE):\n #   plt.figure(figsize=(4,2))\n  #  plt.suptitle(f'Distance = {distance(myEmbeddings[idMy], embeddings[idE]):.2f}') \n#\n #   plt.subplot(1,2,1)\n  #  plt.imshow(align_image(np.asarray(cropped_images[idMy])))\n   # plt.subplot(1,2,2)\n    #plt.imshow(align_image(load_image(metadata[idE].image_path())))\n\n    #plt.show()\n    \n# show positive pair (change this to what you want)\n#show_pair_m(myEmbeddings, embeddings, 10, 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain ID Numbers\nfor i in range(len(person)):\n  print(mapping[str(person[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results2(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        \n        \n        for i in range(len(person)):\n          text = str(person[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n        \n\n\n        #f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results2(the_image, scores, boxes, plot_classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"persons = []\npath = []\n\ndef show_pair(embeddings, id1,secondemb, id2):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(embeddings[id1], embeddings[id2]):.2f}')\n\n    #imagePath = str(metadata[id2].image_path()) # Stores ImagePath, idea is to use ID2 as the name of the person it could be\n    #path.append(imagePath)\n\n    plt.subplot(1,2,1)\n    plt.imshow(align_image(load_image(metadata[id1].image_path())))\n    plt.subplot(1,2,2)\n    \"person \" + str(id2)\n    plt.imshow(cropped_images[id2])\n    plt.imshow(align_image(load_image(metadata[id2].image_path())))\n\n    dist = distance(embeddings[id1], embeddings[id2])\n    if dist < .5:\n        persons.append(metadata[id1].image_path().split('/')[11].replace('_', ' '))\n        persons.append(metadata[id2].image_path().split('/')[11].replace('_', ' '))\n    plt.show()\n    \n# show positive pair (change this to what you want)\nshow_pair(embeddings, 3, 9)\n\n# show negative pair (change this to what you want)\n#show_pair(embeddings, 3, 9)\n\nprint(persons)\n#print(imagePath)\nprint(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass ComlFaceDataGenerator(keras.utils.Sequence):\n    def __init__(self, data_dir, batch_size, anchor_shape = (96,96), n_channels = 3, img_aug = False, face_align = True, shuffle = True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_aug = img_aug\n        self.anchor_shape = anchor_shape\n        self.n_channels = n_channels\n        self.img_aug = img_aug\n        self.shuffle = shuffle\n        self.face_align = face_align\n        self.alignment = AlignDlib('models_lfw/landmarks.dat')\n        \n        persons = os.listdir(data_dir)\n        img_exts = ('.png', '.jpg', '.jpeg')\n\n        # process pos and neg image pairs from data_dir\n        self.positive_pairs = []\n        self.negative_pairs = []\n        for i, person in enumerate(persons):\n            person_path = os.path.join(data_dir, person)\n            person_files = os.listdir(person_path)\n            # get all image files based on extension\n            image_files = [image_file for image_file in person_files if image_file.endswith(img_exts)]\n            \n            # randomly pick 2 images of same person as positive pair \n            if len(image_files) >= 2:\n                image_ids = np.arange(len(image_files))\n                random.shuffle(image_ids)\n                self.positive_pairs.append([(person,image_files[image_ids[0]]),\n                                            (person,image_files[image_ids[1]])])\n                \n                # randomly pick another image as negative pair\n                remaining_persons = persons.copy()\n                remaining_persons.pop(i)\n                while True:\n                    other_person = random.choice(remaining_persons)\n                    other_person_path = os.path.join(data_dir, other_person)\n                    other_person_files = os.listdir(other_person_path)\n                    other_person_image_files = [image_file for image_file in other_person_files if image_file.endswith(img_exts)]\n                    if len(other_person_image_files) >= 1:\n                        random_id = random.randrange(len(other_person_image_files))\n                        self.negative_pairs.append([(person,image_files[image_ids[0]]),\n                                                    (other_person,other_person_image_files[random_id])])\n                        break\n            else:\n                continue\n\n        self.on_epoch_end()\n\n    # update indexs after each epoch\n    def on_epoch_end(self):\n        self.pos_indexes = np.arange(len(self.positive_pairs))\n        self.neg_indexes = np.arange(len(self.negative_pairs))\n        if self.shuffle == True:\n            np.random.shuffle(self.pos_indexes)\n            np.random.shuffle(self.neg_indexes)\n\n    # return the number of batches per epoch\n    def __len__(self):        \n        return int(np.floor(len(self.pos_indexes) / self.batch_size))\n\n    # return the path to the image\n    def get_image_path(self, person, image_file):\n        return os.path.join(self.data_dir, person, image_file)\n\n    # augment img as a batch\n    def augment_images(self, img_batch):\n        seq = iaa.Sequential([\n            iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n            iaa.Fliplr(0.5), # horizontally flip 50% of the images\n            iaa.GaussianBlur(sigma=(0, 1.0)),\n            # Strengthen or weaken the contrast in each image.\n            iaa.LinearContrast((0.75, 1.5)),# blur images with a sigma of 0 to 3.0\n        ])\n        return seq(images=img_batch)\n\n    def align_image(self, img):\n        alignment = self.alignment\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, self.anchor_shape)\n        else:\n            return alignment.align(self.anchor_shape[0],\n                                   img,\n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n    \n    # return the image as numpy tensor\n    def get_image_tensor(self, person, image_file):\n        # read and remove alpha channel\n        img = cv2.imread(self.get_image_path(person, image_file))[:,:,:3] \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.face_align:\n            img = self.align_image(img)\n        else:\n            img = cv2.resize(img, self.anchor_shape)\n        img = img / 255.0\n        img = img.astype(np.float32)\n        img = img[None, ...]\n        return img\n\n    # feed in an index and return a batch\n    def __getitem__(self, index):\n        # slice indexes for current batch\n        pos_indexes = self.pos_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        neg_indexes = self.neg_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        anchor_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        pos_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        neg_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n\n        pos_pairs_batch = [self.positive_pairs[j] for j in pos_indexes]\n        neg_pairs_batch = [self.negative_pairs[k] for k in neg_indexes]\n\n        for i, pos_pair in enumerate(pos_pairs_batch):\n            # process anchor image\n            anc_name, anc_id = pos_pair[0]\n            anchor_img_arr[i] = self.get_image_tensor(anc_name, anc_id)\n\n            # process postive image\n            pos_name, pos_id = pos_pair[1]\n            pos_img_arr[i] = self.get_image_tensor(pos_name, pos_id)\n            \n            # Process negative image\n            neg_pair = neg_pairs_batch[i]\n            neg_name, neg_id = neg_pair[0]\n            if pos_name == neg_name:\n                neg_name, neg_id = neg_pair[1]\n            neg_img_arr[i] = self.get_image_tensor(neg_name, neg_id)\n\n        if self.img_aug:\n            anchor_img_arr = self.augment_images(anchor_img_arr)\n            pos_img_arr = self.augment_images(pos_img_arr)\n            neg_img_arr = self.augment_images(neg_img_arr)\n\n        return anchor_img_arr, pos_img_arr, neg_img_arr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from coml_preprocessor import ComlFaceDataGenerator\ndata_dir = '/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Team 3/Project/data/fire'\nbatch_size = 5\ncoml_generator = ComlFaceDataGenerator(data_dir=data_dir,\n                                       batch_size=batch_size,\n                                       img_aug=False)\n\npos_pairs_embeddings = np.empty((len(coml_generator.positive_pairs), 2, 128))\npos_images = {}\n\nfor i, pair in enumerate(coml_generator.positive_pairs):\n    person, image_file = pair[0]\n    img = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 0] = base_model.predict(img)\n\n    person, image_file = pair[1]\n    img1 = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 1] = base_model.predict(img1)\n    \n    pos_images[i] = [img[0,:,:,:], img1[0,:,:,:]]\n\n    if i%1 == 0: print(i, end=\" \")\n    if i == len(coml_generator.positive_pairs)-1: print('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass ComlFaceDataGenerator(keras.utils.Sequence):\n    def __init__(self, data_dir, batch_size, anchor_shape = (96,96), n_channels = 3, img_aug = False, face_align = True, shuffle = True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_aug = img_aug\n        self.anchor_shape = anchor_shape\n        self.n_channels = n_channels\n        self.img_aug = img_aug\n        self.shuffle = shuffle\n        self.face_align = face_align\n        self.alignment = AlignDlib('models_lfw/landmarks.dat')\n        \n        persons = os.listdir(data_dir)\n        img_exts = ('.png', '.jpg', '.jpeg')\n\n        # process pos and neg image pairs from data_dir\n        self.positive_pairs = []\n        self.negative_pairs = []\n        for i, person in enumerate(persons):\n            person_path = os.path.join(data_dir, person)\n            person_files = os.listdir(person_path)\n            # get all image files based on extension\n            image_files = [image_file for image_file in person_files if image_file.endswith(img_exts)]\n            \n            # randomly pick 2 images of same person as positive pair \n            if len(image_files) >= 2:\n                image_ids = np.arange(len(image_files))\n                random.shuffle(image_ids)\n                self.positive_pairs.append([(person,image_files[image_ids[0]]),\n                                            (person,image_files[image_ids[1]])])\n                \n                # randomly pick another image as negative pair\n                remaining_persons = persons.copy()\n                remaining_persons.pop(i)\n                while True:\n                    other_person = random.choice(remaining_persons)\n                    other_person_path = os.path.join(data_dir, other_person)\n                    other_person_files = os.listdir(other_person_path)\n                    other_person_image_files = [image_file for image_file in other_person_files if image_file.endswith(img_exts)]\n                    if len(other_person_image_files) >= 1:\n                        random_id = random.randrange(len(other_person_image_files))\n                        self.negative_pairs.append([(person,image_files[image_ids[0]]),\n                                                    (other_person,other_person_image_files[random_id])])\n                        break\n            else:\n                continue\n\n        self.on_epoch_end()\n\n    # update indexs after each epoch\n    def on_epoch_end(self):\n        self.pos_indexes = np.arange(len(self.positive_pairs))\n        self.neg_indexes = np.arange(len(self.negative_pairs))\n        if self.shuffle == True:\n            np.random.shuffle(self.pos_indexes)\n            np.random.shuffle(self.neg_indexes)\n\n    # return the number of batches per epoch\n    def __len__(self):        \n        return int(np.floor(len(self.pos_indexes) / self.batch_size))\n\n    # return the path to the image\n    def get_image_path(self, person, image_file):\n        return os.path.join(self.data_dir, person, image_file)\n\n    # augment img as a batch\n    def augment_images(self, img_batch):\n        seq = iaa.Sequential([\n            iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n            iaa.Fliplr(0.5), # horizontally flip 50% of the images\n            iaa.GaussianBlur(sigma=(0, 1.0)),\n            # Strengthen or weaken the contrast in each image.\n            iaa.LinearContrast((0.75, 1.5)),# blur images with a sigma of 0 to 3.0\n        ])\n        return seq(images=img_batch)\n\n    def align_image(self, img):\n        alignment = self.alignment\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, self.anchor_shape)\n        else:\n            return alignment.align(self.anchor_shape[0],\n                                   img,\n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n    \n    # return the image as numpy tensor\n    def get_image_tensor(self, person, image_file):\n        # read and remove alpha channel\n        img = cv2.imread(self.get_image_path(person, image_file))[:,:,:3] \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.face_align:\n            img = self.align_image(img)\n        else:\n            img = cv2.resize(img, self.anchor_shape)\n        img = img / 255.0\n        img = img.astype(np.float32)\n        img = img[None, ...]\n        return img\n\n    # feed in an index and return a batch\n    def __getitem__(self, index):\n        # slice indexes for current batch\n        pos_indexes = self.pos_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        neg_indexes = self.neg_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        anchor_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        pos_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        neg_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n\n        pos_pairs_batch = [self.positive_pairs[j] for j in pos_indexes]\n        neg_pairs_batch = [self.negative_pairs[k] for k in neg_indexes]\n\n        for i, pos_pair in enumerate(pos_pairs_batch):\n            # process anchor image\n            anc_name, anc_id = pos_pair[0]\n            anchor_img_arr[i] = self.get_image_tensor(anc_name, anc_id)\n\n            # process postive image\n            pos_name, pos_id = pos_pair[1]\n            pos_img_arr[i] = self.get_image_tensor(pos_name, pos_id)\n            \n            # Process negative image\n            neg_pair = neg_pairs_batch[i]\n            neg_name, neg_id = neg_pair[0]\n            if pos_name == neg_name:\n                neg_name, neg_id = neg_pair[1]\n            neg_img_arr[i] = self.get_image_tensor(neg_name, neg_id)\n\n        if self.img_aug:\n            anchor_img_arr = self.augment_images(anchor_img_arr)\n            pos_img_arr = self.augment_images(pos_img_arr)\n            neg_img_arr = self.augment_images(neg_img_arr)\n\n        return anchor_img_arr, pos_img_arr, neg_img_arr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from coml_preprocessor import ComlFaceDataGenerator\ndata_dir = '/content/drive/Shareddrives/2021 FIRE-COML-STUDENTS/Spring/Facial Recognition Team Project/Team 3/Project/data/fire'\nbatch_size = 5\ncoml_generator = ComlFaceDataGenerator(data_dir=data_dir,\n                                       batch_size=batch_size,\n                                       img_aug=False)\n\npos_pairs_embeddings = np.empty((len(coml_generator.positive_pairs), 2, 128))\npos_images = {}\n\nfor i, pair in enumerate(coml_generator.positive_pairs):\n    person, image_file = pair[0]\n    img = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 0] = base_model.predict(img)\n\n    person, image_file = pair[1]\n    img1 = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 1] = base_model.predict(img1)\n    \n    pos_images[i] = [img[0,:,:,:], img1[0,:,:,:]]\n\n    if i%1 == 0: print(i, end=\" \")\n    if i == len(coml_generator.positive_pairs)-1: print('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef show_pair_dist(pair_embeddings, images, id):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(pair_embeddings[id, 0], pair_embeddings[id, 1]):.2f}')\n    plt.subplot(121)\n    plt.imshow(images[id][0])\n    plt.subplot(122)\n    plt.imshow(images[id][1])\n    plt.show()\nshow_pair_dist(pos_pairs_embeddings, pos_images, 2)","metadata":{},"execution_count":null,"outputs":[]}]}