{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\n\nfrom skopt import BayesSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sqlalchemy import Table, Column, Float, Integer, BigInteger\nimport missingno as msno\n\nfrom sklearn.feature_selection import SelectFromModel\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Null value check\n#### missingno.matrix-there is a bar on the right side of this diagram. This is a line plot for each row's data completeness.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nmsno.matrix(df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### missingno.heatmap visualizes the correlation matrix about the locations of missing values in columns.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4,4))\nmsno.heatmap(df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above graph we can clearly see that no null value is present in the data","metadata":{}},{"cell_type":"markdown","source":"## Info about the type of data type for feature present in the dataset ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above analysis we can clearly see the only int and float value is these\n","metadata":{}},{"cell_type":"markdown","source":"## check data is balanced or imbalanced?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(df['Bankrupt?'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can clearly see that data in imbalanced. To balance the data we have to do upsampling or downsampling technique","metadata":{}},{"cell_type":"markdown","source":"### Corelation Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(17,17))\nsns.heatmap(df.corr(), annot=False, cmap='coolwarm')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that the some feature are highly corelated to each other. we will eleminate it with feature elemination technique","metadata":{}},{"cell_type":"markdown","source":"### Seperating dependent and independent features","metadata":{}},{"cell_type":"code","source":"y = df['Bankrupt?']\nX = df.drop('Bankrupt?', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data is imbalanced , so to balance it we will use balancing technique ..Here we are using SMOT ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nover = SMOTE()\nX, y = over.fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spliting of data into train and test ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=97, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standarization","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns, index=X_train.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature elemination - using L1 Regularization","metadata":{}},{"cell_type":"code","source":"sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver='liblinear'))\nsel_.fit(X_train_sc, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_.get_support()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_.estimator_.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat = X_train.columns[(sel_.get_support())]\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n      np.sum(sel_.estimator_.coef_ == 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(sel_.estimator_.coef_ == 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\nremoved_feats = removed_feats.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sc.drop(removed_feats, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sc.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_sc = sc.transform(X_test)\nX_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns, index=X_test.index)\nX_test_sc.drop(removed_feats, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_sc.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA Visualization","metadata":{}},{"cell_type":"code","source":"pc = PCA(n_components=len(X_train_sc.columns))\nX_train_pc=pc.fit_transform(X_train_sc)\nPC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PC_df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scree Plot - PCA Analysis\nIn multivariate statistics, a scree plot is a line plot of the eigenvalues of factors or principal components in an analysis. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis (FA) or principal components to keep in a principal component analysis (PCA)\n\nTo select number of principal components elbow method is used\nWe can clearly, proper elbow is not formed in the below graph, so we can select all the components","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(PC_df_train.std())\nplt.title('Scree Plot - PCA components')\nplt.xlabel('Principal Component')\nplt.xticks(rotation=90)\nplt.ylabel('Standard deviation')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that it is forming an elbow at PC_15, So we can take 10 principal components for further analysis","metadata":{}},{"cell_type":"code","source":"pc = PCA(n_components=15)\nX_train_pc=pc.fit_transform(X_train_sc)\nPC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_pc = pc.transform(X_test_sc)\nPC_df_test=pd.DataFrame(X_test_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"print(PC_df_train.shape)\ny_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"classifier = LogisticRegression()\nclassifier.fit(PC_df_train,y_train)\ny_lr=classifier.predict(X_test_pc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Confusion Matrix \\n',confusion_matrix(y_lr,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_lr,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_lr,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVC Classifier","metadata":{}},{"cell_type":"code","source":"classifier = SVC()\nclassifier.fit(pc.fit_transform(X_train_sc),y_train)\ny_svc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_svc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_svc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_svc,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random forest Classifier","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_pc,y_train)\ny_rfc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_rfc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_rfc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_rfc,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boosting classifier","metadata":{}},{"cell_type":"code","source":"classifier = GradientBoostingClassifier()\nclassifier.fit(X_train_pc,y_train)\ny_gbc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_gbc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_gbc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_gbc,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = GaussianNB()\nclassifier.fit(X_train_pc,y_train)\ny_gb=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_gb,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_gb,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_gb,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGB Classifier","metadata":{}},{"cell_type":"code","source":"classifier = XGBClassifier()\nclassifier.fit(X_train_pc, y_train)\ny_xg=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_xg,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_xg,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_xg,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_df = pd.DataFrame(data=[f1_score(y_test,y_lr),accuracy_score(y_test, y_lr), recall_score(y_test, y_lr), precision_score(y_test, y_lr), roc_auc_score(y_test, y_lr)], \n             columns=['Logistic Regression'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nrf_df = pd.DataFrame(data=[f1_score(y_test,y_rfc),accuracy_score(y_test, y_rfc), recall_score(y_test, y_rfc),precision_score(y_test, y_rfc), roc_auc_score(y_test, y_rfc)], \n             columns=['Random Forest Score'],index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nnb_df = pd.DataFrame(data=[f1_score(y_test,y_gb),accuracy_score(y_test, y_gb), recall_score(y_test, y_gb), precision_score(y_test, y_gb), roc_auc_score(y_test, y_gb)], \n             columns=['Naive Bayes'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nxg_df = pd.DataFrame(data=[f1_score(y_test,y_xg),accuracy_score(y_test, y_xg), recall_score(y_test, y_xg), precision_score(y_test, y_xg), roc_auc_score(y_test, y_xg)], \n             columns=['XG Boost'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\ngbc_df = pd.DataFrame(data=[f1_score(y_test,y_gbc),accuracy_score(y_test, y_gbc), recall_score(y_test, y_gbc), precision_score(y_test, y_gbc), roc_auc_score(y_test,y_gbc)], \n             columns=['Gradient Boosting'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nsvc_df = pd.DataFrame(data=[f1_score(y_test,y_svc),accuracy_score(y_test, y_svc), recall_score(y_test, y_svc), precision_score(y_test, y_svc), roc_auc_score(y_test,y_svc)], \n             columns=['Gradient Boosting'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\n\ndf_models = round(pd.concat([lr_df,rf_df,nb_df,gbc_df,xg_df,svc_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n### From the above model analysis we can clearly see Random Forest and XG Boost is giving accuracy of 96%","metadata":{}}]}