{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Netflix Movies and TV \n\nthis is a fork of notebook by Jonathan K [https://www.kaggle.com/onyonixch/netflix-movies-tv-shows-eda-and-clustering](https://www.kaggle.com/onyonixch/netflix-movies-tv-shows-eda-and-clustering)\n\nthe work is his. made public to share some changes to help display the images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is a notbook to solve the tasks given with this dataset (see below).\nI want to try to build a DEC [(Deep Embedded Cluster)](https://arxiv.org/abs/1511.06335) with the given movie / tv show descriptions to find similar movies / tv shows as a recommender system.\nThe model will consist of an [autoencoder](https://en.wikipedia.org/wiki/Autoencoder) / decoder part and a clustering layer part.\n\nLater I want to combine this netflix dataset with the imdb dataset to compare the scores of the movies.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Dataset description:***\n\nTV Shows and Movies listed on Netflix\n\n    This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n\n    In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. \n    The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. \n    It will be interesting to explore what all other insights can be obtained from the same dataset.\n\n    Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Sources**\n\nThe clustering section of the notebook is base of one of my other notebooks [here](https://www.kaggle.com/onyonixch/covid-19-deep-embedded-literature-clustering)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tasks:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n1. **What to watch on Netflix ?**\n    *Find similar movies / tv shows using text similarity techniques*\n\n    Netflix is known for its strong recommendation engines. They use a mix of content-based and collaborative filtering models to recommend tv shows and movies. In this task, one can create a recommendation engine based on text/description similarity techniques.\n    \n    \n2. **Show me the Ratings**\n   --> **Which shows are the best?**\n   \n    *Task Details*:\n        Before watching something on Netflix, I always check shows IMDB score. \n        Here Kaggler's are expected to merge this dataset with IMDB scores of them and visualize top shows. \n        Other insights would also be interesting such as country of origin of the shows, their ratings etc.\n\n    *Expected Submission*:\n        A notebook with code and relevant visualizations.\n    \n\n   *Evaluation*:\n       A good solution would be a creative kernel that shows ratings of the shows on Netflix. \n       If you want to go beyond, you can even make a dashboard on it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Approche in a nutshell:**\n\n1. General dataset exploration\n2. Basic data visualisation\n3. Tokenize text data\n4. Build autencoder and clustering layer\n5. Visualize the cluster with Seaborn\n6. Interactive scatterplot with Bokeh\n\nCurrently try to improve the clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load and ckeck the data:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"INPUT_DIR_NETFLIX = \"../input/netflix-shows/netflix_titles.csv\"\n\ndf_netflix_raw = pd.read_csv(INPUT_DIR_NETFLIX,)\ndf_netflix = df_netflix_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#get a sample of the imported data.\ndf_netflix_raw.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now some basic information about the dataset:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Some dataset properties:\")\nprint()\nprint(f\"1.) The shape of the dataset is {df_netflix_raw.shape}, {df_netflix_raw.shape[0]} rows and {df_netflix_raw.shape[1]} columns\")\nprint(\"-\" * 80)\nprint(\"2.) The dataset columns contain the following datatyps:\")\nprint()\nprint(df_netflix_raw.dtypes)\nprint(\"-\" * 80)\nprint(\"3.) Nan cells in the dataset:\")\nprint()\nprint(df_netflix_raw.isna().sum())\nprint(\"-\" * 80)\nprint(\"4.) Check if there are duplicat titles in the dataset and remove the duplicats:\")\n\ndf_netflix = df_netflix_raw.drop_duplicates(\"title\")\n\nprint()\nprint(f\"{df_netflix_raw.shape[0] - df_netflix.shape[0]} rows of duplicat titles have been removed\")\nprint(\"-\" * 80)\nprint(\"5.) Count number of unique genres:\")\nprint()\ngenres = pd.Series(\", \".join(df_netflix.copy().fillna(\"\")['listed_in']).split(\", \")).unique().sum()\n\nprint(f\"There are {len(genres)} unique categorys / genres in this dataset\")\nprint(\"-\" * 80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Some data is missing, but fortunately all titles and descriptions are there so we have all the data for the clustering.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# General data visualisation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some basic plots about the dataset:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from collections import Counter\n\n#Define displayed years:\nyears = list(range(2008,2020,1))\n\n#separate movies and tv_shows:\nmovie_rows = df_netflix.loc[df_netflix[\"type\"] == \"Movie\"]\ntv_rows = df_netflix.loc[df_netflix[\"type\"] == \"TV Show\"]\n\n#Count movies / tv shows per year\nmovies_counts = movie_rows.release_year.value_counts()\ntv_counts = tv_rows.release_year.value_counts()\n\nindex_years_mov = movies_counts.index.isin(years)\nindex_years_tv = tv_counts.index.isin(years)\n\n#select movies / tv shows between chosen years:\nmovies = movies_counts[index_years_mov]\ntv_shows = tv_counts[index_years_tv]\n\n# Calculate percentages of movies and tv shows:\nmovies_per = round(movie_rows.shape[0] / df_netflix[\"type\"].shape[0] * 100, 2)\ntvshows_per = round(tv_rows.shape[0] / df_netflix[\"type\"].shape[0] * 100, 2)\n\n#Top Movie and TV Show producer country:\ntop5_producer_countrys = df_netflix.country.value_counts().sort_values(ascending=False).head(5)\n\n#Top most commen Actors an directors (Movies and tv shows):\ncasts = \", \".join(df_netflix.copy().fillna(\"\")['cast']).split(\", \")\ncounter_list = Counter(casts).most_common(5)\nmost_commen_actors = [i for i in counter_list if i[0] != \"\"]\nlabels = [i[0] for i in most_commen_actors][::-1]\nvalues = [i[1] for i in most_commen_actors][::-1]\n\nmost_commen_directors = df_netflix.director.value_counts().head(5).sort_values(ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwidth = 0.75\n\nsns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n\ndef autolabel(rects, axes):\n    \"\"\"Helper function to attach a text label above each bar in *rects*, displaying its height.\n        Add specific axes[x, y] for subplot labeling\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        axes.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\n# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(12, 12), sharex=False)\n\n#Line plot of Movies and TV Shows released by Netflix per year\")\nsns.lineplot(data=movies, color=\"b\", ax=axes[0, 0], label=\"Movies / year\")\nsns.lineplot(data=tv_shows, color=\"c\", ax=axes[0, 0], label=\"TV Shows / year\")\n\n# Pie chart of type percentages\naxes[0, 1].pie([movies_per, tvshows_per], explode=(0, 0.1,), labels=[\"Movies\", \"TV Shows\"], autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Bar chart of top 5 Movie / Tv shows producer countrys:\nrects1 = axes[1, 0].bar(top5_producer_countrys.index, top5_producer_countrys.values,)\n\nautolabel(rects1, axes[1, 0])\n\n#Bar chart of top 5 most commen actors and directors:\nrects2 = axes[1, 1].bar(labels, values, width, label='Actors',)\n\nrects3 = axes[1, 1].bar(most_commen_directors.index, most_commen_directors.values, width, label='Directors')\n\nautolabel(rects2, axes[1, 1])\nautolabel(rects3, axes[1, 1])\n\naxes[0, 0].set_ylabel('Publications')\naxes[0, 0].set_title('Movies / Tv Shows relesed per year')\n\naxes[0, 1].set_title('Percentage of Movies and Tv Shows')\n\naxes[1, 0].set_ylabel('Movies and Tv Shows')\naxes[1, 0].set_title('Top 5 producer countrys')\naxes[1, 0].legend()\n\naxes[1, 1].set_ylabel('Number Occurring')\naxes[1, 1].set_xticklabels(labels + list(most_commen_directors.index), rotation=\"vertical\")\naxes[1, 1].set_title('Top 5 most commen actors and directors')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.savefig('output.png')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our goal is to finde similar movies and tv shows. A simple way to do this is to group the movies / series by their directora and categorys.\nWe can use this simple method to check if the cluster is good.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"m_s_groups = df_netflix.groupby([\"title\", \"director\", \"listed_in\",]).apply(lambda df: df.title) #Returns Pandas Series with movie / series title and original index\nm_s_groups.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start Clustering Section: Imports and text tokenizing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we start to creat the clusters to find similar movies and tv shows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom time import time\nimport keras.backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras.layers import Dense, Input, Embedding\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras import callbacks\nfrom keras.initializers import VarianceScaling\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_token = df_netflix[ \"description\"]\n#df_token = df_netflix[[\"listed_in\", \"description\"]].values.tolist()\n\nmaxlen = 1500 #only use this number of most frequent words\ntraining_samples = 800\nvalidation_samples = 450\nmax_words = 10000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df_token) # generates word index\nsequences = tokenizer.texts_to_sequences(df_token) # transforms strings in list of intergers\nword_index = tokenizer.word_index # calculated word index\nprint(f\"{len(word_index)} unique tokens found\")\n\ndata = pad_sequences(sequences, maxlen=maxlen) #transforms integer lists into 2D tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler() \nx = scaler.fit_transform(data) # the values of all features are rescaled into the range of [0, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define autoencoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def autoencoder(dims, act='relu', init='glorot_uniform'):\n    \"\"\"\n    Fully connected symmetric auto-encoder model.\n  \n    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n\n    act: activation function\n    \n    return:\n        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n    \"\"\"\n    n_stacks = len(dims) - 1\n    \n    input_data = Input(shape=(dims[0],), name='input')\n    x = input_data\n    \n    # internal layers of encoder\n    for i in range(n_stacks-1):\n        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n\n    # latent hidden layer\n    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n\n    x = encoded\n    # internal layers of decoder\n    for i in range(n_stacks-1, 0, -1):\n        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n\n    # decoder output\n    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n    \n    decoded = x\n    \n    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n    \n    return autoencoder_model, encoder_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 20 # max numbers of clusters\nn_epochs   = 8 # epchos for autencoder training\nbatch_size = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dims = [x.shape[-1], 500, 500, 1000, 10] \ninit = VarianceScaling(scale=1. / 3., mode='fan_in',\n                           distribution='uniform')\npretrain_optimizer = \"rmsprop\" #SGD(lr=1, momentum=0.9)\npretrain_epochs = n_epochs\nbatch_size = batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dims","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Clustering Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClusteringLayer(Layer):\n    '''\n    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n    '''\n\n    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(ClusteringLayer, self).__init__(**kwargs)\n        self.n_clusters = n_clusters\n        self.alpha = alpha\n        self.initial_weights = weights\n        self.input_spec = InputSpec(ndim=2)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 2\n        input_dim = input_shape[1]\n        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n        \n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        ''' \n        student t-distribution, as used in t-SNE algorithm.\n        It measures the similarity between embedded point z_i and centroid µ_j.\n                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n                 (i.e., a soft assignment)\n       \n        inputs: the variable containing data, shape=(n_samples, n_features)\n        \n        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n        '''\n        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n        q **= (self.alpha + 1.0) / 2.0\n        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n        \n        return q\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) == 2\n        return input_shape[0], self.n_clusters\n\n    def get_config(self):\n        config = {'n_clusters': self.n_clusters}\n        base_config = super(ClusteringLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder, encoder = autoencoder(dims, init=init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='autoencoder.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(encoder, to_file='encoder.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='encoder.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.compile(optimizer=pretrain_optimizer, loss='binary_crossentropy')  #loss='mse'\nautoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)\n#autoencoder.save_weights(save_dir + '/ae_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\nmodel = Model(inputs=encoder.input, outputs=clustering_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(0.01, 0.9), loss='kld') #(optimizer=SGD(0.01, 0.9), loss='kld')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=n_clusters, n_init=20)\ny_pred = kmeans.fit_predict(encoder.predict(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_last = np.copy(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing an auxiliary target distribution\ndef target_distribution(q):\n    weight = q ** 2 / q.sum(0)\n    return (weight.T / weight.sum(1)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = 0\nindex = 0\nmaxiter = 1000 # 8000\nupdate_interval = 100 # 140\nindex_array = np.arange(x.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 0.001 # tolerance threshold to stop training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ite in range(int(maxiter)):\n    if ite % update_interval == 0:\n        q = model.predict(x, verbose=0)\n        p = target_distribution(q)  # update the auxiliary target distribution p\n\n    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n    loss = model.train_on_batch(x=x[idx], y=p[idx])\n    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n\n#model.save_weights(save_dir + '/DEC_model_final.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Eval.\nq = model.predict(x, verbose=0)\np = target_distribution(q)  # update the auxiliary target distribution p\n\n# evaluate the clustering performance\ny_pred = q.argmax(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = df_netflix.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add the cluster prediction to your dataframe:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_all['cluster'] = y_pred\ndata_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\nx_embedded = TSNE(n_components=2).fit_transform(x)\n\nx_embedded.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seaborn scatterplot of the cluster","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"Netflix Movies and Tv Shows, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\")\n# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interactive scatterplot of the cluster","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\n\noutput_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= x_embedded[:,0], \n    y= x_embedded[:,1],\n    x_backup = x_embedded[:,0],\n    y_backup = x_embedded[:,1],\n    desc= y_labels, \n    titles= df_netflix['title'],\n    directors = df_netflix['director'],\n    cast = df_netflix['cast'],\n    description = df_netflix['description'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles\"),\n    (\"Director(s)\", \"@directors\"),\n    (\"Cast\", \"@cast\"),\n    (\"Description\", \"@description\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Netflix Movies and Tv Shows, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"All\"], \n                          active=20)\n\n# search box\n#keyword = TextInput(title=\"Search:\", callback=keyword_callback)\n#header\nheader = Div(text=\"\"\"<h1>Find similar movies / tv shows in corresponding Cluster</h1>\"\"\")\n\n# show\nshow(column(header,p))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the first look the cluster looks pretty good but not good enought for my opinion. I try to improve the cluster by chaning the hyperparamters of the NN.\n\n**I will update the kernel in the next days**\n\nI would really like to hear your feedback","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}