{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Australia's Beer Production Forecast </center>\nThe goal is to <i> provide a forecast of monthly Australian beer production for the year 1996 using only the https://www.kaggle.com/sergiomora823/monthly-beer-production data with a verbal summarization of the forecast and a comment on what was done, why, and how the final forecast was made. </i> <br><br>\nBut first, missing libraries (Pytorch Forecasting, Pystand and Prophet) have to be installed and all the necessary modules loaded.","metadata":{}},{"cell_type":"code","source":"%%capture\n# Installing missing libraries\n!pip install pytorch_forecasting\n!pip install pystan==2.19.1.1\n!pip install prophet\n\n# Loading modules\nimport pandas as pd\nimport numpy as np\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\nimport torch\n\nfrom pytorch_forecasting import Baseline, NBeats, TimeSeriesDataSet\nfrom pytorch_forecasting.data import NaNLabelEncoder\nfrom pytorch_forecasting.data.examples import generate_ar_data\nfrom pytorch_forecasting.metrics import SMAPE\n\nfrom prophet import Prophet\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nfrom IPython.core.display import display, HTML\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-11T05:44:15.452285Z","iopub.execute_input":"2021-06-11T05:44:15.452632Z","iopub.status.idle":"2021-06-11T05:44:35.275788Z","shell.execute_reply.started":"2021-06-11T05:44:15.452603Z","shell.execute_reply":"2021-06-11T05:44:35.274787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <center> Data Loading and Exploration </center>\nFirst it is crucial to inspect the data and investigate whether:\n1. There are no missing data, cleary wrong data or extreme outliers\n2. The data are in a format compatible with statistical or ML packages\n3. The data are or are not stationary\n4. There is or is not a seasonality in the data\n5. There is or is not not a trend in the data\n6. The data has to be transformed \n7. What is the overall nature of data (data types, number of variables etc.)","metadata":{}},{"cell_type":"code","source":"# Loading data\ndata = pd.read_csv(\"/kaggle/input/monthly-beer-production/datasets_56102_107707_monthly-beer-production-in-austr.csv\")\n\ndef display_side_by_side(dfs:list, captions:list):\n    \"\"\"Display tables side by side to save vertical space\n    Input:\n        dfs: list of pandas.DataFrame\n        captions: list of table captions\n    \"\"\"\n    output = \"\"\n    combined = dict(zip(captions, dfs))\n    for caption, df in combined.items():\n        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n        output += \"\\xa0\\xa0\\xa0\"\n    display(HTML(output))\n    \n# Displaying head, tail and descriptive statistics of data\ndisplay_side_by_side([data.head(), data.tail(), data.describe()], ['<b>First 5 rows</b>', '<b>Last 5 rows</b>', '<b>Descriptive Statistics</b>'])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:44:56.555982Z","iopub.execute_input":"2021-06-11T05:44:56.556579Z","iopub.status.idle":"2021-06-11T05:44:56.586503Z","shell.execute_reply.started":"2021-06-11T05:44:56.556522Z","shell.execute_reply":"2021-06-11T05:44:56.58574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing number of missing values\nprint(f\"Missing data: {data.isnull().sum().sum()}\",\"\\n\")\n\n# Rows and columns\nprint(f\"Data shape: {data.shape}\",\"\\n\")\n\n# Column names\nprint(data.columns,\"\\n\")\n\n# Data types\nprint(data.dtypes)\n\n# Data info\n# print(data.info())","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:44:58.70085Z","iopub.execute_input":"2021-06-11T05:44:58.701244Z","iopub.status.idle":"2021-06-11T05:44:58.713624Z","shell.execute_reply.started":"2021-06-11T05:44:58.701208Z","shell.execute_reply":"2021-06-11T05:44:58.712516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe has 476 rows, one per month ranging from 1956-01 to 1995-08, and only two colums, date and monthly beer production. There are no missing data observed in the dataset. Monthly beer production is a 64-bit float datatype which does not need changing. Date is in format \"Y%-%M\" and is currently stored as a string object, not a datetime object, and thus requires reformatting for further easier use.","metadata":{}},{"cell_type":"code","source":"# Data manipulation, setting month an index with datetime format and monthly frequency, renaming columns\ndata[\"Month\"] = pd.to_datetime(data[\"Month\"])\ndata.rename(columns={\"Month\":\"month\",\"Monthly beer production\":\"beer_prod\"}, inplace=True)\ndata.set_index('month', inplace=True)\ndata.index.freq = \"MS\"","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:45:00.817365Z","iopub.execute_input":"2021-06-11T05:45:00.817734Z","iopub.status.idle":"2021-06-11T05:45:00.834863Z","shell.execute_reply.started":"2021-06-11T05:45:00.8177Z","shell.execute_reply":"2021-06-11T05:45:00.833817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First data exploration plot\ndata_exploration_plot = go.Figure()\n\ndata_exploration_plot.add_trace(go.Scatter(x=data.index, y=data[\"beer_prod\"],\n                                mode='lines',\n                                name='Monthly Beer Production'))\n\ndata_exploration_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\ndata_exploration_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"Monthly Beer Production\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\ndata_exploration_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:45:03.209688Z","iopub.execute_input":"2021-06-11T05:45:03.210237Z","iopub.status.idle":"2021-06-11T05:45:03.394151Z","shell.execute_reply.started":"2021-06-11T05:45:03.210187Z","shell.execute_reply":"2021-06-11T05:45:03.393008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visual inspection of the data, several observations can be drawn: <br>\n1. There seems to be seasonality in the data on a yearly basis <br>\n    1.1 The production seems to be highest during Nov-Jan period and lowest during June-August period. This makes sense since it is reasonable to expect the production of beer to be higher during hot months (Australian's summer is during Nov-Jan) and lower during cold months (Australian winter)\n2. Data seem not to be stationary\n3. There seems to be a non-linear trend in the data\n4. The variance seems to be increasing with time <br>\n\nLet's investigate further using seasonal decomposing using moving averages!","metadata":{}},{"cell_type":"code","source":"# Running seasonal decompostion\nseason_decomp_result = seasonal_decompose(data[\"beer_prod\"], model=\"additive\")\n\n# Making seasonl decomposition plotly chart\nseason_trend_resid_plot = make_subplots(rows=4, cols=1)\n\nseason_trend_resid_plot.add_trace(go.Scatter(x=data.index, y=data[\"beer_prod\"],\n                                name=\"Raw Data\",\n                                mode='lines'),\n                                 row=1,\n                                 col=1)\n\nseason_trend_resid_plot.add_trace(go.Scatter(x=data.index, y=season_decomp_result.trend,\n                                name=\"Trend\",\n                                mode='lines'),\n                                 row=2,\n                                 col=1)\n\nseason_trend_resid_plot.add_trace(go.Scatter(x=data.index, y=season_decomp_result.seasonal,\n                                name=\"Seasonality\",\n                                mode='lines'),\n                                 row=3,\n                                 col=1)\n\n\nseason_trend_resid_plot.add_trace(go.Scatter(x=data.index, y=season_decomp_result.resid,\n                                name=\"Residuals\",\n                                mode='lines'),\n                                 row=4,\n                                 col=1)\n\nseason_trend_resid_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nseason_trend_resid_plot.update_layout(\n                                        plot_bgcolor = \"rgba(0,0,0,0)\",\n                                        autosize=True,\n                                        title={\n                                            'text': \"Seasonal Decomposition\",\n                                            'y':0.9,\n                                            'x':0.5,\n                                            'xanchor': 'center',\n                                            'yanchor': 'top'})\n\nseason_trend_resid_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:45:08.689748Z","iopub.execute_input":"2021-06-11T05:45:08.690133Z","iopub.status.idle":"2021-06-11T05:45:08.987282Z","shell.execute_reply.started":"2021-06-11T05:45:08.690093Z","shell.execute_reply":"2021-06-11T05:45:08.986548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The seasonal decomposition provides further evidence for the beforementioned observation as it clearly separates non-linear trend, seasonality and non-uniformly distributed residuals. Although it seems now very likely even from visual inspection that the data are not stationary, it is desirable to conduct Augmented Dickey-Fuller test to evaluate whether the time series does have a unit-root (it is not stationary) with some degree of confidence. Raw data as well as natural logarithm of raw data and Box Cox transformation of the raw data will be tested.","metadata":{}},{"cell_type":"code","source":"def test_stationarity(timeseries, cutoff = 0.05):\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries)\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)\n    print(\"\\n\")\n    \n# Log and boxcox transforms\ndata[\"beer_prod_log\"] = np.log(data[\"beer_prod\"]) # log transform\ndata[\"beer_prod_box_cox\"], lam = boxcox(data[\"beer_prod\"]) # lam stores lambda param for inverse boxcox transformation    \n    \n# Train-test split with 6 years of test data\ndata_train = data.iloc[:-72]\ndata_test = data.iloc[-72:]\n    \n# Testing stationarity\nprint(\"Raw Data - Beer production\")\ntest_stationarity(data[\"beer_prod\"])\nprint(\"Natural Log Transformed Data - Beer production\")\ntest_stationarity(data[\"beer_prod_log\"])\nprint(\"Box Cox Transformed Data - Beer production\")\ntest_stationarity(data[\"beer_prod_box_cox\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:45:12.853779Z","iopub.execute_input":"2021-06-11T05:45:12.854146Z","iopub.status.idle":"2021-06-11T05:45:13.003322Z","shell.execute_reply.started":"2021-06-11T05:45:12.854115Z","shell.execute_reply":"2021-06-11T05:45:13.002186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the results of Dickey-Fuller tests we can draw a conclusion that at the 95% confidence level neither of the transformations of beer production monthly data is stationary, although it seems like the natural log transformation have had the at least contributed to making the data closer to stationarity. Further visual inspection of the data might provide us with more clues.","metadata":{}},{"cell_type":"code","source":"# Plotting all 3 transformations of target variable\ndata_exploration_trans_plot = make_subplots(specs=[[{\"secondary_y\": True}]])\n\ndata_exploration_trans_plot.add_trace(go.Scatter(x=data.index, y=data[\"beer_prod\"],\n                                mode='lines',\n                                name='Beer Production'),\n                                secondary_y=False)\n\ndata_exploration_trans_plot.add_trace(go.Scatter(x=data.index, y=data[\"beer_prod_log\"],\n                                mode='lines',\n                                name='Beer Production - Log '),\n                                secondary_y=True)\n\ndata_exploration_trans_plot.add_trace(go.Scatter(x=data.index, y=data[\"beer_prod_box_cox\"],\n                                mode='lines',\n                                name='Beer Production - BoxCox'),\n                                secondary_y=False)\n\ndata_exploration_trans_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\ndata_exploration_trans_plot.update_layout(\n                            plot_bgcolor = \"rgba(0,0,0,0)\",\n                            autosize=True,\n                            xaxis_title=\"Date\",\n                            yaxis_title=\"Beer Production\",\n                            title={\n                                'text': \"Monthly Beer Production\",\n                                'y':0.9,\n                                'x':0.5,\n                                'xanchor': 'center',\n                                'yanchor': 'top'})\n\ndata_exploration_trans_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:46:14.500825Z","iopub.execute_input":"2021-06-11T05:46:14.501197Z","iopub.status.idle":"2021-06-11T05:46:14.600165Z","shell.execute_reply.started":"2021-06-11T05:46:14.501166Z","shell.execute_reply":"2021-06-11T05:46:14.599305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the visual inspection of the chart, it seems that the log transformation of the data had an effect on reducing the unequal distribution of variance across the time series span. Although it is very common to use natural logarithm of time series when trying to model it, I will avoid this due to several reason:\n1. Even after log transformation, the data are not stationary and as (Lütkepohl & Xu, 2012) suggest, using log transforms of time series when not achieving stationarity might have negative impact on precision of the forecast. For more info see the article: Lütkepohl, H., Xu, F. 2012. <i>The role of the log transformation in forecasting economic variables.</i> Empir Econ 42, 619–638 . https://doi.org/10.1007/s00181-010-0440-1\n2. Several of the models used contain build-in means of making the data stationary (such as differencing) or provide other means of transforming the data.\n3. I have empirically tested whether log transformation have a positive impact on forecast precision performance of the models used and did not find any substantial positive effects (for the sake of efficiency I avoid duplicating results of all the analysis as it would almost double the range of an already lengthy notebook)","metadata":{}},{"cell_type":"markdown","source":"## <center> Searching for the best model </center>\n\nDue to the nature of data, four models will be tested and evaluated against each other. First two are common statistical models used for analyzing time series data with seasonality and trend, these are <b><i>Holt-Winters Exponential Smoothing</i></b> and <b><i>SARIMA (Seasonal Autoregressive Integrated Moving Average)</i></b>. <b><i>Facebook's Prophet </b></i> algorithm will also be fit to the data as it generally provides good performance on univeriate time series data with seasonal compontents. Moreover, <b><i>N-BEATS (Neural basis expansion analysis for interpretable time series forecasting)</i></b>, deep learning based model that achieved state-of-art results in several univariate time series forecasting competitions will be fit to the data.\n\n#### <b><center>Test set evaluation methodology</center></b>\nSince the <b>goal is to forecast data that are 16 months ahead</b> from the last observation with the highest possible precision, the models are evaluated based on their performance on 16 steps ahead forecast. For all models, 4 precision metrics are calculated and averaged across all 16 time points subsets of the test set predictions. All of the models except for N-BEATS are re-estimated after each forecast with +1 time point. That way, the ability to forecast 16 steps ahead is always based on the maximum available data, while keeping the train dataset for initial model selection and training. For N-BEATS, the neural network weights are estimated only once. All models, although very different in terms of complexity and statistical background, are compared on the same set of fit metrics. \nThese are:  <br>\n* Mean Squared Error (MSE)\n* Root Mean Squared Error (RMSE)\n* Mean Absolute Percentage Error (MAPE)\n* Symmetric absolute percentage Error (SMAPE)","metadata":{}},{"cell_type":"code","source":"# Defining functions for evaluations\ndef mape(y_true, y_pred): \n    # Mean absolute percentage error\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef smape(y_true, y_pred):\n    # Symmetric mean absolute percentage error\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return 200 * np.mean(diff)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:52:34.838637Z","iopub.execute_input":"2021-06-11T05:52:34.839007Z","iopub.status.idle":"2021-06-11T05:52:34.847173Z","shell.execute_reply.started":"2021-06-11T05:52:34.838973Z","shell.execute_reply":"2021-06-11T05:52:34.845614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <center> Holt-Winters Exponential Smoothing </center>\n\nFirst, Holt-Winters Exponential Smoothing model is implemented as it takes into account both trend and seasonality that are present in the current data. Both additive and multiplicative methods are calculated and their performance is compared.","metadata":{}},{"cell_type":"code","source":"%%capture\n# Additive\ntripple_additive_holt_winters = ExponentialSmoothing(data_train[\"beer_prod\"], trend=\"add\", seasonal=\"add\")\ntripple_additive_holt_winters_fitted = tripple_additive_holt_winters.fit()\n\n# Multiplicative\ntripple_multiplicative_holt_winters = ExponentialSmoothing(data_train[\"beer_prod\"], trend=\"mul\", seasonal=\"mul\")\ntripple_multiplicative_holt_winters_fitted = tripple_multiplicative_holt_winters.fit()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:52:36.775952Z","iopub.execute_input":"2021-06-11T05:52:36.776321Z","iopub.status.idle":"2021-06-11T05:52:37.358305Z","shell.execute_reply.started":"2021-06-11T05:52:36.77629Z","shell.execute_reply":"2021-06-11T05:52:37.357218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chart Holt-Winters Exponential Smoothing\nhw_plot = go.Figure()\n\nhw_plot.add_trace(go.Scatter(x=data_train.index, y=data_train[\"beer_prod\"].values,\n                                mode='lines',\n                                name='Monthly Beer Production'))\n\nhw_plot.add_trace(go.Scatter(x=data_train.index, y=tripple_additive_holt_winters_fitted.fittedvalues.values,\n                                mode='lines',\n                                name='Additive Trend + Seasonality'))\n\nhw_plot.add_trace(go.Scatter(x=data_train.index, y=tripple_multiplicative_holt_winters_fitted.fittedvalues.values,\n                                mode='lines',\n                                name='Multiplicative Trend + Seasonality'))\n\nhw_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nhw_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"Holt-Winters Exponential Smoothing Train Data Prediction\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\nhw_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:55:48.647779Z","iopub.execute_input":"2021-06-11T05:55:48.648149Z","iopub.status.idle":"2021-06-11T05:55:48.715633Z","shell.execute_reply.started":"2021-06-11T05:55:48.648118Z","shell.execute_reply":"2021-06-11T05:55:48.714766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the visualisation suggests, both additive and multiplicative models generally follow the pattern of the train data. Additive model tends to slightly overestimate both highs and lows at the beginning of time series. Bellow, I calculate the fit statistics for both models on the train data.","metadata":{}},{"cell_type":"code","source":"# Additive Train Metrics\nad_hwes_mse = mean_squared_error(y_true=data_train[\"beer_prod\"].values, \n                                 y_pred=tripple_additive_holt_winters_fitted.fittedvalues.values)\nad_hwes_rmse = mean_squared_error(y_true=data_train[\"beer_prod\"].values,\n                                  y_pred=tripple_additive_holt_winters_fitted.fittedvalues.values,\n                                  squared=False)\nad_hwes_mape = mape(y_true=data_train[\"beer_prod\"].values,\n                    y_pred=tripple_additive_holt_winters_fitted.fittedvalues.values)\nad_hwes_smape = smape(y_true=data_train[\"beer_prod\"].values,\n                      y_pred=tripple_additive_holt_winters_fitted.fittedvalues.values)\n\n# Multiplicative Train Metrics\nml_hwes_mse = mean_squared_error(y_true=data_train[\"beer_prod\"].values, \n                                 y_pred=tripple_multiplicative_holt_winters_fitted.fittedvalues.values)\nml_hwes_rmse = mean_squared_error(y_true=data_train[\"beer_prod\"].values,\n                                  y_pred=tripple_multiplicative_holt_winters_fitted.fittedvalues.values,\n                                  squared=False)\nml_hwes_mape = mape(y_true=data_train[\"beer_prod\"].values,\n                    y_pred=tripple_multiplicative_holt_winters_fitted.fittedvalues.values)\nml_hwes_smape = smape(y_true=data_train[\"beer_prod\"].values,\n                      y_pred=tripple_multiplicative_holt_winters_fitted.fittedvalues.values)\n\nprint(f\"Additive HWES Train Fit: MSE: {ad_hwes_mse:.3f}, RMSE: {ad_hwes_rmse:.3f}, MAPE: {ad_hwes_mape:.3f}, SMAPE: {ad_hwes_smape:.3f}\")\nprint(f\"Multiplicative HWES Train Fit: MSE: {ml_hwes_mse:.3f}, RMSE: {ml_hwes_rmse:.3f}, MAPE: {ml_hwes_mape:.3f}, SMAPE: {ml_hwes_smape:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:52:49.071594Z","iopub.execute_input":"2021-06-11T05:52:49.071943Z","iopub.status.idle":"2021-06-11T05:52:49.087286Z","shell.execute_reply.started":"2021-06-11T05:52:49.071912Z","shell.execute_reply":"2021-06-11T05:52:49.085897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the four test statistics, the multiplicative model seems to be able to model the train data slightly better, altough the difference is not substantial. However, it is important to evaluate how will they perform on the test dataset with 16 points-ahead forecast.","metadata":{}},{"cell_type":"code","source":"def iterate_hw_over_evals(data_train_hw, data_test_hw, target, method):\n    \"\"\"\n    Function iterates over the whole testing datasets\n        1) Makes out of sample prediction of length 16\n        2) Saves the model performance on the current iteration \n        3) Moves one time step ahead and appends one new datapoint to the train_data\n        4) Refits the model and repeats steps 1-3 until the end of the test dataset is reached\n        5) Calculates average MSE, RMSE, MAPE and SMAPE metrics\n        6) Optionally returns all predictions with corresponding true values for further inspection\n    \"\"\"\n    list_of_reals = []\n    list_of_forecasts = []\n    mse_list = []\n    rmse_list = []\n    mape_list = []\n    smape_list = []\n    \n    for x in range(len(data_test_hw)-16):\n        if x == 0:\n            holt_winters_model = ExponentialSmoothing(data_train_hw[target], trend=method, seasonal=method)\n            holt_winters_model_fit = holt_winters_model.fit()\n            holt_winters_forecast_values = holt_winters_model_fit.forecast(16).values\n            \n            list_of_reals.append(data_test[target].values[x:x+16])\n            list_of_forecasts.append(holt_winters_forecast_values)\n\n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n        else:\n            data_train_ar = data_train_hw.append(data_test_hw.iloc[x-1])\n            \n            holt_winters_model = ExponentialSmoothing(data_train_hw[target], trend=method, seasonal=method)\n            holt_winters_model_fit = holt_winters_model.fit()\n            holt_winters_forecast_values = holt_winters_model_fit.forecast(16).values\n            \n            list_of_reals.append(data_test_hw[target].values[x:x+16])\n            list_of_forecasts.append(holt_winters_forecast_values)\n            \n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n    avg_mse = np.array(mse_list).mean()\n    avg_rmse = np.array(rmse_list).mean()\n    avg_mape = np.array(mape_list).mean()\n    avg_smape = np.array(smape_list).mean()\n        \n    return list_of_reals, list_of_forecasts, avg_mse, avg_rmse, avg_mape, avg_smape","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:53:28.814807Z","iopub.execute_input":"2021-06-11T05:53:28.815206Z","iopub.status.idle":"2021-06-11T05:53:28.829537Z","shell.execute_reply.started":"2021-06-11T05:53:28.815168Z","shell.execute_reply":"2021-06-11T05:53:28.828299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Calculating fit statistics for both additive and multiplicative model on test data without saving the prediction time series results\n# Additive\n_, _, avg_mse_hw_ad, avg_rmse_hw_ad, avg_mape_hw_ad, avg_smape_hw_ad = iterate_hw_over_evals(data_train_hw=data_train,\n                                                                                       data_test_hw=data_test,\n                                                                                       target=\"beer_prod\",\n                                                                                       method=\"add\")\n# Multiplicative\n_, _, avg_mse_hw_ml, avg_rmse_hw_ml, avg_mape_hw_ml, avg_smape_hw_ml = iterate_hw_over_evals(data_train_hw=data_train,\n                                                                                       data_test_hw=data_test,\n                                                                                       target=\"beer_prod\",\n                                                                                       method=\"mul\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:53:56.063252Z","iopub.execute_input":"2021-06-11T05:53:56.063612Z","iopub.status.idle":"2021-06-11T05:54:33.567334Z","shell.execute_reply.started":"2021-06-11T05:53:56.063583Z","shell.execute_reply":"2021-06-11T05:54:33.56628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Additive HWES Test Fit - AVG MSE: {avg_mse_hw_ad:.3f}, AVG RMSE: {avg_rmse_hw_ad:.3f}, AVG MAPE: {avg_mape_hw_ad:.3f}, AVG SMAPE: {avg_smape_hw_ad:.3f}\")\nprint(f\"Multiplicative HWES Test Fit - AVG MSE: {avg_mse_hw_ml:.3f}, AVG RMSE: {avg_rmse_hw_ml:.3f}, AVG MAPE: {avg_mape_hw_ml:.3f}, AVG SMAPE: {avg_smape_hw_ml:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:35:04.275152Z","iopub.execute_input":"2021-06-10T20:35:04.275526Z","iopub.status.idle":"2021-06-10T20:35:04.281608Z","shell.execute_reply.started":"2021-06-10T20:35:04.275495Z","shell.execute_reply":"2021-06-10T20:35:04.280186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On training data, the multiplicative model slightly outperforms the additive model, but on the test dataset, additive model performs much better than the multiplicative model. Therefore, additive model is used as a HWES model for the final comparison and prediction.","metadata":{}},{"cell_type":"code","source":"# Final Forecast of Holt-Winters Exponential Smoothing Model\nfinal_holt_winters_model = ExponentialSmoothing(data[\"beer_prod\"],trend=\"add\", seasonal=\"add\")\nfinal_holt_winters_model_fitted = final_holt_winters_model.fit()\nfinal_holt_winters_forecast = final_holt_winters_model_fitted.forecast(16).values","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:35:08.161924Z","iopub.execute_input":"2021-06-10T20:35:08.162302Z","iopub.status.idle":"2021-06-10T20:35:08.332061Z","shell.execute_reply.started":"2021-06-10T20:35:08.162272Z","shell.execute_reply":"2021-06-10T20:35:08.33098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <center> SARIMA </center>\nThe second suitable forecasting model for the current data is SARIMA - Autoregressive Integrated Moving Average model with seasonal component. It contains two components - trend and seasonal, and 7 hyperparameters to tune.\n* p: Trend autoregression order.\n* d: Trend difference order.\n* q: Trend moving average order.\n* P: Seasonal autoregressive order.\n* D: Seasonal difference order.\n* Q: Seasonal moving average order.\n* m: The number of time steps for a single seasonal period. <br>\n\nAlthough it is possible to try to base the (p,d,q) and (P,D,Q,s) parameters on some clues such as autocorrelation and partial autocorrelation plots as well as knowledge about data and it's seasonality, due to the interactions between parameters and associated relationships' complexity (and thanks to the available computational power), it is often useful to perform some variation of the grid search of the best performing combinations of parameters.","metadata":{}},{"cell_type":"code","source":"# Defining functions for grid search and evaluation of models on test data\ndef find_optimal_arima(data, target, p_list=[0], d_list=[0], q_list=[0], P_list=[0], D_list=[0], Q_list=[0], s_list=[0], top_perfomers=3):\n    \"\"\"\n    Iterates over lists of parameters and calculates ARIMA/SARIMA with all their combinations.\n    Fit statistics for all the models are calculated and top N performing parameter combinations for each of the fit statistics are returned.\n    The models are not duplicated so in case that top 4 models would be searched for and 4 models would perform the best on all these fit statistics,\n    only 4 combination of parameters would be returned\n    \"\"\"\n    all_models_params = []\n    all_models_mse = []\n    all_models_rmse = []\n    all_models_mape = []\n    all_models_smape =[]\n    best_models = []\n    \n    x = 0\n    \n    for p in p_list:\n        for d in d_list:\n            for q in q_list:\n                for P in P_list:\n                    for D in D_list:\n                        for Q in Q_list:\n                            for s in s_list:\n                                \n#                                 total_iters = len(p_list) * len(d_list) * len(q_list) * len(P_list) * len(D_list) * len(Q_list) * len(s_list)\n#                                 x += 1\n#                                 print(f\"Running iteration {x} out of total {total_iters}.\")\n                                \n                                try:\n                                    arima_result = ARIMA(endog=data[target].values, order=(p,d,q), seasonal_order=(P,D,Q,s), dates=data.index)\n                                    arima_result_fit = arima_result.fit()\n                                    predicted = arima_result_fit.predict()\n                                    real = data[target].values\n\n                                    mse_result = mean_squared_error(real, predicted)\n                                    rmse_result = mean_squared_error(real, predicted, squared=False)\n                                    mape_result = mape(real, predicted)\n                                    smape_result = smape(real, predicted)\n\n                                    all_models_params.append(f\"order={p,d,q},seasonal_order={P,D,Q,s}\")\n                                    all_models_mse.append(mse_result)\n                                    all_models_rmse.append(rmse_result)\n                                    all_models_mape.append(mape_result)\n                                    all_models_smape.append(smape_result)\n                                except:\n#                                     print(f\"Passing on iteration {x} due to an error\")\n                                    pass\n                                    \n    top_mse = np.argsort(all_models_mse)[:top_perfomers]\n    top_rmse = np.argsort(all_models_rmse)[:top_perfomers]\n    top_mape = np.argsort(all_models_mape)[:top_perfomers]\n    top_smape = np.argsort(all_models_smape)[:top_perfomers]                \n                                    \n    top_indices = np.unique(np.concatenate([top_mse,top_rmse,top_mape,top_smape]))\n    \n    for q in top_indices:\n    \n        best_models.append({all_models_params[q]:[f\"mse:{all_models_mse[q]:.3f}\",f\"rmse:{all_models_rmse[q]:.3f}\",f\"mape:{all_models_mape[q]:.3f}\",f\"smape:{all_models_smape[q]:.3f}\"]})\n    \n    return best_models\n\n\ndef iterate_sarima_over_evals(data_train_ar, data_test_ar, target, arima_order, arima_seasonal_order):\n    \"\"\"\n    Function iterates over the whole testing datasets\n        1) Makes out of sample prediction of length 16\n        2) Saves the model performance on the current iteration \n        3) Moves one time step ahead and appends one new datapoint to the train_data\n        4) Refits the model and repeats steps 1-3 until the end of the test dataset is reached\n        5) Calculates average MSE, RMSE, MAPE and SMAPE metrics\n        6) Optionally returns all predictions with corresponding true values for further inspection\n    \"\"\"\n    list_of_reals = []\n    list_of_forecasts = []\n    mse_list = []\n    rmse_list = []\n    mape_list = []\n    smape_list = []\n    \n    for x in range(len(data_test_ar)-16):\n        if x == 0:\n            sarima_eval_model = ARIMA(data_train_ar[target], order=arima_order, seasonal_order=arima_seasonal_order, dates=data_train_ar.index)\n            sarima_eval_model_fit = sarima_eval_model.fit()\n            sarima_forecast_values = sarima_eval_model_fit.forecast(steps=16).values\n            \n            list_of_reals.append(data_test_ar[target].values[x:x+16])\n            list_of_forecasts.append(sarima_forecast_values)\n\n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n        else:\n            data_train_ar = data_train_ar.append(data_test_ar.iloc[x-1])\n            \n            sarima_eval_model = ARIMA(data_train_ar[target], order=arima_order, seasonal_order=arima_seasonal_order, dates=data_train_ar.index)\n            sarima_eval_model_fit = sarima_eval_model.fit()\n            sarima_forecast_values = sarima_eval_model_fit.forecast(steps=16).values\n            \n            list_of_reals.append(data_test_ar[target].values[x:x+16])\n            list_of_forecasts.append(sarima_forecast_values)\n            \n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n    avg_mse = np.array(mse_list).mean()\n    avg_rmse = np.array(rmse_list).mean()\n    avg_mape = np.array(mape_list).mean()\n    avg_smape = np.array(smape_list).mean()\n        \n    return list_of_reals, list_of_forecasts, avg_mse, avg_rmse, avg_mape, avg_smape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:35:40.857925Z","iopub.execute_input":"2021-06-10T20:35:40.858224Z","iopub.status.idle":"2021-06-10T20:35:40.876631Z","shell.execute_reply.started":"2021-06-10T20:35:40.8582Z","shell.execute_reply":"2021-06-10T20:35:40.875883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Autocorrelation and partial autocorrelation plots\nac_pc_fig = plt.figure(figsize=(12,8))\nax1 = ac_pc_fig.add_subplot(211)\nac_pc_fig = sm.graphics.tsa.plot_acf(data_train[\"beer_prod\"].values, lags=60, ax=ax1) # \nax2 = ac_pc_fig.add_subplot(212)\nac_pc_fig = sm.graphics.tsa.plot_pacf(data_train[\"beer_prod\"].values, lags=60, ax=ax2)# , lags=40","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:56:22.051596Z","iopub.execute_input":"2021-06-11T05:56:22.052102Z","iopub.status.idle":"2021-06-11T05:56:22.601984Z","shell.execute_reply.started":"2021-06-11T05:56:22.052057Z","shell.execute_reply":"2021-06-11T05:56:22.60108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on autocorrelation, there seems to be obvious seasonal pattern. It serves also as a guidance for selection range in which the parameters combinations will be tested. Although it would be beneficial to test high amount of parameters, for computational reasons, only lower amount of most plausible parameters will be tested.","metadata":{}},{"cell_type":"code","source":"# List of parameters for grid search\np_list = [1,2,3]\nd_list = [0,1]\nq_list = [0,1]\nP_list = [1,2,3]\nD_list = [0,1]\nQ_list = [0,1]\ns_list = [12]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Finding parameters \nbest_arima_models = find_optimal_arima(data=data_train, target=\"beer_prod\", \n                                       p_list=p_list, d_list=d_list, q_list=q_list,\n                                       P_list=P_list, D_list=D_list, Q_list=Q_list, \n                                       s_list=s_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Priting best performing arima models\nprint(best_arima_models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grid search suggested higher number of best performing models. However, since their performance on train dataset is very similar, I will select only two of the best performing ones from each category - one with and one without differencing, for evaluation on test dataset.","metadata":{}},{"cell_type":"code","source":"%%capture\n# Best model without differencing\nsarima_no_differencing = ARIMA(endog=data_train[\"beer_prod\"].values, order=(3,0,1), seasonal_order=(3,0,1,12), dates=data_train.index)\nsarima_no_differencing_fit = sarima_no_differencing.fit()\n\n_, _, avg_mse_sarima_ndif, avg_rmse_sarima_ndif, avg_mape_sarima_ndif, avg_smape_sarima_ndif = iterate_sarima_over_evals(data_train_ar=data_train,\n                                                                                                                         data_test_ar=data_test,\n                                                                                                                         target=\"beer_prod\",\n                                                                                                                         arima_order=(3,0,1),\n                                                                                                                         arima_seasonal_order=(3,0,1,12))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:36:02.217531Z","iopub.execute_input":"2021-06-10T20:36:02.217849Z","iopub.status.idle":"2021-06-10T20:47:03.478049Z","shell.execute_reply.started":"2021-06-10T20:36:02.217822Z","shell.execute_reply":"2021-06-10T20:47:03.477395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Best model with differencing\nsarima_differencing = ARIMA(endog=data_train[\"beer_prod\"].values, order=(2,1,1), seasonal_order=(3,1,1,12), dates=data_train.index)\nsarima_differencing_fit = sarima_differencing.fit()\n\n_, _, avg_mse_sarima_dif, avg_rmse_sarima_dif, avg_mape_sarima_dif, avg_smape_sarima_dif = iterate_sarima_over_evals(data_train_ar=data_train,\n                                                                                                                     data_test_ar=data_test,\n                                                                                                                     target=\"beer_prod\",\n                                                                                                                     arima_order=(2,1,1),\n                                                                                                                     arima_seasonal_order=(3,1,1,12))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:47:18.475672Z","iopub.execute_input":"2021-06-10T20:47:18.475938Z","iopub.status.idle":"2021-06-10T20:54:04.760041Z","shell.execute_reply.started":"2021-06-10T20:47:18.475914Z","shell.execute_reply":"2021-06-10T20:54:04.759156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing fit metrics of both models in separate cell\nprint(f\"SARIMA without Differencing Train Fit: MSE: {avg_mse_sarima_ndif:.3f}, RMSE: {avg_rmse_sarima_ndif:.3f}, MAPE: {avg_mape_sarima_ndif:.3f}, SMAPE: {avg_smape_sarima_ndif:.3f}\")\nprint(f\"SARIMA with Differencing Train Fit: MSE: {avg_mse_sarima_dif:.3f}, RMSE: {avg_rmse_sarima_dif:.3f}, MAPE: {avg_mape_sarima_dif:.3f}, SMAPE: {avg_smape_sarima_dif:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:58:30.289928Z","iopub.execute_input":"2021-06-10T20:58:30.29028Z","iopub.status.idle":"2021-06-10T20:58:30.29653Z","shell.execute_reply.started":"2021-06-10T20:58:30.290232Z","shell.execute_reply":"2021-06-10T20:58:30.29537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sarima train set predictions chart\narima_plot = go.Figure()\n\narima_plot.add_trace(go.Scatter(x=data_train.index, y=data_train[\"beer_prod\"].values,\n                                mode='lines',\n                                name='Monthly Beer Production'))\n\narima_plot.add_trace(go.Scatter(x=data_train.index, y=sarima_no_differencing_fit.predict(),\n                                mode='lines',\n                                name=\"SARIMA without differencing\"))\n\narima_plot.add_trace(go.Scatter(x=data_train.index, y=sarima_differencing_fit.predict(),\n                                mode='lines',\n                                name=\"SARIMA with differencing\"))\n\narima_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\narima_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"SARIMA Train Data Predictions\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\narima_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:58:32.194816Z","iopub.execute_input":"2021-06-10T20:58:32.195118Z","iopub.status.idle":"2021-06-10T20:58:32.28078Z","shell.execute_reply.started":"2021-06-10T20:58:32.195092Z","shell.execute_reply":"2021-06-10T20:58:32.279582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sarima residuals plots\nfig_residuals = make_subplots(rows=1, cols=2)\n\nfig_residuals.add_trace(go.Histogram(x=sarima_differencing_fit.resid,\n                                     name=\"Histogram\"),row=1, col=1)\n\nfig_residuals.add_trace(go.Scatter(x=np.arange(len(sarima_differencing_fit.resid)),y=sarima_differencing_fit.resid,\n                                   mode='markers',\n                                   name=\"Scatterplot\"),row=1, col=2)\n\nfig_residuals.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nfig_residuals.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    title={\n                        'text': \"SARIMA with differencing<br>Train Data Residual Plots\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\nfig_residuals.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:21:58.989745Z","iopub.execute_input":"2021-06-10T21:21:58.990032Z","iopub.status.idle":"2021-06-10T21:21:59.029497Z","shell.execute_reply.started":"2021-06-10T21:21:58.990008Z","shell.execute_reply":"2021-06-10T21:21:59.028591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the test dataset, the SARIMA model with differencing is performing better on all fit metrics. However, it still seems that the model does not fit perfectly. The residuals are not normally distributed (even when taking into account that the biggest residual outlier comes from differencing itself). Nevertheless, the model with order of (2,1,1) and seasonal order (3,1,1,12) is used for the final forecast.","metadata":{}},{"cell_type":"code","source":"# Final Forecast Sarima\nfinal_sarima_model = ARIMA(endog=data[\"beer_prod\"].values, order=(2,1,1), seasonal_order=(3,1,1,12), dates=data.index)\nfinal_sarima_model_fitted = final_sarima_model.fit()\nfinal_sarima_full_forecast = final_sarima_model_fitted.get_forecast(16)\nfinal_sarima_full_forecast_yhat = final_sarima_full_forecast.predicted_mean\nfinal_sarima_full_forecast_yhat_conf_int = final_sarima_full_forecast.conf_int(alpha=0.05)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:58:52.197134Z","iopub.execute_input":"2021-06-10T20:58:52.197439Z","iopub.status.idle":"2021-06-10T20:58:59.996772Z","shell.execute_reply.started":"2021-06-10T20:58:52.197416Z","shell.execute_reply":"2021-06-10T20:58:59.996127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <center> Prophet </center>\nProphet is a time series forecasting framework developed by Facebook. As they describe it, it is <i>\"a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well,\"</i>(Facebook, https://facebook.github.io/prophet/) <br>\nMore info about the Prophet can be found in the article: Taylor SJ, Letham B. 2017. Forecasting at scale. PeerJ Preprints 5:e3190v2 https://doi.org/10.7287/peerj.preprints.3190v2 <br>\nProphet requires data in a specific format - \"ds\" column with datetime data and \"y\" column with target variable to be forecasted. For this reason, there is initial data wrangling to transform data into suitable format so that they could be used for making predictions with Prophet.","metadata":{}},{"cell_type":"code","source":"# Changing test and train data to format required by Prophet package\ndata_all_prophet = data.copy()\ndata_all_prophet.reset_index(inplace=True)\ndata_all_prophet.rename(columns={\"month\":\"ds\",\"beer_prod\":\"y\"}, inplace=True)\ndata_all_prophet_pred = data_all_prophet[[\"ds\",\"y\"]]\ndata_train_prophet_pred = data_all_prophet_pred.iloc[:-72]\ndata_test_prophet_pred = data_all_prophet_pred.iloc[-72:]","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:04:52.272746Z","iopub.execute_input":"2021-06-11T06:04:52.273159Z","iopub.status.idle":"2021-06-11T06:04:52.282856Z","shell.execute_reply.started":"2021-06-11T06:04:52.27312Z","shell.execute_reply":"2021-06-11T06:04:52.28196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Intiliazing model\nprophet_model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n# Fitting the model\nprophet_model_fit = prophet_model.fit(data_train_prophet_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:05:13.967289Z","iopub.execute_input":"2021-06-11T06:05:13.967639Z","iopub.status.idle":"2021-06-11T06:05:14.165401Z","shell.execute_reply.started":"2021-06-11T06:05:13.96761Z","shell.execute_reply":"2021-06-11T06:05:14.16443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating train fit statistics\nprophet_train_mse = mean_squared_error(data_train_prophet_pred[\"y\"].values, prophet_model_fit.predict()[\"yhat\"].values)\nprophet_train_rmse = mean_squared_error(data_train_prophet_pred[\"y\"].values, prophet_model_fit.predict()[\"yhat\"].values, squared=False)\nprophet_train_mape = mape(data_train_prophet_pred[\"y\"].values, prophet_model_fit.predict()[\"yhat\"].values)\nprophet_train_smape = smape(data_train_prophet_pred[\"y\"].values, prophet_model_fit.predict()[\"yhat\"].values)\n\nprint(f\"Prophet Train Fit: MSE: {prophet_train_mse:.3f}, RMSE: {prophet_train_rmse:.3f}, MAPE: {prophet_train_mape:.3f}, SMAPE: {prophet_train_smape:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:05:16.81364Z","iopub.execute_input":"2021-06-11T06:05:16.81403Z","iopub.status.idle":"2021-06-11T06:05:26.388691Z","shell.execute_reply.started":"2021-06-11T06:05:16.813997Z","shell.execute_reply":"2021-06-11T06:05:26.387887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chart Prophet\nph_plot = go.Figure()\n\nph_plot.add_trace(go.Scatter(x=data_train.index, y=data_train[\"beer_prod\"].values,\n                                mode='lines',\n                                name='Monthly Beer Production'))\n\nph_plot.add_trace(go.Scatter(x=data_train.index, y=prophet_model_fit.predict()[\"yhat\"].values,\n                                mode='lines',\n                                name=\"Prophet's Prediction\"))\n\nph_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nph_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"Prophet's Train Data Prediction\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\nph_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:05:30.934823Z","iopub.execute_input":"2021-06-11T06:05:30.935378Z","iopub.status.idle":"2021-06-11T06:05:33.466173Z","shell.execute_reply.started":"2021-06-11T06:05:30.93533Z","shell.execute_reply":"2021-06-11T06:05:33.465148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even with default parameters, Prophet seems to be performing very well on the test dataset when compared to previous models. There seems to be a tendency to overestimate highs and lows of data peaks at the beginning of the time series which reverets into slight underestimation of peaks at the end of the training dataset.","metadata":{}},{"cell_type":"code","source":"def iterate_prophet_over_evals(data_train_pr, data_test_pr):\n    \"\"\"\n    Function iterates over the whole testing datasets\n        1) Makes out of sample prediction of length 16\n        2) Saves the model performance on the current iteration \n        3) Moves one time step ahead\n        4) Refits the model and repeats steps 1-3 until the end of test dataset is reached\n        5) Calculates average MSE, RMSE, MAPE and SMAPE metrics\n        6) Optionally returns all predictions with corresponding true values for further inspection\n    \"\"\"\n    list_of_reals = []\n    list_of_forecasts = []\n    mse_list = []\n    rmse_list = []\n    mape_list = []\n    smape_list = []\n    \n    for x in range(len(data_test_pr)-16):\n        if x == 0:\n            prophet_eval_model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n            prophet_eval_model_fit = prophet_eval_model.fit(data_train_pr)\n            prophet_forecast_values = prophet_eval_model_fit.predict(pd.DataFrame(data_test_pr[\"ds\"].iloc[x:x+16]))[\"yhat\"].values\n            \n            list_of_reals.append(data_test_pr[\"y\"].values[x:x+16])\n            list_of_forecasts.append(prophet_forecast_values)\n\n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n        else:\n            \n            data_train_ar = data_train_pr.append(data_test_pr.iloc[x-1])\n            \n            prophet_eval_model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n            prophet_eval_model_fit = prophet_eval_model.fit(data_train_pr)\n            prophet_forecast_values = prophet_eval_model_fit.predict(pd.DataFrame(data_test_pr[\"ds\"].iloc[x:x+16]))[\"yhat\"].values\n            \n            list_of_reals.append(data_test_pr[\"y\"].values[x:x+16])\n            list_of_forecasts.append(prophet_forecast_values)\n            \n            mse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x]))\n            rmse_list.append(mean_squared_error(list_of_reals[x], list_of_forecasts[x], squared=False))\n            mape_list.append(smape(list_of_reals[x], list_of_forecasts[x]))\n            smape_list.append(mape(list_of_reals[x], list_of_forecasts[x]))\n            \n    avg_mse = np.array(mse_list).mean()\n    avg_rmse = np.array(rmse_list).mean()\n    avg_mape = np.array(mape_list).mean()\n    avg_smape = np.array(smape_list).mean()\n        \n    return list_of_reals, list_of_forecasts, avg_mse, avg_rmse, avg_mape, avg_smape","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:06:19.348598Z","iopub.execute_input":"2021-06-11T06:06:19.348978Z","iopub.status.idle":"2021-06-11T06:06:19.363793Z","shell.execute_reply.started":"2021-06-11T06:06:19.348941Z","shell.execute_reply":"2021-06-11T06:06:19.36269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, _, avg_mse_proph, avg_rmse_proph, avg_mape_proph, avg_smape_proph = iterate_prophet_over_evals(data_train_pr=data_train_prophet_pred,\n                                                                                                  data_test_pr=data_test_prophet_pred)\nprint(f\"Prophet Test Fit - AVG MSE: {avg_mse_proph:.3f}, AVG RMSE: {avg_rmse_proph:.3f}, AVG MAPE: {avg_mape_proph:.3f}, AVG SMAPE: {avg_smape_proph:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:06:25.260872Z","iopub.execute_input":"2021-06-11T06:06:25.261398Z","iopub.status.idle":"2021-06-11T06:08:41.746143Z","shell.execute_reply.started":"2021-06-11T06:06:25.261348Z","shell.execute_reply":"2021-06-11T06:08:41.74494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As with other algorithms and models, the same methodology is used to measure performance of the Prophet model on the test dataset and since only the default model is tested, it is also used for making the prediction for the final evaluation and forecasts comparison.","metadata":{}},{"cell_type":"code","source":"# Final Forecast Prophet\nfinal_prophet_model = Prophet()\nfinal_prophet_model_fitted = final_prophet_model.fit(data_all_prophet_pred)\nfinal_prophet_forecast = final_prophet_model_fitted.predict(pd.DataFrame({\"ds\":pd.date_range(start='1995-09-01', end='1996-12-01', freq='MS')}))[\"yhat\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:08:41.747472Z","iopub.execute_input":"2021-06-11T06:08:41.747758Z","iopub.status.idle":"2021-06-11T06:08:44.134404Z","shell.execute_reply.started":"2021-06-11T06:08:41.747729Z","shell.execute_reply":"2021-06-11T06:08:44.133292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <center> N-BEATS </center>\n\nN-BEATS is an algorithm using deep neural architecture based on backward and forward residual links and a deep stack of fully-connected layers. The architecture is interpretable, applicable without modification to a wide array of target domains, and relatively fast to train. It was also tested on several competition datasets including M3, M4 and TOURISM competition datasets containing time series from diverse domains. Here, it's implementation using pytorch-forecasting package is used.<br>\nFor more info see the article <i> N-BEATS: Neural basis expansion analysis for interpretable time series forecasting </i> from Oreshkin et.al (2020) accesible at https://arxiv.org/abs/1905.10437. <br>\nPytorch-forecasting requires data in a specific format so that it could be used to construct TimeSeries Dataset and corresponding dataloaders. Thus, initial datawrangling and transformation has to be conducted.\n","metadata":{}},{"cell_type":"code","source":"# Reformatting data to format required by pytorch-forecasting\ndata_nbeats = data.copy()\ndata_nbeats.reset_index(inplace=True)\ndata_nbeats.reset_index(inplace=True)\ndata_nbeats.rename(columns={'index':'time_idx', \"beer_prod\":\"target\"}, inplace=True)\ndata_nbeats[\"grouping\"] = 1","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:09:01.415271Z","iopub.execute_input":"2021-06-11T06:09:01.415634Z","iopub.status.idle":"2021-06-11T06:09:01.424976Z","shell.execute_reply.started":"2021-06-11T06:09:01.415602Z","shell.execute_reply":"2021-06-11T06:09:01.423717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time series dataset and data loaders\nmax_encoder_length = 24 # two years backast length\nmax_prediction_length = 16 # 16 months prediction length\n\ntraining_cutoff = data_nbeats[\"time_idx\"].max() - 72 # taking 6 years as validation data\n\ncontext_length = max_encoder_length\nprediction_length = max_prediction_length\n\ntraining = TimeSeriesDataSet(\n    data_nbeats[lambda x: x.time_idx <= training_cutoff],\n    time_idx=\"time_idx\",\n    target=\"target\",\n    group_ids=[\"grouping\"],\n    # only unknown variable is \"value\" - and N-Beats can also not take any additional variables\n    time_varying_unknown_reals=[\"target\"],\n    max_encoder_length=context_length,\n    max_prediction_length=prediction_length,\n)\n\ntraining_parameters = training.get_parameters() # getting scaling and encoder parameters so that they are used in transformation of validation and prediction data\n\nvalidation = TimeSeriesDataSet.from_parameters(training_parameters, data_nbeats, min_prediction_idx=training_cutoff + 1)\n\nbatch_size = 128\n\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n\n# calculate baseline\nactuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\nbaseline_predictions = Baseline().predict(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:09:03.290882Z","iopub.execute_input":"2021-06-11T06:09:03.291273Z","iopub.status.idle":"2021-06-11T06:09:04.518216Z","shell.execute_reply.started":"2021-06-11T06:09:03.29124Z","shell.execute_reply":"2021-06-11T06:09:04.516621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the best initial learning rate\npl.seed_everything(42)\ntrainer = pl.Trainer(gpus=0, gradient_clip_val=0.1)\nnet = NBeats.from_dataset(training, learning_rate=3e-2, weight_decay=1e-2, widths=[256, 2048], backcast_loss_ratio=0.1)\nres = trainer.tuner.lr_find(net, train_dataloader=train_dataloader, val_dataloaders=val_dataloader, min_lr=0.0001, max_lr=0.1)\nprint(f\"suggested learning rate: {res.suggestion()}\")\nsuggested_lr = res.suggestion()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T06:09:09.322141Z","iopub.execute_input":"2021-06-11T06:09:09.322512Z","iopub.status.idle":"2021-06-11T06:10:41.148977Z","shell.execute_reply.started":"2021-06-11T06:09:09.322478Z","shell.execute_reply":"2021-06-11T06:10:41.148101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# training n-beats network\n# early stopping to avoid overfitting\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\ntrainer = pl.Trainer(\n    max_epochs=100,\n    gpus=0,\n    weights_summary=\"top\",\n    gradient_clip_val=0.1,\n    callbacks=[early_stop_callback],\n)\n\n# nbeats net specifications\nnet = NBeats.from_dataset(\n    training,\n    learning_rate=suggested_lr,\n    log_interval=-1,\n    log_val_interval=1,\n    weight_decay=0.01,\n    widths=[256, 2048],\n    backcast_loss_ratio=1.0,\n)\n\n# fitting model to the data\ntrainer.fit(\n    net,\n    train_dataloader=train_dataloader,\n    val_dataloaders=val_dataloader,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:05:03.195912Z","iopub.execute_input":"2021-06-10T21:05:03.196226Z","iopub.status.idle":"2021-06-10T21:09:08.788511Z","shell.execute_reply.started":"2021-06-10T21:05:03.196186Z","shell.execute_reply":"2021-06-10T21:09:08.787123Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initial evaluation of the model\n# loading the best iteration of the model\nbest_model_path = trainer.checkpoint_callback.best_model_path\nbest_model = NBeats.load_from_checkpoint(best_model_path)\n\n# getting predictions\npredictions = best_model.predict(val_dataloader)\nprint(f\"Baseline SMAPE: {SMAPE()(baseline_predictions, actuals)}\")\nprint(f\"Model SMAPE: {SMAPE()(predictions, actuals)}\")\n\n# getting raw predictions\nraw_predictions = best_model.predict(val_dataloader, mode=\"raw\", return_x=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:09:54.969344Z","iopub.execute_input":"2021-06-10T21:09:54.969664Z","iopub.status.idle":"2021-06-10T21:09:55.921641Z","shell.execute_reply.started":"2021-06-10T21:09:54.969634Z","shell.execute_reply":"2021-06-10T21:09:55.920473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a substantial improvement over the baseline model in SMAPE. However, for the model performance to be comparable to that of other models, it is necessary to compare its ability of forecasting on the test data using the same methodology. Thus, the forecast part of prediction (not the backcast) is run through the same loop as was done with other models and the same fit metrics are calculated.","metadata":{}},{"cell_type":"code","source":"def iterate_nbeats_over_evals(np_evals, np_predictions):\n    \"\"\"\n    Iterates over all predictions and their actual values\n    Return mean values of fit statistics for all forecasts predicted by the model\n    \"\"\"\n    mse_list = []\n    rmse_list = []\n    mape_list = []\n    smape_list = []\n    \n    for x in range(len(np_predictions)):\n        \n        mse_list.append(mean_squared_error(np_evals[x], np_predictions[x]))\n        rmse_list.append(mean_squared_error(np_evals[x], np_predictions[x], squared=False))\n        mape_list.append(smape(np_evals[x], np_predictions[x]))\n        smape_list.append(mape(np_evals[x], np_predictions[x]))\n        \n    avg_mse = np.array(mse_list).mean()\n    avg_rmse = np.array(rmse_list).mean()\n    avg_mape = np.array(mape_list).mean()\n    avg_smape = np.array(smape_list).mean()\n    \n    return avg_mse, avg_rmse, avg_mape, avg_smape ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:00.026206Z","iopub.execute_input":"2021-06-10T21:10:00.026558Z","iopub.status.idle":"2021-06-10T21:10:00.035036Z","shell.execute_reply.started":"2021-06-10T21:10:00.026524Z","shell.execute_reply":"2021-06-10T21:10:00.033691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating model\nraw_predictions_np = raw_predictions[\"prediction\"].numpy()\nval_data_nbeats = next(iter(val_dataloader))\nval_data_nbeats_np = val_data_nbeats[0][\"decoder_target\"].numpy()\navg_mse_nbeats, avg_rmse_nbeats, avg_mape_nbeats, avg_smape_nbeats = iterate_nbeats_over_evals(val_data_nbeats_np, raw_predictions_np)\nprint(f\"N-BEATS Test Fit - AVG MSE: {avg_mse_nbeats:.3f}, AVG RMSE: {avg_rmse_nbeats:.3f}, AVG MAPE: {avg_mape_nbeats:.3f}, AVG SMAPE: {avg_smape_nbeats:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:02.706508Z","iopub.execute_input":"2021-06-10T21:10:02.707018Z","iopub.status.idle":"2021-06-10T21:10:02.958138Z","shell.execute_reply.started":"2021-06-10T21:10:02.706986Z","shell.execute_reply":"2021-06-10T21:10:02.956997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that in this particular case, N-BEATS will not beat classical time series prediction models in terms of performance on the test dataset. There are, however, many hyperparameters that could be further tuned. Nevertheless, the trained model will be used for producing forecasts that will be used in the final comparison. A uniqiue way of constructing the dataloader for this specific task has to be taken. More precisely, the last n (depending on the backcast length) datapoints of  the original dataset have to be merged with dataset with empty target values that continues with time_idx where the test dataset has ended. This way, a dataloader for the out of sample prediction can be constructed.","metadata":{}},{"cell_type":"code","source":"# Data for out of sample prediction\ndata_oos_part_one = data_nbeats.iloc[-24:]\n# Creating semi-empty dataset\ndata_oos_part_two = pd.DataFrame({\"month\":pd.date_range(start='1995-09-01', end='1996-12-01', freq='MS')})\ndata_oos_part_two[\"target\"] = 0\ndata_oos_part_two[\"beer_prod_log\"] = 0\ndata_oos_part_two[\"beer_prod_box_cox\"] = 0\ndata_oos_part_two[\"grouping\"] = 1\ndata_oos_part_two.reset_index(inplace=True)\ndata_oos_part_two[\"time_idx\"] = data_oos_part_two[\"index\"] + 476\ndata_oos_part_two = data_oos_part_two[[\"time_idx\",\"month\",\"target\",\"beer_prod_log\",\"beer_prod_box_cox\",\"grouping\"]]\n# Putting both datasets together\ndata_oos = data_oos_part_one.append(data_oos_part_two)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:06.348505Z","iopub.execute_input":"2021-06-10T21:10:06.34901Z","iopub.status.idle":"2021-06-10T21:10:06.366711Z","shell.execute_reply.started":"2021-06-10T21:10:06.348975Z","shell.execute_reply":"2021-06-10T21:10:06.365715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final Forecast N-BEATS\noos_pred = TimeSeriesDataSet.from_parameters(training_parameters, data_oos, min_prediction_idx=len(data_nbeats) - 24 + 1)\noos_dataloader = oos_pred.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\nfinal_nbeats_forecast = best_model.predict(oos_dataloader).numpy()[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:09.632113Z","iopub.execute_input":"2021-06-10T21:10:09.632622Z","iopub.status.idle":"2021-06-10T21:10:09.70235Z","shell.execute_reply.started":"2021-06-10T21:10:09.63259Z","shell.execute_reply":"2021-06-10T21:10:09.701589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <center> Best Models Evaluation </center>\nBellow, you can see the table with all selected final models performance on the test dataset. The same methodology was applied to all algorithms. Despite the fact that only one model is selected for presenting the final forecast, I visualize all the predictions so that it can be seen that despite differences in evaluation data fit, all models forecast very similar beer production values in Australia for 1996.","metadata":{}},{"cell_type":"code","source":"# constructing final comparisons table\nfinal_comparison = pd.DataFrame.from_dict({\"Model\":[\"Holt-Winters Exponential Smoothing\",\"SARIMA\",\"Prophet\",\"N-BEATS\"],\n                                           \"AVG MSE\":[avg_mse_hw_ad,avg_mse_sarima_dif,avg_mse_proph,avg_mse_nbeats],\n                                           \"AVG RMSE\":[avg_rmse_hw_ad,avg_rmse_sarima_dif,avg_rmse_proph,avg_rmse_nbeats],\n                                           \"AVG MAPE\":[avg_mape_hw_ad,avg_mape_sarima_dif,avg_mape_proph,avg_mape_nbeats],\n                                           \"AVG SMAPE\":[avg_smape_hw_ad,avg_smape_sarima_dif,avg_smape_proph,avg_smape_nbeats]})\nfinal_comparison.set_index(\"Model\",inplace=True)\n\n#Final forecasts data table\nforecasted_data = pd.DataFrame({\"date\":pd.date_range(start='1995-09-01', end='1996-12-01', freq='MS')})\nforecasted_data[\"HWES\"] = final_holt_winters_forecast\nforecasted_data[\"SARIMA\"] = final_sarima_full_forecast_yhat\nforecasted_data[\"Prophet\"] = final_prophet_forecast\nforecasted_data[\"N-BEATS\"] = final_nbeats_forecast\n\ndisplay(final_comparison)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:25.894087Z","iopub.execute_input":"2021-06-10T21:10:25.894595Z","iopub.status.idle":"2021-06-10T21:10:25.921671Z","shell.execute_reply.started":"2021-06-10T21:10:25.894564Z","shell.execute_reply":"2021-06-10T21:10:25.920491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chart of all modeled predictions for 1996\nfinal_pred_plot = go.Figure()\n\nfinal_pred_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=forecasted_data[\"HWES\"].values[-12:],\n                                mode='lines',\n                                name='Forecast - HWES'))\n\nfinal_pred_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=forecasted_data[\"SARIMA\"].values[-12:],\n                                mode='lines',\n                                name='Forecast - SARIMA'))\n\nfinal_pred_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=forecasted_data[\"Prophet\"].values[-12:],\n                                mode='lines',\n                                name='Forecast - Prophet'))\n\nfinal_pred_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=forecasted_data[\"N-BEATS\"].values[-12:],\n                                mode='lines',\n                                name='Forecast - N-BEATS'))\n\n\nfinal_pred_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nfinal_pred_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"Beer Production Forecasts for 1996\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\nfinal_pred_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:36.335405Z","iopub.execute_input":"2021-06-10T21:10:36.33569Z","iopub.status.idle":"2021-06-10T21:10:36.364649Z","shell.execute_reply.started":"2021-06-10T21:10:36.335666Z","shell.execute_reply":"2021-06-10T21:10:36.362965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> SARIMA was selected as the best performing model </b> for the final forecast due to several reasons:\n1. Overall best fit on evaluation dataset - achieving best performance in all of the selected fit metrics (MSE,RMSE,MAPE,SMAPE)\n2. It provides a possibility to construct confidence interval for forecasts\n3. Relative simplicity of the model and speed of estimation\n\nHowever, it should be taken into consideration that all 4 models performed reasonably well and, as can be seen in the comparison of all 4 forecasts, all of them provided very similar forecasted values. Moreover, further improvements in model performance could be achieved by - feature engineering (adding new datapoints as covariates) or further tuning more complex models such as N-BEATS (more hyperparameters tuning, settting different decays, amount of stacks, neurons in stacks, backcast lenght and backcast weights) or manualy testing different Prophet's hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"## <center> Final SARIMA beer production forecast for 1996 </center>\n\nBellow, you can find plotted forecast based on the SARIMA model for the year 1996 with 95% confidence intervals built around the point estimate of the prediction. You can also compare the data with beer production from 1994 and partially estimated beer production from 1995.","metadata":{}},{"cell_type":"code","source":"# Chart with forecasted and historical data. Forecasts based on SARIMA\nfinal_forecast_plot = go.Figure()\n\nfinal_forecast_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=final_sarima_full_forecast_yhat_conf_int.reshape(32,-1)[::2].reshape(1,16)[0][-12:],\n                                mode='lines',\n                                name='Lower 95% CFI',\n                                line=dict(color='green', width=0.5)))\n\nfinal_forecast_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=final_sarima_full_forecast_yhat_conf_int.reshape(32,-1)[1::2].reshape(1,16)[0][-12:],\n                                mode='lines',\n                                fill=\"tonexty\",\n                                fillcolor='rgba(30, 130, 76, 0.1)',\n                                name=\"Upper 95% CFI\",\n                                line=dict(color='green', width=0.5)))\n\nfinal_forecast_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=final_sarima_full_forecast_yhat[-12:],\n                                mode='lines',\n                                name='1996 Beer Production Forecast',\n                                line=dict(color='green', width=2)))\n\nfinal_forecast_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=np.concatenate([data[\"beer_prod\"].values[-8:],final_sarima_full_forecast_yhat[:4]]),\n                                mode='lines',\n                                name='1995 Beer Production',\n                                line = dict(color='red', width=0.8, dash='dash')))\n\nfinal_forecast_plot.add_trace(go.Scatter(x=forecasted_data[\"date\"][-12:], y=data[\"beer_prod\"].values[-20:-8],\n                                mode='lines',\n                                name='1994 Beer Production',\n                                line = dict(color='royalblue', width=0.8, dash='dash')))\n\nfinal_forecast_plot.update_yaxes(showline=True, linewidth=1, linecolor='black', gridcolor='black')\n\nfinal_forecast_plot.update_layout(\n                    plot_bgcolor = \"rgba(0,0,0,0)\",\n                    autosize=True,\n                    xaxis_title=\"Date\",\n                    yaxis_title=\"Beer Production\",\n                    title={\n                        'text': \"Final Beer Production Forecasts for 1996\",\n                        'y':0.9,\n                        'x':0.5,\n                        'xanchor': 'center',\n                        'yanchor': 'top'})\n\nfinal_forecast_plot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:51.510041Z","iopub.execute_input":"2021-06-10T21:10:51.510358Z","iopub.status.idle":"2021-06-10T21:10:51.562685Z","shell.execute_reply.started":"2021-06-10T21:10:51.510332Z","shell.execute_reply":"2021-06-10T21:10:51.561471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Table with previous years data and 1996 year performance expectations\nbottom_production_1996 = final_sarima_full_forecast_yhat_conf_int.reshape(32,-1)[::2].reshape(1,16)[0][-12:].sum()\ntop_production_1996 = final_sarima_full_forecast_yhat_conf_int.reshape(32,-1)[1::2].reshape(1,16)[0][-12:].sum()\nmean_production_1996 = final_sarima_full_forecast_yhat[-12:].sum()\nmean_production_1995 = np.concatenate([data[\"beer_prod\"].values[-8:],final_sarima_full_forecast_yhat[:4]]).sum()\nproduction_1994 = data[\"beer_prod\"].values[-20:-8].sum()\n\nfinal_pred_df = pd.DataFrame({\"1994\":[round(production_1994,2)],\"1995\":[round(mean_production_1995,2)],\n                              \"Mean 1996\":[round(mean_production_1996,2)],\"Upper 1996\":[round(bottom_production_1996,2)],\n                              \"Lower 1996\":[round(top_production_1996,2)]})\nfinal_pred_df.rename(index={0:'Yearly Beer Prod'},inplace=True)\n\ndata_final_monthly = pd.DataFrame( {\"Jan\":[round(final_sarima_full_forecast_yhat[-12],2)],\n                                    \"Feb\":[round(final_sarima_full_forecast_yhat[-11],2)],\n                                    \"Mar\":[round(final_sarima_full_forecast_yhat[-10],2)],\n                                    \"Apr\":[round(final_sarima_full_forecast_yhat[-9],2)],\n                                    \"May\":[round(final_sarima_full_forecast_yhat[-8],2)],\n                                    \"Jun\":[round(final_sarima_full_forecast_yhat[-7],2)],\n                                    \"Jul\":[round(final_sarima_full_forecast_yhat[-6],2)],\n                                    \"Aug\":[round(final_sarima_full_forecast_yhat[-5],2)],\n                                    \"Sep\":[round(final_sarima_full_forecast_yhat[-4],2)],\n                                    \"Oct\":[round(final_sarima_full_forecast_yhat[-3],2)],\n                                    \"Nov\":[round(final_sarima_full_forecast_yhat[-2],2)],\n                                    \"Dec\":[round(final_sarima_full_forecast_yhat[-1],2)]})       \ndata_final_monthly.rename(index={0:'Monthly Beer Prod'},inplace=True)\n\ndisplay_side_by_side([final_pred_df, data_final_monthly], ['<b>Yearly Beer Production Comparison</b>', '<b>1996 Expected Monthly Beer Production</b>'])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:10:58.981209Z","iopub.execute_input":"2021-06-10T21:10:58.981631Z","iopub.status.idle":"2021-06-10T21:10:58.998212Z","shell.execute_reply.started":"2021-06-10T21:10:58.981606Z","shell.execute_reply":"2021-06-10T21:10:58.997511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicts that there will not be any major movement in the beer production in 1996 compared to two previous years. The overall year producting is expected to be between -14% to +13% compared with both 1995 and 1994, with mean value being almost exactly the same. As in the previous years, the highest beer production is expected to happen in the summer months (January, October, November, December) and the lowest production is expected to be around the winter months (April, May, June, August, September).","metadata":{}}]}