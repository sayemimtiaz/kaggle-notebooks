{"cells":[{"metadata":{},"cell_type":"markdown","source":"In machine learning point of view when we choose spam filtering approach, we can see classification problem. Here we classify email as spam or not spam (Ham) which depends on the features. But, in our point of view the features are the count of each word in the email. Training and Testing are the two modes which a machine learning system operates. In training the AI system is a given labelled data from the training data set. But in our project large set of emails are the labelled training data which is labeled spam or not spam (Ham). In the training process the classifiers prepare from the training data shows the interlink between the email and its label. But in the testing process an unlabeled data is given to the machine learning system. These data are the emails without the spam/ham label. The classifier in the testing shows us either it is spam or ham which depends on the features of an email. In our project we have given the data sets which consist of 5570 messages in which 4845 ham and 725 spam. Now we differentiate spam and ham messages. Pre-processing is the principal stage in which the unstructured information is changed over into progressively organized information. Since watchwords in SMS instant messages are inclined to be supplanted by images. In this investigation, the stop word list remover for English language have been applied to dispose of the stop words in the SMS instant messages. Fig. 2(a) shows the frequencies of words in messages, while Fig. 2(b) shows the most regular words utilized in spam instant messages are from pronoun (for example to) and prepositions (for example your) gatherings. Thus, the top words in ham instant message are involved by either pronoun, prepositions among numerous different sorts of stop words.\nImplementation:\nAfter doing the pre-processing work i.e., collecting all the most frequently used words in spam and ham separately. Next, we will implement this data using two classifiers that are Naïve bayes and SVM\n\n Naïve Bayes\n\nThe Naive Bayesian classifier takes its foundations in the well-known Bayes Theorem: \n\nP(├ H┤|ⅇ)=P(├ e┤|H)P(H)/P(e) \n\nBayes Theorem basically portrays the amount we ought to change the likelihood that our theory (H) will happen, given some new proof (e). \n\nFor our task, we need to decide the likelihood that an email is spam, given the proof of the email's highlights (F1,F2,...Fn). These highlights F1, F2,...Fn are only a Boolean worth (0 or 1) portraying whether the stem 1 through n shows up in the email. At that point, we think about P(Spam|F1,F2,...Fn) to P(Ham|F1,F2,...Fn) and \nfigure out which is more probable. Spam and Ham are viewed as the classes, which are spoken to in the conditions underneath as \"C\". \n\nP(├ C┤| F_1…F_n )=P(C)P(├ F_1,…,F_n ┤|C)/P(F_1,…,F_n ) \n\n\nNote that when we are looking at P(Spam|F1,F2,...Fn) to P(Ham|F1,F2,...Fn), the denominators are the equivalent. Consequently, we can rearrange the examination by noticing that: \n\nP(├ C┤| F_1…F_n )∝ P(C)P(├ F_1…F_n ┤|C)\n\nComputing P(F1,F2,...Fn|C) can get monotonous. In this way, the Naive Bayesian classifier makes a suspicion that definitely improves the figures. The supposition that will be that the probabilities of F1, F2, ..Fn happening are generally autonomous of one another. While this supposition that isn't accurate (as certain words are bound to show up together), the classifier despite everything performs very much given this suspicion. Presently, to decide if an email is spam or ham, we simply select the class (C = spam or C = ham) that expands the accompanying condition:\n\nargmax  p(C=c) ∏_(i=1)^n▒P(F_i=├ f_i ̇  ┤|C=c) \nSupport Vector Machine (SVM)\n\nVector Machine is utilized for grouping and furthermore for relapse issues where the datasets are utilized to prepare the SVM to group any new information that it gets. It is a directed AI calculation that works by finding a hyperplane that groups the dataset into various classes. The SVM amplifies the separation between various classes on account of the presence of numerous straight hyperplanes which is called as edge expansion.\nThe help vector machines (SVM) is anything but difficult to utilize and incredible for information characterization. When creating channel model, we make a vector for every datum in that preparing corpus and afterward SVM will delineate vectors into hyperplane. At that point SVM will realize which sorts of vector are close to which class. Here SVM is a decent moving toward technique in definitely discovering best characterization hyper-plans to augment the edge with the goal that we could arrange another message into spam or ham.\nHere and there, a direct classifier isn't sufficiently perplexing\nFrom \"Bonehead's Guide\": Map information into a more extravagant element space including nonlinear highlights, at that point build a hyperplane in that space with the goal that every single other condition are the equivalent Preprocess the information utilizing a change At that point, utilize a classifier\n\nf(x) = wϕ(x)+b\n\nBefore our decision rule was of the form:\n\nf(x) = wϕ(x)+b\n\nNow, we can write it as:\nf(x)=∑_(i=1)^m▒α_i  ϕ(x_i )⋅ϕ(x)+b\n\nKernel Function is    K(x_i,x)= ϕ(x_i )⋅ϕ(x)\n\nThe most applicable machine learning algorithm for our concern is linear svc. Before jumping into linear svc with our information, we're going to show a straight forward model that should help set your comprehension of working with linear svc.\nThe goal of a linear svc (support vector classifier) is to fit to the information you give, restoring a \"best fit\" hyperplane that partitions, or orders, your information. From that point, in the wake of getting the hyperplane, you would then be able to take care of certain highlights to your classifier to perceive what the \"anticipated\" class is. This makes this particular calculation somewhat appropriate for our utilizations, however you can utilize this for some circumstances.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndata = pd.read_csv(\"../input/emailmsgspamorhamdetection/Dataset.csv\", encoding='latin-1')\ndata.head(n=10)\ncount_Class=pd.value_counts(data[\"v1\"], sort= True)\ncount_Class.plot(kind= 'bar', color= [\"green\", \"red\"])\nplt.title('Bar chart')\nplt.show()\n\ncount1 = Counter(\" \".join(data[data['v1']=='ham'][\"v2\"]).split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0: \"words in Ham\", 1 : \"count\"})\ncount2 = Counter(\" \".join(data[data['v1']=='spam'][\"v2\"]).split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0: \"words in spam\", 1 : \"count_\"})\ndf1.plot.bar(legend = False, color = 'green')\ny_pos = np.arange(len(df1[\"words in Ham\"]))\nplt.xticks(y_pos, df1[\"words in Ham\"])\nplt.title('Repeated words in Ham messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\ndf2.plot.bar(legend = False, color = 'red')\ny_pos = np.arange(len(df2[\"words in spam\"]))\nplt.xticks(y_pos, df2[\"words in spam\"])\nplt.title('Repeated words in Spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\nf = feature_extraction.text.CountVectorizer(stop_words = 'english')\n\nX = f.fit_transform(data[\"v2\"])\nnp.shape(X)\ndata[\"v1\"]=data[\"v1\"].map({'spam':1,'ham':0})\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['v1'], test_size=0.33, random_state=42)\n\n\nlist_alpha = np.arange(1/100000, 20, 0.11)\nscore_train = np.zeros(len(list_alpha))\nscore_test = np.zeros(len(list_alpha))\nrecall_test = np.zeros(len(list_alpha))\nprecision_test= np.zeros(len(list_alpha))\ncount = 0\n\nfor alpha in list_alpha:\n    bayes = naive_bayes.MultinomialNB(alpha=alpha)\n    bayes.fit(X_train, y_train)\n    score_train[count] = bayes.score(X_train, y_train)\n    score_test[count]= bayes.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n    count = count + 1 \n    matrix = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\nmodels[models['Test Precision']==1].head(n=5)\nbest_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\nbayes = naive_bayes.MultinomialNB(alpha=list_alpha[best_index])\nbayes.fit(X_train, y_train)\nmodels.iloc[best_index, :]\nm_confusion_test = metrics.confusion_matrix(y_test, bayes.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\nprint('confusion matrix for naive bayes\\n',m_confusion_test)\nprint(precision_test)\nprint(recall_test)\n\n\n\n###############################SVM#############################################\n\n\n    \nlist_C = np.arange(500, 2000, 100) #100000\nscore_train = np.zeros(len(list_C))\nscore_test = np.zeros(len(list_C))\nrecall_test = np.zeros(len(list_C))\nprecision_test= np.zeros(len(list_C))\ncount = 0\nfor C in list_C:\n    svc = svm.SVC(C=C)\n    svc.fit(X_train, y_train)\n    score_train[count] = svc.score(X_train, y_train)\n    score_test[count]= svc.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, svc.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, svc.predict(X_test))\n    count = count + 1 \n    matrix = np.matrix(np.c_[list_C, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['C', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\n#models[models['Test Precision']==1].head(n=5)\n#best_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\nsvc = svm.SVC(C=list_C[best_index])\nsvc.fit(X_train, y_train)\nmodels.iloc[best_index, :]\nm_confusion_test = metrics.confusion_matrix(y_test, svc.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\nprint('confusion matrix for SVM\\n',m_confusion_test)\nprint(precision_test)\nprint(recall_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}