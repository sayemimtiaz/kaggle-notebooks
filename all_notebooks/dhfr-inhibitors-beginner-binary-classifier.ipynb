{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dihydrofolate Reductase (DHFR) Inhibitors Classification\n\n\n## Binary Activity Classifier\n\nThis Jupyter notebook takes quantitative structure-activity relationship (QSAR) data for a compiled set of dihydrofolate reductase (DHFR) inhibitors to construct a binary classifier to predict active/inactive compounds.\n\n## Dataset\n\nhttps://www.kaggle.com/shashwatwork/dihydrofolate-reductase-inhibitors-data-dhfr\n\n## Motivation\n\nEarly phase drug discovery involves the identification of compounds with therapeutic potential for development (hit discovery). Such candidates must engage the biological target with reasonable selectivity and possess favourable physicochemical properties such as solubility, cell permeability, and metabolic stability. A traditional early discovery phase protocol involves high-throughput screening (HTC) of compound libraries and determining their corresponding affinities via biological assays. While this can be effective, a notable limitation is the arduous task of conducting experiments for an enormous number of compounds which is expensive and slow. With the advent of computational power and sufficiently accurate physics-based models, computer (in silico) simulation and calculations are often used to supplement experimental assays. These calculations may incorporate multiple facets of compound binding, including the compound polarity, atomic bond lengths, atomic angles, etc. and collectively constitute QSAR data. Modern hit discovery utilizes this quantitative data to drive hypotheses and steer experimental efforts towards compounds which are most promising, thus enhancing drug discovery. \n\nSuppose you are given a dataset of historical DHFR inhibitors, denoted active or inactive, with accompanying QSAR data. You are working within a large hit discovery team tasked with designing novel DHFR inhibitors with better binding affinities and physicochemical properties. Given the historical DHFR dataset, you want to construct a classifier to determine whether new compounds are active or inactive. This would eliminate the need for biological assay determination for all compounds (assuming the classifier is sufficiently accurate), saving time, and increasing the ability to probe chemical space (i.e. evaluate diverse compounds which can be advantageous when trying to optimize physicochemical properties). \n\n\n## Objective\n\nUse the DHFR inhibitors dataset to construct a binary classifier to predict active/inactive inhibitor compounds."},{"metadata":{},"cell_type":"markdown","source":"Import standard data processing and visualization libraries."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nDATA_PATH = \"/kaggle/input/dihydrofolate-reductase-inhibitors-data-dhfr/dhfr.csv\"\ndhfr_data = pd.read_csv(DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dhfr_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dhfr_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"228/229 column data series consists of \"float64\" data type. Referring to above head method, we can identify the \"object\" to belong to column \"Y\". Moreover, since the data was loaded using pandas' read_csv method, \"object\" must be text. We confirm this above where \"Y\" values are either \"active\" or \"inactive\"."},{"metadata":{"trusted":false},"cell_type":"code","source":"dhfr_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot histograms to visualize the data distribution."},{"metadata":{"trusted":false},"cell_type":"code","source":"#qsar_metric_names = dhfr_data.columns.values\n\n#sb.set_theme()\n\n#for name in qsar_metric_names:\n    #dhfr_data[name].hist()\n    #plt.title(name)\n    #plt.xlabel(\"QSAR Feature Value\")\n    #plt.ylabel(\"Total Count\")\n    #plt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the histogram of the distribution of the \"Y\" label, inhibitor activity again. We can obtain a starting metric representing the best performance of a classifier that predicts activity at random. In the cell block below, we see that 203/325 (majority label) entries are \"active\" inhibitors. Therefore, a classifer that predicts \"active\" for every inhibitor could achieve ~62.5% accuracy (203/325). The classifier must achieve a greater accuracy."},{"metadata":{"trusted":false},"cell_type":"code","source":"dhfr_data[\"Y\"].hist()\nplt.title(\"DHFR Inhibitors\")\nplt.xlabel(\"Inhibitor Activity\")\nplt.ylabel(\"Total Amount\")\nplt.figure()\n\nprint(dhfr_data[\"Y\"].value_counts())\n# best classifier performance if it were to guess \"active\" every time\nprint(f'\\nBest classifier performance if it predicts \"active\" for all inhibitors: {203/325}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Splitting\n\nTo prepare for model training, let's split the dataset into 3 sets: train/validation/test (60/20/20). The validation set will be used during hyperparameter fine-tuning. It is important that the test set is left untouched until model evaluation to obtain a true estimate of the model's ability to generalize."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# extract the features and the label\nX = np.array(dhfr_data.drop(\"Y\", axis=1))\ny = np.array(dhfr_data[\"Y\"])\n\n# generate the train/test sets following an 80/20 split\n# random_state=42 for reproducibility and shuffling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# split the train sets further to generate a validations set\n# following an overall train/validation/test splitting of 60/20/20\n# random_state=42 for reproducibility\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n\n# verify the splitting is 60/20/20 \nprint(f\"Training set proportion: {len(X_train)/325}\")\nprint(f\"Validation set proportion: {len(X_val)/325}\")\nprint(f\"Test set proportion: {len(X_test)/325}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Processing\n\nSupport Vector Machine (SVM) will be one of the models we fit. It is sensitive to feature scales and therefore, it is a good idea to standardize the quantitative features. In addition, we must handle the categorical label, \"Y\" which again denotes either \"active\" or \"inactive\". \n\nIt is important that all the feature matrices (X_train, X_val, X_test) are standardardized. However, attention needs to be given to standardizing all the sets with the mean and standard deviation of X_train only. This is to avoid data leakage that could bias the X_val and X_test into giving better generalization errors than reality."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd_scalar = StandardScaler()\n\n# standardize X_train with its mean and standard deviation \nX_train = std_scalar.fit_transform(X_train)\n# use only transform so the X_train mean and standard deviation is maintained\nX_val = std_scalar.transform(X_val)\nX_test = std_scalar.transform(X_test)\n\n# write a function to binarize the label matrices instead of using a sklearn class such as LabelBinarizer\n# this is to control the value mappings to follow the convention in drug discovery\n# \"active\" -> 1 and \"inactive\" -> 0\ndef activity_enc(label_matrix: np.array) -> np.array:\n    \n    binarized_label = []\n    \n    for label in label_matrix:\n        if label == \"inactive\":\n            binarized_label.append(0)\n        if label == \"active\":\n            binarized_label.append(1)\n            \n    return np.array(binarized_label)\n\ny_train = activity_enc(y_train)\ny_val = activity_enc(y_val)\ny_test = activity_enc(y_test)\n\n# verify the labels are binarized as intended\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training: LinearSVC, SVC Polynomial Kernel, Random Forest\n\nWe are now ready for model training! Fit LinearSVC, Polynomial SVC, and Random Forest models for binary classification."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# LinearSVC: use default hyperparameters except max_iter to ensure convergence\nlin_svc = LinearSVC(C=1, loss=\"hinge\", max_iter=10000)\n# Polynomial SVC\npoly_svc = SVC(kernel=\"poly\", degree=3, coef0=1, C=1, probability=True)\n# Random Forest: random_state=42 for reproducibility\nrand_forest = RandomForestClassifier(random_state=42)\n\n# fit models\nlin_svc.fit(X_train, y_train.ravel())\npoly_svc.fit(X_train, y_train.ravel())\nrand_forest.fit(X_train, y_train.ravel())\n\n# obtain the train, val, and test set accuracies for all models\nlin_svc_train_score = lin_svc.score(X_train, y_train)\nlin_svc_val_score = lin_svc.score(X_val, y_val)\nlin_svc_test_score = lin_svc.score(X_test, y_test)\n\npoly_svc_train_score = poly_svc.score(X_train, y_train)\npoly_svc_val_score = poly_svc.score(X_val, y_val)\npoly_svc_test_score = poly_svc.score(X_test, y_test)\n\nrand_forest_train_score = rand_forest.score(X_train, y_train)\nrand_forest_val_score = rand_forest.score(X_val, y_val)\nrand_forest_test_score = rand_forest.score(X_test, y_test)\n\n# contruct a DataFrame to display model accuracy performance\nperformance_df = pd.DataFrame({\n\"Model\": [\"LinearSVC\", \"SVC Poly Kernel\", \"Random Forest\"],\n\"Train Acc.\": [lin_svc_train_score, poly_svc_train_score, rand_forest_train_score],\n\"Val. Acc.\": [lin_svc_val_score, poly_svc_val_score, rand_forest_val_score],\n\"Test Acc.\": [lin_svc_test_score, poly_svc_test_score, rand_forest_test_score],\n\"Val. Error\": \n[f\"{round((((lin_svc_train_score-lin_svc_val_score))/lin_svc_train_score*100),2)}%\",\nf\"{round((((poly_svc_train_score-poly_svc_val_score))/poly_svc_train_score*100),2)}%\",\nf\"{round((((rand_forest_train_score-rand_forest_val_score))/rand_forest_train_score*100),2)}%\"],\n\"Test Error\":\n[f\"{round((((lin_svc_train_score-lin_svc_test_score))/lin_svc_train_score*100),2)}%\",\nf\"{round((((poly_svc_train_score-poly_svc_test_score))/poly_svc_train_score*100),2)}%\",\nf\"{round((((rand_forest_train_score-rand_forest_test_score))/rand_forest_train_score*100),2)}%\"]})\n\nprint(performance_df.to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, the accuracy on the validation and test sets is high for all 3 models. However, there is notable generalization error in the case of the LinearSVC and Random Forest models, suggesting overfitting. It is perhaps not surprising that the Random Forest model achieved perfect accuracy on the training set with decreased accuracies on the validation and test sets as no hyperparameter limitations were enforced during fitting. Consequently, the model was given total freedom in tree depth, nodes, etc. which is prone to lead to overfitting. Let's use cross-validation to investigate what the average generalization error is. This would give a better estimate of the generalization error to give us a starting point to improve the models. The goal is to train a model whose performance on the test set is as close as possible to the training set performance, thus minimizing generalization error.  \n\nFirst, re-generate the train/test sets following at 80/20 split. The validation set is not explictly generated because subsequent cross-validation takes care of that."},{"metadata":{"trusted":false},"cell_type":"code","source":"# random_state=42 for reproducibility and shuffling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# perform the same feature processing as before\nX_train = std_scalar.fit_transform(X_train)\nX_test = std_scalar.transform(X_test)\n\ny_train = activity_enc(y_train)\ny_test = activity_enc(y_test)\n\n# verify the splitting is 80/20\nprint(f\"Training set proportion: {len(X_train)/325}\")\nprint(f\"Test set proportion: {len(X_test)/325}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# use cv=4 so the validation set represents 20% of the overall dataset \n# since the train set is 80% of the overall dataset\nlin_svc_cross_val_scores = cross_val_score(lin_svc, X_train, y_train.ravel(), cv=4)\npoly_svc_cross_val_scores = cross_val_score(poly_svc, X_train, y_train.ravel(), cv=4)\nrand_forest_cross_val_scores = cross_val_score(rand_forest, X_train, y_train.ravel(), cv=4)\n\nprint(f\"LinearSVC Cross-Validation Validation Set Scores: {lin_svc_cross_val_scores}\")\nprint(f\"LinearSVC Cross-Validation Validation Set Scores Mean: {lin_svc_cross_val_scores.mean()}\\n\")\nprint(f\"Poly SVC Cross-Validation Validation Set Scores: {poly_svc_cross_val_scores}\")\nprint(f\"Poly SVC Cross-Validation Validation Set Scores Mean: {poly_svc_cross_val_scores.mean()}\\n\")\nprint(f\"Random Forest Cross-Validation Validation Set Scores: {rand_forest_cross_val_scores}\")\nprint(f\"Random Forest Cross-Validation Validation Set Scores Mean: {rand_forest_cross_val_scores.mean()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross-validation scores differ based on the different training folds used, as expected. The mean of all the LinearSVC models is ~ 88.46% which is quite similar to the generalization error when we trained the model above with the pre-generated and constant validation set (as opposed to cross-validation folds validation sets). In contrast, the mean cross-validation score for the Poly SVC and Random Forest models is ~ 93.08 for both which is notably higher. Morevoer, in 1 case for the Ramdom Forest model, the validation accuracy is >95%. Taken together, these observations suggest that the LinearSVC model is overfitting, the Poly SVC performs better and only slightly overfits, and the Random Forest model although overfitting, can perform well given better training subsets.\n\nThe performance of all 3 models may be improved following hyperparameter optimization which can also reduce overfitting:\n\n1) LinearSVC\n* The hyperparameter, C controls the width of the support vector margins\n* In general, reducing C widens the support vector margins and can decrease overfitting\n* Adjusting C in our case will probably not reduce the generalization error as the model has already been trained\n  with C=1 (default and lowest possible value of C)\n  \n2) Poly SVC\n* The hyperparameter, C controls the width of the support vector margins\n* The hyperparameter, degree controls the degree of polynomialization which adds features.\n  High polynomial degrees can increase accuracy but leads to combinatorial explosion and can cause overfitting.\n  Reducing the polynomial degree can reduce overfitting\n* coef0 controls the polynomial features' affect on the model performance\n* Since the Poly SVC model generalizes relatively well, the accuracy could be improved by increasing the polynomial degree   with a relatively lower risk of overfitting \n\n3) Random Forest\n* Many hyperparameters --> below are 2 important ones\n* n_estimators denoting the number of trees \n* max_features denoting the max number of features to consider when splitting \n* The majority of the hyperparameters used to train the Random Forest were static (ex. n_estimators was kept default     which is 100 from sklearn V0.22+)\n* Since the Random Forest model is slightly overfitting, tuning the hyperparameters could reduce the generalization     error and lead to an overall better classifier\n\nLet's try tuning the 3 models."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# LinearSVC hyperparameters to try\nlin_svc_param_grid = [{\"C\": list(range(1,11))}]\n\n# Poly SVC hyperparameter combinations to try\npoly_svc_param_grid = [{\"C\": list(range(1,11)), \n                        \"degree\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n                        \"coef0\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,\n                                  1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]}]\n\n# Random Forest hyperparameter combinations to try (there are many other hyperparameters that are not tried here)\nrand_forest_param_grid = [{\"n_estimators\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n                           \"max_features\": [2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n\n# Fit all 3 models to find the best hyperparameters (cv=4 so the validation set is 20% of the overall dataset)\nlin_svc_gscv = GridSearchCV(lin_svc, param_grid=lin_svc_param_grid, cv=4)\npoly_svc_gscv = GridSearchCV(poly_svc, param_grid=poly_svc_param_grid, cv=4)\nrand_forest_gscv = GridSearchCV(rand_forest, param_grid=rand_forest_param_grid, cv=4)\n\n# re-train all 3 models with the best hyperparameters found\nlin_svc_gscv.fit(X_train, y_train.ravel())\npoly_svc_gscv.fit(X_train, y_train.ravel())\nrand_forest_gscv.fit(X_train, y_train.ravel())\n\n# display the best hyperparameters from the combinations tried\nprint(f\"LinearSVC Best Hyperparameters Found: {lin_svc_gscv.best_params_}\")\nprint(f\"Poly SVC Best Hyperparameters Found: {poly_svc_gscv.best_params_}\")\nprint(f\"Random Forest Best Hyperparameters Found: {rand_forest_gscv.best_params_}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best hyperparameters found are displayed. Importantly, none of the hyperparameter values are at the extreme values tried (ex. Random Forest max_features=6 out of the possible vaues 2-10). If the fine-tuned hyperparameters were at the extreme values (ex. Random Forest max_features=10), then it could be worthwhile to expand the hyperparameter search space.\n\nLet's next evaluate the fine-tuned models on the test set and generate a DataFrame to compare the models' performances pre-fine-tuning and post-fine-tuning."},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract the performance score of all 3 models on the test set\nlin_svc_gscv_test_score = lin_svc_gscv.score(X_test, y_test)\npoly_svc_gscv_test_score = poly_svc_gscv.score(X_test, y_test)\nrand_forest_gscv_test_score = rand_forest_gscv.score(X_test, y_test)\n\n# contruct a DataFrame to compare all 3 models' performance following hyperparameter fine-tuning\nfine_tuned_performance_df = pd.DataFrame({\n\"Model\": [\"LinearSVC\", \"SVC Poly Kernel\", \"Random Forest\"],\n\"Prev. Train Acc.\": [lin_svc_train_score, poly_svc_train_score, rand_forest_train_score],\n\"Prev. Test Acc.\": [lin_svc_test_score, poly_svc_test_score, rand_forest_test_score],\n\"Prev. Test Error\":\n[f\"{round((((lin_svc_train_score-lin_svc_test_score))/lin_svc_train_score*100),2)}%\",\nf\"{round((((poly_svc_train_score-poly_svc_test_score))/poly_svc_train_score*100),2)}%\",\nf\"{round((((rand_forest_train_score-rand_forest_test_score))/rand_forest_train_score*100),2)}%\"],\n\"New Train Acc.\": [lin_svc_gscv.best_score_, poly_svc_gscv.best_score_, rand_forest_gscv.best_score_],\n\"New Test Acc.\": [lin_svc_gscv_test_score, poly_svc_gscv_test_score, rand_forest_gscv_test_score],\n\"New Test Error\":\n[f\"{round((((lin_svc_gscv.best_score_-lin_svc_gscv_test_score))/lin_svc_gscv.best_score_*100),2)}%\",\nf\"{round((((poly_svc_gscv.best_score_-poly_svc_gscv_test_score))/poly_svc_gscv.best_score_*100),2)}%\",\nf\"{round((((rand_forest_gscv.best_score_-rand_forest_gscv_test_score))/rand_forest_gscv.best_score_*100),2)}%\"]})\n\nprint(fine_tuned_performance_df.to_string(index=False))                   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The generalization errors for all 3 models improved a lot! \n\n1) LinearSVC\n* The LinearSVC performance on the test set stayed the same \n* The fine-tuned LinearSVC training accuracy decreased but this is due to taking the average training score from \n  c=4 cross-validation as opposed to training using the entire training set. The more important comparison is the\n  relative performance on the test set and we see that the model now generalizes much better. In fact, the \"New Test     Error\" is negative because the performance on the test set exceeds the training set.\n  \n2) Poly SVC\n* Similar to LinearSVC, the training accuracy decreased. However, the test accuracy is identical to the previous test   accuracy, reducing the generalization error.\n* The slight overfitting previously has been reduced (at least for this test set. This is no guarantee the model will perform as well on completely new data.)\n\n3) Random Forest\n* Training accuracy decreased but test accuracy increased, yielding a much lower generalization error (2.04%)\n* The model was overfitting before (again, maybe expected as the hyperparameters were given total freedom for tree       growth) but now we have reduced the overfitting to yield a model that can generalize much better\n\nBased on these metrics alone, it seems that the Random Forest would be the model of choice given the best test set accuracy and generalization. However, it is useful to look at metrics beyond accuracy especially as this is a classification task. Let's take a look at the confusion matrix along with the precision and recall scores. These metrics will offer further insights into the models' performances."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\n\n# obtain the predictions on X_train using the fine-tuned hyperparameters\nlin_svc_gscv_y_train_pred = lin_svc_gscv.predict(X_train)\npoly_svc_gscv_y_train_pred = poly_svc_gscv.predict(X_train)\nrand_forest_gscv_y_train_pred = rand_forest_gscv.predict(X_train)\n\nprint(f\"Fine-Tuned LinearSVC X_train Confusion Matrix: \\n {confusion_matrix(y_train, lin_svc_gscv_y_train_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {lin_svc_gscv.best_score_}\")\nprint(f\"Precision Score: {precision_score(y_train, lin_svc_gscv_y_train_pred)}\")\nprint(f\"Recall Score: {recall_score(y_train, lin_svc_gscv_y_train_pred)}\\n\")\n\nprint(f\"Fine-Tuned Poly SVC X_train Confusion Matrix: \\n {confusion_matrix(y_train, poly_svc_gscv_y_train_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {poly_svc_gscv.best_score_}\")\nprint(f\"Precision Score: {precision_score(y_train, poly_svc_gscv_y_train_pred)}\")\nprint(f\"Recall Score: {recall_score(y_train, poly_svc_gscv_y_train_pred)}\\n\")\n\nprint(f\"Fine-Tuned Random Forest X_train Confusion Matrix: \\n {confusion_matrix(y_train, rand_forest_gscv_y_train_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {rand_forest_gscv.best_score_}\")\nprint(f\"Precision Score: {precision_score(y_train, rand_forest_gscv_y_train_pred)}\")\nprint(f\"Recall Score: {recall_score(y_train, rand_forest_gscv_y_train_pred)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precision and recall scores for all 3 models trained with the fine-tuned hyperparameters are nearly perfect for the training set! Note that there seems to be a discrepancy between the accuracy and the precision and recall scores. For example, the LinearSVC confusion matrix only displays 1 misclassification which is reflected correctly in the Recall score < 1. However, how is it that the \"accuracy\" is ~88.5% if the model only made 1 misclassification? The reason is because the accuracy score represents the average accuracy score obtained following cross-validation. Recall the training set which made up 80% of the overall dataset was fed into GridSearchCV. Cross-validation used 4 folds and therefore, each model was trained on some combination of 3 folds (60% of the training data) and validated on the remaining fold (20% of the training data). The accuracy represents the average accuracy of this cross-validation. Moreover, once the optimal hyperparameters were determined by GridSearchCV, the models were re-fit using the entire training set (80% of the overall data). These two factors taken together explain the \"discrepancy\" in the accuracy. In reality, GridSearchCV improved the models as shown in the previous DataFrame comparing the generalization errors pre and post hyperparameter fine-tuning. Overfitting was reduced and the models generalize on new data better.\n\nLet's now calculate the precision and recall scores on the test data and observe how the training set performance carries over to test set performance."},{"metadata":{"trusted":false},"cell_type":"code","source":"# obtain the predictions on X_test using the fine-tuned hyperparameters\nlin_svc_gscv_y_test_pred = lin_svc_gscv.predict(X_test)\npoly_svc_gscv_y_test_pred = poly_svc_gscv.predict(X_test)\nrand_forest_gscv_y_test_pred = rand_forest_gscv.predict(X_test)\n\nprint(f\"Fine-Tuned LinearSVC X-test Confusion Matrix: \\n {confusion_matrix(y_test, lin_svc_gscv_y_test_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {lin_svc_gscv.best_score_}\")\nprint(f\"Test Set Accuracy: {lin_svc_gscv_test_score}\")\nprint(f\"Precision Score: {precision_score(y_test, lin_svc_gscv_y_test_pred)}\")\nprint(f\"Recall Score: {recall_score(y_test, lin_svc_gscv_y_test_pred)}\\n\")\n\nprint(f\"Fine-Tuned Poly SVC X_test Confusion Matrix: \\n {confusion_matrix(y_test, poly_svc_gscv_y_test_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {poly_svc_gscv.best_score_}\")\nprint(f\"Test Set Accuracy: {poly_svc_gscv_test_score}\")\nprint(f\"Precision Score: {precision_score(y_test, poly_svc_gscv_y_test_pred)}\")\nprint(f\"Recall Score: {recall_score(y_test, poly_svc_gscv_y_test_pred)}\\n\")\n\nprint(f\"Fine-Tuned Random Forest X_test Confusion Matrix: \\n {confusion_matrix(y_test, rand_forest_gscv_y_test_pred)}\")\nprint(f\"Average Cross-Validation Accuracy: {rand_forest_gscv.best_score_}\")\nprint(f\"Test Set Accuracy: {rand_forest_gscv_test_score}\")\nprint(f\"Precision Score: {precision_score(y_test, rand_forest_gscv_y_test_pred)}\")\nprint(f\"Recall Score: {recall_score(y_test, rand_forest_gscv_y_test_pred)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precision and recall scores for all 3 models have decreased which is to be expected. In general, they are still very good. Note that the accuracy score is the true accuracy displayed by the confusion matrix and not the average accuracy from cross-validation as was the case in the cell block above. This is because the test set was untouched during training and was not fed into GridSearchCV.\n\nTaking the accuracy, precision, and recall metrics together provides insight into choosing the most suitable model to deploy. \n\n1) LinearSVC\n* Lowest cross-validation accuracy \n* Lowest/2nd lowest test set accuracy \n* Lowest test set precision\n* Highest/2nd highest test set recall\n\n2) Poly SVC\n* 2nd highest cross-validation accuracy\n* Lowest/2nd lowest test set accuracy \n* 2nd highest test set precision\n* Lowest recall\n\n3) Random Forest\n* Highest cross-validation accuracy\n* Highest test set accuracy \n* Highest test set precision\n* Highest recall\n\nBased on the observations on accuracy and generalization error, one would probably choose the Random Forest model over the LinearSVC and Poly SVC. This choice is further supported by the test set precision and recall scores where the Random Forest outperforms the other two models.\n\nAs a final note, if the precision and recall of the models were much lower, one could look at tuning these models by increasing either the precision or recall (based on the objective of the classifier), which exhibit an inverse relationship. From a drug discovery perspective, it is probably preferable to maximize recall (minimize the number of \"active\" inhibitors missed and let a few false positives through). Doing so will curate a library of presumably \"active\" compounds which can then be further filtered (ex. based on synthetic feasibility and/or physicochemical properties). This of course assumes a reasonable precision is maintained or else we run into the problem of wasting time and resources to test inhibitors which have a high probability of being \"inactive\". \n\nTo visualize the precision-recall trade-off, let's plot precision-recall curves."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nrand_forest_gscv_y_decision_scores = rand_forest_gscv.predict_proba(X_train)\npoly_svc_gscv_y_decision_scores = poly_svc_gscv.predict_proba(X_train)\n\npoly_svc_gscv_precision, poly_svc_gscv_recall, poly_svc_gscv_thresholds = precision_recall_curve(y_train, poly_svc_gscv_y_decision_scores[:, 1])\nrand_forest_gscv_precision, rand_forest_gscv_recall, rand_forest_gscv_thresholds = precision_recall_curve(y_train, rand_forest_gscv_y_decision_scores[:, 1])\n\ndef precision_recall_curve(precision_1, recall_1, precision_2, recall_2, clf_name, clf2_name):\n    plt.figure(figsize=(12, 8)); plt.rc(\"xtick\", labelsize=16); plt.rc(\"ytick\", labelsize=16)\n    plt.plot(recall_1, precision_1, \"y-\", linewidth=2, label=clf_name)\n    plt.plot(recall_2, precision_2, \"g-\", linewidth=2, label=clf2_name)\n    plt.title(\"Precision-Recall Curve\", fontsize=18)\n    plt.xlabel(\"Recall\", fontsize=18)\n    plt.ylabel(\"Precision\", fontsize=18)\n    plt.legend(fontsize=18)\n    plt.plot([0.96, 0.96], [0.96, 1], \"r--o\")\n    plt.figure()\n\nprecision_recall_curve(poly_svc_gscv_precision, poly_svc_gscv_recall, \n                       rand_forest_gscv_precision, rand_forest_gscv_recall,\n                       \"Poly SVC\", \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precision-recall curves are not as interesting since the models already perform so well. However, it nicely illustrates the precision-recall trade-off. In the case of the Poly SVC model, the training precision and recall scores are as follows:\n\nPrecision Score: 0.9651162790697675\nRecall Score: 1.0\n\nIf we wanted to increase precision to 1.0, the recall would drop to ~0.96 as shown by the red-dotted line in the plot. Note this is not desired for the inhibitor activity classifier as we want to maximize recall. \n\nLet's next generate Receiver Operating Characteristic (ROC) curves."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\npoly_svc_gscv_y_decision_scores = poly_svc_gscv.predict_proba(X_train)\nrand_forest_gscv_y_decision_scores = rand_forest_gscv.predict_proba(X_train)\n\npoly_svc_gscv_false_pos_rates, poly_svc_gscv_true_pos_rates, poly_svc_gscv_thresholds = roc_curve(y_train, poly_svc_gscv_y_decision_scores[:, 1])\nrand_forest_gscv_false_pos_rates, rand_forest_gscv_true_pos_rates, rand_forest_gscv_thresholds = roc_curve(y_train, rand_forest_gscv_y_decision_scores[:, 1])\n\ndef roc_curve(false_pos_rates_1, true_pos_rates_1, false_pos_rates_2, true_pos_rates_2, clf_name, clf2_name):\n    plt.figure(figsize=(12, 8)); plt.rc(\"xtick\", labelsize=16); plt.rc(\"ytick\", labelsize=16)\n    plt.plot(false_pos_rates_1, true_pos_rates_1, \"y-\", linewidth=5, label=clf_name)\n    plt.plot(false_pos_rates_2, true_pos_rates_2, \"g-\", linewidth=5, label=clf2_name)\n    plt.title(\"Receiver Operating Characteristic (ROC) Curve\", fontsize=18)\n    plt.xlabel(\"False Positive Rate\", fontsize=18)\n    plt.ylabel(\"True Positive Rate (Recall)\", fontsize=18)\n    plt.legend(fontsize=18, loc=\"lower right\")\n    plt.axis([0, 1, 0, 1]) \n    plt.figure()\n    \nroc_curve(poly_svc_gscv_false_pos_rates, poly_svc_gscv_true_pos_rates, \n          rand_forest_gscv_false_pos_rates, rand_forest_gscv_true_pos_rates, \n          \"Poly SVC\", \"Random Forest\")\nprint(f\"Poly SVC ROC AUC Score: {roc_auc_score(y_train, poly_svc_gscv_y_decision_scores[:, 1])}\")\nprint(f\"Random Forest ROC AUC Score: {roc_auc_score(y_train, rand_forest_gscv_y_decision_scores[:, 1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again the ROC curves are not as interesting since the models perform so well. The perfect classifier would be to the top left of the plot which is achieved by the Random Forest model. \"Perfect\" here is defined by the performance on the train set. As we observed previously, the Random Forest model does not generalize as well and its performance is certainly not perfect on the test set. Nonetheless, both models perform well on the train set and is quantitatively demonstrated by their Area Under Curve (AUC) scores. Based on the scores alone, the Random Forest model is just so ever slightly better than the Poly SVC model on the train set.\n\nFinally, we re-visit the Random Forest as this is the model we choose to deploy. It possesses the best cross-validation accuracy, the best generalization performance, and the best recall and precision on the test set. These metrics are displayed one final time in the cell block below."},{"metadata":{"trusted":false},"cell_type":"code","source":"# contruct a DataFrame to compare all 3 models' performance following hyperparameter fine-tuning\nfine_tuned_performance_df = pd.DataFrame({\n\"Model\": [\"Random Forest\"],\n\"Fine-Tuned Train Acc.\": [rand_forest_gscv.best_score_],\n\"Fine-Tuned Test Acc.\": [rand_forest_gscv_test_score],\n\"Fine-Tuned Test Error\": \nf\"{round((((rand_forest_gscv.best_score_-rand_forest_gscv_test_score))/rand_forest_gscv.best_score_*100),2)}%\"})\n                                \nprint(fine_tuned_performance_df.to_string(index=False))\n\nprint(f\"\\nFine-Tuned Random Forest X_train Confusion Matrix: \\n {confusion_matrix(y_train, rand_forest_gscv_y_train_pred)}\")\nprint(f\"Train Set Precision Score: {precision_score(y_train, rand_forest_gscv_y_train_pred)}\")\nprint(f\"Train set Recall Score: {recall_score(y_train, rand_forest_gscv_y_train_pred)}\\n\")\n\nprint(f\"Fine-Tuned Random Forest X_test Confusion Matrix: \\n {confusion_matrix(y_test, rand_forest_gscv_y_test_pred)}\")\nprint(f\"Test Precision Score: {precision_score(y_test, rand_forest_gscv_y_test_pred)}\")\nprint(f\"Test Recall Score: {recall_score(y_test, rand_forest_gscv_y_test_pred)}\\n\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}