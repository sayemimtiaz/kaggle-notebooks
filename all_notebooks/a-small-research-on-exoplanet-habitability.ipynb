{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Habitability of Exoplanets :\n\nThis is a fun-filled notebook for all amateur astronomy lovers. I found the website [\"Planetary Habitability Laboratory\"](http://phl.upr.edu/) very informative for keeping oneself updated about the habitable planets. In this project, I have used their latest [catalog](http://phl.upr.edu/projects/habitable-exoplanets-catalog/data/database). The provided link has all data field descriptions in details.\n\nThe target variable is **\"P_HABITABLE\"** which is having values 0 (inhabitable) or 1 (conservatively habitable) or 2 (optimistically habitable). Initially, we will also explore which interesting features are responsible for making an exoplanet (a planet outside our solar system) habitable. Then we will divide the data set into 80% training data and 20% testing data in order to judge performance of different classification models. \n\nIn the end, we also experimented with unsupervised learning after removing the labels and checked whether it is really giving rise to three distinct clusters of planets or not.\n\n**DATA ACKNOWLEDGEMENT: PHL's Exoplanet Catalog of the Planetary Habitability Laboratory @ UPR Arecibo.**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing data\nimport pandas as pd\nfull_data = pd.read_csv('../input/phl-exoplanet-catalog/phl_exoplanet_catalog_2019.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"full_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (9,6))\nfull_data.P_HABITABLE.value_counts(normalize = True, ascending = False).plot(kind='bar', color= ['navy','orange','green'], alpha = 0.8, rot=0)\nplt.title('Habitability Indicator No (0) / Conservatively Yes (1) / Optimistically Yes (2) in the Imbalanced Dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['P_HABITABLE'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Initial Exploration with DABL Library**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install dabl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function() {\n    return False;\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dabl\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\ndabl.plot(full_data, target_col = 'P_HABITABLE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resolving Class Imbalance with Simple Oversampling Strategy\n\nClearly the data set is imbalanced having 98.64% inhabitable planets. Only 0.84% and 0.52% are conservatively habitable and optimistically habitable planets respectively. For getting proper performance of the ML models, we need to balance the data set first where each class will be having the same proportion of representation. We are using simple oversampling technique (resampling strategy) for that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\nno = full_data[full_data.P_HABITABLE == 0]\nyes_cons = full_data[full_data.P_HABITABLE == 1]\nyes_opti = full_data[full_data.P_HABITABLE == 2]\nyes_cons_oversampled = resample(yes_cons, replace=True, n_samples=len(no), random_state=12345)\noversampled = pd.concat([no, yes_cons_oversampled])\nyes_opti_oversampled = resample(yes_opti, replace=True, n_samples=len(no), random_state=12345)\noversampled = pd.concat([oversampled, yes_opti_oversampled])\n\nfig = plt.figure(figsize = (9,6))\noversampled.P_HABITABLE.value_counts(normalize = True, ascending = False).plot(kind='bar', color= ['navy','orange','green'], alpha = 0.8, rot=0)\nplt.title('Habitability Indicator No (0) / Conservatively Yes (1) / Optimistically Yes (2) after Oversampling (Balanced Dataset)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampled['P_HABITABLE'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can observe that each class is having equal proportion of representation in the oversampled data set.\n\n# Data Analysis through Visualization :\n\n* **Planetary Detection Method:**\n\nWe will explore which planetary detection methods have been used extensively (by the word \"extensive\" we assume: more than 5 planets have been discovered using the method) over the years for discovering exoplanets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"by_p_detec = (oversampled\n            .groupby('P_DETECTION')\n            .filter(lambda x : len(x) > 5)\n            .groupby(['P_DETECTION', 'P_YEAR'])\n            .size()\n            .unstack()\n           )\nimport seaborn as sns\nplt.figure(figsize=(12,12))\ng = sns.heatmap(\n    by_p_detec, \n    square=True, \n    cbar_kws={'fraction' : 0.01}, \n    cmap='OrRd',\n    linewidth=1\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe *Transit* and *Radial Velocity* planetary detection techniques have been used most extensively for discovering exoplanets. Now, we will observe, how many planets detected by these two methods have posibility of habitability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nwith sns.axes_style(style='ticks'):\n    g = sns.catplot(\"P_HABITABLE\", col=\"P_DETECTION\", col_wrap=3, data=oversampled[oversampled['P_DETECTION'].isin(['Radial Velocity','Transit'])], kind=\"count\", height=3.0, aspect=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that *Radial Velocity* technique has been used more to detect conservatively habitable exoplanets. *Transit* technique has been used well to detect both optimistically habitable exoplanets and non-habitable exoplanets.\n\n* **Stars and Planets Discovery over the Years:** \n\nNext, we will explore which planets (under which star) were discovered in which year. This will clarify, how the discovery of exoplanets progressed over the years. All the stars (under whom the planets are listed) belong to different solar systems (not the solar system ruled by Sun). Here, we will plot only the stars which have more than 10 planets attached to them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"by_p_s_name = (oversampled\n            .groupby('S_NAME')\n            .filter(lambda x : len(x) > 10)\n            .groupby(['P_NAME', 'S_NAME','P_YEAR'])\n            .size()\n            .unstack()\n           )\n\nplt.figure(figsize=(30,30))\ng = sns.heatmap(\n    by_p_s_name, \n    square=True, \n    cbar_kws={'fraction' : 0.01}, \n    cmap='OrRd', \n    linewidth=1 \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see, for the star GJ667, planets were discovered in older years (2008-2009). For Kepler series stars, the planets were discovered in between years 2011 and 2014. For Trappist and Teegarden series stars, the planets have been discovered in very recent years (2017-2019).\n\nFor better exploration, we will consider P_UPDATED date to observe progress of star discovery and planet discovery over the years. We will see a steep increase in trend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stars_by_date = oversampled.groupby('P_UPDATED')['S_NAME'].sum()\nstars_by_date.iplot(kind='scatter', title='Discovery of Stars outside Solar System over the Years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"planets_by_date = oversampled.groupby('P_UPDATED')['P_NAME'].sum()\nplanets_by_date.iplot(kind='bar', title='Discovery of Exoplanets outside Solar System over the Years')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Influence of Planet Type / Star Spectral Type / Planet Thermal Type :**\n\nNow, we will explore how some important features like planet type *(P_TYPE)*, star spectral type *(S_TEMP_TYPE)* and planet thermal type *(P_TYPE_TEMP)* influence the habitability of the exoplanets. Interested folks can go through about the detailed meaning of these features below.\n\n**Planet Type:** (Jovian, Superterran, Neptunian, Subterran, Terran, Miniterran). Their atmosphere can be potentially different. Terrans are able to hold a significant atmosphere with liquid water within the habitable zone (Eg: Earth). Superterrans are able to hold dense atmospheres with liquid water within the habitable zone. Subterrans are able to hold a significant atmospheres after the outer edges of the habitable zone (Eg: Mars). Jovians can have superdense atmospheres in the hot zone. Neptunians can have dense atmospheres in the hot zone. For details, check [Mass-Radius Classification of Exoplanets](http://phl.upr.edu/library/notes/amassclassificationforbothsolarandextrasolarplanets)\n\n**Star Spectral Type (A,B,F,G,K,M,O):** This classification is based mostly on Hydrogen absorption lines. Class A: Stars with darkest H absorbtion lines; Class B: Stars with not as dark lines as Class A; Classes later in the alphabet correspond to weaker lines. Ref: [Classification by Williamina Fleming's group](https://sites.ualberta.ca/~pogosyan/teaching/ASTRO_122/lect12/lecture12.html)\n\n**Planet Thermal Type :** Cold, Hot, Warm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nwith sns.axes_style(style='ticks'):\n    g = sns.catplot(\"P_HABITABLE\", col=\"P_TYPE\", col_wrap=3, data=oversampled, kind=\"count\", height=2.5, aspect=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"P_HABITABLE\", col=\"S_TYPE_TEMP\", col_wrap=4, data=oversampled, kind=\"count\", height=2.5, aspect=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"P_HABITABLE\", col=\"P_TYPE_TEMP\", col_wrap=3, data=oversampled, kind=\"count\", height=2.5, aspect=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, planet type *superterran* has high number of optimistically habitable planets and planet type *terran* has high number of conservatively habitable planets. Star spectral type *M* has high number of both conservatively habitable and optimistically habitable planets. Planet thermal type *warm* has very high number of both conservatively habitable and optimistically habitable planets.\n\n* **Habitable Planets Discovery Years :**\n\nNext, we will move to see over the years, how many number of habitable planets have been discovered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(y=\"P_YEAR\", hue=\"P_HABITABLE\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=oversampled, aspect=1.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notably, maximum number of conservatively habitable exoplanets have been discovered in 2013, 2017, 2019 respectively. And maximum number of optimistically habitable exoplanets have been discovered in 2014 and 2016 respectively.\n\n* **Earth Similarity Index of a Planet and Influence on Habitability :**\n\nNext, we will check for different planet type, how planet thermal type is influencing the earth similarity index, and whether a high earth similarity index (P_ESI) indicates a high possibility of habilitability or not. Interested folks can check the definition of earth similarity index below.\n\n*P_ESI:* The Earth Similarity Index is an open multiparameter measure of Earth-likeness for solar or extrasolar planets as a number between zero (no similarity) and one (identical to Earth) **(*Schulze-Makuch et al., 2011*)**. For more details, please visit [this link](http://phl.upr.edu/projects/earth-similarity-index-esi#:~:text=The%20Earth%20Similarity%20Index%20(ESI,et%20al.%2C%202011).).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"P_TYPE_TEMP\", y=\"P_ESI\", hue=\"P_HABITABLE\",\n            col=\"P_TYPE\", col_wrap=3, aspect=0.8,\n            kind=\"boxen\", data=oversampled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We infer that a high earth similarity index of an exoplanet actually corroborates with high possibility of habitability. For superterran planet type, when the planet thermal type is warm, it indicates a high number of optimistically habitable planets. For terran planet type, when planet thermal type is warm, it indicates a high number of conservatively habitable planets.\n\n* **Stellar Constellations having Potentially Habitable Planets :**\n\nNext, we will check the names of star constellations where most of the habitable exoplanets belong.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_size = 10\nplt.rcParams['xtick.labelsize'] = label_size \nchart = sns.catplot(\n    data=oversampled[oversampled['P_HABITABLE'].isin([1,2])],\n    x='S_CONSTELLATION',\n    kind='count',\n    palette='pastel',\n    col='P_HABITABLE',\n    aspect=1.2,\n)\nchart.set_xticklabels(rotation=65, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that solar constellation *Aquarius* has the maximum number of conservatively habitable exoplanets. *Cygnus* and *Lyra* have the maximum number of optimistically habitable exoplanets respectively. \n\n* **Time Frame when Planets with High ESI Discovered :**\n\nWell, now we will scrutinize the exact time frame when planets with high Earth Similarity Index (ESI) have been discovered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p_plot = sns.catplot(x=\"P_YEAR\", y=\"P_ESI\", hue=\"P_HABITABLE\", kind=\"point\", data=oversampled, aspect=2.0)\np_plot.set_xticklabels(rotation=65)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Till 2011, only non-habitable planets having ESI less than or equal to 0.3 were discovered. From 2011 till 2019, habitable planets having high ESI (hovering in the range 0.6 - 0.9) have been discovered.\n\n* **Influence of Planet Mass / Planet Radius / Planet Eccentricity on Habitability:**\n\nNext, we will check how planet mass, planet radius and planet eccentricity influence habitability. While understanding mass and radius are pretty simple, understanding eccentricity can be difficult for some. Below is the definition of eccentricity for reference. \n\n[Eccentricity](https://www.enchantedlearning.com/subjects/astronomy/glossary/Eccentricity.shtml) indicates how the planetary orbit deviates from a perfect circle. A perfectly circular orbit has an eccentricity = 0. A higher number indicates elliptical orbit. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(oversampled, x=oversampled.P_MASS, y=oversampled.P_ESI, color=oversampled.P_HABITABLE,\n                 hover_name=oversampled.P_NAME, log_x=True, size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Planet mass in range 0.4 to 3.93 units correspond to conservatively habitable exoplanets. Planet mass in range 5.4 to 8.92 units correspond to optimistically habitable exoplanets. It is interesting to note that conservatively habitable exoplanets have a bit higher ESI compared to the ESI of optimistically habitable exoplanets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(oversampled, x=oversampled.P_RADIUS, y=oversampled.P_ESI, color=oversampled.P_HABITABLE,\n                 hover_name=oversampled.P_NAME, log_x=True, size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Planet radius in range 0.77 to 1.41 units correspond to conservatively habitable exoplanets. Planet radius in range 1.52 to 2.46 units correspond to optimistically habitable exoplanets. It is interesting to note that in most of the cases, conservatively habitable exoplanets have a bit higher ESI compared to the ESI of optimistically habitable exoplanets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(oversampled, x=oversampled.P_ECCENTRICITY, y=oversampled.P_ESI, color=oversampled.P_HABITABLE,\n                 hover_name=oversampled.P_NAME, log_x=True, size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Planet eccentricity in the range 0.02 to 0.35 units indicate a possibility of habitability. \n\n* **Influence of Planetary Flux on Habitability :**\n\nNext, we will check the influence of planet's stellar flux in habitability. For this purpose, we have filtered our data first to select only data pertaining to Earth Similarity Index > 0.65. To know how to calculate flux of a planet, check out [this link](http://www3.mpifr-bonn.mpg.de/div/hhertz/documents/smtoum/smtoum/node253.html).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = oversampled.query(\"P_ESI > 0.65\")\nfig = px.bar(df, x=\"P_NAME\", y=\"P_FLUX\", color=\"P_HABITABLE\") \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see very high flux (150-300+ units on Y-axis) is associated with conservatively habitable planets, and moderately high flux (hovering around 150 units on Y-axis) is associated with optimistically habitable planets. Non-habitable planets have very low flux (less than 5 units).\n\n* **Influence of Age of Planet on Habitability :**\n\nNext, we will proceed to check if the age of planet (P_PERIOD) has any significant influence on deciding its habitability. Again we have filtered data to select only the planets with ESI > 0.65","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = oversampled.query(\"P_ESI > 0.65\")\nfig = px.line(df, x=\"P_NAME\", y=\"P_PERIOD\", color=\"P_HABITABLE\", line_group=\"P_TYPE\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the conservatively habitable cluster has a period ranging between 9 days to 267 days. Planets under stars GJ 667, Kepler 62, Teegarden's Star, Trappist 1, Kepler 1229, GJ 1061 belong to this cluster. \n\nOptimistically habitable cluster has a period ranging between 18 days to 448 days. Planets under starts Kepler 1606, Kepler 1638, Kepler 1540, Kepler 1410, Kepler 1653, Kepler 296, K2-296, GJ 832, Kepler 1632, Kepler 1544, LHS 1140, Kepler 26, Kepler 62, Kepler 298, HD 40307 belong to this cluster.   \n\n# Preprocessing of Data :\n\n* **Missing Data Pattern and Imputation :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nsns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = oversampled.isnull().sum().sort_values(ascending=False)\npercent = (oversampled.isnull().sum()/oversampled.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice, from 'P_GEO_ALBEDO' till 'P_MASS' all the features are having > 50% missing values. Hence, we will discard those features to avoid bias.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data = oversampled.drop(['P_GEO_ALBEDO', 'P_DETECTION_MASS', 'P_DETECTION_RADIUS', 'P_ALT_NAMES', 'P_ATMOSPHERE', 'S_DISC', 'S_MAGNETIC_FIELD', \n                 'P_TEMP_MEASURED', 'P_GEO_ALBEDO_ERROR_MIN', 'P_GEO_ALBEDO_ERROR_MAX', 'P_TPERI_ERROR_MAX', 'P_TPERI_ERROR_MIN', 'P_TPERI', \n                 'P_DENSITY', 'P_ESCAPE', 'P_GRAVITY', 'P_POTENTIAL', 'P_OMEGA_ERROR_MAX', 'P_OMEGA_ERROR_MIN', 'P_OMEGA', 'P_INCLINATION_ERROR_MAX', \n                 'P_INCLINATION_ERROR_MIN', 'P_INCLINATION', 'P_ECCENTRICITY_ERROR_MAX', 'P_ECCENTRICITY_ERROR_MIN', 'S_AGE_ERROR_MIN', 'S_AGE_ERROR_MAX', \n                 'P_IMPACT_PARAMETER_ERROR_MIN', 'P_IMPACT_PARAMETER_ERROR_MAX', 'P_IMPACT_PARAMETER', 'P_MASS_ERROR_MAX', 'P_MASS_ERROR_MIN', 'P_HILL_SPHERE', \n                 'P_MASS'], axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The compact data set is having 77 features. Now we will identify the categorical columns which have missing values, and we will impute them first with mode.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data_obj = compact_data.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = compact_data_obj.isnull().sum().sort_values(ascending=False)\npercent = (compact_data_obj.isnull().sum()/compact_data_obj.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compact_data['S_TYPE'] = compact_data['S_TYPE'].fillna(compact_data['S_TYPE'].mode()[0])\ncompact_data['P_TYPE_TEMP'] = compact_data['P_TYPE_TEMP'].fillna(compact_data['P_TYPE_TEMP'].mode()[0])\ncompact_data['S_TYPE_TEMP'] = compact_data['S_TYPE_TEMP'].fillna(compact_data['S_TYPE_TEMP'].mode()[0])\ncompact_data['P_TYPE'] = compact_data['P_TYPE'].fillna(compact_data['P_TYPE'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Convert Categorical Features to Numerical :**\n\nNow, we will convert the categorical columns to numeric ones using label encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in compact_data.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    compact_data[col] = lencoders[col].fit_transform(compact_data[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will impute the missing values for entire data set (actually only the numeric ones, because we imputed the categorical ones using mode earlier) using MICE package. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = compact_data.copy(deep=True) \nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :] = mice_imputer.fit_transform(compact_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MiceImputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MiceImputed.isna().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Removing Multicollinearity :**\n\nNext, we will check whether perfect correlation exists among any feature pair. To avoid multicollinearity, we will exclude one and keep one from those pairs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = MiceImputed.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=False, linewidths=.5, cbar_kws={\"shrink\": 0.9})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We avoided annotation above to maintain clarity of the image, but the dark red squares are evidence of perfect correlation among the pairs of intersecting features. We will discard them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop perfectly correlated features\nworking_data = MiceImputed.drop(['S_NAME', 'P_RADIUS', 'P_RADIUS_ERROR_MIN', 'P_RADIUS_ERROR_MAX', 'P_DISTANCE', 'P_PERIASTRON', 'P_APASTRON', \n                                 'P_DISTANCE_EFF', 'P_FLUX_MIN', 'P_FLUX_MAX', 'P_TEMP_EQUIL', 'P_TEMP_EQUIL_MIN', 'P_TEMP_EQUIL_MAX', \n                                 'S_RADIUS_EST', 'S_RA_H', 'S_RA_T', 'S_LUMINOSITY', 'S_HZ_OPT_MIN', 'S_HZ_OPT_MAX', 'S_HZ_CON_MIN', \n                                 'S_HZ_CON_MAX', 'S_HZ_CON0_MIN', 'S_HZ_CON0_MAX', 'S_HZ_CON1_MIN', 'S_HZ_CON1_MAX', 'S_SNOW_LINE', \n                                'P_PERIOD_ERROR_MIN', 'P_PERIOD_ERROR_MAX', 'S_MAG', 'S_DISTANCE_ERROR_MIN', 'S_DISTANCE_ERROR_MAX', \n                                 'S_METALLICITY', 'S_METALLICITY_ERROR_MIN', 'S_METALLICITY_ERROR_MAX', 'S_AGE', 'S_TEMPERATURE_ERROR_MIN', \n                                 'S_TEMPERATURE_ERROR_MAX', 'S_ABIO_ZONE', 'P_ESI', 'S_CONSTELLATION_ABR', 'P_SEMI_MAJOR_AXIS_EST'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"working_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The working data set is having only 36 features at this stage. Now we will cross-check their correlation once again with annotation whether any square comes up with an annotation \"1\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heatmap for reduced working data set\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = working_data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": 0.9})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some high correlations are spotted but there is no perfect correlation i.e. \"1\". \n\n* **Removal of Outliers :**\n\nNext, we will proceed to identify the outliers using IQR (Inter Quartile Range) and will remove those.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detecting outliers with IQR\nQ1 = working_data.quantile(0.25)\nQ3 = working_data.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers from dataset\nworking_data = working_data[~((working_data < (Q1 - 1.5 * IQR)) |(working_data > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection :\n\nAfter completion of data preprocessing, we will select the really important features which are contributing towards habitability of the exoplanets. We will use permutation importance using Random Forest and wrapper method using Random Forest as well as Extra Trees classifier. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = working_data.drop('P_HABITABLE', axis=1)\ny = working_data['P_HABITABLE']\nperm = PermutationImportance(rf(n_estimators=10, random_state=0).fit(X,y),random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n#X = working_data.drop('P_HABITABLE', axis=1)\n#y = MiceImputed['P_HABITABLE']\nselector = SelectFromModel(rf(n_estimators=1000, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=1000, random_state=0).fit(X,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier as et\n\n#X = working_data.drop('P_HABITABLE', axis=1)\n#y = MiceImputed['P_HABITABLE']\nselector = SelectFromModel(et(n_estimators=1000, random_state=123))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(et(n_estimators=100, random_state=123).fit(X,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the three sets of important features, we will choose the ones which appear repeatedly in more than one method. The chosen ones are really important features.\n\n# Train-Test Split :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = working_data[['P_TYPE_TEMP','P_PERIOD','S_DEC','S_DISTANCE','S_MASS','S_TEMPERATURE','P_TYPE','S_TIDAL_LOCK','P_HABZONE_OPT','P_RADIUS_EST']]\ntarget = working_data['P_HABITABLE']\n\n# Split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=12345)\n\n# Normalize Features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling with Supervised Learning:\n\nWe are using ***one-vs-rest strategy*** for all classifiers for ***multi-class classification*** problem. This strategy breaks down a multi-class classification problem to multiple binary classification problems and then compares/ combines the results. The following models are used for classification purpose.\n* Logistic Regression with Ridge Penalty\n* Stochastic Gradient Descent with Lasso Penalty\n* Multinomial Naive Bayes\n* Passive Aggressive Classifier with Hinge Loss\n* Perceptron\n* Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common function\nimport time\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score, hamming_loss, cohen_kappa_score, plot_confusion_matrix, classification_report\ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model_ovr = OneVsRestClassifier(model)\n        model_ovr.fit(X_train,y_train, verbose=0)\n    else:\n        model_ovr = OneVsRestClassifier(model)\n        model_ovr.fit(X_train,y_train)\n    y_pred = model_ovr.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    h_loss = hamming_loss(y_test, y_pred)\n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"Hamming Loss = {}\".format(h_loss))\n    print(\"Cohen's Kappa = {}\".format(coh_kap))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model_ovr, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')   \n    \n    return accuracy, h_loss, coh_kap, time_taken","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[1] Logistic Regression with Ridge Penalty :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'l1', 'solver':'saga', 'multi_class':'multinomial'} #Ridge regularization\nmodel_lr = LogisticRegression(**params_lr)\naccuracy_lr, h_loss_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[2] Stochastic Gradient Descent with Hinge Loss and Lasso Penalty :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n# Lasso regularization\nparams_sgd = {'loss':'hinge', 'penalty':'l2', 'alpha': 1e-3, 'random_state': 12345, 'max_iter': 6, 'tol': None}\nmodel_sgd = SGDClassifier(**params_sgd)\naccuracy_sgd, h_loss_sgd, coh_kap_sgd, tt_sgd = run_model(model_sgd, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[3] Multinomial Naive Bayes :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel_mnb = MultinomialNB()\naccuracy_mnb, h_loss_mnb, coh_kap_mnb, tt_mnb = run_model(model_mnb, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[4] Passive Aggressive Classifier with Hinge Loss :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nparams_pac = {'fit_intercept':True, 'random_state': 12345, 'loss':'hinge'}\nmodel_pac = PassiveAggressiveClassifier(**params_pac)\naccuracy_pac, h_loss_pac, coh_kap_pac, tt_pac = run_model(model_pac, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[5] Simple Perceptron without any Penalty :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nparams_p = {'penalty':None, 'alpha': 1e-5, 'fit_intercept': True, 'random_state': 12345}\nmodel_p = Perceptron(**params_p)\naccuracy_p, h_loss_p, coh_kap_p, tt_p = run_model(model_p, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[6] Gradient Boosting Classifier :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nparams_gb = {'loss':'deviance', 'criterion': 'mse', 'n_estimators': 100, 'max_depth': 4, 'random_state': 12345, 'max_features': 'auto'}\nmodel_gb = GradientBoostingClassifier(**params_gb)\naccuracy_gb, h_loss_gb, coh_kap_gb, tt_gb = run_model(model_gb, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Decision Boundary Plotting for the Models :**\n\nWe will also perform decision region plotting in order to determine model performance properly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.50\nwidth = 0.75\n\nclf1 = LogisticRegression(solver='saga', penalty='l1', random_state=12345)\nclf2 = SGDClassifier(random_state=12345)\nclf3 = MultinomialNB()\nclf4 = PassiveAggressiveClassifier(random_state=12345) \nclf5 = Perceptron(random_state=12345, verbose = 0)\nclf6 = GradientBoostingClassifier(n_estimators=1000, random_state=12345, verbose=0)\n\n#Only taking the important planetarial features \nX_list = working_data[[\"P_TYPE_TEMP\", \"P_HABZONE_OPT\", \"P_RADIUS_EST\"]] \nX = np.asarray(X_list, dtype=np.float32)\ny_list = working_data[\"P_HABITABLE\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(2,3)\nfig = plt.figure(figsize=(16,10))\n\nlabels = ['Logistic Regression',\n          'Stochastic GD',\n          'Naive Bayes',\n          'Passive Aggressive',\n          'Perceptron',\n          'Gradient Boosting']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value}, \n                                filler_feature_ranges={2: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe a striking similarity in decision boundaries plotted by Passive Aggressive classifier and Peceptron model. Both of them have demarcated three class regions aptly and have very low misclassification. Apart from these two models, linear models like Logistic Regression and Stochastic Gradient Descent have also lower misclassifiction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Supervised Models Comparison :\n\nWe will take accuracy score, hinge loss, cohen kappa and time taken for execution into account for judging model performance. This apart we have already explored our observation from decision boundary plotting in previous step above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_scores = [accuracy_lr, accuracy_sgd, accuracy_mnb, accuracy_pac, accuracy_p, accuracy_gb]\nh_loss_scores = [h_loss_lr, h_loss_sgd, h_loss_mnb, h_loss_pac, h_loss_p, h_loss_gb]\ncoh_kap_scores = [coh_kap_lr, coh_kap_sgd, coh_kap_mnb, coh_kap_pac, coh_kap_p, coh_kap_gb]\ntt = [tt_lr, tt_sgd, tt_mnb, tt_pac, tt_p, tt_gb]\n\nmodel_data = {'Model': ['Logistic Regression','Stochastic GD','Naive Bayes','Passive Aggressive','Perceptron','GB'],\n              'Accuracy': accuracy_scores,\n              'Hamming_Loss': h_loss_scores,\n              'Cohen_Kappa': coh_kap_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(12,10))\nax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13)\ncolor = 'tab:green'\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Accuracy', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passive Aggressive classifier, Perceptron, Stochastic GD have the highest accuracy and lowest execution time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax3 = plt.subplots(figsize=(12,10))\nax3.set_title('Model Comparison: Hamming Loss and Cohens Kappa', fontsize=13)\ncolor = 'tab:blue'\nax3.set_xlabel('Model', fontsize=13)\nax3.set_ylabel('Hamming_Loss', fontsize=13, color=color)\nax4 = sns.barplot(x='Model', y='Hamming_Loss', data = data, palette='winter')\nax3.tick_params(axis='y')\nax4 = ax3.twinx()\ncolor = 'tab:red'\nax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color)\nax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color)\nax4.tick_params(axis='y', color=color)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Passive Aggressive classifier*** has zero hinge loss and the highest Cohen's Kappa. Hence, this is the ***best performing model*** applicable on exoplanet habitability classificaion data set.\n\n# Experimenting with Unsupervised Learning :\n\nWe will remove the habitability labels from the data set first. Then we will apply K-Means clustering model to cross-check whether it is giving rise to three distinct clusters or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"working_data_unsup = working_data.drop(['P_HABITABLE'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Elbow Curve\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\nvisualizer.fit(working_data_unsup)    \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, we got 3 clusters as the ideal number. Now, we will fit data into the K-Means model to get the cluster centers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting data into K-Means model with 3 clusters\nkm_3 = KMeans(n_clusters=3,random_state=12345)\nkm_3.fit(working_data_unsup)\nprint(km_3.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(km_3.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 1818 unhabitable, 786 conservatively habitable and 775 optimistically habitable exoplanets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate Silhouette Coefficient for K=3\nfrom sklearn import metrics\nmetrics.silhouette_score(working_data_unsup, km_3.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_labels = km_3.fit_predict(working_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will label each data point with its appropriate cluster type (0,1,2) respectively. We will store the labels in a newly added column **\"KM_Clusters\"** in the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\npreds = km_3.labels_\ndata_df = pd.DataFrame(working_data_unsup)\ndata_df['KM_Clusters'] = preds\ndata_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now choose some random pairs of features to visualize three distinct clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Feature Pair-1\nimport seaborn as sns\nplt.rcParams['axes.facecolor'] = 'white'\nplt.figure(figsize=(12,8))\n#Planet Flux vs Star Tidal Lock Region \ng =sns.scatterplot(x=working_data_unsup.iloc[:,24], y=working_data_unsup.iloc[:,28],\n              hue=cluster_labels,\n              data=working_data_unsup, \n              palette=['green','orange','red'], edgecolor='white', size='P_ECCENTRICITY', sizes=(50,200));\ng.set(xscale=\"log\");\ng.grid(False)\nplt.title(\"Planet Flux vs Star Tidal Lock Region sized by Eccentricity (Visualize KMeans Clusters)\", fontsize=15)\nplt.xlabel(\"Planet Flux\", fontsize=12)\nplt.ylabel(\"Star Tidal Lock Region\", fontsize=12)\nplt.rcParams['axes.facecolor'] = 'white'\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Feature Pair-2\nplt.rcParams['axes.facecolor'] = 'white'\nplt.figure(figsize=(12,6))\n#Estimated Planet Radius vs Star Declination \ng =sns.scatterplot(x=working_data_unsup.iloc[:,35], y=working_data_unsup.iloc[:,12],\n              hue=cluster_labels,\n              data=working_data_unsup, \n              palette=['green','orange','red'], size='P_PERIOD', sizes=(100,300));\ng.set(xscale=\"log\");\ng.grid(False)\nplt.title(\"Estimated Planet Radius vs Star Declination sized by Period (Visualize KMeans Clusters)\", fontsize=15)\nplt.xlabel(\"Estimated Planet Radius\", fontsize=12)\nplt.ylabel(\"Star Declination\", fontsize=12)\nplt.rcParams['axes.facecolor'] = 'white'\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we found very clearly distinguishable three observable clusters in both of these pictures above. Hence, we can conclude that the labels marked by K-Means algorithm are credible enough. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export KMeans results to file\ndata_df.to_csv(\"KMeans_results.csv\", index = False)\nprint(\"Submission successful\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}