{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Credit Card Prediction\n\nIn this notebook, there will be 4 main focuses which are:-\n\n1. [**EDA**](#Exploratory-Data-Analysis---EDA)\n2. [**Feature Engineering**](#Feature-Engineering)\n    - [Feature Encoding](#Feature/-Data-Encoding)\n    - [Scaling](#Feature-Scaling)\n    - [Decomposition (PCA)](#Decomposition-(PCA))\n3. [**Modelling & Hyperparameter Tuning**](#Modelling-&-Hyperparameter-Tuning)\n4. [**Evaluation Metrics**](#Evaluation-Metrics)\n    - [Precision-Recall Curve](#Precision-Recall-Curve)\n    - [Receiver Operating Characteristic curve (ROC)](#Receiver-Operating-Characteristic-curve-(ROC))\n    - [AUC](#AUC)\n\n[**Summary**](#Summary)\n\n\nP/s: I am new in this data science field, please feel free to give better suggestions or if you happen to find any mistake along this notebook, feel free to comment. Finally, got to apply the knowledge I have gained from my Data Science certificate in this dataset. Now, let's dig in!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary packages\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nplt.style.use('default')\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis - EDA\n\nWe will explore the data first, to get an overall understanding of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the csv using panda\ndf_cred = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\n\n# let's check the shape and the last 5 rows of the data\nprint(df_cred.shape)\ndf_cred.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns that are unneccessary\n# Clientnum and the last two Naive Bayes columns is not related as been explained in the data description\n\ndf_cred.drop(['CLIENTNUM', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', \n              'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], \n             axis=1, inplace=True)\n\nprint(df_cred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recommended to check the table after dropping column(s) on another cell because of the drop is inplace=True\n# So, to avoid errors when running the cell for multiple times\n\n# List of columns\nprint(df_cred.columns)\nprint(\"\")\n\n# Let's see if there is any missing values\nprint(df_cred.info())\n\n# Let's see the statistics for numerical values\ndisplay(df_cred.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise the frequency of each categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We want to see the frequency and unique features of all categorical variable in the data by plotting graph\n\ndef pltCountplot(category):\n    \n    fig, axis = plt.subplots(3, 2, figsize=(20,17))  # graph 3 by 2\n\n    index = 0\n    for i in range(3):\n        for j in range(2):\n            \n            ax = sns.countplot(category[index], data=df_cred, ax=axis[i][j])\n            \n            if category[index] in ['Education_Level', 'Income_Category']:  # because the x-label of edu and income category is quite long, so we need to rotate it a bit for visually pleasing\n                for item in ax.get_xticklabels():\n                    item.set_rotation(15)\n                \n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()/2.,\n                        height + 3,\n                        '{:1.2f}%'.format(height/len(df_cred)*100),\n                        ha=\"center\") \n            index += 1\n            \n            \ncategory = ['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\npltCountplot(category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n1. The dataset has no missing values which is good and there are 20 features we are currently working on before applying some feature engineering on them which we will explore later.\n2. We can see the dataset is unbalanced where the number of customer attrited is 67.66% lower than existing.\n3. The gender of the customer is quite balance.\n4. Most of the customer holds blue card category and have an income less than $40K."},{"metadata":{},"cell_type":"markdown","source":"### Visualise the features frequency of customer who attrited between the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pltCountplot_attrited(category, attrition_flag):\n    \n    fig, axis = plt.subplots(3, 2, figsize=(22,18))  # graph 3 by 2\n    \n    index = 0\n    for i in range(3):\n        for j in range(2):\n            \n            ax = sns.countplot(category[index], data=df_cred, hue=attrition_flag, ax=axis[i][j])\n            \n            if category[index] in ['Education_Level', 'Income_Category']:\n                for item in ax.get_xticklabels():\n                    item.set_rotation(15)\n                \n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()/2.,\n                        height + 3,\n                        '{:1.2f}%'.format(height/len(df_cred)*100),\n                        ha=\"center\") \n            index += 1\n            \n            \npltCountplot_attrited(category, 'Attrition_Flag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n1. We can see clearly here that attrited customer is significantly less than the existing customer on all the categorical features.\n2. For the education level, the ratio of customer who attrited is roughly the same from high school up until college level. And, slowly decreasing in ratio from postgraduate to doctorate level.\n3. Ratio of attrited customer in the '60K-80K' income category, is 6.41 where it is higher than the rest income range (exclude Unknown).\n\nThese are observations we can see within the respective category only."},{"metadata":{},"cell_type":"markdown","source":"### Now, let's see the features correlation between the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = ['Customer_Age','Dependent_count', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n             'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1',\n             'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n\n\ncorr_data = df_cred.loc[:, numerical].corr()\n\nplt.figure(figsize=(15,10))\nsns.heatmap(corr_data.abs(), annot=True, fmt='.3f',cmap='viridis',square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\nThere are 8 numerical features who correlated above 0.5, which are:-\n\n1. Average Open To Buy (Credit Line) and Credit Limit (on the card) = 0.996, high positive correlation.\n2. Total Transaction Amount and Total Transaction Count = 0.807, high positive correlation.\n3. Months on book and Customer age = 0.789, positive correlation.\n4. Total revolving balance and Average Utilization Ratio = 0.624, slightly positive correlation."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Feature/ Data Encoding\n\nFor the 6 categorical features, I have divided it into 2 sub-categories which are Nominal and Ordinal. Nominal categories will be using dummy function for non-binary and map function for binary categorical. Whereas, Ordinal will be mapped to their respective level according to their respective category.\n\n* Nominal: Attrition_Flag, Gender, and Marital_Status\n* Ordinal: Education_Level, Income_Category, and Card_Category"},{"metadata":{},"cell_type":"markdown","source":"#### Binary categorical (Nominal)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new df\ndf_cred_updated = pd.DataFrame()\n\n# Target variable\n# Customer whom attrited will be replaced to 1, else 0\ndf_cred_updated[\"Attrit\"] = df_cred.Attrition_Flag.map({\"Attrited Customer\":1, \"Existing Customer\":0})\n\n# Gender\ndf_cred_updated[\"Gender\"] = df_cred.Gender.map({\"M\":1, \"F\":0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dummy variable (Nominal)"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dum_marital = pd.get_dummies(df_cred.Marital_Status, prefix='marital', drop_first=True)\ndum_marital.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal Categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ordinal -- ordinal variable because it has natural ordering\n\ndf_cred_updated['Education_Level'] = df_cred.Education_Level.map({'Uneducated':1, 'High School':2, 'College':3, \n                                                         'Graduate':4, 'Post-Graduate':5, 'Doctorate':6, 'Unknown':7})\ndf_cred_updated['Income_Category'] = df_cred.Income_Category.map({'Less than $40K':1, '$40K - $60K':2, '$60K - $80K':3, \n                                                          '$80K - $120K':4, '$120K +':5, 'Unknown':6})\ndf_cred_updated['Card_Category'] = df_cred.Card_Category.map({'Blue':1, 'Silver':2, 'Gold':3, 'Platinum':4})\n\ndf_cred_updated = pd.concat([df_cred_updated, dum_marital, df_cred[numerical]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the updated credit data in more detail\nprint(\"Shape of updated credit dataset:\", df_cred_updated.shape)\nprint(\"\\nList of updated columns: \\n\\n\", df_cred_updated.columns)\ndisplay(df_cred_updated.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\n\n3 scaler will be used to scale the data:-\n   * Standard Scaler\n   * MinMax Scaler\n   * Power Transformer Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n\nfrom sklearn.pipeline import make_pipeline\n\n\nX = df_cred_updated.drop('Attrit', axis=1)\ny = df_cred_updated['Attrit']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\n# Feature scaling -- Standard Scaler\nscaler_S = StandardScaler().fit(X_train)\nX_train_s = scaler_S.transform(X_train)\nX_test_s = scaler_S.transform(X_test)\n\n\n# Feature scaling -- MinMax Scaler\nscaler_MM = MinMaxScaler().fit(X_train)\nX_train_mm = scaler_MM.transform(X_train)\nX_test_mm = scaler_MM.transform(X_test)\n\n\n# Feature scaling -- Power Transformer\nscaler_PT = PowerTransformer().fit(X_train)\nX_train_pt = scaler_PT.transform(X_train)\nX_test_pt = scaler_PT.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decomposition (PCA)\n\nChoose the most important feature components that preserves the maximal data variance in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# PCA -- Standard Scaler, MinMax, PT\npca_s = PCA().fit(X_train_s)\npca_mm = PCA().fit(X_train_mm)\npca_pt = PCA().fit(X_train_pt)\n\n\n# Plot the graph to see the number of components needed to explain the most data variance\nfig, axs = plt.subplots(1, 3, figsize=(20,5))\nfig.text(0.5, -0.04, 'Number of Components', ha='center', size=\"x-large\")\nfig.text(0.08, 0.5, 'Cumulative Explain Variance', va='center', rotation='vertical', size=\"x-large\")\n\naxs[0].plot(np.cumsum(pca_s.explained_variance_ratio_), color=\"red\")\naxs[0].set_title(\"PCA_StandardScaler\")\naxs[1].plot(np.cumsum(pca_mm.explained_variance_ratio_), color=\"green\")\naxs[1].set_title(\"PCA_MinMax\")\naxs[2].plot(np.cumsum(pca_pt.explained_variance_ratio_), color=\"blue\")\naxs[2].set_title(\"PCA_PowerTransformer\");\n\n\n# do the PCA first so that we know which feature is most important -- plus, dont want our model to be complicated\n# If we add these irrelevant features in the model, it will just make the model worst (Garbage In Garbage Out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\nThis curve quantifies how much of the total, 22-dimensional variance is contained within the first  ùëÅ  components.\n\n1. PCA_StandardScaler - We can see, with the updated dataset. The first 14 components contain approximately 90% of the variance while we need around 17 components to describe close to 100% of the variance.\n    \n2. PCA_MinMax - For the PCA after scaled using MinMax, the first 10 components contain approximately 90% of the variance while we need around 16 components to describe close to 100% of the variance.\n    \n2. PCA_PowerTransformer - Lastly, for the PCA after scaled using PowerTransformer, the first 13 components contain approximately 90% of the variance while we need around 16 components to describe close to 100% of the variance.\n\n\nHence, by looking at this plot for a high-dimensional dataset, we can choose how many components needed to train our model. It is useful in reducing redundancy present in the dataset plus it also help to improve the time execution."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# PCA -- SS\npca1_s = PCA(n_components=14)\nX_train_s_pca = pca1_s.fit_transform(X_train_s)\nX_test_s_pca = pca1_s.transform(X_test_s)\n\n# PCA -- MM\npca1_mm = PCA(n_components=10)\nX_train_mm_pca = pca1_mm.fit_transform(X_train_mm)\nX_test_mm_pca = pca1_mm.transform(X_test_mm)\n\n# PCA -- PT\npca1_pt = PCA(n_components=13)\nX_train_pt_pca = pca1_pt.fit_transform(X_train_pt)\nX_test_pt_pca = pca1_pt.transform(X_test_pt)\n\n\n# Shape\nprint(\"Original shape: \", X_train.shape)\nprint(\"\\nAfter PCA & SS: \", X_train_s_pca.shape)\nprint(\"After PCA & MM: \", X_train_mm_pca.shape)\nprint(\"After PCA & PT: \", X_train_pt_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Below is the visualization of the first 14 importance features after being scaled using Standard Scaler\n\nWe can see clearly the first component hold the most cumulative variance of features compared to the last component."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.matshow(pca1_s.components_, cmap='viridis', fignum=1)\nplt.yticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], \n           [\"1st component\", \"2nd component\", \"3rd component\", \"4th component\", \"5th component\", \n            \"6th component\", \"7th component\", \"8th component\", \"9th component\", \"10th component\", \n            \"11th component\", \"12th component\", \"13th component\", \"14th component\"])\nplt.colorbar()\nplt.xticks(range(21), df_cred_updated.iloc[:, 1:], rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling & Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Preamble -- imbalanced dataset\n\nSince, we have an imbalanced dataset where Existing Customer:8500 and Attrited Customer:1627. We will handle it by applying KFold and over-sampling (SMOTE) method to the dataset. Our aim is to find a classifier with a good recall (i.e. we want our classifier to find as many attrited cases as it can). However, we need to be aware when using this metric because we do not want to simply label people who are going to attrit their credit card. \n\n\n\n#### Note: We use dataset that is being scaled using StandardScaler and PCA(14)"},{"metadata":{},"cell_type":"markdown","source":"### Let's Standardizing our splits\n\nLet's make sure that our results are consistent as we try different methods. It is a little simpler to have cv=5 in all of our grid searches and cross-validations, but we will get different splits each time.\n\nIf we use cv=kf, where kf is a KFold object we can ensure that we get the same splits each time.\n\nKFold cross-validation will be applied first before data resampling being done. It is important to avoid the data overfit our model to a specific artificial bootstrapping result. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won‚Äôt be an overfitting problem."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\" From here, we can see our targets are preamble dataset. So, we use over-sampling (SMOTE) method to \nbalance the dataset by increasing the size of the rare sample, in our case Churned size.\n\n{0:Existing, 1=Churned}\"\"\"\n\n\nfrom collections import Counter\nCounter(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary packages\n\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nkf = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General function for all model except forest\ndef imba_pipe(model):\n    imba_pipeline = make_pipeline(SMOTE(random_state=42), model)\n    return cross_val_score(imba_pipeline, X_train_s_pca, y_train, scoring=\"recall\", cv=kf)\n\n# Function for forest model -- the model does not use scaled data or dimension reduction data\ndef imba_pipe_forest(model):\n    imba_pipeline = make_pipeline(SMOTE(random_state=42), model)\n    return cross_val_score(imba_pipeline, X_train, y_train, scoring=\"recall\", cv=kf)\n\n# Function to fit the train and test dataset to models\ndef imba_pipe_fit(model):\n    imba_pipeline = Pipeline(steps=[('smote', SMOTE(random_state=42)), \n                                    ('model', model)\n                                   ])\n    return imba_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. KNeighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Before GridSearchCV\nknn = KNeighborsClassifier(n_neighbors=100)\nknn_recall = imba_pipe(knn)\nprint(\"Array of KNN recall score after KFold and resample: \", knn_recall)\nprint(\"Mean of array KNN recall score: \", round(knn_recall.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best hypertuning params -- using GridSearchCV\nknn1 = KNeighborsClassifier()\nknn_params = {\"model__n_neighbors\":[2,5,10,87,100],\n              \"model__weights\":[\"uniform\", \"distance\"],\n              \"model__p\":[1,2]\n             }\n\nknn_grid = GridSearchCV(imba_pipe_fit(knn1), knn_params, cv=kf, scoring=\"recall\")\nknn_grid.fit(X_train_s_pca, y_train)\n\nprint(\"Best parameters for KNN: \", knn_grid.best_params_)\nprint(f\"\\nKNN Grid best train recall score: {knn_grid.best_score_ :.2f}\")\nprint(f\"KNN Grid best test recall score: {recall_score(y_test, knn_grid.best_estimator_.predict(X_test_s_pca)) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Since data are not necessarily scaled when using RandomForest algorithm. Hence, we use unscaled data.\n\n# Before GridSearchCV\nrfc = RandomForestClassifier(max_depth= None, max_features=5, min_samples_split=2, criterion=\"gini\")\nrfc_recall = imba_pipe_forest(rfc)\nprint(\"Array of RFC recall score after KFold and resample: \", rfc_recall)\nprint(\"Mean of array RFC recall score: \", round(rfc_recall.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best hypertuning params -- using GridSearchCV\nrfc1 = RandomForestClassifier(max_depth= None)\n\nrfc_params = {\"model__criterion\":[\"gini\", \"entropy\"],\n              \"model__min_samples_split\": np.arange(2,6), \n              \"model__max_features\":[5, \"auto\", \"sqrt\", \"log2\"]\n             }\n\nrfc_grid = GridSearchCV(imba_pipe_fit(rfc1), rfc_params, cv=kf, scoring=\"recall\")\nrfc_grid.fit(X_train, y_train)\n\nprint(\"Best parameters for RFC: \", rfc_grid.best_params_)\nprint(f\"\\nRFC Grid best train recall score: {rfc_grid.best_score_ :.2f}\")\nprint(f\"RFC Grid best test recall score: {recall_score(y_test, rfc_grid.best_estimator_.predict(X_test)) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Since data are not necessarily scaled when using XGBC algorithm. Hence, we use unscaled data.\n\n# Before GridSearchCV\nxgbc = XGBClassifier(booster=\"gbtree\", learning_rate=0.1, max_delta_step=1, verbosity=0, use_label_encoder=False)\nxgbc_recall = imba_pipe_forest(xgbc)\nprint(\"Array of XGBC recall score after KFold and resample: \", xgbc_recall)\nprint(\"Mean of array XGBC recall score: \", round(xgbc_recall.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best hypertuning params -- using GridSearchCV\nxgbc1 = XGBClassifier(booster=\"gbtree\", verbosity=0, use_label_encoder=False)\n\nxgbc_params = {\"model__learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n               \"model__max_depth\": np.arange(2,7),\n               \"model__max_delta_step\": np.arange(1,11)\n              }\n\nxgbc_grid = GridSearchCV(imba_pipe_fit(xgbc1), xgbc_params, cv=kf, scoring=\"recall\")\nxgbc_grid.fit(X_train, y_train)\n\nprint(\"Best parameters for XGBC: \", xgbc_grid.best_params_)\nprint(f\"\\nXGBC Grid best train recall score: {xgbc_grid.best_score_ :.2f}\")\nprint(f\"XGBC Grid best test recall score: {recall_score(y_test, xgbc_grid.best_estimator_.predict(X_test)) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Before GridSearchCV\nlogr = LogisticRegression(fit_intercept=True, solver=\"saga\", penalty=\"l1\", C=0.5)\nlogr_recall = imba_pipe(logr)\nprint(\"Array of LogReg recall score after KFold and resample: \", logr_recall)\nprint(\"Mean of array LogReg recall score: \", round(logr_recall.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best hypertuning params -- using GridSearchCV\nlogr1 = LogisticRegression()\n\nlogr_params = {\"model__solver\": [\"lbfgs\", \"sag\", \"saga\"],\n               \"model__C\": np.arange(0.1,2,0.1), \n               \"model__class_weight\": [\"balanced\", None]\n              }\n\nlogr_grid = GridSearchCV(imba_pipe_fit(logr1), logr_params, cv=kf, scoring=\"recall\")\nlogr_grid.fit(X_train_s_pca, y_train)\n\nprint(\"Best parameters for LogReg: \", logr_grid.best_params_)\nprint(f\"\\nLogreg Grid best train recall score: {logr_grid.best_score_ :.2f}\")\nprint(f\"Logreg Grid best test recall score: {recall_score(y_test, logr_grid.best_estimator_.predict(X_test_s_pca)) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Support Vector Machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(kernel=\"rbf\", C=0.5, gamma=\"auto\")\nsvc_recall = imba_pipe(svc)\nprint(\"Array of SVC recall score after KFold and resample: \", svc_recall)\nprint(\"Mean of array SVC recall score: \", round(svc_recall.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best hypertuning params -- using GridSearchCV\nsvc1 = SVC()\n\nsvc_params = {\"model__C\": [0.1, 0.2, 0.3, 0.4],\n              \"model__kernel\": [\"rbf\", \"sigmoid\"],\n              \"model__gamma\": [0.2, \"scale\", \"auto\"]\n             }\n\nsvc_grid = GridSearchCV(imba_pipe_fit(svc1), svc_params, cv=kf, scoring=\"recall\")\nsvc_grid.fit(X_train_s_pca, y_train) \n\nprint(\"Best parameters for SVC: \", svc_grid.best_params_)\nprint(f\"\\nSVC Grid best train recall score: {svc_grid.best_score_ :.2f}\")\nprint(f\"SVC Grid best test recall score: {recall_score(y_test, svc_grid.best_estimator_.predict(X_test_s_pca)) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Metrics\n\n### Evaluation for classification\n\nSo far, we have evaluated classifiers using recall accuracy, the proportion of actual positives was identified correctly.\n\nSimple accuracy may not often be the right goal for your particular machine learning application. For example with credit card churned or credit card fraud, false positives and false negatives might have very different real world effects for users or for organization outcomes. So, it's important to select an evaluation metric that reflects those user application or business needs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary packages for classification evaluation\n\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. KNearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_pred = knn_grid.best_estimator_.predict(X_test_s_pca)\nknn_confusion = confusion_matrix(y_test, knn_pred)\nknn_class_report = classification_report(y_test, knn_pred, target_names=[\"not 1\", \"1\"])\n\nprint(\"KNN confusion matrix\\n\", knn_confusion)\nprint(\"______________________________________\\n\")\nprint(\"KNN classification report\\n\\n\", knn_class_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_pred = rfc_grid.best_estimator_.predict(X_test)\nrfc_confusion = confusion_matrix(y_test, rfc_pred)\nrfc_class_report = classification_report(y_test, rfc_pred, target_names=[\"not 1\", \"1\"])\n\nprint(\"RFC confusion matrix\\n\", rfc_confusion)\nprint(\"______________________________________\\n\")\nprint(\"RFC classification report\\n\\n\", rfc_class_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc_pred = xgbc_grid.best_estimator_.predict(X_test)\nxgbc_confusion = confusion_matrix(y_test, xgbc_pred)\nxgbc_class_report = classification_report(y_test, xgbc_pred, target_names=[\"not 1\", \"1\"])\n\nprint(\"XGBC confusion matrix\\n\", xgbc_confusion)\nprint(\"______________________________________\\n\")\nprint(\"XGBC classification report\\n\\n\", xgbc_class_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logr_pred = logr_grid.best_estimator_.predict(X_test_s_pca)\nlogr_confusion = confusion_matrix(y_test, logr_pred)\nlogr_class_report = classification_report(y_test, logr_pred, target_names=[\"not 1\", \"1\"])\n\nprint(\"Logistic Regression confusion matrix\\n\", logr_confusion)\nprint(\"______________________________________\\n\")\nprint(\"Logistic Regression classification report\\n\\n\", logr_class_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Support Vector Machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_pred = svc_grid.best_estimator_.predict(X_test_s_pca)\nsvc_confusion = confusion_matrix(y_test, svc_pred)\nsvc_class_report = classification_report(y_test, svc_pred, target_names=[\"not 1\", \"1\"])\n\nprint(\"SVC confusion matrix\\n\", svc_confusion)\nprint(\"______________________________________\\n\")\nprint(\"SVC classification report\\n\\n\", svc_class_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision-Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\ny_proba_knn = knn_grid.best_estimator_.fit(X_train_s_pca, y_train).predict_proba(X_test_s_pca)\ny_proba_rfc = rfc_grid.best_estimator_.fit(X_train, y_train).predict_proba(X_test)\ny_proba_xgbc = xgbc_grid.best_estimator_.fit(X_train, y_train).predict_proba(X_test)\ny_scores_logr = logr_grid.best_estimator_.fit(X_train_s_pca, y_train).decision_function(X_test_s_pca)\ny_scores_svc = svc_grid.best_estimator_.fit(X_train_s_pca, y_train).decision_function(X_test_s_pca)\n\n\nknn_precision, knn_recall, knn_thresholds = precision_recall_curve(y_test, y_proba_knn[:,1])\nrfc_precision, rfc_recall, rfc_thresholds = precision_recall_curve(y_test, y_proba_rfc[:,1])\nxgbc_precision, xgbc_recall, xgbc_thresholds = precision_recall_curve(y_test, y_proba_xgbc[:,1])\nlogr_precision, logr_recall, logr_thresholds = precision_recall_curve(y_test, y_scores_logr)\nsvc_precision, svc_recall, svc_thresholds = precision_recall_curve(y_test, y_scores_svc)\n\n\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(knn_recall, knn_precision, lw=1, label='KNeighbors: Precision-Recall Curve', color='blue')\nplt.plot(rfc_recall, rfc_precision, lw=1, label='RandForest: Precision-Recall Curve', color='orange')\nplt.plot(xgbc_recall, xgbc_precision, lw=1, label='XGBoost: Precision-Recall Curve', color='yellow')\nplt.plot(logr_recall , logr_precision, lw=1,  label='LogReg: Precision-Rec all Curve', color='red')\nplt.plot(svc_recall, svc_precision, lw=1, label='SVC: Precision-Recall Curve', color='cyan')\nplt.xlabel('Recall', fontsize=16)\nplt.ylabel('Precision', fontsize=16)\nplt.legend(loc='lower left', fontsize=10)\nplt.axes().set_aspect('equal')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Receiver Operating Characteristic curve (ROC)\n\nROC curves or Receiver Operating Characteristic curves illustrate the performance of a binary classifier. It is created by plotting the true positive rate (TPR) (or recall) against the false positive rate (FPR).\n\nROC curves on the X-axis show a classifier's False Positive Rate so that would go from 0 to 1.0, and on the Y-axis they show a classifier's True Positive Rate so that will also go from 0 to 1.0.\n\nROC curves are very help with understanding the balance between true-positive rate and false positive rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary packages\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr_knn, tpr_knn, _ = roc_curve(y_test, y_proba_knn[:,1])\nfpr_rfc, tpr_rfc, _ = roc_curve(y_test, y_proba_rfc[:,1])\nfpr_xgbc, tpr_xgbc, _ = roc_curve(y_test, y_proba_xgbc[:,1])\nfpr_logr, tpr_logr, _ = roc_curve(y_test, y_scores_logr)\nfpr_svc, tpr_svc, _ = roc_curve(y_test, y_scores_svc)\n\n\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_knn, tpr_knn, lw=1, label='KNeighbors: ROC curve', color='blue')\nplt.plot(fpr_rfc, tpr_rfc, lw=1, label='RandForest: ROC curve', color='orange')\nplt.plot(fpr_xgbc, tpr_xgbc, lw=1, label='XGBoost: ROC curve', color='yellow')\nplt.plot(fpr_logr, tpr_logr, lw=1, label='LogRegr: ROC curve', color='red')\nplt.plot(fpr_svc, tpr_svc, lw=1, label='SVC: ROC curve', color='cyan')\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\nplt.legend(loc='lower right', fontsize=13)\nplt.axes().set_aspect('equal')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AUC\nLet's see the AUC score for each models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AUC\n\nprint(f\"AUC score for KNN {auc(fpr_knn, tpr_knn) :.2f}\")\nprint(f\"AUC score for Random Forest {auc(fpr_rfc, tpr_rfc) :.2f}\")\nprint(f\"AUC score for XGBoost {auc(fpr_xgbc, tpr_xgbc) :.2f}\")\nprint(f\"AUC score for LogReg {auc(fpr_logr, tpr_logr) :.2f}\")\nprint(f\"AUC score for SVC Non-linear {auc(fpr_svc, tpr_svc) :.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"XGBoost and Random Forest has the best model compared to the other with all metrics score (precision, recall, f1, accuracy) has shown at least 85% accuracy. Also, both XGBoost and Random Forest have AUC score of 99% that shows the model is learning the data well enough.\n\nEven though most of the models are improving in their recall score, some of the models have significant reduction in their precision score; which are KNN, Logistic Regression, and Support Vector Machine. This is important as we must aware by getting a high recall score and a low precision score, it could be a sign that the model might simply predicting customer that want to churn their credit card (increasing in **False Positive**)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":" "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}