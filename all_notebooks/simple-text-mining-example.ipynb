{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"This Kernel will give a quick demonstration on a **text mining example**  on the Consumer Financial complaints data published on the Consumer Financial Protection Bureau (CFPB) website.\n\nThis kernel  utilizes the text attribute - Consumer complaint narrative to answer one simple question: **what are the top 10 key complaint word that the top 3 credit bureau agencies (Equifax, Experian, and TransUnion) received ?**\n\nBy extracting those keywords, it might help financial agencies like credit bureau in this case, especially the compliance department to better target any potential risk or issue and eventually control the risk.\n\n**The kernel can be break into three parts:**\n\n1. count the word and get the words' frequency\n2. calculate the ratio (between word frequency of individual agency and word frequency of the entire complaint lists) and get the words with the highest ratio value\n3. improve the ratio by excluding common words related to company' name\nThere are 555,957 complaints(records) and 18 features (variables), but this kernel, I will be only using the text attribute - Consumer complaint narrative After excluding the row with missing consumer complaint narrative, we are left 66,806 complaints(records).\n\nLet's get started!"},{"metadata":{"trusted":true,"_uuid":"dd24b8013fa27c8489336ce660489cf6905dd3da"},"cell_type":"code","source":"#Load library\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"798b30fc14077b538027b2ddcd898c458d79bf8f"},"cell_type":"code","source":"# For dispaly purpose\nclass display(object):\n    \"\"\"Display HTML representation of multiple objects\"\"\"\n    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n    </div>\"\"\"\n    def __init__(self, *args):\n        self.args = args\n        \n    def _repr_html_(self):\n        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n                         for a in self.args)\n    \n    def __repr__(self):\n        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n                           for a in self.args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00da3320f93ac06b3cd9aa9c48b24727ed0c5668"},"cell_type":"code","source":"df = pd.read_csv('../input/consumer_complaints.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb2ccaa12997bd59b87ff07f0350f377e9f3dd0a"},"cell_type":"code","source":"# take a look the first five observation in the dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f26435bd857e7c21ca34fbb7e6d6cf3c71bea571"},"cell_type":"code","source":"print('Complain data set shape: ', df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c39a7d473e36cbd92915dc3fd3f1f05b038a029a"},"cell_type":"markdown","source":"Complain data set shape:  (555957, 18)\nThere are 555,957 complaints(records)a and 18 features (variables).\n\nAfter we get a sense of the dataset, let's dive in the text attribute - consumer complaint narrative\n\nfirst, take a look the complaint narrative examples:"},{"metadata":{"trusted":true,"_uuid":"61e679661cba32d06dd2e4f3898643eff9e4b03c"},"cell_type":"code","source":"df[(df['consumer_complaint_narrative'].notnull())]['consumer_complaint_narrative'].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"031d6dd9357237ec13d14e62210f6f1ee9971c94"},"cell_type":"markdown","source":" **Step 1: count the word frequency**"},{"metadata":{"trusted":true,"_uuid":"272c233dd3a53bd77aac44bb5f4aea8222ae6334"},"cell_type":"code","source":"# Subset the data by company\nEQU = df[(df['consumer_complaint_narrative'].notnull())&(df['company']=='Equifax')]\nEXP = df[(df['consumer_complaint_narrative'].notnull())&(df['company']=='Experian')]\nTRU = df[(df['consumer_complaint_narrative'].notnull())&(df['company']=='TransUnion Intermediate Holdings, Inc.')]\nTOTAL = df[(df['consumer_complaint_narrative'].notnull())]\n\n# Take a look how many complaint related to each company\nprint(len(EQU),'complaints related to Equifax')\nprint(len(EXP),'complaints related to Experian')\nprint(len(TRU),'complaints related to TransUnion')\nprint(len(TOTAL),'complaints in Total')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0b9a88ee42280a99ad4ebe57f118a7f1c61dc16"},"cell_type":"code","source":"# Create a empty Counter Object for the next step counting\nEQU_counts = Counter()\nEXP_counts = Counter()\nTRU_counts = Counter()\nTOTAL_counts = Counter()\n\nEQU_lt = EQU['consumer_complaint_narrative'].tolist()\nEXP_lt = EXP['consumer_complaint_narrative'].tolist()\nTRU_lt = TRU['consumer_complaint_narrative'].tolist()\nTOTAL_lt = TOTAL['consumer_complaint_narrative'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba6d28cb5b8e1fc0c6d68a6883b72e243e9dc182"},"cell_type":"code","source":"#loop over all the words in the complaints and add up the counts\ndef count_word(complaints,word_counts):\n    for i in range(len(complaints)):\n        for word in re.split(r'\\W+',  complaints[i]):\n            word_counts[word] +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f13f1eaebe2d4f47546083dca2c1e0a6bed5a5c"},"cell_type":"code","source":"# count the word for each company's complaint lists\ncount_word(EQU_lt,EQU_counts)\ncount_word(EXP_lt,EXP_counts)\ncount_word(TRU_lt,TRU_counts)\ncount_word(TOTAL_lt,TOTAL_counts)\n\n# extract the most common 10 words used in each company's complaint\nEQU_counts_10 = EQU_counts.most_common(10)\nEXP_counts_10 = EXP_counts.most_common(10)\nTRU_counts_10 = TRU_counts.most_common(10)\nTOTAL_counts_10 = TOTAL_counts.most_common(10)\n\n\n# convert to dataframe for display\nEQU_df = pd.DataFrame({'most 10 common (EQU)':EQU_counts_10})\nEXP_df = pd.DataFrame({'most 10 common (EXP)':EXP_counts_10})\nTRU_df = pd.DataFrame({'most  10 common (TRU)':TRU_counts_10})\nTotal_df = pd.DataFrame({'most 10 common (Total)':TOTAL_counts_10})\n\ndisplay('EQU_df', 'EXP_df', 'TRU_df', 'Total_df')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a3c677cf83f3b73161150bf6817822248b8a3f8"},"cell_type":"markdown","source":"As the side by side comparison shown above for the top 10 common words used in different companies and entire complaint lists, common words like \"the\" is ranked the top among Equifax, Experian, TransUnion, and the Entire complaints lists.\n\nInstead of finding the common words in the Equifax or Experian or TransUnion complaints, what we really want is those words that are shown far more often among one company's complain rather than the total complaint list. In other word, what complaint key word is concentrated uniquely for this company."},{"metadata":{"_uuid":"d9d4cbf8c6f0997099247a9f2edc39b23633964a"},"cell_type":"markdown","source":"**Step2: Calculate the frequent ratio**\n\nTo accomplish this, we' ll need to calculate the word usage ratio between individual company and the entire list.\n\nUse \"the\" as an example,\n\n$$FreqRatio = \\frac{Count_{EQU}['the']}{(Count_{TOTAL}['the'] + 1)}$$\nNote: the \"+ 1 \" here is added in case the TOTAL_counts for some words is zero.\n\nDividing the company specific count on a word by the total count of the same word, we can let the company unique complaint key word stands out, and suppress the importance of common words like \"the\".\n\nLet's calculated the ratio!"},{"metadata":{"trusted":true,"_uuid":"34a8764efbeaf2a2f9b04eac2dade4161bbe2ead"},"cell_type":"code","source":"def calculate_ratio(word_counts,ratios):\n    for word in list(word_counts):\n        ratio = word_counts[word] / float(TOTAL_counts[word]+1)\n        ratios[word] = ratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa2b904be707a6e4c38945d75dddc9069ccbcbb"},"cell_type":"code","source":"# Again, create Counter object for ratio calculation\nEQU_ratios = Counter()\nEXP_ratios = Counter()\nTRU_ratios = Counter()\n\n# calculate the ratio for each company's complaint words\ncalculate_ratio(EQU_counts,EQU_ratios)\ncalculate_ratio(EXP_counts,EXP_ratios)\ncalculate_ratio(TRU_counts,TRU_ratios)\n\n\n# words with the highest ratio \nEQU_df = pd.DataFrame({'most_common (EQU)':EQU_ratios.most_common(10)})\nEXP_df = pd.DataFrame({'most_common (EXP)':EXP_ratios.most_common(10)})\nTRU_df = pd.DataFrame({'most_common (TRU)':TRU_ratios.most_common(10)})\n\ndisplay('EQU_df', 'EXP_df', 'TRU_df')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"561347ba5e2ee1c82d28410932caa288d04ab986"},"cell_type":"markdown","source":"As the side by side comparison shown above this time, we started to see the differentiation. Unfortunately, biggest differentiation in the complaints is the company name, which is again not we really interested about, we want to find what's the real financial service issue in those complaints!\n\nBut we already very close to the answer we are trying to answer, just need one more step - leave out those word related to company names.\n\nTo accomplish this, we'll skip the counting when it is company related name."},{"metadata":{"_uuid":"256ba6bb2d799a9cdbf846e72bf471f40abd14a8"},"cell_type":"markdown","source":"**Step 3: improve the ratio**\n\nOne simply way is just say if any time we see 'Equifax', for example, we skip counting the word frequency, so that this word will be automatically, have a zero frequency, and showing in our most common list.\n\n**However, as we can see in the result above, when customer wrote complaint, they misspelled a lot. So by excluding just 'Equifax', is not gonna get us what we want.\n**\n\n**Two ways of dealing with it:\n**\n1. manually summarize the misspelled pattern\n2. using the library FuzzyWuzzy to implement a fuzzy matching\n\nAn example below is demonstrating how the fuzzywuzzy works. basically, it calculate a distance (called Levenshtein distance) to measure the difference between two sequence, in our case, two words. the higher the score"},{"metadata":{"trusted":true,"_uuid":"23acbb3229a9243b2f3323a42de7403b9836fe0e"},"cell_type":"code","source":"# illustrate how the fuzzywuzzy.process work\nfrom fuzzywuzzy import process\n\nmisspelled1 = 'Exquifaax'\nmisspelled2 = 'Exclude'\nmatch = ['equifax']\nfuzzy_score1 = process.extract(misspelled1, match)\nfuzzy_score2 = process.extract(misspelled2, match)\nprint(fuzzy_score1)\nprint(fuzzy_score2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1193fa3f663b188f15c4bd5baf543cdf8877bae"},"cell_type":"markdown","source":"Back to our example, we for sure want to exclude those with high fuzzy score, which are those misspelled.\n\n**The pros and cons for the two approach I mentioned above:**\n\n* the manually summarizing is fast, but can't exclude the fuzzy words\n* the fuzzy matching is accurately excluding those fuzzy word, but the code can take very long time to run, since it has to compute the fuzzy score for every word in the complaint list.\n\nTherefore, A mixed approach that combined first and second option can by more reasonable and leads to faster and more accurate result.The mixed approach is, if a word contain certain string like \"eq\" in the Equifax complaints, those words are far more likely is to be a misspelled 'Equifax '.\n\n**Therefore:**\n\n* if the condition like containing â€œeqâ€ is met, fuzzy matching function will be called.\n* if condition is not matched, simply count the word with out any other processing.\n* when the fuzzy matching function is called, is the fuzzy_score is greater than 85, skip the counting, since it is probably a misspelled 'Equifax', which we don't care that much\n\nFor the implementation, everything else keep the same as shown previously, and only revise this count_word function, since we need to update our new logic"},{"metadata":{"trusted":true,"_uuid":"d126f29995a66c0aa8b40c4d4d448c958faa91c0"},"cell_type":"code","source":"#loop over all the words in the complaints and add up the counts\ndef count_word_new(word_lt,word_cnt,c_name_int,c_name):\n    for i in range(len(word_lt)):\n        lt = filter(None, re.split(r'\\W+',  word_lt[i]))\n        for word in lt:\n            if word.lower().find(c_name_int) != -1:\n                fuzzy_score = process.extract(word, c_name)[0][1]\n                if fuzzy_score>=80:\n                     continue\n                else:\n                    word_cnt[word] += 1\n\n            else:\n                word_cnt[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65976723c0c3832859cca9a3a11d2a2da411b1d0"},"cell_type":"code","source":"# Create a empty Counter Object for the next step counting\nEQU_counts2 = Counter()\nEXP_counts2 = Counter()\nTRU_counts2 = Counter()\n\n# Again, create Counter object for ratio calculation\nEQU_ratios2 = Counter()\nEXP_ratios2 = Counter()\nTRU_ratios2 = Counter()\n\n# use the count_word_new to count\ncount_word_new(EQU_lt,EQU_counts2,\"eq\",[\"Equifax\"])\ncount_word_new(EXP_lt,EXP_counts2,\"expe\",[\"Experian\"])\ncount_word_new(TRU_lt,TRU_counts2,\"tran\",[\"TransUnion\"])\n\ncalculate_ratio(EQU_counts2,EQU_ratios2)\ncalculate_ratio(EXP_counts2,EXP_ratios2)\ncalculate_ratio(TRU_counts2,TRU_ratios2)\n\nEQU_df = pd.DataFrame({'most_common (EQU)':EQU_ratios2.most_common(10)})\nEXP_df = pd.DataFrame({'most_common (EXP)':EXP_ratios2.most_common(10)})\nTRU_df = pd.DataFrame({'most_common (TRU)':TRU_ratios2.most_common(10)})\n\ndisplay('EQU_df', 'EXP_df', 'TRU_df')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d361882270714710f059172557790602a9dcf39"},"cell_type":"markdown","source":"Those misspelled decreased a lot, although not all!\n\n**Based on this complaint list, we can summarize**\n\n* Equifax's complaint are mostly concentrated in TrustedID, Intruders, segmentation, re alleges, cyber attack and 2013correct.\n* Experian's complaints are mostly concentrated in Geographical, Credit Works, free credit report, Inquiry, Delinquency.\n* Trans Union's complaints are mostly concentrated in 3rd party info, Libellant, LLCConsumer, Inquiry, Financing.\nThat's it. ðŸ˜ƒ\n\nWe answered our question - what are the top 10 key complaint word that the top 3 credit bureau agencies (Equifax, Experian, and TransUnion) received ?\n\nHopefully, this kernel demonstrate the power of the text mining, even it's a simple one, in helping compliance department in the financial industry to gain insights on what are the potential risk based on the customer text data.\n\nAppendix link for the dataset: https://www.consumerfinance.gov/data-research/consumer-complaints/\n\nSpecail thanks to the display code from: https://jakevdp.github.io/PythonDataScienceHandbook"},{"metadata":{"trusted":true,"_uuid":"f727d288ccedcdf3b913f3176ddd2f41fc870033"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}