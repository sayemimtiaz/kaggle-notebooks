{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"darkblue\" ><b> <center><u>Term Deposit Subscription Prediction </u></center></b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of contents</h3>\n\n* [Notebook](#note)\n    * [1. Data](#n1)\n    * [2. EDA](#n2)\n    * [3. Log Transformation of numerical columns](#n3)\n    * [4. Box plots of numerical columns](#n4)\n    * [5. Unique Values of Various Categorical Columns](#n5)\n    * [6. Label Encoder with StandardScaler](#n6)\n    * [7. Label Encoder with MinMaxScaler](#n7)\n    * [8. One-Hot Encoder](#n8)\n    * [9. Target Encoder](#n9)\n\t* [10. Hashing Encoder](#n10)\n\t* [11. Weight of Evidence](#n11)\n\t* [12. Binary Encoder](#n12)\n\t* [13. Comparison Table - Encoders](#n13)\n\t* [14. Models Comparison](#n14)\n\t* [15. Modelling with Best Classifier](#n15)\n\t* [16. Confusion Matrix and Classification report](#n16)"},{"metadata":{},"cell_type":"markdown","source":"<font color=\"darkblue\" size=+3>Imports and Installations</font>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom category_encoders import HashingEncoder\nfrom category_encoders import WOEEncoder\nfrom category_encoders import BinaryEncoder\n\nfrom sklearn import metrics\nimport itertools\nimport gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install category_encoders","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n1\"></a>\n<font color=\"darkblue\" size=+3>Reading Data and types of data</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/term-deposit-prediction-data-set/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/term-deposit-prediction-data-set/test.csv\")\n\ntrain_back = train.copy()\ntest_back = test.copy()\n\nnumerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\nprint(\"Numerical_features: \",numerical_features)\ncategorical_features = [feature for feature in train.columns if train[feature].dtypes == 'O']\nprint(\"Categorical_features: \",categorical_features)\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n2\"></a>\n<font color=\"darkblue\" size=+3>EDA through Profiling Report</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(train)\nprofile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n3\"></a>\n<font color=\"darkblue\" size=+3>Log transformation of Numerical Columns</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hack - There are some zero or 01 values. So ading 1 or 2, before doing log transformation.\ntrain['duration'] = train['duration'] + 1\ntrain['previous'] = train['previous'] + 2\ntrain['pdays'] = train['pdays'] + 2\ntrain['balance'] = train['balance'] + 1\ncols = ['age','pdays','previous','campaign','duration']\n\nfor feature in cols:\n    print(\"\\nMin/Max values of {} are {}, {}\".format(feature, train[feature].min(), train[feature].max()))    \n    if 0 in train[feature].unique():\n        pass\n    else:\n        try:\n            train[feature] = np.log(train[feature])           \n        except:\n            print(\"some error in train: \", feature)\n    print(\"After log, transformation - Min/Max values of {} are {} - {}\".format(feature, train[feature].min(), train[feature].max()))\n\nprint(\" = \" * 60)\ntest['duration'] = test['duration'] + 1\ntest['previous'] = test['previous'] + 2\ntest['pdays'] = test['pdays'] + 2\n\nfor feature in cols:\n    print(\"\\nMin/Max values of {} are {}, {}\".format(feature, test[feature].min(), test[feature].max()))    \n    if 0 in test[feature].unique():\n        pass\n    else:\n        try:\n            test[feature] = np.log(test[feature])\n        except:\n            print(\"some error in test: \", feature)\n    print(\"After log transformation, Min/Max values of {} are {} - {}\".format(feature, test[feature].min(), test[feature].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n4\"></a>\n<font color=\"darkblue\" size=+3>Box plots of Numerical Columns</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = ['Age', 'Pdays', 'Previous', 'Duration', 'Campaign']\n\nN = 50\n\ny0 = train['age']\ny1 = train['pdays']\ny2 = train['previous']\ny3 = train['duration']\ny4 = train['campaign']\n\ny_data = [y0, y1, y2, y3, y4]\n\ncolors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', 'rgba(207, 114, 255, 0.5)']\n\nfig = go.Figure()\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        fig.add_trace(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints='outliers', notched=True,\n            jitter=0.5,\n            whiskerwidth=0.4,\n            fillcolor=cls,\n            marker_size=2,\n            line_width=1)\n        )\n\nfig.update_layout(\n    title='Box plots of numerical columns',\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=5,\n        gridcolor='rgb(255, 255, 255)',\n        gridwidth=1,\n        zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2,\n    ),\n    margin=dict(l=40, r=30, b=80, t=100,),\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)',\n    showlegend=False\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n5\"></a>\n<font color=\"darkblue\" size=+3>Unique Values of Various categorical Columns</font>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Discover the number of categories within each categorical feature:\nlen(train.job.unique()),  len(train.poutcome.unique()),len(train.month.unique()),len(train.contact.unique()), len(train.marital.unique()), len(train.loan.unique()), len(train.education.unique()), len(train.housing.unique()),len(train.default.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_back","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoders\n* Since there are a bunch of categorical columns, I decided to try the efficacy of various encoders.\n* Label encoder was the simplest and most effective\n* Since dates are not given and I presume it didn't matter\n* I have used the months, days columns as categorical variables\n* To compare the different encoders, I have used basic models - Logistic Regression and Random Forest models to conclude\n* Label Encoder - I have used with Standard scaler and MinMax scaler. But there was not a remarkable difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [feature for feature in train_back.columns if ( train_back[feature].dtypes == 'O') ]\ncat_features.append('day')\nprint(\"Removed columns - \", cat_features.pop(-2))\n\n# We create a helper function to get the scores for each encoding method:\ndef get_score(model, X, y, X_val, y_val,X_test):\n    model.fit(X, y)\n    y_pred = model.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    y_pred = model.predict(X_test)\n    return score,y_pred\n\ntarget_feature =  'subscribed'\n\nSEED = 123\nlogit = LogisticRegression(random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n6\"></a>\n<font color=\"darkblue\" size=+3>Label Encoder with StandardScaler</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_train = train_back.copy()\nlb_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  lb_train[feature]= label_encoder.fit_transform(lb_train[feature]) \n  lb_test[feature]= label_encoder.fit_transform(lb_test[feature]) \n\nlb_train[target_feature] = lb_train[target_feature].map({\"yes\":1, \"no\":0})\nlb_y = lb_train[target_feature]\nlb_train.drop([target_feature],axis= 1, inplace=True)\n\n# feature scaling\nscaler = StandardScaler()\nlb_train = scaler.fit_transform(lb_train)\nlb_test = scaler.transform(lb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(lb_train, lb_y, test_size=0.2, random_state = SEED)\n\nbaseline_logit_with_standard,y_pred_logit = get_score(logit, X_train, y_train, X_val, y_val, lb_test)\nprint('Logistic Regression score without feature engineering:', baseline_logit_with_standard)\n\nbaseline_rf_with_standard,y_pred_rf = get_score(rf, X_train, y_train, X_val, y_val, lb_test)\nprint('Random Forest score without feature engineering:', baseline_rf_with_standard)\n\ndel lb_train;\ngc.collect() \ndel lb_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n7\"></a>\n<font color=\"darkblue\" size=+3>Label Encoder with MinMaxScaler</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_train = train_back.copy()\nlb_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  lb_train[feature]= label_encoder.fit_transform(lb_train[feature]) \n  lb_test[feature]= label_encoder.fit_transform(lb_test[feature]) \n\nlb_train[target_feature] = lb_train[target_feature].map({\"yes\":1, \"no\":0})\nlb_y = lb_train[target_feature]\nlb_train.drop([target_feature],axis= 1, inplace=True)\n\n# feature scaling\nscaler = MinMaxScaler()\nlb_train = scaler.fit_transform(lb_train)\nlb_test = scaler.transform(lb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(lb_train, lb_y, test_size=0.2, random_state = SEED)\n\nbaseline_logit_with_minmax, y_pred_logit_minmax = get_score(logit, X_train, y_train, X_val, y_val, lb_test)\nprint('Logistic Regression score without feature engineering:', baseline_logit_with_minmax)\n\nbaseline_rf_with_minmax,y_pred_rf_minmax = get_score(rf, X_train, y_train, X_val, y_val, lb_test)\nprint('Random Forest score without feature engineering:', baseline_rf_with_minmax)\n\ndel lb_train;\ngc.collect() \ndel lb_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n8\"></a>\n<font color=\"darkblue\" size=+3>One-Hot Encoder</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe_train = train_back.copy()\nohe_test = test_back.copy()\n\none_hot_enc = OneHotEncoder(sparse=False)\n\nohe_train[target_feature] = ohe_train[target_feature].map({\"yes\":1, \"no\":0})\nohe_y = ohe_train[target_feature]\nohe_train.drop([target_feature],axis= 1, inplace=True)\n\nprint(\"Before Target Encoder - Shape of Train/Test: \", ohe_train.shape, ohe_test.shape)\nohe_train = (one_hot_enc.fit_transform(ohe_train[cat_features]))\nohe_test = (one_hot_enc.transform(ohe_test[cat_features]))\nprint(\"After One-Hot Encoder - Shape of Train/Test: \", ohe_train.shape, ohe_test.shape)\n\n# feature scaling\nscaler = StandardScaler()\nohe_train = scaler.fit_transform(ohe_train)\nohe_test = scaler.transform(ohe_test)\n\n# Split dataset into train and validation subsets:\nohe_X_train, ohe_X_val, ohe_y_train, ohe_y_val = train_test_split(ohe_train, ohe_y, test_size=0.2, random_state = SEED)\n\nohe_logit_score, y_pred_logit_ohe = get_score(logit, ohe_X_train, ohe_y_train, ohe_X_val, ohe_y_val, ohe_test)\nprint('Logistic Regression score without feature engineering:', ohe_logit_score)\n\nohe_rf_score, y_pred_rf_ohe = get_score(rf, ohe_X_train, ohe_y_train, ohe_X_val, ohe_y_val, ohe_test)\nprint('Random Forest score without feature engineering:', ohe_rf_score)\n\ndel ohe_train;\ngc.collect() \ndel ohe_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n9\"></a>\n<font color=\"darkblue\" size=+3>Target Encoder</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_train = train_back.copy()\nhash_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  hash_train[feature]= label_encoder.fit_transform(hash_train[feature]) \n  hash_test[feature]= label_encoder.fit_transform(hash_test[feature]) \n\nhash_train[target_feature] = hash_train[target_feature].map({\"yes\":1, \"no\":0})\nhash_y = hash_train[target_feature]\nhash_train.drop([target_feature],axis= 1, inplace=True)\n\nfrom category_encoders import TargetEncoder\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\n\ntarg_enc = TargetEncoder(cols = columns, smoothing=8, min_samples_leaf=5).fit(hash_train, hash_y)\n\nprint(\"Before Target Encoder - Shape of Train/Test: \", hash_train.shape, hash_test.shape)\nhash_train_te = targ_enc.transform(hash_train.reset_index(drop=True))\nhash_test_te = targ_enc.transform(hash_test.reset_index(drop=True))\nprint(\"After Target Encoder - Shape of Train/Test: \", hash_train_te.shape, hash_test_te.shape)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(hash_train_te, hash_y, test_size=0.2, random_state = SEED)\n\nte_logit_score, y_pred_logit_te = get_score(logit, X_train, y_train, X_val, y_val, hash_test)\nprint('Logistic Regression score with target encoding:', te_logit_score)\n\nte_rf_score, y_pred_rf_te = get_score(rf, X_train, y_train, X_val, y_val, hash_test)\nprint('Random Forest score with target encoding:', te_rf_score)\n\ndel hash_train;\ngc.collect() \ndel hash_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n10\"></a>\n<font color=\"darkblue\" size=+3>Hashing Encoder</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"he_train = train_back.copy()\nhe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  he_train[feature]= label_encoder.fit_transform(he_train[feature]) \n  he_test[feature]= label_encoder.fit_transform(he_test[feature]) \n\nhe_train[target_feature] = he_train[target_feature].map({\"yes\":1, \"no\":0})\nhe_y = he_train[target_feature]\nhe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\n\ntarg_enc = HashingEncoder(cols = columns,  n_components=1000).fit(he_train, he_y)\nprint(\"Before Hashing Encoder - Shape of Train/Test: \", he_train.shape, he_test.shape)\nhe_train = targ_enc.transform(he_train.reset_index(drop=True))\nhe_test = targ_enc.transform(he_test.reset_index(drop=True))\nprint(\"After Hashing Encoder - Shape of Train/Test: \", he_train.shape, he_test.shape)\n\n# Split dataset into train and validation subsets:\nX_train_te, X_val_te, y_train_te, y_val_te = train_test_split(he_train, he_y, test_size=0.2, random_state = SEED)\n\nhe_logit_score, y_pred_logit_he = get_score(logit, X_train_te, y_train_te, X_val_te, y_val_te, he_test)\nprint('Logistic Regression score with Hashing encoding:', he_logit_score)\n\nhe_rf_score, y_pred_rf_he = get_score(rf, X_train_te, y_train_te, X_val_te, y_val_te, he_test)\nprint('Random Forest score with Hashing encoding:', he_rf_score)\n\ndel he_train;\ngc.collect() \ndel he_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n11\"></a>\n<font color=\"darkblue\" size=+3>Weight Of Evidence (WOE)</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"woe_train = train_back.copy()\nwoe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  woe_train[feature]= label_encoder.fit_transform(woe_train[feature]) \n  woe_test[feature]= label_encoder.fit_transform(woe_test[feature]) \n\nwoe_train[target_feature] = woe_train[target_feature].map({\"yes\":1, \"no\":0})\nwoe_y = woe_train[target_feature]\nwoe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\nwoe_enc = WOEEncoder(cols=columns, random_state=17).fit(woe_train, woe_y)\n\nprint(\"Before WOE Encoder - Shape of Train/Test: \", woe_train.shape, woe_test.shape)\nwoe_train_wo = woe_enc.transform(woe_train.reset_index(drop=True))\nwoe_test_wo = woe_enc.transform(woe_test.reset_index(drop=True))\nprint(\"After WOE Encoder - Shape of Train/Test: \", woe_train_wo.shape, woe_test_wo.shape)\n\n# Split dataset into train and validation subsets:\nX_train_woe, X_val_woe, y_train_woe, y_val_woe = train_test_split(woe_train_wo, woe_y, test_size=0.2, random_state = SEED)\n\nwoe_logit_score, y_pred_logit_woe = get_score(logit, X_train_woe, y_train_woe, X_val_woe, y_val_woe, woe_test)\nprint('Logistic Regression score with Weight Of Evidence encoding:', woe_logit_score)\n\nwoe_rf_score, y_pred_rf_woe = get_score(rf, X_train_woe, y_train_woe, X_val_woe, y_val_woe, woe_test)\nprint('Random Forest score with Weight Of Evidence encoding:', woe_rf_score)\n\ndel woe_train;\ngc.collect() \ndel woe_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n12\"></a>\n<font color=\"darkblue\" size=+3>Binary Encoder</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"be_train = train_back.copy()\nbe_test = test_back.copy()\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  be_train[feature]= label_encoder.fit_transform(be_train[feature]) \n  be_test[feature]= label_encoder.fit_transform(be_test[feature]) \n\nbe_train[target_feature] = be_train[target_feature].map({\"yes\":1, \"no\":0})\nbe_y = be_train[target_feature]\nbe_train.drop([target_feature],axis= 1, inplace=True)\n\ncolumns = ['job', 'marital', 'education', 'default', \\\n       'housing', 'loan', 'contact', 'day', 'month', 'poutcome']\nbe_enc = BinaryEncoder(cols=columns).fit(be_train, be_y)\n\nprint(\"Before Binary Encoder - Shape of Train/Test: \", be_train.shape, be_test.shape)\nbe_train = be_enc.transform(be_train.reset_index(drop=True))\nbe_test = be_enc.transform(be_test.reset_index(drop=True))\nprint(\"After Binary Encoder - Shape of Train/Test: \",be_train.shape, be_test.shape)\n\n# Split dataset into train and validation subsets:\nX_train_be, X_val_be, y_train_be, y_val_be = train_test_split(be_train, be_y, test_size=0.2, random_state = SEED)\n\nbe_logit_score, y_pred_logit_be = get_score(logit, X_train_be, y_train_be, X_val_be, y_val_be, be_test)\nprint('Logistic Regression score with Binary encoding:', be_logit_score)\n\nbe_rf_score, y_pred_rf_be = get_score(rf, X_train_be, y_train_be, X_val_be, y_val_be, be_test)\nprint('Random Forest score with Binary encoding:', be_rf_score)\n\ndel be_train;\ngc.collect() \ndel be_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n13\"></a>\n<font color=\"darkblue\" size=+3>Comparison Table - Encoders</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\n\nmyTable = PrettyTable([\"SNo.\", \"Encoder\", \"Logistic\", \"Random Forest\", \"No. of Cols added\"]) \n  \n# Add rows \nmyTable.add_row([\"1\", \"One Hot\", round(ohe_logit_score,4), round(ohe_rf_score,4), 58 ]) \nmyTable.add_row([\"2\", \"Hashing\", round(he_logit_score,4), round(he_rf_score,4), 0]) \nmyTable.add_row([\"3\", \"Target\", round(te_logit_score,4), round(te_rf_score,4), 0 ]) \nmyTable.add_row([\"4\", \"Weight of Evaluation\", round(woe_logit_score,4), round(woe_rf_score,4), 0]) \nmyTable.add_row([\"5\", \"Binary\", round(be_logit_score,4), round(be_rf_score,4) , 24 ]) \nmyTable.add_row([\"6\", \"Label Encoder + Standard Scaler\", round(baseline_logit_with_standard,4), round(baseline_rf_with_standard,4), 0 ]) \nmyTable.add_row([\"7\", \"Label Encoder + MinMax Scaler\", round(baseline_logit_with_minmax,4), round(baseline_rf_with_minmax, 4), 0 ]) \n\nprint(myTable)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n* Target Encoder, Weight of Evalaution, Binary Encoder performed well in Random forest with 0.94 accuracy score.\n* However the above encoders were not performing well with  logistic regression.\n* Label encoder was performing well with both logistic and randomforest models\n* Extra Columns were added only to One hot and Binary encoders."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n14\"></a>\n<font color=\"darkblue\" size=+3>Models Comparison</font>"},{"metadata":{},"cell_type":"markdown","source":"* Label Encoder has been most effective encoder for categorical columns.\n* Below code has been borrrowed from Mohapatra's notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), KNeighborsClassifier(), SVC(), XGBClassifier()]\nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'SVC', 'XGBClassifier']\naccuracy_train = []\naccuracy_val = []\nfor model in models:\n    mod = model\n    mod.fit(X_train, y_train)\n    y_pred_train = mod.predict(X_train)\n    y_pred_val = mod.predict(X_val)\n    accuracy_train.append(accuracy_score(y_train, y_pred_train))\n    accuracy_val.append(accuracy_score(y_val, y_pred_val))\ndata = {'Modelling Algorithm' : model_names, 'Train Accuracy' : accuracy_train, 'Validation Accuracy' : accuracy_val}\ndata = pd.DataFrame(data)\ndata['Difference'] = ((np.abs(data['Train Accuracy'] - data['Validation Accuracy'])) * 100)/(data['Train Accuracy'])\ndata.sort_values(by = 'Difference')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n15\"></a>\n<font color=\"darkblue\" size=+3>Modelling with Best Classifier</font>"},{"metadata":{},"cell_type":"markdown","source":"* XGBClassifier is high on train and validation accuracy. So, I have picked this up.\n* roc_auc_score is at .93.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\nparameters = {   'eta': [0.1], 'colsample_bytree':[0.7],\n               'min_child_weight': [5], 'max_depth' :[7], 'max_features':[5],'subsample': [0.7],\n               'reg_alpha':[1], 'n_estimators': [100] ,'seed':[11] }\n\nxgb_clf = GridSearchCV(xgb, parameters, cv = 5, n_jobs = -1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_train = train_back.copy()\nxgb_test = test_back.copy()\n\nxgb_train['duration'] = xgb_train['duration'] + 1\nxgb_train['previous'] = xgb_train['previous'] + 2\nxgb_train['pdays'] = xgb_train['pdays'] + 2\n\ncols = ['age','pdays','previous','campaign','duration']\n\nfor feature in cols:\n    print(\"\\nMin/Max values of {} are {}, {}\".format(feature, xgb_train[feature].min(), xgb_train[feature].max()))    \n    if 0 in xgb_train[feature].unique():\n        pass\n    else:\n        try:\n            xgb_train[feature] = np.log(xgb_train[feature])           \n        except:\n            print(\"some error in train: \", feature)\n    print(\"After log, transformation - Min/Max values of {} are {} - {}\".format(feature, xgb_train[feature].min(), xgb_train[feature].max()))\n\nprint(\" = \" * 60)\n\nxgb_test['duration'] = xgb_test['duration'] + 1\nxgb_test['previous'] = xgb_test['previous'] + 2\nxgb_test['pdays'] = xgb_test['pdays'] + 2\n\nfor feature in cols:\n    print(\"\\nMin/Max values of {} are {}, {}\".format(feature, xgb_test[feature].min(), xgb_test[feature].max()))    \n    if 0 in xgb_test[feature].unique():\n        pass\n    else:\n        try:\n            xgb_test[feature] = np.log(xgb_test[feature])\n        except:\n            print(\"some error in test: \", feature)\n    print(\"After log transformation, Min/Max values of {} are {} - {}\".format(feature, xgb_test[feature].min(), xgb_test[feature].max()))\n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor feature in cat_features:\n  xgb_train[feature]= label_encoder.fit_transform(xgb_train[feature]) \n  xgb_test[feature]= label_encoder.fit_transform(xgb_test[feature]) \n\nxgb_train[target_feature] = xgb_train[target_feature].map({\"yes\":1, \"no\":0})\nxgb_y = xgb_train[target_feature]\nxgb_train.drop([target_feature],axis= 1, inplace=True)\n\nxgb_train.drop(['ID'],axis= 1, inplace=True)\nxgb_test.drop(['ID'],axis= 1, inplace=True)\n\n# feature scaling\nscaler = StandardScaler()\nxgb_train = scaler.fit_transform(xgb_train)\nxgb_test = scaler.transform(xgb_test)\n\n# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(xgb_train, xgb_y, test_size=0.25, random_state = SEED)\n\nxgb_clf.fit(X_train, y_train)\npredictions = xgb_clf.predict_proba(X_val)[:,1]\nscore = roc_auc_score(y_val,predictions)\nprint(\"XGB score: \",score)\n\ny_pred = xgb_clf.predict(X_val)\n\ndel xgb_train;\ngc.collect() \ndel xgb_test;\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test = pd.DataFrame(y_pred, columns = ['Prediction'])\nprint(y_pred_test.head())\n\ny_pred_test.to_csv('Prediction.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"n16\"></a>\n<font color=\"darkblue\" size=+3>Confusion Matrix and Classification report</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate Confusion Matrix\n\nconf_matrix = confusion_matrix(y_pred,y_val)\nprint(conf_matrix)\n\nprint(\" = \"*60)\nprint(classification_report(y_pred,y_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}