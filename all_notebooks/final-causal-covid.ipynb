{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Causal Relation extraction : from unstructured text data in biorxiv abstract\n\n- We want to identify risk factors for the COVID-19 corona virus.  How they co-exist and how they interact. These risk factors can be psychological, demographical, environmental as well as genetic. The following are the algorithms we will be using to get closer to the goal. \n\n- We are Extracting  biomedical named entities using sciSpacy. Identifying cause-effect relationships among various entities using the causal-relation-prediction model [BERT-based causal relation extraction](https://github.com/wang-h/bert-relation-classification).This is an attempt to identify bioNER that has cause-effect with coronavirus or similar viruses. \n\n- Finally, this notebook part of the puzzle not end to end solution. "},{"metadata":{},"cell_type":"markdown","source":"> "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"* ### Movinng pretrained R-Bert model to working directory after adding them to data: search for `causalcovidupdated`  : it is made public "},{"metadata":{"trusted":true},"cell_type":"code","source":"## copied the causal model file in working directory \n\n!cp -r ../input/causalcovidupdated/models ../working/models\n!cp -r ../input/causalcovidupdated/bert_relation/.* ../working/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Let's download the requrie models and dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" !pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n    \n !pip install pytorch-transformers==1.1\n !pip install scispacy\n !pip install negspacy\n\n !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz ## scispacy model\n !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz ## scispacy model \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Let's load the needed libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport argparse\nfrom argparse import ArgumentParser\nimport glob\nimport logging\nimport os\nimport os.path as p\n\nimport sys\nimport random\nimport torch.nn as nn\nimport numpy as np\nimport torch\nimport socket\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm, trange\n\n\n\npd.set_option('display.max_colwidth', 500)\npd.set_option(\"display.width\", 1000)\n\n\n\n## importaing library to clean and tokenize the dataset \nfrom negspacy.negation import Negex\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\nimport spacy\n\n\n## loading more library from downloaded libraries  \nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# from tensorboardX import SummaryWriter\nfrom pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertTokenizer)\nfrom pytorch_transformers import AdamW, WarmupLinearSchedule\nimport torch.nn.functional as F\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## loading files for causal relation extraction : imported data folder in the working directory \n\nimport bert\nfrom utils import *\n\nfrom config import Config ## from working directory \n#load the configuration file\nconfig1 = Config(\"config.ini\")\n\nadditional_special_tokens = []\n\n\nfrom model import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained(\"models\")\ntokenizer = BertTokenizer.from_pretrained(\"models\", do_lower_case=True, additional_special_tokens=additional_special_tokens)\n\nlogger = logging.getLogger(__name__)\n#additional_special_tokens = [\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"]\n\n#additional_special_tokens = [\"e11\", \"e12\", \"e21\", \"e22\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Loading the dataset and saving as df: just reading biorxiv in this notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n## Let's start with the metadata csv file\ndf_metadata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\",\n                        na_values=[], keep_default_na=False)\ndf_metadata.head()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf_citation = pd.read_csv(\"/kaggle/input/citation-network-output/ConsolidatedDfwithScore.csv\", na_values=[], keep_default_na=False)\ndf_citation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_citation.merge(df_biorxiv, on = \"paper_id\").head().sort(ScoreApproach2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### More functions to get dafaframe from provided json"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\n\n\n\n\n\n\n\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames),filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfile = all_files[0]\nprint(\"Dictionary keys:\", file.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\ndf_biorxiv = pd.DataFrame(cleaned_files, columns=col_names)\ndf_biorxiv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the dataset for input to the causal model \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sys, os\n# import os.path as p\n\n## a function to find teh downloaded model path in environment\n\ndef find_model(model_name):\n    path_to_env = p.abspath(p.join(sys.executable, \"../..\"))\n    path_to_modules = p.join(path_to_env, f\"lib/python{sys.version[:3]}/site-packages\")\n    path_to_model = p.join(path_to_modules, model_name)\n    if not p.exists(path_to_model):\n        raise FileNotFoundError(path_to_model)\n    model_dir = [d for d in os.listdir(path_to_model) if d.startswith(model_name)][0]\n    return p.join(path_to_model, model_dir)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NLP parsers \n* we are using two sciSpacy models to extract biomedical entities from the provided abstract : `en_ner_bionlp13cg_md` and `en_ner_bc5cdr_md`\n* negspacy for negation detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nnlp_path = find_model(\"en_ner_bionlp13cg_md\")\nnlp2_path = find_model(\"en_ner_bc5cdr_md\")\n\n#nlp = spacy.load(\"en_core_web_sm\")\nnlp = spacy.load(nlp_path)\n#nlp = spacy.load(\"/usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.2.4\")\nnlp2 = spacy.load(nlp2_path)\nnegex = Negex(nlp, language = \"en_clinical_sensitive\")\nnlp.add_pipe(negex)\n#linker = UmlsEntityLinker(resolve_abbreviations=True)\n#nlp.add_pipe(linker)\nnlp2.add_pipe(negex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions to clean and annotate the dataset : for pretrained causal BERT based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Functions to clean the dataset \n\n\nimport re\ndef remove_braces(text):\n    text = re.sub(r\" ?\\([^)]+\\)\", \"\", text)\n    return text\n\n\n#######################################################\ndef clean_sentence(clean_sen):\n  clean_sen = clean_sen.replace(\"\\n\",\" \")\n  clean_sen = remove_braces(clean_sen)\n  return clean_sen\n\n\n#######################################################\ndef filter_spans(spans):\n    # Filter a sequence of spans so they don't contain overlaps\n    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n    result = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        # Check for end - 1 here because boundaries are inclusive\n        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n            result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n    return result\n\n\n\n\n\n\n#######################################################\ndef extract_all_relations(doc,chunks):\n    # Merge entities and noun chunks into one token\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n\n    relations = []\n    for entity in filter(lambda w: w.text in chunks, doc):\n        if entity.dep_ in (\"attr\", \"dobj\"):\n            subject = [w for w in entity.head.lefts if w.dep_ == \"nsubj\"]\n            if subject:\n                subject = subject[0]\n                relations.append((subject, entity))\n        elif entity.dep_ == \"pobj\" and entity.head.dep_ == \"prep\":\n            relations.append((entity.head.head, entity))\n    return relations\n#######################################################\n\n\n\ndef parse_sentence(sentence_text):\n  tokens = nlp(sentence_text)\n  tokens2 = nlp2(sentence_text)\n  noun_chunks = [chunk.text for chunk in tokens.ents]\n  noun_chunks2 = [chunk.text for chunk in tokens2.ents]\n  \n  ## what this negs doing \n  negs = [chunk._.negex for chunk in tokens.ents]\n  negs2 = [chunk._.negex for chunk in tokens2.ents]\n\n    \n    ## why we are combining these like this....\n  for index in range(len(noun_chunks2)):\n    if (noun_chunks2[index] not in noun_chunks):\n      noun_chunks.append(noun_chunks2[index])\n      negs.append(negs2[index])\n\n  return noun_chunks,negs\n\n#################################################################\n\ndef annotate_sentence(cleaned_text,cleaned_nounchunks,negs):\n  l_sentences = []\n  l_start_nodes = []\n  l_end_nodes = []\n  l_negs = []\n  size = len(cleaned_nounchunks)\n  if (size >= 2):\n    for x in range(0,size):\n      for y in range(x,size):\n        if (x != y): # not relevant\n          mod_sen = cleaned_text.replace(cleaned_nounchunks[x],\"#\"+cleaned_nounchunks[x]+\"# \")\n          mod_sen = mod_sen.replace(cleaned_nounchunks[y],\"$\"+cleaned_nounchunks[y]+\"$ \")\n          if ((mod_sen.count('#') == 2) and (mod_sen.count('$') == 2)):\n              #if (((mod_sen.count('#') % 2) == 0) & (((mod_sen.count('$') % 2) == 0))):\n              #tokens_a = tokenizer.tokenize(mod_sen)\n              #if ((\"#\" in tokens_a) & (\"$\" in tokens_a)):\n              #if ((len(tokens_a) >= 8) and ((tokens_a.count('#') == 2) and (tokens_a.count('$') == 2))):\n              l_sentences.append(mod_sen)\n              l_start_nodes.append(cleaned_nounchunks[x])\n              l_end_nodes.append(cleaned_nounchunks[y])\n              if (negs[x] | negs[y]):\n                    l_negs.append(True)\n              else:\n                    l_negs.append(False)\n  return l_sentences, l_start_nodes, l_end_nodes, l_negs\n\n\n\n########################################################################\n\ndef prepare_annotationset_biorxiv(frame_df):\n\n    dataset_sentences_paperid = []\n    dataset_sentences = []\n    dataset_start_nodes = []\n    dataset_end_nodes = []\n    dataset_negs = []\n\n    #print(bio_df.index)\n    for ind in frame_df.index:\n         text = frame_df['abstract'][ind]\n         #print(text)\n         if ((text != None) & (type(text) == str)):\n             doc = nlp(text)\n             for sentence in doc.sents:\n               sen = clean_sentence(sentence.text)\n               noun_chunks, negs = parse_sentence(sen)\n#                dataset_sentences_paperid.extend(frame_df['paper_id'][ind])\n               dataset_sentences_paperid.extend([frame_df.at[ind,'paper_id']])\n                \n               annotated_sen, start_node, end_node, node_neg = annotate_sentence(sen,noun_chunks,negs)\n               dataset_sentences.extend(annotated_sen)\n               dataset_start_nodes.extend(start_node)\n               dataset_end_nodes.extend(end_node)\n               dataset_negs.extend(node_neg)\n    return dataset_sentences_paperid, dataset_sentences, dataset_start_nodes, dataset_end_nodes, dataset_negs\n\n##############################################################################\n\n\n\ndef write_annotation_set(sentences,output_file):\n  ofile = open(output_file,\"w+\") \n  count = 8001\n  for val in sentences:\n    ofile.write(str(count)+\"\\t\"+val+\"\\t\"+\"6\"+\"\\n\")\n    count = count+1\n  ofile.close()\n\n################################################################################\n\n\ndef read_result():\n  relations_found = []\n  result_file = open(\"eval/sem_res.txt\",\"r\")\n  lines = result_file.readlines()\n  for line in lines:\n    split_vals = line.split(\"\\t\")\n    relations_found.append(split_vals[1].rstrip(\"\\n\"))\n  return relations_found\n\n\n\n################################################################################\n\ndef write_causal_set(sentences,output_file):\n  ofile = open(output_file,\"w+\") \n  count = 1\n  for val in sentences:\n    ofile.write(str(count)+\"\\t\"+val+\"\\n\")\n    count = count+1\n  ofile.close()\n\n###############################################################################\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"## trying above defined functions to parse and annotate a single sentence \nentities, negs= parse_sentence(\"Developing a deep learning-based model for automatic COVID-19 detection on chest CT is helpful to counter the outbreak of SARS-CoV-2 \")\nprint(entities)\nprint(negs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## removing rows with abstract == NAN\n# df_causal = df_merged[df_merged['abstract'].notna()]\n\n\n# ## taking top-k abstract -- as ranked by citation network\n\n# k = 1000\n# df_causal = df_causal[:k]\n# df_causal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_biorxiv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I will be usign biorxiv dataset for further investigation "},{"metadata":{"trusted":true},"cell_type":"code","source":"## annotating the sentences and saving to data folder in workign directory: this data folder is part of improrted stuffs from Input directory\n\ndataset_sentences_paperid, sentences , start_nodes, end_nodes, all_negs = prepare_annotationset_biorxiv(df_biorxiv) #df_biorxiv -- dataframe constructed usign biorxiv json files\nlen(sentences)\nwrite_annotation_set(sentences,\"data/dev.tsv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## causal model on Annotated sentences "},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the configuration file,  trained model and vocabulary are loaded earlier -- they are in workign directory \n\n# set up details for the type of device\nif config1.local_rank == -1 or config1.no_cuda:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and not config1.no_cuda else \"cpu\")\n    config1.n_gpu = torch.cuda.device_count()\nelse:\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n    torch.cuda.set_device(config1.local_rank)\n    device = torch.device(\"cuda\", args.local_rank)\n    torch.distributed.init_process_group(backend='nccl')\n    config1.n_gpu = 1\nconfig1.device = device\nmodel.to(config1.device)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## bert.evaluate(config1,model,tokenizer)\nbert.evaluate(config1,model,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"COVID19_SYM_FILTER = ['covid',\n                    'sars-cov-2',\n                    'covid-19',\n                    'covid19',\n                    'coronavirus',\n                    'corona',\n                    'coronavirus disease']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relations_found = read_result()\nlen(relations_found)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_negs), len(end_nodes), len(start_nodes), len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relations_found = read_result()\ncausal_sentences = []\n\ncolumn_names = [\"paper_id\", \"risk_factor\", \"sentence\"]\n\n# causal_for_covid_df = pd.DataFrame(columns = column_names)\n\nlist_of_list = []\n\n## we can \nfor rel_index in range(len(relations_found)):\n  if (\"Cause-Effect\" in relations_found[rel_index]):\n      risk_factor = \"\"\n\n      \n      if ((start_nodes[rel_index] in COVID19_SYM_FILTER) or (end_nodes[rel_index] in COVID19_SYM_FILTER)):\n        if (sentences[rel_index] and (all_negs[rel_index] == False)):\n            risk_factor = (start_nodes[rel_index], end_nodes[rel_index])\n            list_of_list.append([str(dataset_sentences_paperid[rel_index]),  str(risk_factor), str(sentences[rel_index])])\n            \n#           if (start_nodes[rel_index] not in COVID19_SYM_FILTER):\n#             risk_factor = end_nodes[rel_index]\n#           elif (end_nodes[rel_index] not in COVID19_SYM_FILTER):\n#             risk_factor = start_nodes[rel_index]\n#           else:\n#             risk_factor = None\n#           if (risk_factor != None):\n#             list_of_list.append([str(dataset_sentences_paperid[rel_index]),  str(risk_factor), str(sentences[rel_index])])\n#           else:\n#             print(\"tada\")\n#             print([str(dataset_sentences_paperid[rel_index]),  str(risk_factor), str(sentences[rel_index])])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"causal_for_covid_df = pd.DataFrame(list_of_list, columns=column_names)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# causal_for_covid_df = causal_for_covid_df[causal_for_covid_df.risk_factor not in COVID19_SYM_FILTER]\n\n# write_causal_set(causal_sentences,\"causal.txt\")\ncausal_for_covid_df.to_csv(\"causal.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"causal_for_covid_df = causal_for_covid_df.groupby('paper_id').agg({'risk_factor': ', '.join, 'sentence':'.'.join }).reset_index()\ncausal_for_covid_df.head()                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion: \n- we are able to extract risk-factors and sentences with risk-factors in varioius abstract.\n- In the dataframe (causal_for_covid_df):\n    * risk-factor column: a list of tuples -->. listing risk-factors having cause-effect relationship with coronavirus/Covid-19, etc.\n    * sentences column: the sentences in the abstract -->> with these risk factors "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# relations_found = read_result()\n# causal_sentences = []\n\n# column_names = [\"paper_id\", \"risk_factor\", \"sentence\"]\n\n# causal_for_covid_df = pd.DataFrame(columns = column_names)\n\n# # print(causal_for_covid_df)\n\n# ## we can \n# for rel_index in range(0,len(relations_found)):\n# #   print(\"\\n\")\n#   if (\"Cause-Effect\" in relations_found[rel_index]):\n#       print(\"\\n\")\n#       risk_factor = \"\"\n    \n    \n#       if ((start_nodes[rel_index] in COVID19_SYM_FILTER) or (end_nodes[rel_index] in COVID19_SYM_FILTER)):\n#         if (sentences[rel_index] and (all_negs[rel_index] == True)):\n#             # we are ignoring this for now but could be useful later\n#             print(start_nodes[rel_index]+\" not caused \"+end_nodes[rel_index]+ \" in : \"+sentences[rel_index])\n#             causal_sentences.append(start_nodes[rel_index]+\" not caused \"+end_nodes[rel_index]+ \" in : \"+sentences[rel_index])\n#         else:  \n#             print(start_nodes[rel_index]+\" caused \"+end_nodes[rel_index]+ \" in : \"+sentences[rel_index])\n            \n#             if (start_nodes[rel_index] not in COVID19_SYM_FILTER):\n#                 risk_factor = end_nodes[rel_index]\n#             elif (end_nodes[rel_index] not in COVID19_SYM_FILTER):\n#                 risk_factor = start_nodes[rel_index]\n#             else:\n#                 risk_factor = None\n            \n#             if (risk_factor != None):\n#                 print(dataset_sentences_paperid[rel_index])\n                \n#                 df_row = pd.DataFrame(dataset_sentences_paperid[rel_index], risk_factor, sentences[rel_index])\n#                 causal_for_covid_df.append(df_row)\n                \n#             causal_sentences.append(start_nodes[rel_index]+\" caused \"+end_nodes[rel_index]+ \" in : \"+sentences[rel_index])\n\n# write_causal_set(causal_sentences,\"causal.txt\")\n# causal_for_covid_df.to_csv(\"causal.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# import re\n    \n# space_or_dash_regex = r\"(?:\\s+|\\s*-\\s*)\"\n# space_or_comma_regex = r\"(?:\\s+|\\s*[,;]\\s*)\"\n# space_or_colon_regex = r\"(?:\\s+|\\s*:\\s*)\"\n# int_number_regex = r\"[+-]?\\d+\"\n# real_number_regex = int_number_regex + r\"(?:\\.\\d+)?\"\n# or_value_regex = r\"OR\\s+\" + real_number_regex\n# real_interval_regex = r\"(?:\" + real_number_regex + r\"\\s*-\\s*\" + real_number_regex + r\"|\\[\" + real_number_regex +\\\n#                       r\"\\s+-\\s+\" + real_number_regex + r\"\\])\"\n# ci_regex = r\"CI\" + space_or_colon_regex + real_interval_regex\n# or_ci_regex = or_value_regex + r\"(?:\" + space_or_comma_regex + r\"\\(?\\s*\" + real_number_regex + r\"\\s*%\\s*\" + ci_regex +\\\n#               r\"(?:\\s*\\))?)?\"\n# comparator_or_space_regex = r\"(?:\\s*[=<>]\\s*|\\s+)\"\n# p_value_regex = r\"p(?:\" + space_or_dash_regex + r\"value)?\" + comparator_or_space_regex + real_number_regex\n# severe_regex = r\"(?:\" + or_ci_regex + \")\"\n# design_regex = r\"|\".join([\n#     r\"case study\",\n#     r\"cross\" + space_or_dash_regex + \"sectional(?:\\s+case\" + space_or_dash_regex + \"control)?\",\n#     r\"meta\" + space_or_dash_regex + r\"analysis\",\n#     r\"(?:matched\\s+)?case\" + space_or_dash_regex + \"control\",\n#     r\"medical\\s+records\\s+review\",\n#     r\"(?:non\" + space_or_dash_regex + \")?randomized(?:\\s+control)?\\s+trial\",\n#     r\"prospective\\s+case\" + space_or_dash_regex + \"control\",\n#     r\"prospective\\s+cohort\",\n#     r\"retrospective\\s+cohort\",\n#     r\"seroprevalence\\s+survey\",\n#     r\"syndromic\\s+surveillance\"\n# ])\n# sample_regex = r\"(?:(?:sample|population)(?:\\s+size)?(?:\\s+of)?\\s+\" + int_number_regex + r\"|\" +\\\n#                int_number_regex + r\"\\s+(?:sample|population)(?:\\s+size)?\" + r\")\"\n\n# severe_pattern = re.compile(severe_regex)\n# design_pattern = re.compile(design_regex)\n# sample_pattern = re.compile(sample_regex)\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_relevant_information = causal_for_covid_df.copy()\n# df_relevant_information['severe'] = None\n# df_relevant_information['severe_significant'] = None\n# df_relevant_information['design'] = None\n# df_relevant_information['sample'] = None\n\n# for text in df_relevant_information['text'].values:    \n#     severe = severe_pattern.findall(text)\n#     design = design_pattern.findall(text)\n#     sample = sample_pattern.findall(text)\n#     if len(severe) > 0:\n#         df_relevant_information.loc[id, 'severe'] = severe[0]\n        \n#     if len(design) > 0:\n#         df_relevant_information.loc[id, 'design'] = design[0]\n#     if len(sample) > 0:\n#         df_relevant_information.loc[id, 'sample'] = sample[0]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}