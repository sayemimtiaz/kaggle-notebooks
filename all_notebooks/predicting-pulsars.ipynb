{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Pulsars\n### Stellar Classification With Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n#### Data Science Workflow\nAn introduction and summary of the approach I took to complete this project\n#### Business Case and Business Value  \nBackground  \nKey question asked about the dataset  \n#### Obtaining The Data\nSourcing the data  \nImporting the CSV  \n#### Scrubbing The Data\nExplore potential missing values  \nEnsure columns are of the correct data types  \nCopy the dataset to later normalize  \n#### Exploring The Data\nUse basic plots to visually examine relationships within the dataset  \nExploration and familiarization with the dataset through statistical analysis  \nDetecting potential skews  \nInvestigating possible multicollinearity   \n#### Modeling The Data\nTrain and test split the data  \nCreate a baseline model to work up from  \nModel the non-transformed data and review results   \nModel the normalized & transformed data and compare results to the non-transformed    \nCreate multiple types of models to see what is the most accurate, efficient, and informative   \nTune hyperparameters to find the ideal parameters for best performing model  \n#### Interpreting The Data\nCompare the models created in the last step and weigh the pros & cons of each type  \nReview the findings table to see which model is best, depending on our goals(accuracy, computational efficiency, etc)  \n#### Conclusion\nExpand upon our model review from the interpretation stage  \nConclusion and opinion on my ideal model    \nPotential future work  \nReferences"},{"metadata":{},"cell_type":"markdown","source":"## Data Science Workflow\n##### For this project I am going to use the OSEMiN process\n##### (Obtain, Scrub, Explore, Model, Interpret) \n\n- **Obtain** - This part was simple for this project, my dataset is Dr Robert Lyon's Predicting a Pulsar Star dataset from Kaggle: (https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star/data)\n- **Scrub** - The objective here is to identify errors, potential missing or corrupted fields in the data, and to clean up the dataset by discarding, replacing, or filling missing values and errors. \n- **Explore** - This is largely what EDA is. The goal in this phase is to understand patterns within our dataset by visualizing and testing to examine relationships in the data. \n- **Model** - This step encompases the higher level pre-processing such as normalization and synthetic oversampling of the minority class. After that we're ready to train some models to be able to give us some accurate predictive power to make intelligent business decisions. There are multiple steps of evaluation and refining of the models to ensure they're as accurate and unbiased as possible. \n- **Interpret** - In the final step, we examine our findings developed by our modeling and indentify important insights. As more data becomes available, this cycle can repeat and continue in a loop to build a faster, less biased, and more accurate model.\n \nBelow you can follow the different stages of the EDA and modeling process by scrolling to the respective title."},{"metadata":{},"cell_type":"markdown","source":"## Business Case\n#### Background\nFor this project, I assumed the role of a Data Scientist working with an astronomical research agency to train a machine learning model to classify whether a star is a pulsar or not.  \nA pulsar is a rotating, magnetized Neutron star that emits radiation(mostly radio, X-ray, and gamma wavelength) from its poles. Other than black holes, pulsars are the most dense objects in the known universe. Pulsars rotate with a very short and precise period which vary from star to star, however due to Radio Frequency Interference and background radiation noise, there are many false-positives when attempting to detect pulsars.  \nFor further reading on pulsars visit the Wikipedia page [here](https://en.wikipedia.org/wiki/Pulsar). \n\n#### Overall Goal:  \nThe goal is to train a machine learning model to classify pulsar candidate stars as either a true pulsar, or a false positive. \nThe overall goal is model accuracy, as computational cost is not of concern in this application.  \n"},{"metadata":{},"cell_type":"markdown","source":"## Business Value  \n### Why is this important?\n#### Labor\nIn the most basic way, the ability to effectively train a machine learning algorithm to accurately classify a pulsar from a false positive would free up human assets to be able to complete other tasks. Astronomers are highly educated, and therefore, generally well paid. Freeing up these monetary assets allows them to be productive elsewhere.  \n#### Cutting edge scientific discovery\nPulsars prove to be very helpful for physicists and astronomers for a variety of other reasons. Being able to quickly and effectively identify pulsars would allow more of them to be discovered and used for other types of experiments.  \nResearch on Pulsars has already yeilded two Nobel Peace Prizes. Pulsar research is at the tip of the spear in humanity's quest to understand gravitational waves and relativistic physics.  \n#### Gravitational waves\nObserving the precisely timed pulses from various pulsars scattered across the galaxy, astronomers aim to measure the slight variations in their signals which are thought to be caused by gravitational waves. Pulsar timing arrays are being used to uncover secrets of the early universe by hopefully confirming the observation of gravitational waves caused by pairs of supermassive black holes in the early universe, multidimensional strings, quasars, and other possible events from the first few moments following the Big Bang.  \n#### Nuclear physics  \nWith pulsars being very small stellar objects(~10km radius) but having an average mass of ~1.4 solar masses, they are incredibly dense. In fact, the density of a pulsar is several times greater than the density of atomic nuclei. There is little understood about the physics of what goes on inside a pulsar and therefore are an incredibly interesting phenomena for physicists. \n#### Mapping interstellar space\nDue to the regularity of the emissions from pulsars, they can be used to triangulate position in the galaxy. Humanity has included pulsar maps on the two Pioneer plaques in addition to the Voyager Golden Record. These maps use 14 pulsars, and the position of the Sun to show where we are on the galactic plane. This form of navigation could prove to be very useful as Humanity progresses to a Type II civilization(see: [Kardashev Scale](https://en.wikipedia.org/wiki/Kardashev_scale)).\n#### Measuring time\nAgain, using the regularity of a pulsar's pulses one can measure the passing of time, specifically establishing Ephemeris time which is free of the irregular constraints related to fluctuating mean solar time. Although pulsars are very precise and regular, the interference in the readings make this form of time measurement slightly less accurate and dependable when compared to an atomic clock - this however may change as our radio telescopes become more advanced.  "},{"metadata":{},"cell_type":"markdown","source":"# Obtain the Data  \nFor this project, I used Dr Robert Lyon's Predicting a Pulsar Star dataset. Also included in this step will be the importing of any libraries I'll need throughout all the steps of my analysis and modeling. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary tools\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nimport itertools\nimport math\nimport time\n\nimport statsmodels.formula.api as smf\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom scipy import stats\nfrom sklearn import svm\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, \\\n    roc_curve, auc, mean_squared_error, roc_auc_score, recall_score, precision_score, \\\n    plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n    GradientBoostingClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/pulsar-star/pulsar_stars.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are 17,898 pulsar candidates within this dataset. \n# 8 columns of descriptive data about each row/star and one column of categorical data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explaining the Features\nThe dataset describes 17,898 potential pulsar stars. There are 8 continuous variables describing the stars, and a class variable indicating whether or not the stars are actually a pulsar or not. \n\nTo understand what these variables mean, we'll need to look into some terminology related to the classification of pulsars.  \n\n**Integrated Pulse Profile:**  \nPulsars are distinguished by their integrated pulse profile which is a representation of its periodic pulsation signal. It is a visual representation of the pulsars emission signals with respect to its rotational period. Here's some examples of integrated pulse profiles:[1] ![Integrated Pulse Profiles](Images/Pulsar_IPP1.png)  \n\n**DM-SNR Curve:**  \nDM = dispersion measure. SNR = spectral signal-to-noise ratio.  \nDispersion measurement is the spread in frequency of the pulsar's emission after traveling through space. Free electrons and ionization within the interstellar medium affects the radiation in two ways. Due to the dispersive nature of interstellar plasma, higher-frequency waves traverse these distances more quickly than lower-frequency radio waves. The resulting delay is referred to as the dispersion measure. In other words, the DM is the \"total column density of free electrons between the observer and the pulsar\".  \nThe spectral SNR is the signal to noise ratio of all DM signal measurements taken.  \nThis is a common way to differentiate and classify candidate stars. Pulsar timing is exact, like that of a delta function, but radio telescopes pickup a spread centered around the peak value. This measurement is represented by the DM value, of which there is a spread from multiple measurements peaked around some DM value. You then take your DM signals and compare them as a signal-to-noise ratio for each candidate. \n\n- **Mean of the itegrated profile** - A mean taken from the respective pulsar's IPP.  \n- **Standard deviation of the integrated profile** - The standard deviation of a pulsar's IPP.  \n- **Excess kurtosis of the integrated profile** - Kurtosis measures the \"fatness\" of the tails of a given distribution. Positive excess kurtosis means a distribution has larger tails than a normal distribution. This means there is a less accurate distribution and may point to more noise.  \n- **Skewness of the integrated profile** - Skewness measures the asymettry of a distribution. In this context, if a candidate star's IPP has a negative skew, the signal is strong initially but then tapers off quickly. If a candidate star has a positive skew, the signal is first small before peaking to the right side of the distribution.  \n- **Mean of the DM-SNR curve** - This is the mean value of all dispersion measurments of a given star's signal to noise ratio.  \n- **Standard deviation of the DM-SNR curve** - The standard deviation of the DM-SNR curve shows how much variation is found in the measurements of a candidate star. The higher the standard deviation could suggest there is more noise and/or interference affecting the measurements.  \n- **Excess kurtosis of the DM-SNR curve** - High levels of DM-SNR kurtosis suggests more noise in an observation. \n- **Skewness of the DM-SNR curve** - A highly skewed DM-SNR curve means that there's a high amount of interference in the observation which could mean that there's a lot of ISM noise in between the candidate star and the radio telescope recording the signals.  \n- **target_class** - Is this candidate a pulsar?(1:yes, 0:no)\n"},{"metadata":{},"cell_type":"markdown","source":"# Scrub the Data\nNow that my data has been imported, the focus will shift to preprocessing the data to be able to give us some accurate models. This includes but not limited to identifying the removing nulls, appropriately dealing with outliers, normalizing the data, and accounting for missing or improperly entered data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null value check\ndata.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunate for me, the dataset is pretty complete and clean from the source. There are no missing values, incorrect data types, or other data entry errors. Before moving on to the exploration, I'm going to clone this dataset so I can work off of a normalized and transformed set, and keep one free of normalization and transformation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TN(Transformed/Normalized)data\nTNdata = data.copy()\nTNdata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the Data\nIn this step I'll use some basic visualizations to begin to explore relations in the dataset. \nI will further look into the dataset using visualizations and statistical analysis to gain a higher understanding of the distribution, possible multicollinearity, and other preparatory tasks to ensure I will be able to train a reliable model with the dataset."},{"metadata":{},"cell_type":"markdown","source":"### Target_class counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First I want to see how many instances of each class we have in the dataset \nplt.figure(figsize=(12,12))\nax = sns.countplot(y = data.target_class, palette=[\"red\", \"green\"],\n                  linewidth=1, edgecolor=\"black\")\nfor i, j in enumerate(data[\"target_class\"].value_counts().values):\n    ax.text(.75, i, j, fontsize=26)\nplt.title(\"Count for target_class in dataset\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 9% of our target_class are actual pulsars, leaving 91% of the candidate stars in the false-positive category. "},{"metadata":{},"cell_type":"markdown","source":"### Describing the dataset, visualized through a heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll use describe() again here but in a visualization to make it a bit easier to understand\nplt.figure(figsize=(12,12))\nax = sns.heatmap(data.describe().transpose(),\n           annot=True, linecolor=\"black\",\n            linewidth=1, square=True, vmin=0, vmax=1000, \n                 cmap=sns.color_palette(\"Set2\"))\nplt.title(\"Description Heatmap\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()\n\n# unfortunately due to an issue with the current release of matplotlib, the top\n# and bottom rows are cut in half without using the workaround code in lines 7-8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation & multicollinearity heatmap for the features "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next I'll look at possible multicollinearity among the features\ncorrelation = data.corr()\nplt.figure(figsize=(12,12))\nax = sns.heatmap(correlation, annot=True, \n                cmap=sns.color_palette(\"coolwarm\", 7),\n                linewidth=1, linecolor=\"white\")\nplt.title(\"Collinearity Between Features\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of mean and standard deviation between attributes for target_class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison of mean and standard deviation between attributes for target_class\ncompareMean = data.groupby(\"target_class\")[[' Mean of the integrated profile', \n                                            ' Standard deviation of the integrated profile', \n                                            ' Excess kurtosis of the integrated profile', \n                                            ' Skewness of the integrated profile',\n                                            ' Mean of the DM-SNR curve', \n                                            ' Standard deviation of the DM-SNR curve', \n                                            ' Excess kurtosis of the DM-SNR curve',\n                                        ' Skewness of the DM-SNR curve']].mean().reset_index()\n\ncompareMean = compareMean.drop(\"target_class\", axis=1)\ncompareMean = compareMean.transpose().reset_index()\ncompareMean = compareMean.rename(columns={\"index\":\"Features\", 0:\"Not a Pulsar\", 1:\"Pulsar\"})\n\nplt.figure(figsize=(15,15))\nplt.subplot(211)\nsns.pointplot(x=\"Features\", y=\"Not a Pulsar\", data=compareMean, color=\"red\")\nsns.pointplot(x=\"Features\", y=\"Pulsar\", data=compareMean, color=\"blue\")\nplt.xticks(rotation=75)\nplt.xlabel(\"\")\nplt.grid(True, alpha=0.5)\nplt.title(\"Comparing Mean of Attributes for Target Class\")\n\ncompareStd = data.groupby(\"target_class\")[[' Mean of the integrated profile', \n                                            ' Standard deviation of the integrated profile', \n                                            ' Excess kurtosis of the integrated profile', \n                                            ' Skewness of the integrated profile',\n                                            ' Mean of the DM-SNR curve', \n                                            ' Standard deviation of the DM-SNR curve', \n                                            ' Excess kurtosis of the DM-SNR curve',\n                                        ' Skewness of the DM-SNR curve']].std().reset_index()\n\ncompareStd = compareStd.drop(\"target_class\", axis=1)\ncompareStd = compareStd.transpose().reset_index()\ncompareStd = compareStd.rename(columns={'index':\"Features\", 0:\"Not a Pulsar\", 1:\"Pulsar\"})\nplt.subplot(212)\nsns.pointplot(x=\"Features\", y=\"Not a Pulsar\", data=compareStd, color=\"red\")\nsns.pointplot(x=\"Features\", y=\"Pulsar\", data=compareStd, color=\"blue\")\nplt.xticks(rotation=75)\nplt.grid(True, alpha=0.5)\nplt.title(\"Comparing Standard Deviation of Attributes for Target Class\")\nplt.subplots_adjust(hspace = 1)\nprint(\"Blue = Pulsar, Red = Not a Pulsar\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotted distributions of features within the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [' Mean of the integrated profile', ' Standard deviation of the integrated profile',\n           ' Excess kurtosis of the integrated profile', ' Skewness of the integrated profile',\n            ' Mean of the DM-SNR curve', ' Standard deviation of the DM-SNR curve', \n           ' Excess kurtosis of the DM-SNR curve', ' Skewness of the DM-SNR curve']\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(13,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length/2,length/4,j+1)\n    sns.distplot(data[i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(data[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(data[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    \nprint (\"Plotting the distributions of features in dataset\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatter plot showing skewness and kurtosis of the IPP and DM-SNR Curve related to target_class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plotting skewness and kurtosis for Integrate Pulse Profile and\n# the DM-SNR Curve related to the target_class\nplt.figure(figsize=(14,7))\nplt.subplot(121)\nplt.scatter(x = \" Excess kurtosis of the integrated profile\",\n            y = \" Skewness of the integrated profile\",\n            data=data[data[\"target_class\"] == 1],alpha=.7,\n            label=\"pulsar stars\",s=30,color = \"g\",linewidths=.4,edgecolors=\"black\")\nplt.scatter(x = \" Excess kurtosis of the integrated profile\",\n            y = \" Skewness of the integrated profile\",\n            data=data[data[\"target_class\"] == 0],alpha=.6,\n            label=\"not pulsar stars\",s=30,color =\"r\",linewidths=.4,edgecolors=\"black\")\nplt.axvline(data[data[\"target_class\"] == 1][\" Skewness of the integrated profile\"].mean(),\n            color = \"g\",linestyle=\"dashed\",label=\"mean pulsar star\")\nplt.axvline(data[data[\"target_class\"] == 0][\" Skewness of the integrated profile\"].mean(),\n            color = \"r\",linestyle=\"dashed\",label =\"mean non pulsar star\")\nplt.axhline(data[data[\"target_class\"] == 1][\" Skewness of the integrated profile\"].mean(),\n            color = \"g\",linestyle=\"dashed\")\nplt.axhline(data[data[\"target_class\"] == 0][\" Skewness of the integrated profile\"].mean(),\n            color = \"r\",linestyle=\"dashed\")\nplt.legend(loc =\"best\")\nplt.xlabel(\"Kurtosis of IPP\")\nplt.ylabel(\"Skewness of IPP\")\nplt.title(\"Scatter plot for skewness and kurtosis or IPP for target class\")\nplt.subplot(122)\nplt.scatter(x = \" Skewness of the DM-SNR curve\",y = ' Excess kurtosis of the DM-SNR curve',\n            data=data[data[\"target_class\"] == 0],alpha=.7,\n            label=\"not pulsar stars\",s=30,color =\"r\",linewidths=.4,edgecolors=\"black\")\nplt.scatter(x = \" Skewness of the DM-SNR curve\",y = ' Excess kurtosis of the DM-SNR curve',\n            data=data[data[\"target_class\"] == 1],alpha=.7,\n            label=\"pulsar stars\",s=30,color = \"g\",linewidths=.4,edgecolors=\"black\")\nplt.axvline(data[data[\"target_class\"] == 1][\" Excess kurtosis of the DM-SNR curve\"].mean(),\n            color = \"g\",linestyle=\"dashed\",label =\"mean pulsar star\")\nplt.axvline(data[data[\"target_class\"] == 0][\" Excess kurtosis of the DM-SNR curve\"].mean(),\n            color = \"r\",linestyle=\"dashed\",label =\"mean non pulsar star\")\nplt.axhline(data[data[\"target_class\"] == 1][\" Skewness of the DM-SNR curve\"].mean(),\n            color = \"g\",linestyle=\"dashed\")\nplt.axhline(data[data[\"target_class\"] == 0][\" Skewness of the DM-SNR curve\"].mean(),\n            color = \"r\",linestyle=\"dashed\")\nplt.legend(loc =\"best\")\nplt.xlabel(\"Skewness of DM-SNR Curve\")\nplt.ylabel('Kurtosis of DM-SNR Curve')\nplt.title(\"Scatter plot for skewness and kurtosis of DM-SNR Curve for target class\")\nplt.subplots_adjust(wspace =.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplots for attributes related to target class\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [x for x in data.columns if x not in [\"target_class\"]]\nlength  = len(columns)\nplt.figure(figsize=(13,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(4,2,j+1)\n    sns.lvplot(x=data[\"target_class\"],y=data[i],palette=[\"red\",\"blue\"])\n    plt.title(i)\n    plt.subplots_adjust(hspace=.3)\n    plt.axhline(data[i].mean(),linestyle = \"dashed\",color =\"k\",label =\"Mean value for data\")\n    plt.legend(loc=\"best\")\n    \nprint (\"Boxplots for features related to target_class\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Histogram normality check and confirmation of absense of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_cols = [' Mean of the integrated profile', \n            ' Standard deviation of the integrated profile', \n            ' Excess kurtosis of the integrated profile', \n            ' Skewness of the integrated profile',\n            ' Mean of the DM-SNR curve', \n            ' Standard deviation of the DM-SNR curve', \n            ' Excess kurtosis of the DM-SNR curve',\n            ' Skewness of the DM-SNR curve']\n\n# Define figure size & axis\nfig,ax = plt.subplots(figsize = (20,16))\n\n# Plot histograms\ndata.hist(column=hist_cols,ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Summary\nBy looking at the target class counts bar chart, we can clearly see there is a \nlarge class imbalance. True positives of our target class instance only make\nup 9% of the dataset. \n\nThe two features of kurtosis and skewness of the integrated pulse profile are \nclosely correlated, in the early stages of the modeling section, I may end up removing\none of them. I will reexamine that after doing some baseline modeling and begin\ndoing feature selection and engineering.   \nThe mean and standard deviation  of the DM-SNR curve attributes also seem to be\ncorrelated, but again, that will be revisited in the early stages of modeling. \nAlso, the kurtosis and skewness of the DM-SNR curve also seem to be correlated. \nFinally, the kurtosis of the integrated pulse profile seems to be fairly correlated(.79)\nwith our target_class. I'll continue to test this correlation and see if there is a \ntrue, solid relationship between them.   \n\nBy comparing the means between our attributes to the target class, we can see some\npossible relationships in the pointplots. It seems that the higher the mean of the \nintegrated pulse profile and the skewness of the the DM-SNR curve, the less likely\nthe candidate stars are to be actual pulsars.   \nWhen we look at the standard deviation of the two classes compared to one another, \nthe higher the skewness of the DM-SNR curve also seems to suggest a higher likelihood\nof being a false positive.   \n\nNone of the features follow a true normal distribution. Kurtosis and skewness of the IPP \nare close to normal, but both are ever so slightly positively skewed.  \nAs mentioned before, I will model both the raw data and transformed & normalized data \nto see if there is an increase in accuracy from normalizing these attributes.   \n\nWhen we examine the scatterplots that display the relationships between the skewness\nand kurtosis of the IPP and DM-SNR curves when related to the target_class, we can \nmake the following assumptions:   \nThe higher the kurtosis and skewness of the IPP, the more likely it is that the candidate star is a pulsar.   \nThe lower kurtosis and skewness of the DM-SNR curve, the more likely it is that the candidate star is a pulsar.   \n\nBy looking at the boxplots of the attributes, here are some brief summaries of what is shown:   \n- The mean of the IPP appears to be higher for false positives.    \n- The standared deviation of the IPP for the false positives appear to be slightly higher and have more upper end outliers.   \n- The kurtosis of the IPP seems to be smaller/close to 0 for false positives.   \n- The skewness of the IPP again is around 0 for false positives, while for actual pulsars, it is much larger.   \n- The mean of the DM-SNR curve is typically higher for actual pulsars, compared to false positives.   \n- The standard deviation of the DM-SNR curve is higher for actual pulsars than false positives.  \n- The kurtosis of the DM-SNR curve is higher for false positives than for actual pulsars.   \n- The skewness of the DM-SNR curve is significantly higher for false positives than for actual pulsars.    \n\nFinally, we can see again in the histograms that there are no normal distributions and \nnone of our data is categorical. "},{"metadata":{},"cell_type":"markdown","source":"# Model the Data\nIn this section I'll dive into the pre-processing related to modeling, normalize the secondary dataset, account for class imbalance, run a baseline model to be improved upon, and use a variety of libraries to create a number of types of models to see what performs best in the context. \"Best\" is going to be related to the model accuracy, using F1 score(especially for target class) as a metric of success. We'll take a close look at confusion matrices for each model type, and review all types in a conclusion."},{"metadata":{},"cell_type":"markdown","source":"### Transformation and Normalization \nBelow I'll normalize our cloned dataset to be modeled alongside with and compared to our raw dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the preview of the dataset \nTNdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use sklearn's preprocessing MinMaxScaler\nx = TNdata.values # returns a numpy array\nmin_max_scaler = MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nTNdata = pd.DataFrame(x_scaled, columns=TNdata.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TNdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirm the transformation didn't affect our target class\nTNdata.target_class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test and Train Splits\nNow I'll separate out the target class from the predictor variables and do a train-test split for both sets of data, and account for XGBoost. I've opted to do a 75/25 split."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define X and y of our datasets\ny_TN = TNdata['target_class']\nX_TN = TNdata.drop(['target_class'], axis=1)\n\ny = data['target_class']\nX = data.drop(['target_class'], axis=1)\n\n# define random seed\nseed = 25\n\n# split data into train and test sets\nX_train_TN, X_test_TN, y_train_TN, y_test_TN = train_test_split(X_TN, \n                                                    y_TN, \n                                                    test_size=0.25, \n                                                    random_state=seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.25, \n                                                    random_state=seed)\n\n# split in numpy array format for XGBoost\nX_train_xgb_TN, X_test_xgb_TN, y_train_xgb_TN, y_test_xgb_TN = train_test_split(X_TN.values, \n                                                                    y_TN.values, \n                                                                    test_size=0.25, \n                                                                    random_state=seed)\n\nX_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X.values, \n                                                                    y.values, \n                                                                    test_size=0.25, \n                                                                    random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Imbalance\nI'm going to use SMOTE(Synthetic Minority Oversampling TEchnique) to oversample the target class.  \nIt it important to only use SMOTE on the training data as to not disturb the test set.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Previous original class distribution\nprint(\"Before:\")\nprint(f\"\\n{y_train.value_counts()}\")\nprint(f\"\\n{pd.Series(y_train_xgb).value_counts()}\")\n\n# Fit SMOTE to training data\nsmote = SMOTE()\nX_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train)\nX_train_resampled_TN, y_train_resampled_TN = smote.fit_sample(X_train_TN, y_train_TN)\n\n# format for XGBoost\nX_train_resampled_xgb, y_train_resampled_xgb = smote.fit_sample(X_train_xgb, y_train_xgb)\nX_train_resampled_xgb_TN, y_train_resampled_xgb_TN = smote.fit_sample(X_train_xgb_TN, y_train_xgb_TN)\n\n# Preview synthetic sample class distribution\nprint(\"\\nAfter:\\n\")\nprint(f\"{pd.Series(y_train_resampled).value_counts()}\") \nprint(f\"\\n{pd.Series(y_train_resampled_xgb).value_counts()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline Model\nFor the baseline model, I'll use the Naive Bayes Classification Algorithm. The Naive Bayes models are very fast and simple algorithms often used for high-dimensional datasets. I'm opting to use this algorithm as a baseline model due to its speed, few tunable parameters, and its reputation as a \"quick-and-dirty\" baseline for classification problems.   \nIn order to create a true baseline, I'll only be using the raw dataset for this, and not using the normalized set.  "},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"aggs = data.groupby('target_class').agg(['mean', 'std'])\naggs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating conditional probability point estimates\ndef p_x_given_class(obs_row, feature, class_):\n    mu = aggs[feature]['mean'][class_]\n    std = aggs[feature]['std'][class_]\n    \n    # A single observation\n    obs = data.iloc[obs_row][feature] \n    \n    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n    return p_x_given_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing function with one feature\np_x_given_class(0, ' Mean of the integrated profile', 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# multinomial Bayes using all the features\nrow = 1\nc_probs = []\nfor c in range(2):\n    # Initialize probability to relative probability of class \n    p = len(data[data['target_class'] == c])/len(data) \n    for feature in X.columns:\n        p *= p_x_given_class(row, feature, c) \n        # Update the probability using the point estimate for each feature\n        c_probs.append(p)\n\nc_probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating class probabilities for obs\ndef predict_class(row):\n    c_probs = []\n    for c in range(2):\n        # Initialize probability to relative probability of class\n        p = len(data[data['target_class'] == c])/len(data) \n        for feature in X.columns:\n            p *= p_x_given_class(row, feature, c)\n        c_probs.append(p)\n    return np.argmax(c_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_class(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply prediction function to the train and test sets\ny_hat_train = [predict_class(X_train.iloc[idx]) for idx in range(len(X_train))]\ny_hat_test = [predict_class(X_test.iloc[idx]) for idx in range(len(X_test))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate training and test accuracy\nresiduals_train = y_hat_train == y_train\nacc_train = residuals_train.sum()/len(residuals_train)\n\nresiduals_test = y_hat_test == y_test\nacc_test = residuals_test.sum()/len(residuals_test)\nprint('Training Accuracy: {}\\tTesting Accuracy: {}'.format(acc_train, acc_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Baseline model summary\nAs expected, the accuracy of this model is pretty abysmal. It gives us a good starting place to work from and shows how nuanced our dataset is and how dependant on multiple variables the target class is. "},{"metadata":{},"cell_type":"markdown","source":"### Create Classifiers - Definining Critical Functions\nBelow I'll define a few functions that will be used in this final modeling phase, instantiate classifiers, run the various models, and then compare the results.  \nThe models to be run are as follows:  \n- Gaussian Naive Bayes\n- Decision Tree\n- Bagging Tree \n- K Nearest Neighbor\n- Random Forest \n- Logistic Regression\n- Extra Tree \n- Gradient Boost\n- AdaBoost\n- XGBoost "},{"metadata":{},"cell_type":"markdown","source":"#### Important Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function to batch classifiers\n\ndef batch_clf(X_train, y_train, X_test, y_test, clf_dict, verbose = True):\n    '''\n    Fits a dictionary of classifiers, makes predictions, plots ROC, returns metrics\n    \n    Args:\n        X_train: {array-like, sparse matrix} of shape (n_samples, n_features) train input values\n        y_train: array-like of shape (n_samples,) train target values\n        X_test: {array-like, sparse matrix} of shape (m_samples, m_features) test input values\n        y_test: array-like of shape (m_samples,) test target values\n        clf_dict: dictionary with key name of classifier and value classifier instance\n        verbose: if True, prints time taken to fit and predict for each classifier\n        \n    Returns:\n        Results dataframe\n    '''\n    # Create empty DataFrame to store results\n    times = []\n    train_acc_scores = []\n    test_acc_scores = []\n    train_f1_scores = []\n    test_f1_scores = []\n    train_precision_scores = []\n    test_precision_scores = []\n    train_recall_scores = []\n    test_recall_scores = []\n    train_roc_data = []\n    test_roc_data = []\n    test_profit_scores = []\n    \n    # Loop through dictionary items\n    for key, clf in clf_dict.items():\n        start_time = time.clock()\n        \n        # Fit classifier\n        clf_fitted = clf.fit(X_train,y_train)\n        \n        # Get Predictions\n        train_preds = clf_fitted.predict(X_train)\n        test_preds = clf_fitted.predict(X_test)\n  \n        \n        #Get accuracy scores\n        train_acc = accuracy_score(y_train, train_preds)\n        train_acc_scores.append(round(train_acc,2))\n        test_acc = accuracy_score(y_test, test_preds)\n        test_acc_scores.append(round(test_acc,2))\n        \n        #Get F1 Scores\n        train_f1 = f1_score(y_train, train_preds)\n        train_f1_scores.append(round(train_f1,2))\n        test_f1 = f1_score(y_test, test_preds)\n        test_f1_scores.append(round(test_f1,2))\n        \n        # Get Precision Scores\n        train_precision = precision_score(y_train, train_preds)\n        train_precision_scores.append(round(train_precision,2))\n        test_precision = precision_score(y_test, test_preds)\n        test_precision_scores.append(round(test_precision,2))\n        \n        # Get Recall Scores\n        train_recall = recall_score(y_train, train_preds)\n        train_recall_scores.append(round(train_recall,2))\n        test_recall = recall_score(y_test, test_preds)\n        test_recall_scores.append(round(test_recall,2))\n        \n        # Get Probability Predictions\n        train_hat = clf_fitted.predict_proba(X_train)\n        train_proba = train_hat[:,1]\n        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_proba)\n        train_roc_data.append([fpr_train, tpr_train, thresholds_train])\n            \n        test_hat = clf_fitted.predict_proba(X_test)\n        test_proba = test_hat[:,1]\n        fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_proba)\n        test_roc_data.append([fpr_test, tpr_test, thresholds_test])\n        \n        # Get Profit\n        \n        #best_t, best_profit = profit_threshold_optimizer(y_test, test_proba)\n        #test_profit_scores.append(best_profit)\n\n        end_time = time.clock()\n        time_elapsed = end_time - start_time\n        times.append(round(time_elapsed,2))\n        \n        if verbose:\n            print(f'trained {key} in {round(time_elapsed,2)}')\n        \n    # Create results dataframe\n    results = pd.DataFrame({'Model': list(clf_dict.keys()), \n                            'Time': times,\n                            'Train Accuracy': train_acc_scores,\n                            'Test Accuracy': test_acc_scores, \n                            'Train F1': train_f1_scores,\n                            'Test F1': test_f1_scores,\n                            'Train Precision' : train_precision_scores,\n                            'Test Precision' : test_precision_scores,\n                            'Train Recall': train_recall_scores,\n                            'Test Recall': test_recall_scores,\n                            'Test Profit' : test_profit_scores\n                            })\n    \n   # Plot side by side ROC curve\n    fig, axes = plt.subplots(1,2, figsize = (13,6))\n    \n    for i in range(len(train_roc_data)):\n        axes[0].plot(train_roc_data[i][0], train_roc_data[i][1], lw=4, \\\n                 label= f'{list(clf_dict.keys())[i]}')\n        \n    for i in range(len(test_roc_data)):\n        axes[1].plot(test_roc_data[i][0], test_roc_data[i][1], lw=4, \\\n                 label= f'{list(clf_dict.keys())[i]}')    \n        \n    for ax in axes:\n        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.legend(loc='lower right')\n    axes[0].set_title('Receiver operating characteristic (ROC) Curve \\n Training Set')\n    axes[1].set_title('Receiver operating characteristic (ROC) Curve \\n Test Set')\n    plt.show()\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function to fit our models and predict on the training and test sets\n\ndef clf_pred(models):\n    '''\n    Intakes our DataFrame with our models \n        and our X & y, train & test data\n    \n    Returns a DataFrame with all of our models, \n        metrics and accuracy scores   \n    '''\n    \n    # Initialize matrix to fill\n    clf_df = np.zeros((len(models), 26), dtype=object)\n    \n    for i,model in models.iterrows():\n        \n\n        # Classifier Names & Models\n        clf_df[i,0] = model['clf_name']\n        clf_df[i,1] = model['clfs'] \n        \n        ### Assign Variables ###\n        # We do this here because XGBoost takes \n        # np.array format unlike the other models\n        X_train = model['X_train']\n        X_test = model['X_test']\n        y_train = model['y_train']\n        y_test = model['y_test']\n\n        ### Initialize Timer ### \n        start_time = time.time()\n        \n        ### Fit & Predict ###\n    \n        # Fit Model\n        fitted = model['clfs'].fit(X_train, y_train)\n        clf_df[i,2] = fitted\n        \n        # Calculate time to fit model\n        stop_time = time.time()\n        runtime = (stop_time - start_time)\n        clf_df[i,3] = runtime\n    \n        # Predict\n        y_pred_train = fitted.predict(X_train)\n        y_pred_test = fitted.predict(X_test)\n\n        clf_df[i,4] = y_pred_train\n        clf_df[i,5] = y_pred_test\n    \n        # y_score\n        y_score_train = fitted.predict_proba(X_train)\n        y_score_test = fitted.predict_proba(X_test)\n        clf_df[i,6] = y_score_train\n        clf_df[i,7] = y_score_test\n    \n        # False & True Positive Rates\n        clf_df[i,8], clf_df[i,9], thresholds_train = roc_curve(y_train, y_score_train[:,1])\n        clf_df[i,10], clf_df[i,11], thresholds_test = roc_curve(y_test, y_score_test[:,1])\n        \n        \n        ### Accuracy Scores ### \n        \n        # Precision\n        clf_df[i,12] = precision_score(y_train, y_pred_train)\n        clf_df[i,13] = precision_score(y_test, y_pred_test)\n        \n        # Recall \n        clf_df[i,14] = recall_score(y_train, y_pred_train)\n        clf_df[i,15] = recall_score(y_test, y_pred_test)        \n        \n        # F1\n        clf_df[i,16] = f1_score(y_train, y_pred_train)\n        clf_df[i,17] = f1_score(y_test, y_pred_test)  \n        \n        # Accuracy\n        clf_df[i,18] = accuracy_score(y_train, y_pred_train)\n        clf_df[i,19] = accuracy_score(y_test, y_pred_test)\n        \n        # AUC\n        clf_df[i,20] = roc_auc_score(y_train, y_pred_train)\n        clf_df[i,21] = roc_auc_score(y_test, y_pred_test)\n        \n        ### Add X & y values to have everything in one place ### \n        # These are class balanced/resampled #\n        clf_df[i,22] = X_train\n        clf_df[i,23] = X_test\n        clf_df[i,24] = y_train\n        clf_df[i,25] = y_test        \n    \n    ### Create DataFrame ###\n    \n    # Column Names\n    columns = ['Classifier',\n               'Model',\n               'Fitted Model',\n               'Runtime',\n               'Train Preds',\n               'Test Preds',\n               'Train y-Score',\n               'Test y-Score',\n               'Train FPR',\n               'Train TPR',\n               'Test FPR',\n               'Test TPR',\n               'Train Precision',\n               'Test Precision',\n               'Train Recall',\n               'Test Recall',\n               'Train F1',\n               'Test F1',\n               'Train Accuracy',\n               'Test Accuracy',\n               'Train ROC AUC',\n               'Test ROC AUC',\n               'X_train',\n               'X_test',\n               'y_train',\n               'y_test'\n              ]\n    \n    # Create DataFrame\n    clf_df = pd.DataFrame(clf_df, columns=columns)\n    \n    return clf_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to combine classification report, confusion matrix, \n# accuracy score, roc curve, and auc in one easy way\n\ndef model(algorithm, X_train, y_train, X_test, y_test, of_type):\n    algorithm.fit(X_train, y_train)\n    preds = algorithm.predict(X_test)\n    \n    print(algorithm)\n    print(\"\\naccuracy_score:\",accuracy_score(y_test, preds))\n    print(\"\\nclassification report:\\n\", (classification_report(y_test, preds)))\n    \n    plt.figure(figsize=(14,10))\n    plt.subplot(221)\n    ax = sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt=\"d\", \n                linecolor=\"k\", linewidth=2)\n    # work around for top & bottom cutoff is pasted below\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    plt.title(\"Confusion Matrix\", fontsize=20)\n    \n    predicting_probabilities = algorithm.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, predicting_probabilities)\n    plt.subplot(222)\n    plt.plot(fpr, tpr, label = (\"Area Under the Curve:\", auc(fpr, tpr)), color=\"r\")\n    plt.plot([1,0], [1,0], linestyle = \"dashed\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(\"ROC Curve and Area Under Curve\", fontsize=20)\n    \n    if of_type == \"feat\":\n        dataframe = pd.DataFrame(algorithm.feature_importances_, X_train.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\", 0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\", ascending=False)\n        # plot conf_matrix, roc/auc, and feat importances\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\", y = \"features\", data = dataframe, palette = \"husl\")\n        plt.title(\"Feature Importances\", fontsize=20)\n        for i, j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011, i, j, weight = \"bold\")\n        \n    elif of_type == \"coef\":\n        dataframe = pd.DataFrame(algorithm.coef_.ravel(), X_train.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\", 0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\", ascending=False)\n        # plot conf_matrix, roc/auc, and feat importances\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\", y = \"features\", data = dataframe, palette = \"husl\")\n        plt.title(\"Feature Importances\", fontsize=20)\n        for i, j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011, i, j, weight = \"bold\")\n            \n    elif of_type == \"none\":\n        return(algorithm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\ngnb = GaussianNB() # none\n\n# Decision Tree\ndt = DecisionTreeClassifier() # feat\n\n# Bagging Tree \nbag = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                        random_state=seed) # feat\n\n# K Nearest Neighbor\nknn = KNeighborsClassifier() # none\n\n# Random Forest \nrf = RandomForestClassifier() # feat\n\n# Logistic Regression\nlog = LogisticRegression() # coef\n\n# Extra Tree \next = ExtraTreeClassifier()  # feat\n\n# Gradient Boost\ngb = GradientBoostingClassifier() # feat\n\n# AdaBoost\nadb = AdaBoostClassifier() # feat\n\n# XGBoost \nxgb = xgb.XGBClassifier(random_state=seed)\n\n# Create dictionary of classifiers for raw data\nclf_dict = {0:\n    {'clf_name': 'Gaussian Naive Bayes',\n     'clfs': gnb,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n            1:\n    {'clf_name': 'Decision Tree',\n     'clfs': dt,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n            2:\n    {'clf_name': 'Bagging Tree',\n     'clfs': bag,\n     'X_train': X_train_resampled,\n     'X_test': X_test_xgb,\n     'y_train': y_train_resampled,\n     'y_test': y_test_xgb},\n            3:\n    {'clf_name': 'K Nearest Neighbor',\n     'clfs': knn,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           4:\n    {'clf_name': 'Random Forest',\n     'clfs': rf,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           5:\n    {'clf_name': 'Logistic Regression',\n     'clfs': log,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           6:\n    {'clf_name': 'Extra Tree',\n     'clfs': ext,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           7:\n    {'clf_name': 'Gradient Boost',\n     'clfs': gb,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           8:\n    {'clf_name': 'AdaBoost',\n     'clfs': adb,\n     'X_train': X_train_resampled,\n     'X_test': X_test,\n     'y_train': y_train_resampled,\n     'y_test': y_test},\n           9:\n    {'clf_name': 'XGBoost',\n     'clfs': xgb,\n     'X_train': X_train_resampled_xgb,\n     'X_test': X_test_xgb,\n     'y_train': y_train_resampled_xgb,\n     'y_test': y_test_xgb}}\n\n# Convert into DataFrame\n\nmodels_df = pd.DataFrame.from_dict(clf_dict, orient='index')\n\n# Create dictionary of classifiers for normalized data\nclf_dict_TN = {0:\n    {'clf_name': 'Gaussian Naive Bayes',\n     'clfs': gnb,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n            1:\n    {'clf_name': 'Decision Tree',\n     'clfs': dt,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n            2:\n    {'clf_name': 'Bagging Tree',\n     'clfs': bag,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test_xgb,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test_xgb},\n            3:\n    {'clf_name': 'K Nearest Neighbor',\n     'clfs': knn,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           4:\n    {'clf_name': 'Random Forest',\n     'clfs': rf,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           5:\n    {'clf_name': 'Logistic Regression',\n     'clfs': log,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           6:\n    {'clf_name': 'Extra Tree',\n     'clfs': ext,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           7:\n    {'clf_name': 'Gradient Boost',\n     'clfs': gb,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           8:\n    {'clf_name': 'AdaBoost',\n     'clfs': adb,\n     'X_train': X_train_resampled_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_TN,\n     'y_test': y_test},\n           9:\n    {'clf_name': 'XGBoost',\n     'clfs': xgb,\n     'X_train': X_train_resampled_xgb_TN,\n     'X_test': X_test,\n     'y_train': y_train_resampled_xgb_TN,\n     'y_test': y_test}}\n\n# Convert into DataFrame\n\nmodels_df_TN = pd.DataFrame.from_dict(clf_dict_TN, orient='index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit and Predict\nI'll use the above classifier predictions function to run through both dictionaries of Classifiers. The function organizes the results on a table for easy comparison. "},{"metadata":{},"cell_type":"markdown","source":"#### Prediction on raw data"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_preds = clf_pred(models_df)\nclf_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(gnb, X_train, y_train, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(gnb, X_train_TN, y_train_TN, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The non-normalized data vastly outperformed the normalized data in the Gaussian Naive Bayes model with an accuracy score of 95 vs 74.  \nA target F1 score of .75, this model performed moderately overall. An AUC of .95 indicates decent performance. "},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(dt, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(dt, X_train_TN, y_train_TN, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the normalized data performed horribly - a .12 F1 score for the target class.    \nHowever, for our raw data the model performed well. With an overall accuracy of .97 and a target class F1 of .82, it performed better than out Gaussian Naive Bayes model. The AUC of .90 indicates the model performed well.   \nWe can see that the overwhelmingly most important feature is the Kurtosis of the Integrated Pulse Profile.  \nThere were far fewer false positives and a similar level of false negatives as the GNB. "},{"metadata":{},"cell_type":"markdown","source":"### Bagging Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(bag, X_train, y_train, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(bag, X_train_TN, y_train_TN, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This seems to be becoming a trend...The normalized data performed terribly - a .01 F1 score for the target class.    \nOur raw data the model performed well. An overall accuracy of .98 and a target class F1 of .88 is good, with very few false positives and less false negatives than GNB or Decision Tree models. The high AUC of .95 supports that. "},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(knn, X_train, y_train, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(knn, X_train_TN, y_train_TN, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, normalized model can be ignored.  \nAn overall accuracy of .97 and a target F1 score of .85 is good, supported by the low number of false positives and comparable(to GNB, Dec Tree, Bagging Tree) false negatives and high AUC of .93."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(rf, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(rf, X_train_TN, y_train_TN, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With an accuracy score of .98, a target class F1 score of .87, only 30 false positives, 73 false negatives, and an AUC of .96 this model is our best performing so far. As you can see by looking at the feature importance graph, it seems to take the most amount of features into account which makes me feel a bit more confident in this algorithm and its ability to deal with this type of dataset. "},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(log, X_train, y_train, X_test, y_test, \"coef\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(log, X_train_TN, y_train_TN, X_test, y_test, \"coef\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model performed very similarly to our Random Forest model in most metrics. "},{"metadata":{},"cell_type":"markdown","source":"### Extra Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(ext, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(ext, X_train_TN, y_train_TN, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Extra Tree model performed similarly to the Decision tree model, almost the exact same metrics.  \nEven though the normalized model is for all intents and purposes, useless, it's interesting to see the feature importances graph topped by the mean IPP feature. "},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw data\nmodel(gb, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(gb, X_train_TN, y_train_TN, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The raw data Gradient Boost model is so far the best performing model with an accuracy score of .98, a macro avg F1 of .93, a target class F1 of .88, the lowest number of false positives and negatives out of the models run thusfar, and a AUC of .97."},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw\nmodel(adb, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalized data\nmodel(adb, X_train_TN, y_train_TN, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The raw data AdaBoost model performed on level with the Gradient Boost model, which is quite well. Almost identical metrics.   "},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing with non-normalized data\nclf = XGBClassifier(random_state=25)\nclf.fit(X_train, y_train)\ntraining_preds = clf.predict(X_train)\nval_preds = clf.predict(X_test)\ntraining_accuracy = accuracy_score(y_train, training_preds)\nval_accuracy = accuracy_score(y_test, val_preds)\n\nprint(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\nprint(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbo = XGBClassifier(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix for XGB\nmodel(xgbo, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of Models\nDue to their horrible levels of performance, the models built off of the normalized dataset will be ignored entirely for this comparison. The table below describes the performance of the models:\n"},{"metadata":{},"cell_type":"markdown","source":"| Classifier | Accuracy Score | Macro Avg. F1 Score | Target Class F1 score | False Pos | False Neg | AUC | Runtime(sec) |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| XGBoost | 99 | .93 | .87 | 30 | 72 | .97 | 0.673 |\n| AdaBoost | 98 | .93 | .87 | 29 | 74 | .97 | 1.230 |\n| Bagging Tree | 98 | .93 | .88 | 29 | 71 | .95 | 1.237 |\n| Random Forest | 98 | .93 | .87 | 30 | 73 | .96 | 3.447 |\n| Gradient Boost | 98 | .93 | .88 | 38 | 64 | .97 | 5.597 |\n| Logistic Regression | 98 | .93 | .87 | 27 | 75 | .97 | 0.124 |\n| Decision Tree | 97 | .90 | .82 | 70 | 77 | .90 | 0.221 |\n| KNN | 97 | .92 | .85 | 38 | 81 | .93 | 0.043 |\n| Extra Tree | 97 | .90 | .82 | 78 | 74 | .90 | 0.010 |\n| Gassian Naive Bayes | 95 | .86 | .75 | 161 | 69 | .95 | 0.006 |\n"},{"metadata":{},"cell_type":"markdown","source":"## Tuning Model Hyperparameters  \nIn this stage, I take my top 3 best performing models: XGBoost, AdaBoost, and Bagging Tree, and look to increase their performance via GridSearch to find the ideal parameters. "},{"metadata":{},"cell_type":"markdown","source":"#### GridSearch optimization for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(random_state=25)\n\nparam_grid = {\n    'max_depth': [20, 50, 100, 200],\n    'learning_rate': [0.125, 0.25, 0.5, 1],\n    'subsample': [.6, .75, .9],\n    'min_child_weight': [10, 20, 30],\n    'n_estimators': [10, 100, 500]\n}\ngridsearch_xgb = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3)\ngridsearch_xgb.fit(X_train, y_train)\nprint(f\"Training Accuracy: {gridsearch_xgb.best_score_ :.2%}\")\nprint(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))\nprint(\"\")\nprint(f\"Optimal Parameters: {gridsearch_xgb.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboTuned = XGBClassifier(random_state=seed, learning_rate=0.25, \n                              max_depth=20, min_child_weight=20,\n                              n_estimators=100, subsample=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(xgboTuned, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearch optimization for AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier()\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 200],\n    'learning_rate': [0.125, 0.25, 0.5, 1],\n}\ngridsearch_adb = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3)\ngridsearch_adb.fit(X_train, y_train)\nprint(f\"Training Accuracy: {gridsearch_adb.best_score_ :.2%}\")\nprint(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))\nprint(\"\")\nprint(f\"Optimal Parameters: {gridsearch_adb.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adbTuned = AdaBoostClassifier(learning_rate=0.25, n_estimators=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(adbTuned, X_train, y_train, X_test, y_test, \"feat\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearch optimization for Bagging Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                        random_state=seed)\n\nparam_grid = {\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n    'n_estimators': [10, 50, 100],\n    'max_samples' : [10, 100],\n    'base_estimator__criterion': ['gini','entropy'],\n    'base_estimator__max_features' : [0.6, 0.8, 1.0],\n    'base_estimator__min_samples_leaf': [1, 3, 5, 10]\n}\ngridsearch_bag = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3)\ngridsearch_bag.fit(X_train, y_train)\nprint(f\"Training Accuracy: {gridsearch_bag.best_score_ :.2%}\")\nprint(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))\nprint(\"\")\nprint(f\"Optimal Parameters: {gridsearch_bag.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bagTuned = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n                                                                  max_features=1,\n                                                                  min_samples_leaf=1,\n                                                                  random_state=seed),\n                             bootstrap=True, bootstrap_features=False,\n                             max_samples=100, n_estimators=100, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(bagTuned, X_train, y_train, X_test, y_test, \"none\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpret the Data - Conclusion\n\nHere in the final step, I examine the findings developed by modeling and indentify important insights. As more data becomes available, this cycle can repeat and continue in a loop to build a faster, less biased, and more accurate model. For now, I'll review the models completed with the data available.  \n\n### Model Performance (Top 3 models, comparing metrics)\n\nThese findings are presented in the following format: (tuned) base. All models scored a 97.72% Validation accuracy, supporting their resistance to overtraining. \n\n| Classifier | Accuracy Score | Macro Avg. F1 Score | Target Class F1 score | False Pos | False Neg | AUC | Runtime(sec) |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| XGBoost | (98)99 | (.93).93 | (.88).87 | (33)30 | (67)72 | (.97).97 | 0.673 |\n| AdaBoost | (98)98 | (.93).93 | (.86).87 | (31)29 | (77)74 | (.97).97 | 1.230 |\n| Bagging Tree | (97)98 | (.91).93 | (.83).88 | (20)29 | (107)71 | (.96).95 | 1.237 |\n\nThe three best performing classifiers were XGBoost, AdaBoost, and Bagging Tree. They all were able to produce high accuracy, high validation test scores, and low numbers of type I and type II errors. XGBoost performed narrowly better than the latter two, with a higher accuracy, macro average F1 score, Target class F1 score, and AUC. All of the top 3 models scored a 97.72 in validation score, showing their resistance to overtraining. Also, the top 3 models all had similar type I and type II errors.  \n\nComing from a baseline model with a <20% accuracy, there was significant improvement made by dealing with class imbalances, using more robust algorithms, and tuning those algorithms to the ideal parameters. \n\nXGBoost is the ideal model, based off of accuracy, validation, and runtime. \n\n\n### Feature Importance\n\nBy far, excess kurtosis of the Integrated Pulse Profile was the most important feature for classifying these candidate stars.  \n\nBy examining the feature importance charts, we can see how impactful it was in the XGBoost and Adaboost models. The Bagging Tree classifier seems to be less one dimensional in that regard and shows a fairly high importance for Standard Deviation of the DM-SNR curve, and the Mean of the IPP.  \n\n\n### EDA Summary -  Confirming Suspicions\n\nNow to revisit my post-EDA impressions below:\nBy looking at the target class counts bar chart, we can clearly see there is a \nlarge class imbalance. True positives of our target class instance only make\nup 9% of the dataset. \n\nThe two features of kurtosis and skewness of the integrated pulse profile are \nclosely correlated, in the early stages of the modeling section, I may end up removing\none of them. I will reexamine that after doing some baseline modeling and begin\ndoing feature selection and engineering.   \nThe mean and standard deviation  of the DM-SNR curve attributes also seem to be\ncorrelated, but again, that will be revisited in the early stages of modeling. \nAlso, the kurtosis and skewness of the DM-SNR curve also seem to be correlated. \nFinally, the kurtosis of the integrated pulse profile seems to be fairly correlated(.79)\nwith our target_class. I'll continue to test this correlation and see if there is a \ntrue, solid relationship between them.   \n\nBy comparing the means between our attributes to the target class, we can see some\npossible relationships in the pointplots. It seems that the higher the mean of the \nintegrated pulse profile and the skewness of the the DM-SNR curve, the less likely\nthe candidate stars are to be actual pulsars.   \nWhen we look at the standard deviation of the two classes compared to one another, \nthe higher the skewness of the DM-SNR curve also seems to suggest a higher likelihood\nof being a false positive.   \n\nNone of the features follow a true normal distribution. Kurtosis and skewness of the IPP \nare close to normal, but both are ever so slightly positively skewed.  \nAs mentioned before, I will model both the raw data and transformed & normalized data \nto see if there is an increase in accuracy from normalizing these attributes.   \n\nWhen we examine the scatterplots that display the relationships between the skewness\nand kurtosis of the IPP and DM-SNR curves when related to the target_class, we can \nmake the following assumptions:   \nThe higher the kurtosis and skewness of the IPP, the more likely it is that the candidate star is a pulsar.   \nThe lower kurtosis and skewness of the DM-SNR curve, the more likely it is that the candidate star is a pulsar.   \n\nBy looking at the boxplots of the attributes, here are some brief summaries of what is shown:   \n- The mean of the IPP appears to be higher for false positives.    \n- The standared deviation of the IPP for the false positives appear to be slightly higher and have more upper end outliers.   \n- The kurtosis of the IPP seems to be smaller/close to 0 for false positives.   \n- The skewness of the IPP again is around 0 for false positives, while for actual pulsars, it is much larger.   \n- The mean of the DM-SNR curve is typically higher for actual pulsars, compared to false positives.   \n- The standard deviation of the DM-SNR curve is higher for actual pulsars than false positives.  \n- The kurtosis of the DM-SNR curve is higher for false positives than for actual pulsars.   \n- The skewness of the DM-SNR curve is significantly higher for false positives than for actual pulsars.    \n\nFinally, we can see again in the histograms that there are no normal distributions and \nnone of our data is categorical. \n\nAs I had expected, the kurtosis of the IPP was strongly related to confirming a True Positive. \n\n### Final Thoughts - Recommendations and Potential Future Work\n\n#### Model Selection\nI recommend using XGBoost. It is a faster, more robust model, capable of high levels of accuracy and validation. \n\n#### Business Value\nOverall, our model(s) performed well and could prove to be valuable assets to the field. With the ability to quickly and reliably classify candidate stars, we can more easily identify pulsars to study further. \n\n#### Larger Dataset\nFor future work, I would find it beneficial to have a larger dataset, with more anomolies in it to be able to refine our model even further. Looking at other forms of radio interference and how to train our telescopes to avoid misclassification of these candidate stars would help \"trim the fat\" off of a larger dataset and effectively pre-process for us. \n\n#### More Computational Power\nIt would be beneficial in the case of a larger dataset to be able to take advantage of a supercomputer or a distributed system to be able to efficiently process massive datasets. These things are more available in the research scenarios and professional observational astronomy labs. \n\n#### More Complex ML / DL\nI am curious to see if more advanced machine learning & deep learning techniques would yeild better results. "},{"metadata":{},"cell_type":"markdown","source":"### References\n\nSpecial thanks to Kuo Liu of the Max Planck Institute for allowing me to cite and use his work on pulsars. \n\n[1] Kuo Liu (2017). \"Introduction to Pulsar, Pulsar Timing, and measuring of Pulse Time-of-Arrivals\", Max-Planck-Institut fr Radioastronomie, Bonn, Germany\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}