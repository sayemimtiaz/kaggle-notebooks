{"cells":[{"cell_type":"markdown","source":"## Introduction\nTitanic: The Unsinkable Ship! Jack B. Thayer, one of the Titanic survivors said:\n> \"There was peace and the world had an even tenor to it's way. Nothing was revealed in the morning the trend of which was not known the night before. It seems to me that the disaster about to occur was the event that not only made the world rub it's eyes and awake but woke it with a start keeping it moving at a rapidly accelerating pace ever since with less and less peace, satisfaction and happiness. To my mind the world of today awoke April 15th, 1912.\" \n\nI have always been fascinated with Machine Learning techniques. To me, it sounds interesting to be able to predict the chance of survival of anybody on that ship by analyzing their attributes. And that's I'm about to start. This analysis is in three separate parts. I tried to explain steps as much as possible.\n\n**Please note,** I am new to Kaggle, and I am working on my Data Science skills. However, I'd like to think systematically when it comes to solving problems. Therefore, if you have any ideas about the way I think, please feel free to leave a comment. I do appreciate it!\n\n\n# Part 1: Data cleaning and fitting our first model\n\nIn this part, I will start the process of data cleaning. As you will see, I have selected Age, Sex, and Pcalss as my primary variables in this part of analysis. Eventually, I wil fit a model to see how things look. \nIn the next part of analysis, I will explain feature engineering, and eventually I will try to tweak the fitted model to reach to the maximum possible accuracy.","metadata":{"_cell_guid":"aa170077-6827-4391-b086-6edef1f748ee","_uuid":"30c0d82d3b8c4b83792d17475583719758a625d6"}},{"execution_count":null,"cell_type":"code","source":"#importing modules and reading the train and test sets.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\n\nprint('Shape of train set is:',train.shape)\nprint('Shape of test set is:',test.shape)\ntrain.head()","metadata":{"_cell_guid":"6f439fc4-274b-48eb-a7d4-05c2852a4892","_uuid":"3ec822080c3cd94df151415768b97ec1b66eb1c2"},"outputs":[]},{"cell_type":"markdown","source":"### Background research\n\nBackground research is very very important. Getting some domain knowledge about the incident, I realized that women and kids were given priority in lifeboats. Also, there has been three classes among passengers. Therefore, sex, age, and class could be good variables to look at.","metadata":{"_cell_guid":"8e0ee64e-b67a-4ae6-97f7-7b710b7ad387","_uuid":"66f24962b6b7520bfdd76b7962549bb4f169baaf"}},{"execution_count":null,"cell_type":"code","source":"#using pivot tables to get an idea how much of each sex survived.\nsex_pivot=train.pivot_table(index='Sex',values='Survived')\nsex_pivot","metadata":{"collapsed":true,"_cell_guid":"eef60154-0e9b-436c-b701-f50b960dc37c","_uuid":"0f61b8a803f4708c997013b823fca8908251d14b"},"outputs":[]},{"cell_type":"markdown","source":"**74%** of female survived. It means, according to [this post on Kaggle Kernels](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score/notebook), if we predicted all women survive and all men parished, we would at least get an accuracy of 0.76. ","metadata":{"_cell_guid":"25aeebe1-ee2e-4be1-bdad-3076cfc45358","_uuid":"9088cb9b3f96520fdd51abc196500978d9864d45"}},{"execution_count":null,"cell_type":"code","source":"#pivot table of Pclass\nclass_pivot=train.pivot_table(index='Pclass',values='Survived')\nclass_pivot","metadata":{"collapsed":true,"_cell_guid":"52652c0c-ad15-498f-9541-6f27bed34f23","_uuid":"2215c57e044d3eb0163fe41017a3750066e10a0a"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"train.Age.describe()","metadata":{"collapsed":true,"_cell_guid":"808a2a2c-77d6-444c-874c-04df192382ce","_uuid":"b24e80ea1cb0dcdb8f363b2cf9fe405bad621281"},"outputs":[]},{"cell_type":"markdown","source":"Age has missing values (compare 714 with 891 shape of train set). Age is fractional for kids younger than one year (we know this by looking at dataset description). Therfore, unlike sex and pclass which are categorical variables, age should be treated differently. \nOne way to look at continous data is through lens of histograms:","metadata":{"_cell_guid":"c54cf81f-4d7a-4f28-9614-9fe8b5cded7b","_uuid":"ca9a9e6be6076ab974c0cddf2c8120941125599c"}},{"execution_count":null,"cell_type":"code","source":"survived = train[train[\"Survived\"] == 1]\ndied = train[train[\"Survived\"] == 0]\nfig, ax=plt.subplots(figsize=(8,6))\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='red',bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5,color='blue',bins=50)\nplt.legend(['Survived','Died'])\nplt.show()","metadata":{"collapsed":true,"_cell_guid":"951070cb-1adc-4def-8284-5f79353aa2ae","_uuid":"9619648a8133e013bde6b2640c93c7ea501150b7"},"outputs":[]},{"cell_type":"markdown","source":"Red surpasses blue more in younger ages. In order to convert age to categorical items, we will cut the Age column in categories. But we should not forget about the null values. Let's fill nulls with -0.5 and use pandas.cut() function for cutting the Age column.\n**Please note,** whatever changes to dataset we make on train set, we should do it test set as well.","metadata":{"_cell_guid":"fef68f92-7ce0-4672-b590-816e7eaebba0","_uuid":"bce7976e553cef69e5f526e3a53d2dd5fdda0799"}},{"execution_count":null,"cell_type":"code","source":"#I define a function, so I can re-use it on test set as well.\ndef cut_age(df,cut_limits,label_names):\n    df['Age']=df['Age'].fillna(-.5)\n    df['Age_cats']=pd.cut(df['Age'],cut_limits,labels=label_names)\n    return df\n\ncut_limits=[-1,0,5,12,18,35,60,100] #These limits are something to alter in the future\nlabel_names=['Missing','Infant','Child','Teenager','Young Adult','Adult','Senior']\n\n#we defined a function to apply to both train and test sets.\ntrain=cut_age(train,cut_limits,label_names)\ntest=cut_age(test,cut_limits,label_names)\n\ntrain.pivot_table(index='Age_cats',values='Survived').plot.bar()","metadata":{"collapsed":true,"_cell_guid":"a6b18ad5-4fa3-410e-a821-4b05303b0e70","_uuid":"6d90c5c7a90391c32dd83457b6cb4f2bd13fcde5"},"outputs":[]},{"cell_type":"markdown","source":"Okay! now we have Age as a categorical variable.\n\nMachine learning algorithms handle numerical variables better than text. For the Pcalss, although the unique values are 1,2,3, these are not mathematically related, meaning a class 2 is not worth two times of class 1, for instance. \n\nWe will use pd.get_dummies() function to create dummy variables.","metadata":{"_cell_guid":"1b4be707-d85c-425c-8e79-b831ba3b8f83","_uuid":"c56e1b89adc068152f5a140ebb40adabd4aa104f"}},{"execution_count":null,"cell_type":"code","source":"#again, defining a function in order to be able to reuse on test set.\ndef create_dummies(df,col_name):\n    dummies=pd.get_dummies(df[col_name],prefix=col_name)\n    df=pd.concat([df,dummies],axis=1)\n    return df\ntrain = create_dummies(train,\"Pclass\")\ntest = create_dummies(test,\"Pclass\")\ntrain = create_dummies(train,\"Age_cats\")\ntest = create_dummies(test,\"Age_cats\")\ntrain = create_dummies(train,\"Sex\")\ntest = create_dummies(test,\"Sex\")\n\n#let's see how our columns look now:\ntrain.columns","metadata":{"collapsed":true,"_cell_guid":"e78197b0-5c82-4c39-b8e7-0d32f2e8a11f","_uuid":"b8a1771953df0089ce19a1053de6459a181dd2e2"},"outputs":[]},{"cell_type":"markdown","source":"## Fitting our first model\n\nThe goal is to predict whether a passenger survives or not. This is a binary classification problem. One way to approach these problems is using Logistic Regression models. These models are easy to implement (using Scikit-learn), and can be meaningfully intrepreted.\n\nLet's fit a logistic regression model for now, and see what insights could we draw from it.","metadata":{"_cell_guid":"be2c085a-3fa4-4a4e-8a4d-cf56293eaa75","_uuid":"f13235063596e7b79b8f04ef371991f13da43f8e"}},{"execution_count":null,"cell_type":"code","source":"#importing LogiticRegression class from sklearn\nfrom sklearn.linear_model import LogisticRegression\n\n#Although we have a test set, but that is only for submission purposes. We should still split our train set into...\n#...two seperate sets. This helps us measure the accuracy of our model.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nlr=LogisticRegression()\n\n#Let's only select sex, pclass, and age related columns.\nfeatures=['Pclass_1','Pclass_2', 'Pclass_3','Age_cats_Missing', 'Age_cats_Infant','Age_cats_Child', 'Age_cats_Teenager', \n          'Age_cats_Young Adult','Age_cats_Adult', 'Age_cats_Senior', 'Sex_female', 'Sex_male']\ntarget='Survived'\n\nall_X=train[features]\nall_y=train[target]\n\n#we will hold out our original test model\nholdout=test\n\n#splitting train set into two seperate sets. I use 80% of data for training and 20% for testing.\ntrain_X,test_X,train_y,test_y=train_test_split(all_X,all_y,test_size=0.2,random_state=0)\n\n#let's now fit the model and make predictions.\nlr.fit(train_X,train_y)\npredictions=lr.predict(test_X)\n\n#calculating accuracy using sklearn function\naccuracy=accuracy_score(test_y,predictions)\nprint('Accuracy of model is {0:.2f} percent'.format(accuracy*100))","metadata":{"collapsed":true,"_cell_guid":"60fc6895-10b8-402a-b2c0-0a19a86ab627","_uuid":"b1d5b981a039811d073a8644860e5922dc021668"},"outputs":[]},{"cell_type":"markdown","source":"Well, well, well. Seems like we did a good job, right? But wait! Isn't this a little too high for a first time model fitting? We should check if our model is overfitting or not by cross validation?\n\nWhat is overfitting overall? It means how optimistic our model is. Although hoping for the passenger's survivals are helpful in this case, we don't want to be too much hopeful!\n\n## Cross validation","metadata":{"_cell_guid":"1355fd11-78b2-449e-bd32-164cd6d6419f","_uuid":"6ac299a6bd81eed4b91c79b36ed37f091c890b02"}},{"execution_count":null,"cell_type":"code","source":"#I use the cross validation score function of sklearn.\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nlr=LogisticRegression()\nscores=cross_val_score(lr,all_X,all_y,cv=10) #10 folds\naccuracy=np.mean(scores)\nprint(scores)\nprint('Cross-validated accuracy of model is {0:.2f} percent'.format(accuracy*100))","metadata":{"collapsed":true,"_cell_guid":"edf5da5f-6958-4912-b327-76bccbb23fae","_uuid":"5772399b5301ac18402d949b2881f90a8128d5f0"},"outputs":[]},{"cell_type":"markdown","source":"That's actually very close! Therefore, our cross validated model shows similar accuracy level to our original model. \n\nLet's now train the model on the original train set, and test it on the original test set.","metadata":{"_cell_guid":"626a07e6-0ee3-4d9f-9000-508454a19f50","_uuid":"94027009fffa8120df881cb3633f50f7763fdd1d"}},{"execution_count":null,"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(all_X,all_y)\nholdout_predictions=lr.predict(holdout[features])","metadata":{"collapsed":true,"_cell_guid":"9af8409e-c3ea-4ed7-9f18-f3a16791fa79","_uuid":"a80f55e22b9841b3f733084c3da69e7cd227a225"},"outputs":[]},{"cell_type":"markdown","source":"### Submission file\n\nWhat I'm about to here? I will submit these results and see when I stand.","metadata":{"_cell_guid":"09d6d2ff-a09c-488f-b3a8-8ff269e8f35c","_uuid":"41eb6bdae4de1719ee055e18c6d44c2718233afa"}},{"execution_count":null,"cell_type":"code","source":"holdout_ids=holdout['PassengerId']\nsubmission_df={\n    'PassengerId':holdout_ids,\n    'Survived':holdout_predictions,\n                  }\n\nsubmission=pd.DataFrame(submission_df)\nsubmission_file=submission.to_csv('TitanicSubmission.csv',index=False)","metadata":{"collapsed":true,"_cell_guid":"3023ca5d-47a6-4a1a-9b1a-afa9e332c844","_uuid":"377afa72e38f6e3c6425433f5400afddc321fc76"},"outputs":[]},{"cell_type":"markdown","source":"Okay! I just submitted and it seems like our actual accuracy is around %75...well, let's work on that to improve our model. There are two ways we could do that:\n* Improving our features\n* Improving our model itself\n\nIn the next section, I will explain how can we work on our features and improve our model.\n\n\n# Part 2: Feature Engineering\n\n## Feature selection\nSelecting proper features is very important. By feature selection helps to exclude those that are not related to those that are realted to each other.\n\nLet's see if we can find meaningful features to include in our analysis. As you noticed, we only included sex, age, and calss so far. We will use DafaFrame.describe() method to find meaningful information on values in the remaining columns.","metadata":{"collapsed":true,"_cell_guid":"7540080f-871b-4674-aff1-35059aa4bad9","_uuid":"e7f19b9019bb170d6ad53508f09583a1f6f883f9"}},{"execution_count":null,"cell_type":"code","source":"cols=['SibSp','Parch','Fare','Cabin','Embarked']\ntrain[cols].describe(include='all',percentiles=[])","metadata":{"collapsed":true,"_cell_guid":"05cdfe45-902a-4240-aee9-49ed582f4536","_uuid":"a82f457a11ac90e2b8aaffc64b1b68ba1f0bab47"},"outputs":[]},{"cell_type":"markdown","source":"AMong these features, `Cabin` has a lot of missing values (only 204 out of 891 observations). And it seems like most of the values are unique. Therefore, we will drop this column for now.\n`Embarked` seems like a categorical column, has only 3 unique values, and 2 missing values. We can simply replace these two with most frequent value in this column, `S`, which has repeated 644 times!\nThe rest of columns seem like regular numerical features with no missing values. However, the range of `SibSp` and `Parch` are different than `Fare`. Therefore, we would need to **rescale** these columns so we give them equal weights in our model fitting.","metadata":{"_cell_guid":"a211bf3a-de27-4717-ae05-683f2af24ba6","_uuid":"6b0cd586a1b02e2940111bba026fce95f518004d"}},{"execution_count":null,"cell_type":"code","source":"train['Embarked']=train['Embarked'].fillna('S')\n#As you remember, whatever we do on train data set, we will do the same on test (holdout).\nholdout['Embarked']=holdout['Embarked'].fillna('S')\n\n\n#holdout has one missing value in Fare columns, let's replace it with mean of that column.\nholdout['Fare']=holdout['Fare'].fillna(train['Fare'].mean())\n\nholdout[cols].describe(include='all',percentiles=[])","metadata":{"collapsed":true,"_cell_guid":"a6be773f-2685-454a-bdf5-4fe98f8305fe","_uuid":"a527909bd81204a617e36bbed1b175e189b01141"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"#creating dummy variables for Embarked\ntrain = create_dummies(train,\"Embarked\")\nholdout = create_dummies(holdout,\"Embarked\")","metadata":{"collapsed":true,"_cell_guid":"8d756c68-3599-48e9-b022-e51ff349934a","_uuid":"2f2d8f7af9068cb19719e6a97c0e4e981cee95d8"},"outputs":[]},{"cell_type":"markdown","source":"For rescaling the numerical columns, I will use sklearn's minmax_scale function.","metadata":{"_cell_guid":"609121b2-1f82-467b-9d29-832fcdf180c8","_uuid":"6ce5e52c22f5a7985454093750ec2871195b0bed"}},{"execution_count":null,"cell_type":"code","source":"#rescaling the numerical columns\nfrom sklearn.preprocessing import minmax_scale\ncols=['SibSp','Parch','Fare']\nfor col in cols:\n    train[col + \"_scaled\"] = minmax_scale(train[col])\n    holdout[col + \"_scaled\"] = minmax_scale(holdout[col])","metadata":{"collapsed":true,"_cell_guid":"b59bd347-2dc5-4edf-a9a8-2196514e60d2","_uuid":"10ce8666aa61ea546168951794cb0eaa74df326a"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"train.columns","metadata":{"collapsed":true,"_cell_guid":"f15269e0-d495-4201-81af-045b90e61ac7","_uuid":"5ce916a48b9a8712528fabd155a88e7d94978486"},"outputs":[]},{"cell_type":"markdown","source":"After rescaling, we can fit a new model with all the features and try to select the most important variables. We can do this by sorting out the coefficients in Logistic Regression algorithm.","metadata":{"_cell_guid":"060c1b7f-522d-476a-9c15-2ec5bfcf811f","_uuid":"fde3d586e70c1002a1bd4ca485310e56dca8cec6"}},{"execution_count":null,"cell_type":"code","source":"columns=['Pclass_1', 'Pclass_2', 'Pclass_3', 'Age_cats_Missing', 'Age_cats_Infant',\n       'Age_cats_Child', 'Age_cats_Teenager', 'Age_cats_Young Adult',\n       'Age_cats_Adult', 'Age_cats_Senior', 'Sex_female', 'Sex_male',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n       'Parch_scaled', 'Fare_scaled']\n       \nlr=LogisticRegression()\nlr.fit(train[columns],train['Survived'])\n\n#finding the coefficients\ncoeffs=lr.coef_\nimportance_of_features=pd.Series(coeffs[0],index=train[columns].columns).abs().sort_values()\nimportance_of_features.plot.barh()","metadata":{"collapsed":true,"_cell_guid":"7346cb1d-e3f7-40bb-a350-bfb6a2a9a3ef","_uuid":"47f3247c2fe8020e53b3284210fed64207cbf3c3"},"outputs":[]},{"cell_type":"markdown","source":"Well, great! We could rank the features based on their coefficients in the model. However, we should forget about **Collinearity**. Let's form the heatmap of correlation matrix.\nBut before doing that, let's finalize our feature enegineering process. We can also work with Fare columns, to convert it into a categorical item. This approach is called **Bining**. It is similar to what we did fo age in first part of analysis.\n\nLet's first take a look at the histogram of Fare column.","metadata":{"collapsed":true,"_cell_guid":"498f7f0f-9965-4c08-9fdf-d48f7b2a0acd","_uuid":"f5c418800029ea786b232ec47146f08fbd120bce"}},{"execution_count":null,"cell_type":"code","source":"train['Fare'].hist(bins=20,range=(0,100))","metadata":{"collapsed":true,"_cell_guid":"d0194a66-ff27-4f6e-8981-48cfe95cfef2","_uuid":"e429bd6da4008099fcd9d88dd250a3accab93160"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"# defining a function for binning fare column\ndef process_fare(df,cut_points,lebel_names):\n    df['Fare_cats']=pd.cut(df['Fare'],cut_points,labels=label_names)\n    return df\n\ncut_points=[0,12,50,100,1000]\nlabel_names=['0-12','12-50','50-100','100+']\n\n#cutting the fare column using our function\ntrain=process_fare(train,cut_points,label_names)\nholdout=process_fare(test,cut_points,label_names)\n\n#creating dummy columns:\ntrain=create_dummies(train,'Fare_cats')\nholdout=create_dummies(test,'Fare_cats')","metadata":{"collapsed":true,"_cell_guid":"797770f9-82bd-4749-9743-e0c67e8ee385","_uuid":"c240e379ef6b139f92090edea5f5a96e16d2870b"},"outputs":[]},{"cell_type":"markdown","source":"Now, let's take a look at the name column in more detail.","metadata":{"collapsed":true,"_cell_guid":"7031b88d-4460-4d33-ae1b-f1ffca979f44","_uuid":"2285d550cd99bc87057c13831c4796790ac70af4"}},{"execution_count":null,"cell_type":"code","source":"train[['Name','Cabin']].head(10)","metadata":{"collapsed":true,"_cell_guid":"997973e3-5145-41ea-93cf-0bb66c1f95bb","_uuid":"6ae6e0cbec998eb4a4e571a0f773bd55d4dc1106"},"outputs":[]},{"cell_type":"markdown","source":"We see a trend of Mr., Mrs, Miss, etc. By looking at the entire records, we can come up with some unique titles, listed in the below dictionary. \nHow can we parse these titles from the name column? The answer is the `extract` method of dataframe! We will also use `regular expressions`. Please refer to [this](www.regex101.com) amaizng website when you have questions how a regex syntaxt will end up in your code.\n\nWe can also see that the first character of `Cabin` column could be a categorical item, let's extract that one as well.","metadata":{"collapsed":true,"_cell_guid":"cc4ebe84-0372-4021-87f0-6989845136c8","_uuid":"0ba8e878d73c06ef735bd6091e67d533612cef7c"}},{"execution_count":null,"cell_type":"code","source":"#creating a mapping dictionary\ntitles={\n    \"Mr\" :         \"Mr\",\n    \"Mme\":         \"Mrs\",\n    \"Ms\":          \"Mrs\",\n    \"Mrs\" :        \"Mrs\",\n    \"Master\" :     \"Master\",\n    \"Mlle\":        \"Miss\",\n    \"Miss\" :       \"Miss\",\n    \"Capt\":        \"Officer\",\n    \"Col\":         \"Officer\",\n    \"Major\":       \"Officer\",\n    \"Dr\":          \"Officer\",\n    \"Rev\":         \"Officer\",\n    \"Jonkheer\":    \"Royalty\",\n    \"Don\":         \"Royalty\",\n    \"Sir\" :        \"Royalty\",\n    \"Countess\":    \"Royalty\",\n    \"Dona\":        \"Royalty\",\n    \"Lady\" :       \"Royalty\"    \n}\n\ndef titles_cabin_process(df):\n    #extracting titles from 'Name' column\n    extracted_titles=df['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)\n    df['Title']=extracted_titles.map(titles)\n    \n    #extracting first letter of 'Cabin' column\n    df['Cabin_type']=df['Cabin'].str[0]\n    df['Cabin_type']=df['Cabin_type'].fillna('Unknown')\n    \n    #creating dummy variables\n    df=create_dummies(df,'Title')\n    df=create_dummies(df,'Cabin_type')\n    \n    return df\n    \ntrain=titles_cabin_process(train)\nholdout=titles_cabin_process(holdout)","metadata":{"collapsed":true,"_cell_guid":"8d7c6d8c-d181-474d-8451-9f3df3abae53","_uuid":"1faba2e75057cfa7d92eaed4f013e2ad74aebeef"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"train.columns","metadata":{"collapsed":true,"_cell_guid":"f3bc4f91-2ad6-4591-bb08-dcbf6eb1b920","_uuid":"f3fd7031f818644f273a63b9daa2d7374ee6de79"},"outputs":[]},{"cell_type":"markdown","source":"#### Checking for Collinearity\n\nI will do that by looking at the correlation matrix of our features. We will use Seaborn's heatmap for this.","metadata":{"collapsed":true,"_cell_guid":"cee39b19-548f-4ef4-95f3-e5f151da4671","_uuid":"6190f84473702b43acd96b1975a781acbb98133a"}},{"execution_count":null,"cell_type":"code","source":"#writing a function that graphs nice heatmaps\ndef plot_corr_heatmap(df):\n    import seaborn as sns\n    corrs=df.corr()\n    sns.set(style='white')\n    mask=np.zeros_like(corrs,dtype=np.bool)\n    mask[np.triu_indices_from(mask)]=True\n    \n    f,ax=plt.subplots(figsize=(11,9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    \n    sns.heatmap(corrs, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()","metadata":{"collapsed":true,"_cell_guid":"1c664d4e-491f-4e64-85e5-6a4555da7c35","_uuid":"83c2bf0fcedfc9960bdb6261201853546302bc31"},"outputs":[]},{"cell_type":"markdown","source":"Let's create a column list from the features that we think are ready to go into model fitting process.","metadata":{"collapsed":true,"_cell_guid":"8e7ac44c-97c9-4d1e-ae09-ab586255b5b2","_uuid":"74f1081df643e1d2cc6857156cf07f9288acd878"}},{"execution_count":null,"cell_type":"code","source":"ready_columns=['Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Age_cats_Missing', 'Age_cats_Infant',\n       'Age_cats_Child', 'Age_cats_Teenager', 'Age_cats_Young Adult',\n       'Age_cats_Adult', 'Age_cats_Senior', 'Sex_female', 'Sex_male',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n       'Parch_scaled', 'Fare_cats_0-12',\n       'Fare_cats_12-50', 'Fare_cats_50-100', 'Fare_cats_100+',\n        'Cabin_type_A', 'Cabin_type_B', 'Cabin_type_C',\n       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n       'Cabin_type_T', 'Cabin_type_Unknown', 'Title_Master', 'Title_Miss',\n       'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n\nplot_corr_heatmap(train[ready_columns])","metadata":{"collapsed":true,"_cell_guid":"bdd52446-5952-49b1-abdd-7772281b6d34","_uuid":"92989faa54be2a383203926f5a430248d740f21a"},"outputs":[]},{"cell_type":"markdown","source":"Beautiful! What we can easily realize is that sex_female has a lot of correlation with sex_female! it is comething called `dummy variable trap`, becuase we created complimentary set of variables, setting the value of one to True, the other one becomes False automatically. Therefore, let's remove these collinear columns. but let's remove the ones with the least amount of data.","metadata":{"collapsed":true,"_cell_guid":"3fad3498-7050-49b3-ae9a-3af7a9a011b8","_uuid":"dcf23c05b132a4e81296a2823a243ea63ccb6ee0"}},{"execution_count":null,"cell_type":"code","source":"final_ready_columns=['Pclass_1','Pclass_3', 'Age_cats_Missing', 'Age_cats_Infant',\n       'Age_cats_Child', 'Age_cats_Teenager', 'Age_cats_Young Adult',\n       'Age_cats_Adult', 'Embarked_C', 'Embarked_S', 'SibSp_scaled',\n       'Parch_scaled', 'Fare_cats_0-12', 'Fare_cats_12-50', 'Fare_cats_50-100',\n        'Cabin_type_A', 'Cabin_type_B', 'Cabin_type_C',\n       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n       'Cabin_type_Unknown', 'Title_Master', 'Title_Miss',\n       'Title_Mr', 'Title_Mrs', 'Title_Officer']","metadata":{"collapsed":true,"_cell_guid":"e7fe0407-cf44-458e-a39b-ac36a25355ce","_uuid":"ad63716e037be533707db7ff2450303e873a388c"},"outputs":[]},{"cell_type":"markdown","source":"Also, instead of fitting a model again, sorting the features, and showing them on a barchart, we will use `recursive feature elemination` with cross-validation, using sklearn's `RFECV` class.","metadata":{"_cell_guid":"52810ac1-4794-4f38-921e-0633458769ac","_uuid":"845a11720387061021b9431fc241f9b2e5d38c62"}},{"execution_count":null,"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nall_X=train[final_ready_columns]\nall_y=train['Survived']\n\nlr=LogisticRegression()\n\n#just like any other sklearn class, we will instantiate the class first, then fit the model\nselector=RFECV(lr,cv=10)\nselector.fit(all_X,all_y)\n\n#usuing RFECV.support_ we can find the most import features. It provides a boolean list.\noptimized_columns=all_X.columns[selector.support_]\noptimized_columns","metadata":{"collapsed":true,"_cell_guid":"0776bed9-5c1f-4eff-b6d3-0b2d413f54a3","_uuid":"dd70e1e28bbaa7aac9756f7d7e693e568d9102ca"},"outputs":[]},{"cell_type":"markdown","source":"Now let's fit the model and see the score.","metadata":{"collapsed":true,"_cell_guid":"76ce4a6b-c849-41bc-af19-93f180ef654b","_uuid":"90e9b20b2d4cc2773b6dbba2a03709b8a42f3dc4"}},{"execution_count":null,"cell_type":"code","source":"all_X = train[optimized_columns]\nall_y = train[\"Survived\"]\nlr=LogisticRegression()\nscores=cross_val_score(lr,all_X,all_y,cv=10)\naccuracy=scores.mean()\nprint('Cross-validated accuracy of model is {0:.2f} percent'.format(accuracy*100))","metadata":{"collapsed":true,"_cell_guid":"ea068608-5396-4957-990b-fc73cba4e233","_uuid":"99837b235c5ba35454a17e70f9a1a5dc203b576a"},"outputs":[]},{"cell_type":"markdown","source":"It's s decent improvement compared to our last try!","metadata":{"collapsed":true,"_cell_guid":"02988d66-9260-428a-96e2-d2e840d62e17","_uuid":"1162153c698752da708d7336ff9f15e560a90938"}},{"cell_type":"markdown","source":"## Part 3: Model selection and tuning\n\nUntil now, we have been working with Logistoc Regression approach. Let's use a different algorithm now. \n`random forest` perfomrs well in non-linear situations. Let's see if we can have a better acuracy with that algorithm.\n\nWe will perform a grid search in hyper paramter optimization.","metadata":{"collapsed":true,"_cell_guid":"c0bc5872-ab97-4872-bac6-108b91492c19","_uuid":"a6d52bce867f935b600d235b70d7bb69c58fd8d1"}},{"execution_count":null,"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nhyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [5, 10],\n                   \"max_features\": [\"log2\", \"sqrt\"],\n                   \"min_samples_leaf\": [1, 5],\n                   \"min_samples_split\": [3, 5],\n                   \"n_estimators\": [6, 9]\n}\n\nclf = RandomForestClassifier(random_state=1)\ngrid = GridSearchCV(clf,param_grid=hyperparameters,cv=10)\n\ngrid.fit(all_X, all_y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nprint('Cross-validated accuracy of model is {0:.2f} percent'.format(best_score*100))","metadata":{"collapsed":true,"_cell_guid":"3772892f-a142-42c2-bc79-70ad08ef4a8d","_uuid":"df41ec4b50cbeb7debd2972f3d9a6630e3623d1c"},"outputs":[]},{"cell_type":"markdown","source":"------------------------------Thank you--------------------------\n\nI would love to hear your thoughts.\nAlso, These steps are aligned with what has been done at [Dataquest](dataquest.io)","metadata":{"collapsed":true,"_cell_guid":"2c2d8720-4eeb-4389-934d-38601ef9d488","_uuid":"b739fbe6c1e0fea8e874ac0b63541576b5778ded"}}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"nbformat":4}