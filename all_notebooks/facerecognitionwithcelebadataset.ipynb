{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"**Data Preparation**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Model Expects three images that are converted to vector\n* One training execution will have 3 images each of it converted to image vectors\n* out of which each 2 of the images should be of same person and third one should be of a different person","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Model Building**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nK.set_image_data_format('channels_first')\nimport cv2\nimport os\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport tensorflow as tf\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### PART OF THIS CODE IS USING CODE FROM VICTOR SY WANG: https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/utils.py ####\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport cv2\nfrom numpy import genfromtxt\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nimport h5py\nimport matplotlib.pyplot as plt\n\n\n_FLOATX = 'float32'\n\ndef variable(value, dtype=_FLOATX, name=None):\n    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n    _get_session().run(v.initializer)\n    return v\n\ndef shape(x):\n    return x.get_shape()\n\ndef square(x):\n    return tf.square(x)\n\ndef zeros(shape, dtype=_FLOATX, name=None):\n    return variable(np.zeros(shape), dtype, name)\n\ndef concatenate(tensors, axis=-1):\n    if axis < 0:\n        axis = axis % len(tensors[0].get_shape())\n    return tf.concat(axis, tensors)\n\ndef LRN2D(x):\n    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n\ndef conv2d_bn(x,\n              layer=None,\n              cv1_out=None,\n              cv1_filter=(1, 1),\n              cv1_strides=(1, 1),\n              cv2_out=None,\n              cv2_filter=(3, 3),\n              cv2_strides=(1, 1),\n              padding=None):\n    num = '' if cv2_out == None else '1'\n    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n    tensor = Activation('relu')(tensor)\n    if padding == None:\n        return tensor\n    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n    if cv2_out == None:\n        return tensor\n    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n    tensor = Activation('relu')(tensor)\n    return tensor\n\nWEIGHTS = [\n  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n  'dense_layer'\n]\n\nconv_shape = {\n  'conv1': [64, 3, 7, 7],\n  'conv2': [64, 64, 1, 1],\n  'conv3': [192, 64, 3, 3],\n  'inception_3a_1x1_conv': [64, 192, 1, 1],\n  'inception_3a_pool_conv': [32, 192, 1, 1],\n  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n  'inception_3b_pool_conv': [64, 256, 1, 1],\n  'inception_3b_1x1_conv': [64, 256, 1, 1],\n  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n  'inception_4a_pool_conv': [128, 640, 1, 1],\n  'inception_4a_1x1_conv': [256, 640, 1, 1],\n  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n  'inception_5a_pool_conv': [96, 1024, 1, 1],\n  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n  'inception_5b_pool_conv': [96, 736, 1, 1],\n  'inception_5b_1x1_conv': [256, 736, 1, 1],\n}\n\ndef load_weights_from_FaceNet(FRmodel):\n    # Load weights from csv files (which was exported from Openface torch model)\n    weights = WEIGHTS\n    weights_dict = load_weights()\n\n    # Set layer weights of the model\n    for name in weights:\n        if FRmodel.get_layer(name) != None:\n            FRmodel.get_layer(name).set_weights(weights_dict[name])\n        elif model.get_layer(name) != None:\n            model.get_layer(name).set_weights(weights_dict[name])\n\ndef load_weights():\n    # Set weights path\n    dirPath = '/kaggle/input/facerecotrainedweights/weights'\n    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n    paths = {}\n    weights_dict = {}\n\n    for n in fileNames:\n        paths[n.replace('.csv', '')] = dirPath + '/' + n\n\n    for name in WEIGHTS:\n        if 'conv' in name:\n            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n            conv_w = np.reshape(conv_w, conv_shape[name])\n            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n            weights_dict[name] = [conv_w, conv_b]     \n        elif 'bn' in name:\n            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n        elif 'dense' in name:\n            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n            dense_w = np.reshape(dense_w, (128, 736))\n            dense_w = np.transpose(dense_w, (1, 0))\n            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n            weights_dict[name] = [dense_w, dense_b]\n\n    return weights_dict\n\n\ndef load_dataset():\n    train_dataset = h5py.File('datasets/train_happy.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_happy.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\ndef img_to_encoding(image_path, model):\n    img1 = cv2.imread(image_path, 1)\n    img1 = cv2.resize(img1,(96,96), interpolation = cv2.INTER_AREA)\n    img = img1[...,::-1]\n    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n    x_train = np.array(img)\n    embedding = model.predict_on_batch(x_train)\n    return embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom numpy import genfromtxt\nfrom keras import backend as K\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.core import Lambda, Flatten, Dense\n\ndef inception_block_1a(X):\n    \"\"\"\n    Implementation of an inception block\n    \"\"\"\n    \n    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name ='inception_3a_3x3_conv1')(X)\n    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name = 'inception_3a_3x3_bn1')(X_3x3)\n    X_3x3 = Activation('relu')(X_3x3)\n    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3a_3x3_conv2')(X_3x3)\n    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_3x3_bn2')(X_3x3)\n    X_3x3 = Activation('relu')(X_3x3)\n    \n    X_5x5 = Conv2D(16, (1, 1), data_format='channels_first', name='inception_3a_5x5_conv1')(X)\n    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn1')(X_5x5)\n    X_5x5 = Activation('relu')(X_5x5)\n    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n    X_5x5 = Conv2D(32, (5, 5), data_format='channels_first', name='inception_3a_5x5_conv2')(X_5x5)\n    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn2')(X_5x5)\n    X_5x5 = Activation('relu')(X_5x5)\n\n    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n    X_pool = Activation('relu')(X_pool)\n    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n\n    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3a_1x1_conv')(X)\n    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_1x1_bn')(X_1x1)\n    X_1x1 = Activation('relu')(X_1x1)\n        \n    # CONCAT\n    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n\n    return inception\n\ndef inception_block_1b(X):\n    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name='inception_3b_3x3_conv1')(X)\n    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn1')(X_3x3)\n    X_3x3 = Activation('relu')(X_3x3)\n    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3b_3x3_conv2')(X_3x3)\n    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn2')(X_3x3)\n    X_3x3 = Activation('relu')(X_3x3)\n\n    X_5x5 = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3b_5x5_conv1')(X)\n    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn1')(X_5x5)\n    X_5x5 = Activation('relu')(X_5x5)\n    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n    X_5x5 = Conv2D(64, (5, 5), data_format='channels_first', name='inception_3b_5x5_conv2')(X_5x5)\n    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn2')(X_5x5)\n    X_5x5 = Activation('relu')(X_5x5)\n\n    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n    X_pool = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_pool_conv')(X_pool)\n    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_pool_bn')(X_pool)\n    X_pool = Activation('relu')(X_pool)\n    X_pool = ZeroPadding2D(padding=(4, 4), data_format='channels_first')(X_pool)\n\n    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_1x1_conv')(X)\n    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_1x1_bn')(X_1x1)\n    X_1x1 = Activation('relu')(X_1x1)\n\n    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n\n    return inception\n\ndef inception_block_1c(X):\n    X_3x3 = conv2d_bn(X,\n                           layer='inception_3c_3x3',\n                           cv1_out=128,\n                           cv1_filter=(1, 1),\n                           cv2_out=256,\n                           cv2_filter=(3, 3),\n                           cv2_strides=(2, 2),\n                           padding=(1, 1))\n\n    X_5x5 = conv2d_bn(X,\n                           layer='inception_3c_5x5',\n                           cv1_out=32,\n                           cv1_filter=(1, 1),\n                           cv2_out=64,\n                           cv2_filter=(5, 5),\n                           cv2_strides=(2, 2),\n                           padding=(2, 2))\n\n    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n\n    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n\n    return inception\n\ndef inception_block_2a(X):\n    X_3x3 = conv2d_bn(X,\n                           layer='inception_4a_3x3',\n                           cv1_out=96,\n                           cv1_filter=(1, 1),\n                           cv2_out=192,\n                           cv2_filter=(3, 3),\n                           cv2_strides=(1, 1),\n                           padding=(1, 1))\n    X_5x5 = conv2d_bn(X,\n                           layer='inception_4a_5x5',\n                           cv1_out=32,\n                           cv1_filter=(1, 1),\n                           cv2_out=64,\n                           cv2_filter=(5, 5),\n                           cv2_strides=(1, 1),\n                           padding=(2, 2))\n\n    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n    X_pool = conv2d_bn(X_pool,\n                           layer='inception_4a_pool',\n                           cv1_out=128,\n                           cv1_filter=(1, 1),\n                           padding=(2, 2))\n    X_1x1 = conv2d_bn(X,\n                           layer='inception_4a_1x1',\n                           cv1_out=256,\n                           cv1_filter=(1, 1))\n    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n\n    return inception\n\ndef inception_block_2b(X):\n    #inception4e\n    X_3x3 = conv2d_bn(X,\n                           layer='inception_4e_3x3',\n                           cv1_out=160,\n                           cv1_filter=(1, 1),\n                           cv2_out=256,\n                           cv2_filter=(3, 3),\n                           cv2_strides=(2, 2),\n                           padding=(1, 1))\n    X_5x5 = conv2d_bn(X,\n                           layer='inception_4e_5x5',\n                           cv1_out=64,\n                           cv1_filter=(1, 1),\n                           cv2_out=128,\n                           cv2_filter=(5, 5),\n                           cv2_strides=(2, 2),\n                           padding=(2, 2))\n    \n    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n\n    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n\n    return inception\n\ndef inception_block_3a(X):\n    X_3x3 = conv2d_bn(X,\n                           layer='inception_5a_3x3',\n                           cv1_out=96,\n                           cv1_filter=(1, 1),\n                           cv2_out=384,\n                           cv2_filter=(3, 3),\n                           cv2_strides=(1, 1),\n                           padding=(1, 1))\n    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n    X_pool = conv2d_bn(X_pool,\n                           layer='inception_5a_pool',\n                           cv1_out=96,\n                           cv1_filter=(1, 1),\n                           padding=(1, 1))\n    X_1x1 = conv2d_bn(X,\n                           layer='inception_5a_1x1',\n                           cv1_out=256,\n                           cv1_filter=(1, 1))\n\n    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n\n    return inception\n\ndef inception_block_3b(X):\n    X_3x3 = conv2d_bn(X,\n                           layer='inception_5b_3x3',\n                           cv1_out=96,\n                           cv1_filter=(1, 1),\n                           cv2_out=384,\n                           cv2_filter=(3, 3),\n                           cv2_strides=(1, 1),\n                           padding=(1, 1))\n    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n    X_pool = conv2d_bn(X_pool,\n                           layer='inception_5b_pool',\n                           cv1_out=96,\n                           cv1_filter=(1, 1))\n    X_pool = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_pool)\n\n    X_1x1 = conv2d_bn(X,\n                           layer='inception_5b_1x1',\n                           cv1_out=256,\n                           cv1_filter=(1, 1))\n    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n\n    return inception\n\ndef faceRecoModel(input_shape):\n    \"\"\"\n    Implementation of the Inception model used for FaceNet\n    \n    Arguments:\n    input_shape -- shape of the images of the dataset\n\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"\n        \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # First Block\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n    X = Activation('relu')(X)\n    \n    # Zero-Padding + MAXPOOL\n    X = ZeroPadding2D((1, 1))(X)\n    X = MaxPooling2D((3, 3), strides = 2)(X)\n    \n    # Second Block\n    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n    X = Activation('relu')(X)\n    \n    # Zero-Padding + MAXPOOL\n    X = ZeroPadding2D((1, 1))(X)\n\n    # Second Block\n    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    # Zero-Padding + MAXPOOL\n    X = ZeroPadding2D((1, 1))(X)\n    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n    \n    # Inception 1: a/b/c\n    X = inception_block_1a(X)\n    X = inception_block_1b(X)\n    X = inception_block_1c(X)\n    \n    # Inception 2: a/b\n    X = inception_block_2a(X)\n    X = inception_block_2b(X)\n    \n    # Inception 3: a/b\n    X = inception_block_3a(X)\n    X = inception_block_3b(X)\n    \n    # Top layer\n    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n    X = Flatten()(X)\n    X = Dense(128, name='dense_layer')(X)\n    \n    # L2 normalization\n    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n\n    # Create model instance\n    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FRmodel = faceRecoModel(input_shape=(3, 96, 96))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: triplet_loss\n\ndef triplet_loss(y_true, y_pred, alpha = 0.2):\n    \"\"\"\n    Implementation of the triplet loss as defined by formula (3)\n    \n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor images, of shape (None, 128)\n            positive -- the encodings for the positive images, of shape (None, 128)\n            negative -- the encodings for the negative images, of shape (None, 128)\n    \n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n    \n    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n    \n    ### START CODE HERE ### (≈ 4 lines)\n    # Step 1: Compute the (encoding) distance between the anchor and the positive\n    pos_dist = tf.square(anchor-positive)\n\n    # Step 2: Compute the (encoding) distance between the anchor and the negative\n    neg_dist = tf.square(anchor-negative)\n\n    # Step 3: subtract the two previous distances and add alpha.\n    basic_loss = tf.reduce_sum(pos_dist-neg_dist)+alpha\n    \n    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n    loss = tf.reduce_sum(tf.maximum(basic_loss,0.))\n    ### END CODE HERE ###\n    \n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading Pretrained Weights**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"load_weights_from_FaceNet(FRmodel) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: verify\n\ndef verify(image_path, identity, database, model):\n    \"\"\"\n    Function that verifies if the person on the \"image_path\" image is \"identity\".\n    \n    Arguments:\n    image_path -- path to an image\n    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    dist -- distance between the image_path and the image of \"identity\" in the database.\n    door_open -- True, if the door should open. False otherwise.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n    encoding = img_to_encoding(image_path, FRmodel)\n    \n    # Step 2: Compute distance with identity's image (≈ 1 line)\n    dist = np.linalg.norm(encoding-database[identity])\n    \n    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n    if dist<0.7:\n        print(\"It's \" + str(identity) + \", welcome home!\")\n        door_open = 1\n    else:\n        print(\"It's not \" + str(identity) + \", please go away\")\n        door_open = 0\n        \n    ### END CODE HERE ###\n        \n    return dist, door_open","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: who_is_it\n\ndef who_is_it(image_path, database, model):\n    \"\"\"\n    Implements face recognition for the happy house by finding who is the person on the image_path image.\n    \n    Arguments:\n    image_path -- path to an image\n    database -- database containing image encodings along with the name of the person on the image\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n    identity -- string, the name prediction for the person on image_path\n    \"\"\"\n    \n    ### START CODE HERE ### \n    \n    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n    encoding = img_to_encoding(image_path, model)\n    \n    ## Step 2: Find the closest encoding ##\n    \n    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n    min_dist = 100\n    \n    # Loop over the database dictionary's names and encodings.\n    for (name, db_enc) in database.items():\n        \n        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n        dist = np.linalg.norm(encoding-database[name])\n\n        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n        if dist<min_dist:\n            min_dist = dist\n            identity = name\n\n    ### END CODE HERE ###\n    \n    if min_dist > 0.7:\n        print(\"Not in the database.\")\n    else:\n        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n        \n    return min_dist, identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"database ={}\ndatabase[\"idontknow\"]= img_to_encoding('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg',FRmodel)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"who_is_it(\"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg\", database, FRmodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test the Model   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"  print(FRmodel.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(image_path):\n    img1 = cv2.imread(image_path, 1)\n    print(img1.shape)\n    img1 = cv2.resize(img1,(96,96), interpolation = cv2.INTER_AREA)\n    #print(img1.shape)\n    \n    #img1\n    img = img1[...,::-1]\n    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n    x_train = np.array(img)\n    \n    print(np.shape(x_train))\n    return\ntest('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}