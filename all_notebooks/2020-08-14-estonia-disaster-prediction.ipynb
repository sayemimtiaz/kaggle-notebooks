{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Estonia Disaster Passenger Survival\nAttempting to predict passanger survival Random Forest and XGBoost models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the data set and take a peak.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '../input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv'\ndf = pd.read_csv(file_path)\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do a little pre-processing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove rows with missing target, separate target from predictors\ndf.dropna(axis=0, subset=['Survived'], inplace=True)\ny = df['Survived']\nX = df.drop(['Survived'], axis=1)\n\n\n# Break off validation set from training data.\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, check for missing values in the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train_full.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there aren't any!  \nLet's investigate some of the columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train_full[col]) == set(X_valid_full[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that could be dropped from the dataset:', bad_label_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can safely drop Firstname and Lastname columns, but let's dig into Country before dropping it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train_full.drop(['Firstname', 'Lastname'], axis=1)\nX_valid = X_valid_full.drop(['Firstname', 'Lastname'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_countries = X_train['Country'].unique()\nvalid_countries = X_valid['Country'].unique()\ndiff_countries = list(set(training_countries) - set(valid_countries))\nprint(\"Unique countries in training set:\")\nprint(training_countries)\nprint(\"Unique countries in validation set:\")\nprint(valid_countries)\nprint(\"Countries in one and not in the other:\")\nprint(diff_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there are some countries in the validation set that aren't in the training set.  \nWe can handle this in our pipeline.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\nX_train.drop(['PassengerId'], axis=1, inplace=True)\nX_valid.drop(['PassengerId'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's establish a baseline with the naive assumption that everyone died.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_baseline = np.zeros(len(y_valid))\nprint('Baseline MAE:', mean_absolute_error(y_valid, y_baseline))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for categorical data. \n# Set handle_unknown='ignore' so that new categories are set to zeros.\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nrf_model_1 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nrf_pipeline_1 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model_1)\n                     ])\n\n# Preprocessing of training data, fit model \nrf_pipeline_1.fit(X_train, y_train.values.ravel())\n\n# Preprocessing of validation data, get predictions\npreds_1 = rf_pipeline_1.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds_1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like performance is slightly worse than baseline with the current test and validation set.  \nNow let's see performance with 5-fold cross-validation (this agrees with the proportion of train-test split above).  \nFirst, we'll need to preprocess the entirety of X.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()\nX_proc = X.drop(['Firstname','Lastname'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(rf_pipeline_1, X_proc, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little better, oddly enough. Maybe the single random training and validation sets were a little harder for the model than average over 5.  \nThis model still performs worse than baseline, however.  \nLet's see if we can find a better n_estimators parameter.  \nFirst we'll need to preprocess X without separating into test and validation sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score_rf(n_estimators):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n    my_pipeline = Pipeline(steps=[\n                    ('preprocessor', preprocessor),\n                    ('model', RandomForestClassifier(n_estimators=n_estimators, random_state=0))\n                    ])\n    scores = -1 * cross_val_score(my_pipeline, X_proc, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    return scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\nfor i in range(1,9):\n    results[i*50] = get_score_rf(i*50)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like n_estimators=50, 100, 150 are equally the best.We'll choose 50 for efficiency.  \nHere's the final model and predictions below for Random Forest.  \nEither way we're still worse than baseline.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model_final = RandomForestClassifier(n_estimators=50, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nrf_pipeline_final = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model_final)\n                     ])\n\n# Preprocessing of training data, fit model \nrf_pipeline_final.fit(X_train, y_train.values.ravel())\n\n# Preprocessing of validation data, get predictions\npreds_rf_final = rf_pipeline_final.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds_rf_final))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Preprocessing for categorical data. \n# # Set handle_unknown='ignore' so that new categories are set to zeros.\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Bundle preprocessing for numerical and categorical data\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Define model\n# xgb_model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=3)\n\n# # Bundle preprocessing and modeling code in a pipeline\n# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                       ('model', xgb_model)\n#                      ])\n\n# # Preprocessing of training data, fit model \n# xgb_pipeline.fit(X_train, y_train.values.ravel(), model__early_stopping_rounds=5, \n#                  model__eval_set=[(X_valid, y_valid)], model__verbose=False)\n\n# # Preprocessing of validation data, get predictions\n# preds_xgb = xgb_pipeline.predict(X_valid)\n\n# print('MAE:', mean_absolute_error(y_valid, preds_xgb))\n\n\n#xgb_model.fit(X_train, y_train, \n#             early_stopping_rounds=5, \n#             eval_set=[(X_valid, y_valid)], \n#             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like XGBoost model doesn't play nicely with a sci-kit learn pipeline.  \nWe'll have to set up categorical columns manually.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encode the data\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n\n# Fill any NaN columns (should only happen with country) with 0s\nX_train.fillna(0, inplace=True)\nX_valid.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model\nxgb_model = xgb.XGBClassifier(random_state=1, learning_rate = 0.05, n_estimators=1000, n_jobs=3)\n\n# Fit the model\nxgb_model.fit(X_train,y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)])\n\n# Get predictions\npreds_xgb = xgb_model.predict(X_valid) # Your code here\n\n# Calculate MAE\nmae_xgb = mean_absolute_error(y_valid, preds_xgb) # Your code here\n\nprint(\"Mean Absolute Error:\" , mae_xgb)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey, now we're better than baseline!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}