{"cells":[{"metadata":{},"cell_type":"markdown","source":"Present notebook gives an example of application of ClinTrajan package (in particular ElPiGraph) for analysis of METABRIC breast cancer dataset. It is based on notebook: https://github.com/auranic/ClinTrajan/blob/master/ClinTrajan_tutorial.ipynb \n\nClinTrajan package:\nhttps://github.com/auranic/ClinTrajan \n\nElPiGraph package: \nhttps://github.com/j-bac/elpigraph-python\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.set_option('display.max_rows', 500)\n#pd.set_option('display.max_columns', 500)\n#pd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example of using ClinTrajan"},{"metadata":{},"cell_type":"markdown","source":"# Part 1. Quantification of Data"},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries for quantification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom scipy.stats import mode\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom importlib import reload  \nimport scipy.stats\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install lifelines\nfrom lifelines import KaplanMeierFitter\nfrom lifelines.utils import concordance_index\n!pip install  --no-dependencies  git+https://github.com/j-bac/elpigraph-python.git\nimport elpigraph\n!pip install trimap\n\nimport sys\nsys.path.insert(0,'/kaggle/input/breast-cancer-omics-bulk-data/code/')# \"/path/to/your/package_or_module\")\nprint(sys.path)\n\nfrom clintraj_qi import *\nfrom clintraj_eltree import *\nfrom clintraj_util import *\nfrom clintraj_ml import *\nfrom clintraj_optiscale import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data (categorical variables are assumed to be dummy-encoded already)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load omics data\ndf1 = pd.read_csv('/kaggle/input/breast-cancer-omics-bulk-data/METABRIC.txt', sep = '\\t', index_col = 0)\ndf1=df1.T\ni1 = [s.replace('BRCA-METABRIC-S1-','') for s in df1.index ]\n#print('number of common ids:', len(set(i2) & set(df1.index) ) )\ndf1.index = i1\ndf1\n# load clinical data\ndf2 = pd.read_csv('/kaggle/input/breast-cancer-omics-bulk-data/METABRIC_clinical.txt', sep = '\\t')#, index_col = 0)\ndf2 = df2.set_index('Patient ID')\ndf2\ndf = df2.join(df1, how = 'inner')\nprint('Joined data shape', df.shape)\ndf\nm = df['Relapse Free Status'].notnull()\nprint( m.sum() )\ndf = df[m].copy()\ndf['Relapse Free Status'] = df['Relapse Free Status'].map({'0:Not Recurred':0,'1:Recurred':1 } )\nprint(df.shape)\ndisplay(df.head())\n\ndf_full = df.copy()\ndf = df.iloc[:,37:] # OMICS data only\n\nX = df.values  \nX1 = X.copy()\nX_columns = df.columns\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = pd.read_csv('data/infarction/all_dummies.txt',delimiter='\\t')\ndisplay(df)\nquantify_nans(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cut 1000 top variance variables \n\nStep is intended to use for gene expression data , so we leave 1000 out of dozen thousands of gene expressions \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_var = X.var(axis = 0)\nprint( X_var.shape, X.shape, X_var[:5] )\nix = np.argsort(X_var)\nX = X[:,ix[-1000:]]\ndf = df.iloc[:,ix[-1000:]]\nprint(X.shape,df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detecting variable types\n\nWe need to know what types different variables have, which can be 'BINARY', 'ORDINAL', 'CONTINUOUS:"},{"metadata":{"trusted":true},"cell_type":"code","source":"variable_types, binary, continuous, ordinal = detect_variable_type(df,10,verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( type(variable_types), type(binary), type(continuous), type(ordinal) ) # All are list\nvariable_types[:3], binary[:3], continuous[:3], ordinal[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simplest univariate quantification (can work when there are missing values)\n\nWe need to impute missing values in the table, but for this we need some quantification already. Simple univariate quantification can be done via quantify_dataframe_univariate function \n\nNote that we have written down just in case the quantification parameters such that we could use them after imputation of missing values and restore the data table to the initial variable scales.\n\nNow we can impute the missing values. One of simple ideas is to compute SVD on the complete part of the data matrix and then project the data points with missing variables onto the principal components. The imputed value will be the value of the variable in the projection point."},{"metadata":{"trusted":true},"cell_type":"code","source":"if 0:\n    dfq,replacement_info = quantify_dataframe_univariate(df,variable_types)\n    with open('temp.txt','w') as fid:\n        fid.write(replacement_info)\nelse:        \n    dfq = df        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simplest missing value imputation using SVD computed at complete part of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"if 0:\n    dfq_imputed = SVDcomplete_imputation_method(dfq, variable_types, verbose=True,num_components=-1)\n    #dequant_info = invert_quant_info(replacement_info)\n    dequant_info = invert_quant_info(load_quantification_info('temp.txt'))\n    df_imputed = dequantify_table(dfq_imputed,dequant_info)\n    display(df_imputed)\nelse:    \n    dfq_imputed = df.copy()    \n    df_imputed = dfq_imputed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we quantify (and optimize) the ordinal variables via optimal scaling\n\nNow, we are ready to quantify the data table. We will do it by applying optimal scaling to the ordinal values."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df = remove_constant_columns_from_dataframe(df_imputed)\nvariable_names = [str(s) for s in df.columns[0:]]\nX = df[df.columns[0:]].to_numpy()\nX_original = X\nX_before_scaling = X.copy()\nX,cik = optimal_scaling(X,variable_types,verbose=True,vmax=0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### OK, we finished preparing the data matrix X, which is now complete and properly quantified. We also keep the 'original matrix' X_original, with 'raw' values of the variables (will be needed for visualizations)"},{"metadata":{},"cell_type":"markdown","source":"# Part 2. Computing the principal tree"},{"metadata":{},"cell_type":"markdown","source":"## Visualization function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ntry :\n    import umap\nexcept:\n    print('cannot import umap')\n\ndef plot_graph(edges, nodes_positions, data = None, dim_reduction = 'PCA', graph_color = 'black', graph_linewidth=2, \n               plot_data = True, data_linewidth = 1,  data_color = 'tab:red', data_transparency_alpha = 0.9,\n               umap_n_neighbors = 50, umap_min_dist = 0.99):\n  '''\n  #' Plots graphs defined by edges and nodes_positions, optionally - scatter plot the \"data\" on the same plot,\n  #' Optionally performs PCA/etc (depending on dim_reduction)\n  #'\n  #' @param edges Nx2-shape matrix with edges ends, i.e. edges[k,0], edges[k,1] - ends of k-th edge  \n  #' @param nodes_positions  matrix of nodes positions \n  #' @param data  \"original dataset\", basically arbitrary dataset for scatter plot, it should have same shape[1] as nodes_positions\n  #' @param plot_data  True/False - to scatterplot or not data\n  #' @param dim_reduction  'PCA', 'plot_first2axis', 'umap'\n  #' @param data_color can be a vector or predefined color - argument for c = data_color in scatter\n\n  #' @examples\n  # edges = np.array([ [0,1],[1,2],[2,0] ] )\n  # nodes_positions = np.random.rand(3,10) # 3 points in 10d space\n  # plot_graph(edges, nodes_positions)\n  #\n  # t = elpigraph_output\n  # edges = t[0]['Edges'][0]\n  # nodes_positions = t[0]['NodePositions']\n  # plot_graph(edges, nodes_positions)\n  '''\n  str_dim_reduction = dim_reduction\n  if dim_reduction in ['PCA', 'umap' ]: #  not 'plot_first2axis':\n    if dim_reduction.upper() == 'PCA':\n      reducer = PCA()\n    elif dim_reduction.lower() == 'umap':\n      n_neighbors = umap_n_neighbors#  50\n      min_dist= umap_min_dist # 0.99\n      #n_components=n_components\n      reducer = umap.UMAP( n_neighbors=n_neighbors,        min_dist=min_dist, n_components = 2)\n\n    if data is not None:\n      data2 = reducer.fit_transform(data)\n      if plot_data == True:\n        if data_color is None:\n          plt.scatter(data2[:,0],data2[:,1], linewidth = data_linewidth , alpha = data_transparency_alpha)# ,cmap=plt.cm.Paired) # ,c=np.array(irx) \n          plt.xlabel(str_dim_reduction+'1')\n          plt.ylabel(str_dim_reduction+'2')\n        else:\n          #plt.scatter(data2[:,0],data2[:,1] ,cmap=plt.cm.Paired,c= data_color, linewidth = data_linewidth, alpha = data_transparency_alpha ) \n          sns.scatterplot( x=data[:,0], y=data[:,1], hue = data_color )\n\n          plt.xlabel(str_dim_reduction+'1')\n          plt.ylabel(str_dim_reduction+'2')\n    else:\n      reducer.fit(nodes_positions)\n\n    nodes_positions2 = reducer.transform( nodes_positions )\n  else:\n    if plot_data == True:\n      if data is not None:\n        if data_color is None:\n          plt.scatter(data[:,0],data[:,1] , linewidth = linewidth, alpha = data_transparency_alpha )# ,cmap=plt.cm.Paired) # ,c=np.array(irx) \n        else:\n          plt.scatter(data[:,0],data[:,1] ,cmap=plt.cm.Paired,c= data_color , linewidth = data_linewidth, alpha = data_transparency_alpha ) \n          #sns.scatterplot( x=data[:,0], y=data[:,1], hue = data_color )\n\n    nodes_positions2 = nodes_positions\n\n  plt.scatter(nodes_positions2[:,0],nodes_positions2[:,1],c = graph_color, linewidth = graph_linewidth)#, cmap=plt.cm.Paired)\n\n  edgeCount = edges.shape[0]\n  for k in range(edgeCount):\n    n0 = edges[k,0]\n    n1 = edges[k,1]\n    x_line = [ nodes_positions2[n0,0],  nodes_positions2[n1,0] ]\n    y_line = [ nodes_positions2[n0,1],  nodes_positions2[n1,1] ]\n    plt.plot(x_line, y_line, graph_color, linewidth = graph_linewidth) # 'black')\n\n    \nedges = np.array([ [0,1],[1,2],[2,0] ] )\nnodes_positions = np.random.rand(3,10) # 3 points in 10d space\nplot_graph(edges, nodes_positions)\nplt.title('Example graph plot with  plot_graph function')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading ClinTrajan libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from clintraj_eltree import *\nfrom clintraj_util import *\nfrom clintraj_ml import *\nfrom clintraj_optiscale import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First of all, we will reduce the dimension using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_dimension = 30\nX = scipy.stats.zscore(X)\npca = PCA(n_components=X.shape[1],svd_solver='full')\nY = pca.fit_transform(X)\nv = pca.components_.T\nmean_val = np.mean(X,axis=0)\nX = Y[:,0:reduced_dimension]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We are ready to compute the principal tree, let us do it"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#import sys\n#print(sys.path)\n#sys.path.append('/home/zinovyev/anaconda3/lib/python3.7/site-packages')\n#print(sys.path)\n\nnnodes = 20\ntree_elpi = elpigraph.computeElasticPrincipalTree(X,nnodes, # drawPCAView=True,\n                                                  alpha=0.01,Mu=0.1,Lambda=0.05,\n                                                  FinalEnergy='Penalized')\ntree_elpi = tree_elpi[0]\n# some additional pruning of the graph\nprune_the_tree(tree_elpi)\n# extend the leafs to reach the extreme data points\ntree_extended = ExtendLeaves_modified(X, tree_elpi, Mode = \"QuantDists\", ControlPar = .5, DoSA = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we will create two data partitioning, by nodes of the principal tree and by the linear segments of the principal tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# paritioning the data by tree branches\nvec_labels_by_branches = partition_data_by_tree_branches(X,tree_extended)\nprint(len(set(vec_labels_by_branches)),'labels generated')\n# paritioning the data by proximity to nodes\nnodep = tree_elpi['NodePositions']\npartition, dists = elpigraph.src.core.PartitionData(X = X, NodePositions = nodep, \n                                                    SquaredX = np.sum(X**2,axis=1,keepdims=1),\n                                                    MaxBlockSize = 100000000, TrimmingRadius = np.inf\n                                                    )\npartition_by_node = np.zeros(len(partition))\nfor i,p in enumerate(partition):\n    partition_by_node[i] = p[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vec_labels_by_branches.shape, np.unique( vec_labels_by_branches) )  # .unique() \n# column4color = 'Pam50 + Claudin-low subtype' #\n# vec4color = df_full[column4color]\n\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npca = PCA\nr = pca().fit_transform(X = X )\nc = 0; fig = plt.figure(figsize = (20,10))\n\nc+=1; fig.add_subplot(1, 2 , c) \nvec4color = vec_labels_by_branches\nsns.scatterplot( x=r[:,0], y=r[:,1], hue = vec4color )\nplt.title('PCA for Omics data colored by Graph groups')\n\n\n\nc+=1; fig.add_subplot(1, 2 , c) \ncolumn4color = 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\n\nsns.scatterplot( x=r[:,0], y=r[:,1], hue = vec4color )\nplt.title('PCA for Omics data colored by Pam50 groups')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_groups_correspondence_approximate = {0:'Graph LumAB', 1:'Graph LumB',2:'Graph Basal',3:'Graph Her2',4:'Graph LumA'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_small['Status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_small = df_full[['Relapse Free Status (Months)', 'Relapse Free Status']].copy()\nf = 'Pam50 + Claudin-low subtype'\ndf_small[['Groups1']] = df_full[f].copy()\ndf_small[['Groups2']] = vec_labels_by_branches\ndf_small[['Groups2']] = df_small[['Groups2']].astype(int) # .apply(lambda x: int(x))\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf_small[['Groups1']] = le.fit_transform(df_small[['Groups1']])\n\n\ndf_small.columns = ['Months', 'Status', 'Groups1','Groups2']\n\nfor i in range(len(df_small)):\n    m = df_small['Months'].iat[i]\n    if m > 60: \n        df_small['Months'].iat[i] = 60\n        df_small['Status'].iat[i] = 0\n        \n        \n        \n\ndf_small.to_csv('RelapseFreeAndGroups.csv', index=False)\ndf_small\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    T = df_small.iloc[:,0]# [c1][m] \n    E = df_small.iloc[:,1]#[c2][m]\n    #T = df['Overall Survival (Months)'][m]\n    #E =  df['Overall Survival Status'][m].map({'Living':1, 'Deceased':0} )\n    kmf = KaplanMeierFitter()\n    kmf.fit(T,E)\n    kmf.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc1, c2 = 'Relapse Free Status (Months)', 'Relapse Free Status'\nT0 = df_full[c1] \nE0 = df_full[c2]\n\n\nc = 0; fig = plt.figure(figsize = (20,10))\n\nc+=1; fig.add_subplot(1, 2 , c) \n\nfor uv in  np.unique( vec_labels_by_branches):\n    mask = vec_labels_by_branches == uv\n    print(uv, mask.sum())\n  \n    T = T0[mask]\n    E = E0[mask]\n    #T = df['Overall Survival (Months)'][m]\n    #E =  df['Overall Survival Status'][m].map({'Living':1, 'Deceased':0} )\n    lbl = dict_groups_correspondence_approximate[uv]\n    kmf = KaplanMeierFitter(label=lbl)\n    kmf.fit(T,E)\n    kmf.plot()    \nplt.xlim([0,200])\nplt.title('Relapse Free' + ' Graph based groups') # str(c2.split(' ')[:2] ) ) \n\nc+=1; fig.add_subplot(1, 2 , c) \nf = 'Pam50 + Claudin-low subtype'\nvec4types = df_full[f]\nfor uv in  np.unique( vec4types):\n    mask = vec4types == uv\n    print(uv, mask.sum())\n    if mask.sum() < 30 : continue\n  \n    T = T0[mask]\n    E = E0[mask]\n    #T = df['Overall Survival (Months)'][m]\n    #E =  df['Overall Survival Status'][m].map({'Living':1, 'Deceased':0} )\n    lbl = uv # dict_groups[uv]\n    kmf = KaplanMeierFitter(label=lbl)\n    kmf.fit(T,E)\n    kmf.plot()    \nplt.xlim([0,200])\nplt.title('Relapse Free ' + ' Original PAM50 groups') # str(c2.split(' ')[:2] ) ) \n\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc1, c2 = 'Relapse Free Status (Months)', 'Relapse Free Status'\nT0 = df_full[c1] \nE0 = df_full[c2]\n\n\nc = 0; fig = plt.figure(figsize = (20,10))\n\nc+=1; fig.add_subplot(1, 2 , c) \n\nfor uv in  np.unique( vec_labels_by_branches):\n    mask = vec_labels_by_branches == uv\n    print(uv, mask.sum())\n  \n    T = T0[mask]\n    E = E0[mask]\n    #T = df['Overall Survival (Months)'][m]\n    #E =  df['Overall Survival Status'][m].map({'Living':1, 'Deceased':0} )\n    lbl = dict_groups_correspondence_approximate[uv]\n    kmf = KaplanMeierFitter(label=lbl)\n    kmf.fit(T,E)\n    kmf.plot()    \nplt.xlim([0,12*5])\nplt.title('Relapse Free' + ' Graph based groups') # str(c2.split(' ')[:2] ) ) \n\nc+=1; fig.add_subplot(1, 2 , c) \nf = 'Pam50 + Claudin-low subtype'\nvec4types = df_full[f]\nfor uv in  np.unique( vec4types):\n    mask = vec4types == uv\n    print(uv, mask.sum())\n    if mask.sum() < 30 : continue\n  \n    T = T0[mask]\n    E = E0[mask]\n    #T = df['Overall Survival (Months)'][m]\n    #E =  df['Overall Survival Status'][m].map({'Living':1, 'Deceased':0} )\n    lbl = uv # dict_groups[uv]\n    kmf = KaplanMeierFitter(label=lbl)\n    kmf.fit(T,E)\n    kmf.plot()    \nplt.xlim([0,12*5])\nplt.title('Relapse Free ' + ' Original PAM50 groups') # str(c2.split(' ')[:2] ) ) \n\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let us visualize the tree, with data points, colored by the tree segments"},{"metadata":{"trusted":true},"cell_type":"code","source":"column4color = 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color].values.copy()\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nvec4color = le.fit_transform(vec4color) # [1, 1, 2, 6])\n\nfig = plt.figure(figsize=(8, 8))\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_partitioning = True, visualize_partition =vec4color)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column4color = 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\n\nfig = plt.figure(figsize=(8, 8))\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_partitioning = True, visualize_partition =vec4color)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column4color = 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\n\nfig = plt.figure(figsize=(8, 8))\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_partitioning = True, visualize_partition =vec4color)\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_partitioning = True, visualize_partition = vec_labels_by_branches)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column4color =  'Nottingham prognostic index'# 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\nfig = plt.figure(figsize=(8, 8))\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_partitioning = True, visualize_partition = vec4color)\nplt.title('colored by'+column4color)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column4color = 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\n\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npca = PCA\nr = pca().fit_transform(X = X )\nplt.figure(figsize = (20,10))\nsns.scatterplot( x=r[:,0], y=r[:,1], hue = vec4color )\nplt.title('PCA for Omics data colored by Pam50 groups')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncolumn4color =  'Nottingham prognostic index'# 'Pam50 + Claudin-low subtype' #\nvec4color = df_full[column4color]\n\nplt.figure(figsize = (20,10))\nsns.scatterplot( x=r[:,0], y=r[:,1], hue = vec4color )\nplt.title('PCA for Omics data colored by Pam50 groups')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let us show, on top of the tree, lethal cases, and show the lethality trend by the edge width"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nnon_lethal_feature = 'LET_IS_0'\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,'k',variable_names,\n                          Color_by_feature=non_lethal_feature, Feature_Edge_Width=non_lethal_feature,\n                           Invert_Edge_Value=True,Min_Edge_Width=10,Max_Edge_Width=50,\n                           Visualize_Edge_Width_AsNodeCoordinates=True,cmap='winter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let us higlight patients with AGE<65 and having bronchyal asthma in their anamneses"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\ninds = np.where((X_original[:,variable_names.index('AGE')]<=65)&(X_original[:,variable_names.index('zab_leg_03')]==1))[0]\ncolors = ['k' for i in range(len(X))]\nfor i in inds:\n    colors[i] = 'r'\ncolors = list(colors)\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,colors,variable_names,\n                          highlight_subset=inds,Big_Point_Size=100,cmap='hot')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let us quantify the pseudotime value, for each trajectory\n\n#### 1. We need to specify the root node. In order to do this, we will highlight all cases without any myocardial infarction complications, and will select the node where the complications are rare. In order to make the selection visual, we will show the node numbers as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(1,1,1)\ncomplication_vars = ['FIBR_PREDS','PREDS_TAH','JELUD_TAH','FIBR_JELUD',\n                     'A_V_BLOK','OTEK_LANC','RAZRIV','DRESSLER',\n                     'ZSN','REC_IM','P_IM_STEN']\ninds_compl = [variable_names.index(a) for a in complication_vars]\nlethal = 1-X_original[:,variable_names.index('LET_IS_0')]\nhas_complication = np.sum(X_original[:,inds_compl],axis=1)>0\ninds = np.where((has_complication==0)&(lethal==0))[0]\ncolors = ['r' for i in range(len(X))]\nfor i in inds:\n    colors[i] = 'k'\nvisualize_eltree_with_data(tree_extended,X,X_original,v,mean_val,colors,variable_names,\n                          highlight_subset=inds,Big_Point_Size=2,Normal_Point_Size=2,showNodeNumbers=True)\nadd_pie_charts(ax,tree_extended['NodePositions2D'],colors,['r','k'],partition,scale=30)\nplt.show()\nroot_node = 8\nprint('Root node=',8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Now we are ready to quantify pseudo-time"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_trajectories,all_trajectories_edges = extract_trajectories(tree_extended,root_node)\nprint(len(all_trajectories),' trajectories found.')\nProjStruct = project_on_tree(X,tree_extended)\nPseudoTimeTraj = quantify_pseudotime(all_trajectories,all_trajectories_edges,ProjStruct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Let us find all associations by regression of a clinical variable with pseudotime along all trajectories"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars = ['ritm_ecg_p_01','ritm_ecg_p_02','ritm_ecg_p_04']\nfor var in vars:\n    List_of_Associations = regression_of_variable_with_trajectories(PseudoTimeTraj,var,variable_names,\n                                                                    variable_types,X_original,R2_Threshold=0.5,\n                                                                    producePlot=True,\n                                                                    Continuous_Regression_Type='gpr',\n                                                                    verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. We can plot several variable dependencies against pseudotime"},{"metadata":{"trusted":true},"cell_type":"code","source":"pstt = PseudoTimeTraj[1]\ncolors = ['r','b','g']\nfor i,var in enumerate(vars):\n    vals = draw_pseudotime_dependence(pstt,var,variable_names,variable_types,X_original,colors[i],\n                                               linewidth=3,draw_datapoints=False)\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Now let us show how we can plot anything as a function of pseudotime, for example, cumulative hazard of death estimated using standard survival analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lifelines\nfrom lifelines import SplineFitter\nfrom lifelines import NelsonAalenFitter\nfrom lifelines import KaplanMeierFitter\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w','tab:pink','tab:green']\n\nevent_data = np.zeros((len(df),2))\nevents = 1-np.array(df['LET_IS_0'])\nlabel = 'Death'\n\nfor i,pstt in enumerate(PseudoTimeTraj):\n    points = pstt['Points']\n    times = pstt['Pseudotime']\n    for i,p in enumerate(points):\n        event_data[p,0] = times[i]\n        event_data[p,1] = events[p]\n\nplt.figure(figsize=(8,8))\n\nfor i,pstt in enumerate(PseudoTimeTraj):\n    TrajName = 'Trajectory:'+str(pstt['Trajectory'][0])+'--'+str(pstt['Trajectory'][-1])\n    points = pstt['Points']\n    naf = NelsonAalenFitter()\n    T = event_data[points,0]\n    E = event_data[points,1]\n    naf.fit(event_data[points,0], event_observed=event_data[points,1],label=TrajName)  \n    naf.plot_hazard(bandwidth=3.0,fontsize=20,linewidth=10,color=colors[i])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}