{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/cardiovascular-disease-dataset/cardio_train.csv\",delimiter=\";\",index_col=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data we are using for this assignment is Cardiovascular Disease and this dataset contains 13\nattributes with 70000 populations. The outcome of this dataset is in binary form which 1\nindicates having cardiovascular disease and 0 represents none.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(columns=[\"id\"])\ndf[\"age\"]=df[\"age\"].div(365)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that id is irrelevant and will not improve our accuracies in our model. Therefore,\nit has to be dropped. Besides that, Age is in days form and it has to be converted to years for\nbetter analytical understanding. After that we can consider merging weight and height to form\nBody Mass Index (BMI), but first we have to remove all outliers first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"duplicate {}\".format(df.duplicated().sum()))\nprint(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr= df.corr()\nplt.figure(figsize=(16, 6))\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace=True)\nprint(\"duplicate {}\".format(df.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age: Age is looking normal for youngest to be 29 and oldest to be 64.   \nHeight: Height of 55 and 250 isn’t normal at all.  \nWeight: Weight of 10kg already tell us there is outlier.  \nSystolic: A negative value of blood pressure is impossible.  \nDiastolic: A negative value of blood pressure is impossible.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_height=((df[\"height\"]>200) | (df[\"height\"]<140))\ndf=df[~outlier_height]\noutlier_weight=((df[\"weight\"]>150) | (df[\"weight\"]<40))\ndf=df[~outlier_weight]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For height, we can set the range to be 140 and 200, since negative growth could lead to\nshorter height while for weight, a range of 40 to 150 is set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='weight', y='height', hue='gender', data=df, fit_reg=False, height=6)\ndf[\"bmi\"] = df[\"weight\"]/ (df[\"height\"]/100)**2\ndf=df.drop(columns=[\"height\",\"weight\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" we can observe that weight and height is having correlation, therefore we can remove\nweight and height from the dataframe and replace it with a body mass index (BMI).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"blood_pressure = df.loc[:,['ap_lo','ap_hi']]\nsns.boxplot(x = 'variable',y = 'value',data = blood_pressure.melt())\nprint(\"Diastolic pressure is higher than systolic one in {0} cases\".format(df[df['ap_lo']> df['ap_hi']].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_bp1= ((df[\"ap_hi\"]>250) | (df[\"ap_lo\"]>160))\noutlier_bp2 = ((df[\"ap_hi\"] < 80) | (df[\"ap_lo\"] < 30))\ndf = df[~outlier_bp1]\ndf= df[~outlier_bp2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While for diastolic we consider dropping all values bigger than 200 and smaller than 30, and\ndropping all values bigger than 250 and smaller than 80 for systolic blood pressure. The\nabove is considered that having a diastolic 120 and systolic of 180 is considered hypertensive\ncrisis, for conservative purpose, we set it at 160 and 200.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Diastilic pressure is higher than systolic one in {0} cases\".format(df[df['ap_lo']> df['ap_hi']].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[(df['ap_hi'] > df['ap_hi'].quantile(0.975)) | (df['ap_hi'] < df['ap_hi'].quantile(0.025))].index,inplace=True)\ndf.drop(df[(df['ap_lo'] > df['ap_lo'].quantile(0.975)) | (df['ap_lo'] < df['ap_lo'].quantile(0.025))].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Diastolic pressure is higher than systolic one in {0} cases\".format(df[df['ap_lo']> df['ap_hi']].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blood_pressure = df.loc[:,['ap_lo','ap_hi']]\nsns.boxplot(x = 'variable',y = 'value',data = blood_pressure.melt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr= df.corr()\nplt.figure(figsize=(16, 6))\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(14,6))\nsns.countplot(x=df[\"alco\"],hue=df[\"cholesterol\"],ax=ax[0])\nsns.countplot(x=df[\"smoke\"],hue=df[\"cholesterol\"],ax=ax[1])\nax[0].set_title(\"Alcoholic vs Cholesterol\")\nax[1].set_title(\"Smoker vs Cholesterol\")\nax[0].set_xlabel(\"Acoholic\")\nax[1].set_xlabel(\"Smoker\")\nax[0].set_xticklabels([\"No\",\"Yes\"])\nax[1].set_xticklabels([\"No\",\"Yes\"])\nax[0].legend([\"Low CHolesterol\",\"High Cholesterol\",\"Very High Cholesterol\"],loc=\"center right\")\nax[1].legend([\"Low CHolesterol\",\"High Cholesterol\",\"Very High Cholesterol\"],loc=\"center right\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(14,6))\nsns.countplot(x=df[\"alco\"],hue=df[\"cardio\"],ax=ax[0])\nsns.countplot(x=df[\"smoke\"],hue=df[\"cardio\"],ax=ax[1])\nax[0].set_title(\"Alcoholic vs Cardio\")\nax[1].set_title(\"Smoker vs Cardio\")\nax[0].set_xlabel(\"Acoholic\")\nax[1].set_xlabel(\"Smoker\")\nax[0].set_xticklabels([\"No\",\"Yes\"])\nax[1].set_xticklabels([\"No\",\"Yes\"])\nax[0].legend([\"Low CHolesterol\",\"High Cholesterol\",\"Very High Cholesterol\"],loc=\"center right\")\nax[1].legend([\"Low CHolesterol\",\"High Cholesterol\",\"Very High Cholesterol\"],loc=\"center right\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax=sns.boxplot(x=df[\"cholesterol\"],y=df[\"bmi\"])\nax.set_title(\"Boxplot of Cholesterol Level against bmi\")\nax.set_xticklabels([\"Low Cholesterol\",\"High Cholesterol\",\"Very High Cholesterol\"])\nax.set_xlabel(\"Cholesterol\")\nax.set_ylabel(\"bmi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cardio'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(columns=[\"cardio\"])\ny=df[\"cardio\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(X,y,train_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nX = normalize(X)\nX_train = normalize(X_train)\nX_test = normalize(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we start anything, we have to determine the data is well-balanced. Using the count\nmethod, we know that 33362 (51%) patients do not have cardiovascular disease and 31972\n(49%) have cardiovascular. Since it is a balanced dataset, we can normalize the dataset and\nbegin to decide what type of classification to be used. The purpose of normalizing dataset is\nso that observation can be presented in a normal distribution, an initial transformation applied\nover here is helpful in improving data classification accuracies.\nThen we can split feature and outcome into train and test. By splitting factor of 0.2, 80\npercent will be training data and 20 percent will be testing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\ndec = DecisionTreeClassifier()\nran = RandomForestClassifier()\nknn = KNeighborsClassifier()\nsvm = SVC(random_state=0)\nnaive = GaussianNB()\nlog=LogisticRegression()\n\nmodels = {\"Decision tree\" : dec,\"Random forest\" : ran,\"KNN\" : knn,\"SVM\" : svm,\"Naive bayes\" : naive,\"Logistic regression\": log}\nscores= { }\n\nfor key, value in models.items():    \n    model = value\n    model.fit(X_train, y_train)\n    scores[key] = model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_frame = pd.DataFrame(scores, index=[\"Accuracy Score\"]).T\nscores_frame.sort_values(by=[\"Accuracy Score\"], axis=0,inplace=True)\nscores_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nacc_random_forest = cross_val_score(estimator=ran,X= X_train,y= y_train, cv=10)\nacc_decission_tree=cross_val_score(estimator=dec, X=X_train, y=y_train, cv=10)\nacc_knn = cross_val_score(estimator=knn, X=X_train, y=y_train, cv=10)\nacc_svm =cross_val_score(estimator=svm ,X=X_train, y=y_train, cv=10)\nprint(\"Random Forest Accuracy: \", acc_random_forest.mean())\nprint(\"Random Forest Standard Deviation: \", acc_random_forest.std())\nprint(\"Decission Tree Accuracy: \",acc_decission_tree.mean())\nprint(\"Decission Tree Standard Deviation: \", acc_decission_tree.std())\nprint(\"KNN Average Accuracy: \", acc_knn.mean())\nprint(\"KNN Standard Deviation: \", acc_knn.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nDT_pred = DecisionTreeClassifier()#max_depth=3, min_samples_split=50, min_samples_leaf=50, random_state=0\nDT_pred=DT_pred.fit(X_train, y_train)\ny_pred = DT_pred.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Clasification Accuracies\\n\",classification_report(y_test,y_pred))\nDT_acc = round(accuracy_score(y_test, y_pred), 2)\nprint(\"Overall accuracy score: {} \".format(DT_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sensitivity is the measure of proportion of actual positive cases got predicted as positive. A\n62 percent of sensitivity tells us that from a population of only people having cardiovascular\ndisease, this model able to predict correctly 62% of the population. Which is actually\nundesirable since the model loses its ability to predict the remaining 38% of the patients. A\nhigher sensitivity is desirable especially in healthcare industry.   \n  \nMeanwhile for specificity, it means the ability to predict healthy people as healthy. The\nrelationship of specificity tells us that the higher the value of specificity, it means higher\nvalue of true negative and lower false positive. While a lower specificity means lower true\nnegative and higher value of false positive. In this case we would prefer high rate of true\nnegative and low false positive. From specificity we can derive false positive rate, which is 1-0.636 = 0.364, a 34 percent of false positive rate. To be able to predict the patient correctly,\nwe must lower the false positive rate as it can cause unnecessary panic among patients.\nTherefore, 63 percent is still acceptable but an improvement would be desired.  \n  \nSince the dataset is balanced, a F1 score is unnecessary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 10,10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=max_depth)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results,\"b\", label=\"Train AUC\")\nline2, = plt.plot(max_depths, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Tree depth\")\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depth indicates the depth of tree; it is something we needs to control so that the nodes do not\nexpand too much and become complicated and difficult to analyse. The deeper the tress, the\nmore splitting there is and it will be ended up too deep. Therefore, it is important to select the\ndepth carefully. By looking at the plot, we can see there is underfitting when the depth is\nlower than 3 and overfitting when the depth is bigger than 4. An ideal situation would be for\nthe tree to have a depth between these 2 conditions, which is 4. At the depth of 4, there is no\nunderfitting and overfitting. Therefore, the first hyperparameter is determined.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_split in min_samples_splits:\n    dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nline1, = plt.plot(min_samples_splits, train_results,\"b\", label=\"Train AUC\")\nline2, = plt.plot(min_samples_splits, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Min split\")\nplt.xticks(np.linspace(0.1, 1.0, 10))\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The third parameter we are tuning is min_sample_leaf. It tells us that the minimum number of\nsamples to be required at the node, which is the base of the tree. Judging from this AUC\ncurve, the best score is when leaf have a sample leaf of 0.1, which reaching a constant score\nat 0.2 to 0.4, then it decreases over the increment of min_sample_leaf. Train AUC and Test\nAUC intersect when approaching 0.45 and underfitting happening right when the line passes\nthrough min_sample_leaf of 0.45. Therefore, the third desired parameter would be 0.1, or if\nthe line seems too close to overfit, min_sample_leaf between 0.2 to 0.4 can be choosen.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_leaf in min_samples_leafs:\n    dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, train_results,\"b\", label=\"Train AUC\")\nline2, = plt.plot(min_samples_leafs, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Min leaf\")\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = list(range(1,X.shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n    dt = DecisionTreeClassifier(max_features=max_feature)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, train_results,\"b\", label=\"Train AUC\")\nline2, = plt.plot(max_features, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Max feature\")\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features indicates the number of features to be considered during the split. As we can see the\nentire feature is overfitted. In this case we couldn’t decide which value to be used, therefore\nsetting it to be default.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfeature=[\"age\",\"gender\",\"ap_hi\",\"ap_lo\",\"cholesterol\",\"gluc\",\"smoke\",\"alco\",\"active\",\"bmi\"]\nfig, ax=plt.subplots(figsize=(16,10))\nclf = DecisionTreeClassifier(criterion=\"gini\", min_samples_split=0.1,max_depth=4,min_samples_leaf= 0.1)\npred=clf.fit(X,y)\nax=tree.plot_tree(pred.fit(X_train,y_train),feature_names=feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=4,min_samples_split=0.1,min_samples_leaf= 0.1)\npred=clf.fit(X,y)\ny_pred=pred.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nAccuracy=(TP+TN)/(TP+TN+FN+FP)\nError=(FP+FN)/(TP+TN+FN+FP)\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Accuracy,Error,Precision, Recall, F1_Score]],columns=[\"Accuracy\",\"Error\",\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the confusion matrix, we obtained a higher accuracy, lower error rate, higher precision\nand better recall. This indicated the fine tuning is indeed important and it shows a significant\nimprovement. Therefore, we would use max_depth=4, min_samples_split=0.1,\nmin_samples_leaf= 0.1 and default on max_feature. Refer to appendix 5 for the plotted\ndecision tree. We can tell that there is a significant increase of Accuracy, from 63 percent to\n71 percent, 64 percent precision increased to 71 percent, 62 percent of sensitivity increased to\n67 perfcent and a lower error rate. Generally the performance of the model after\nhyperparameter tuning is sastifying","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_pred = DecisionTreeClassifier(max_depth=4, min_samples_split=0.1, min_samples_leaf=0.1, random_state=0)\nDT_pred=DT_pred.fit(X_train, y_train)\ny_pred = DT_pred.predict(X_test)\ny_pred\nprint(\"Clasification Accuracies\\n\",classification_report(y_test,y_pred))\nDT_acc = round(accuracy_score(y_test, y_pred), 2)\nprint(\"Overall accuracy score: {} \".format(DT_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_pred = KNeighborsClassifier()\nKNN=KNN_pred.fit(X_train, y_train)\ny_pred_knn = KNN.predict(X_test)\ny_pred_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next classifier we are tuning is K Nearest Neighbours. According to previous accuracy\ntesting, it has an accuracy of 0.631, which is 0.003 lower compared to decision tree. The first\nthing we would be referring is the Confusion Matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to Decision Tree, KNN have a higher sensitivity before tuning. Accuracy and\nprecision are not interchangeable. A model that able to predict a patient health condition\nwithout consistency is not useful. This model able to predict 67 percent of people having\ncardiovascular disease out of all patient that have cardiovascular disease. This model also has\nan average result for specificity, it is able to predict healthy people as healthy. Therefore, this\nmodel predicted 63 percent of healthy people as healthy and 36 percent of healthy people as\nhaving cardiovascular disease. Through model tuning, we wish to increase all of the\npercentages so that lesser errors and higher accuracy and precision.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = np.linspace(1, 30,30, endpoint=True).astype(int)\ntrain_results = []\ntest_results = []\nfor n_neighbor in n_neighbors:\n    dt = KNeighborsClassifier(n_neighbors=n_neighbor)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n\n    test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_neighbors, train_results,\"b\", label=\"Train AUC\")\nline2, = plt.plot(n_neighbors, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"N neighbor\")\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this curve we can observe that the higher the N neighbour, the lesser the overfitting is.\nThere is a very obvious overfitting from the start. However, test score reached maximum\nafter neighbour value of 10, while train score continues to decreases over the ranges.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"distances = np.linspace(1,5,5,endpoint=True).astype(int)\ntrain_results = []\ntest_results = []\nfig = plt.figure()\nax = fig.add_subplot(111)\nfor distance in distances:\n    dt = KNeighborsClassifier(p=distance)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = ax.plot(distances, train_results,\"b\", label=\"Train AUC\")\nline2, = ax.plot(distances, test_results, \"r\", label=\"Test AUC\")\nymax_train = max(train_results)\nxpos_train = train_results.index(ymax_train)\nxmax_train = distances[xpos_train]\n#ax.annotate('local max', xy=(xmax_train, ymax_train), xytext=(xmax_train, ymax_train+1),\n           # arrowprops=dict(facecolor='black', shrink=0.05),\n            #)\nax.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nax.set_ylabel(\"AUC score\")\nax.set_xlabel(\"Distance\")\nax.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next parameter we viewing is p value, which normally only consist of 2 value. When p\nis equal to 1, it refers that the model is using manhattan_distance, where p equals to 2, the\nmodels is using euclidian_distance. So, on this plot we will be only observe distance 1 and\ndistance 2. According to the plot, distance 1 completely outdo distance 2. Although both\nshows overfitting, but we will determine by choosing the best AUC score.  \n  \nTherefore, we will be choosing a number of ranges within 5 to 30 and 1 for n_neighbors, and\n1 for p, which is manhattan_distance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num=np.linspace(1,10,10).astype(int)\nparam_dist={\"n_neighbors\":num,\n           \"weights\":[\"uniform\",\"distance\"],\n           \"algorithm\":[\"ball_tree\", \"kd_tree\", \"brute\"],\n           \"p\":np.linspace(1,2,2)}\nKNN = KNeighborsClassifier()\nKNN_cv=GridSearchCV(KNN,param_dist,cv=5)\nKNN_cv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through GridSearchCV we can find out what is the best parameters for KNN classifier. Since\nwe do not have a specific value for K yet, which we only classify it within a range of 5 to 30.\nWe needed a specific value to tune our model. By adding each hyperparameters into a\ndictionary, the method will return us all the parameters best for tuning. Since it is a method, I\nwill choose 4 parameters which is algorithm, n_neighbors, p and weight.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tuned Parameter: {}\".format(KNN_cv.best_params_))\nprint(\"Best Score: {}\".format(KNN_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results returned is indeed what we desired. Algorithm best used is ball_tree, n_neighbors\nfalls within the range of 5 to 30, p value of 1 and weight as uniform. We further evaluate the\nmodel performance in confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(algorithm=\"ball_tree\", n_neighbors= 9, p= 1.0, weights= 'uniform')\npred=clf.fit(X,y)\ny_pred=pred.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nAccuracy=(TP+TN)/(TP+TN+FN+FP)\nError=(FP+FN)/(TP+TN+FN+FP)\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Accuracy,Error,Precision, Recall, F1_Score]],columns=[\"Accuracy\",\"Error\",\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that from 63 percent accuracy, the model already improved into 74.8 percent\naccuracy, which is an increment of 11%. A 36.4 percent of error rate has dropped to 25\npercent. Precision increased from 64.1 percent to 75.79 percent, an increment of 11.69\npercent. Also, sensitivity increased from 67 percent to 71 percent. These increment in\naccuracies tells us that through fine tuning the model can perform better and quicker.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]},cv=5).fit(X_train, y_train)\nrandom_forest.fit(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)\nacc_test_random_forest = round(random_forest.score(X_test, y_test) * 100, 2)\nacc_test_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\ntrain_results = []\ntest_results = []\nfor estimator in n_estimators:\n   rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n   rf.fit(X_train, y_train)\n   train_pred = rf.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = rf.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, train_results, \"b\", label=\"Train AUC\")\nline2, = plt.plot(n_estimators, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"n_estimators\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n   rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n   rf.fit(X_train, y_train)\n   train_pred = rf.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = rf.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, \"b\", label=\"Train AUC\")\nline2, = plt.plot(max_depths, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Tree depth\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_split in min_samples_splits:\n   rf = RandomForestClassifier(min_samples_split=min_samples_split)\n   rf.fit(X_train, y_train)\n   train_pred = rf.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = rf.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, train_results, \"b\", label=\"Train AUC\")\nline2, = plt.plot(min_samples_splits, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"min samples split\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = list(range(1,X_train.shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n   rf = RandomForestClassifier(max_features=max_feature)\n   rf.fit(X_train, y_train)\n   train_pred = rf.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = rf.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, train_results, \"b\", label=\"Train AUC\")\nline2, = plt.plot(max_features, test_results, \"r\", label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"max features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier( bootstrap = True,max_leaf_nodes=33,n_estimators= 188,max_depth=12,min_samples_split=0.6,max_features=\"sqrt\")\n \npred=clf.fit(X,y)\ny_pred=pred.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nAccuracy=(TP+TN)/(TP+TN+FN+FP)\nError=(FP+FN)/(TP+TN+FN+FP)\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Accuracy,Error,Precision, Recall, F1_Score]],columns=[\"Accuracy\",\"Error\",\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Hyperparameter grid\nparam_grid = {\n    'n_estimators': np.linspace(10, 200).astype(int),\n    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n    'min_samples_split': [2, 5, 10],\n    'bootstrap': [True, False]\n}\n\n# Estimator for use in random search\nestimator = RandomForestClassifier(random_state = 0)\n\n# Create the random search model\nrs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n                        scoring = 'roc_auc', cv = 3, \n                        n_iter = 10, verbose = 1, random_state=0)\n\n# Fit \nrs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_nodes = []\nmax_depths = []\n\nfor ind_tree in best_model.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint(f'Average number of nodes {int(np.mean(n_nodes))}')\nprint(f'Average maximum depth {int(np.mean(max_depths))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nsample_leaf_options = [1,2,3,4,5,10,20]\n#X_train=X_train.reshape(1,-1)\n# for loop to iterate for each leaf size\nfor leaf_size in sample_leaf_options :\n    model = RandomForestClassifier(n_estimators = 200, n_jobs = -1,random_state =0, min_samples_leaf = leaf_size)\n    model.fit(X_train,y_train)\n    print(\"\\n Leaf size :\", leaf_size)\n    print (\"AUC - ROC : \", roc_auc_score(y_train,model.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=RandomForestClassifier(n_estimators = 1000, n_jobs = -1,random_state =0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_lbl=[\"age\",\"gender\",\"ap_hi\" ,\"ap_low\",\"cholesterol\",\"gluc\",\"smoke\",\"alco\",\"active\",\"bmi\"]\nfor feature in zip(feature_lbl, clf.feature_importances_):\n    print(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nsfm = SelectFromModel(clf, threshold=0.1)\nsfm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature_list_index in sfm.get_support(indices=True):\n    print(feature_lbl[feature_list_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_important = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nclf_important.fit(X_important_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_important_pred = clf_important.predict(X_important_test)\n\naccuracy_score(y_test, y_important_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=188,min_samples_leaf=1,max_leaf_nodes=33,max_features=4,max_depth=12, random_state=0, n_jobs=-1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_important = RandomForestClassifier(n_estimators=188,max_leaf_nodes=33,max_features=0.799,max_depth=12, random_state=0, n_jobs=-1)\nclf_important.fit(X_important_train, y_train)\ny_important_pred = clf_important.predict(X_important_test)\naccuracy_score(y_test, y_important_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test, y_important_pred)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nAccuracy=(TP+TN)/(TP+TN+FN+FP)\nError=(FP+FN)/(TP+TN+FN+FP)\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Accuracy,Error,Precision, Recall, F1_Score]],columns=[\"Accuracy\",\"Error\",\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_important = RandomForestClassifier(n_estimators=188,max_leaf_nodes=33,max_depth=12, min_samples_split=0.6,max_features=\"sqrt\",random_state=0, n_jobs=-1)\nclf_important.fit(X_important_train, y_train)\ny_important_pred = clf_important.predict(X_important_test)\naccuracy_score(y_test, y_important_pred)\n#max_features=0.799","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test, y_important_pred)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nAccuracy=(TP+TN)/(TP+TN+FN+FP)\nError=(FP+FN)/(TP+TN+FN+FP)\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Accuracy,Error,Precision, Recall, F1_Score]],columns=[\"Accuracy\",\"Error\",\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}