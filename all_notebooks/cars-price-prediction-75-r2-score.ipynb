{"cells":[{"metadata":{},"cell_type":"markdown","source":"# US Cars Dataset Price Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"US Cars' data was scraped from AUCTION EXPORT.com. This dataset included Information about 28 brands of clean and used vehicles for sale in US. Twelve initial features were assembled for each car in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\npd.set_option('display.max_rows', None)\nplt.rcParams['figure.figsize']=(16, 8.27) #set graphs size to A4 dimensions\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/usa-cers-dataset/USA_cars_datasets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop unnecessary columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(['Unnamed: 0','vin','lot'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create and transform features ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are going to extract how many minutes left from the \"Condition\" column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['value']= dataset['condition'] .str.split(' ').str[0]\ndataset['days']= dataset['condition'] .str.split(' ').str[1]\n\ndef days_to_min_converter(time):\n    return int(time)*1440\n\ndef hours_to_min_converter(time):\n    return int(time)*60\n\n\ntemp_data=pd.concat([dataset[dataset['days']=='days']['value'].apply(days_to_min_converter),\n           dataset[dataset['days']=='hours']['value'].apply(hours_to_min_converter),\n           dataset[dataset['days']=='minutes']['value'].astype(int)]).rename('Minutes_Left',inplace=True)\n\n\n\ndataset=pd.concat([dataset,temp_data],axis=1)\ndataset['Minutes_Left'].fillna(-200,inplace=True)\n\ndataset.drop(['condition','value','days'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the Year column to represent which year of car's registration is in progress (e.g first, second, third, etc...)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def year_transform(year):\n    return 2021-year\n\ndataset['year']=dataset['year'].apply(year_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create new feature which represents Mileage per Year for each car.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['miles/year']=dataset['mileage']/dataset['year']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find out numerical and categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']\n\nnumerical_features=[feature for feature in dataset.columns if dataset[feature].dtype!='O']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Create some visualizations and print some information about features distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframes=[]\nfor feature in categorical_features:\n    dataframe=dataset[feature].value_counts().rename_axis(feature).reset_index(name='counts')\n    dataframes.append(dataframe)\n\nfor i in range(len(dataframes)):\n    print(dataframes[i],'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numerical_features:\n    sns.distplot(dataset[feature])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SPLITTING DATASET TO TRAIN AND TEST SET AND APPLYING SOME FEATURE ENGINEERING TECHNIQUES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.drop('price',axis=1)\ny=dataset['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n\ntrain_set=pd.concat([X_train,y_train],axis=1)\ntest_set=pd.concat([X_test,y_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### APPLYING TARGET MEAN ENCODING FOR CATEGORICAL FEATURES AND SCALING THE TRAIN AND TEST SET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in categorical_features:\n    feature_labels=train_set.groupby(feature)['price'].mean().sort_values().index\n    feature_labels={k:i for i,k in enumerate(feature_labels,0)}\n    train_set[feature]=train_set[feature].map(feature_labels)\n    test_set[feature]=test_set[feature].map(feature_labels)\n\ntest_set.dropna(inplace=True)\n\nscaler=StandardScaler()\n\nscaled_X_train=pd.DataFrame(scaler.fit_transform(train_set.drop('price',axis=1)), columns=X_train.columns)\nscaled_X_train.index=train_set.index\nscaled_X_test=pd.DataFrame(scaler.transform(test_set.drop('price',axis=1)), columns=X_test.columns)\nscaled_X_test.index=test_set.index\n\n\nscaled_train=pd.concat([scaled_X_train,train_set['price']],axis=1)\nscaled_test=pd.concat([scaled_X_test,test_set['price']],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE SELECTION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Now we are going to check feature importances using Random Forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg=RandomForestRegressor()\nreg.fit(scaled_train.drop('price',axis=1),scaled_train['price'])\n\nfeat_importances = pd.Series(reg.feature_importances_, index=scaled_train.drop('price',axis=1).columns)\nfeat_importances.nlargest(scaled_train.drop('price',axis=1).shape[1]).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see 'Country' is not a useful feature and we drop it from train and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train.drop('country',axis=1,inplace=True)\nscaled_test.drop('country',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL SELECTION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=scaled_train.drop('price',axis=1)\ny_train=scaled_train['price']\n\nX_test=scaled_test.drop('price',axis=1)\ny_test=scaled_test['price']\n\n\n\n\nlm=LinearRegression()\nsvr=SVR()\nrf=RandomForestRegressor()\nxgb_reg=xgb.XGBRegressor()\nlgb_reg=lgb.LGBMRegressor()\n\n\nscore_lm=cross_val_score(lm,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_svr=cross_val_score(svr,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_rf=cross_val_score(rf,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_xgb_reg=cross_val_score(xgb_reg,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_lgb_reg=cross_val_score(lgb_reg,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\n\n\nscores=pd.DataFrame({'Model':['Linear Regression','SVR','Random Forest','XGBoost','LightGBM'],\n                    'Mean Squared Error':[-score_lm.mean(),-score_svr.mean(),-score_rf.mean(),\n                                           -score_xgb_reg.mean(),-score_lgb_reg.mean()]})\n\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to evaluate and apply Hyper Parameter tuning for Random Forest only.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Evaluating the model's performance on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train,y_train)\ny_pred_rf=rf.predict(X_test)\n\nprint('MSE: ',mean_squared_error(y_test,y_pred_rf))\nprint('R2: ',r2_score(y_test,y_pred_rf))\nprint('MAE: ',mean_absolute_error(y_test,y_pred_rf))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_rf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4,8,16,32]\n\n#learning_rate = [0.1, 0.01, 0.001]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               #'learning_rate':learning_rate,\n               'min_samples_leaf':min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrandom_rf=RandomizedSearchCV(rf,cv=10,param_distributions=random_grid,scoring='neg_mean_squared_error',n_jobs=-1,verbose=1)\nrandom_rf.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Î•valuating best model's performance according to hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rf=random_rf.best_estimator_\nbest_rf.fit(X_train,y_train)\ny_pred_bestrf=best_rf.predict(X_test)\n\nprint('MSE: ',mean_squared_error(y_test,y_pred_bestrf))\nprint('R2: ',r2_score(y_test,y_pred_bestrf))\nprint('MAE: ',mean_absolute_error(y_test,y_pred_bestrf))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_bestrf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_test-y_pred_bestrf)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(y_test,y_pred_bestrf)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scatterplot above we see that relationship between actual and predicted values tends to be linear, so we have built a good model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}