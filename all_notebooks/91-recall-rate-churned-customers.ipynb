{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom xgboost import XGBClassifier\n\npd.set_option(\"display.max_columns\", 100)\npd.set_option('display.width', 1000)\n\npath = '/kaggle/input/credit-card-customers/BankChurners.csv'\ndf = pd.read_csv(path)\n\n\n#-------------some pre data processing(quite obvious ones)-----------------------\ndf['Attrition_Flag'].replace({'Existing Customer':0, 'Attrited Customer':1},inplace=True)\ndf.drop(df.columns[[0,-1,-2]].values,axis=1,inplace=True)\n# print(df.head(2))\n# print(df.shape)\ndf.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data Analysis**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#------------Exploratory Data Analysis---------------------\nplt.hist(df['Customer_Age'],bins=40,density=True)    # age follows normal curve\nplt.xticks(range(25,75,1))\nplt.show()\n\nsizes = (df['Attrition_Flag'].value_counts()).tolist()\nplt.pie(sizes,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\nplt.show()\n\n#______gender based division\nsizes_f = df.loc[df['Gender']=='F']['Attrition_Flag'].value_counts()\nsizes_m = df.loc[df['Gender']=='M']['Attrition_Flag'].value_counts()\nfig,(ax1,ax2) = plt.subplots(1,2)\nax1.pie(sizes_f,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\nax1.title.set_text('Females')\nax2.pie(sizes_m,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\nax2.title.set_text('Males')\nplt.show()\n\nsizes_gender = df['Gender'].value_counts()    #not much difference. thus it's eually distributed.\nplt.pie(sizes_gender.tolist(),autopct='%1.2f%%',labels=sizes_gender.index.values,explode=[0.1,0],shadow=True)\nplt.show()\n\n#______card category based\nsizes_b = df.loc[df['Card_Category']=='Blue']['Attrition_Flag'].value_counts().tolist()\nsizes_s = df.loc[df['Card_Category']=='Silver']['Attrition_Flag'].value_counts().tolist()\nsizes_g = df.loc[df['Card_Category']=='Gold']['Attrition_Flag'].value_counts().tolist()\nsizes_p = df.loc[df['Card_Category']=='Platinum']['Attrition_Flag'].value_counts().tolist()\nfig,((axs0, axs1), (axs2, axs3)) = plt.subplots(2,2)\naxs0.pie(sizes_b,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs0.title.set_text('Blue Card')\naxs1.pie(sizes_s,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs1.title.set_text('Silver Card')\naxs2.pie(sizes_g,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs2.title.set_text('Gold Card')\naxs3.pie(sizes_p,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs3.title.set_text('Platinum Card')\nplt.show()\n\n\n#______income category based\n\nsizes_0 = df.loc[df['Income_Category']=='Less than $40K']['Attrition_Flag'].value_counts().tolist()\nsizes_40 = df.loc[df['Income_Category']=='$40K - $60K']['Attrition_Flag'].value_counts().tolist()\nsizes_60 = df.loc[df['Income_Category']=='$60K - $80K']['Attrition_Flag'].value_counts().tolist()\nsizes_80 = df.loc[df['Income_Category']=='$80K - $120K']['Attrition_Flag'].value_counts().tolist()\nsizes_120 = df.loc[df['Income_Category']=='$120K +']['Attrition_Flag'].value_counts().tolist()\nsizes_unkn = df.loc[df['Income_Category']=='Unknown']['Attrition_Flag'].value_counts().tolist()\n\nfig,((axs0, axs1, axs2), (axs3, axs4, axs5)) = plt.subplots(2,3)\naxs0.pie(sizes_0,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs0.title.set_text('<40K')\naxs1.pie(sizes_40,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs1.title.set_text('40-60K')\naxs2.pie(sizes_60,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs2.title.set_text('60-80K')\naxs3.pie(sizes_80,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs3.title.set_text('80-120K')\naxs4.pie(sizes_120,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs4.title.set_text('>120K')\naxs5.pie(sizes_unkn,explode=[0,0.1],shadow=True,autopct='%1.2f%%',labels=['Existing','Churned'])\naxs5.title.set_text('Unknown')\nplt.show()\n\n#-----education based----\nsizes_edu = df['Education_Level'].value_counts()\nplt.pie(sizes_edu.tolist(),labels=sizes_edu.index.values,autopct='%1.2f%%')\nplt.show()\n#\n# #-------------------correlation between numeric variables and target--------------\nnumeric_data = df.select_dtypes(include=[np.number])\ncorr_numeric = numeric_data.corr()\nsbn.heatmap(corr_numeric,cmap=\"YlGnBu\",annot=True)\nplt.xticks(rotation=45)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **DATA PROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"**Conversion of categorical variables into numerical**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #--------------let's convert some categorical variables into numerical--------------\n# #ordinal to numerical\nmap_education_level = {'High School':1,'Graduate':3,'Uneducated':0,'College':2,'Post-Graduate':4,'Doctorate':5}\nmap_income_level = {'$60K - $80K':3,'Less than $40K':1, '$80K - $120K':4,'$40K - $60K':2,'$120K +':5}\nmap_card_category = {'Blue':1,'Gold':3,'Silver':2,'Platinum':4}\ndf['Education_Level'].replace(map_education_level,inplace=True)\ndf['Income_Category'].replace(map_income_level,inplace=True)\ndf['Card_Category'].replace(map_card_category,inplace=True)\n\n#\n# #hot encoding of gender category\ndf.insert(2,'Gender_M',df['Gender'],True)\ndf.rename({'Gender':'Gender_F'},axis=1,inplace=True)\ndf['Gender_M'].replace({'M':1,'F':0},inplace=True)\ndf['Gender_F'].replace({'M':0,'F':1},inplace=True)\n#\n# #hot encoding of marital status\ndf.insert(7,'Single',df['Marital_Status'],True)\ndf.insert(7,'Divorced',df['Marital_Status'],True)\ndf.insert(7,'Unknown',df['Marital_Status'],True)\ndf.rename({'Marital_Status':'Married'},axis=1,inplace=True)\ndf['Married'].replace({'Single':0, 'Married':1, 'Divorced':0, 'Unknown':0},inplace=True)\ndf['Single'].replace({'Single':1, 'Married':0, 'Divorced':0, 'Unknown':0},inplace=True)\ndf['Divorced'].replace({'Single':0, 'Married':0, 'Divorced':1, 'Unknown':0},inplace=True)\ndf['Unknown'].replace({'Single':0, 'Married':0, 'Divorced':0, 'Unknown':1},inplace=True)\n\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dealing With missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df.loc[df['Income_Category']!='Unknown']['Income_Category'])   # income is rightly skewed. so central value is median\nplt.show()\n\nplt.hist(df.loc[df['Education_Level']!='Unknown']['Education_Level'])   # education is normally distributed. so central value is mean\nplt.show()\n\n#Missing values in education column\neducatedDF = df.loc[df['Education_Level']!='Unknown']\nmean_education = educatedDF['Education_Level'].mean()\ndf['Education_Level'].replace({'Unknown':mean_education},inplace=True)\n\n#Missing values in income column\nsalariedDF = df.loc[df['Income_Category']!='Unknown']\nmedian_salaries = salariedDF['Income_Category'].median()\ndf['Income_Category'].replace({'Unknown':median_salaries},inplace=True)\n\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,1:]\ny = df.iloc[:,0]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Upsampling using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\n#-----Upsampling----\nfrom sklearn.utils import resample\nfrom collections import Counter\n\nprint(\"Before Upsampling:-\")\nprint(Counter(y_train))\n\n# X_train_upsampled, y_train_upsampled = resample(x_train[y_train == 1],\n#                                                 y_train[y_train == 1],\n#                                                 replace=True,\n#                                                 n_samples=x_train[y_train == 0].shape[0],\n#                                                 random_state=123)\n\n\n# Let's use SMOTE to oversample\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nx_train_upsampled, y_train_upsampled = oversample.fit_resample(x_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(Counter(y_train_upsampled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FITTING INTO MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#-----Random Forest after upsampling------\nprint(\"\\n\\n\\n\\n AFTER UPSAMPLING\\n\\n\")\nclassifier = RandomForestClassifier(n_estimators = 50, random_state = 0)\nclassifier.fit(x_train_upsampled, y_train_upsampled)\n# Predicting result for training set and validation set\npredict_val_rf = classifier.predict(x_test)\n\n\n# Model Performance\n\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf) *  100)\nprint(confusion_matrix(y_test, predict_val_rf))\nprint(classification_report(y_test, predict_val_rf))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using XGboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(x_train_upsampled, y_train_upsampled)\n# Predicting result for training set and validation set\npredict_val_rf2 = model.predict(x_test)\n\n\n# Model Performance\n\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf2) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf2) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf2) *  100)\nprint(confusion_matrix(y_test, predict_val_rf2))\nprint(classification_report(y_test, predict_val_rf2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every time we run this model, we won't find the same exact score. Since smote wont create the exact same observations every time. I ran it like 10 times, and it was always around 91% of recall, so i'm gonna consider that only as my final result. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}