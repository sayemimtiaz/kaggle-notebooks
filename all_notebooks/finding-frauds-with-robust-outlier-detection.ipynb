{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nFraud detection is a common application of machine learning. There is a reason why the [Credit Card Fraud Detection Dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) is the most popular dataset on kaggle.\n\nBut most of the methods addressing the problem of fraud detection are methods of supervised learning, where you have information about which data belongs to a fraud.<br>\n**But what if you don't have this information?**<br>\n\nIn this notebook I am going to present a method of outlier detection, which can be put to good use in the context of fraud detection.\n\n*By the way: This is my first notebook, so some feedback would be greatly appreciated.\n*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Robust outlier detection using FSRMCD-MAC\nIn this notebook I want to demonstrate the FSRMCD-MAC algorithm introduced in [Jobe, Pokojovy (2015)](https://www.researchgate.net/publication/268817944_A_Cluster-Based_Outlier_Detection_Scheme_for_Multivariate_Data). It is a cluster-based approach to outlier detection and is based on three main steps, which I will explain later on.<br>\nFSRMCD-MAC stands for **finite sample reweighted minimum covariance determinant mode association clustering**. So I will keep it short and refer to it as the FSRMCD-MAC algorithm.\n\nAs data we are going to use the [Swiss bank note dataset](https://www.kaggle.com/chrizzles/swiss-banknote-conterfeit-detection), which was also used in the original paper of the algorithm. It contains information on 200 swiss banknotes and wether they are a genuine or a conterfeit banknote.\n\n### What are we going to do?\n1. **Introduction**: If you're reading this, we're already past that point.\n2. **Short data exploration**: This notebook is not about data analysis, but we will still have a short look at the data.\n3. **The FSRMCD-MAC algorithm**: Short overview over the actual algorithm, as simple as possible.\n4. **Comparing different methods for Outlier detection**: Comparing the FSRMCD-MAC algorithm with other outlier detection methods.\n5. **Conclusion**\n6. **Further Information**: Here you can find the relevant papers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.covariance import MinCovDet\nimport scipy as sc\nimport plotly.graph_objects as go\nfrom scipy.spatial.distance import mahalanobis, euclidean\nfrom scipy.stats import chi2\nfrom scipy.stats import multivariate_normal\nfrom scipy.linalg import sqrtm\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nimport scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_raw=pd.read_csv(\"/kaggle/input/swiss-banknote-conterfeit-detection/banknotes.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Short data exploration\nBefore we get started, we should have look at the data.  \nThe data consists of the following Attributes:\n* **conterfeit**: Wether a banknote is conterfeit (1) or genuine (0)\n* **Length**: Length of bill (mm)\n* **Left**: Width of left edge (mm)\n* **Right**: Width of right edge (mm)\n* **Bottom**: Bottom margin width (mm)\n* **Top**: Top margin width (mm)\n* **Diagonal**: Length of diagonal (mm)\n\nFrom the datasets description we know, that the feature 'conterfeit' is evenly distributed, **there are 100 genuine and 100 fake banknotes.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop label\nSince we will be using unsupervised learning, the label is now dropped and only used in the end for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop conterfeit information\ndata=data_raw.drop([\"conterfeit\"],axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Density plot\nWe want to know what the distribution for each feature looks like. Is it normal distributed or not? Is it unimodal or multimodal?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2,3,figsize=(18, 8))\ncolumns=data.columns.values\nfor index,name in enumerate(columns):\n    x,y=(0,index) if index<3 else (1,len(columns)-index-2)\n    sns.distplot(data[name],ax=ax[x][y])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the plot, most of the features seem to form some kind of normal distribution.\nEspecially the features '**Length**' and '**Top**' have a really nice looking curve.\nThe Features '**Bottom**' and '**Diagonal**' on the other hand, seem to come from some kind of multimodal distribution, since they have two peeks (especially 'Diagonal').","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Correlation heatmap\nWe now want to take a look at the correlation of the different features. This might indicate wether some features have redundant information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(data.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest correlation is between the features 'left' and 'right', which makes sense, since both edges of a banknote should be somewhat equally sized.  The feature 'Diagonal' has a negative correlation with all features except 'length'. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# The FSRMCD-MAC Algorithm\n## Basic idea\nI will try to explain the basic steps of the algorithm as simple as possible.<br>\n**Assumptions:**<br>\nThe data is assumed to come from a finite mixture of multivariate normal densities, with the biggest component representing the main cluster of datapoints. All datapoints which are outside of this cluster are treated as potential outliers.<br>\n**Aim:**<br>\nThe aim of the algorithm is to estimate the multivariate probability density function of the data. With this estimation different clusters can be identified and the largest cluster is extracted. The data from the largest cluster is then used to estimate the mean and the covariance of the data. All data which are \"too far away\" from the mean are now identified as outliers.\n\n### Steps of the algorithm\nThe algorithm consists of three main steps, which I will describe a little further. <br>\n1. **FSRMCD-step**<br>\nIn this step robust estimators for the mean and covariance of the data are created. Why do we need robust estimators?<br> A robust method in a statistical sense, is a method that is less affected by outliers than \"normal\" methods.<br> So while the \"usual\" estimators of mean and covariance would be hugely distorted, due to the outliers in the dataset, the effect is not as big using the robust estimators.<br>\nThe estimator used in this algorithm is the FSRMCD estimator. You can find more information about it at the end of the notebook.\n2. **MAC-step**<br>\nThe robust estimators are used to estimate the probability density function of the data using KDE (Kernel density estimation). This function is now maximized for each data point. All data points which correspond to the same maximum are put in the same cluster. This process is called modal association clustering (MAC). From these clusters, the largest one is extracted and its mean and covariance are calculated.\n3. **Outlier detection step**<br>\nUsing the mean and covariance from the largest cluster a distance measure, the mahalanobis distance, is calculated for each datapoint. All datapoints with a mahalanobis distance exceeding a specified level are now identified as outliers.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Implementation","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"def get_mcd_outliers(data,threshold,type=\"outlier\"):\n    #MCD Estimators and Mahalanobis distances of data\n    mcd_estimator=MinCovDet().fit(data)\n    mahalanobis=mcd_estimator.mahalanobis(data)\n    #Calculate outliers based on threshold\n    if type==\"outlier\":\n        transformation = lambda x: 0 if x <= threshold else 1\n    elif type==\"weights\":\n        transformation = lambda x: 1 if x <= threshold else 0\n    outliers = np.array([transformation(xi) for xi in mahalanobis])\n    return outliers\n\ndef get_fsrmcd_estimators(data,threshold,n,p,type=\"estimator\"):\n    weights=get_mcd_outliers(data,threshold,\"weights\")\n    #FSRMD Location\n    fsrmcd_location = np.matrix(np.average(data, axis=0, weights=weights))\n\n    #FSRMCD Covariance\n    fsrmcd_covariance = 0\n    for x in range(0, n):\n        fsrmcd_covariance = fsrmcd_covariance + weights[x] * np.dot(\n            np.transpose((data.loc[x].values - fsrmcd_location)),\n            (data.loc[x].values - fsrmcd_location))\n\n    k_constant=(np.floor(0.975*n)/n)/(chi2.cdf(threshold,p+2))\n\n    fsrmcd_covariance=(k_constant/(sum(weights)-1))*fsrmcd_covariance\n    #Returns estimators\n    if type==\"estimator\":\n        return (fsrmcd_location,fsrmcd_covariance)\n    \n    #Returns outliers for FSRMCD method only\n    elif type==\"outlier\":\n        #Calculate mahalanobis distances and determine the outliers\n        mahalanobis_fsrmcd=np.array([])\n        for x in range(0,n):    \n            maha=np.power(sc.spatial.distance.mahalanobis(data.loc[x].values,fsrmcd_location,np.linalg.inv(fsrmcd_covariance)),2)\n            mahalanobis_fsrmcd=np.append(mahalanobis_fsrmcd,maha) \n        transformation = lambda x: 0 if x <= threshold else 1\n        outliers = np.array([transformation(xi) for xi in mahalanobis_fsrmcd])\n        return outliers\n\n\ndef get_fsrmcdmac_outliers(data):\n    #Data shape\n    n=len(data)\n    p=data.shape[1]\n    \n    #Threshold Chi-Square Distribution\n    threshold = chi2.ppf(0.975, p)\n    \n    #Get FSRMCD Estimators\n    fsrmcd_location,fsrmcd_covariance=get_fsrmcd_estimators(data,threshold,n,p)\n\n    #Calculate bandwidth matrix\n    #Split matrix in pre factor hb and matrix H\n    hb_factor=np.power((4/(4+p)),(1/(p+6)))*np.power(n,(-1/(p+6)))\n    #Factor is different for different values for p, see the paper for details\n    cnp_factor=0.7532+37.51*np.power(n,-0.9834)\n    H=sqrtm(cnp_factor*fsrmcd_covariance)\n    H_inverse=np.linalg.inv(H)\n\n    #Copy and standardize data\n    data_standardized = data.copy()\n    for x in range(0, n):\n        data_standardized.loc[x] = np.reshape(\n            np.dot(H_inverse, np.transpose((data_standardized.loc[x].values - fsrmcd_location))),p)\n\n    #Mode Association Clustering Algorithm\n    modes=np.zeros((p,n))\n    for x in range(0,n):\n\n        #Select each datapoint as starting point from where to find the local maximum\n        x0=data_standardized.loc[x].values\n        x0_old=x0+1\n\n        #Define stopping-criteria\n        cnt=0\n        err=100\n\n        #Iterative algorithm to find the maximum\n        while ((err>0.0001) and (cnt<150)):\n            diag=np.zeros((p,p),int)\n            np.fill_diagonal(diag,1)\n            kde=multivariate_normal.pdf(data_standardized,x0,hb_factor*diag)\n\n            d=kde/sum(kde)\n            x0=np.dot(np.transpose(d),data_standardized)\n\n            err=np.linalg.norm(x0-x0_old,ord=2)/np.maximum(np.linalg.norm(x0,ord=2),1)\n\n            x0_old=x0\n            cnt=cnt+1\n\n        modes[:,x]=x0\n\n    #Put different modes into different clusters\n    clusters=np.zeros((1,n))\n    clust_cnt=0\n\n    err=1/(2*n)\n\n    for x in range(0,n):\n        if clusters[:,x]==0:\n            clust_cnt=clust_cnt+1\n            clusters[:,x]=clust_cnt\n\n            for y in range(0,n):\n                if clusters[:,y]==0:\n                    if np.linalg.norm((modes[:,x]-modes[:,y]),ord=2)<err:\n                        clusters[:,y]=clust_cnt\n\n    #Get largest cluster and corresponding mode\n    clust_max=-1\n    clust_s=0\n    for x in range(1,clust_cnt+1):\n        s=len(clusters[clusters==x])\n\n        if s>clust_s:\n            clust_max=x\n            clust_s=s\n\n            ind=min(clusters[clusters==x])\n            mode=modes[:,int(ind)]\n\n    bulk=np.where(clusters==clust_max)[1]\n\n    #Get mode by reverting the standardization\n    mode=np.dot(H,mode)+fsrmcd_location\n\n    #Save final Cluster\n    if len(data.loc[bulk])<(p+1):\n        cluster_final=data\n    else:\n        cluster_final=data.loc[bulk]\n        cluster_final.reset_index(inplace=True)\n        cluster_final.drop(\"index\",axis=1,inplace=True)\n\n    #Get estimators from cluster\n    mean_cluster = cluster_final.mean().values\n    covariance_cluster = cluster_final.cov().values\n    weights=np.array([])\n\n    #Get Mahalanobis distance and outliers\n    for x in range(0, len(cluster_final)):\n        maha = np.power(sc.spatial.distance.mahalanobis(cluster_final.loc[x].values, mean_cluster,\n                                               np.linalg.inv(covariance_cluster)),2)\n        if maha <= threshold:\n            weights=np.append(weights,1)\n        else:\n            weights=np.append(weights,0)\n\n\n    #Get final robust estimators\n    #Location\n    robust_location = np.matrix(np.average(cluster_final, axis=0, weights=weights))\n\n    #Covariance\n    robust_covariance = 0\n    for x in range(0, len(cluster_final)):\n        robust_covariance = robust_covariance + weights[x] * np.dot(\n            np.transpose((data.loc[x].values - robust_location)),\n            (data.loc[x].values - robust_location))\n\n    robust_covariance=(1/(sum(weights)-1))*robust_covariance\n\n    #Calculate final mahalanobis distances and determine the final outliers\n    mahalanobis_robust=np.array([])\n    for x in range(0,n):    \n        maha=np.power(sc.spatial.distance.mahalanobis(data.loc[x].values,robust_location,np.linalg.inv(robust_covariance)),2)\n        mahalanobis_robust=np.append(mahalanobis_robust,maha)    \n\n    #Outlier thresholds\n    #L1 and L2 depend on the dimension of the dataset. See the paper for values for different dimensions\n    L1=31.9250\n    L2=16.9710\n    if mahalanobis_robust.max()<L1:\n        outliers=np.repeat(0,200)\n    else:\n        outliers=np.array([])\n        for x in range(0,n):\n            if mahalanobis_robust[x]>L2:\n                outliers=np.append(outliers,1)\n            else:\n                outliers=np.append(outliers,0)\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results and confusion matrix\nHow did the method perform?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results=data_raw.copy()\nresults[\"Outlier_FSRMCDMAC\"]=get_fsrmcdmac_outliers(data)\nresults[\"Outlier_FSRMCDMAC\"]=results[\"Outlier_FSRMCDMAC\"].astype(int)\nprint(\"Accuracy: \"+str(accuracy_score(results[\"conterfeit\"],results[\"Outlier_FSRMCDMAC\"])))\nskplt.metrics.plot_confusion_matrix(results[\"conterfeit\"],results[\"Outlier_FSRMCDMAC\"],normalize=\"true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the FSRMCD-MAC algorithm was able to detect all fake banknotes and almost all of the genuine banknotes, without having any information about which is which.<br> The overall accuracy is **98%**, which is pretty amazing.<br>\nBut to see if this is actually a special result, we should compare it to some other outlier detection methods.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Comparing different methods for Outlier detection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we want to compare the FSRMCD-MAC algorithm to different methods for outlier detection.  I've decided to include three methods as comparison.\n* **Isolation Forest**: Density based outlier detection method implemented in sklearn.\n* **Local Outlier Factor**: Outlier detection based on random forests, implemented in sklearn.\n* **MCD-Method**: Robust covariance estimation using MCD and outlier detection using Mahalanobis distance measure, available in sklearn.\n\n### What makes a good method?\nSince we are looking at the case of fraud detection, it is more important to detect all frauds, than all non-frauds. Therefore a good method detects as many conterfeits as possible. Since we defined conterfeit as 1 and genuine banknote as 0, this is the same as having a **high TPR (True positive rate) or a high sensitivity.**<br>\nThe TNR (True negative rate) shouldn't be ignored though, we don't want to accuse too many innocent people of faking a banknote.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Isolation Forest\nisf=IsolationForest()\nisf_outliers=isf.fit_predict(data)\nresults[\"Outlier_ISF\"]=isf_outliers\nresults[\"Outlier_ISF\"]=results[\"Outlier_ISF\"].map({1:0,-1:1})\n\n#Local Outlier Factor\nlof=make_pipeline(StandardScaler(),LocalOutlierFactor())\nlof_outliers=lof.fit_predict(data)\nresults[\"Outlier_LOF\"]=lof_outliers\nresults[\"Outlier_LOF\"]=results[\"Outlier_LOF\"].map({1:0,-1:1})\n\n#Mahalanobis Distance with MCD Estimators\np=data.shape[1]\nthreshold = chi2.ppf(0.975, p)\nmcd_outliers = get_mcd_outliers(data,threshold)\nresults[\"Outlier_MCD\"]=mcd_outliers\n\n#Compare different methods\nmethods=[\"Outlier_ISF\",\"Outlier_LOF\",\"Outlier_MCD\",\"Outlier_FSRMCDMAC\"]\ncomparison=pd.DataFrame(columns=[\"TN\",\"FP\",\"FN\",\"TP\"])\nfor method in methods:\n    comparison.loc[method]=confusion_matrix(results[\"conterfeit\"],results[method]).reshape(4)\ncomparison[\"Accuracy\"]=(comparison[\"TP\"]+comparison[\"TN\"])/len(data)\ncomparison[\"Sensitivity\"]=(comparison[\"TP\"])/(len(data)/2)\ncomparison[\"Specificity\"]=(comparison[\"TN\"])/(len(data)/2)\ncomparison.sort_values(by=\"Sensitivity\",inplace=True)\ncomparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can already see how the different methods compare, but lets look at a visualization to make it a bit easier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"methods=[\"Local Outlier Factor\",\"MCD\",\"Isolation Forest\",\"FSRMCD-MAC\"]\nfig=go.Figure()\nfig.add_trace(go.Bar(y=methods,x=comparison[\"TN\"]/100,orientation=\"h\",name=\"Correctly identified genuine banknotes (TNR) (%)\"))\nfig.add_trace(go.Bar(y=methods,x=comparison[\"TP\"]/100,orientation=\"h\",name=\"Correctly identified fake banknotes (TPR) (%)\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FSRMCD-MAC beats them all\nAs you can see in the plot, none of the other methods came close in detecting the fake banknotes.<br>\nWhile the true negative rate is somewhat similar for all methods, **the FSRMCD-MAC algorithm detected 5 times more conterfeits than all the other methods.**\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conlusion\nThe FSRMCD-MAC algorithm is a powerful method for outlier detection and even works if half of the data is made of outliers. It performed way better than other comparable methods.\n\nYou should be a bit careful though, since the dataset is from the original paper, it is possible that the algorithm performs especially good on it.<br>\nWhat about data which is more distorted, categorical data, or data which is clearly not normal distributed?<br>\n**Feel free to copy this notebook and try out the algorithm on different datasets.**\n\nI hope you liked the notebook and maybe you can use it for your next Data Science project.<br>\n**Since this was my first notebook, feedback would be greatly appreciated.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Further information\nIf you want to obtain further information on this algorithm and the ones its based on, here are the main resources.\n* **Jobe, J. M. und Pokojovy, M. (2015). A cluster-based outlier detection scheme for multivariate data.**\n* **Li, J., Ray, S., und Lindsay, B. (2007). A nonparametric statistical approach to clustering via mode identification.**\n* **Cerioli, A. (2010). Multivariate outlier detection with high-breakdown estimators.**\n* **Rousseeuw, P. und Driessen, K. (1999). A fast algorithm for the minimum covariance determinant estimator.**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}