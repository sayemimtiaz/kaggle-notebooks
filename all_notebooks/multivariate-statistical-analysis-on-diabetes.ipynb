{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nIn multivariate analysis, i will not focus on distribution of data, this has been discussed in Notebook 1 (Univariate Statistical Analysis). Please find the notebook below ==>\n\n[Univariate Statistical Analysis](https://www.kaggle.com/ravichaubey1506/univariate-statistical-analysis-on-diabetes)\n\nThe notebook you are reading is second notebook in this series.\n\nHere is third notebook which gives you an understanding of how to make inference about population from population.\n\n[Inferential Statistics on Diabetes](https://www.kaggle.com/ravichaubey1506/inferential-statistics-on-diabetes)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## What is “multivariate”?\n\nMultivariate data analysis is a set of statistical models that examine patterns in multidimensional data by considering, at once, several data variables. It is an expansion of bivariate data analysis, which considers only two variables in its models. As multivariate models consider more variables, they can examine more complex phenomena and find data patterns that more accurately represent the real world."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I am conveting Outcome as Diab for Diabetic Patients and Non-Diab for Non Diabetic Patients. Also i will be renaming column name DiabetesPedigreeFunction to DPF.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Outcome = df.Outcome.replace({0:'Non-Diab',1:'Diab'})\ndf.DiabetesPedigreeFunction = df.rename({'DiabetesPedigreeFunction':'DPF'},inplace = True,axis =1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Summary\n\nData is related to healthcare Industry having 768 observations with 9 variable. Target variable is Outcome. It looks like there is no missing value, and boolean, float , integers are different datatypes available. Well descriptive analysis shows that variable Glucose, BoodPressure,SckinThickness, Insulin and BMI have minimum value 0 which does not make any sense, these values are either missing or outliers, But i am not going to alter them so that i can see actual statistics of Data. I can see in Pregnancies column, minimum is 0 (May be this is sign for no pregnancy) which is considerable, But maximum month of pregnancy is 17 which does not make any sense. Variance among different predictor variable is varying at large scale , Scaling data will be helpful for Predective modelling."},{"metadata":{},"cell_type":"markdown","source":"# Pairplot\n\n**Let us take a closer look at data by doing a quick visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi=120)\nsns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary ==>\n\nWell, Pregnancies, Insulin, DBF and Age having skewed distribution. We know most of the machine learning models uses assumpton of normality so these variables might need to be scaled, But we may consider the assumption to be true according Central Limit Theorem that if number of observation is large we can consider the distribution to be normal or bell shaped. Removing Outliers may also help us to achieve normal distribution of that variable.\n\nIt looks like Glucose, BP and BMI variables have some outliers.\n\nVariables are not correlated strongly with each other. I will plot a correlation matrix later."},{"metadata":{},"cell_type":"markdown","source":"#### Let us Plot pairplot according to outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi = 120)\nsns.pairplot(df,hue = 'Outcome',palette = 'plasma')\nplt.legend(['Non Diabetic','Diabetic'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary ==>\n\nWe can clearly see that data points are not seperable linearly according to Outcome. <font color = 'blue'>Distribution of variables are normal</font>, In some variables they are skewed to right due to Outliers. Treating Outliers may help to get rid of them. Because data points are spread non linear, Fitting tree based models might help us to get better accuracy or SVC with Non Linear Dicision Boundry."},{"metadata":{},"cell_type":"markdown","source":"# Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi = 120,figsize= (5,4))\nmask = np.triu(np.ones_like(df.corr(),dtype = bool))\nsns.heatmap(df.corr(),mask = mask, fmt = \".2f\",annot=True,lw=1,cmap = 'plasma')\nplt.yticks(rotation = 0)\nplt.xticks(rotation = 90)\nplt.title('Correlation Heatmap')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'>**Nice, Variables are not much associated linearly.**</font>"},{"metadata":{},"cell_type":"markdown","source":"# Jointplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of Glucose with Other Variables ==> \\n\")\nfor i in  df.columns:\n    if i != 'Glucose' and i != 'Outcome':\n        print(f\"Correlation between Glucose and {i} ==> \",df.corr().loc['Glucose'][i])\n        sns.jointplot(x='Glucose',y=i,data=df,kind = 'regression',color = 'purple')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>Glucose shows positive weak linear association with other variable in given dataset.</font> That means On increasing Glucose level in patients, Other variables will also increase. Weak linear association is good, so that we can escape out from Multicollinearity effect in Predective Modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df.columns)\nidx = col.index('BloodPressure')\n\nplt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of BloodPressure with Other Variables ==> \\n\")\nfor i in  range(idx+1,len(col)-1):\n    print(f\"Correlation between BloodPressure and {col[i]} ==> \",df.corr().loc['BloodPressure'][col[i]])\n    sns.jointplot(x='BloodPressure',y=col[i],data=df,kind = 'regression',color = 'green')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>BloodPressure shows positive weak linear association with other variable in given dataset.</font> That means On increasing BP level in patients, Other variables will also increase."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df.columns)\nidx = col.index('SkinThickness')\n\nplt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of SkinThickness with Other Variables ==> \\n\")\nfor i in  range(idx+1,len(col)-1):\n    print(f\"Correlation between SkinThickness and {col[i]} ==> \",df.corr().loc['SkinThickness'][col[i]])\n    sns.jointplot(x='SkinThickness',y=col[i],data=df,kind = 'regression',color = 'blue')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>SkinThickness shows positive weak linear association with other variable in given dataset <font color = 'red'> ,(Except with Age) </font>.</font> That means On increasing SkinThickness in patients, Other variables will also increase.<font color = 'blue'>SkinThickness with Age show a weak negative correlation, that means on increasing SkinThickness , Age must decrease.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df.columns)\nidx = col.index('Insulin')\n\nplt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of Insulin with Other Variables ==> \\n\")\nfor i in  range(idx+1,len(col)-1):\n    print(f\"Correlation between Insulin and {col[i]} ==> \",df.corr().loc['Insulin'][col[i]])\n    sns.jointplot(x='Insulin',y=col[i],data=df,kind = 'regression',color = 'green')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>Insulin shows positive weak linear association with other variable in given dataset <font color = 'red'> ,(Except with Age) </font>.</font> That means On increasing Insulin level in patients, Other variables will also increase.<font color = 'blue'>Insulin with Age show a weak negative correlation, that means on increasing SkinThickness , Age must decrease.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df.columns)\nidx = col.index('BMI')\n\nplt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of BMI with Other Variables ==> \\n\")\nfor i in  range(idx+1,len(col)-1):\n    print(f\"Correlation between BMI and {col[i]} ==> \",df.corr().loc['BMI'][col[i]])\n    sns.jointplot(x='BMI',y=col[i],data=df,kind = 'regression',color = 'green')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>BMI shows positive weak linear association with other variable in given dataset.</font> That means On increasing BMI level in patients, Other variables will also increase."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df.columns)\nidx = col.index('DPF')\n\nplt.figure(dpi = 100, figsize = (5,4))\nprint(\"Joint plot of DPF with Other Variables ==> \\n\")\nfor i in  range(idx+1,len(col)-1):\n    print(f\"Correlation between DPF and {col[i]} ==> \",df.corr().loc['DPF'][col[i]])\n    sns.jointplot(x='DPF',y=col[i],data=df,kind = 'regression',color = 'red')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\n<font color='blue'>DPF shows positive weak linear association with other variable in given dataset.</font> That means On increasing DPF in patients, Other variables will also increase."},{"metadata":{},"cell_type":"markdown","source":"# Outcome\n\nLet us see, how data is behaving with Target variable using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df.iloc[:,:-1].values\ny= df.iloc[:,-1].values\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(x)\n\nx_new = pca.transform(x)\n\nxs = x[:,0]\nys = x[:,1]\n\nplt.figure(dpi=100)\nsns.scatterplot(x=xs,y=ys,hue=y).set_title('Dependency of Data with Outcome')\nplt.xlabel('PCA Feature 1')\nplt.ylabel('PCA Feature 2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Fitting a linea model to this data will not lead to better accuracy because data points are not linearly seperable, May be in higher dimension we can some more details in my 4th Notebook. For now fitting a tree based model or neural network will help us to achieve more accuracy**\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}