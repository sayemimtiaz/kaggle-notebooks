{"cells":[{"metadata":{"_uuid":"eb8d8ef88ed1299b5780dac27afd87e4a56dc253"},"cell_type":"markdown","source":"# ML for Insights Challenge\nThis is day 2 of the ML for Insights challenge.\n\nThe earlier parts of the challenge are:\n0. Use Cases for ML Insights: **[Tutorial](https://www.kaggle.com/dansbecker/use-cases-for-model-insights)**\n1. Permutation Importance: **[Tutorial](https://www.kaggle.com/dansbecker/permutation-importance) - [Exercise](https://www.kaggle.com/dansbecker/exercise-permutation-importance)**\n\n\n# Partial Dependence Plots\n\nWhile feature importance shows what variables most affect predictions, partial dependence plots show *how* a feature affects predictions.\n\nThis is useful to answer questions like:\n\n* Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?\n\n* Are predicted health differences between two groups due to differences in their diets, or due to some other factor?\n\nIf you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models.  Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models.  If you aren't familiar with linear or logistic regressions, don't worry about this comparison.\n\nWe will show a couple examples, explain the interpretation of these plots, and then review the code to create these plots.\n\n# How it Works\n\nLike permutation importance, **partial dependence plots are calculated after a model has been fit.**  The model is fit on real data that has not been artificially manipulated in any way.  \n\nIn our soccer example, teams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features.\n\nTo see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\n\nWe will use the fitted model to predict our outcome (probability their player won \"man of the game\"). But we **repeatedly alter the value for one variable** to make a series of predictions.  We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time.  Then predict again for 60%.  And so on.  We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n\nIn this description, we used only a single row of data.  Interactions between features may cause the plot for a single row to be atypical.  So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.  \n\n# Code Example\n\nModel building isn't our focus, so we won't focus on the data exploration or model building code."},{"metadata":{"_uuid":"aaa66f0e0673422029516069082adde4ee680fbf","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dde49ea98b555ecdf1375f82d3ff171225d6b60"},"cell_type":"markdown","source":"For the sake of explanation, our first example uses a Decision Tree which you can see below. In practice, you'll use more sophistated models for real-world applications."},{"metadata":{"_uuid":"d98257005530221d4aa97a2fd391270e999afbb5","trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz\n\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\ngraphviz.Source(tree_graph)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4109fc6e22171dd8c203f5a89aa84f191de4fa5f"},"cell_type":"markdown","source":"As guidance to read the tree:\n- Leaves with children show their splitting criterion on the top\n- The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree."},{"metadata":{"_uuid":"226478c460b640cd8150b28b4ef4fcff56fbb094"},"cell_type":"markdown","source":"\nHere is the code to create the Partial Dependence Plot using the [PDPBox library](https://pdpbox.readthedocs.io/en/latest/)."},{"metadata":{"_uuid":"93159fb78af1de88b520a46f4175901b8097ce71","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c64c176000023c7c8ec29f00e4a6b5882cc8cc3"},"cell_type":"markdown","source":"A few items are worth pointing out as you interpret this plot\n- The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n- A blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \"Player of The Game.\"  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:"},{"metadata":{"_uuid":"5d3940185acece4fbb93d2fd9bb9c1759d5b0a42","trusted":true},"cell_type":"code","source":"feature_to_plot = 'Distance Covered (Kms)'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ee81bb93b6b0cf365d8d60b9cae32887a9d666"},"cell_type":"markdown","source":"This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model."},{"metadata":{"_uuid":"23f294b3a0549f06a2fa29709af8ec129fda88e6","trusted":true},"cell_type":"code","source":"# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\npdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65a105621cdaee09c2cb6f8d49297bce83b46c56"},"cell_type":"markdown","source":"This model thinks you are more likely to win *Player of The Game* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\n# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify what this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n"},{"metadata":{"_uuid":"c926135b7213b6fe1f26cfde9cbdf0dcccd9e3de","trusted":true},"cell_type":"code","source":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f90746ce50447318e47ab7bc6505c3b86b14680d"},"cell_type":"markdown","source":"This graph shows predictions for any combination of Goals Scored and Distance covered. \n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.  If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?"},{"metadata":{"_uuid":"12709467c682035abee0f0fed53047b69a202d08"},"cell_type":"markdown","source":"# Your Turn\n**[Test your understanding](https://www.kaggle.com/kernels/fork/1637380)** on conceptual questions and a short coding challenge.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}