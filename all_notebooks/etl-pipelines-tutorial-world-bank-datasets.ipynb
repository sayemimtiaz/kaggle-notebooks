{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ETL Pipelines Tutorial | World Bank Datasets "},{"metadata":{},"cell_type":"markdown","source":"**ETL stands for Extract, Transform, Load.**\n\nThis lesson uses data from the World Bank. The data comes from two sources:\n\n[World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.\n\n[World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.\n\n#### Outline of this notebook:\n**Extract data from different sources such as:**\n\n- csv files\n- json files\n- APIs\n\n**Transform data**\n- combining data from different sources\n- data cleaning\n- data types\n- parsing dates\n- file encodings\n- missing data\n- duplicate data\n- dummy variables\n- remove outliers\n- scaling features\n- engineering features\n\n**Load**\n- send the transformed data to a database\n\n**ETL Pipeline**\n- code an ETL pipeline\n\nThe end goal is to clean these data sets and bring them together into one table. As you'll see, it's not as easy as one might hope. By the end of the notebook, I'll have written an ETL pipeline to extract, transform, and load this data into a new database.\n\nThe goal of the notebook is to combine these data sets together so that you can run a machine learning model predicting World Bank Project total costs. \n\nIn the process, you'll need to transform these data sets in different ways. And finally, you'll write a single Python module that reads in these date sets, transforms them, and loads the results into the database all in one step."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing necessary libraries for the project\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract"},{"metadata":{},"cell_type":"markdown","source":">  ##  Extracting data from a csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the projects_data.csv file using the pandas library\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a dtype warning, Read about what this warning is in the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.errors.DtypeWarning.html).\n\nPandas tries to figure out programatically the data type of each column (integer, float, boolean, string). In this case, pandas could not automatically figure out the data type. That is because some columns have more than one possible data types. In other words, this data is messy.\n\nYou can use the dtype option to specify the data type of each column. Because there are so many columns in this data set, you can set all columns to be strings at least for now.\n\nTry reading in the data set again using the read_csv() method. This time, also use the option dtype=str so that pandas treats everything like a string."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read in the projects_data.csv file using the read_csv method and dtype = str option\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv',dtype='str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the cell below to see what the data looks like\ndf_projects.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the number of null values in the data set\ndf_projects.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the number 18248 shows up multiple times. There is also a countryname column with 0 missing values and a Country column with 14045 missing values. This data set clearly has some issues that will need to be solved in the transform part of the pipeline.\n\nNext, output the shape of the data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  output the shape of the data frame\ndf_projects.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, use the pandas read_csv method to read in the population_data.csv file. When you try to read in this data set using pandas, you'll get an error because there is something wrong with the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the population_data.csv file using the read_csv() method\n# Put the results in a variable called df_population\ndf_population = pd.read_csv('../input/world-bank-datasets/population_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What might have happened? Try printing out the first few lines of the data file to see what the issue might be."},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"../input/world-bank-datasets/population_data.csv\")\nfor i in range(10):\n    line = f.readline()\n    print('line: ', i, line)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first four lines in the file are not properly formatted and don't contain data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in population data skipping first four rows\ndf_population = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_population.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of null values in each column\ndf_population.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like every year column has at least one NaN value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum the null values by column\ndf_population.isnull().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And it looks like almost every row has only one null value. That is probably from the 'Unnamed: 62' column that doesn't have any relevant information in it. Drop the 'Unnamed: 62' column from the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_population = df_population.drop('Unnamed: 62', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code outputs any row that contains a null value\n# The purpose is to see what rows contain null values now that \n#   'Unnamed: 62' was dropped from the data.\ndf_population[df_population.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Extract from JSON and XML"},{"metadata":{},"cell_type":"markdown","source":"I'll extract the same population data, except the data will be in a different format.\n\nBoth JSON and XML are common formats for storing data. XML was established before JSON, and JSON has become more popular over time. They both tend to be used for sending data via web APIs, which I'll learn about later in the lesson.\n\nSometimes, you can obtain the same data in either JSON or XML format. The World Bank indicator data is available in either form\n\nFirst, you'll practice extracting data from a JSON file."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_lines(n, file_name):\n    f = open(file_name)\n    for i in range(n):\n        print(f.readline())\n    f.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_lines(1, '../input/world-bank-datasets/population_data.json')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first \"line\" in the file is actually the entire file. JSON is a compact way of representing data in a dictionary-like format. Luckily, pandas has a method to [read in a json file](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) and parse the results for you. \n\nIf you open the link with the documentation, you'll see there is an *orient* option that can handle JSON formatted in different ways:\n```\n'split' : dict like {index -> [index], columns -> [columns], data -> [values]}\n\n'records' : list like [{column -> value}, ... , {column -> value}]\n\n'index' : dict like {index -> {column -> value}}\n\n'columns' : dict like {column -> {index -> value}}\n\n'values' : just the values array\n```\n\nIn this case, the JSON is formatted with a 'records' orientation, so you'll need to use that value in the read_json() method. You can tell that the format is 'records' by comparing the pattern in the documentation with the pattern in the JSON file.\n\nNext, read in the population_data.json file using pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the population_data.json file using pandas's \n# read_json method. Don't forget to specific the orient option\n# store the results in df_json\n\ndf_json = pd.read_json('../input/world-bank-datasets/population_data.json',orient='records')\ndf_json.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other Ways to Read in JSON\n\nBesides using pandas to read JSON files, you can use the json library. Python treats JSON data like a dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\n# read in the JSON file\n\nwith open('../input/world-bank-datasets/population_data.json') as f:\n    json_data = json.load(f)\n\n# read the first record in the JSON file\nprint(json_data[0])\nprint('\\n')\n\n# show that JSON data is essentially a dictionary\nprint(json_data[0]['Country Name'])\nprint(json_data[0]['Country Code'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Extract XML"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_lines(15, '../input/world-bank-datasets/population_data.xml')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XML looks very similar to HTML. XML is formatted with tags having values inside the tags. XML is not as easy to navigate as JSON. Pandas cannot read in XML directly. One reason is that tag names are user defined. Every XML file might have different formatting. You can imagine why XML has fallen out of favor relative to JSON.\n\n### How to read and navigate XML\n\nThere is a Python library called BeautifulSoup, which makes reading in and parsing XML data easier. Here is the link to the documentation: [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/)\n\nThe find() method will find the first place where an xml element occurs. For example using find('record') will return the first record in the xml file:\n\n```xml\n<record>\n  <field name=\"Country or Area\" key=\"ABW\">Aruba</field>\n  <field name=\"Item\" key=\"SP.POP.TOTL\">Population, total</field>\n  <field name=\"Year\">1960</field>\n  <field name=\"Value\">54211</field>\n</record>\n```\n\nThe find_all() method returns all of the matching tags. So find_all('record') would return all of the elements with the `<record>` tag.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bs4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the BeautifulSoup library\nfrom bs4 import BeautifulSoup\n\n# open the population_data.xml file and load into Beautiful Soup\nwith open(\"../input/world-bank-datasets/population_data.xml\") as fp:\n    soup = BeautifulSoup(fp,\"lxml\") #lxml is the parser type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output the first 5 records in the xml file\n# this is an example of how to navigate the XML document with BeautifulSoup\n\ni = 0\n# use the find_all method to get all record tags in the document\nfor record in soup.find_all('record'):\n    # use the find_all method to get all fields in each record\n    i += 1\n    for record in record.find_all('field'):\n        print(record['name'], ': ' , record.text)\n    print()\n    if i == 5:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Extract Data from SQL Databases\n\n### Pandas and sqlite3\n\nYou can use Pandas to open a SQL database or to run a SQL query against a database. There is more than one way to do this depending on the type of SQL database you are working with: the [sqlite3 library](https://www.sqlite.org/about.html) or the [sqlalchemy library](https://www.sqlalchemy.org/).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n\n#connection to the database \nconn = sqlite3.connect('../input/world-bank-datasets/population_data.db')\n\n#run a query\npd.read_sql('SELECT * FROM population_data', conn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_sql('SELECT \"Country_Name\", \"Country_Code\", \"1960\" FROM population_data', conn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SQLAlchemy and Pandas\n\nIf you are working with a different type of database such as MySQL or PostgreSQL, you can use the SQLAlchemy library with pandas. Here are the instructions for connecting to [different types of databases using SQLAlchemy](http://docs.sqlalchemy.org/en/latest/core/engines.html).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sqlalchemy import create_engine\n\n### \n# create a database engine \n# to find the correct file path, use the python os library:\n# import os\n# print(os.getcwd())\n#\n###\n\n# engine = create_engine('sqlite:////home/workspace/3_sql_exercise/population_data.db')\n# pd.read_sql(\"SELECT * FROM population_data\", engine)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## APIs \n\n### Example Indicators API\n\nRun the code example below to request data from the World Bank Indicators API. According to the documntation, you format your request url like so:\n\n`http://api.worldbank.org/v2/countries/` + list of country abbreviations separated by ; + `/indicators/` + indicator name + `?` + options\n\nwhere options can include\n* per_page - number of records to return per page\n* page - which page to return - eg if there are 5000 records and 100 records per page\n* date - filter by dates\n* format - json or xml\n \n and a few other options that you can read about [here](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structure)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nimport pandas as pd\n\nurl = 'http://api.worldbank.org/v2/countries/br;cn;us;de/indicators/SP.POP.TOTL/?format=json&per_page=1000'\nr = requests.get(url)\nr.json()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This json data isn't quite ready for a pandas data frame. Notice that the json response is a list with two entries. The first entry is \n```\n{'lastupdated': '2018-06-28',\n  'page': 1,\n  'pages': 1,\n  'per_page': 1000,\n  'total': 232}\n```\n\nThat first entry is meta data about the results. For example, it says that there is one page returned with 232 results. \n\nThe second entry is another list containing the data. This data would need some cleaning to be used in a pandas data frame. That would happen later in the transformation step of an ETL pipeline. Run the cell below to read the results into a dataframe and see what happens."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(r.json()[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some issues with this dataframe. The country and indicator variables don't look particularly useful in their current form. Again, dealing with those issues would come in the transformation phase of a pipeline.\n\nUse the Indicators API to request rural population data for Switzerland in the years 1995 through 2001. Here are a few helpful resources:\n* [documentation included how to filter by year](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structure)\n* [2-character iso country codes](https://www.nationsonline.org/oneworld/country_code_list.htm)\n* [search box for World Bank indicators](https://data.worldbank.org)\n\nTo find the indicator code, first search for the indicator here: https://data.worldbank.org\nClick on the indicator name. The indicator code is in the url. For example, the indicator code for total population is SP.POP.TOTL, which you can see in the link [https://data.worldbank.org/indicator/SP.RUR.TOTL](https://data.worldbank.org/indicator/SP.RUR.TOTL)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: get the url ready\nurl = 'http://api.worldbank.org/v2/country/CH/indicator/SP.POP.TOTL/?format=json&date=1995:2001'\n\n# TODO: send the request\nr = requests.get(url)\nr.json()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform"},{"metadata":{},"cell_type":"markdown","source":"> ## Combining Data\n\nThere are two csv files:\n* rural_population_percent.csv\n* electricity_access_percent.csv\n\nThey both come from the World Bank Indicators data. \n* https://data.worldbank.org/indicator/SP.RUR.TOTL.ZS\n* https://data.worldbank.org/indicator/EG.ELC.ACCS.ZS\n\nThe rural populaton data represents the percent of a country's population that is rural over time. The electricity access data shows the percentage of people with access to electricity.\n\nI will combine these two data sets together into one pandas data frame."},{"metadata":{},"cell_type":"markdown","source":"Combine the two data sets using the [pandas concat method](https://pandas.pydata.org/pandas-docs/stable/merging.html). In other words, find the union of the two data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"../input/world-bank-datasets/rural_population_percent.csv\")\nfor i in range(10):\n    line = f.readline()\n    print('line: ', i, line)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rural = pd.read_csv('../input/world-bank-datasets/rural_population_percent.csv',skiprows=4)\ndf_rural.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"../input/world-bank-datasets/electricity_access_percent.csv\")\nfor i in range(10):\n    line = f.readline()\n    print('line: ', i, line)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_electricity = pd.read_csv('../input/world-bank-datasets/electricity_access_percent.csv',skiprows=4)\ndf_electricity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove the `Unnamed:62` column from each data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rural.drop(['Unnamed: 62'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_electricity.drop(['Unnamed: 62'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rural.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df_rural, df_electricity])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Cleaning Data\n\nEvery data set might have its own issues whether that involves missing values, duplicated entries, data entry mistakes, etc. In this notebook, I'll do some data cleaning on the World Bank projects and World Bank indicators data sets.\n\nCurrently, the projects data and the indicators data have different values for country names. My task in this notebook is to clean both data sets so that they have consistent country names. This will allow you to join the two data sets together. Cleaning data, unfortunately, can be tedious and take a lot of your time as a data scientist.\n\nWhy might you want to join these data sets together? What if, for example, you wanted to run linear regression to try to predict project costs based on indicator data? Or you might want to analyze the types of projects that get approved versus the indicator data. For example, do countries with low rates of rural electrification have more rural themed projects?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_indicator = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\ndf_indicator.drop(['Unnamed: 62'], axis=1, inplace=True)\n\n# read in the projects data set with all columns type string\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\ndf_projects.drop(['Unnamed: 56'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next code cell outputs the unique country names and ISO abbreviations in the population indicator data set. You'll notice a few values that represent world regions such as 'East Asia & Pacific' and 'East Asia & Pacific (excluding high income)'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_indicator[['Country Name', 'Country Code']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to see the unique country names in the project data set. Notice that the projects data has two columns for country name. One is called 'countryname' and the other is called 'Country'. The 'Country' column only has NaN values.\n\nAnother thing of note: It would've been easier to join the two data sets together if the projects data had the [ISO country abbreviations](https://en.wikipedia.org/wiki/ISO_3166-1) like the indicator data has. Unfortunately, the projects data does not have the ISO country abbreviations. To join these two data sets together, you essentially have two choices:\n* add a column of ISO 3 codes to the projects data set\n* find the difference between the projects data country names and indicator data country names. Then clean the data so that they are the same.\n\nRun the code cell below to see what the project countries look like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects['countryname'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did you notice a pattern in the projects data country names? The entries are repeated and separated by a semi-colon like this:\n```text\n'Kingdom of Spain;Kingdom of Spain'\n'New Zealand;New Zealand'\n```\n\nThe first step is to clean the country name column and get rid of the semi-colon."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects['Official Country Name'] = df_projects['countryname'].str.split(';').str.get(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install and import the pycountry library\n!pip install pycountry\nfrom pycountry import countries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code cell to see an example of how the library works\ncountries.get(name='Spain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code cell to see how you can also look up countries without specifying the key\ncountries.lookup('Kingdom of Spain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal is to add the ISO codes to the projects data set. To start, use the pycountry library to make a dictionary mapping the unique countries in 'Official Country Name' to the ISO code.\n\nIterate through the unique countries in df_projects['Official Country Name']. Create a dictionary mapping the 'Country Name' to the alpha_3 ISO abbreviations. \n\nThe dictionary should look like:\n`{'Kingdom of Spain':'ESP'}`\n\nIf a country name cannot be found in the pycountry library, add it to a list called `country_not_found`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up the libraries and variables\nfrom collections import defaultdict\ncountry_not_found = [] # stores countries not found in the pycountry library\nproject_country_abbrev_dict = defaultdict(str) # set up an empty dictionary of string values\n\n# iterate through the country names in df_projects. \n# Create a dictionary mapping the country name to the alpha_3 ISO code\nfor country in df_projects['Official Country Name'].drop_duplicates().sort_values():\n    try: \n        # look up the country name in the pycountry library\n        # store the country name as the dictionary key and the ISO-3 code as the value\n        project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n    except:\n        # If the country name is not in the pycountry library, then print out the country name\n        # And store the results in the country_not_found list\n        print(country, ' not found')\n        country_not_found.append(country)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite a few country names were not in the pycountry library. Some of these are regions like \"South Asia\" or \"Southern Africa\", so it makes sense that these would not show up in the pycountry library.\n\nPart 3 - Making a Manual Mapping\n\nPerhaps some of these missing df_projects countries are already in the indicators data set. In the next cell, check if any of the countries in the country_not_found list are in the indicator list of countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this cell to iterate through the country_not_found list and check if the country name is in the df_indicator data set\nindicator_countries = df_indicator[['Country Name', 'Country Code']].drop_duplicates().sort_values(by='Country Name')\n\nfor country in country_not_found:\n    if country in indicator_countries['Country Name'].tolist():\n        print(country)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, there aren't too many country names that match between df_indicator and df_projects. This is where data cleaning becomes especially tedious, but in this case, we've done a lot of the work for you.\n\nWe've manually created a dictionary that maps all of the countries in country_not_found to the ISO-3 alpha codes. You **could** try to do this programatically using some sophisticated string matching rules. That might be worth your time for a larger data set. But in this case, it's probably faster to type out the dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"country_not_found_mapping = {'Co-operative Republic of Guyana': 'GUY',\n             'Commonwealth of Australia':'AUS',\n             'Democratic Republic of Sao Tome and Prin':'STP',\n             'Democratic Republic of the Congo':'COD',\n             'Democratic Socialist Republic of Sri Lan':'LKA',\n             'East Asia and Pacific':'EAS',\n             'Europe and Central Asia': 'ECS',\n             'Islamic  Republic of Afghanistan':'AFG',\n             'Latin America':'LCN',\n              'Caribbean':'LCN',\n             'Macedonia':'MKD',\n             'Middle East and North Africa':'MEA',\n             'Oriental Republic of Uruguay':'URY',\n             'Republic of Congo':'COG',\n             \"Republic of Cote d'Ivoire\":'CIV',\n             'Republic of Korea':'KOR',\n             'Republic of Niger':'NER',\n             'Republic of Kosovo':'XKX',\n             'Republic of Rwanda':'RWA',\n              'Republic of The Gambia':'GMB',\n              'Republic of Togo':'TGO',\n              'Republic of the Union of Myanmar':'MMR',\n              'Republica Bolivariana de Venezuela':'VEN',\n              'Sint Maarten':'SXM',\n              \"Socialist People's Libyan Arab Jamahiriy\":'LBY',\n              'Socialist Republic of Vietnam':'VNM',\n              'Somali Democratic Republic':'SOM',\n              'South Asia':'SAS',\n              'St. Kitts and Nevis':'KNA',\n              'St. Lucia':'LCA',\n              'St. Vincent and the Grenadines':'VCT',\n              'State of Eritrea':'ERI',\n              'The Independent State of Papua New Guine':'PNG',\n              'West Bank and Gaza':'PSE',\n              'World':'WLD'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, update the project_country_abbrev_dict variable with these new values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update the project_country_abbrev_dict with the country_not_found_mapping dictionary\n# HINT: This is relatively straightforward. Python dictionaries have a method called update(), which essentially\n# appends a dictionary to another dictionary\n\nproject_country_abbrev_dict.update(country_not_found_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Part 5 - Make a 'Country Code' Column\n\nNext, create a 'Country Code' column in the data_projects data frame. Use the project_country_abbrev_dict and df_projects['Country Name'] column to create a new columns called 'Country 'Code'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the project_country_abbrev_dict and the df_projects['Country Name'] column to make a new column\n# of the alpha-3 country codes. This new column should be called 'Country Code'.\n\n# HINT: Use the apply method and a lambda function\n# HINT: The lambda function will use the project_country_abbrev_dict that maps the country name to the ISO code\n# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\n\ndf_projects['Country Code'] = df_projects['Official Country Name'].apply(lambda x: project_country_abbrev_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code cell to see which projects in the df_projects data frame still have no country code abbreviation.\n# In other words, these projects do not have a matching population value in the df_indicator data frame.\ndf_projects[df_projects['Country Code'] == '']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the df_projects dataframe and the df_indicator dataframe have a matching column called 'Country Code'. But these two data frames can't be merged quite yet. \n\nEach project in the df_projects dataframe also has a date associated with it. The idea would be to merge the df_projects dataframe with the df_indicator dataframe so that each project also had a population value associated with it. There are still more data transformations to do in order for that to be possible. "},{"metadata":{},"cell_type":"markdown","source":">## Data Types\n\nWhen reading in a data set, pandas will try to guess the data type of each column like float, integer, datettime, bool, etc. In Pandas, strings are called \"object\" dtypes. \n\nHowever, Pandas does not always get this right. That was the issue with the World Bank projects data. Hence, the dtype was specified as a string:\n```\ndf_projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n```\n\nRun the code cells below to read in the indicator and projects data. Then run the following code cell to see the dtypes of the indicator data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# read in the population data and drop the final column\ndf_indicator = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\ndf_indicator.drop(['Unnamed: 62'], axis=1, inplace=True)\n\n# read in the projects data set with all columns type string\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\ndf_projects.drop(['Unnamed: 56'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_indicator.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results look reasonable. Country Name, Country Code, Indicator Name and Indicator Code were all read in as strings. The year columns, which contain the population data, were read in as floats.\n\nSince the population indicator data was read in correctly, you can run calculations on the data. In this first exercise, sum the populations of the United States, Canada, and Mexico by year."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the population sum by year for Canada,\n#       the United States, and Mexico.\n\n# the keepcol variable makes a list of the column names to keep. You can use this if you'd like\nkeepcol = ['Country Name']\nfor i in range(1960, 2018, 1):\n    keepcol.append(str(i))\n\n# In the df_nafta variable, store a data frame that only contains the rows for \n#      Canada, United States, and Mexico.\ndf_nafta = df_indicator[(df_indicator['Country Name'] == 'Canada') | \n             (df_indicator['Country Name'] == 'United States') | \n            (df_indicator['Country Name'] == 'Mexico')].iloc[:,]\n\n\n# Calculate the sum of the values in each column in order to find the total population by year.\n# You can use the keepcol variable if you want to control which columns get outputted\ndf_nafta.sum(axis=0)[keepcol]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, run the code cell below to look at the dtypes for the projects data set. They should all be \"object\" types, ie strings, because that's what was specified in the code when reading in the csv file. As a reminder, this was the code:\n```\ndf_projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many of these columns should be strings, so there's no problem; however, a few columns should be other data types. For example, `boardapprovaldate` should be a datettime and `totalamt` should be an integer. You'll learn about datetime formatting in the next part of the lesson. For this exercise, focus on the 'totalamt' and 'lendprojectcost' columns. Run the code cell below to see what that data looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects[['totalamt', 'lendprojectcost']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects['totalamt'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What just happened? Pandas treated the totalamts like strings. In Python, adding strings concatenates the strings together.\n\nThere are a few ways to remedy this. When using pd.read_csv(), you could specify the column type for every column in the data set. The pd.read_csv() dtype option can accept a dictionary mapping each column name to its data type. You could also specify the `thousands` option with `thousands=','`. This specifies that thousands are separated by a comma in this data set. \n\nHowever, this data is somewhat messy, contains missing values, and has a lot of columns. It might be faster to read in the entire data set with string types and then convert individual columns as needed. For this next exercise, convert the `totalamt` column from a string to an integer type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the totalamt column from a string to a float and save the results back into the totalamt column\n\n# Step 1: Remove the commas from the 'totalamt' column\n# HINT: https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.str.replace.html\n\n# Step 2: Convert the 'totalamt' column from an object data type (ie string) to an integer data type.\n# HINT: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.to_numeric.html\n\ndf_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With messy data, you might find it easier to read in everything as a string; however, you'll sometimes have to convert those strings to more appropriate data types. When you output the dtypes of a dataframe, you'll generally see these values in the results:\n* float64\n* int64\n* bool\n* datetime64\n* timedelta\n* object\n\nwhere timedelta is the difference between two datetimes and object is a string. As you've seen here, you sometimes need to convert data types from one type to another type. Pandas has a few different methods for converting between data types, and here are link to the documentation:\n\n* [astype](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.astype.html#pandas.DataFrame.astype)\n* [to_datetime](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_datetime.html#pandas.to_datetime)\n* [to_numeric](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_numeric.html#pandas.to_numeric)\n* [to_timedelta](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_timedelta.html#pandas.to_timedelta)"},{"metadata":{},"cell_type":"markdown","source":"> ## Parsing Dates\n\nAnother common data transformation involves parsing dates. Parsing generally means that you start with a string and then transform that string into a different data type. In this case, that means taking a date in the format of a string and transforming the string into a date type. Run the next cell to see an example."},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date = pd.to_datetime('January 1st, 2017')\nparsed_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date.second","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sometimes date string are formatted in unexpected ways. For example, in the United States, dates are given with the month first and then the day. That is what pandas expects by default. However, some countries write the date with the day first and then the month. Run the next three examples to see Panda's default behavior and how you can specify the date formatting."},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date = pd.to_datetime('5/3/2017 5:30')\nparsed_date.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date = pd.to_datetime('3/5/2017 5:30', format='%d/%m/%Y %H:%M')\nparsed_date.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_date = pd.to_datetime('5/3/2017 5:30', format='%m/%d/%Y %H:%M')\nparsed_date.month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The formatting abbreviations are actually part of the python standard. You can see examples at [this link](http://strftime.org/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the projects data set with all columns type string\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\ndf_projects.drop(['Unnamed: 56'], axis=1, inplace=True)\ndf_projects.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice there are three columns associated with dates: boardapprovaldate, board_approval_month, and closingdate. Run the code cell below to see what these values look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_projects.head(15)[['boardapprovaldate', 'board_approval_month', 'closingdate']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the pandas to_datetime method to convert the boardapprovaldate and closingdate columns into datetime objects."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the pandas to_datetime method to convert these two columns \n#   (boardapprovaldate, closingdate) into date times.\n# HINT: It's easier to do this one column at a time\n\ndf_projects['boardapprovaldate'] = pd.to_datetime(df_projects['boardapprovaldate'])\ndf_projects['closingdate'] = pd.to_datetime(df_projects['closingdate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the code cells below to see how you can access the different parts of the datetime objects\n# Series.dt gives access to the datetime object as explained here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.html\ndf_projects['boardapprovaldate'].dt.second","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code cell to see the output\ndf_projects['boardapprovaldate'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code to see the output\n# weekday represents the day of the week from 0 (Monday) to 6 (Sunday).\ndf_projects['boardapprovaldate'].dt.weekday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Part 2 - Create new columns\n\nNow that the boardapprovaldate and closingdates are in datetime formats, create a few new columns in the df_projects data frame:\n* approvalyear\n* approvalday\n* approvalweekday\n* closingyear\n* closingday\n* closingweekday"},{"metadata":{"trusted":true},"cell_type":"code","source":"### \n# create the follwing new columns in the df_projects data frame\n#\n# approvalyear\n# approvalday\n# approvalweekday\n# closingyear\n# closingday\n# closingweekday\n#\n#\n###\n\ndf_projects['approvalyear'] = df_projects['boardapprovaldate'].dt.year\ndf_projects['approvalday'] = df_projects['boardapprovaldate'].dt.day\ndf_projects['approvalweekday'] = df_projects['boardapprovaldate'].dt.weekday\ndf_projects['closingyear'] = df_projects['closingdate'].dt.year\ndf_projects['closingday'] = df_projects['closingdate'].dt.day\ndf_projects['closingweekday'] = df_projects['closingdate'].dt.weekday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Encodings\n\nEncodings are a set of rules mapping string characters to their binary representations. Python supports dozens of different encoding as seen here in [this link](https://docs.python.org/3/library/codecs.html#standard-encodings). Because the web was originally in English, the first encoding rules mapped binary code to the English alphabet. \n\nThe English alphabet has only 26 letters. But other languages have many more characters including accents, tildes and umlauts. As time went on, more encodings were invented to deal with languages other than English. The utf-8 standard tries to provide a single encoding schema that can encompass all text.\n\nThe problem is that it's difficult to know what encoding rules were used to make a file unless somebody tells you. The most common encoding by far is utf-8. Pandas will assume that files are utf-8 when you read them in or write them out.\n\nRun the code cell below to read in the population data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas should have been able to read in this data set without any issues. Next, run the code cell below to read in the 'mystery.csv' file."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/world-bank-datasets/mystery.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should have gotten an error: **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte**. This means pandas assumed the file had a utf-8 encoding but had trouble reading in the data file."},{"metadata":{"trusted":true},"cell_type":"code","source":"from encodings.aliases import aliases\n\nalias_values = set(aliases.values())\n\nfor encoding in set(aliases.values()):\n    try:\n        df=pd.read_csv(\"mystery.csv\", encoding=encoding)\n        print('successful', encoding)\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are dozens of encodings that Python can handle; however, Pandas assumes a utf-8 encoding. This makes sense since utf-8 is very common. However, you will sometimes come across files with other encodings. If you don't know the encoding, you have to search for it.\n\nThere is a Python library that can be of some help when you don't know an encoding: chardet. Run the code cells below to see how it works."},{"metadata":{"trusted":true},"cell_type":"code","source":"# install the chardet library\n!pip install chardet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the chardet library\nimport chardet \n\n# use the detect method to find the encoding\n# 'rb' means read in the file as binary\nwith open(\"../input/world-bank-datasets/mystery.csv\", 'rb') as file:\n    print(chardet.detect(file.read()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a machine learning algorithm won't work with missing values. This is essentially correct; however, there are a couple of situations where this isn't quite true. For example, if you had a categorical variable, you could keep the NULL value as one of the options."},{"metadata":{},"cell_type":"markdown","source":"> ## Imputing Data\n\nWhen a dataset has missing values, you can either remove those values or fill them in."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ndf.drop('Unnamed: 62', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run this code cell to see what the data looks like\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this code cell to check how many null values are in the data set\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are quite a few null values. Run the code below to plot the data for a few countries in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# put the data set into long form instead of wide\ndf_melt = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'], var_name='year', value_name='GDP')\n\n# convert year to a date time\ndf_melt['year'] = pd.to_datetime(df_melt['year'])\n\ndef plot_results(column_name):\n    # plot the results for Afghanistan, Albania, and Honduras\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    df_melt[(df_melt['Country Name'] == 'Afghanistan') | \n            (df_melt['Country Name'] == 'Albania') | \n            (df_melt['Country Name'] == 'Honduras')].groupby('Country Name').plot('year', column_name, legend=True, ax=ax)\n    ax.legend(labels=['Afghanistan', 'Albania', 'Honduras'])\n    \nplot_results('GDP')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afghanistan and Albania are missing data, which show up as gaps in the results. \n"},{"metadata":{},"cell_type":"markdown","source":" Exercise - Part 1\n\nYour first task is to calculate mean GDP for each country and fill in missing values with the country mean. This is a bit tricky to do in pandas. Here are a few links that should be helpful:\n* https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.groupby.html\n* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html\n* https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.fillna.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Use the df_melt dataframe and fill in missing values with a country's mean GDP\n# If you aren't sure how to do this, \n# look up something like \"how to group data and fill in nan values in pandas\" in a search engine\n# Put the results in a new column called 'GDP_filled'.\n\n# HINT: You can do this with these methods: groupby(), transform(), a lambda function, fillna(), and mean()\n\ndf_melt['GDP_filled'] = df_melt.groupby('Country Name')['GDP'].transform(lambda x: x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the results\nplot_results('GDP_filled')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is somewhat of an improvement. At least there is no missing data; however, because GDP tends to increase over time, the mean GDP is probably not the best way to fill in missing values for this particular case. Next, try using forward fill to deal with any missing values."},{"metadata":{},"cell_type":"markdown","source":"Excercise - Part 2\n\nUse the fillna forward fill method to fill in the missing data. Here is the [documentation](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.fillna.html).\n\nThe pandas fillna method has a forward fill option. For example, if you wanted to use forward fill on the GDP dataset, you could execute `df_melt['GDP'].fillna(method='ffill')`. However, there are two issues with that code. \n1. You want to first make sure the data is sorted by year\n2. You need to group the data by country name so that the forward fill stays within each country\n\nWrite code to first sort the df_melt dataframe by year, then group by 'Country Name', and finally use the forward fill method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use forward fill to fill in missing GDP values\n# HINTS: use the sort_values(), groupby(), and fillna() methods\n\ndf_melt['GDP_ffill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the results\nplot_results('GDP_ffill')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks better at least for the Afghanistan data; however, the Albania data is still missing values. You can fill in the Albania data using back fill. That is what you'll do next."},{"metadata":{},"cell_type":"markdown","source":"Exercise - Part 3\n\nThis part is similar to Part 2, but now you will use backfill. Write code that backfills the missing GDP data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use back fill to fill in missing GDP values\n# HINTS: use the sort_values(), groupby(), and fillna() methods\n\ndf_melt['GDP_bfill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('GDP_bfill')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, the GDP data for all three countries is now complete. Note that forward fill did not fill all the Albania data because the first data entry in 1960 was NaN. Forward fill would try to fill the 1961 value with the NaN value from 1960.\n\nTo completely fill the entire GDP data for all countries, you might have to run both forward fill and back fill. Note as well that the results will be slightly different depending on if you run forward fill first or back fill first. Afghanistan, for example, is missing data in the middle of the data set. Hence forward fill and back fill will have slightly different results.\n\nRun this next code cell to see if running both forward fill and back fill end up filling all the GDP NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run forward fill and backward fill on the GDP data\ndf_melt['GDP_ff_bf'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='ffill').fillna(method='bfill')\n\n# Check if any GDP values are null\ndf_melt['GDP_ff_bf'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Duplicate Data\n\nA data set might have duplicate data: in other words, the same record is represented multiple times. Sometimes, it's easy to find and eliminate duplicate data like when two records are exactly the same. At other times, like what was discussed in the video, duplicate data is hard to spot. \n\nFrom the World Bank GDP data, count the number of countries that have had a project totalamt greater than 1 billion dollars (1,000,000,000). To get the count, you'll have to remove duplicate data rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\nprojects.drop('Unnamed: 56', axis=1, inplace=True)\nprojects['totalamt'] = pd.to_numeric(projects['totalamt'].str.replace(',', ''))\nprojects['countryname'] = projects['countryname'].str.split(';', expand=True)[0]\nprojects['boardapprovaldate'] = pd.to_datetime(projects['boardapprovaldate'])\n\n# filter the data frame for projects over 1 billion dollars\n\n# count the number of unique countries in the results\n\nprojects[projects['totalamt'] > 1000000000]['countryname'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Dummy Variables\n\nIn this exercise, I'll create dummy variables from the projects data set. The idea is to transform categorical data like this:\n\n| Project ID | Project Category |\n|------------|------------------|\n| 0          | Energy           |\n| 1          | Transportation   |\n| 2          | Health           |\n| 3          | Employment       |\n\ninto new features that look like this:\n\n| Project ID | Energy | Transportation | Health | Employment |\n|------------|--------|----------------|--------|------------|\n| 0          | 1      | 0              | 0      | 0          |\n| 1          | 0      | 1              | 0      | 0          |\n| 2          | 0      | 0              | 1      | 0          |\n| 3          | 0      | 0              | 0      | 1          |\n\n\n(Note if you were going to use this data with a model influenced by multicollinearity, you would want to eliminate one of the columns to avoid redundant information.) \n\nThe reasoning behind these transformations is that machine learning algorithms read in numbers not text. Text needs to be converted into numbers. You could assign a number to each category like 1, 2, 3, and 4. But a categorical variable has no inherent order, so you want to reflect this in your features.\n\nPandas makes it very easy to create dummy variables with the [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) method. In this exercise, you'll create dummy variables from the World Bank projects data; however, there's a caveat. The World Bank data is not particularly clean, so you'll need to explore and wrangle the data first.\n\nYou'll focus on the text values in the sector variables.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\nprojects.drop('Unnamed: 56', axis=1, inplace=True)\nprojects['totalamt'] = pd.to_numeric(projects['totalamt'].str.replace(',', ''))\nprojects['countryname'] = projects['countryname'].str.split(';', expand=True)[0]\nprojects['boardapprovaldate'] = pd.to_datetime(projects['boardapprovaldate'])\n\n# keep the project name, lending, sector and theme data\nsector = projects.copy()\nsector = sector[['project_name', 'lendinginstr', 'sector1', 'sector2', 'sector3', 'sector4', 'sector5', 'sector',\n          'mjsector1', 'mjsector2', 'mjsector3', 'mjsector4', 'mjsector5',\n          'mjsector', 'theme1', 'theme2', 'theme3', 'theme4', 'theme5', 'theme ',\n          'goal', 'financier', 'mjtheme1name', 'mjtheme2name', 'mjtheme3name',\n          'mjtheme4name', 'mjtheme5name']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the code cell below. This cell shows the percentage of each variable that is null. Notice the mjsector1 through mjsector5 variables are all null. The mjtheme1name through mjtheme5name are also all null as well as the theme variable. \n\nBecause these variables contain so many null values, they're probably not very useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"# output percentage of values that are missing\n100 * sector.isnull().sum() / sector.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sector1 variable looks promising; it doesn't contain any null values at all. In the next cell, store the unique sector1 values in a list and output the results. Use the sort_values() and unique() methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of the unique values in sector1. Use the sort_values() and unique() pandas methods. \n# And then convert those results into a Python list\nuniquesectors1 = sector['sector1'].sort_values().unique()\nuniquesectors1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run this code cell to see the number of unique values\nprint('Number of unique values in sector1:', len(uniquesectors1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few issues with this 'sector1' variable. First, there are values labeled '!$!0'. These should be substituted with NaN.\n\nFurthermore, each sector1 value ends with a ten or eleven character string like '!$!49!$!EP'. Some sectors show up twice in the list like:\n 'Other Industry; Trade and Services!$!70!$!YZ',\n 'Other Industry; Trade and Services!$!63!$!YZ',\n\nBut it seems like those are actually the same sector. You'll need to remove everything past the exclamation point. \n\nMany values in the sector1 variable start with the term '(Historic)'. Try removing that phrase as well.\n\n### replace() method\n\nWith pandas, you can use the replace() method to search for text and replace parts of a string with another string. If you know the exact string you're looking for, the replace() method is straight forward. For example, say you wanted to remove the string '(Trial)' from this data:\n\n| data                     |\n|--------------------------|\n| '(Trial) Banking'        |\n| 'Banking'                |\n| 'Farming'                |\n| '(Trial) Transportation' |\n\nYou could use `df['data'].replace('(Trial'), '')` to replace (Trial) with an empty string.\n\n### regular expressions\nWhat about this data?\n\n| data                                           |\n|------------------------------------------------|\n| 'Other Industry; Trade and Services?$ab' |\n| 'Other Industry; Trade and Services?ceg' |\n\nThis type of data is trickier. In this case, there's a pattern where you want to remove a string that starts with an exclamation point and then has an unknown number of characters after it. When you need to match patterns of character, you can use [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).\n\nThe replace method can take a regular expression. So\ndf['data'].replace('?.+', regex=True) where '?.+' means find a set of characters that starts with a question mark is then followed by one or more characters. You can see a [regular expression cheat sheet](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285) here.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: In the sector1 variable, replace the string '!$10' with nan\n# HINT: you can use the pandas replace() method and numpy.nan\nsector['sector1'] = sector['sector1'].replace('!$!0', np.nan)\n\n# TODO: In the sector1 variable, remove the last 10 or 11 characters from the sector1 variable.\n# HINT: There is more than one way to do this including the replace method\n# HINT: You can use a regex expression '!.+'\n# That regex expression looks for a string with an exclamation\n# point followed by one or more characters\n\nsector['sector1'] = sector['sector1'].replace('!.+', '', regex=True)\n\n# TODO: Remove the string '(Historic)' from the sector1 variable\n# HINT: You can use the replace method\nsector['sector1'] = sector['sector1'].replace('^(\\(Historic\\))', '', regex=True)\n\nprint('Number of unique sectors after cleaning:', len(list(sector['sector1'].unique())))\nprint('Percentage of null values after cleaning:', 100 * sector['sector1'].isnull().sum() / sector['sector1'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now there are 156 unique categorical values. That's better than 3060. If you were going to use this data with a supervised learning machine model, you could try converting these 156 values to dummy variables. You'd still have to train and test a model to see if those are good features.\n\nIn this next exercise, use the pandas pd.get_dummies() method to create dummy variables. Then use the concat() method to concatenate the dummy variables to a dataframe that contains the project totalamt variable and the project year from the boardapprovaldate."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.DataFrame(pd.get_dummies(sector['sector1']))\n\n#  Filter the projects data for the totalamt, the year from boardapprovaldate, and the dummy variables\nprojects['year'] = projects['boardapprovaldate'].dt.year\ndf = projects[['totalamt','year']]\ndf_final = pd.concat([df, dummies], axis=1)\n\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Finding Outliers\n\nIn this, I'll practice looking for outliers. I'll look at the World Bank GDP and population data sets. First, you'll look at the data from a one-dimensional perspective and then a two-dimensional perspective.\n\nRun the code below to import the data sets and prepare the data for analysis. The code:\n* reads in the data sets\n* reshapes the datasets to a long format\n* uses back fill and forward fill to fill in missing values\n* merges the gdp and population data together\n* shows the first 10 values in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the projects data set and do basic wrangling \ngdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ngdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\npopulation = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\npopulation.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n\n\n# Reshape the data sets so that they are in long format\ngdp_melt = gdp.melt(id_vars=['Country Name'], \n                    var_name='year', \n                    value_name='gdp')\n\n# Use back fill and forward fill to fill in missing gdp values\ngdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n\npopulation_melt = population.melt(id_vars=['Country Name'], \n                                  var_name='year', \n                                  value_name='population')\n\n# Use back fill and forward fill to fill in missing population values\npopulation_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n\n# merge the population and gdp data together into one data frame\ndf_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n\n# filter data for the year 2016\ndf_2016 = df_country[df_country['year'] == '2016']\n\n# see what the data looks like\ndf_2016.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore the data set to identify outliers using the Tukey rule."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \n\n# Make a boxplot of the population data for the year 2016\ndf_2016.plot('population',kind='box');\n\n# Make a boxplot of the gdp data for the year 2016\ndf_2016.plot('gdp',kind='box');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the Tukey rule to determine what values of the population data are outliers for the year 2016. The Tukey rule finds outliers in one-dimension. The steps are:\n\n* Find the first quartile (ie .25 quantile)\n* Find the third quartile (ie .75 quantile)\n* Calculate the inter-quartile range (Q3 - Q1)\n* Any value that is greater than Q3 + 1.5 * IQR is an outlier\n* Any value that is less than Qe - 1.5 * IQR is an outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"population_2016 = df_2016[['Country Name','population']]\n\n# Calculate the first quartile of the population values for 2016\n# HINT: you can use the pandas quantile method \nQ1 = population_2016['population'].quantile(0.25)\n\n# Calculate the third quartile of the population values for 2016\nQ3 = population_2016['population'].quantile(0.75)\n\n# Calculate the interquartile range Q3 - Q1\nIQR = Q3 - Q1\n\n# Calculate the maximum value and minimum values according to the Tukey rule\n# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\nmax_value = Q3 + 1.5 * IQR\nmin_value = Q1 - 1.5 * IQR\n\n# filter the population_2016 data for population values that are greater than max_value or less than min_value\npopulation_outliers = population_2016[(population_2016['population'] > max_value) | (population_2016['population'] < min_value)]\npopulation_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly many of these outliers are due to regional data getting aggregated together. \n\nRemove these data points and redo the analysis. There's a list provided below of the 'Country Name' values that are not actually countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the rows from the data that have Country Name values in the non_countries list\n# Store the filter results back into the df_2016 variable\n\nnon_countries = ['World',\n 'High income',\n 'OECD members',\n 'Post-demographic dividend',\n 'IDA & IBRD total',\n 'Low & middle income',\n 'Middle income',\n 'IBRD only',\n 'East Asia & Pacific',\n 'Europe & Central Asia',\n 'North America',\n 'Upper middle income',\n 'Late-demographic dividend',\n 'European Union',\n 'East Asia & Pacific (excluding high income)',\n 'East Asia & Pacific (IDA & IBRD countries)',\n 'Euro area',\n 'Early-demographic dividend',\n 'Lower middle income',\n 'Latin America & Caribbean',\n 'Latin America & the Caribbean (IDA & IBRD countries)',\n 'Latin America & Caribbean (excluding high income)',\n 'Europe & Central Asia (IDA & IBRD countries)',\n 'Middle East & North Africa',\n 'Europe & Central Asia (excluding high income)',\n 'South Asia (IDA & IBRD)',\n 'South Asia',\n 'Arab World',\n 'IDA total',\n 'Sub-Saharan Africa',\n 'Sub-Saharan Africa (IDA & IBRD countries)',\n 'Sub-Saharan Africa (excluding high income)',\n 'Middle East & North Africa (excluding high income)',\n 'Middle East & North Africa (IDA & IBRD countries)',\n 'Central Europe and the Baltics',\n 'Pre-demographic dividend',\n 'IDA only',\n 'Least developed countries: UN classification',\n 'IDA blend',\n 'Fragile and conflict affected situations',\n 'Heavily indebted poor countries (HIPC)',\n 'Low income',\n 'Small states',\n 'Other small states',\n 'Not classified',\n 'Caribbean small states',\n 'Pacific island small states']\n\n# remove non countries from the data\ndf_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-rerun the Tukey code with this filtered data to find population outliers\n\n# Filter the data for the year 2016 and put the results in the population_2016 variable. You only need\n# to keep the Country Name and population columns\npopulation_2016 = df_2016[['Country Name','population']]\n\n# Calculate the first quartile of the population values\n# HINT: you can use the pandas quantile method \nQ1 = population_2016['population'].quantile(0.25)\n\n# Calculate the third quartile of the population values\nQ3 = population_2016['population'].quantile(0.75)\n\n# Calculate the interquartile range Q3 - Q1\nIQR = Q3 - Q1\n\n# Calculate the maximum value and minimum values according to the Tukey rule\n# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\nmax_value = Q3 + 1.5 * IQR\nmin_value = Q1 - 1.5 * IQR\n\n# filter the population_2016 data for population values that are greater than max_value or less than min_value\npopulation_outliers = population_2016[(population_2016['population'] > max_value) | (population_2016['population'] < min_value)]\npopulation_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the data for the year 2016 and put the results in the population_2016 variable. You only need\n# to keep the Country Name and population columns\ngdp_2016 = df_2016[['Country Name','gdp']]\n\n# Calculate the first quartile of the population values\n# HINT: you can use the pandas quantile method \nQ1 = gdp_2016['gdp'].quantile(0.25)\n\n# Calculate the third quartile of the population values\nQ3 = gdp_2016['gdp'].quantile(0.75)\n\n# Calculate the interquartile range Q3 - Q1\nIQR = Q3 - Q1\n\n# Calculate the maximum value and minimum values according to the Tukey rule\n# max_value is Q3 + 1.5 * IQR while min_value is Q1 - 1.5 * IQR\nmax_value = Q3 + 1.5 * IQR\nmin_value = Q1 - 1.5 * IQR\n\n# filter the population_2016 data for population values that are greater than max_value or less than min_value\ngdp_outliers = gdp_2016[(gdp_2016['gdp'] > max_value) | (gdp_2016['gdp'] < min_value)]\ngdp_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, write code to determine which countries are in the population_outliers array and in the gdp_outliers array. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find country names that are in both the population_outliers and the gdp_outliers \n# HINT: you can use the pandas intersection() method and python set() and list() methods\n\nlist(set(population_outliers['Country Name']).intersection(gdp_outliers['Country Name']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These countries have both relatively high populations and high GDPs. That might be an indication that although these countries have high values for both gdp and population, they're not true outliers when looking at these values from a two-dimensional perspective.\n\nNow write code to find countries in population_outliers but not in the gdp_outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find country names that are in the population outliers list but not the gdp outliers list\n# HINT: Python's set() and list() methods should be helpful\n\nlist(set(population_outliers['Country Name']) - set(gdp_outliers['Country Name']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These countries are population outliers but not GDP outliers. If looking at outliers from a two-dimensional perspective, there's some indication that these countries might be outliers.\n\nAnd finally, write code to find countries that are in the gdp_outliers array but not the population_outliers array."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find country names that are in the gdp outliers list but not the population outliers list\n# HINT: Python's set() and list() methods should be helpful\n\nlist(set(gdp_outliers['Country Name']) - set(population_outliers['Country Name']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the other hand, these countries have high GDP but are not population outliers.\n\n\n2-Dimensional Analysis\n\nNext, look at the data from a two-dimensional perspective.\n\nThe next code cell plots the GDP vs Population data including the country name of each point."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(df_2016['population'])\ny = list(df_2016['gdp'])\ntext = df_2016['Country Name']\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.scatter(x, y)\nplt.title('GDP vs Population')\nplt.xlabel('population')\nplt.ylabel('GDP')\nfor i, txt in enumerate(text):\n    ax.annotate(txt, (x[i],y[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The United States, China, and India have such larger values that it's hard to see this data. Let's take those countries out for a moment and look at the data again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_large = (df_2016['Country Name'] != 'United States') & (df_2016['Country Name'] != 'India') & (df_2016['Country Name'] != 'China')\nx = list(df_2016[df_no_large]['population'])\ny = list(df_2016[df_no_large]['gdp'])\ntext = df_2016[df_no_large]['Country Name']\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.scatter(x, y)\nplt.title('GDP vs Population')\nplt.xlabel('population')\nplt.ylabel('GDP')\nfor i, txt in enumerate(text):\n    ax.annotate(txt, (x[i],y[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# fit a linear regression model on the population and gdp data\nmodel = LinearRegression()\nmodel.fit(df_2016['population'].values.reshape(-1, 1), df_2016['gdp'].values.reshape(-1, 1))\n\n# plot the data along with predictions from the linear regression model\ninputs = np.linspace(1, 2000000000, num=50)\npredictions = model.predict(inputs.reshape(-1,1))\n\ndf_2016.plot('population', 'gdp', kind='scatter')\nplt.plot(inputs, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the United States to see what happens with the linear regression model\ndf_2016[df_2016['Country Name'] != 'United States'].plot('population', 'gdp', kind='scatter')\n# plt.plot(inputs, predictions)\nmodel.fit(df_2016[df_2016['Country Name'] != 'United States']['population'].values.reshape(-1, 1), \n          df_2016[df_2016['Country Name'] != 'United States']['gdp'].values.reshape(-1, 1))\ninputs = np.linspace(1, 2000000000, num=50)\npredictions = model.predict(inputs.reshape(-1,1))\nplt.plot(inputs, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the code now ouputs a GDP value of 5.26e+12 when population equals 1e9. In other words, removing the United States shifted the linear regression line down.\n\nData scientists sometimes have the task of creating an outlier removal model. In this exercise, you've used the Tukey rule. There are other one-dimensional models like eliminating data that is far from the mean. There are also more sophisticated models that take into account multi-dimensional data."},{"metadata":{},"cell_type":"markdown","source":">## Eliminating Outliers\n\nEliminating outliers is a big topic. There are many different ways to eliminate outliers. A data engineer's job isn't necessarily to decide what counts as an outlier and what does not. A data scientist would determine that. The data engineer would code the algorithms that eliminate outliers from a data set based on any criteria that a data scientist has decided.\n\nIn this exercise, you'll write code to eliminate outliers based on the Tukey rule."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# read in the projects data set and do basic wrangling \ngdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ngdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\npopulation = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\npopulation.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n\n\n# Reshape the data sets so that they are in long format\ngdp_melt = gdp.melt(id_vars=['Country Name'], \n                    var_name='year', \n                    value_name='gdp')\n\n# Use back fill and forward fill to fill in missing gdp values\ngdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n\npopulation_melt = population.melt(id_vars=['Country Name'], \n                                  var_name='year', \n                                  value_name='population')\n\n# Use back fill and forward fill to fill in missing population values\npopulation_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n\n# merge the population and gdp data together into one data frame\ndf_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n\n# filter data for the year 2016\ndf_2016 = df_country[df_country['year'] == '2016']\n\n# filter out values that are not countries\nnon_countries = ['World',\n 'High income',\n 'OECD members',\n 'Post-demographic dividend',\n 'IDA & IBRD total',\n 'Low & middle income',\n 'Middle income',\n 'IBRD only',\n 'East Asia & Pacific',\n 'Europe & Central Asia',\n 'North America',\n 'Upper middle income',\n 'Late-demographic dividend',\n 'European Union',\n 'East Asia & Pacific (excluding high income)',\n 'East Asia & Pacific (IDA & IBRD countries)',\n 'Euro area',\n 'Early-demographic dividend',\n 'Lower middle income',\n 'Latin America & Caribbean',\n 'Latin America & the Caribbean (IDA & IBRD countries)',\n 'Latin America & Caribbean (excluding high income)',\n 'Europe & Central Asia (IDA & IBRD countries)',\n 'Middle East & North Africa',\n 'Europe & Central Asia (excluding high income)',\n 'South Asia (IDA & IBRD)',\n 'South Asia',\n 'Arab World',\n 'IDA total',\n 'Sub-Saharan Africa',\n 'Sub-Saharan Africa (IDA & IBRD countries)',\n 'Sub-Saharan Africa (excluding high income)',\n 'Middle East & North Africa (excluding high income)',\n 'Middle East & North Africa (IDA & IBRD countries)',\n 'Central Europe and the Baltics',\n 'Pre-demographic dividend',\n 'IDA only',\n 'Least developed countries: UN classification',\n 'IDA blend',\n 'Fragile and conflict affected situations',\n 'Heavily indebted poor countries (HIPC)',\n 'Low income',\n 'Small states',\n 'Other small states',\n 'Not classified',\n 'Caribbean small states',\n 'Pacific island small states']\n\n# remove non countries from the data\ndf_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\n\n\n# plot the data\nx = list(df_2016['population'])\ny = list(df_2016['gdp'])\ntext = df_2016['Country Name']\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.scatter(x, y)\nplt.title('GDP vs Population')\nplt.xlabel('GDP')\nplt.ylabel('Population')\nfor i, txt in enumerate(text):\n    ax.annotate(txt, (x[i],y[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write a function that uses the Tukey rule to eliminate outliers from an array of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write a function that uses the Tukey rule to detect outliers in a dataframe column \n# and then removes that entire row from the data frame. For example, if the United States \n# is detected to be a GDP outlier, then remove the entire row of United States data.\n# The function inputs should be a data frame and a column name.\n# The output is a data_frame with the outliers eliminated\n\n# HINT: Re-use code from the previous exercise\n\ndef tukey_rule(data_frame, column_name):\n    data = data_frame[column_name]\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n\n    IQR = Q3 - Q1\n\n    max_value = Q3 + 1.5 * IQR\n    min_value = Q1 - 1.5 * IQR\n    \n    return data_frame[(data_frame[column_name] < max_value) & (data_frame[column_name] > min_value)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use the function to eliminate population outliers and then gdp outliers from the dataframe. Store results in the df_outlier_removed variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the tukey_rule() function to make a new data frame with gdp and population outliers removed\n# Put the results in the df_outlier_removed variable\n\ndf_outlier_removed = df_2016.copy()\n\nfor column in ['population','gdp']:\n    df_outlier_removed = tukey_rule(df_outlier_removed, column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the data\nx = list(df_outlier_removed['population'])\ny = list(df_outlier_removed['gdp'])\ntext = df_outlier_removed['Country Name']\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.scatter(x, y)\nplt.title('GDP vs Population')\nplt.xlabel('GDP')\nplt.ylabel('Population')\nfor i, txt in enumerate(text):\n    ax.annotate(txt, (x[i],y[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">## Scaling Data\n\nIn this exercise, you'll practice scaling data. Sometimes, you'll see the terms **standardization** and **normalization** used interchangeably when referring to feature scaling. However, these are slightly different operations. Standardization refers to scaling a set of values so that they have a mean of zero and a standard deviation of one. Normalization refers to scaling a set of values so that the range if between zero and one.\n\nIn this exercise, you'll practice implementing standardization and normalization in code. There are libraries, like scikit-learn, that can do this for you; however, in data engineering, you might not always have these tools available."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# read in the projects data set and do basic wrangling \ngdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ngdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\npopulation = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\npopulation.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n\n\n# Reshape the data sets so that they are in long format\ngdp_melt = gdp.melt(id_vars=['Country Name'], \n                    var_name='year', \n                    value_name='gdp')\n\n# Use back fill and forward fill to fill in missing gdp values\ngdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n\npopulation_melt = population.melt(id_vars=['Country Name'], \n                                  var_name='year', \n                                  value_name='population')\n\n# Use back fill and forward fill to fill in missing population values\npopulation_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n\n# merge the population and gdp data together into one data frame\ndf_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n\n# filter data for the year 2016\ndf_2016 = df_country[df_country['year'] == '2016']\n\n# filter out values that are not countries\nnon_countries = ['World',\n 'High income',\n 'OECD members',\n 'Post-demographic dividend',\n 'IDA & IBRD total',\n 'Low & middle income',\n 'Middle income',\n 'IBRD only',\n 'East Asia & Pacific',\n 'Europe & Central Asia',\n 'North America',\n 'Upper middle income',\n 'Late-demographic dividend',\n 'European Union',\n 'East Asia & Pacific (excluding high income)',\n 'East Asia & Pacific (IDA & IBRD countries)',\n 'Euro area',\n 'Early-demographic dividend',\n 'Lower middle income',\n 'Latin America & Caribbean',\n 'Latin America & the Caribbean (IDA & IBRD countries)',\n 'Latin America & Caribbean (excluding high income)',\n 'Europe & Central Asia (IDA & IBRD countries)',\n 'Middle East & North Africa',\n 'Europe & Central Asia (excluding high income)',\n 'South Asia (IDA & IBRD)',\n 'South Asia',\n 'Arab World',\n 'IDA total',\n 'Sub-Saharan Africa',\n 'Sub-Saharan Africa (IDA & IBRD countries)',\n 'Sub-Saharan Africa (excluding high income)',\n 'Middle East & North Africa (excluding high income)',\n 'Middle East & North Africa (IDA & IBRD countries)',\n 'Central Europe and the Baltics',\n 'Pre-demographic dividend',\n 'IDA only',\n 'Least developed countries: UN classification',\n 'IDA blend',\n 'Fragile and conflict affected situations',\n 'Heavily indebted poor countries (HIPC)',\n 'Low income',\n 'Small states',\n 'Other small states',\n 'Not classified',\n 'Caribbean small states',\n 'Pacific island small states']\n\n# remove non countries from the data\ndf_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\n\n\n# show the first ten rows\nprint('first ten rows of data')\ndf_2016.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalize the Data\n\nTo normalize data, you take a feature, like gdp, and use the following formula\n\n$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n\nwhere \n* x is a value of gdp\n* x_max is the maximum gdp in the data\n* x_min is the minimum GDP in the data\n\nFirst, write a function that outputs the x_min and x_max values of an array. The inputs are an array of data (like the GDP data). The outputs are the x_min and x_max values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def x_min_max(data):\n    minimum = min(data)\n    maximum = max(data)\n    return minimum, maximum\n\nx_min_max(df_2016['gdp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, write a function that normalizes a data point. The inputs are an x value, a minimum value, and a maximum value. The output is the normalized data point"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(x, x_min, x_max):\n    # Complete this function\n    # The input is a single value \n    # The output is the normalized value\n    return (x - x_min) / (x_max - x_min)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why are you making these separate functions? Let's say you are training a machine learning model and using normalized GDP as a feature. As new data comes in, you'll want to make predictions using the new GDP data. You'll have to normalize this incoming data. To do that, you need to store the x_min and x_max from the training set. Hence the x_min_max() function gives you the minimum and maximum values, which you can then store in a variable.\n\n\nA good way to keep track of the minimum and maximum values would be to use a class. In this next section, fill out the Normalizer() class code to make a class that normalizes a data set and stores min and max values."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Normalizer():\n    # Complete the normalizer class\n    # The normalizer class receives a dataframe as its only input for initialization\n    # For example, the data frame might contain gdp and population data in two separate columns\n    # Follow the TODOs in each section\n    \n    def __init__(self, dataframe):\n        \n        # complete the init function. \n        # Assume the dataframe has an unknown number of columns like [['gdp', 'population']] \n        # iterate through each column calculating the min and max for each column\n        # append the results to the params attribute list\n        \n        # For example, take the gdp column and calculate the minimum and maximum\n        # Put these results in a list [minimum, maximum]\n        # Append the list to the params variable\n        # Then take the population column and do the same\n        \n        # HINT: You can put your x_min_max() function as part of this class and use it\n        \n        self.params = []\n\n        for column in dataframe.columns:\n            self.params.append(x_min_max(dataframe[column]))\n            \n    def x_min_max(data):\n        # complete the x_min_max method\n        # HINT: You can use the same function defined earlier in the exercise\n        minimum = min(data)\n        maximum = max(data)\n        return minimum, maximum\n\n    def normalize_data(self, x):\n        # complete the normalize_data method\n        # The function receives a data point as an input and then outputs the normalized version\n        # For example, if an input data point of [gdp, population] were used. Then the output would\n        # be the normalized version of the [gdp, population] data point\n        # Put the results in the normalized variable defined below\n        \n        # Assume that the columns in the dataframe used to initialize an object are in the same\n        # order as this data point x\n        \n        # HINT: You cannot use the normalize_data function defined earlier in the exercise.\n        # You'll need to iterate through the individual values in the x variable        \n        # Use the params attribute where the min and max values are stored \n        normalized = []\n        for i, value in enumerate(x):\n            x_max = self.params[i][1]\n            x_min = self.params[i][0]\n            normalized.append((x[i] - x_min) / (x_max - x_min))\n        return normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gdp_normalizer = Normalizer(df_2016[['gdp', 'population']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gdp_normalizer.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gdp_normalizer.normalize_data([13424475000000.0, 1300000000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWhen normalizing or standardizing features for machine learning, you'll need to store the parameters you used to do the scaling. That way you can scale new data points when making predictions. In this exercise, you stored the minimum and maximum values of a feature. When standardizing data, you would need to store the mean and standard deviation. The standardization formula is:\n\n$x_{standardized} = \\frac{x - \\overline{x}}{S}$"},{"metadata":{},"cell_type":"markdown","source":">## Feature Engineering\n\nPractice creating new features from the GDP and population data. \n\nYou'll create a new feature gdppercapita, which is GDP divided by population. You'll then write code to create new features like GDP squared and GDP cubed. \n\nStart by running the code below. It reads in the World Bank data, filters the data for the year 2016, and cleans the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# read in the projects data set and do basic wrangling \ngdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ngdp.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\npopulation = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\npopulation.drop(['Unnamed: 62', 'Country Code', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n\n\n# Reshape the data sets so that they are in long format\ngdp_melt = gdp.melt(id_vars=['Country Name'], \n                    var_name='year', \n                    value_name='gdp')\n\n# Use back fill and forward fill to fill in missing gdp values\ngdp_melt['gdp'] = gdp_melt.sort_values('year').groupby('Country Name')['gdp'].fillna(method='ffill').fillna(method='bfill')\n\npopulation_melt = population.melt(id_vars=['Country Name'], \n                                  var_name='year', \n                                  value_name='population')\n\n# Use back fill and forward fill to fill in missing population values\npopulation_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n\n# merge the population and gdp data together into one data frame\ndf_country = gdp_melt.merge(population_melt, on=('Country Name', 'year'))\n\n# filter data for the year 2016\ndf_2016 = df_country[df_country['year'] == '2016']\n\n# filter out values that are not countries\nnon_countries = ['World',\n 'High income',\n 'OECD members',\n 'Post-demographic dividend',\n 'IDA & IBRD total',\n 'Low & middle income',\n 'Middle income',\n 'IBRD only',\n 'East Asia & Pacific',\n 'Europe & Central Asia',\n 'North America',\n 'Upper middle income',\n 'Late-demographic dividend',\n 'European Union',\n 'East Asia & Pacific (excluding high income)',\n 'East Asia & Pacific (IDA & IBRD countries)',\n 'Euro area',\n 'Early-demographic dividend',\n 'Lower middle income',\n 'Latin America & Caribbean',\n 'Latin America & the Caribbean (IDA & IBRD countries)',\n 'Latin America & Caribbean (excluding high income)',\n 'Europe & Central Asia (IDA & IBRD countries)',\n 'Middle East & North Africa',\n 'Europe & Central Asia (excluding high income)',\n 'South Asia (IDA & IBRD)',\n 'South Asia',\n 'Arab World',\n 'IDA total',\n 'Sub-Saharan Africa',\n 'Sub-Saharan Africa (IDA & IBRD countries)',\n 'Sub-Saharan Africa (excluding high income)',\n 'Middle East & North Africa (excluding high income)',\n 'Middle East & North Africa (IDA & IBRD countries)',\n 'Central Europe and the Baltics',\n 'Pre-demographic dividend',\n 'IDA only',\n 'Least developed countries: UN classification',\n 'IDA blend',\n 'Fragile and conflict affected situations',\n 'Heavily indebted poor countries (HIPC)',\n 'Low income',\n 'Small states',\n 'Other small states',\n 'Not classified',\n 'Caribbean small states',\n 'Pacific island small states']\n\n# remove non countries from the data\ndf_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]\ndf_2016.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a new feature called gdppercapita in a new column. This feature should be the gdp value divided by the population."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new feature called gdppercapita, \n#      which is the gdp value divided by the population value for each country\n\ndf_2016['gdppercapita'] = df_2016['gdp'] / df_2016['population']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load \n\nIn this part, I'll load data into different formats: a csv file, a json file, and a SQLite database."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# read in the projects data set and do basic wrangling \ngdp = pd.read_csv('../input/world-bank-datasets/gdp_data.csv', skiprows=4)\ngdp.drop(['Unnamed: 62', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\npopulation = pd.read_csv('../input/world-bank-datasets/population_data.csv', skiprows=4)\npopulation.drop(['Unnamed: 62', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n\n\n# Reshape the data sets so that they are in long format\ngdp_melt = gdp.melt(id_vars=['Country Name', 'Country Code'], \n                    var_name='year', \n                    value_name='gdp')\n\n# Use back fill and forward fill to fill in missing gdp values\ngdp_melt['gdp'] = gdp_melt.sort_values('year').groupby(['Country Name', 'Country Code'])['gdp'].fillna(method='ffill').fillna(method='bfill')\n\npopulation_melt = population.melt(id_vars=['Country Name', 'Country Code'], \n                                  var_name='year', \n                                  value_name='population')\n\n# Use back fill and forward fill to fill in missing population values\npopulation_melt['population'] = population_melt.sort_values('year').groupby('Country Name')['population'].fillna(method='ffill').fillna(method='bfill')\n\n# merge the population and gdp data together into one data frame\ndf_indicator = gdp_melt.merge(population_melt, on=('Country Name', 'Country Code', 'year'))\n\n# filter out values that are not countries\nnon_countries = ['World',\n 'High income',\n 'OECD members',\n 'Post-demographic dividend',\n 'IDA & IBRD total',\n 'Low & middle income',\n 'Middle income',\n 'IBRD only',\n 'East Asia & Pacific',\n 'Europe & Central Asia',\n 'North America',\n 'Upper middle income',\n 'Late-demographic dividend',\n 'European Union',\n 'East Asia & Pacific (excluding high income)',\n 'East Asia & Pacific (IDA & IBRD countries)',\n 'Euro area',\n 'Early-demographic dividend',\n 'Lower middle income',\n 'Latin America & Caribbean',\n 'Latin America & the Caribbean (IDA & IBRD countries)',\n 'Latin America & Caribbean (excluding high income)',\n 'Europe & Central Asia (IDA & IBRD countries)',\n 'Middle East & North Africa',\n 'Europe & Central Asia (excluding high income)',\n 'South Asia (IDA & IBRD)',\n 'South Asia',\n 'Arab World',\n 'IDA total',\n 'Sub-Saharan Africa',\n 'Sub-Saharan Africa (IDA & IBRD countries)',\n 'Sub-Saharan Africa (excluding high income)',\n 'Middle East & North Africa (excluding high income)',\n 'Middle East & North Africa (IDA & IBRD countries)',\n 'Central Europe and the Baltics',\n 'Pre-demographic dividend',\n 'IDA only',\n 'Least developed countries: UN classification',\n 'IDA blend',\n 'Fragile and conflict affected situations',\n 'Heavily indebted poor countries (HIPC)',\n 'Low income',\n 'Small states',\n 'Other small states',\n 'Not classified',\n 'Caribbean small states',\n 'Pacific island small states']\n\n# remove non countries from the data\ndf_indicator  = df_indicator[~df_indicator['Country Name'].isin(non_countries)]\ndf_indicator.reset_index(inplace=True, drop=True)\n\ndf_indicator.columns = ['countryname', 'countrycode', 'year', 'gdp', 'population']\n\n# output the first few rows of the data frame\ndf_indicator.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run this code cell to read in the countries data set. This will create a data frame called df_projects containing the World Bank projects data. The data frame only has the 'id', 'countryname', 'countrycode', 'totalamt', and 'year' columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycountry\nfrom pycountry import countries\n\n# read in the projects data set with all columns type string\ndf_projects = pd.read_csv('../input/world-bank-datasets/projects_data.csv', dtype=str)\ndf_projects.drop(['Unnamed: 56'], axis=1, inplace=True)\n\ndf_projects['countryname'] = df_projects['countryname'].str.split(';').str.get(0)\n\n# set up the libraries and variables\nfrom collections import defaultdict\ncountry_not_found = [] # stores countries not found in the pycountry library\nproject_country_abbrev_dict = defaultdict(str) # set up an empty dictionary of string values\n\n# iterate through the country names in df_projects. \n# Create a dictionary mapping the country name to the alpha_3 ISO code\nfor country in df_projects['countryname'].drop_duplicates().sort_values():\n    try: \n        # look up the country name in the pycountry library\n        # store the country name as the dictionary key and the ISO-3 code as the value\n        project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n    except:\n        # If the country name is not in the pycountry library, then print out the country name\n        # And store the results in the country_not_found list\n        country_not_found.append(country)\n        \n# run this code cell to load the dictionary\n\ncountry_not_found_mapping = {'Co-operative Republic of Guyana': 'GUY',\n             'Commonwealth of Australia':'AUS',\n             'Democratic Republic of Sao Tome and Prin':'STP',\n             'Democratic Republic of the Congo':'COD',\n             'Democratic Socialist Republic of Sri Lan':'LKA',\n             'East Asia and Pacific':'EAS',\n             'Europe and Central Asia': 'ECS',\n             'Islamic  Republic of Afghanistan':'AFG',\n             'Latin America':'LCN',\n              'Caribbean':'LCN',\n             'Macedonia':'MKD',\n             'Middle East and North Africa':'MEA',\n             'Oriental Republic of Uruguay':'URY',\n             'Republic of Congo':'COG',\n             \"Republic of Cote d'Ivoire\":'CIV',\n             'Republic of Korea':'KOR',\n             'Republic of Niger':'NER',\n             'Republic of Kosovo':'XKX',\n             'Republic of Rwanda':'RWA',\n              'Republic of The Gambia':'GMB',\n              'Republic of Togo':'TGO',\n              'Republic of the Union of Myanmar':'MMR',\n              'Republica Bolivariana de Venezuela':'VEN',\n              'Sint Maarten':'SXM',\n              \"Socialist People's Libyan Arab Jamahiriy\":'LBY',\n              'Socialist Republic of Vietnam':'VNM',\n              'Somali Democratic Republic':'SOM',\n              'South Asia':'SAS',\n              'St. Kitts and Nevis':'KNA',\n              'St. Lucia':'LCA',\n              'St. Vincent and the Grenadines':'VCT',\n              'State of Eritrea':'ERI',\n              'The Independent State of Papua New Guine':'PNG',\n              'West Bank and Gaza':'PSE',\n              'World':'WLD'}\n\nproject_country_abbrev_dict.update(country_not_found_mapping)\n\ndf_projects['countrycode'] = df_projects['countryname'].apply(lambda x: project_country_abbrev_dict[x])\n\ndf_projects['boardapprovaldate'] = pd.to_datetime(df_projects['boardapprovaldate'])\n\ndf_projects['year'] = df_projects['boardapprovaldate'].dt.year.astype(str).str.slice(stop=4)\n\ndf_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',\"\"))\n\ndf_projects = df_projects[['id', 'countryname', 'countrycode', 'totalamt', 'year']]\n\ndf_projects.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first few cells in this workbook loaded and cleaned the World Bank Data. You now have two data frames:\n* df_projects, which contain data from the projects data set\n* df_indicator, which contain population and gdp data for various years\n\nThey both have country code variables. Note, however, that there could be countries represented in the projects data set that are not in the indicator data set and vice versus.\n\nIn this first exercise, merge the two data sets together using country code and year as common keys. When joining the data sets, keep all of the data in the df_projects dataframe even if there is no indicator data for that country code."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged = df_projects.merge(df_indicator, how='left', on=['countrycode', 'year'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged[(df_merged['year'] == '2017') & (df_merged['countryname_y'] == 'Jordan')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the df_merged dataframe as a json file. You can use the pandas [to_json() method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged.to_json('countrydata.json', orient='records')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the df_merged dataframe as a csv file. You can use the pandas [to_csv() method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged.to_csv('countrydata.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the df_merged dataframe as a sqlite database file. For this exercise, you can put all of the data as one table. In the next exercise, you'll create a database with multiple tables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n\n# connect to the database\n# the database file will be worldbank.db\n# note that sqlite3 will create this database file if it does not exist already\nconn = sqlite3.connect('worldbank.db')\n\ndf_merged.to_sql('merged', con = conn, if_exists='replace', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_sql('SELECT * FROM merged WHERE year = \"2017\" AND countrycode = \"BRA\"', con = conn).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the data to a SQL database like in the previous exercise; however, this time, put the df_indicator data in one table and the df_projects data in another table. Call the df_indicator table 'indicator' and the df_projects table 'projects'."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n\n# connect to the database\n# the database file will be worldbank.db\n# note that sqlite3 will create this database file if it does not exist already\nconn = sqlite3.connect('worldbank.db')\n\ndf_indicator.to_sql('indicator', con = conn, if_exists='replace', index=False)\ndf_projects.to_sql('projects', con = conn, if_exists='replace', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_sql('SELECT * FROM projects LEFT JOIN indicator ON \\\nprojects.countrycode = indicator.countrycode AND \\\nprojects.year = indicator.year WHERE \\\nprojects.year = \"2017\" AND projects.countrycode = \"BRA\"', con = conn).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# commit any changes to the database and close the database\nconn.commit()\nconn.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}