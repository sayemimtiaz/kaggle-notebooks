{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **utils.py**","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport logging\n\nimport re\nimport yaml\nimport json\n\nfrom ast import literal_eval\nfrom nltk import sent_tokenize\nfrom string import punctuation\nfrom collections import Counter\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def torch2np(tensor: torch.Tensor) -> np.array:\n    if torch.cuda.is_available():\n        tensor = tensor.cpu()\n    return tensor.numpy()\n\n\ndef count_parameter(trainable_variables):\n    total_parameters = np.sum(\n        [np.prod(var.get_shape().as_list()) for var in trainable_variables]\n    )\n    return (total_parameters)\n\n\ndef min_max_normal(tensor):\n    dim = tf.shape(tensor)[-1]\n    max_value = tf.reduce_max(tensor, -1, keepdims=True)\n    max_value = tf.tile(max_value, [1, 1, dim])\n    min_value = tf.reduce_min(tensor, -1, keepdims=True)\n    min_value = tf.tile(min_value, [1, 1, dim])\n    norm_tensor = (tensor-min_value) / (max_value-min_value+1e-6)\n    return norm_tensor\n\n\ndef z_score_normal(tensor):\n    dim = tf.shape(tensor)[-1]\n    axes = [2]\n    mean, variance = tf.nn.moments(tensor, axes, keep_dims=True)\n    std = tf.sqrt(variance)\n    mean = tf.tile(mean, [1, 1, dim])\n    std = tf.tile(std, [1, 1, dim])\n    norm_tensor = (tensor - mean) / (std + 1e-6)\n    return norm_tensor\n\n\ndef plot_history(history, figsize=(6, 9), return_figure: bool=True, **kwargs):\n    \"\"\"\n    Plot the training history of one or more models.\n    This creates a column of plots, with one plot for each metric recorded during training, with the\n    plot showing the metric vs. epoch. If multiple models have been trained (that is, a list of\n    histories is passed in), each metric plot includes multiple train and validation series.\n    Validation data is optional (it is detected by metrics with names starting with ``val_``).\n    \n    Args:\n        history: the training history, as returned by :meth:`tf.keras.Model.fit`\n        individual_figsize (tuple of numbers): the size of the plot for each metric\n        return_figure (bool): if True, then the figure object with the plots is returned, None otherwise.\n        kwargs: additional arguments to pass to :meth:`matplotlib.pyplot.subplots`\n    \n    Returns:\n        :class:`matplotlib.figure.Figure`: The figure object with the plots if ``return_figure=True``, None otherwise\n    \n    Reference:\n        https://github.com/stellargraph/stellargraph/blob/develop/stellargraph/utils/history.py\n    \"\"\"\n\n    # explicit colours are needed if there's multiple train or multiple validation series, because\n    # each train series should have the same color. This uses the global matplotlib defaults that\n    # would be used for a single train and validation series.\n    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n    color_train = colors[0]\n    color_validation = colors[1]\n\n    if not isinstance(history, list):\n        history = [history]\n\n    def remove_prefix(text, prefix):\n        return text[text.startswith(prefix) and len(prefix) :]\n\n    metrics = sorted({remove_prefix(m, \"val_\") for m in history[0].history.keys()})\n\n    height, width = figsize\n    overall_figsize = (width, len(metrics)*height)\n\n    # plot each metric in a column, so that epochs are aligned (squeeze=False, so we don't have to\n    # special case len(metrics) == 1 in the zip)\n    fig, all_axes = plt.subplots(\n        len(metrics), 1, squeeze=False, sharex=\"col\", figsize=overall_figsize, **kwargs\n    )\n\n    has_validation = False\n    for ax, m in zip(all_axes[:,0], metrics):\n        for h in history:\n            # summarize history for metric m\n            ax.plot(h.history[m], c=color_train)\n\n            try:\n                val = h.history[\"val_\" + m]\n            except KeyError:\n                # no validation data for this metric\n                pass\n            else:\n                ax.plot(val, c=color_validation)\n                has_validation = True\n\n        ax.set_ylabel(m, fontsize=\"x-large\")\n\n    # don't be redundant: only include legend on the top plot\n    labels = [\"train\"]\n    if has_validation:\n        labels.append(\"validation\")\n    all_axes[0, 0].legend(labels, loc=\"best\", fontsize=\"x-large\")\n\n    # ... and only label \"epoch\" on the bottom\n    all_axes[-1, 0].set_xlabel(\"epoch\", fontsize=\"x-large\")\n\n    # minimise whitespace\n    fig.tight_layout()\n\n    if return_figure:\n        return fig\n\n\ndef log_summary(model, line_length=None, positions=None):\n    \"\"\"\n    Log a summary of a model.\n    \n    Args:\n        model: Keras model instance.\n        line_length: Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions: Relative or absolute positions of log elements in each line.\n            If not provided, defaults to `[.33, .55, .67, 1.]`.\n    \n    Return: text of model summary\n    \"\"\"\n\n    def log_row(fields, positions):\n        line = ''\n        for i in range(len(fields)):\n            if i > 0:\n                line = line[:-1] + ' '\n            line += str(fields[i])\n            line = line[:positions[i]]\n            line += ' ' * (positions[i]-len(line))\n        return line+'\\n'\n\n    def log_layer_summary(layer):\n        \"\"\"\n        Log a summary for a single layer.\n        Args:\n            layer: target layer.\n        \"\"\"\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        except RuntimeError:  # output_shape unknown in Eager mode.\n            output_shape = '?'\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        if not layer.built and not getattr(layer, '_is_graph_network', False):\n            # If a subclassed model has a layer that is not called in Model.call, the\n            # layer will not be built and we cannot call layer.count_params().\n            params = '0 (unused)'\n        else:\n            params = layer.count_params()\n        fields = [name + ' (' + cls_name + ')', output_shape, params]\n        return log_row(fields, positions)\n\n    def log_layer_summary_with_connections(layer):\n        \"\"\"\n        Log a summary for a single layer (including topological connections).\n        Args:\n            layer: target layer.\n        \"\"\"\n        summary = ''\n        try:\n            output_shape = layer.output_shape\n        except AttributeError:\n            output_shape = 'multiple'\n        connections = []\n        for node in layer._inbound_nodes:\n            if relevant_nodes and node not in relevant_nodes:\n                # node is not part of the current network\n                continue\n\n            for inbound_layer, node_index, tensor_index, _ in node.iterate_inbound():\n                connections.append(f'{inbound_layer.name}[{node_index}][{tensor_index}]')\n\n        name = layer.name\n        cls_name = layer.__class__.__name__\n        if not connections:\n            first_connection = ''\n        else:\n            first_connection = connections[0]\n        fields = [\n            name + ' (' + cls_name + ')', output_shape, layer.count_params(), first_connection\n        ]\n        summary += log_row(fields, positions)\n        if len(connections) > 1:\n            for i in range(1, len(connections)):\n                fields = ['', '', '', connections[i]]\n                summary += log_row(fields, positions)\n        return summary\n\n    if model.__class__.__name__ == 'Sequential':\n        sequential_like = True\n    elif not model._is_graph_network:\n        # We treat subclassed models as a simple sequence of layers, for logging purposes.\n        sequential_like = True\n    else:\n        sequential_like = True\n        nodes_by_depth = model._nodes_by_depth.values()\n        nodes = []\n        for v in nodes_by_depth:\n            if (len(v) > 1) or \\\n                (len(v) == 1 and len(tf.nest.flatten(v[0].keras_inputs)) > 1):\n                # if the model has multiple nodes or if the nodes have multiple inbound_layers,\n                # the model is no longer sequential\n                sequential_like = False\n                break\n            nodes += v\n        if sequential_like:\n            # search for shared layers\n            for layer in model.layers:\n                flag = False\n                for node in layer._inbound_nodes:\n                    if node in nodes:\n                        if flag:\n                            sequential_like = False\n                            break\n                        else:\n                            flag = True\n                if not sequential_like:\n                    break\n\n    if sequential_like:\n        line_length = line_length or 65\n        positions = positions or [.45, .85, 1.]\n        if positions[-1] <= 1:\n            positions = [int(line_length * p) for p in positions]\n        # header names for the different log elements\n        to_display = ['Layer (type)', 'Output Shape', 'Param #']\n    else:\n        line_length = line_length or 98\n        positions = positions or [.33, .55, .67, 1.]\n        if positions[-1] <= 1:\n            positions = [int(line_length * p) for p in positions]\n        # header names for the different log elements\n        to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Connected to']\n        relevant_nodes = []\n        for v in model._nodes_by_depth.values():\n            relevant_nodes += v\n\n    summary = f'Model: \"{model.name}\"\\n'\n    summary += '_' * line_length + '\\n'\n    summary += log_row(to_display, positions)\n    summary += '=' * line_length + '\\n'\n    \n    layers = model.layers\n    for i in range(len(layers)):\n        if sequential_like:\n            summary += log_layer_summary(layers[i])\n        else:\n            summary += log_layer_summary_with_connections(layers[i])\n        if i == len(layers) - 1:\n            summary += '=' * line_length + '\\n'\n        else:\n            summary += '_' * line_length + '\\n'\n\n    if hasattr(model, '_collected_trainable_weights'):\n        trainable_count = count_params(model._collected_trainable_weights)\n    else:\n        trainable_count = count_params(model.trainable_weights)\n\n    non_trainable_count = count_params(model.non_trainable_weights)\n\n    summary += 'Total params: {:,}\\n'.format(trainable_count + non_trainable_count)\n    summary += 'Trainable params: {:,}\\n'.format(trainable_count)\n    summary += 'Non-trainable params: {:,}\\n'.format(non_trainable_count)\n    summary += '_' * line_length + '\\n'\n    return summary\n\n\ndef count_params(weights):\n    \"\"\"\n    Count the total number of scalars composing the weights.\n    Args:\n        weights: An iterable containing the weights on which to compute params\n    Returns:\n        The total number of scalars composing the weights\n    \"\"\"\n    unique_weights = {id(w): w for w in weights}.values()\n    weight_shapes = [w.shape.as_list() for w in unique_weights]\n    standardized_weight_shapes = [\n        [0 if w_i is None else w_i for w_i in w] for w in weight_shapes\n    ]\n    return int(sum(np.prod(p) for p in standardized_weight_shapes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **layers.py**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (\n    Input, InputLayer, Layer, Embedding, \n    Conv1D, Conv2D, Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Lambda, Reshape, \n    Softmax, Maximum, Minimum,\n)\nfrom tensorflow.keras.activations import softmax, sigmoid, relu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bernoulli(shape, mean):\n    return tf.nn.relu(\n        tf.sign(mean-tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32))\n    )\n\n\ndef to_float(tensor):\n    return tf.cast(tensor, tf.float32)\n\n\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit.\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n    refer : https://github.com/google-research/bert/blob/bee6030e31e42a9394ac567da170a89a98d2062f/modeling.py#L264\n    Args:\n        x: float Tensor to perform activation.\n    Returns:\n        `x` with the GELU activation applied.\n    \"\"\"\n    cdf = 0.5 * (1.0 + tf.math.tanh(\n        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.math.pow(x, 3)))))\n    return x * cdf\n\n\n################################## \n#           Keras Layer          #\n##################################\n\nclass DropBlock2D(Layer):\n    \n    # Adopted from https://github.com/DHZS/tf-dropblock \n\n    def __init__(self, keep_prob: float=1.0, block_size: int=1, scale: bool=True, **kwargs):\n        super(DropBlock2D, self).__init__(**kwargs)\n        self.keep_prob = keep_prob\n        self.block_size = block_size\n        self.scale = tf.constant(scale, dtype=tf.bool)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def build(self, input_shape):\n        assert len(input_shape) == 4\n        _, self.h, self.w, self.channel = input_shape.as_list()\n        # pad the mask\n        p1 = (self.block_size-1) // 2\n        p0 = (self.block_size-1) - p1\n        self.padding = [[0, 0], [p0, p1], [p0, p1], [0, 0]]\n        self.set_keep_prob()\n        super(DropBlock2D, self).build(input_shape)\n\n    def _create_mask(self, input_shape):\n        sampling_mask_shape = tf.stack([input_shape[0],\n                                        self.h-self.block_size+1,\n                                        self.w-self.block_size+1,\n                                        self.channel])\n        mask = _bernoulli(sampling_mask_shape, self.gamma)\n        mask = tf.pad(mask, self.padding)\n        mask = tf.nn.max_pool(mask, [1, self.block_size, self.block_size, 1], [1, 1, 1, 1], padding='SAME')\n        mask = 1 - mask\n        return mask\n\n    def call(self, inputs, training=None, **kwargs):\n        def drop():\n            mask = self._create_mask(tf.shape(inputs))\n            mask_size = to_float(tf.size(mask))\n            inputs_masked = inputs * mask\n            output = tf.cond(self.scale,\n                             true_fn=lambda: inputs_masked*mask_size/tf.reduce_sum(mask),\n                             false_fn=lambda: inputs_masked)\n            return output\n\n        if training is None:\n            training = K.learning_phase()\n            print(self.name, training)\n        output = tf.cond(\n            tf.logical_or(tf.logical_not(bool(training)), \n                          tf.equal(self.keep_prob, 1.0)), \n            true_fn=lambda: inputs, \n            false_fn=drop\n        )\n        return output\n\n    def set_keep_prob(self, keep_prob=None):\n        \"\"\"This method only supports Eager Execution\"\"\"\n        if keep_prob is not None:\n            self.keep_prob = keep_prob\n        w, h = to_float(self.w), to_float(self.h)\n        dropout_rate = 1. - self.keep_prob\n        self.gamma = dropout_rate*(w*h) / (self.block_size**2) / ((w-self.block_size+1)*(h-self.block_size+1))\n\n\nclass L2Norm(Layer):\n    def __init__(self, axis=-1, **kwargs):\n        super(L2Norm, self).__init__(**kwargs)\n        self.axis = axis\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        # return K.l2_normalize(inputs, axis=self.axis)\n        return tf.math.l2_normalize(inputs, axis=self.axis)\n\n\nclass SoftMask2D(Layer):\n    def __init__(self, scale: bool=False, **kwargs):\n        super(SoftMask2D, self).__init__(**kwargs)\n        self.scale = scale\n        self.supports_masking = True\n\n    def call(self, inputs):\n        x, mask = inputs\n        if self.scale:\n            dim = tf.shape(x)[-1]\n            max_x = tf.math.reduce_max(x, axis=-1, keepdims=True, name='max_x')\n            max_x = tf.tile(max_x, [1, 1, dim], name='max_x_tiled')\n            x = tf.math.subtract(x, max_x, name='x_scaled')\n        length = tf.shape(mask)[1]\n        mask_d1 = tf.tile(tf.expand_dims(mask, axis=1), [1, length, 1], name='mask_d1')\n        y = tf.math.multiply(tf.exp(x), mask_d1, name='y')\n        sum_y = tf.math.reduce_sum(y, axis=-1, keepdims=True, name='sum_y')\n        att = tf.math.divide(y, sum_y+K.epsilon(), name='att')\n\n        mask_d2 = tf.tile(tf.expand_dims(mask, axis=2), [1, 1, length], name='mask_d2')\n        att = tf.math.multiply(att, mask_d2, name='att_masked')\n        return att\n\n\nclass ExpandDim(Layer):\n    def __init__(self, axis=-1, **kwargs):\n        super(ExpandDim, self).__init__(**kwargs)\n        self.axis = axis\n        self.supports_masking = False\n\n    def call(self, inputs): \n        return tf.expand_dims(inputs, axis=self.axis)\n\n\nclass Squeeze(Layer):\n    def __init__(self, axis=-1, **kwargs):\n        super(Squeeze, self).__init__(**kwargs)\n        self.axis = axis\n        self.supports_masking = False\n\n    def call(self, inputs):\n        return tf.squeeze(inputs, axis=self.axis)\n\n\nclass ReduceDim(Layer):\n    def __init__(self, method: str='mean', axis=None, **kwargs):\n        super(ReduceDim, self).__init__(**kwargs)\n        self.axis = axis\n        self.method = method.lower()\n        self.supports_masking = False\n\n    def call(self, inputs):\n        if self.method == 'sum':\n            return tf.math.reduce_sum(inputs, axis=self.axis)\n        elif self.method == 'mean':\n            return tf.math.reduce_mean(inputs, axis=self.axis)\n        elif self.method == 'max':\n            return tf.math.reduce_max(inputs, axis=self.axis)\n        elif self.method == 'min':\n            return tf.math.reduce_min(inputs, axis=self.axis)\n        elif self.method == 'std':\n            return tf.math.reduce_std(inputs, axis=self.axis)\n        elif self.method == 'variance':\n            return tf.math.reduce_variance(inputs, axis=self.axis)\n        else:\n            raise ValueError(f'method={self.method} has been implemented yet!')\n\n\nclass MatMul(Layer):\n    def __init__(self, adjoint_a=False, adjoint_b=False, \n                    transpose_a=False, transpose_b=False,     \n                    a_is_sparse=False, b_is_sparse=False, **kwargs):\n        super(MatMul, self).__init__(**kwargs)\n        self.adjoint_a, self.adjoint_b = adjoint_a, adjoint_b\n        self.transpose_a, self.transpose_b = transpose_a, transpose_b\n        self.a_is_sparse, self.b_is_sparse = a_is_sparse, b_is_sparse        \n        self.supports_masking = False\n\n    def call(self, inputs):\n        args = {\n            'a': inputs[0], 'b': inputs[1],\n            'adjoint_a': self.adjoint_a, 'adjoint_b': self.adjoint_b,\n            'transpose_a': self.transpose_a, 'transpose_b': self.transpose_b,\n            'a_is_sparse': self.a_is_sparse, 'b_is_sparse': self.b_is_sparse,\n        }\n        return tf.linalg.matmul(**args)\n\n\nclass RACL_Block(Model):\n    \n    def __init__(self, opt, Normalizer, DropBlocks, TileBlock,\n                conv_args: dict, dense_args: dict, block_id: int, **kwargs):\n        super(RACL_Block, self).__init__(**kwargs)\n        self._name = f'RACL_Block_{block_id}'\n        self.opt = opt\n        self.block_id = block_id\n\n        self.Tile = TileBlock\n        self.Normalizer = Normalizer\n        self.DropBlock_aspect, self.DropBlock_opinion, self.DropBlock_context = DropBlocks\n\n        self.Aspect_Extractor = Conv1D(filters=self.opt.n_filters, name=f'Aspect_Conv-{block_id}', **conv_args)\n        self.Opinion_Extractor = Conv1D(filters=self.opt.n_filters, name=f'Opinion_Conv-{block_id}', **conv_args)\n        self.Context_Extractor = Conv1D(filters=self.opt.embedding_dim, name=f'Context_Conv-{block_id}', **conv_args)\n\n        self.Aspect_Classifier = Dense(name=f'Aspect_Classifier-{block_id}', **dense_args)\n        self.Opinion_Classifier = Dense(name=f'Opinion_Classifier-{block_id}', **dense_args)\n        self.Sentiment_Classifier = Dense(name=f'Sentiment_Classifier-{block_id}', **dense_args)\n\n    def call(self, inputs):\n        aspect_input, opinion_input, context_input, context_query, word_mask, position_att = inputs\n        i = self.block_id\n        \n        # Extract Private Features for each task\n        aspect_conv = self.Aspect_Extractor(aspect_input)\n        opinion_conv = self.Opinion_Extractor(opinion_input)\n        context_conv = self.Context_Extractor(context_input)\n\n        # Normalize\n        aspect_conv_norm = self.Normalizer(aspect_conv)\n        opinion_conv_norm = self.Normalizer(opinion_conv)\n        context_conv_norm = self.Normalizer(context_conv)\n\n        # Relation R1\n        aspect_see_opinion = MatMul(adjoint_b=True, name=f'aspect_see_opinion-{i}')([aspect_conv_norm, opinion_conv_norm])\n        aspect_attend_opinion = SoftMask2D(name=f'aspect_attend_opinion-{i}')([aspect_see_opinion, word_mask])\n        aspect_weigh_opinion = MatMul(name=f'aspect_weigh_opinion-{i}')([aspect_attend_opinion, opinion_conv])\n        aspect_interact = Concatenate(axis=-1, name=f'aspect_interact-{i}')([aspect_conv, aspect_weigh_opinion])\n\n        opinion_see_aspect = MatMul(adjoint_b=True, name=f'opinion_see_aspect-{i}')([opinion_conv_norm, aspect_conv_norm])\n        opinion_attend_aspect = SoftMask2D(name=f'opinion_attend_aspect-{i}')([opinion_see_aspect, word_mask])\n        opinion_weigh_aspect = MatMul(name=f'opinion_weigh_aspect-{i}')([opinion_attend_aspect, aspect_conv])\n        opinion_interact = Concatenate(axis=-1, name=f'opinion_interact-{i}')([opinion_conv, opinion_weigh_aspect])\n\n        # AE & OE Prediction\n        aspect_pred = self.Aspect_Classifier(aspect_interact)\n        opinion_pred = self.Opinion_Classifier(opinion_interact)\n\n        # OE Confidence - a slight difference from the original paper.\n        # For propagating R3, we calculate the confidence of each candidate opinion word.\n        # Only when a word satisfies the condition Prob[B,I] > Prob[O] in OE, it can be propagated to SC.\n        opinion_condition = Lambda(lambda x: 1-2.*tf.nn.softmax(x, axis=-1)[:,:,0], name=f'opinion_condition-{i}')(opinion_pred)\n        opinion_confidence = Lambda(lambda x: tf.math.maximum(0., x), name=f'opinion_confidence-{i}')(opinion_condition)\n        mask = self.Tile(word_mask)\n        opinion_propagated = self.Tile(opinion_confidence)\n        opinion_propagated = MatMul(name=f'opinion_propagated_masked-{i}')([opinion_propagated, mask])\n        opinion_propagated = MatMul(name=f'opinion_propagated-{i}')([opinion_propagated, position_att])\n\n        # SC Aspect-Context Attention\n        word_see_context = MatMul(adjoint_b=True, name=f'word_see_context-{i}')([(context_query), context_conv_norm])\n        word_see_context = MatMul(name=f'word_see_context_masked-{i}')([word_see_context, position_att])\n        word_attend_context = SoftMask2D(scale=True, name=f'word_attend_context-{i}')([word_see_context, word_mask])\n\n        # Relation R2 & R3\n        word_attend_context += aspect_attend_opinion + opinion_propagated\n        word_weigh_context = MatMul(name=f'word_weigh_context-{i}')([word_attend_context, context_conv])\n        context_interact = context_query + word_weigh_context\n\n        # SC Prediction\n        sentiment_pred = self.Sentiment_Classifier(context_interact)\n\n        # We use DropBlock to enhance the learning of the private features for AE, OE & SC.\n        # For more details, refer to \n        #   http://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks for more details.\n        aspect_interact = ExpandDim(axis=-1)(aspect_interact)\n        aspect_interact = self.DropBlock_aspect(aspect_interact, self.opt.is_training)\n        aspect_interact = Squeeze(axis=-1)(aspect_interact)\n\n        opinion_interact = ExpandDim(axis=-1)(opinion_interact)\n        opinion_interact = self.DropBlock_opinion(opinion_interact, self.opt.is_training)\n        opinion_interact = Squeeze(axis=-1)(opinion_interact)\n        \n        context_conv = ExpandDim(axis=-1)(context_conv)\n        context_conv = self.DropBlock_context(context_conv, self.opt.is_training)\n        context_conv = Squeeze(axis=-1)(context_conv)\n\n        return [(aspect_pred, opinion_pred, sentiment_pred), \n                (aspect_interact, opinion_interact, context_interact, context_conv)]\n\n\ndef dropoutize_embeddings(opt, layer_name: str='embeddings_dropout', model_name: str='dropoutize_embeddings'):\n    model = Sequential(name=model_name)\n    model.add(Dropout(rate=1-opt.keep_prob_1, \n                      input_shape=(opt.max_sentence_len, opt.embedding_dim), \n                      seed=opt.random_seed,\n                      name=layer_name))\n    word_inputs, word_embeddings = model.inputs, model.outputs\n    return word_inputs[0], word_embeddings[0]\n\n\ndef create_embeddings(inputs, opt, embedding_dim: int, layer_prefix: str='', pretrained_embeddings=None):\n    embedding_args = {\n        'input_dim': opt.vocab_size+1, # MUST: +1\n        'output_dim': embedding_dim,\n        'name': layer_prefix+'_embeddings',\n        'trainable': False # finetune only happens after warm-up\n    }\n    if pretrained_embeddings is not None \\\n        and pretrained_embeddings.shape==(embedding_args['input_dim'], embedding_args['output_dim']):\n        embedding_args['weights'] = [pretrained_embeddings]\n    embeddings = Embedding(**embedding_args)(inputs)\n    embeddings = Dropout(1-opt.keep_prob_1, name=layer_prefix+'_embeddings_dropout')(embeddings)\n    return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **losses.py**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crossentropy(preds, labels, weights=None, name='loss'):\n    \n    with tf.name_scope(name) as scope:\n        loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels, name='unweighted_loss')\n\n        if weights is None:\n            loss = tf.reduce_mean(loss, name='final_loss')\n            return loss\n\n        class_weights = tf.constant(weights, name='class_weights')\n        sample_weights = tf.reduce_sum(class_weights*labels, axis=1, name='sample_weights')\n        loss = tf.reduce_mean(sample_weights*loss, name='weighted_loss')\n        return loss\n\n\ndef RACL_losses(y_true, y_pred, masks, opt):\n    ae_label, oe_label, sc_label = y_true\n    ae_pred, oe_pred, sc_pred = y_pred\n    word_mask, sentiment_mask = masks\n\n    # Format predictions\n    ae_pred = tf.cast(ae_pred, tf.float32, name='ae_pred')\n    oe_pred = tf.cast(oe_pred, tf.float32, name='oe_pred')\n    sc_pred = tf.cast(sc_pred, tf.float32, name='sc_pred')\n    word_mask = tf.cast(word_mask, tf.float32, name='word_mask')\n    sentiment_mask = tf.cast(sentiment_mask, tf.float32, name='sentiment_mask')\n\n    # Convert values to probabilities\n    ae_prob = tf.nn.softmax(ae_pred, axis=-1, name='ae_prob')\n    oe_prob = tf.nn.softmax(oe_pred, axis=-1, name='oe_prob')\n    # sc_prob = tf.nn.softmax(sc_pred, axis=-1, name='sc_prob')\n\n    # Define shapes\n    batch_size = ae_pred.shape[0]\n    output_shape = [-1, opt.n_classes]\n    mask_shape = [1, 1, opt.n_classes]\n\n    # Mask AE, OE, SC Predictions\n    word_mask = tf.tile(tf.expand_dims(word_mask, axis=-1), mask_shape)\n    ae_pred = tf.reshape(word_mask*ae_pred, output_shape, name='ae_pred_masked')\n    oe_pred = tf.reshape(word_mask*oe_pred, output_shape, name='oe_pred_masked')\n\n    sentiment_mask = tf.tile(tf.expand_dims(sentiment_mask, axis=-1), mask_shape)\n    sc_pred = tf.reshape(sentiment_mask*sc_pred, output_shape, name='sc_pred_masked')\n\n    # Relation R4 (only in Training)\n    # In training / validation, sentiment masks are set to 1.0 only for aspect terms.\n    # In testing, sentiment masks are set to 1.0 for all words (except padded ones).\n\n    # Format Labels\n    ae_label = tf.cast(ae_label, tf.float32, name='ae_label')\n    oe_label = tf.cast(oe_label, tf.float32, name='oe_label')\n    sc_label = tf.cast(sc_label, tf.float32, name='sc_label')\n    ae_label = tf.reshape(ae_label, output_shape, name='ae_label_flat')\n    oe_label = tf.reshape(oe_label, output_shape, name='oe_label_flat')\n    sc_label = tf.reshape(sc_label, output_shape, name='sc_label_flat')\n\n    # AE & OE Regularization cost - only get Beginning [1] and Inside [2] values\n    ae_cost = tf.reduce_sum(ae_prob[:,:,1:], axis=-1, name='ae_cost')\n    oe_cost = tf.reduce_sum(oe_prob[:,:,1:], axis=-1, name='oe_cost')\n    total_cost = ae_cost + oe_cost - 1.\n    total_cost = tf.maximum(0., total_cost, name='total_cost')\n    reg_cost = tf.reduce_sum(total_cost) / tf.reduce_sum(word_mask)\n    reg_cost = tf.identity(reg_cost, name='regularization_cost') \n\n    # Weighted SoftMax Categorical Cross-Entropy for AE, OE, SC\n    ae_loss = crossentropy(ae_pred, ae_label, opt.term_weights, name='aspect') \n    oe_loss = crossentropy(oe_pred, oe_label, opt.term_weights, name='opinion') \n    sc_loss = crossentropy(sc_pred, sc_label, opt.polarity_weights, name='sentiment') \n\n    loss = opt.aspect_weight * ae_loss + \\\n           opt.opinion_weight * oe_loss + \\\n           opt.sentiment_weight * sc_loss + \\\n           opt.regularization_weight * reg_cost\n    loss = tf.identity(loss, name='overall_loss')\n    return loss, ae_loss, oe_loss, sc_loss, reg_cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **metrics.py**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.keras.callbacks import Callback","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilon = K.epsilon()\n\nterm_code = {\n    'begin': 1, \n    'inside': 2, \n    'outside': 0, \n}\n\nidx2polarity = {\n    0: 'background', \n    1: 'positive', \n    2: 'negative', \n    3: 'neutral', \n    4: 'conflict', \n}\n\npolarity2idx = {\n    v: k for k,v in idx2polarity.items()\n}\n\n\ndef mask_absa(y_aspect, y_sentiment, mask) -> (list, list):\n    # Mask background words and conflict-sentiment words\n    #       for not to count in evaluation\n    Ys_aspect, Ys_sentiment = [], []\n    for seq_aspect, seq_sentiment, seq_mask in zip(y_aspect, y_sentiment, mask):\n        labels_aspect, labels_sentiment = [], []\n        for l_a, l_s, m in zip(seq_aspect, seq_sentiment, seq_mask):\n            if m == 0:\n                break\n            labels_aspect.append(np.argmax(l_a))\n            if not np.any(l_s):\n                # all 0s means background or conflict-sentiment word \n                #       -> be not counted for evaluation\n                labels_sentiment.append(0)\n            else:\n                labels_sentiment.append(np.argmax(l_s)+1)\n\n        Ys_aspect.append(labels_aspect)\n        Ys_sentiment.append(labels_sentiment)\n    return Ys_aspect, Ys_sentiment\n\n\ndef score_absa_single_sample(t_true, t_pred, s_true=[], s_pred=[], counters: dict={}, term_only: bool=False):\n    n_relevants, n_corrects, n_predicteds = 0, 0, 0\n    n_words = len(t_true)\n    for j in range(n_words):\n        if t_true[j] == term_code['begin']:\n            n_relevants += 1\n            if not term_only:\n                if s_true[j] != polarity2idx['background']:\n                    counters['overall'][idx2polarity[s_true[j]]] += 1\n            \n            if t_pred[j] == term_code['begin']:\n                matching = True\n                for k in range(j+1, len(t_true)):\n                    if t_true[k] == term_code['inside'] and t_pred[k] == term_code['inside']:\n                        continue\n                    elif t_true[k] != term_code['inside'] and t_pred[k] != term_code['inside']:\n                        break\n                    else:\n                        matching = False\n                        break\n\n                if matching:\n                    n_corrects += 1\n                    if not term_only:\n                        if s_true[j] != polarity2idx['background']: \n                            counters['gold'][idx2polarity[s_true[j]]] += 1\n                            counters['pred'][idx2polarity[s_pred[j]]] += 1\n                            if s_true[j] == s_pred[j]:\n                                counters['correct'][idx2polarity[s_pred[j]]] += 1\n                        else:\n                            counters['pred']['conflict'] += 1\n\n    for t_p in t_pred:\n        if t_p == term_code['begin']:\n            n_predicteds += 1\n    \n    if term_only:\n        return n_relevants, n_corrects, n_predicteds\n    return [n_relevants, n_corrects, n_predicteds], counters\n\n\ndef score_absa(terms_true, terms_pred,\n               sentiments_true: list=[], sentiments_pred: list=[],\n               method: str='micro', term_only: bool=False):\n    # Define useful variables\n    if not term_only:\n        # Sentiment Distribution for Aspect / Opinion Terms:\n        #          pred_count: predicted results that are correctly extracted\n        #          gold_count: gold results that are correctly extracted\n        #       correct_count: results that get both span & prediction correctly\n        #       overall_count: ground-truth\n        counters = {\n            'gold': {'positive': 0, 'negative': 0, 'neutral': 0, }, \n            'pred': {'positive': 0, 'negative': 0, 'neutral': 0, 'conflict': 0}, \n            'correct': {'positive': 0, 'negative': 0, 'neutral': 0, }, \n            'overall': {'positive': 0, 'negative': 0, 'neutral': 0, }\n        }\n\n    # Do statistics\n    n_corrects, n_predicteds, n_relevants = 0, 0, 0\n    n_samples = len(terms_true)\n    for i in range(n_samples):\n        t_true, t_pred = terms_true[i], terms_pred[i]\n\n        if term_only:\n            sample_relevants, sample_corrects, sample_predicteds = score_absa_single_sample(t_true, t_pred, term_only=term_only)\n        else:\n            s_true, s_pred = sentiments_true[i], sentiments_pred[i]\n            [sample_relevants, sample_corrects, sample_predicteds], \\\n                counters = score_absa_single_sample(t_true, t_pred, s_true, s_pred, counters, term_only)\n\n        n_corrects += sample_corrects\n        n_relevants += sample_relevants\n        n_predicteds += sample_predicteds\n\n    # Calculate evaluation metrics for Term (of Aspect or Opinion)\n    term_P = n_corrects / (n_predicteds+epsilon) # precision\n    term_R = n_corrects / (n_relevants+epsilon) # recall\n    term_F1 = 2*term_P*term_R / (term_P+term_R+epsilon)\n\n    if term_only:\n        return term_F1\n\n    sentiment_Acc, sentiment_F1, absa_F1 = score_sentiment_and_overall(n_predicteds, counters, method)\n    return term_F1, sentiment_Acc, sentiment_F1, absa_F1\n\n\ndef score_sentiment_and_overall(n_predicteds: int, counters: dict, method: str='micro'):\n    # Precision and Recall per each sentiment polarity\n    positive_P = counters['correct']['positive'] / (counters['pred']['positive']+epsilon)\n    positive_R = counters['correct']['positive'] / (counters['gold']['positive']+epsilon)\n\n    negative_P = counters['correct']['negative'] / (counters['pred']['negative']+epsilon)\n    negative_R = counters['correct']['negative'] / (counters['gold']['negative']+epsilon)\n\n    neutral_P = counters['correct']['neutral'] / (counters['pred']['neutral']+epsilon)\n    neutral_R = counters['correct']['neutral'] / (counters['gold']['neutral']+epsilon)\n\n    # Calculate evaluation metrics for Sentiment\n    n_corrects_sentiment = counters['correct']['positive'] + counters['correct']['negative'] + counters['correct']['neutral']\n    n_corrects_aspect = counters['gold']['positive'] + counters['gold']['negative'] + counters['gold']['neutral']\n    n_overall = counters['overall']['positive'] + counters['overall']['negative'] + counters['overall']['neutral']\n\n    sentiment_Acc = n_corrects_sentiment / (n_corrects_aspect+epsilon)\n    if method == 'micro':\n        sentiment_P = (positive_P+negative_P+neutral_P) / 3.0\n        sentiment_R = (positive_R+negative_R+neutral_R) / 3.0\n        sentiment_F1 = 2*sentiment_P*sentiment_R / (sentiment_P+sentiment_R+epsilon)\n    elif method == 'macro':\n        positive_F1 = 2*positive_P*positive_R / (positive_P+positive_R+epsilon)\n        negative_F1 = 2*negative_P*negative_R / (negative_P+negative_R+epsilon)\n        neutral_F1 = 2*neutral_P*neutral_R / (neutral_P+neutral_R+epsilon)\n        sentiment_F1 = (positive_F1+negative_F1+neutral_F1) / 3.0\n    else:\n        raise ValueError('method must be either micro or macro')\n\n    # Calculate evaluation metrics for ABSA\n    absa_P = n_corrects_sentiment / (n_predicteds-counters['pred']['conflict']+epsilon)\n    absa_R = n_corrects_sentiment / (n_overall+epsilon)\n    absa_F1 = 2*absa_P*absa_R / (absa_P+absa_R+epsilon)    \n\n    return sentiment_Acc, sentiment_F1, absa_F1\n\n\ndef evaluate_absa(aspects_true, aspects_pred,\n                  opinions_true, opinions_pred,\n                  sentiments_true, sentiments_pred,\n                  mask, include_opinion: bool=True, threshold: float=0.5):\n    aspects_true, sentiments_true = mask_absa(aspects_true, sentiments_true, mask)\n    aspects_pred, sentiments_pred = mask_absa(aspects_pred, sentiments_pred, mask)\n    absa_scores = score_absa(aspects_true, aspects_pred, sentiments_true, sentiments_pred)\n    # aspect_f1, sentiment_acc, sentiment_f1, absa_f1 = absa_scores\n\n    if include_opinion:\n        opinions_true, _ = mask_absa(opinions_true, sentiments_true, mask)\n        opinions_pred, _ = mask_absa(opinions_pred, sentiments_pred, mask)\n        opinion_f1 = score_absa(opinions_true, opinions_pred, term_only=True)\n        absa_scores = [opinion_f1] + list(absa_scores)\n\n    return absa_scores\n\n\nepsilon = K.epsilon()\n\nterm_code = {\n    'begin': 1, \n    'inside': 2, \n    'outside': 0, \n}\n\nidx2polarity = {\n    0: 'background', \n    1: 'positive', \n    2: 'negative', \n    3: 'neutral', \n    4: 'conflict', \n}\n\npolarity2idx = {\n    v: k for k,v in idx2polarity.items()\n}\n\n\ndef mask_absa(y_aspect, y_sentiment, mask) -> (list, list):\n    # Mask background words and conflict-sentiment words\n    #       for not to count in evaluation\n    Ys_aspect, Ys_sentiment = [], []\n    for seq_aspect, seq_sentiment, seq_mask in zip(y_aspect, y_sentiment, mask):\n        labels_aspect, labels_sentiment = [], []\n        for l_a, l_s, m in zip(seq_aspect, seq_sentiment, seq_mask):\n            if m == 0:\n                break\n            labels_aspect.append(np.argmax(l_a))\n            if not np.any(l_s):\n                # all 0s means background or conflict-sentiment word \n                #       -> be not counted for evaluation\n                labels_sentiment.append(0)\n            else:\n                labels_sentiment.append(np.argmax(l_s)+1)\n\n        Ys_aspect.append(labels_aspect)\n        Ys_sentiment.append(labels_sentiment)\n    return Ys_aspect, Ys_sentiment\n\n\ndef score_absa_single_sample(t_true, t_pred, s_true=[], s_pred=[], counters: dict={}, term_only: bool=False):\n    n_relevants, n_corrects, n_predicteds = 0, 0, 0\n    n_words = len(t_true)\n    for j in range(n_words):\n        if t_true[j] == term_code['begin']:\n            n_relevants += 1\n            if not term_only:\n                if s_true[j] != polarity2idx['background']:\n                    counters['overall'][idx2polarity[s_true[j]]] += 1\n            \n            if t_pred[j] == term_code['begin']:\n                matching = True\n                for k in range(j+1, len(t_true)):\n                    if t_true[k] == term_code['inside'] and t_pred[k] == term_code['inside']:\n                        continue\n                    elif t_true[k] != term_code['inside'] and t_pred[k] != term_code['inside']:\n                        break\n                    else:\n                        matching = False\n                        break\n\n                if matching:\n                    n_corrects += 1\n                    if not term_only:\n                        if s_true[j] != polarity2idx['background']: \n                            counters['gold'][idx2polarity[s_true[j]]] += 1\n                            counters['pred'][idx2polarity[s_pred[j]]] += 1\n                            if s_true[j] == s_pred[j]:\n                                counters['correct'][idx2polarity[s_pred[j]]] += 1\n                        else:\n                            counters['pred']['conflict'] += 1\n\n    for t_p in t_pred:\n        if t_p == term_code['begin']:\n            n_predicteds += 1\n    \n    if term_only:\n        return n_relevants, n_corrects, n_predicteds\n    return [n_relevants, n_corrects, n_predicteds], counters\n\n\ndef score_absa(terms_true, terms_pred,\n               sentiments_true: list=[], sentiments_pred: list=[],\n               method: str='micro', term_only: bool=False):\n    # Define useful variables\n    if not term_only:\n        # Sentiment Distribution for Aspect / Opinion Terms:\n        #          pred_count: predicted results that are correctly extracted\n        #          gold_count: gold results that are correctly extracted\n        #       correct_count: results that get both span & prediction correctly\n        #       overall_count: ground-truth\n        counters = {\n            'gold': {'positive': 0, 'negative': 0, 'neutral': 0, }, \n            'pred': {'positive': 0, 'negative': 0, 'neutral': 0, 'conflict': 0}, \n            'correct': {'positive': 0, 'negative': 0, 'neutral': 0, }, \n            'overall': {'positive': 0, 'negative': 0, 'neutral': 0, }\n        }\n\n    # Do statistics\n    n_corrects, n_predicteds, n_relevants = 0, 0, 0\n    n_samples = len(terms_true)\n    for i in range(n_samples):\n        t_true, t_pred = terms_true[i], terms_pred[i]\n\n        if term_only:\n            sample_relevants, sample_corrects, sample_predicteds = score_absa_single_sample(t_true, t_pred, term_only=term_only)\n        else:\n            s_true, s_pred = sentiments_true[i], sentiments_pred[i]\n            [sample_relevants, sample_corrects, sample_predicteds], \\\n                counters = score_absa_single_sample(t_true, t_pred, s_true, s_pred, counters, term_only)\n\n        n_corrects += sample_corrects\n        n_relevants += sample_relevants\n        n_predicteds += sample_predicteds\n\n    # Calculate evaluation metrics for Term (of Aspect or Opinion)\n    term_P = n_corrects / (n_predicteds+epsilon) # precision\n    term_R = n_corrects / (n_relevants+epsilon) # recall\n    term_F1 = 2*term_P*term_R / (term_P+term_R+epsilon)\n\n    if term_only:\n        return term_F1\n\n    sentiment_Acc, sentiment_F1, absa_F1 = score_sentiment_and_overall(n_predicteds, counters, method)\n    return term_F1, sentiment_Acc, sentiment_F1, absa_F1\n\n\ndef score_sentiment_and_overall(n_predicteds: int, counters: dict, method: str='micro'):\n    # Precision and Recall per each sentiment polarity\n    positive_P = counters['correct']['positive'] / (counters['pred']['positive']+epsilon)\n    positive_R = counters['correct']['positive'] / (counters['gold']['positive']+epsilon)\n\n    negative_P = counters['correct']['negative'] / (counters['pred']['negative']+epsilon)\n    negative_R = counters['correct']['negative'] / (counters['gold']['negative']+epsilon)\n\n    neutral_P = counters['correct']['neutral'] / (counters['pred']['neutral']+epsilon)\n    neutral_R = counters['correct']['neutral'] / (counters['gold']['neutral']+epsilon)\n\n    # Calculate evaluation metrics for Sentiment\n    n_corrects_sentiment = counters['correct']['positive'] + counters['correct']['negative'] + counters['correct']['neutral']\n    n_corrects_aspect = counters['gold']['positive'] + counters['gold']['negative'] + counters['gold']['neutral']\n    n_overall = counters['overall']['positive'] + counters['overall']['negative'] + counters['overall']['neutral']\n\n    sentiment_Acc = n_corrects_sentiment / (n_corrects_aspect+epsilon)\n    if method == 'micro':\n        sentiment_P = (positive_P+negative_P+neutral_P) / 3.0\n        sentiment_R = (positive_R+negative_R+neutral_R) / 3.0\n        sentiment_F1 = 2*sentiment_P*sentiment_R / (sentiment_P+sentiment_R+epsilon)\n    elif method == 'macro':\n        positive_F1 = 2*positive_P*positive_R / (positive_P+positive_R+epsilon)\n        negative_F1 = 2*negative_P*negative_R / (negative_P+negative_R+epsilon)\n        neutral_F1 = 2*neutral_P*neutral_R / (neutral_P+neutral_R+epsilon)\n        sentiment_F1 = (positive_F1+negative_F1+neutral_F1) / 3.0\n    else:\n        raise ValueError('method must be either micro or macro')\n\n    # Calculate evaluation metrics for ABSA\n    absa_P = n_corrects_sentiment / (n_predicteds-counters['pred']['conflict']+epsilon)\n    absa_R = n_corrects_sentiment / (n_overall+epsilon)\n    absa_F1 = 2*absa_P*absa_R / (absa_P+absa_R+epsilon)    \n\n    return sentiment_Acc, sentiment_F1, absa_F1\n\n\ndef evaluate_absa(aspects_true, aspects_pred,\n                  opinions_true, opinions_pred,\n                  sentiments_true, sentiments_pred,\n                  mask, include_opinion: bool=True, threshold: float=0.5):\n    aspects_true, sentiments_true = mask_absa(aspects_true, sentiments_true, mask)\n    aspects_pred, sentiments_pred = mask_absa(aspects_pred, sentiments_pred, mask)\n    absa_scores = score_absa(aspects_true, aspects_pred, sentiments_true, sentiments_pred)\n    # aspect_f1, sentiment_acc, sentiment_f1, absa_f1 = absa_scores\n\n    if include_opinion:\n        opinions_true, _ = mask_absa(opinions_true, sentiments_true, mask)\n        opinions_pred, _ = mask_absa(opinions_pred, sentiments_pred, mask)\n        opinion_f1 = score_absa(opinions_true, opinions_pred, term_only=True)\n        absa_scores = [opinion_f1] + list(absa_scores)\n\n    return absa_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **optimizers.py**","metadata":{}},{"cell_type":"code","source":"pip install gradient-centralization-tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gctf\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer(optimizer_option: str, use_grad_centralized: bool=True, learning_rate: float=0.001, **kwargs):\n\n    optimizer_option = optimizer_option.lower()\n    if optimizer_option == 'adam':\n        if use_grad_centralized:\n            from gctf.optimizers import adam as Optimizer\n        else:\n            from tensorflow.keras.optimizers import Adam as Optimizer\n    elif optimizer_option == 'nadam':\n        if use_grad_centralized:\n            from gctf.optimizers import nadam as Optimizer\n        else:\n            from tensorflow.keras.optimizers import Nadam as Optimizer\n    elif optimizer_option == 'adagrad':\n        if use_grad_centralized:\n            from gctf.optimizers import adagrad as Optimizer\n        else:\n            from tensorflow.keras.optimizers import Adagrad as Optimizer\n    elif optimizer_option == 'adadelta':\n        if use_grad_centralized:\n            from gctf.optimizers import adadelta as Optimizer\n        else:\n            from tensorflow.keras.optimizers import Adadelta as Optimizer\n    elif optimizer_option == 'rmsprop':\n        if use_grad_centralized:\n            from gctf.optimizers import rmsprop as Optimizer\n        else:\n            from tensorflow.keras.optimizers import RMSprop as Optimizer\n    else:\n        if use_grad_centralized:\n            from gctf.optimizers import sgd as Optimizer\n        else:\n            from tensorflow.keras.optimizers import SGD as Optimizer\n    \n    return Optimizer(learning_rate=learning_rate, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **lr_schedulers.py**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def noam_scheme(global_step, init_lr, warmup_steps=16):\n    \"\"\"\n    Noam scheme learning rate decay\n        init_lr: (scalar) initial learning rate. \n        global_step: (scalar) current training step\n        warmup_steps: (scalar) During warmup_steps, learning rate increases until it reaches init_lr.\n    \"\"\"\n    step = tf.cast(global_step+1, dtype=tf.float32, name=\"global_step\")\n    return init_lr * (warmup_steps**0.5) * tf.minimum(step*(warmup_steps**-1.5), step**-0.5)\n\n\nclass CyclicLR(Callback):\n    \"\"\"\n    This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with some constant frequency, \n        as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n    \n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"halving\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exponential\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n\n    For more detail, please read the paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {original, halving, exponential}.\n            Default 'original'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.1, step_size=2000., mode='original',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'halving':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exponential':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n            else:\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None, new_step_size=None):\n        \"\"\"\n        Resets cycle iterations.\n            Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        new_lr = self.clr()\n        K.set_value(self.model.optimizer.lr, new_lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **data_generators.py**","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nimport numpy as np\nimport sklearn as skl\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(Sequence):\n\n    def __init__(self, data_root, opt, validate: bool=False):\n        \n        self.opt = opt\n        self.data_root = data_root\n\n        # list of files containing both word-embeddings and multi-labels\n        if isinstance(self.data_root, str):\n            self.files = glob(os.path.join(self.data_root, 'sample_*.npz'))\n        elif isinstance(self.data_root, (list, tuple)):\n            self.files = []\n            for data_dir in self.data_root:\n                self.files += glob(os.path.join(data_dir, 'sample_*.npz'))\n                \n        self.indices = np.array(list(range(len(self.files))))\n        if not validate:\n            self.batch_size = opt.batch_size\n            self.shuffle = True\n        else:\n            self.batch_size = len(self.indices)\n            self.shuffle = False\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\" Denotes the number of batches per epoch \"\"\"\n        n_samples = len(self.files)\n        return n_samples//self.batch_size + (0 if n_samples%self.batch_size==0 else 1)\n\n    def __getitem__(self, index):\n        \"\"\" Generate single batch of data \"\"\"\n\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        emb_batch = []\n        em_batch, sm_batch, pa_batch = [], [], []\n        ay_batch, oy_batch, sy_batch = [], [], []\n\n        for idx in indices:\n            sample_file = self.files[idx]\n            sample_data = np.load(sample_file)\n            \n            # Load embeddings\n            emb_pad = np.zeros((self.opt.max_sentence_len, self.opt.embedding_dim))\n            emb = sample_data['sent_emb']\n            emb_pad[:emb.shape[0],:] = emb\n            emb_batch += [emb_pad]\n\n            # Load masks\n            em_batch += [sample_data['sent_mask']]\n            sm_batch += [sample_data['s_mask']]\n            pa_batch += [sample_data['pos_att']]\n            \n            # Load labels\n            ay = sample_data['a_y'].astype(float)\n            oy = sample_data['o_y'].astype(float)\n            sy = sample_data['s_y'].astype(float)\n            ay_batch += [self.smooth_labels(ay) if self.opt.label_smoothing else ay]\n            oy_batch += [self.smooth_labels(oy) if self.opt.label_smoothing else oy]\n            sy_batch += [self.smooth_labels(sy) if self.opt.label_smoothing else sy]\n        \n        data_batch = [emb_batch, em_batch, sm_batch, pa_batch, ay_batch, oy_batch, sy_batch]\n        data_batch = [np.array(d) for d in data_batch]\n        return data_batch[:4], data_batch[4:]\n\n    def smooth_labels(self, labels, factor=0.1):\n        # smooth the labels\n        labels *= (1 - factor)\n        labels += (factor / labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \"\"\" Update indices after each epoch \"\"\"\n        if self.shuffle:\n            self.indices = skl.utils.shuffle(self.indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RACL.py**","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport ntpath\nimport time\nimport pickle\nimport logging\nfrom tqdm import tqdm as print_progress\n\nimport math\nimport numpy as np\nimport tensorflow as tf\nif not tf.executing_eagerly():\n    tf.enable_eager_execution()\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (\n    Input, InputLayer, Embedding, LSTM, Bidirectional,\n    Conv1D, Conv2D, Dropout, Dense, \n    Dot, Concatenate, Average, Add, Multiply,\n    Lambda, Reshape, \n    Softmax, Maximum, Minimum,\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal, GlorotUniform\nfrom tensorflow.keras.optimizers import Adam, Nadam, Adagrad, Adadelta, RMSprop, SGD\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, LearningRateScheduler, EarlyStopping, ModelCheckpoint, Callback\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ABSA_Evaluation(Callback):\n\n    def __init__(self, validation_data, logger, opt, include_opinion: bool=True, threshold: float=0.2, name='ABSA_scores', **kwargs):\n        super().__init__(**kwargs) # handle base args (e.g. dtype)\n        self._name = name\n        self.Xs, self.Ys_true = validation_data\n        self.opt = opt\n        self.logger = logger\n        self.include_opinion = include_opinion\n        self.threshold = threshold\n        self.records = {\n            'opinion_f1': [], 'OE_loss': [], 'Reg_cost': [],\n            'aspect_f1': [], 'AE_loss': [], \n            'sentiment_acc': [], 'sentiment_f1': [], 'SC_loss': [],\n            'ABSA_f1': [], 'total_loss': [],\n        }\n\n    def on_train_begin(self, logs={}):\n        ...\n        \n    def on_epoch_end(self, epoch, logs={}):\n        start = time.time()\n\n        # Forward pass\n        *Ys_pred, word_mask, sentiment_mask = self.model(self.Xs, training=False)\n\n        # Compute losses\n        losses = RACL_losses(self.Ys_true, Ys_pred, [word_mask, sentiment_mask], self.opt)\n\n        # Evaluate\n        scores = evaluate_absa(self.Ys_true[0], Ys_pred[0],\n                               self.Ys_true[1], Ys_pred[1],\n                               self.Ys_true[2], Ys_pred[2],\n                               word_mask, self.include_opinion)\n        end = time.time()\n\n        metrics = {\n            'opinion_f1': scores[0], 'OE_loss': losses[1],\n            'aspect_f1': scores[1], 'AE_loss': losses[2], 'Reg_cost': losses[4],\n            'sentiment_acc': scores[2], 'sentiment_f1': scores[3], 'SC_loss': losses[3],\n            'ABSA_f1': scores[4], 'total_loss': losses[0],\n        }\n\n        self.max_score_index, self.min_loss_index = self.update_metrics(metrics)\n        display_text = f'Epoch {epoch+1:03d} - Evaluation in {int(end-start)} seconds\\n' + \\\n            f'\\tOE_loss={losses[1]:.3f}, AE_loss={losses[2]:.3f}, SC_loss={losses[3]:.3f}, Reg_cost={losses[4]:.3f}, total_loss={losses[0]:.3f}' + \\\n            f'\\n\\topinion_f1={scores[0]:.7f}, aspect_f1={scores[1]:.7f}, sentiment_acc={scores[2]:.7f}, sentiment_f1={scores[3]:.7f}, ABSA_f1={scores[4]:.7f}' + \\\n            f'\\n--> Best score at Epoch {self.max_score_index}' + \\\n            f'\\n--> Best loss at Epoch {self.min_loss_index}'\n        self.logger.info(display_text)\n        return metrics\n\n    def update_metrics(self, metrics):\n        for k, v in metrics.items():\n            self.records[k].append(v)\n        return np.argmax(self.records['ABSA_f1'])+1, \\\n                np.argmin(self.records['total_loss'])+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RACLModel(Model):\n\n    def set_opt(self, opt):\n        self.opt = opt\n\n    def train_step(self, data):\n        Xs, Ys_true = data\n\n        with tf.GradientTape() as tape:\n            # Forward pass\n            *Ys_pred, word_mask, sentiment_mask = self(Xs, training=True)  \n\n            # Compute the loss value. Loss function is configured in `compile()`.\n            losses = RACL_losses(Ys_true, Ys_pred, [word_mask, sentiment_mask], self.opt)\n\n        # Backward progagation - Compute gradients & Update weights\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(losses[0], trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        return {\n            'OE_loss': losses[1], 'AE_loss': losses[2], 'SC_loss': losses[3], \n            'Reg_cost': losses[4], 'loss': losses[0], 'lr': self.optimizer.learning_rate,\n        }\n\n    def test_step(self, data):\n        # Unpack the data\n        Xs, Ys_true = data\n\n        # Compute predictions\n        *Ys_pred, word_mask, sentiment_mask = self(Xs, training=False)\n\n        # Compute the loss value\n        losses = RACL_losses(Ys_true, Ys_pred, [word_mask, sentiment_mask], self.opt)\n        return {\n            'OE_loss': losses[1], 'AE_loss': losses[2], 'SC_loss': losses[3], \n            'Reg_cost': losses[4], 'loss': losses[0], \n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RACL(object):\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.mode = 'train' if self.opt.is_training else 'predict'\n        if opt.random_type == 'uniform':\n            self.initializer = GlorotUniform(seed=self.opt.random_seed)\n        else:\n            self.initializer = GlorotNormal(seed=self.opt.random_seed)\n\n        if self.opt.is_training:\n            # Build logger\n            log_dir = os.path.join(opt.logs_path, self.opt.task)\n            if not os.path.isdir(log_dir):\n                os.makedirs(log_dir)\n            filename = os.path.join(log_dir, f'{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}.txt')\n            self.logger = logging.getLogger(filename)\n            self.logger.setLevel(logging.DEBUG)\n            # self.logger.propagate = False\n            self.logger.addHandler(logging.StreamHandler())\n            self.logger.addHandler(logging.FileHandler(filename, 'a'))\n\n            # Log hyper-parameters\n            info = ''\n            for arg in vars(self.opt):\n                info += ('>>> {0}: {1}\\n'.format(arg, getattr(opt, arg)))\n            self.logger.info('{:-^80}\\n{}\\n'.format('Parameters', info))\n\n        # Build checkpoint & tensorboard\n        if opt.is_training:\n            self.ckpt_dir = opt.ckpt_path\n            self.board_dir = os.path.join(opt.output_path, \"tensorboard\")\n            self.viz_dir = os.path.join(opt.output_path, \"visualization\")\n            for dir_ in [self.ckpt_dir, self.board_dir, self.viz_dir]:\n                if not os.path.isdir(dir_):\n                    os.makedirs(dir_)\n\n        # Build model\n        inputs, embeddings, position_att, word_mask, sentiment_mask = self.build_input_block()\n        predictions = list(self.build_RACL_block(embeddings, position_att, word_mask))\n        \n        if self.opt.is_training:\n            model_inputs = [inputs, word_mask, sentiment_mask, position_att]\n            model_outputs = predictions + [word_mask, sentiment_mask]\n            self.model = RACLModel(inputs=model_inputs, outputs=model_outputs, name='RACL')\n    \n            model_summary = log_summary(self.model)\n            self.logger.info(model_summary)\n            self.visualize_architecture()\n        else:\n            predictions_as_prob = self.build_output_block(predictions)\n            self.model = RACLModel(inputs=[inputs, word_mask, position_att], \n                                   outputs=predictions_as_prob, name='RACL')\n            # self.load_weights(opt.ckpt_path)\n            self.model.summary()\n\n    def visualize_architecture(self):\n        plot_model(self.model, to_file=f'{self.opt.model}_{self.mode}.png', dpi=128, show_shapes=True, show_layer_names=True)\n\n    def build_input_block(self):\n        inputs, embeddings = dropoutize_embeddings(self.opt)\n        inputs._name = 'embeddings_concat'\n\n        # Inputs for Masking\n        position_att = Input(shape=(self.opt.max_sentence_len, self.opt.max_sentence_len), name='position_att')\n        word_mask = Input(shape=(self.opt.max_sentence_len,), name='word_mask')\n        sentiment_mask = Input(shape=(self.opt.max_sentence_len,), name='sentiment_mask')\n        return inputs, embeddings, position_att, word_mask, sentiment_mask\n        \n    def build_RACL_block(self, embeddings, position_att, word_mask):\n        # Preprocessing\n        inputs = Dropout(rate=1-self.opt.keep_prob_1, name='inputs_dropout')(embeddings)\n\n        # Shared Features\n        conv_args = {'kernel_size': 1, 'strides': 1, 'padding': 'same', 'activation': 'relu', }\n        Feature_Extractor = Conv1D(filters=self.opt.embedding_dim, name='shared_features', **conv_args)\n        shared_features = Feature_Extractor(inputs)\n        shared_features = Dropout(rate=1-self.opt.keep_prob_1, name='shared_features_dropout')(shared_features)\n\n        # Define repeatable layers in RACL interactions\n        DropBlock_aspect = DropBlock2D(keep_prob=self.opt.keep_prob_2, block_size=3, name='DropBlock2D_aspect')\n        DropBlock_opinion = DropBlock2D(keep_prob=self.opt.keep_prob_2, block_size=3, name='DropBlock2D_opinion')\n        DropBlock_context = DropBlock2D(keep_prob=self.opt.keep_prob_2, block_size=3, name='DropBlock2D_context')\n\n        L2Normalize = L2Norm()\n        Tile = Lambda(lambda x: tf.tile(tf.expand_dims(x, axis=1), [1, self.opt.max_sentence_len, 1]), name='Tiler-in-RACL')\n\n        # We found that the SC task is more difficult than the AE and OE tasks.\n        # Hence, we augment it with a memory-like mechanism by updating the aspect query with the retrieved contexts.\n        # For more details about the memory network, refer to \n        #       https://www.aclweb.org/anthology/D16-1021/ .\n        aspect_inputs, opinion_inputs, context_inputs = [shared_features], [shared_features], [shared_features]\n        aspect_preds, opinion_preds, sentiment_preds = [], [], []\n        context_queries = [shared_features]\n        \n        conv_args['kernel_size'] = self.opt.kernel_size\n        dense_args = {'units': self.opt.n_classes, 'kernel_initializer': self.initializer}\n\n        for interact_i in range(self.opt.n_interactions):            \n            racl_block = RACL_Block(self.opt, L2Normalize, [DropBlock_aspect, DropBlock_opinion, DropBlock_context], Tile,\n                                    conv_args, dense_args, block_id=interact_i)\n            output_preds, output_interacts = racl_block([aspect_inputs[-1], opinion_inputs[-1], context_inputs[-1], context_queries[-1], word_mask, position_att])\n            aspect_pred, opinion_pred, sentiment_pred = output_preds\n            aspect_interact, opinion_interact, context_interact, context_conv = output_interacts\n\n            # Stacking\n            aspect_preds.append(ExpandDim(axis=-1, name=f'aspect_pred-{interact_i}')(aspect_pred))\n            opinion_preds.append(ExpandDim(axis=-1, name=f'opinion_pred-{interact_i}')(opinion_pred))\n            sentiment_preds.append(ExpandDim(axis=-1, name=f'sentiment_pred-{interact_i}')(sentiment_pred))\n\n            aspect_inputs.append(aspect_interact)\n            opinion_inputs.append(opinion_interact)\n            context_inputs.append(context_conv)\n            context_queries.append(context_interact) # update query\n\n        # Multi-layer Short-cut\n        aspect_preds = Concatenate(axis=-1, name='aspect_preds')(aspect_preds)\n        opinion_preds = Concatenate(axis=-1, name='opinion_preds')(opinion_preds)\n        sentiment_preds = Concatenate(axis=-1, name='sentiment_preds')(sentiment_preds)\n        aspect_pred = ReduceDim('mean', axis=-1, name='AE_pred')(aspect_preds)\n        opinion_pred = ReduceDim('mean', axis=-1, name='OE_pred')(opinion_preds)\n        sentiment_pred = ReduceDim('mean', axis=-1, name='SC_pred')(sentiment_preds)\n        return aspect_pred, opinion_pred, sentiment_pred\n\n    def build_output_block(self, preds):\n        aspect_pred, opinion_pred, sentiment_pred = preds\n\n        # Scale probability\n        aspect_prob = Softmax(axis=-1, name='aspect_prob')(aspect_pred)\n        opinion_prob = Softmax(axis=-1, name='opinion_prob')(opinion_pred)\n        sentiment_prob = Softmax(axis=-1, name='sentiment_prob')(sentiment_pred)\n        return aspect_prob, opinion_prob, sentiment_prob\n    \n    def train(self):\n\n        # Load generators\n        train_set = DataGenerator(self.opt.train_path, self.opt, validate=False)\n        val_set = DataGenerator(self.opt.val_path, self.opt, validate=True)\n        test_set = DataGenerator(self.opt.test_path, self.opt, validate=True)\n        n_trains, n_vals, n_tests = len(train_set), len(val_set), len(test_set)\n\n        ################################\n        #     Training Procedure       #\n        ################################\n        Evaluator = ABSA_Evaluation(val_set[0], self.logger, opt=self.opt, include_opinion=self.opt.include_opinion)\n        train_arguments = {\n            'x': train_set,\n            'steps_per_epoch': n_trains,\n            'validation_data': val_set,\n            'validation_steps': n_vals,\n            'verbose': 1,\n            'callbacks': [\n                # ReduceLROnPlateau(monitor='val_loss', factor=0.69, patience=5, min_lr=1e-7, verbose=1),\n                CyclicLR(mode='exponential', base_lr=self.opt.lr//169, max_lr=self.opt.lr, step_size=n_trains*2),\n                # TensorBoard(self.board_dir),\n                ModelCheckpoint(os.path.join(self.opt.ckpt_path, 'RACL-epoch={epoch:03d}.h5'), save_weights_only=True, monitor='loss', verbose=1),\n                Evaluator, \n                # EarlyStopping(monitor=\"val_loss\", patience=11, restore_best_weights=True, verbose=1)\n            ]\n        }\n        self.model.set_opt(self.opt)\n        self.model.compile(optimizer=get_optimizer(self.opt.optimizer, learning_rate=self.opt.lr))\n\n        phases = ['all', 'opinion', 'aspect', 'sentiment', 'all']\n        epochs = [p*self.opt.n_epochs for p in range(len(phases)+1)]\n        histories = []\n\n        for l in range(self.opt.n_loops):\n            self.logger.info(f\"\\n\\tLoop {l+1:03d} / {self.opt.n_loops:03d}\")\n            for p_i, phase in enumerate(phases):\n                self.logger.info(f\"\\n\\t\\tPhase {p_i+1}: Training {phase.upper()} layers ...\")\n                history = self.train_per_phase(initial_epoch=l*self.opt.n_epochs*len(phases)+epochs[p_i], \n                                                      epochs=l*self.opt.n_epochs*len(phases)+epochs[p_i+1], \n                                             train_arguments=train_arguments, \n                                                       phase=phase)\n                histories.append(history)\n                \n                # Update weights for losses\n                self.logger.info(f\"\\n\\t\\tPhase {p_i+1}: Updating loss weights ...\")\n                if p_i >= len(phases)-1:\n                    f1_o, f1_a, _, f1_s, _ = self.evaluate(test_set=val_set)\n                    scores = np.array([f1_a, f1_o, f1_s], dtype=float)\n                    weights = 1 / (scores+K.epsilon())\n                    weights /= np.min(weights)\n                    weights = np.clip(weights, 1., 16.9)\n                else:\n                    next_phase = phases[p_i+1]\n                    if next_phase == 'aspect':\n                        weights = [1.69, 1.00, 1.00]\n                    elif next_phase == 'opinion':\n                        weights = [1.00, 1.69, 1.00]\n                    elif next_phase == 'sentiment':\n                        weights = [1.00, 1.00, 1.69]\n\n                self.opt.aspect_weight = weights[0]\n                self.opt.opinion_weight = weights[1]\n                self.opt.sentiment_weight = weights[2]\n                self.logger.info(f\"\\n\\t\\t\\t aspect_weight = {weights[0]} \\n\\t\\t\\t opinion_weight = {weights[1]} \\n\\t\\t\\t sentiment_weight = {weights[2]}\")\n\n            # Save best weights per phase\n            ckpt_ids_to_keep = [Evaluator.min_loss_index, Evaluator.max_score_index]\n            for ckpt_id, ckpt_type in zip(ckpt_ids_to_keep, ['loss', 'score']):\n                model_ckpt = os.path.join(self.ckpt_dir, f'RACL-epoch={ckpt_id:03d}.h5')\n                self.model.load_weights(model_ckpt)\n                self.model.save_weights(os.path.join(self.ckpt_dir, f'RACL-best-{ckpt_type}-loop={l+1}.h5'))\n\n            # Clean epoch weights\n            ckpt_ids_to_keep = [Evaluator.min_loss_index, Evaluator.max_score_index]\n            for ckpt_file in glob(os.path.join(self.ckpt_dir, 'RACL-epoch=*.h5')):\n                ckpt_id = int(ntpath.basename(ckpt_file)[11:14])\n                if ckpt_id in ckpt_ids_to_keep:\n                    continue\n                if os.path.isfile(ckpt_file):\n                    os.remove(ckpt_file)\n                        \n        # Visualization\n        try:\n            history_fig = plot_history(histories)\n            history_fig.savefig(os.path.join(self.viz_dir, 'training_history.png'))\n\n            for t_i, train_history in enumerate(histories):\n                with open(f'train_history_{t_i}.hst', 'wb') as f_writer:\n                    pickle.dump(train_history.history, f_writer)\n        except Exception:\n            pass\n    \n        # Testing Process\n        self.logger.info('\\n\\tTesting')\n        for ckpt_file in sorted(glob(os.path.join(self.ckpt_dir, 'RACL-best-*.h5'))):\n            scores = self.evaluate(model_ckpt=ckpt_file, test_set=test_set)\n            self.logger.info(f'\\n\\tPrediction by {ntpath.basename(ckpt_file)}')\n            self.logger.info(f'\\t\\topinion_f1={scores[0]:.7f}\\n\\t\\taspect_f1={scores[1]:.7f}\\n\\t\\tsentiment_acc={scores[2]:.7f}\\n\\t\\tsentiment_f1={scores[3]:.7f}\\n\\t\\tABSA_f1={scores[4]:.7f}')\n\n    def train_aspect(self, initial_epoch: int, epochs: int, train_arguments: dict):\n        for layer in self.model.layers:\n            if 'aspect' in layer.name.lower():\n                layer.trainable = True\n                self.logger.info(f\"\\t\\t\\t{layer.name}\")\n            else:\n                layer.trainable = False\n        history = self.model.fit(initial_epoch=initial_epoch, epochs=epochs, **train_arguments)\n        return history\n\n    def train_opinion(self, initial_epoch: int, epochs: int, train_arguments: dict):\n        for layer in self.model.layers:\n            if 'opinion' in layer.name.lower():\n                layer.trainable = True \n                self.logger.info(f\"\\t\\t\\t{layer.name}\")\n            else:\n                layer.trainable = False\n        history = self.model.fit(initial_epoch=initial_epoch, epochs=epochs, **train_arguments)\n        return history\n\n    def train_sentiment(self, initial_epoch: int, epochs: int, train_arguments: dict):\n        for layer in self.model.layers:\n            if any(ss in layer.name.lower() for ss in ['sentiment', 'context']):\n                layer.trainable = True\n                self.logger.info(f\"\\t\\t\\t{layer.name}\")\n            else:\n                layer.trainable = False\n        history = self.model.fit(initial_epoch=initial_epoch, epochs=epochs, **train_arguments)\n        return history\n\n    def train_all(self, initial_epoch: int, epochs: int, train_arguments: dict):\n        for layer in self.model.layers:\n            layer.trainable = True\n        history = self.model.fit(initial_epoch=initial_epoch, epochs=epochs, **train_arguments)\n        return history\n\n    def train_per_phase(self, initial_epoch: int, epochs: int, train_arguments: dict, phase: str='all'):\n\n        phase = phase.lower()\n        if phase == 'embedding':\n            history = self.train_embedding(initial_epoch, epochs, train_arguments)\n        elif phase == 'aspect':\n            history = self.train_aspect(initial_epoch, epochs, train_arguments)\n        elif phase == 'opinion':\n            history = self.train_opinion(initial_epoch, epochs, train_arguments)\n        elif phase == 'sentiment':\n            history = self.train_sentiment(initial_epoch, epochs, train_arguments)\n        else:\n            history = self.train_all(initial_epoch, epochs, train_arguments)\n        return history\n\n    def evaluate(self, model_ckpt='', test_set: DataGenerator=None):\n        # Load generator\n        if not isinstance(test_set, DataGenerator):\n            test_set = DataGenerator(self.opt.test_path, self.opt, validate=True)\n\n        # Load weights\n        if model_ckpt != '' and os.path.isfile(model_ckpt):\n            self.model.load_weights(model_ckpt)\n\n        # Evaluate\n        Xs, Ys_true = test_set[0]\n        *Ys_pred, word_mask, _ = self.model.predict(Xs)\n        scores = evaluate_absa(Ys_true[0], Ys_pred[0],\n                               Ys_true[1], Ys_pred[1],\n                               Ys_true[2], Ys_pred[2],\n                               word_mask, include_opinion=self.opt.include_opinion)\n        return scores\n\n    def predict(self, sentence, word_mask, position_att):\n        ae_pred, oe_pred, sc_pred = self.model.predict([sentence, word_mask, position_att])\n        return ae_pred, oe_pred, sc_pred\n\n    def load_weights(self, weights_path):\n        if not os.path.isfile(weights_path):\n            raise FileNotFoundError(f\"weights_path:\\n\\t{weights_path}\\ndoesn't exist!\")\n        try:\n            self.model.load_weights(weights_path)\n        except Exception as e:\n            print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **train.py**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport json\nimport time\nimport argparse\nfrom pathlib import Path\n\nimport random\nimport numpy as np\nimport tensorflow as tf\ntf.autograph.set_verbosity(3) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_config(config_path: str):\n    \"\"\"\n    load config file (yaml)\n\n        Parameters\n        ----------\n            config_path: config path to yaml file\n\n        Returns\n        -------\n        config : dict\n    \"\"\"\n\n    logger.info(f\" Loading config from {config_path}...\")\n    with open(config_path, 'r') as f:\n        config = yaml.full_load(f)\n    \n    return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_basic_arguments(parser):\n    # Define arguments\n    parser.add_argument('--model', default='racl', type=str, help='model name')\n    parser.add_argument('--task', default='hotel', type=str, help='task name')\n    parser.add_argument('--n_loops', default=2, type=int, help='number of loops to repeat `n_epochs` for training procedure')\n    parser.add_argument('--n_epochs', default=10, type=int, help='number of epochs for training per phase')\n    parser.add_argument('--batch_size', default=64, type=int, help='number of samples per batch')\n    parser.add_argument('--lr', default=0.00069, type=float, help='learning rate')\n    parser.add_argument('--max_sentence_len', default=256, type=int, help='maximum number of words in sentence')\n    parser.add_argument('--embedding_dim', default=768, type=int, help='embedding dimension')\n    parser.add_argument('--n_interactions', default=6, type=int, help='number of RACL blocks to interact')\n    parser.add_argument('--keep_prob_1', default=.97, type=float, help='keep prob for inputs')\n    parser.add_argument('--keep_prob_2', default=.97, type=float, help='keep prob for tasks')\n    parser.add_argument('--n_filters', default=96, type=int, help='number of filters in convolution')\n    parser.add_argument('--kernel_size', default=13, type=int, help='kernel size in convolution')\n    parser.add_argument('--optimizer', default='adam', type=str, help='optimizer for model | default: SGD (Stochastic Gradient Descent)')\n    parser.add_argument('--random_type', default='normal', type=str, help='random type: uniform or normal (default)')\n    parser.add_argument('--random_seed', default=4_10_20, type=int, help='random seed')\n    parser.add_argument('--aspect_weight', default=1., type=float, help='weight of aspect loss')\n    parser.add_argument('--opinion_weight', default=1., type=float, help='weight of opinion loss')\n    parser.add_argument('--sentiment_weight', default=1.69, type=float, help='weight of sentiment loss')\n    parser.add_argument('--regularization_weight', default=1e-4, type=float, help='weight of regularization loss')\n    parser.add_argument('--label_smoothing', default=True, type=bool, help='label smoothing for regularization')\n    parser.add_argument('--load_pretrained', default=False, type=bool, help='whether to load an existing checkpoint')\n    parser.add_argument('--include_opinion', default=True, type=bool, help='whether to use opinion for model')\n    opt = parser.parse_known_args()[0]\n\n    opt.n_classes = 3\n    opt.is_training = True\n\n    opt.term_weights = [.2, .5, .3] # Outside-Beginning-Inside\n    opt.polarity_weights = [.3, .3, .4] # Positive-Negative-Neutral\n\n    random.seed(opt.random_seed)\n    np.random.seed(opt.random_seed)\n    tf.random.set_seed(opt.random_seed)\n    return opt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(parser, args):\n    \n    opt = load_basic_arguments(parser)\n        \n    # Assign pre-defined paths\n    opt.logs_path = \"./logs\"\n    opt.ckpt_path = \"./checkpoint\"\n    opt.output_path = \"./output\"\n    opt.evaluate_path = \"./evaluate\"\n    opt.prediction_path = \"./predictions\"\n    opt.train_path = \"../input/absa-hotel-distiluse/dataset/train\"\n    opt.test_path = \"../input/absa-hotel-distiluse/dataset/test\"\n    opt.val_path = \"../input/absa-hotel-distiluse/dataset/val\"\n\n    for path in [opt.logs_path, opt.output_path, opt.ckpt_path, opt.evaluate_path, opt.prediction_path]:\n        if not os.path.isdir(path):\n            os.makedirs(path)\n\n    # Train\n    start_time = time.time()\n    model = RACL(opt)\n    # model.visualize_architecture()\n    model.load_weights(\"../input/raclhotel-distiluse-256l/RACL-epoch200.h5\")\n    model.train()\n    end_time = time.time()\n    time_running = end_time - start_time\n    run_hours = int(time_running//3600)\n    run_minutes = int((time_running-run_hours*3600)//60)\n    run_seconds = int(time_running - run_hours*3600 - run_minutes*60)\n    run_time = f'\\n\\n\\nTraining in {run_hours}h {run_minutes}m {run_seconds}s'\n    model.logger.info(run_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='Model Training')\nparser.add_argument('-c', '--config-path', default='../input/absa-hotel-distiluse/model_config.yml', type=str, help='Config path')\nargs = parser.parse_known_args()[0]\n\ntrain(parser, args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# os.chdir(r'/kaggle/working')\n# dir_path = '/kaggle/working/'\n# shutil.make_archive(dir_path+\"data\", 'zip', dir_path)\n# shutil.rmtree('/kaggle/working/checkpoint')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}