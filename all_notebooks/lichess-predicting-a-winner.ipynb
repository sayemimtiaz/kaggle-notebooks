{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing relevant modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_style('darkgrid')\ndf = pd.read_csv('/kaggle/input/chess/games.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Couple observations can be made:\n\n1. Columns `id`, `white_id`, `black_id` are not important.\n\n2. Either column `created_at` or `last_move_at` must contain some errors. Per [description](https://www.kaggle.com/datasnaek/chess) provided by the uploader, `created_at` stands for the time when the game began, and `last_move_at` stands for the time when the game ended. Hence $($ `last_move_at` $-$ `created_at` $)$ must represent a duration of the game. But according to the dataset, for some games, the duration is $0$ (which, given the cirumstances, is not possible). For example, consider the game with `id` TZJHLljE (first row in the dataset). The value in the column `created_at` is 1.504210e+12. And the value in `last_move_at` is also 1.504210e+12. But then it follows that (1.504210e+12)-(1.504210e+12)=0, implying that the duration of the game is zero. But also note that according to the dataset, the game ended because either of two players ran out of time (the value in the column `victory_status` equals to \"outoftime\"), which clearly contradicts the fact that the duration of the game was 0 seconds (or whatever metric was used to calculate time). Hence it is likely that either of two columns contains false number. In fact, we can see that there are numerous games that purportedly had zero duration:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dur = df[df['last_move_at']-df['created_at'] == 0].shape[0]\nprint(f'Number of games that had zero duration: {dur}, which makes \\\nup {round(dur/df.shape[0],2)*100}% of all games')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The upshot is: We should remove the columns `last_move_at` and `created_at`."},{"metadata":{},"cell_type":"markdown","source":"There is one more thing: column `moves` should be removed. If particular game was ended by a check mate, then by using data from the column `moves` we can say with 100% certainty who won (esentially with data in `moves` we can reconstruct the whole game). Our main goal is to see whether we can accurately predict a winner based on LIMITED information."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id','white_id','black_id','last_move_at','created_at','moves']\ndf.drop(cols_to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the label distribution (i.e., `winner`)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['winner'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['winner'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this analysis, we will restrict our attention only to those games where there is a winner (i.e., we will not conisder draws)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = df[df['winner'] != 'draw']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at categorical features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cat_features = np.array(['rated','victory_status','increment_code','opening_eco','opening_name'])\ncount = np.array([df[feature].unique().size for feature in cat_features])\n\nto_sort = np.argsort(count)[::-1]\ncat_features = cat_features[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\nsns.barplot(cat_features,count)\nplt.title(\"Number of unique values per each feature\")\nplt.ylabel('Count')\nplt.xlabel('Feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most of the features have very high cardinality."},{"metadata":{},"cell_type":"markdown","source":"For each feature, we will calculate how many values occur only once."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cat_features = np.array(['rated','victory_status','increment_code','opening_eco','opening_name'])\ncount = []\n\nfor feature in cat_features:\n    freq = 0\n    for value in df[feature].unique():\n        if df[df[feature] == value].shape[0] == 1:\n            freq+=1\n    count.append(freq)\n    \npd.DataFrame({'Feature': cat_features, 'Count of rare values': count})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, features with high cardinality have a lot of extremely rare values (i.e., values that occur only once)."},{"metadata":{},"cell_type":"markdown","source":"Now we will have a look at continuous features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cont_features = ['white_rating','black_rating','opening_ply']\ndf[cont_features].describe().round(2).T","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cont_features = ['white_rating','black_rating','opening_ply']\nWIDTH = 16\nLENGTH = 7\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cont_features = ['white_rating','black_rating','opening_ply']\ncat_variable = 'winner'\nWIDTH = 16\nLENGTH = 7\n\nrows = math.ceil(len(cont_features)/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features alone doesn't seem to be doing good job at separating the winner. Let's use ANOVA test to verify independence."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import f_oneway\n\ncont_features = ['white_rating','black_rating','opening_ply']\nlabel = 'winner'\n\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    'statistic': []}\n\n\nfor feature in cont_features:\n    values = []\n    for value in df[label].unique():\n        values.append(df[df[label] == value][feature].values)\n    \n    statistic, pval = f_oneway(*values)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['statistic'].append(statistic)\n\n\npd.DataFrame(dic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rather unexpectedly, the ANOVA test suggests that each feature and target variable are in fact dependent."},{"metadata":{},"cell_type":"markdown","source":"Let's try to define new feature: difference between white_rating and black_rating, i.e., `white_rating`-`black_rating`."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df['rating_diff'] = df['white_rating']-df['black_rating']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One can see that, the larger the value in `rating_diff`, the more advantage (rating-wise) white side has over black side. Let's see the summary of our new variable and the distribution."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['rating_diff'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature_1 = 'blueWins'\nfeature_2 = 'rating_diff'\nplt.figure(figsize=(7,7))\nsns.boxplot(y=feature_2, data=dataframe)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.hist(df['rating_diff'],alpha=0.6)\nplt.title(\"Difference between white rating and black rating\")\nplt.xlabel('difference')\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As wee see, in most cases, games are relatively fair (i.e., both players have similar rating; more concretely $|\\text{white_rating} - \\text{black_rating}| â‰¤ 500$). But there is decent number of games where the discrepancy is relatively large.\n\nNow, let's have a look at how our new feature discerns the winner of the game."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature_1 = 'winner'\nfeature_2 = 'rating_diff'\nplt.figure(figsize=(7,7))\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected: The smaller the value (i.e., black side has higher rating), the less likely it is that white side will win (and vice versa). Let's use ANOVA to check whether there is a statistical signifance of the difference between the conditional distributions. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import f_oneway\n\ncont_features = ['rating_diff']\nlabel = 'winner'\n\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    'statistic': []}\n\n\nfor feature in cont_features:\n    values = []\n    for value in df[label].unique():\n        values.append(df[df[label] == value][feature].values)\n    \n    statistic, pval = f_oneway(*values)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['statistic'].append(statistic)\n\n\npd.DataFrame(dic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the difference between ratings is indeed a good predictor of a winner."},{"metadata":{},"cell_type":"markdown","source":"Now we will try to classify. The features we will be using are the folliwng:\n\n'increment_code', 'opening_eco', 'opening_name', 'rated', 'victory_status',\n                    'turns', 'white_rating', \n                   'black_rating', 'opening_ply', \n                   'rating_diff'"},{"metadata":{},"cell_type":"markdown","source":"One hot encoding features with high cardinality"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"highcardf = df[['increment_code','opening_eco','opening_name']].copy()\nsparse_high = OneHotEncoder().fit_transform(highcardf)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing continuous/non-high cardinality categorical features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy import sparse\n\n\nnohighcar_df = df[['rated','victory_status',\n                   'turns','white_rating', \n                   'black_rating', 'opening_ply', \n                   'rating_diff']].copy()\n\n#Process categorical features\nnohighcar_df['rated'] = nohighcar_df['rated'].map({False: 0, True:1})\nnohighcar_df = pd.get_dummies(nohighcar_df)\n\n\n#Move all numerical features to the right of the dataframe\nnum_feat = df[['turns', 'white_rating', 'black_rating', 'opening_ply',\n       'rating_diff']]\nnohighcar_df.drop(['turns', 'white_rating', 'black_rating', 'opening_ply',\n       'rating_diff'],axis=1,inplace=True)\nnohighcar_df = pd.concat([nohighcar_df,num_feat],axis=1)\n\nnohigh_sparse = sparse.csr_matrix(nohighcar_df.values)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X,y = sparse.hstack((sparse_high,nohigh_sparse)), df['winner']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=11)\n\n\nsc = StandardScaler()\n\n\n\nleft = X_train[:,:-5]\nright = sparse.csr_matrix(sc.fit_transform(X_train[:,-5:].todense()))\nX_train = sparse.hstack((left,right)).tocsr()\n\n\nleft = X_test[:,:-5]\nright = sparse.csr_matrix(sc.transform(X_test[:,-5:].todense()))\nX_test = sparse.hstack((left,right)).tocsr()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having prepared our data, let's try to classify. We will only use two models here: Logistic regression and decision trees."},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"log_random_state = None\nlog_clf = LogisticRegression(random_state=log_random_state,max_iter=500).fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=log_clf.predict(X_test)))\nplot_confusion_matrix(log_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision trees"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tree_clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=tree_clf.predict(X_test)))\nplot_confusion_matrix(tree_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that even relatively simple model (with no hyperparameters tuning) gives us reasonable results."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}