{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis is meant to be a very brief introduction to the Machine Learning work flow in building models.\nThe models done here are in no way state-of-the art or very usefull, but it demonstrates the workflow of data processing, feature engineering, model building and verification."},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing and feature engineering\nThis will probably take 90% of the ML time..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\ndataset = pd.read_csv('/kaggle/input/beer-efficiency/beer_efficiency.csv')\ndata = dataset.copy()\n\n#Remove any rows with NAN values\ndata = data.dropna(subset=['name','calories','abv','efficiency'])\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the data\ndef plot_hist(df, col):\n    plt.hist(data[col], bins=75)\n    plt.xlabel(col)\n    plt.ylabel('count')\n    plt.show()\n    \nplot_hist(data, 'calories')\nplot_hist(data, 'abv')\nplot_hist(data, 'efficiency')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find a list of words used in beer names\nwords = {}\nfor i, row in data.iterrows():\n    name = row['name'].split(' ')\n    for word in name:\n        if words.get(word,0) == 0:\n            words[word] = 1\n        else:\n            words[word] += 1\n\nword_count = pd.DataFrame.from_dict(words, orient='index').sort_values(by=0, ascending=False)\nword_count = word_count.rename(columns={0:'count'})\nword_count[word_count['count']>=5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select words that signify a beer type (and have a significant amount of entries in sample names)\n\nbeer_types = ['Stout', 'Wheat', 'Porter','Pale','Light','Lager','IPA', 'Beer', 'Ale', 'Amber']\n\n# Turn categorical beer descriptors into values by one-hot-encoding. Large sparse matrix usually... \nfor beer_type in beer_types:\n    data[beer_type] = data['name'].str.contains(beer_type)*1\n\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare dataset for modelling\nSeparate dataset into training and testing parts and scale/normalize\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfeature_cols = beer_types + ['calories', 'abv']\nlabel_col = 'efficiency'\n\nX = data[feature_cols]\ny = data[label_col]\n\n\n# Divide data into training and tes\ntest_fraction = 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=test_fraction,\n                                                    shuffle=True,\n                                                    random_state=None)\n\n# Data normalization/scaling - This helps model fit converging and can give better models\n# Scaling ONLY based on training data, such that test data remains hidden from model\nscaler = StandardScaler().fit(X_train) \nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nXtrain = X_train_scaled\nXtest = X_test_scaled\n\nn_features = X_train.shape[1]\nn_train = X_train.shape[0]\nn_test = X_test.shape[0]\n\nprint(f'{n_features} training features:', feature_cols)\nprint('Training data examples:', n_train)\nprint('Testing data examples:', n_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear regression\n\nLets try the most basic linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \n\nlr_model = LinearRegression(fit_intercept=True)\n\nlr_model.fit(Xtrain, y_train)\n\nw = lr_model.coef_\nb = lr_model.intercept_\nlinear_coefficients = w\nprint('Linear feature weights w:\\n', w)\nprint('Bias/intercept term:\\n', b)\nplt.bar(feature_cols, w)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score on training data\nprediction_train = lr_model.predict(Xtrain)\nRMSE_train = np.round(np.dot(y_train - prediction_train, y_train - prediction_train)**(1/2)/n_train, 2)\nR2_train = np.round(lr_model.score(Xtrain, y_train),2)\n\nprediction_test = lr_model.predict(Xtest)\nRMSE_test = np.round(np.dot(y_test - prediction_test, y_test - prediction_test)**(1/2)/n_test, 2)\nR2_test = np.round(lr_model.score(Xtest, y_test),2)\n\nprint(f'Training R2 and RMSE: ({R2_train}, {RMSE_train})')\nprint(f'Test R2 and RMSE: ({R2_test}, {RMSE_test})')\n\nerror_rel_train = (y_train-prediction_train)/y_train\nerror_rel_test = (y_test-prediction_test)/y_test\nplt.plot(error_rel_train,'b.', label='Training')\nplt.plot(error_rel_test,'r*',label='Test')\nplt.axhline(0)\nplt.ylabel('Relative prediction error')\nplt.legend()\nplt.show()\n\ny = [error_rel_train, error_rel_test]\nplt.boxplot(y,labels=['Training','Test'])\nplt.axhline(0)\nplt.ylabel('Relative prediction error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic classification\nLets try to just classify the beers as efficient or not-effecint. If the effeciency is over 70% is is deemed effecient, if below it's deemed non-effecient.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix\n\n# Turn continous labels into discrete classes\n# over 70% effeciency is classified as \"Effecient\"\ny_train_class = (y_train >= 70)*1 \ny_test_class = (y_test >= 70)*1\n\n# Define model\nlog_model = LogisticRegression(max_iter=1000)\nlog_model.fit(Xtrain,y_train_class)\n\nw =  log_model.coef_[0]\nprint('Linear feature weights w:\\n', w)\nprint('Bias/intercept term:\\n', log_model.intercept_)\nplt.bar(feature_cols, w)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score on training and test data\ny_pred_train = log_model.predict(Xtrain)\ny_pred_test = log_model.predict(Xtest)\naccuracy_train = balanced_accuracy_score(y_train_class, y_pred_train)\naccuracy_test = balanced_accuracy_score(y_test_class, y_pred_test)\ncm_train = confusion_matrix(y_train_class, y_pred_train, labels=[1, 0])\ncm_test = confusion_matrix(y_test_class, y_pred_test, labels=[1, 0])\n\nprint('Training accuracy:', accuracy_train)\nprint('Confusion Matrix\\n',cm_train)\nprint('')\nprint('Testing accuracy:', accuracy_test)\nprint('Confusion Matrix\\n',cm_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyper parameter tuning\nacc_train = []\nacc_test = []\nc_test = [0.01, 0.1, 1, 2,3,4,5,6,7,8,9,10]\n\nfor c in c_test:\n    log_model = LogisticRegression(penalty='l2',\n                                   tol=0.00001,\n                                   C=c,\n                                   class_weight='balanced',\n                                   fit_intercept=True,\n                                   random_state=None,\n                                   max_iter=1000)\n    \n    log_model.fit(Xtrain,y_train_class)\n    \n    y_pred_train = log_model.predict(Xtrain)\n    acc_train.append(balanced_accuracy_score(y_train_class, y_pred_train))\n    \n    y_pred_test = log_model.predict(Xtest)\n    acc_test.append(balanced_accuracy_score(y_test_class, y_pred_test))\n\nplt.plot(c_test,acc_train, 'b-o',label='Train')\nplt.plot(c_test,acc_test, 'r-*', label='Test')\nplt.axhline(1)\nplt.axvline(1)\nplt.legend()\nplt.xlabel('C - Regularization parameter')\nplt.ylabel('Model accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bestmodel\n\nbest_model = LogisticRegression(penalty='l2',\n                                   tol=0.00001,\n                                   C=2,\n                                   class_weight=None,\n                                   fit_intercept=True,\n                                   random_state=None,\n                                   max_iter=1000)\n\nbest_model.fit(Xtrain,y_train_class)\n\n# Model score on training and test data\ny_pred_train = log_model.predict(Xtrain)\ny_pred_test = log_model.predict(Xtest)\naccuracy_train = balanced_accuracy_score(y_train_class, y_pred_train)\naccuracy_test = balanced_accuracy_score(y_test_class, y_pred_test)\ncm_train = confusion_matrix(y_train_class, y_pred_train, labels=[1, 0])\ncm_test = confusion_matrix(y_test_class, y_pred_test, labels=[1, 0])\n\nprint('Training accuracy:', accuracy_train)\nprint('Confusion Matrix\\n',cm_train)\nprint('')\nprint('Testing accuracy:', accuracy_test)\nprint('Confusion Matrix\\n',cm_test)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}