{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n#### Experiment Approach\n- **Null Hypothesis** Hₒ: p = pₒ          \"There is no significant difference between the ad success rate of both groups\"\n- **Alternative Hypothesis** Hₐ: p ≠ pₒ   \"There is significant difference between the ad success rate of both groups\"\n    - Given we don’t know if the new design will perform better/worse/equal as our current design, we will perform a two-tailed test\n    - **Confidence Level**: 95% (α=0.05)\n    - p and pₒ stand for the conversion rate of the new and old design We’ll also set a confidence level of 95%\n    \n#### Business Objectives\n- The company expects the new ad startegy to increase ad success from 45% to 50% (+5pp / +11%).\n\n#### Columns Description\n- **auction_id**: the unique id of the online user who has been presented the BIO. In standard terminologies this is called an impression id. The user may see the BIO questionnaire but choose not to respond. In that case both the yes and no columns are zero.\n- **experiment**: which group the user belongs to - control or exposed.\n    - ***control***: users who have been shown a dummy ad\n    - ***exposed***: users who have been shown a creative, an online interactive ad, with the SmartAd brand.\n\n\n- **date**: the date in YYYY-MM-DD format\n- **hour**: the hour of the day in HH format.\n- **device_make**: the name of the type of device the user has e.g. Samsung\n- **platform_os** : the id of the OS the user has.\n- **browser**: the name of the browser the user uses to see the BIO questionnaire.\n- **yes**: 1 if the user chooses the “Yes” radio button for the BIO questionnaire.\n- **no**: 1 if the user chooses the “No” radio button for the BIO questionnaire.","metadata":{}},{"cell_type":"markdown","source":"## Package & Data Imports","metadata":{}},{"cell_type":"code","source":"# Programming\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Statistics\nimport statsmodels.stats.api as sms\nfrom statsmodels.stats.proportion import proportions_ztest, proportion_confint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Import\ndf = pd.read_csv('/kaggle/input/ad-ab-testing/AdSmartABdata - AdSmartABdata.csv', parse_dates=['date'])\nprint(df.shape)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"In order to preprocess the data we will proceed with the following steps:\n- **Converting 'auction_id' to df index** This will allow us to index for specific observations if necessary.\n- **Check for null values** to validate that we work with observations that have actionable data.\n- **Remove non-answer observations** (both 'yes' and 'no' columns are equal to 0). This may remove a significant percentage of the observations, but non-answers are not useful for our analysis as we can not infer if the ad was successful or not. We will also unify the 'yes' and 'no' columns in a single column where 1 == ad_success and 0 == ad_failure","metadata":{}},{"cell_type":"code","source":"# Making the auction_id feature our index\ndf = df.set_index('auction_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for nulls\ntotal_nulls = df.isnull().sum()\nprint('Null values:')\nprint(total_nulls)\nprint('')\n\n# Removing non-answers\nad_success =[]\nfor x, y in zip(df.yes, df.no):\n    if (x == 1) and (y == 0):\n        ad_success.append(1)\n    elif (x == 0) and (y == 1):\n        ad_success.append(0)\n    else:\n        ad_success.append('no_response')\ndf['ad_success'] = ad_success\ndf = df.loc[~df.ad_success.isin(['no_response'])]\ndf = df.drop(['yes', 'no'], axis = 1)\n\n# Check and codify categorical values\n    # Checked the values within each column and coded them based on the most common values\n    # device_make_list = df.device_make.unique()\n    # browser_list = df.browser.unique()\ndevice_list_codified = []\nfor x in df.device_make:\n    if 'Samsung' in x:\n        device_list_codified.append(1)\n    elif 'iPhone' in x:\n        device_list_codified.append(2)\n    else:\n        device_list_codified.append(0)\ndf.device_make = device_list_codified\n\nbrowser_list_codified = []\nfor x in df.browser:\n    if 'Chrome' in x:\n        browser_list_codified.append(1)\n    elif 'Safari' in x:\n        browser_list_codified.append(2)\n    else:\n        browser_list_codified.append(0)\ndf.browser = browser_list_codified\n\ndel ad_success, device_list_codified, browser_list_codified\n\n# Prinitng processed df\nprint('# Observations: {}'.format(df.shape[0]))\nprint('# Variables: {}'.format(df.shape[1]))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\nWe will perform some checks to validate that both groups are representative of the total population, and thus ensuring that the differences on ad success are not caused by other factors.","metadata":{}},{"cell_type":"code","source":"# Sample sizes\nsns.set(rc={'figure.figsize':(10,5)})\nsns.countplot(x='experiment', data=df)\nplt.title('Count of Observations per Group')\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distributions for categorical variables\nsns.set(rc={'figure.figsize':(30,10)})\nfig, ax = plt.subplots(1,5)\nsns.kdeplot(x='device_make', hue='experiment', data=df, ax=ax[0])\nsns.kdeplot(x='browser', hue='experiment', data=df, ax=ax[1])\nsns.kdeplot(x='platform_os', hue='experiment', data=df, ax=ax[2])\nsns.kdeplot(x='hour', hue='experiment', data=df, ax=ax[3])\nsns.kdeplot(x='date', hue='experiment', data=df, ax=ax[4])\nplt.xticks(rotation=90)\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dfs for each group\ndf_control = df[df.experiment =='control']\ndf_exposed = df[df.experiment =='exposed']\n\n# Computing mean (as success == 1 and failure == 0 the mean is effectively our success rate)\nmean_success_control = df_control.ad_success.mean()\nmean_success_exposed = df_exposed.ad_success.mean()\n\n# Printing results\nprint('Ad Success Control group {}%'.format((mean_success_control*100).round(2)))\nprint('Ad Success Exposed group {}%'.format((mean_success_exposed*100).round(2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### EDA Conclusions\n- **Control and Experiment properly represent the population** This can be seen on the fact that 'device_make', 'browser', 'platform_os' present almost equal distributions for both results. This is a good sign, as it shows that the population partition between the control and exposed group has been performed in a way where both groups are representative of the total population.\n- **Hour and Date differences** We can see that the hour and date distributions do present siginificant differences, which can probably be explained by when was the ad implmented (we are missing info on the experiment implementations here),\n- **Ad success is higher on the exposed group by 4% / 1.83pp**","metadata":{}},{"cell_type":"markdown","source":"## Statistical Significance","metadata":{}},{"cell_type":"code","source":"# Computing the difference (improvement) we want to obtain. From 45% 'ad_success' to 50%\neffect_size = sms.proportion_effectsize(0.45, 0.50)\n\n# Computing the needed sample size (per group) to ensure that we capture siginificant differences\n    # Effect_size = The difference you want to observe\n    # Power = The probability that we will capture an existing difference. 0.8 is standard practice\n    # Alpha = alpha value for your desired statistical significance\nrequired_n = np.ceil(sms.NormalIndPower().solve_power(effect_size, power=0.8, alpha=0.05, ratio=1))\n\n# Printing required sample\nprint(f'Number of observations needed by group: {int(required_n)}')\nprint(f'Number of total observations on dataset: {df.shape[0]}')\nprint('')\n\n# Counting successes on each group\nad_succes_count = [df_control.ad_success.sum(), df_exposed.ad_success.sum()]\n\n# Counting observations on each group\nobs_count = [df_control.ad_success.count(), df_exposed.ad_success.count()]\n\n# Computing p-value of the ad_success distribution\nz_stat, pval = proportions_ztest(ad_succes_count, nobs=obs_count)\n\n# Computing 95% confidence intervals\n(l_ci_con, l_ci_exp), (u_ci_con, u_ci_exp) = proportion_confint(ad_succes_count, nobs=obs_count, alpha=0.05)\n\n# Prinitng results\nprint('The p-value of ad_success is {}'.format(pval.round(4)))\nprint(f'The 95% CI for ad_success on the control group is [{l_ci_con.round(4)}, {u_ci_con.round(4)}]')\nprint(f'The 95% CI for ad_success on the exposed group is [{l_ci_exp.round(4)}, {u_ci_exp.round(4)}]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n#### Statistical Conclusion\n- The p-value obtained for 'ad_success' (0.5185) is way above the decided threshold (95%, α=0.05) and thus we ***can not reject the 'Null Hypothesis' (Hₒ: p = pₒ)*** , which stated that there is no significant difference between the two groups.\n- Once we remove from the dataset observations with no answers (both 'yes' and 'no' columns == 0) we are only left with 1243 observations (-6834 obs / -84.61%). This significant loss of data causes a lack of observations to ensure that significant differences are detected.\n\n#### Business Conclusion\n- The fact that the observed increase on 'ad_success' is not significant means that we can not disprove that the apparently better result was just chance. This indicates that the **differences between the 'dummy ad' shown to the 'control' group, and the 'creative ad' shown to the 'exposed' group do not convert into better ad performance**. This findings indicate that there is no solid business reason to push the implementation of the new ad design over the old one, as it will yield no extra benefit.\n- **The target of 50% has not been achieved with the new add**. Ad success has not achieved the goal we had set up. Based on the results only on the best possible day the 50% target would be met, as indicated by the [0.4306, 0.507] 95% CI. Considering the almost idetincal results can be observed on the 95% CI for the existing ad ([0.4102, 0.4908]) we see no reason to implement the new ad strategy.","metadata":{}}]}