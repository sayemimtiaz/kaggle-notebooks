{"nbformat_minor":0,"metadata":{"language_info":{"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"cells":[{"outputs":[],"cell_type":"markdown","source":"Introduction\n-------\nIn the below, I will test the ability of various classifiers at predicting a certain wine variety from UCI's dataset. The idea is to single out a particular wine variety (class 1) and attempt to identify it from the other 2 classes, using a small selection of predictor variables from the data set. \n\nFor the purposes of this notebook, I was less interested in the importance of variable selection and more interested in the relative performance of the different classifiers, using the same variables. Hence, I have picked just 2 predictor variables (alcohol and flavanoids content) and evaluated models on the basis of accuracy and other metrics from the confusion matrices such as Sensitivity, Specificity and Precision.\n\nThe classifiers I have compared are:\n\n - *Logistic Regression* (for an example of a model which provides a predictability score and has options for tuning parameters)\n - *Support Vector Machine* (for an example of a strong linear classifier applied to an \"almost linearly separable\" scenario)\n - *Decision Tree* (for an example of a non-parametric learner)\n - *AdaBoost ensemble* method (for an example of an ensemble method of weak learners)\n - *Multi-Layer Perceptron* (for an example of a neural network model more suited to high-dimensional, non-linear examples)\n\n## Summary \nThe results for the optimized models are as follows:\n\n**Confusion Matrix Metrics**\n\n - *Logistic Regression* -          Accuracy:=0.92958, Sensitivity:=0.81818, Specificity:=0.97959, Precision:=0.94737 \n - *Support Vector Machine* - Accuracy:=0.98592, Sensitivity:=1.00000, Specificity:=0.97959, Precision:=0.95652\n - *Decision Tree* -                    Accuracy:=0.95775, Sensitivity:=0.86364, Specificity:=1.00000, Precision:=1.00000\n - *AdaBoost ensemble*  -        Accuracy:=0.95775, Sensitivity:=0.86364, Specificity:=1.00000, Precision:=1.00000 \n - *Multi-Layer Perceptron*  -  Accuracy:=0.98592, Sensitivity:=0.95455, Specificity:=1.00000, Precision:=1.00000\n\n**Cross Validated Accuracy**\n\n - *Logistic Regression* -          Cross Validated Accuracy:= 0.91667 +/- 0.09044\n - *Support Vector Machine* - Cross Validated Accuracy:= 0.92778 +/- 0.07049\n - *Decision Tree* -                    Cross Validated Accuracy:= 0.91667 +/- 0.10015\n - *AdaBoost ensemble*  -        Cross Validated Accuracy:= 0.92222 +/- 0.10000\n - *Multi-Layer Perceptron*  -  Cross Validated Accuracy:= 0.95556 +/- 0.05443\n\nLogistic Regression performed worst overall, having the lowest scores on all confusion matrix metrics and a cross-validated accuracy that was joint worst with the Decision Tree.\n\nThe Decision Tree and the AdaBoost ensemble performed identically. This makes sense given that the AdaBoost ensemble uses Decision Trees as base classifiers.\n\nThe SVM achieved the joint highest test accuracy and out-performed the Logistic Regression model on all scores, except for tying with Specificity. \n\nThe MLP was more specific and precise than the SVM but less sensitive and they both had the same test accuracy. However, the cross-validated accuracy for the MLP is greater and had fairly low variance. \n\nThe MLP was the best performing model, overall, and we may expect this, given that the classes are not perfectly linearly separable and the neural network is more suited to non-linear classification tasks.","metadata":{"_cell_guid":"3b2f5ee7-7e22-4d1c-bc0e-bcf0566f0762","collapsed":false,"_uuid":"4d657bd6daa217647cfead38fe7624a8e2486af7","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"Importing libraries and the wine data set\n----------------------------------------","metadata":{"_cell_guid":"d1afcbb2-c720-4680-99fe-76c1d5d7e515","collapsed":false,"_uuid":"73332d9e1938713b28ddddd3e2a76cf4cb008051","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, precision_score\nfrom imblearn.metrics import sensitivity_score, specificity_score\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom itertools import count\n\n# Input wine data from the following URL:\nwine = pd.read_csv('../input/Wine.csv')\n\n# Name the features\nwine.columns = [  'name'\n                 ,'alcohol'\n             \t,'malicAcid'\n             \t,'ash'\n            \t,'ashalcalinity'\n             \t,'magnesium'\n            \t,'totalPhenols'\n             \t,'flavanoids'\n             \t,'nonFlavanoidPhenols'\n             \t,'proanthocyanins'\n            \t,'colorIntensity'\n             \t,'hue'\n             \t,'od280_od315'\n             \t,'proline'\n                ]\n\n# Select a subset of the features for the purposes of this notebook\nwineSub = wine[['name','alcohol','malicAcid','magnesium','flavanoids']]\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","metadata":{"trusted":false,"_cell_guid":"2ae43091-d69c-4f83-a3d8-786d62ca1a02","_uuid":"08a81505a09f5223b2714fc43005f4ada5253211","_execution_state":"idle"},"execution_count":1},{"outputs":[],"cell_type":"code","source":"# Encode wine class 1 as itself and all others as 0\nwineSub.loc[wineSub['name'] != 1, 'name'] = 0\n\n# Check the prevalence of class 1 within the dataset\nprint(wineSub.groupby('name').size())\n","metadata":{"trusted":false,"_cell_guid":"b3e5c700-1d18-40d3-9a95-6592c0da494f","collapsed":false,"_uuid":"17584d48cf5df118c8474e4005a2198e06a8d4bd","_execution_state":"idle"},"execution_count":2},{"outputs":[],"cell_type":"markdown","source":"## Exploratory Data Analysis ##\nApply some initial EDA to check which variables appear to have a relationship with the wine class and do not correlate \nwith each other too much","metadata":{"_cell_guid":"5ffdd1dc-3a14-4d8d-9909-a6b6b09c8e19","collapsed":false,"_uuid":"591b7ea861c2943c50458fd84e670db078677409","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Plotting the variables against each other\nsns.pairplot(wineSub,hue='name')","metadata":{"trusted":false,"_cell_guid":"61a69876-fb8e-4f6c-9cb7-4a533808266a","collapsed":false,"_uuid":"8bc39843930f336b159a7be0c831f94891c9a352","_execution_state":"idle"},"execution_count":3},{"outputs":[],"cell_type":"markdown","source":"## Visual checks of class separation ##\nVisually check how much the 2 candidate predictor variables separate wine class 1 from the others","metadata":{"_cell_guid":"fd22fac2-d543-4965-a5e1-e34009069be3","collapsed":false,"_uuid":"12f352b4647b7f19509f6f7dc779aed3e3031f3a","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Select alcohol and flavanoids as candidate variables\n\nX = np.array(wineSub[['alcohol','flavanoids']])\ny = np.array(wineSub['name'])\n\n# Note DO NOT RESHAPE y TO HAVE MORE THAN 1 DIMENSION. OTHERWISE THE BELOW INDEXING WON'T WORK\n# Are the classes separable? - Yes\nplt.scatter(X[y == 1, 0],X[y == 1, 1],label='Class 1',marker='^',color='red')\nplt.scatter(X[y == 0, 0],X[y == 0, 1],label='Other classes',marker='*',color='blue')\nplt.xlabel('alcohol')\nplt.ylabel('flavanoids')\nplt.legend(loc='upper right')\nplt.show()\n\nsns.kdeplot(X[y == 1,0], label='Class 1 alcohol')\nsns.kdeplot(X[y == 0,0], label='Other Classes alcohol')\nplt.title('Distribution across alcohol content')\nplt.xlabel('alcohol content')\nplt.show()\n\nsns.kdeplot(X[y == 1,1], label='Class 1 flavanoids')\nsns.kdeplot(X[y == 0,1], label='Other Classes flavanoids')\nplt.title('Distribution across flavanoid content')\nplt.xlabel('flavanoid content')\nplt.show()","metadata":{"trusted":false,"_cell_guid":"433ae439-68e3-42a4-a644-5e60b9366edb","collapsed":false,"_uuid":"6ff5695a8aa37fd38ce12c68ff8d2250cb29598a","_execution_state":"idle"},"execution_count":4},{"outputs":[],"cell_type":"markdown","source":"## Training, Testing & Evaluation set-up  ##\nThen split the data into training and test matrices. \nTrain the models on the training matrices and evaluate them on the test matrices using the confusion matrix metrics","metadata":{"_cell_guid":"2b56ba8c-ec9b-4961-9618-1d1ac052208a","collapsed":false,"_uuid":"4d9ef91bc0b70951507b9086f048b08812dcbf8a","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)","metadata":{"trusted":false,"_cell_guid":"ec1a8361-ac75-4fab-822a-57d33976487b","collapsed":false,"_uuid":"d71bedc272d248c1e1390400bd0e645b6fd148c2","_execution_state":"idle"},"execution_count":5},{"outputs":[],"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":false,"_cell_guid":"cc902181-a7ee-4cc3-bcae-a9b0856a3625","collapsed":false,"_uuid":"6775096bf05221484ccb47cb7f72129681634db9","_execution_state":"idle"},"execution_count":6},{"outputs":[],"cell_type":"markdown","source":"## Logistic Regression ##\nRun iterative checks on the parameters and penalty terms (L2 & L1) for logistic regression to see where accuracy and AUC peak","metadata":{"_cell_guid":"8e139a08-57b8-43db-8799-db944c9f0e92","collapsed":false,"_uuid":"0e13a6ed21ad71f2f5ee9ca1e438dc4c63eb0831","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Logistic Regression\n# Using l2 penalty term\nparamsl2, scoresl2, aucsl2 = [],[],[]\nfor c in range(-5,6):\n    lr = LogisticRegression(C=10**c, penalty='l2')\n    lr.fit(X_train,y_train)\n    ypred = lr.predict(X_test)\n    score = accuracy_score(y_test,ypred)\n    fpr, tpr, thresholds = roc_curve(y_test,ypred)\n    from sklearn.metrics import auc\n    auc = auc(fpr,tpr)\n    paramsl2.append(lr.C)\n    scoresl2.append(score)\n    aucsl2.append(auc)\n\n# Using l1 penalty term\nparamsl1, scoresl1, aucsl1 = [],[],[]\nfor c in range(-5,6):\n    lr = LogisticRegression(C=10**c, penalty='l1')\n    lr.fit(X_train,y_train)\n    ypred = lr.predict(X_test)\n    score = accuracy_score(y_test,ypred)\n    fpr, tpr, thresholds = roc_curve(y_test,ypred)\n    from sklearn.metrics import auc\n    auc = auc(fpr,tpr)\n    paramsl1.append(lr.C)\n    scoresl1.append(score)\n    aucsl1.append(auc)\n    \nplt.plot(paramsl2,scoresl2,linestyle='--',color='blue', label='l2')\nplt.plot(paramsl1,scoresl1,linestyle='--',color='red', label='l1')\nplt.legend(loc='lower right')\nplt.xscale('log')\nplt.xlabel('parameter values')\nplt.ylabel('Accuracy')\nplt.ylim(0.65,1.0)\nplt.title('Accuracy scores across parameter values for l1 and l2 regularization')\nplt.show()\n\nplt.plot(paramsl2, aucsl2, linestyle='--',color='blue', label='l2')\nplt.plot(paramsl1, aucsl1, linestyle='--',color='red', label='l1')\nplt.legend(loc='lower right')\nplt.xscale('log')\nplt.xlabel('parameter values')\nplt.ylabel(\"AUCs'\")\nplt.ylim(0.45,1.0)\nplt.title('AUC values across parameter values for l1 and l2 regularization')\nplt.show()\n\nprint(\"Smallest C parameter value for max l2 accuracy: %0.3f\" % paramsl2[min([i for i, s in zip(count(), scoresl2) if s == max(scoresl2)])])\nprint(\"Max l2 accuracy: %0.3f\" % max(scoresl2))\nprint(\"Smallest C parameter value for max l1 accuracy: %0.3f\" % paramsl1[min([i for i, s in zip(count(), scoresl1) if s == max(scoresl1)])])\nprint(\"Max l1 accuracy: %0.3f\" % max(scoresl1))\n\nprint(\"Smallest C parameter value for max l2 AUC: %0.3f\" % paramsl2[min([i for i, s in zip(count(), aucsl2) if s == max(aucsl2)])])\nprint(\"Max l2 AUC: %0.3f\" % max(aucsl2))\nprint(\"Smallest C parameter value for max l1 AUC: %0.3f\" % paramsl1[min([i for i, s in zip(count(), aucsl1) if s == max(aucsl1)])])\nprint(\"Max l1 AUC: %0.3f\" % max(aucsl1))","metadata":{"trusted":false,"_cell_guid":"186044f0-e95d-43ab-b832-3761ee1d9c8a","collapsed":false,"_uuid":"4615f22cfbc33b6d2ea87947d19aa44e980ac9d3","_execution_state":"idle"},"execution_count":7},{"outputs":[],"cell_type":"markdown","source":"## Optimal Logistic Regression ##\nTrain the model using the optimal parameter values on the training data set.\nThen plot the ROC curve and final evaluation metrics","metadata":{"_cell_guid":"ccd810b0-aadd-4e51-b862-02c569bb47a9","collapsed":false,"_uuid":"217a4cd6384f1ee2619d422f43b5730155484c47","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Optimal LR model hyperparamters; C=10 with l1 penalty\nlr = LogisticRegression(C=10, penalty='l1')\nlr.fit(X_train,y_train)\nypred = lr.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test,ypred, drop_intermediate=False)\nfrom sklearn.metrics import auc\nauc = auc(fpr,tpr)\n\n# Plot ROC Curve\nplt.plot(fpr,tpr, color='orange', lw=2, label='AUC : %0.3f' % auc)\nplt.plot([0,1],[0,1], linestyle='--', color='blue')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.title('Receiver Operator Curve')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":false,"_cell_guid":"1d5b31d8-eb9b-4507-b47c-9987e0fd14de","collapsed":false,"_uuid":"47a6f10928d9c1e8537130532727ab52cb507c4a","_execution_state":"idle"},"execution_count":8},{"outputs":[],"cell_type":"markdown","source":"## Verify model produces sensible outputs ##\nCheck the distribution of probability values given to show the model is meaningful for the full range of input samples","metadata":{"_cell_guid":"7ece2a94-e8b3-4b7a-84f8-9ece79705e90","collapsed":false,"_uuid":"7f0bbd845312cdfe326a4a6f1a4300203955264a","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Plot the distrubtion of probability values\nprint('Model Coefficients: ', lr.coef_)\nprint('Model Intercept: ', lr.intercept_)\ny_predProb = lr.predict_proba(X)\nsns.distplot(y_predProb[1])\nplt.xlim(0.00,1.00)\nplt.title('Distribution of Probability values produced by Logistic Regression')\nplt.show()","metadata":{"trusted":false,"_cell_guid":"897f289d-80ef-4e61-88c6-c8f4db465db6","collapsed":false,"_uuid":"f2fb7a0e1bc59b9edb2e1fd7e3fe99f969956cd9","_execution_state":"idle"},"execution_count":29},{"outputs":[],"cell_type":"markdown","source":"Cross Validation\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space","metadata":{"_cell_guid":"c49b84df-4683-4650-9600-12e179d489b6","collapsed":false,"_uuid":"de4d8f0d504d34986a7757f4889e3a5b7a33b909","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Cross validating the results\nkfold = KFold(n_splits=10, random_state=1)\ncv_results = cross_val_score(lr, X, y, cv=kfold, scoring='accuracy')\nsns.distplot(cv_results)\nplt.title('Distribution of Accuracy scores')\nprint('Cross validated Accuracy: %.5f +/- %.5f' % (np.mean(cv_results), np.std(cv_results)))","metadata":{"trusted":false,"_cell_guid":"9c482b50-29d4-4b4b-b504-82803c01cd4e","collapsed":false,"_uuid":"676e59d8c320cd298ef2c76e664e2112f549b2ce","_execution_state":"idle"},"execution_count":30},{"outputs":[],"cell_type":"markdown","source":"## Model Evaluation Metrics ##\nConsult full list of model evaluation metrics","metadata":{"_cell_guid":"ef77ab02-b9a8-41d1-b58b-71a54a7c700e","collapsed":false,"_uuid":"9411670423ee4e27668cfafcf2a79f426dc3cdd9","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Use several evaluation metrics for the model\nprint('Accuracy: %0.5f' % accuracy_score(y_test,ypred))\nprint('AUC: %0.5f' % auc)\nprint('Sensitivity: %0.5f' % sensitivity_score(y_test, ypred))\nprint('Specificity: %0.5f' % specificity_score(y_test, ypred))\nprint('Precision: %0.5f' % precision_score(y_test, ypred))","metadata":{"trusted":false,"_cell_guid":"0f40558d-ee1d-442a-ad1e-4dd74e560b91","collapsed":false,"_uuid":"48b2a996c8725eb73cc92775affc14316f0d35ae","_execution_state":"idle"},"execution_count":12},{"outputs":[],"cell_type":"markdown","source":"## Support Vector Machine ##\nRun iterative checks on the cost function parameters for tuning the support vector machine to see where accuracy peaks","metadata":{"_cell_guid":"64c1654e-02ca-4f5c-8d16-05f9af70100e","collapsed":false,"_uuid":"5ada5287f573945117a4bb00ae6a4a81f21c803e","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Using a SVM\n# Varying the parameter C\nparams, scores = [],[]\nfor c in range(-5,6):\n    svm = SVC(C=10**c)\n    svm.fit(X_train,y_train)\n    yPredsvm = svm.predict(X_test)\n    score = accuracy_score(y_test,yPredsvm)\n    params.append(svm.C)\n    scores.append(score)\n       \nplt.plot(params,scores,linestyle='--',color='blue', label='SVM')\nplt.legend(loc='lower right')\nplt.xscale('log')\nplt.xlabel('parameter values')\nplt.ylabel('Accuracy')\nplt.title('Accuracy scores')\nplt.show()\n\nprint(\"First Paramater value of max Accuracy: %0.3f\" % params[min([i for i, s in zip(count(),scores) if s == max(scores)])])\nprint(\"Max Accuracy: %0.3f\" % max(scores))","metadata":{"trusted":false,"_cell_guid":"c9a03232-2add-47d9-9ab5-568dbda3757b","collapsed":false,"_uuid":"f3d2eecc3a771914a368fddfea38fe9cdc2e038a","_execution_state":"idle"},"execution_count":13},{"outputs":[],"cell_type":"markdown","source":"## Optimal Support Vector Machine ##\nTrain the model using the optimal parameter values on the training data set.","metadata":{"_cell_guid":"50a5f3fb-bd5a-4788-bce5-a03463ddcd5a","collapsed":false,"_uuid":"70404d93d068d97114a02c2c238e0dd6590f1771","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Choose best performing SVM using C=1000 to maxmize accuracy\nsvm = SVC(C=1000)\nsvm.fit(X_train, y_train)\nyPredsvm = svm.predict(X_test)","metadata":{"trusted":false,"_cell_guid":"40dd2d55-e486-4ddf-86ba-9e0cca2f1dee","collapsed":false,"_uuid":"2f64f40aed5940771ead7e26b49f9d015ec8d761","_execution_state":"idle"},"execution_count":31},{"outputs":[],"cell_type":"markdown","source":"Cross Validation\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space","metadata":{"_cell_guid":"0148cc1c-569b-4cc8-a33b-fdc5a5cd3d9a","collapsed":false,"_uuid":"4804f2babb4a361c0dcffd593dc69031b632633e","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Cross validate the SVM\nkfold = KFold(n_splits=10, random_state=0)\ncv_results = cross_val_score(svm, X, y, cv=kfold, scoring='accuracy')\nprint('Cross validated accuracy: %.5f +/- %.5f' % (np.mean(cv_results), np.std(cv_results)))","metadata":{"trusted":false,"_cell_guid":"7e24134c-0226-42da-adcb-c85cb667deef","collapsed":false,"_uuid":"734c8b6b536d551a6266a556be75faf69f04485a","_execution_state":"idle"},"execution_count":32},{"outputs":[],"cell_type":"markdown","source":"## Model Evaluation Metrics ##\nConsult full list of model evaluation metrics","metadata":{"_cell_guid":"f2905f87-4591-44dc-86ff-f9439e27a251","collapsed":false,"_uuid":"3c666537d63384cfb7750acc3e9e3b5ef9c8fff1","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Use several evaluation metrics for the model\nprint('Accuracy: %0.5f' % accuracy_score(y_test,yPredsvm))\nprint('Sensitivity: %0.5f' % sensitivity_score(y_test, yPredsvm))\nprint('Specificity: %0.5f' % specificity_score(y_test, yPredsvm))\nprint('Precision: %0.5f' % precision_score(y_test, yPredsvm))","metadata":{"trusted":false,"_cell_guid":"8226afcb-918e-4a56-9546-0a9685f239bb","collapsed":false,"_uuid":"c3c2cafd4bc7586ea6c4fe2dcd1825dd51654c50","_execution_state":"idle"},"execution_count":16},{"outputs":[],"cell_type":"markdown","source":"## Decision Tree - with Entropy ##\nApply a decision tree classifier, using entropy as the function for the impurity measure at each node of the tree","metadata":{"_cell_guid":"c83a7f8d-41ce-4b87-9319-69cae9bd2293","collapsed":false,"_uuid":"4999a76824be644c725fc3171ffc41afec5ff77d","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Using a Decision Tree\n# With Entropy as the impurity measure\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=0)\ntree.fit(X_train, y_train)\nyPredTree = tree.predict(X_test)\nscore = accuracy_score(y_test,yPredTree)\n\nprint(\"Test Accuracy: %0.5f\" % score)","metadata":{"trusted":false,"_cell_guid":"389dd84a-5407-4fd3-bc7d-10386b18e9ec","collapsed":false,"_uuid":"0b9b5ed8d0ff840eb0b45d3d76b65893606ea6b3","_execution_state":"idle"},"execution_count":17},{"outputs":[],"cell_type":"markdown","source":"## Decision Tree - with Gini##\nApply a decision tree classifier, using Gini as the function for the impurity measure at each node of the tree","metadata":{"_cell_guid":"dcf5eda7-395a-48f8-884a-08c6aa8e4bf1","collapsed":false,"_uuid":"800b555a32f012aaed0b975d0157191e40297c8e","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Using a Decision Tree\n# With Gini as the impurity measure\ntree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=0)\ntree.fit(X_train, y_train)\nyPredTree = tree.predict(X_test)\nscore = accuracy_score(y_test,yPredTree)\n\nprint(\"Test Accuracy: %0.5f\" % score)","metadata":{"trusted":false,"_cell_guid":"b1852202-eb64-4f9d-887e-984dc8eaeb42","collapsed":false,"_uuid":"8dcf96d9603a515d9d778169d533f333aec8a003","_execution_state":"idle"},"execution_count":18},{"outputs":[],"cell_type":"markdown","source":"## Final Decision Tree ##\nRun final decision tree on full data set using Gini as impurity measure. There was no discernible different between Gini and Entropy as impurity measures","metadata":{"_cell_guid":"5fa90fa4-e80f-417c-9ffd-fb7849adbe13","collapsed":false,"_uuid":"c7fe9a231c227bda6b39e0944bc016493019798b","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Using a Decision Tree\n# With Gini as the impurity measure\ntree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=0)\ntree.fit(X_train, y_train)\nyPredTree = tree.predict(X_test)","metadata":{"trusted":false,"_cell_guid":"e811c812-2ae9-40e2-a8f2-c6ae26815ba5","collapsed":false,"_uuid":"2633d5e2f023efd67edf60cbc07b568230a8f224","_execution_state":"idle"},"execution_count":19},{"outputs":[],"cell_type":"markdown","source":"Cross Validation\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space","metadata":{"_cell_guid":"5478f00f-9cd2-4e33-8952-c43a1411f9a1","collapsed":false,"_uuid":"f7390b7f082fc12f1241d249c79408c02ea73a78","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Cross validate the Tree\nkfold = KFold(n_splits=10, random_state=0)\ncv_results = cross_val_score(tree, X, y, cv=kfold, scoring='accuracy')\nprint('Cross validated accuracy: %.5f +/- %.5f' % (np.mean(cv_results), np.std(cv_results)))","metadata":{"trusted":false,"_cell_guid":"4d20a3c9-b9c3-4dea-990f-78a0b41a7a15","collapsed":false,"_uuid":"0b54377f5218b834d642e0bad24a52598b01b70e","_execution_state":"idle"},"execution_count":20},{"outputs":[],"cell_type":"markdown","source":"## Model Evaluation Metrics ##\nConsult full list of model evaluation metrics","metadata":{"_cell_guid":"6a459450-1aa3-4727-a2f8-222fda4be358","collapsed":false,"_uuid":"f60025d4cb4c215ff200eccd125e2a1da5f11cb7","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Use several evaluation metrics for the model\nprint('Accuracy: %0.5f' % accuracy_score(y_test,yPredTree))\nprint('Sensitivity: %0.5f' % sensitivity_score(y_test, yPredTree))\nprint('Specificity: %0.5f' % specificity_score(y_test, yPredTree))\nprint('Precision: %0.5f' % precision_score(y_test, yPredTree))","metadata":{"trusted":false,"_cell_guid":"9259e328-06e1-45dd-921c-a617c38ec159","collapsed":false,"_uuid":"0c458f5982fc95448ba4b34457b2561f88fa1d51","_execution_state":"idle"},"execution_count":21},{"outputs":[],"cell_type":"markdown","source":"## AdaBoost Ensemble ##\nApply the adaptive boosting ensemble method of weak learner decision trees","metadata":{"_cell_guid":"b607f0ed-a8d2-4782-9e2e-b8a77f0fb2f0","collapsed":false,"_uuid":"1d78f8fd05e35de74d31d5d0c39ac3ebb66ca371","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"#Using AdaBoost ensemble\nada = AdaBoostClassifier(base_estimator=tree\n                         ,n_estimators=500\n                         ,learning_rate=0.1\n                         ,random_state=1)\nada.fit(X_train, y_train)\nyPredAda = ada.predict(X_test)\n\nscore = accuracy_score(y_test,yPredAda)\n\nprint(\"Test Accuracy: %0.5f\" % score)","metadata":{"trusted":false,"_cell_guid":"88cb2e14-f4c0-4de4-9ae4-3faf0273e9ed","collapsed":false,"_uuid":"ad68362c3a61044dc36d9a90e38222dd82d7a275","_execution_state":"idle"},"execution_count":22},{"outputs":[],"cell_type":"markdown","source":"Cross Validation\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space","metadata":{"_cell_guid":"eb96093c-4af4-4278-b795-37240d7d6578","collapsed":false,"_uuid":"9829e88ff2e826880f472040962ec6dce96cf61e","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Cross validate the Bag\nkfold = KFold(n_splits=10, random_state=0)\ncv_results = cross_val_score(ada, X, y, cv=kfold, scoring='accuracy')\nprint('Cross validated accuracy: %.5f +/- %.5f' % (np.mean(cv_results), np.std(cv_results)))","metadata":{"trusted":false,"_cell_guid":"4c924981-9daa-4304-bf18-66ec2ae7a100","collapsed":false,"_uuid":"3d77bb5df48ea8ca126ede3835d41370fcf96f8f","_execution_state":"idle"},"execution_count":23},{"outputs":[],"cell_type":"markdown","source":"## Model Evaluation Metrics ##\nConsult full list of model evaluation metrics","metadata":{"_cell_guid":"daefc8eb-2a8c-4ef3-97a9-f154b931eeb1","collapsed":false,"_uuid":"ac333506a1b1e9919f9d5ae85314f73977d4e624","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Use several evaluation metrics for the model\nprint('Accuracy: %0.5f' % accuracy_score(y_test,yPredAda))\nprint('Sensitivity: %0.5f' % sensitivity_score(y_test, yPredAda))\nprint('Specificity: %0.5f' % specificity_score(y_test, yPredAda))\nprint('Precision: %0.5f' % precision_score(y_test, yPredAda))","metadata":{"trusted":false,"_cell_guid":"61dfb7e6-5470-4dfa-ba91-dcf808194369","collapsed":false,"_uuid":"a4e6a08cb6c62c4175efd22e000bd2aab540e5de","_execution_state":"idle"},"execution_count":24},{"outputs":[],"cell_type":"markdown","source":"## Multi-Layer Perceptron ##\nImplement this neural network with the hidden layer architecture set to (50,25,10,5). This is already pre-trained so I can simply iterate through different values of the alpha parameter which sets the impact of the L2 regularization term in the cost function to be minimized","metadata":{"_cell_guid":"a4c4959e-341f-4662-b133-26e00e37534f","collapsed":false,"_uuid":"28c5e1ee11b2705963ecbf935b7cadfd02b6dc7e","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Using a Multi-layer perceptron\n# Testing multiple values of alpha\nparams, scores = [],[]\nfor a in range(-5,6):\n    mlp = MLPClassifier(solver='lbfgs'\n                    , activation = 'relu'\n                    , alpha=10**a\n                    , hidden_layer_sizes=(50,25,10,5)\n                    , random_state=1)\n    mlp.fit(X_train, y_train)\n    yPredMLP = mlp.predict(X_test)\n    score = accuracy_score(y_test,yPredMLP)\n    params.append(mlp.alpha)\n    scores.append(score)\n        \nplt.plot(params,scores,linestyle='--',color='blue', label='MLP')\nplt.legend(loc='lower right')\nplt.xscale('log')\nplt.xlabel('parameter values')\nplt.ylabel('Accuracy')\nplt.title('Accuracy scores')\nplt.show()\n\nprint(\"First Paramater value of max Accuracy: %0.5f\" % params[min([i for i, s in zip(count(),scores) if s == max(scores)])])\nprint(\"Max Accuracy: %0.5f\" % max(scores))","metadata":{"trusted":false,"_cell_guid":"90d18d7b-0198-4e0f-a85c-21c2647c9531","collapsed":false,"_uuid":"b250d5687298e00a2a70a4b4d38b79852dce2a3c","_execution_state":"idle"},"execution_count":25},{"outputs":[],"cell_type":"markdown","source":"Cross Validation of the Optimal model\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space having built it on the training data set","metadata":{"_cell_guid":"e58d3e82-c2e4-4cbd-b63d-184aa0a7c7fe","collapsed":false,"_uuid":"70f109e5ad2d48b1c17b44e1d89e639ec225b492","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Cross Validate the best performing network\nmlp = MLPClassifier(solver='lbfgs'\n                    ,activation = 'relu'\n                    ,alpha=10**-5\n                    ,hidden_layer_sizes=(50,25,10,5)\n                    ,random_state=1)\nmlp.fit(X_train, y_train)\nyPredMLP = mlp.predict(X_test)","metadata":{"trusted":false,"_cell_guid":"0c35c053-0e51-4eb0-b277-17cd9ac1a4c7","collapsed":false,"_uuid":"4d6c68a734d9134ec2e2b406ff9868c0a0e0ae56","_execution_state":"idle"},"execution_count":26},{"outputs":[],"cell_type":"markdown","source":"Cross Validation\n----------------\n\nCross validate the model to check variance of the model over different subsets of the sample space","metadata":{"_cell_guid":"5cc84f07-0235-465c-a7d1-6bddf9403c81","collapsed":false,"_uuid":"9b8a0f322f4b2328531033b6452895999c6601ed","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"kfold = KFold(n_splits=10, random_state=1)\ncv_results = cross_val_score(mlp, X , y, cv=kfold ,scoring='accuracy')\nsns.kdeplot(cv_results)\nprint('Cross validated accuracy: %.5f +/- %.5f' % (np.mean(cv_results), np.std(cv_results)))","metadata":{"trusted":false,"_cell_guid":"69ec63d7-1a13-4f66-8e1b-3655fd685089","collapsed":false,"_uuid":"3101115af5c2649ff045978b181de203b22e141d","_execution_state":"idle"},"execution_count":27},{"outputs":[],"cell_type":"markdown","source":"## Model Evaluation Metrics ##\nConsult full list of model evaluation metrics","metadata":{"_cell_guid":"c8ee6848-8e0e-4702-9df4-59c9c0bb3b75","collapsed":false,"_uuid":"7625572d4189b1e33edc66f2bfde2e88ee7ba9a4","_execution_state":"idle"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Use several evaluation metrics for the model\nprint('Accuracy: %0.5f' % accuracy_score(y_test,yPredMLP))\nprint('Sensitivity: %0.5f' % sensitivity_score(y_test, yPredMLP))\nprint('Specificity: %0.5f' % specificity_score(y_test, yPredMLP))\nprint('Precision: %0.5f' % precision_score(y_test, yPredMLP))","metadata":{"trusted":false,"_cell_guid":"a0e2af96-8b8e-44c8-a1cd-d592d71fe3e1","collapsed":false,"_uuid":"125a7325602f60ffab533e96f842c64c169ee210","_execution_state":"idle"},"execution_count":28}]}