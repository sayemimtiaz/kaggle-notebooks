{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation\n\nFor a service provider, customer complaints may carry a negative connotation; however, we should look to complaints as **insights** for several reasons.\n\n* They are often a good indicator of what is going wrong. They can highlight to management not only about challenges with people and processes, but also can provide ideas for new products and services.\n* They provide an opportunity for the service provider to resolve the customer’s problems on time and thus reduce dissatisfaction levels.\n* Customers who have had a problem resolved by a service provide effeciently, often have a stronger loyalty to the company compared with those that have *never* had a problem.\n\nWith the amount of unstructured text data that is currently on the internet - in forms of social media, product reviews, etc. This leaves businesses open to a wealth of data that they can process and then draw insights from. Text data is **notoriously** difficult to analyze; however, recent advances in **Natural Language Processing (NLP)** have made it easy for data scientist and software developers to build pipelines to achieve this task.\n\nIn this notebook, we will use NLP to automatically group consumer complaints into topics. These topics will enable us to discover and adapt to current and emerging trends and identify problem areas. By pinpointing and these types of issues, we can make recommendations to better deliver customer service experiences.\n\nCrossposted at my blog here: https://horvay.dev/complaint-classifier/"},{"metadata":{},"cell_type":"markdown","source":"# The Comcast Consumer Complaints Dataset\n\nThe [Comcast Consumer Complaints Dataset][comcast-dataset] is a public dataset compiled by [Charlie H.][charlie] on Kaggle. His motivative for uploading provides a description of the dataset: \"Comcast is notorious for terrible customer service and despite repeated promises to improve, they continue to fall short. Only last month (October 2016) the FCC fined them a cool $2.3 million after receiving [over 1000 consumer complaints][complaints]. After dealing with their customer service for hours yesterday, I wanted to find out more about others' experiences.\"\n\nThe dataset consists of two CSV files. The first is [comcast_consumeraffairs_complaints.csv][consumeraffair-csv], which has four columns author, posted_on, rating, and text. The second is [comcast_fcc_complaints_2015.csv][fcc-csv], which has Ticket #, Customer Complaint, Date, Time, Received Via, City, State, Zip code, Status, Filing on Behalf of Someone, and Description.\n\nThe most interesting part of these two files are that they both contain verbatim text. Meaning we have the customer complaint iteself! We will leverage this for our analysis.\n\n[comcast-dataset]:https://www.kaggle.com/archaeocharlie/comcastcomplaints\n[charlie]:https://www.kaggle.com/archaeocharlie\n[complaints]:money.cnn.com/2016/10/11/news/companies/comcast-fine-fcc/\n[consumeraffair-csv]:https://raw.githubusercontent.com/dhorvay/consumer-complaint-classifier/master/comcastcomplaints/comcast_consumeraffairs_complaints.csv\n[fcc-csv]:https://raw.githubusercontent.com/dhorvay/consumer-complaint-classifier/master/comcastcomplaints/comcast_fcc_complaints_2015.csv"},{"metadata":{},"cell_type":"markdown","source":"# NLP - Machine Learning Flow\n\nThis explains the flow that we will be using for our Topic Modeling.\n\n![alt text](https://horvay.dev/assets/images/2020-03-29-complaint-classifier/2020-03-29-nlp-flow.png \"NLP Flow\")"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport pickle\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook, save\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.transform import cumsum\nfrom bokeh.io import curdoc\ncurdoc().theme = 'dark_minimal'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ca_df = pd.read_csv('/kaggle/input/comcastcomplaints/comcast_consumeraffairs_complaints.csv')\nca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fcc_df = pd.read_csv('/kaggle/input/comcastcomplaints/comcast_fcc_complaints_2015.csv')\nfcc_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's give our charts the famous peacock colors\n\n![alt text](https://horvay.dev/assets/images/2020-03-29-complaint-classifier/2020-03-29-peacock.png \"Peacock Theme\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try to make it Comcast (new logo after NBC merge) themed\npeacock_theme = ['#cc004c','#f37021','#fcb711','#6460aa','#0089d0','#0db14b','#ffc0cb','#00ffff']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Customer Statisfaction (CSAT) Calculation\n\n### What is Customer Satisfaction (CSAT)?\n\nA metrics that measures short-term happiness, or how a customer feels about a specific service or product.\n\n### How is CSAT measured?\n\nOn a scale of 0-5, how would you measure your satifaction with [Company]?\n\n$$\\dfrac {\\text{# of Satisfied Customers}}{\\text{# of Reviews}} \\cdot 100= \\text{% of Satisfied Customers}$$\n\nA satisfied customer in the case of 0-5 would be any customer that would be greater than or equal to a score of 4.\n\n<img src=\"https://horvay.dev/assets/images/2020-03-29-complaint-classifier/2020-03-29-csat.png\" alt=\"Customer Satisfaction Score (CSAT)\" style=\"width: 50%;\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate CSAT\ncsat = len(ca_df.loc[ca_df['rating']>=4])/len(ca_df)\nprint(\"Comcast's Customer Satisfaction Score calculated from ConsumerAffairs is {:.2f}%\".format(csat*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Customer satisfaction score of 0.90% is really, really bad...\n\nI think what we are seeing with the ConsumersAffairs dataset is a subset of customers who are generally unhappy with the services Comcast provides. I doubt that Comcast's *true* CSAT score is 0.90%; however, what this tells me is that many of the customers who are writing reviews are definately providing us insights on what is going wrong at Comcast. According to ASCI, [their score is 62][comcast-asci], I will take theirs as more accurate, but they use a different calculcation that what I have here.\n\n[comcast-asci]:https://www.theacsi.org/?option=com_content&view=article&id=149&catid=&Itemid=214&c=Comcast&i=Subscription+Television+Service\n\nLet's get a better visual on the ratings..."},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df = pd.DataFrame({'class': ['0', '1', '2', '3', '4', '5'],\n                          'percent' : ca_df['rating'].value_counts(normalize=True).sort_index()*100,\n                          'angle': ca_df['rating'].value_counts(normalize=True).sort_index() * 2 * np.pi,\n                          'color': peacock_theme[0:6]})\nrating_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_src = ColumnDataSource(rating_df)\n# Date range of ratings\nsorted_dates = ca_df['posted_on'].sort_values().reset_index(drop=True).tolist()\nrating_plt_title = 'Comcast ConsumerAffairs Ratings {} - {}'.format(sorted_dates[0], sorted_dates[-1])\n\n# Create the Figure object \"rating_plt\"\nrating_plt = figure(title=rating_plt_title, tools=['save', 'hover'], tooltips='@percent{0.00}%')\n\n# Add circular sectors to \"rating_plt\"\nrating_plt.wedge(x=0, y=0, radius=0.8, source=rating_src, start_angle=cumsum('angle', include_zero=True),\n                 end_angle=cumsum('angle'), fill_color='color', line_color=None, legend_field='class')\n\n# Change parameters of \"rating_plt\"\nrating_plt.axis.visible = False\nrating_plt.grid.grid_line_color = None\nrating_plt.legend.orientation = 'horizontal'\nrating_plt.legend.location = 'top_center'\noutput_notebook()\nshow(rating_plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time series analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"ca_df['posted_on'] = pd.to_datetime(ca_df['posted_on'])\n\ngroupby_posted_on = ca_df.groupby('posted_on').count()\n\nts_src = ColumnDataSource(groupby_posted_on)\n\nts_plt_title = 'Number of reviews per day {} - {}'.format(sorted_dates[0], sorted_dates[-1])\nts_plt = figure(title=ts_plt_title, x_axis_type='datetime', tools=['save', 'hover'], tooltips=[('Count', '@rating')])\n\nts_plt.line(x='posted_on', y='rating', line_width=2, source=ts_src, color=peacock_theme[0])\n\nts_plt.yaxis.axis_label = 'Number of Reviews'\n\nshow(ts_plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outliers\n\nSeems that there was a lot of activity on a certain date in 2016, with the number of reviews being over 50 - much more than the average, which is less than 10. Maybe there was some major outage around this time...can we find if anything unusual happened?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What the hell happened?\ngroupby_posted_on.loc[groupby_posted_on['rating'] > 50]\n\n# Change pandas settings to allow max rows be 100\nca_df.loc[ca_df['posted_on'] == '2016-02-24']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even looking at the text, it doesn't seem that there is any unusual problem that sticks out for this day. This gives me some doubts about the dataset we have; however, there is always just a possibility that it is a legitimate outlier.\n\nLet's layer the FCC Complaints on top of this."},{"metadata":{"trusted":true},"cell_type":"code","source":"fcc_df['Date'] = pd.to_datetime(fcc_df['Date'])\n\nca_df['Count'] = 0\nca_df2 = ca_df.loc[ca_df['posted_on'] >= '2015-01-01']\ngroupby_posted_on = ca_df2.groupby('posted_on').count()\n\nts_ca_src = ColumnDataSource(groupby_posted_on)\n\nfcc_df['Count'] = 0\ngroupby_date = fcc_df.groupby('Date').count()\n\nts_fcc_src = ColumnDataSource(groupby_date)\n\nts_fcc_plt = figure(title=\"Number of Customer FCC Complaints and ConsumerAffairs Reviews Per Day\",\n                    x_axis_type='datetime', tools=['save', 'hover'], tooltips=[('Count', '@Count')])\n\nts_fcc_plt.line(x='Date', y='Customer Complaint', line_width=2, source=ts_fcc_src, color=peacock_theme[1],\n                legend_label=' # of FCC Customer Complaints')\n\nts_fcc_plt.line(x='posted_on', y='rating', line_width=2, source=ts_ca_src, color=peacock_theme[0],\n                legend_label='# of ConsumerAffairs Reviews')\n\nshow(ts_fcc_plt)\n\nfcc_df = fcc_df.drop(columns=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Current complaint categories\n\nLet's look at our current complaint categories (from now refered to as **topics**). This is what we will refine later when we implement our NLP algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"fcc_df['Customer Complaint'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to customers, there are 1842 different topics, which we know is not the case. Can we use simple string manipulation to get a better grasp on what customers are complaining about?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_simple_topic_percentage(topic):\n    \"\"\"\n    Returns a percentage of rows that this particular topic is found\n    in using simple string manipulation. Note: this can have overlaps,\n    for example if you have two topics, one 'Internet' and one 'Speed',\n    you will get duplicate findings if the customer has 'Internet Speed'\n    as their topic.\n    \n    topic: the customer complaint category entered by the customer.\n    \"\"\"\n    return fcc_df[fcc_df['Customer Complaint'].str.contains(topic, case=False)].shape[0] / len(fcc_df['Customer Complaint']) * 100\n    \n\nprint('Comcast:', get_simple_topic_percentage('comcast'))\nprint('Data cap:', get_simple_topic_percentage('data'))\nprint('Speed:', get_simple_topic_percentage('speed'))\nprint('Internet:', get_simple_topic_percentage('internet'))\nprint('Price:', get_simple_topic_percentage('price'))\nprint('Bill:', get_simple_topic_percentage('bill'))\nprint('Customer Service:', get_simple_topic_percentage('customer service'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nWe will use the [spaCy][spacy] library for preprocessing. This library is amazing and its tagline is 'industrial strength Natural Language Processing', which it definately lives up to. We can preprocess our complaints in just a few lines.\n\n### Tokenization\nDifferent languages will have different tokenization rules. Let's look at an example of how tokenization might work in English. For the sentence – 'It's nice out today.', it would be broken up as follows, with the appropriate numerical indices:\n\n| 0  | 1  | 2    |  3  |   4   |\n|----|----|------|-----|-------|\n| It | is | nice | out | today |\n\nThis looks like the result when we run `.split(' ')` in Python – why do we give it a special name of tokening?\n\nIf you notice carefully, the sentence has been split differenly for example: **It's** into **It** *and* **is** and punctuation has been removed.\n\n### Remove Stopwords\n\nStopwords are words which are filtered out before processing our data. Though \"stopwords\" usually refers to the most common words in a language (e.g for english, 'the', 'a', etc..) there is no single universal list of stopwords used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stopwords to support phrase search. \n\n### Lemmatization and stemming\n\nParaphrasing [Introduction to Information Retrieval by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze][ir-retrieval]: \"Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\"\n\n[spacy]:https://spacy.io/\n[ir-retrieval]: https://nlp.stanford.edu/IR-book"},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en import English\nnlp = English()\n\ncustomize_stop_words = ['comcast', 'i', 'fcc', 'hello', 'service', 'services', 'issue',\n                        'issues', 'problem', 'problems', 'xfinity', 'customer', 'complaint', '$']\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True\n\ndef preprocess(verbatim):\n    \"\"\"\n    Tokenizes, removes stopwords, and lemmatizes a verbatim text\n    \n    verbatim: a free-form text complaint\n    \"\"\"\n    # Every verbatim ends with the FCC follow up, let's remove this.\n    verbatim = verbatim.split('\\n')[0].lower()\n    doc = nlp(verbatim)\n    sent = []\n    for word in doc:\n        # If it's not a stop word or punctuation mark, add it to our article!\n        if word.text != 'n' and not word.is_stop and not word.is_punct and not word.like_num:\n            # We add the lematized version of the word\n            sent.append(word.lemma_.lower())\n    return sent\n\n# Tokenize each complaint\ndocs = fcc_df['Description'].apply(lambda verbatim: preprocess(verbatim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Revisiting categorization using NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import FreqDist\ncats = fcc_df['Customer Complaint'].apply(lambda verbatim: preprocess(verbatim))\nfiltered_complaints = [c for cl in cats for c in cl]\nfdist = FreqDist(filtered_complaints)\nprint(fdist.most_common(30))\nfdist.plot(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Latent Dirichlet Allocation (LDA)\n\nFor the purpose of this explaination I will be using terms interchangably, let's define those:\n\n* **Topic** means the same as **Complaint Category**.\n* **Document** means the same as a **Complaint Verbatim**.\n* **Corpus** means the same as **all the Complaint Verbatims in the CSV**.\n\nLatent Dirichlet Allocation (LDA) is a probabilistic model that assumes that every topic is a bag of words and every document is a bag of topics that each can be chosen with from the bag with some probability.\n\nLDA is an unsupervised learning method. The number of topics, **k**, are left to us to determine and the *interpretation* of the topics is also left to us.\n\nFor example, let's say you have a complaint like - **\"I was charged a late fee but I paid my bill on the 16th\"**. With LDA, we will look at all the documents in our corpus and begin separting them into topics. After, for this particular document, we can say that this complaint is made up of two topics: it is 40% \"Fees\" and 60% \"Payments\". We can also say if we select a word at random from \"Fees\" we might get the word \"charged \" with a probability of 5.0% (just an example).\n\n[Here](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/), you can find a very good write-up of it by [Edwin Chen](http://blog.echen.me).\n\n*How LDA sees the complaints*\n<img src=\"https://horvay.dev/assets/images/2020-03-29-complaint-classifier/2020-03-29-lda.png\" alt=\"Latent Dirichlet Allocation (LDA)\" style=\"width: 50%;\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.corpora import Dictionary\n\ndictionary = Dictionary(docs)\n\nprint('Distinct words in initial documents:', len(dictionary))\n\n# Filter out words that occur less than 10 documents, or more than 30% of the documents.\ndictionary.filter_extremes(no_below=10, no_above=0.3)\n\nprint('Distinct words after removing rare and common words:', len(dictionary))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train and create the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import CoherenceModel, LdaModel\nimport pyLDAvis.gensim\n\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\nnum_topics = 8\n\n# Check for .pickle\nfilename = '/kaggle/input/lda-modelpickle/lda_model.pickle'\nmodel = []\nfound = False\ntry: \n    infile = open(filename,'rb')\n    model = pickle.load(infile)\n    infile.close()\n    found = True\n    print('Model found..loaded.')\nexcept:\n    print('Model not found!')\n\nif not found:\n    %time model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=20, workers=2)\n    outfile = open(filename,'wb')\n    pickle.dump(model, outfile)\n    outfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(model, corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explaining the pyLDAvis output\n\nThe left panel displays the Intertopic Distance Map, which represents different topics and the distance between them. The closer the topics are in meaning the closer they appear, the same goes for dissimilar topics. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus, which you can examine by hovering over the topic.\n\nThe right panel, displays a bar chart representing the 'salientcy' of words. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. When no topic is selected in the plot on the left, the bar chart shows the top-30 most salient terms in the entire corpus, while selecting a topic on the left modifies the bar chart to show its relevent topics. Relevence is defined as in footer 2 and can be tuned by parameter λ, smaller λ gives higher weight to the term's distinctiveness while larger λ's corresponds to probablity of the term occurance per topics."},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the model\n\nWhen evaluating unsupervised learning techniques, we need to get a little creative. Most of the magic was done in the hyperparameter tuning we did previously - selecting **k**, etc.\n\nOne way we can evaluate is by splitting our dataset into two and comparing the topics found in each section. The more similar the topics found, the better."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nfcc_df_super = fcc_df.copy()\n\nfcc_df_super['Tokens'] = docs\n\ndocs_upper = fcc_df_super['Tokens'].apply(lambda l: l[:int(len(l)/2)])\ndocs_lower = fcc_df_super['Tokens'].apply(lambda l: l[int(len(l)/2):])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_upper = [dictionary.doc2bow(doc) for doc in docs_upper]\ncorpus_lower = [dictionary.doc2bow(doc) for doc in docs_lower]\n\n# Using the corpus LDA model tranformation\nlda_corpus_upper = model[corpus_upper]\nlda_corpus_lower = model[corpus_lower]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\ndef get_doc_topic_dist(model, corpus, kwords=False): \n    '''\n    LDA transformation, for each doc only returns topics with non-zero weight\n    This function makes a matrix transformation of docs in the topic space.\n    \n    model: the LDA model\n    corpus: the documents\n    kwords: if True adds and returns the keys\n    '''\n    top_dist =[]\n    keys = []\n    for d in corpus:\n        tmp = {i:0 for i in range(num_topics)}\n        tmp.update(dict(model[d]))\n        vals = list(OrderedDict(tmp).values())\n        top_dist += [np.asarray(vals)]\n        if kwords:\n            keys += [np.asarray(vals).argmax()]\n\n    return np.asarray(top_dist), keys","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cosine-similarity\n\nWe can use cosine similiarity to find how close the word are between the corresponding parts of a document - this is called **Intra-similarity** and a higher number is better. We can also find how close words are between random parts of a document - this is called **Inter-similarity** and a lower number is better."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_dist_upper, _ = get_doc_topic_dist(model, lda_corpus_upper)\ntop_dist_lower, _ = get_doc_topic_dist(model, lda_corpus_lower)\n\nprint(\"Intra-similarity:\", np.mean([cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0] for c1, c2 in zip(top_dist_upper, top_dist_lower)]))\n\nrandom_pairs = np.random.randint(0, len(fcc_df_super['Description']), size=(400, 2))\n\nprint(\"Inter-similarity:\", np.mean([cosine_similarity(top_dist_upper[0].reshape(1, -1), top_dist_lower[1].reshape(1, -1))]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Labeling the Topics\n\nWe can now iterate through our model and find the 'Top N' words for each topic. Let's assign the topics labels finally."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\nfor i in range(num_topics):\n    print('\\nTopic {}\\n'.format(str(i)))\n    for term, frequency in model.show_topic(i, topn=10):\n        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now assign labels for our topics by interpreting these top words and frequencies:\n* Pricing\n* Billing\n* Data Caps\n* Missed Appointments\n* Moving Services\n* Customer Services\n* Internet Speed\n* Business Contracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_labels = {0: 'Pricing', 1:'Billing', 2:'Data Caps', 3:'Missed Appointments', 4:'Moving Services', 5: 'Customer Services', 6:'Internet Speed', 7: 'Business Contracts'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tf-idf\n\ntf-idf stands for \"Term Frequency-Inverse Document Frequency\". It is a numeric measure that is use to score the importance of a word in a document based on how often did it appear in that document and a given collection of documents. The intuition for this measure is : If a word appears frequently in a document, then it should be important and we should give that word a high score. But if a word appears in too many other documents, it’s probably not a unique identifier, therefore we should assign a lower score to that word. The math formula for this measure \n\n$$\\mbox{tf-idf}_{t,d} = (1 +\\log \\mbox{tf}_{t,d}) \\cdot \\log \\frac{N}{\\mbox{df}_t}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\n                                  tokenizer=preprocess, ngram_range=(1,3), min_df=40, max_df=0.20,\n                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n\ndtm = tvectorizer.fit_transform(fcc_df_super['Description']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_dist, lda_keys= get_doc_topic_dist(model, corpus, True)\nfeatures = tvectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words = []\nfor n in range(len(dtm)):\n    inds = np.int0(np.argsort(dtm[n])[::-1][:4])\n    top_words += [', '.join([features[i] for i in inds])]\n    \nfcc_df_super['Description Top Words'] = pd.DataFrame(top_words)\nfcc_df_super['Topic'] = pd.DataFrame(lda_keys)\n# Fill missing values with dummy\nfcc_df_super['Topic'].fillna(-1, inplace=True)\nfcc_df_super.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# t-SNE Visualization\n\nAccording to the author, [Laurens van der Maaten](https://lvdmaaten.github.io/tsne/), \"t-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\"\n\nTo simplify it, we can map our model onto a 2d space to give it a more human readable visualization.\n\nThis [notebook](https://www.kaggle.com/ykhorramz/lda-and-t-sne-interactive-visualization) by [Yasmin](https://www.kaggle.com/ykhorramz) helped me greatly during the new few cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(top_dist)\n\nfcc_df_super['Description Truncated'] = fcc_df_super['Description'].apply(lambda x: x[0:140])\nfcc_df_super['X_tsne'] = X_tsne[:,0]\nfcc_df_super['Y_tsne'] = X_tsne[:,1]\n\nfcc_df_super['Colors'] = fcc_df_super['Topic'].apply(lambda topic_num: peacock_theme[topic_num])\n\nsource = ColumnDataSource(dict(\n    x=fcc_df_super['X_tsne'],\n    y=fcc_df_super['Y_tsne'],\n    color=fcc_df_super['Colors'],\n    label=fcc_df_super['Topic'].apply(lambda t: top_labels[t]),\n    old_topic=fcc_df_super['Customer Complaint'],\n    top_words=fcc_df_super['Description Top Words'],\n    description=fcc_df_super['Description Truncated']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting t-SNE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'T-SNE Visualization of Topics'\nplot_tsne = figure(plot_width=1000, plot_height=600, title=title,\n                   tools=['pan', 'wheel_zoom', 'save', 'hover'], tooltips=[(\"Old Topic\",\"@old_topic\"),\n                                                                           (\"Description\",\"@description\"),\n                                                                           (\"Top Words\",\"@top_words\")])\n\nplot_tsne.scatter(x='x', y='y', legend_field='label', source=source, color='color', alpha=0.6, size=5.0)\nplot_tsne.legend.location = \"top_right\"\n\nshow(plot_tsne)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting Insights\n\nFrom this visualization we can see what our main problem areas from our topics. We have narrowed them down from 1000+ free-text user-entered to 8 LDA-generated. We are able to understand without having to go through one-by-one which each customer is complaining about. We can see some of the centrals themes such as \"Billing\", which affects every other topic, whereas something peripheral themes such as \"Missed Appointments\" might require its own strategy to resolve.\n\nFor further improvements, could we run something like sentiment analysis on this interactive graph to futher understand how servere each particular topic is? Perhaps it is something we can tackle in the future."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}