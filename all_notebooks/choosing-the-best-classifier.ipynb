{"cells":[{"metadata":{},"cell_type":"markdown","source":" ##### For multi-class classification, 3 wine classification dataset is used in this notebook.<br>\n \n ## In this notebook, you will find the following things:\n \n 1. How to plot pair-wise relations using seaborn's pairplot.<br>\n \n 1. Evaluating three classifiers - Gaussian Naive Bayes, Decision Tree and KNN classifier.<br>\n \n 1. You will be able to use any of the following criteria for evaluation, as per your requirements - <br><br> \na) Training time taken by the classifier<br>\nb) Metrics including overall accuracy, classwise accuracy, precision,recall,F1-score and AUC for both training an testing data.<br>\n      \n 1. How to use confusion matrix to find TPR(true positive rate) and FPR(false positive rate).<br>\n \n 1. Tuning the hyper-parameters of KNN and Decision Tree to achieve better results.<br><br>\n \nNote: This notebook is only for demonstration purpose and you are free to use any other   \n       classifier.\n       For evaluation, call the evaluateClassifier function in a similar way as shown in the notebook.<br><br>\n**Do upvote if you find it useful.So let's get started!**       \n       ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler,normalize,label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix, roc_curve, auc, f1_score, precision_score,recall_score, roc_auc_score\nfrom time import time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/3wine-classification-dataset/wine.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Pairwise relations in dataset using seaborn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df,hue=\"Wine\", markers=[\"o\", \"s\", \"D\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.iloc[:,0]\nx = df.iloc[:,1:]\n\nsc = StandardScaler()\nx = sc.fit_transform(x)\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.7,random_state = 42)\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function for evaluating the classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluateClassifier(x,y,y_pred,y_score):\n    cm = pd.DataFrame(\n        confusion_matrix(y, y_pred),\n        columns=['Predicted Wine 1', 'Predicted Wine 2','Predicted Wine 3'],\n        index=['True Wine 1', 'True Wine 2','True Wine 3']\n    )\n    print('\\nConfusion Matrix: \\n')\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.show()\n    w1 = cm['Predicted Wine 1']['True Wine 1'] / (cm['Predicted Wine 1']['True Wine 1'] + cm['Predicted Wine 2']['True Wine 1'] + cm['Predicted Wine 3']['True Wine 1'])\n    w2 = cm['Predicted Wine 2']['True Wine 2'] / (cm['Predicted Wine 1']['True Wine 2'] + cm['Predicted Wine 2']['True Wine 2'] + cm['Predicted Wine 3']['True Wine 2'])\n    w3 = cm['Predicted Wine 3']['True Wine 3'] / (cm['Predicted Wine 1']['True Wine 3'] + cm['Predicted Wine 2']['True Wine 3'] + cm['Predicted Wine 3']['True Wine 3'])\n    print('\\nClasswise accuracy: ')\n    print('\\nWine 1: ',w1 * 100)\n    print('\\nWine 2: ',w2 * 100)\n    print('\\nWine 3: ',w3 * 100)\n    \n    indices = ['Accuracy','Precision','F1 score','Recall  score']\n    eval = pd.DataFrame([accuracy_score(y,y_pred) * 100,precision_score(y,y_pred,average = 'macro') * 100,f1_score(y,y_pred,average = 'macro') * 100,recall_score(y,y_pred,average = 'macro') * 100],columns=['Value'],index=indices)\n    eval.index.name = 'Metrics'\n    print('\\n',eval)\n    y = label_binarize(y, classes = range(1,4))\n    for i in range(3):\n        fpr, tpr, _ = roc_curve(y[:, i], y_score[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.2, 1.05])\n    plt.ylim([0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.title('ROC curve')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Gaussian Naive Bayes Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### On training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\nstart = time()\ngnb.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,gnb.predict(x_train),gnb.predict_proba(x_train))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateClassifier(x_test,y_test,gnb.predict(x_test),gnb.predict_proba(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier with hyper-parameter tuning****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## On training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dtree_grid_search(X,y,nfolds):\n    param_grid = { 'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n    dtree_model=DecisionTreeClassifier()\n    dtree_gscv = GridSearchCV(dtree_model, param_grid, cv = nfolds)\n    dtree_gscv.fit(X, y)\n    return dtree_gscv.best_params_\n\noptimal_params = dtree_grid_search(x_train,y_train,5)\n\ndtc = DecisionTreeClassifier(criterion = optimal_params['criterion'],max_depth = optimal_params['max_depth'])\nstart = time()\ndtc.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,dtc.predict(x_train),dtc.predict_proba(x_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_params = dtree_grid_search(x_test,y_test,5)\n\ndtc = DecisionTreeClassifier(criterion = optimal_params['criterion'],max_depth = optimal_params['max_depth'])\ndtc.fit(x_train, y_train)\nevaluateClassifier(x_test,y_test,dtc.predict(x_test),dtc.predict_proba(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Classifier with hyper-parameter tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## On training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def knn_grid_search(X,y,nfolds):\n    param_grid = { 'n_neighbors':range(25)}\n    knn_model = KNeighborsClassifier()\n    knn_gscv = GridSearchCV(knn_model, param_grid, cv = nfolds)\n    knn_gscv.fit(X, y)\n    return knn_gscv.best_params_\n\noptimal_params = knn_grid_search(x_train,y_train,5)\n\nknn = KNeighborsClassifier(n_neighbors = optimal_params['n_neighbors'])\nstart = time()\nknn.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,knn.predict(x_train),knn.predict_proba(x_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_params = knn_grid_search(x_test,y_test,5)\n\nknn = KNeighborsClassifier(n_neighbors = optimal_params['n_neighbors'])\nknn.fit(x_train, y_train)\nevaluateClassifier(x_test,y_test,knn.predict(x_test),knn.predict_proba(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Inferences:\n> ### 1.On testing data, Naive Bayes give the best accuracy.\n> ### 2.The F1 score is also maximum in case of Naive Bayes.\n> ### 3.In terms of training time, KNN Classifier gives the best results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ## Conclusion\n> ### Gaussian Naive Bayes is the best classification model for the given data set.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}