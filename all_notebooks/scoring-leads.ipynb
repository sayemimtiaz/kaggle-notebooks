{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Business-Goals\" data-toc-modified-id=\"Business-Goals-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Business Goals</a></span></li><li><span><a href=\"#Analysis-Approach-&amp;-Conclusions\" data-toc-modified-id=\"Analysis-Approach-&amp;-Conclusions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analysis Approach &amp; Conclusions</a></span></li><li><span><a href=\"#Importing-Data\" data-toc-modified-id=\"Importing-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Importing Data</a></span></li><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Incorrect-data-types\" data-toc-modified-id=\"Incorrect-data-types-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Incorrect data types</a></span></li><li><span><a href=\"#Duplicates\" data-toc-modified-id=\"Duplicates-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Duplicates</a></span></li><li><span><a href=\"#Separating-ID-columns\" data-toc-modified-id=\"Separating-ID-columns-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Separating ID columns</a></span></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Missing Values</a></span></li><li><span><a href=\"#Disguised-missing-Values\" data-toc-modified-id=\"Disguised-missing-Values-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Disguised missing Values</a></span></li><li><span><a href=\"#Imputation\" data-toc-modified-id=\"Imputation-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Imputation</a></span></li><li><span><a href=\"#Incorrect-Labels\" data-toc-modified-id=\"Incorrect-Labels-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Incorrect Labels</a></span></li><li><span><a href=\"#Cleaning-Categorical-Features\" data-toc-modified-id=\"Cleaning-Categorical-Features-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Cleaning Categorical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-Unnecessary-Columns\" data-toc-modified-id=\"Dropping-Unnecessary-Columns-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>Dropping Unnecessary Columns</a></span></li><li><span><a href=\"#Grouping-Labels-with-less-leads\" data-toc-modified-id=\"Grouping-Labels-with-less-leads-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>Grouping Labels with less leads</a></span><ul class=\"toc-item\"><li><span><a href=\"#Country\" data-toc-modified-id=\"Country-5.8.2.1\"><span class=\"toc-item-num\">5.8.2.1&nbsp;&nbsp;</span>Country</a></span></li><li><span><a href=\"#Lead-Origin\" data-toc-modified-id=\"Lead-Origin-5.8.2.2\"><span class=\"toc-item-num\">5.8.2.2&nbsp;&nbsp;</span>Lead Origin</a></span></li><li><span><a href=\"#Lead-Source\" data-toc-modified-id=\"Lead-Source-5.8.2.3\"><span class=\"toc-item-num\">5.8.2.3&nbsp;&nbsp;</span>Lead Source</a></span></li><li><span><a href=\"#Last-Activity\" data-toc-modified-id=\"Last-Activity-5.8.2.4\"><span class=\"toc-item-num\">5.8.2.4&nbsp;&nbsp;</span>Last Activity</a></span></li><li><span><a href=\"#Specialization\" data-toc-modified-id=\"Specialization-5.8.2.5\"><span class=\"toc-item-num\">5.8.2.5&nbsp;&nbsp;</span>Specialization</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Retained-Data\" data-toc-modified-id=\"Retained-Data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Retained Data</a></span></li><li><span><a href=\"#Data-Imbalance\" data-toc-modified-id=\"Data-Imbalance-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Imbalance</a></span></li><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Univariate Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lead-Origin\" data-toc-modified-id=\"Lead-Origin-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Lead Origin</a></span></li><li><span><a href=\"#Lead-Source\" data-toc-modified-id=\"Lead-Source-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Lead Source</a></span></li><li><span><a href=\"#Do-not-Email\" data-toc-modified-id=\"Do-not-Email-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Do not Email</a></span></li><li><span><a href=\"#Last-Activity\" data-toc-modified-id=\"Last-Activity-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Last Activity</a></span></li><li><span><a href=\"#Country\" data-toc-modified-id=\"Country-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Country</a></span></li><li><span><a href=\"#Specialization\" data-toc-modified-id=\"Specialization-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>Specialization</a></span></li><li><span><a href=\"#What-is-your-current-occupation\" data-toc-modified-id=\"What-is-your-current-occupation-8.7\"><span class=\"toc-item-num\">8.7&nbsp;&nbsp;</span>What is your current occupation</a></span></li><li><span><a href=\"#City\" data-toc-modified-id=\"City-8.8\"><span class=\"toc-item-num\">8.8&nbsp;&nbsp;</span>City</a></span></li><li><span><a href=\"#A-free-copy-of-Mastering-The-Interview.\" data-toc-modified-id=\"A-free-copy-of-Mastering-The-Interview.-8.9\"><span class=\"toc-item-num\">8.9&nbsp;&nbsp;</span>A free copy of Mastering The Interview.</a></span></li><li><span><a href=\"#TotalVisits\" data-toc-modified-id=\"TotalVisits-8.10\"><span class=\"toc-item-num\">8.10&nbsp;&nbsp;</span>TotalVisits</a></span></li><li><span><a href=\"#Total-TIme-Spent-on-Website.\" data-toc-modified-id=\"Total-TIme-Spent-on-Website.-8.11\"><span class=\"toc-item-num\">8.11&nbsp;&nbsp;</span>Total TIme Spent on Website.</a></span></li><li><span><a href=\"#Page-Views-Per-Visit.\" data-toc-modified-id=\"Page-Views-Per-Visit.-8.12\"><span class=\"toc-item-num\">8.12&nbsp;&nbsp;</span>Page Views Per Visit.</a></span></li></ul></li><li><span><a href=\"#Bivariate-Analysis\" data-toc-modified-id=\"Bivariate-Analysis-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bivariate Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#TotalVisits-vs-A-free-copy-of-Mastering-The-Interview\" data-toc-modified-id=\"TotalVisits-vs-A-free-copy-of-Mastering-The-Interview-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>TotalVisits vs A free copy of Mastering The Interview</a></span></li><li><span><a href=\"#Lead-Source-vs-Country\" data-toc-modified-id=\"Lead-Source-vs-Country-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Lead Source vs Country</a></span></li><li><span><a href=\"#Occupation-vs-City\" data-toc-modified-id=\"Occupation-vs-City-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Occupation vs City</a></span></li><li><span><a href=\"#Last-Activity-vs-Country\" data-toc-modified-id=\"Last-Activity-vs-Country-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Last Activity vs Country</a></span></li></ul></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mapping-Binary-Variables-to-0-/-1\" data-toc-modified-id=\"Mapping-Binary-Variables-to-0-/-1-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Mapping Binary Variables to 0 / 1</a></span></li><li><span><a href=\"#Creating-Indicator-Variables\" data-toc-modified-id=\"Creating-Indicator-Variables-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Creating Indicator Variables</a></span></li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Correlation</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Train-Test Split</a></span></li><li><span><a href=\"#Standardizing-Continuous-Variables\" data-toc-modified-id=\"Standardizing-Continuous-Variables-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Standardizing Continuous Variables</a></span></li></ul></li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recurvise-Feature-Elimination\" data-toc-modified-id=\"Recurvise-Feature-Elimination-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Recurvise Feature Elimination</a></span></li><li><span><a href=\"#Manual-Feature-Elimination\" data-toc-modified-id=\"Manual-Feature-Elimination-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Manual Feature Elimination</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1\" data-toc-modified-id=\"Model-1-11.2.1\"><span class=\"toc-item-num\">11.2.1&nbsp;&nbsp;</span>Model 1</a></span></li><li><span><a href=\"#Model-2\" data-toc-modified-id=\"Model-2-11.2.2\"><span class=\"toc-item-num\">11.2.2&nbsp;&nbsp;</span>Model 2</a></span></li><li><span><a href=\"#Model-3\" data-toc-modified-id=\"Model-3-11.2.3\"><span class=\"toc-item-num\">11.2.3&nbsp;&nbsp;</span>Model 3</a></span></li><li><span><a href=\"#Model-4\" data-toc-modified-id=\"Model-4-11.2.4\"><span class=\"toc-item-num\">11.2.4&nbsp;&nbsp;</span>Model 4</a></span></li><li><span><a href=\"#Model-5\" data-toc-modified-id=\"Model-5-11.2.5\"><span class=\"toc-item-num\">11.2.5&nbsp;&nbsp;</span>Model 5</a></span></li><li><span><a href=\"#Model-6\" data-toc-modified-id=\"Model-6-11.2.6\"><span class=\"toc-item-num\">11.2.6&nbsp;&nbsp;</span>Model 6</a></span></li><li><span><a href=\"#Model-7\" data-toc-modified-id=\"Model-7-11.2.7\"><span class=\"toc-item-num\">11.2.7&nbsp;&nbsp;</span>Model 7</a></span></li><li><span><a href=\"#Model-8\" data-toc-modified-id=\"Model-8-11.2.8\"><span class=\"toc-item-num\">11.2.8&nbsp;&nbsp;</span>Model 8</a></span></li><li><span><a href=\"#Model-9\" data-toc-modified-id=\"Model-9-11.2.9\"><span class=\"toc-item-num\">11.2.9&nbsp;&nbsp;</span>Model 9</a></span></li><li><span><a href=\"#Model-10\" data-toc-modified-id=\"Model-10-11.2.10\"><span class=\"toc-item-num\">11.2.10&nbsp;&nbsp;</span>Model 10</a></span></li><li><span><a href=\"#Model-11---Final-Model\" data-toc-modified-id=\"Model-11---Final-Model-11.2.11\"><span class=\"toc-item-num\">11.2.11&nbsp;&nbsp;</span>Model 11 - Final Model</a></span></li></ul></li></ul></li><li><span><a href=\"#Final-Features\" data-toc-modified-id=\"Final-Features-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Final Features</a></span></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predictions-on-Train-set\" data-toc-modified-id=\"Predictions-on-Train-set-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Predictions on Train set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Actual-Conversions-vs-Conversion-Predictions\" data-toc-modified-id=\"Actual-Conversions-vs-Conversion-Predictions-13.1.1\"><span class=\"toc-item-num\">13.1.1&nbsp;&nbsp;</span>Actual Conversions vs Conversion Predictions</a></span></li><li><span><a href=\"#Predictions-with-cut-off-=-0.5\" data-toc-modified-id=\"Predictions-with-cut-off-=-0.5-13.1.2\"><span class=\"toc-item-num\">13.1.2&nbsp;&nbsp;</span>Predictions with cut off = 0.5</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.1.3\"><span class=\"toc-item-num\">13.1.3&nbsp;&nbsp;</span>Confusion Matrix</a></span></li><li><span><a href=\"#Accuracy-of-the-Model\" data-toc-modified-id=\"Accuracy-of-the-Model-13.1.4\"><span class=\"toc-item-num\">13.1.4&nbsp;&nbsp;</span>Accuracy of the Model</a></span></li><li><span><a href=\"#Metrics-beyond-simple-accuracy\" data-toc-modified-id=\"Metrics-beyond-simple-accuracy-13.1.5\"><span class=\"toc-item-num\">13.1.5&nbsp;&nbsp;</span>Metrics beyond simple accuracy</a></span></li><li><span><a href=\"#Plotting-ROC-Curve.\" data-toc-modified-id=\"Plotting-ROC-Curve.-13.1.6\"><span class=\"toc-item-num\">13.1.6&nbsp;&nbsp;</span>Plotting ROC Curve.</a></span></li><li><span><a href=\"#Finding-Optimal-Cutoff-Point\" data-toc-modified-id=\"Finding-Optimal-Cutoff-Point-13.1.7\"><span class=\"toc-item-num\">13.1.7&nbsp;&nbsp;</span>Finding Optimal Cutoff Point</a></span></li><li><span><a href=\"#Precision-and-Recall\" data-toc-modified-id=\"Precision-and-Recall-13.1.8\"><span class=\"toc-item-num\">13.1.8&nbsp;&nbsp;</span>Precision and Recall</a></span></li><li><span><a href=\"#Precision-and-Recall-Tradeoff\" data-toc-modified-id=\"Precision-and-Recall-Tradeoff-13.1.9\"><span class=\"toc-item-num\">13.1.9&nbsp;&nbsp;</span>Precision and Recall Tradeoff</a></span></li></ul></li><li><span><a href=\"#Predictions-on-Test-set\" data-toc-modified-id=\"Predictions-on-Test-set-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Predictions on Test set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Actual-Conversions-vs-Conversion-Probability\" data-toc-modified-id=\"Actual-Conversions-vs-Conversion-Probability-13.2.1\"><span class=\"toc-item-num\">13.2.1&nbsp;&nbsp;</span>Actual Conversions vs Conversion Probability</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.2.2\"><span class=\"toc-item-num\">13.2.2&nbsp;&nbsp;</span>Confusion Matrix</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-13.2.3\"><span class=\"toc-item-num\">13.2.3&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#Metrics-Beyond-Simple-Accuracy\" data-toc-modified-id=\"Metrics-Beyond-Simple-Accuracy-13.2.4\"><span class=\"toc-item-num\">13.2.4&nbsp;&nbsp;</span>Metrics Beyond Simple Accuracy</a></span></li></ul></li></ul></li><li><span><a href=\"#Lead-Scoring\" data-toc-modified-id=\"Lead-Scoring-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Lead Scoring</a></span></li><li><span><a href=\"#Score-Sheet-for-X-Education\" data-toc-modified-id=\"Score-Sheet-for-X-Education-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Score Sheet for X Education</a></span></li><li><span><a href=\"#KS-Statistic\" data-toc-modified-id=\"KS-Statistic-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>KS Statistic</a></span></li><li><span><a href=\"#Gain-Chart\" data-toc-modified-id=\"Gain-Chart-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Gain Chart</a></span></li><li><span><a href=\"#Lift-Chart\" data-toc-modified-id=\"Lift-Chart-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Lift Chart</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"\n# Lead Scoring for X Education\n"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n \nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n \nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:\n \nX Education has appointed us to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires us to build a model wherein we need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%."},{"metadata":{},"cell_type":"markdown","source":"## Business Goals"},{"metadata":{},"cell_type":"markdown","source":"- X-Education wants to improve their lead conversion.\n- Rather than randomly pursuing leads, the company wants to create a pool of Hot Leads the sales team could focus on.\n- They have tasked us to score their leads betwen 0-100 based on the probability of conversion. 100 being the most likely to convert and 0 being unlikely to convert."},{"metadata":{},"cell_type":"markdown","source":"## Analysis Approach & Conclusions"},{"metadata":{},"cell_type":"markdown","source":"Lead scoring is a class probability estimation problem, a form of classification problem. The target variable in the data set has two classes : 0 - Un converted and 1 - Converted. The objective is to model the probability(p) that each lead belongs to the class - `Converted`. Since there are just two classes - it follows that the probability of belonging to class - Un-Converted is (1-p). The relationship between probability of conversion of each lead and its characteristics is modelled using Logistic Regression. And the leads are scored on a scale of 0-100, 100 being most probable conversion candidate. \n\nThe final solution has been provided in two parts. \n    1. Scoring the leads provided by the company in the order of probability of conversion (0-100)\n    2. Insights into the relationship between characteristics of a lead and the log-odds probability of conversion that could help the company score leads in the future. \n      \n      \nA logistic regression model is created using lead features. To arrive at the list of features which significantly affect conversion probability, a mixed feature elimination approach is followed. 25 most important features are obtained through Recursive Feature Elimination and then reduced to 15 via p-value / VIF approach. The dataset is randomly divided into train and test set. (70 - 30 split).   \n  \nThe final relationship between log Odds of Conversion Probability and lead features is    \n  \n`logOdds(Conversion Probability)` = -0.6469 - 1.5426 * `Do Not Email` -1.2699 * `Unknown Occupation` -0.9057 * `No Specialization` -0.8704 * `Hospitality Management` - 0.6584 * `Outside India` + 1.7923 * `SMS Sent` + 1.1749 * `Other Last Activity` + 2.3769 * `Working Professional` - 0.8614 * `Olark Chat Conversation` + 5.3886 * `Welingak Website` + 3.0246 * `Reference`  + 1.1876 * `Olark Chat` -1.0250 * `Landing Page Submission` + 1.1253 * `Total Time Spent on Website` + 0.6106 * `Email Opened`   \n  \n  where `Total Time Spent on Website` is standardized to $\\mu=0,\\sigma=1$   \n    \nInterpreting Top 6 features affecting Conversion Probability :   \n- A lead from `Welingak Website` has 5.4 times higher log odds of conversion than those from `Google`. \n- Leads through `Reference` have 3 times higher log odds of conversion than those from `Google`.\n- Leads from `Working Professional` have 2.38 times higher log odds of conversion than those from `Businessman`.\n- Leads with `SMS Sent` have 1.8 times higher log odds of conversion than those with no SMS sent.\n- Leads with `Do Not Email` have 1.5 times lesser log odds of conversion compared to leads who would like email updates.\n- Leads with `Unknown Occupation` have 1.27 times lesser log odds of conversion compared to those from `Businessman`.\n\nLead Scores :  \n- Score sheet can be generated by running coding in the cell named `Score Sheet for X Education` cell in the analysis notebook.\n  \nAt an optimum cut-off probability of 0.36, model performance is as follows.   \n\nModel Performance on Training Set :   \n\n- Accuracy : 81.7%\n- Sensitivity / Recall:  80.393 %\n- Specificity :  81.772 %\n- Precision / Positive Predictive Power :  72.924 %\n- False Positive Rate :  18.228 %\n- AUC Score : 0.81\n\nModel Performance for Test Set :   \n\n- Accuracy : 79.593 %\n- Sensitivity / Recall :  77.605 % \n- Specificity : 80.81% \n- Precision / Positive Predictive Power :  71.224 %\n- False Positive Rate :  19.19 %\n- AUC Score : 0.79\n\nKS statistic :   \n- Max KS Statistic is 59.76 for 5th decile\n- This model discriminates between Converted and Non-converted leads well since KS Statistic in 4th decile (58.11) is greater than 40%. Hence, this is a reasonably good model. \n\nGain : \n- Instead of pursuing leads randomly, pursuing the top 40% leads scored by the model would let the sales team reach 80% of leads likely to convert.\n\nLift : \n- The model outperforms a random model by alteast 2 times in identifying the top 40% potentially convertible leads. \n- As opposed to 10% conversions from 10% leads pursued randomly, pursuing the top 10% leads scored by this model would lead to 24% conversions. \n     \n       \n       \nNote : \n- Incorrect data types have been corrected \n- Columns with high missing values have been dropped. \n- Columns which do not explain variability in the model have been dropped. \n- Columns with sales teams notes like `Tags` where the classes are not mutually exclusive have been dropped.\n- Features with low missing values have been imputed with the most frequent values. \n- Categories in a feature with less than 1% contribution have been grouped together to reduce the number of levels. \n- Inconsistencies in Categories have been corrected. \n- 97.5 % of the leads provided by the company have been used for analysis. \n- Class imbalance = 0.6\n- Indicator variables have been created for all categorical variables with the first category as the reference. \n- Continuous variables have been standardized $\\mu : 0 , \\sigma = 1$ before modelling.\n "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport seaborn as sns \nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n!pip install tabulate\nfrom tabulate import tabulate \n\n!pip install sidetable\nimport sidetable\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to table print  a dataframe \ndef tab(ser) : \n        print(tabulate(pd.DataFrame(ser), headers='keys', tablefmt=\"psql\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# importing the dataset\nleads = pd.read_csv('../input/leads-dataset/Leads.csv')\n\n# Inspecting few column heads at a time \nfor i in range(0,leads.shape[1], 5) : \n    if i+4 <= leads.shape[1] : \n        print('Columns : ',i,' to ',i+4)\n    else : \n        print('Columns : ',i,' to last')\n    tab(leads.iloc[:,i : i+5].head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# dataset information \nleads.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This data set has a total of 9240 records ,each with 36 features.\n- Each record represents the characteristics of a lead and whether the lead was converted. \n- `Converted` column indicates whether the particular lead was converted to a client. This is our target variable."},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning "},{"metadata":{},"cell_type":"markdown","source":"### Incorrect data types \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Converted' is a binary categorical variable but the info shows it is `int64`. Converting to `category` data type \nleads['Converted'] = leads['Converted'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Duplicates "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Checking for any duplicate leads / prospects \n\nduplicate_prospect_ids = leads['Prospect ID'][leads['Prospect ID'].duplicated()].sum()\nduplicate_lead_no = leads['Lead Number'][leads['Lead Number'].duplicated()].sum()\nprint('No of Duplicate Prospect IDs : ', duplicate_prospect_ids)\nprint('No of Duplicate Lead Nos : ', duplicate_lead_no)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are no duplicate prospect IDs or Lead Numbers\n- Since, these are dimensions (i.e identification columns) not required for analysis, they could be popped for re-indentification at a later step."},{"metadata":{},"cell_type":"markdown","source":"### Separating ID columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Popping Prospect ID and Lead Number columns for later use\nprospect_ids = leads.pop('Prospect ID')\nlead_no = leads.pop('Lead Number')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Null values in each Column\nnulls = pd.DataFrame(100*leads.isnull().sum()/leads.shape[0])\nnulls.columns = ['Null Percentage']\n\n# Sorting null percentages in descending order and highlighting null % > 45 \nnulls[nulls['Null Percentage'] !=0].sort_values(by ='Null Percentage', ascending=False).style.applymap(lambda x : 'color : red' if x > 45 else '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- More than 45% of the leads have missing values in `Lead Quality`,`Asymmetrique Profile Score`,\n`Asymmetrique Activity Score`,\n`Asymmetrique Profile Index`,\n`Asymmetrique Activity Index`\n- Further, the data in these columns is filled by the sales team and the values depend heavily on the team's judgement. These columns are not good candidates for modelling since the values are subjective. \n- Hence these columns could be dropped. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns with null percentage > 45\nhigh_null_col = nulls[nulls['Null Percentage'] >=45].index\nleads.drop(columns=high_null_col, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Rows Missing Target Variable \nprint('Number of rows with missing Target Variable : ',leads['Converted'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No rows with missing target value "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rows missing more than 50% of values \nhighNullRowsCondition = leads.isnull().sum(axis=1)/leads.shape[1] > 0.5\nleads[highNullRowsCondition].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No rows missing more than 50% of values."},{"metadata":{},"cell_type":"markdown","source":"### Disguised missing Values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Categorical columns\ncondition = leads.dtypes == 'object'\ncategoricalColumns = leads.dtypes[condition].index.values\ncategoricalColumns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# value counts of each label in a categorical feature \ndef cat_value_counts(column_name) : \n    '''\n    prints unique values and value counts of each label in categorical column\n    '''\n    print(tabulate(pd.DataFrame(leads.stb.freq([column_name])), headers='keys', tablefmt='psql'))\n    print(pd.DataFrame(leads[column_name]).stb.missing(),'\\n\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Looking at value counts of each label in categorical variables\nfor col in sorted(categoricalColumns) : \n    print(col)\n    cat_value_counts(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The following columns have a label `Select` which is a disguised missing value.\n- `Select` is the default option in online forms and this value might mean that the lead hasn't selected any option.\n- We shall replace them with `np.nan`. \n    - `Specialization`\n    - `Lead Profile`\n    - `City`\n    - `How did you hear about X Education`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing Select with NaN value\nleads.replace({'Select' : np.nan},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Looking at Missing Values again \n\n# Null values in each Column\nnulls = pd.DataFrame(100*leads.isnull().sum()/leads.shape[0])\nnulls.columns = ['Null Percentage']\n\n# Sorting null percentages in descending order and highlighting null % > 50 \nnulls[nulls['Null Percentage'] !=0].sort_values(by ='Null Percentage', ascending=False).style.applymap(lambda x : 'color : red' if x > 50 else '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Lead Profile` &  `How did you hear about X Education` have very high percentage of nulls. Let's drop these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.drop(columns=['Lead Profile','How did you hear about X Education'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputation"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Sorting null percentages in ascending order and highlighting null % < 16\ndef lowNulls() : \n    nulls = pd.DataFrame(100*leads.isnull().sum()/leads.shape[0])\n    nulls.columns = ['Null Percentage']\n    return nulls[nulls['Null Percentage'] !=0].sort_values(by ='Null Percentage', ascending=True).style.applymap(lambda x : 'color : green' if x < 16 else '')\n\nlowNulls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'Lead Source','Last Activity','TotalVisits','Page Views Per Visit' have less than 2% missing values. These rows could be dropped.\n- We could impute columns with higher missing values on a case by case basis.\n- Missing values are imputed by the metric most representative of the feature's distribution. \n- For categorical features, missing values could be imputed by the most frequently occuring label i.e **MODE** value, since this is the most representative metric of a categorical feature. \n- For continuous features, if there are outliers, the most representative metric of the feature's distribution is **MEDIAN**, else it is **MEAN**. Continuous feature imputations are thus dependent on presence of outliers. "},{"metadata":{},"cell_type":"markdown","source":"- About 26% of data in Country Column is missing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country Imputation : \ntab(leads.stb.freq(['Country']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since 95% of leads come from India, it is probable that missing values are from India."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Imputing missing values in Country feature with \"India\"\nleads['Country'].fillna('India', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Specialization` feature has 36% of missing values.\n- Since there's no one label that's driving leads, replacement would mislead the analysis.\n- Hence, we could impute missing values with a new label 'No Specialization'"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Imputing Null Values by filling it using \"No Specialization\".\nleads['Specialization'].fillna(\"No Specialization\",inplace=True)\nprint('Missing values in Specialization feature ', leads['Specialization'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- About 39% of values in `City` column are missing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputation of missing cities \nleads.stb.freq(['City'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Missing Cities vs Country\ncondition_india = leads['Country'] == 'India'\nprint('Total Missing City values :', leads['City'].isnull().sum())\nprint('Missing City values in leads from India : ',leads.loc[condition_india,'City'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Looks like 3609 out 3669 leads with Missing City label are from India. \n- As can be seen from the value counts of `City` feature, 60% of the leads come from Mumbai.\n- Since, we could impute missing `City` value for leads from India with `Mumbai`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing Null Cities in India with Mumbai\ncondition = (leads['City'].isnull()) & condition_india\nleads.loc[condition,'City'] = 'Mumbai'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 29% of values in `What is your current occupation` column are missing. \n- Let's look at the distribution of levels in this column"},{"metadata":{"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"tab(leads.stb.freq(['What is your current occupation']))\ntab(leads['What is your current occupation'].reset_index().stb.missing())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since, the business problem clearly says the company targets working professionals, this is an extremely important variable. \n- So to keep the analysis unbiased, we could impute missing values with a new level for now. \n- Let's replace missing values in `What is your current occupation` with 'Unknown Occupation'"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['What is your current occupation'].fillna('Unknown Occupation',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Missing Values in `What matters most to you in choosing a course`\nftr = 'What matters most to you in choosing a course'\ntab(leads.stb.freq([ftr]))\ntab(leads[ftr].reset_index().stb.missing())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Almost all leads ( >99%) show interest in the company's offerings for `Better Career Prospects` , excluding leads who havent filled this feature. \n- Since this might be a very important feature from the analysis perspective, to keep the analysis unbiased, instead of imputation with an existing label, let's impute with a new label for now. \n- Let's replace missing values in `What matters most to you in choosing a course` with 'Unknown Target' for now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling Missing Values with `Unknown`\nleads[ftr].fillna('Unknown Target',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Incorrect Labels"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Looking at labels in each Categorical Variable to check for incorrect labels. \ncategoricalFeatures = leads.dtypes[leads.dtypes == 'object'].index.values\nprint('Categorical Features : ', categoricalFeatures,'\\n\\n')\nfor feature in categoricalFeatures : \n    print('Levels in ',feature,' are ' , leads[feature].unique(),'\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can clearly see that Google is appearing twice in 'Lead Source'- (Google,google)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing 'google' with 'Google\nleads['Lead Source']=leads['Lead Source'].str.replace(\"google\",\"Google\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Categorical Features\n#### Dropping Unnecessary Columns"},{"metadata":{"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"# Missing Values and Value Counts for all categorical Variables \ntab(leads.stb.missing())\nprint('Value Counts of each Feature : \\n')\nfor feature in sorted(categoricalFeatures) : \n    tab(leads.stb.freq([feature]))\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's look for columns have more than 99% leads have the same level\n- Such variables do not explain any variability. \n- Hence, these columns are unnecessary to the analysis . They could be dropped "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns having only one label - since these donot explain any variability in the dataset\n\ninvariableCol = ['Digital Advertisement','Do Not Call','Get updates on DM Content','Magazine','Newspaper','Newspaper Article','Receive More Updates About Our Courses','Search',\n'Update me on Supply Chain Content','Through Recommendations',\n'I agree to pay the amount through cheque',\"What matters most to you in choosing a course\",'X Education Forums']\nleads.drop(columns=invariableCol, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Tags feature\ntab(leads.stb.freq(['Tags']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Tags` column shows remarks by the sales team. This a subjective variable based on judgement of the team and cannot be used for analysis since the lables might change or might not always be available. \n- Also, it has a lot of levels that don't seem like mutually exclusive classes\n- Let's drop this feature for this analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping Tags feature \nleads.drop(columns=['Tags'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Last Notable Activity ` & `Last Activity` seem to have similar levels \n- Lets look at the possibility of dropping one of them"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Last Notable Activity vs Last Activity\nleads_copy = leads.copy()\nleads_copy['Converted'] = leads_copy['Converted'].astype('int')\ntab(leads_copy.stb.freq(['Last Notable Activity'],value='Converted'))\ntab(leads_copy.stb.freq(['Last Activity'],value='Converted'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Last Activity` has more levels compared to `Last Notable Activity`\n-  `Last Notable Activity` seems like a column derived by the sales team using `Last Activity`. \n- Since this insight might not be available for a new lead, let'd drop `Last Notable Activity`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.drop(columns = ['Last Notable Activity'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Looking at Missing Values again \ntab(leads.stb.missing())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above, the number of missing values is less than 2%. These are deemed missing completely at random. And these rows could be dropped without affecting the analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.dropna(inplace=True)\ntab(leads.stb.missing())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grouping Labels with less leads"},{"metadata":{},"cell_type":"markdown","source":"##### Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country distribution\ntab(leads.stb.freq(['Country']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We see that leads from India make 97% of all leads. And others collectively make up 3% and the contribution of each of these countries is <= 1%.\n- To reduce the levels, let us group the minority labels into a new label called 'Outside India'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping Countries with very low lead count into 'Outside India' \n\nleadsByCountry = leads['Country'].value_counts(normalize=True)\nlowLeadCountries = leadsByCountry[leadsByCountry <= 0.01].index\n\nleads['Country'].replace(lowLeadCountries,'Outside India',inplace=True)\ntab(leads.stb.freq(['Country']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lead Origin"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = 'Lead Origin'\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We see that lead origins like `Lead Add Form`,`Lead Import` are less than 1% of all origins. \n- Let's group them into a level called 'Other Lead Origins'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping lead origins \nleadOriginsToGroup = [\"Lead Add Form\",\"Lead Import\"]\nleads[feature] = leads[feature].replace(leadOriginsToGroup, ['Other Lead Origins']*2)\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lead Source"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = 'Lead Source'\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lead sources from #7 to #18 contribute to less than 1% of all sources. \n- Let's group these into a new label called 'Other Lead Sources'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping lead Sources\nlabelCounts = leads[feature].value_counts(normalize=True)\n\n# labels with less than 1% contribution\nlabelsToGroup = labelCounts[labelCounts < 0.01].index.values\n\nleads[feature] = leads[feature].replace(labelsToGroup, ['Other '+feature+'s']*len(labelsToGroup))\n\ntab(leads.stb.freq([feature]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Last Activity"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"feature = 'Last Activity'\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Leads from #9 to #16 contribute to less than 1% of all last activity labels.Moreover, each of these labels contributes to less than 1% of leads. \n- Let's group these into a new label called 'Other Last Activity'"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Grouping Last Activity\nlabelCounts = leads[feature].value_counts(normalize=True)\n\n# labels with less than 2% contribution\nlabelsToGroup = labelCounts[labelCounts < 0.01].index.values\n\nleads[feature] = leads[feature].replace(labelsToGroup, ['Other '+feature]*len(labelsToGroup))\n\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Specialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = 'Specialization'\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lead from from #16 to #18 contribute to less than 2% of all Specialization categories. Moreover, each of these categories contributes to less than 1% of leads. \n- Let's group these into a new label called 'Other Specializations'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping Last Activity\nlabelCounts = leads[feature].value_counts(normalize=True)\n\n# labels with less than 2% contribution\nlabelsToGroup = labelCounts[labelCounts <=0.012121].index.values\n\nleads[feature] = leads[feature].replace(labelsToGroup, ['Other '+feature]*len(labelsToGroup))\n\ntab(leads.stb.freq([feature]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Cleaning tasks have been completed. No more missing values exist"},{"metadata":{},"cell_type":"markdown","source":"## Retained Data"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Columns retained \nprint('Retained Columns\\n\\n', leads.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Retained rows\nprint('Retained rows : ',leads.shape[0]) \nprint(\"Ratio of retained rows\", 100*leads.shape[0]/9240)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Imbalance"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tab(leads.stb.freq(['Converted']))","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"converted_cond = leads['Converted'] == 1\nimbalance = leads[converted_cond].shape[0]/leads[~converted_cond].shape[0]\nprint('Class Imbalance : Converted /Un-converted =', np.round(imbalance,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above, you can see that this data set contains 37% of converted leads and 62% of un-converted leads.  \n- Ratio of classes = 0.6  \n- The dataset is skewed towards 'unconverted leads'  "},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def categoricalUAn(column,figsize=[8,8]) : \n    \n    ''' Function for categorical univariate analysis '''\n    print('Types of ' + column)\n    tab(leads.stb.freq([column]))\n    \n    converted = leads[leads['Converted'] == 1]\n    unconverted = leads[leads['Converted'] == 0]\n      \n    print(column + ' for Converted Leads')\n    \n    tab(converted.stb.freq([column]))\n    \n    print(column + ' for Un-Converted Leads')\n    \n    tab(unconverted.stb.freq([column]))\n    \n    print(column + ' vs Conversion Rate')\n    \n    tab((converted[column].value_counts()) / (converted[column].value_counts() + unconverted[column].value_counts()))\n    \n    # bar plot\n    plt.figure(figsize=figsize)\n    ax = sns.countplot(y=column,hue='Converted',data=leads)\n    title = column + ' vs Lead Conversion'\n    ax.set(title= title)\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lead Origin"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"column = 'Lead Origin'\ncategoricalUAn(column,figsize=[8,8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  Leads from `Landing Page Submission` followed by `API` make up 93% of all leads. \n- But it is interesting that 8.3% of leads coming from other sources have the highest conversion rate of 87.5% "},{"metadata":{},"cell_type":"markdown","source":"### Lead Source"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"column = 'Lead Source'\ncategoricalUAn(column,figsize=[8,8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most leads that get converted come from `Google`(31%), followed by `Direct Traffic`(28%) and `Olark Chat`(19%)\n- And leads through `Reference` have a very high conversion rate (91%)"},{"metadata":{},"cell_type":"markdown","source":"### Do not Email "},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'Do Not Email'\ncategoricalUAn(feature,figsize=[8,8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 92% of leads prefer to be sent Emails about the company. `Do not Email = No`\n- And these are the most converted customers (40%)"},{"metadata":{},"cell_type":"markdown","source":"### Last Activity"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# 'Last Activity'\nfeature = 'Last Activity'\ncategoricalUAn(feature,figsize=[8,8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n- Most leads open emails sent to them (38%) and that's their last activity. \n- Among those leads who's last activity is opening emails, 37% are converted. \n- Only 4% of last activity indicators show `Converted to Lead`\n- Last activiy as 'SMS Sent' has highest conversion rate (62%).\n- Last activiy as 'Email Bounced' has lowest conversion rate (7.9%).\n"},{"metadata":{},"cell_type":"markdown","source":"### Country"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'Country'\ncategoricalUAn(feature,figsize=[8,8])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most leads come from India (97%) \n- Out of these 38% are converted. "},{"metadata":{},"cell_type":"markdown","source":"### Specialization"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'Specialization'\ncategoricalUAn(feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Specialization of 36% of leads is missing.\n- We have mapped those missing values with 'No Specialization'.There might be two reason for this,\n    - Lead might be a fresher.\n    - Lead missed to fill it. \n- Among all the specializations, ' Banking, Investment And Insurance' has the highest conversion rate(48.9%).\n\n     "},{"metadata":{},"cell_type":"markdown","source":"### What is your current occupation"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'What is your current occupation'\ncategoricalUAn(feature,figsize=[8,8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Although the conversion rate for Working Professional is the highest ! 91.6%, they only make 7.4% of all leads. 60% leads are Unemployed customers followed by 29% with unknown nature of employment\n- Among all the converted leads, Unemployed and Working Professionals top the list. \n- Conversion for Housewife segment is 100% "},{"metadata":{},"cell_type":"markdown","source":"### City"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'City'\ncategoricalUAn(feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most Leads come from 'Mumbai' and they have a decent conversion rate of 36.4%.\n- Leads from Thane and outskirts make up 8.2% with a conversion rate of  44%"},{"metadata":{},"cell_type":"markdown","source":"### A free copy of Mastering The Interview."},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"feature = 'A free copy of Mastering The Interview'\ncategoricalUAn(feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 68% of the leads said \"No\" for a free copy of 'Mastering The Interview'.\n- Conversion rate of leads who said \"No\" is high (39.8%)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_univariate_analysis(column_name,scale='linear') : \n    \n    converted = leads[leads['Converted'] == 1]\n    unconverted = leads[leads['Converted'] == 0]\n\n    plt.figure(figsize=(8,6))\n    ax = sns.boxplot(x=column_name, y='Converted', data = leads)\n    title = 'Boxplot of ' + column_name+' vs Conversion'\n    ax.set(title=title)\n    if scale == 'log' :\n        ax.set_xscale('log')\n        ax.set(ylabel=column_name + '(Log Scale)')\n        \n    print(\"Spread for range of \"+column_name+\" that were Converted\")\n    tab(converted[column_name].describe())\n    print(\"Spread for range of \"+column_name+\" that were not converted\")\n    tab(unconverted[column_name].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### TotalVisits\n"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"column_name = 'TotalVisits'\nnum_univariate_analysis(column_name,scale='log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Looks like `Total Visits` have a lot of outliers among both `Converted` and `Un-converted` leads. \n- Let's take a look at the quantiles between 90 and 100. "},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Looking at Quantiles\ntab(leads[column_name].quantile(np.linspace(.90,1,20)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above, it is clear that outliers exist and these might skew the analyses. \n- For now, let's cap the outliers about 99th percentile to 99th percentile value.  `soft range` capping. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capping outliers to 99th perentile value \ncap = leads[column_name].quantile(.99)\ncondition = leads[column_name] > cap \nleads.loc[condition, column_name] = cap ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total TIme Spent on Website."},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"column = 'Total Time Spent on Website'\nnum_univariate_analysis(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'Total Time Spend on Website' has many outliers. \n- Let's look quantiles to confirm this. "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"tab(leads[column].quantile(np.linspace(0.75,1,25)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads[column].quantile(np.linspace(0.75,1,50)).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capping `Total Time Spent on Website` values to 99th percentile \ncap = leads[column].quantile(.99)\ncondition = leads[column] > cap \nleads.loc[condition, column] = cap ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Page Views Per Visit."},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"column = 'Page Views Per Visit'\nnum_univariate_analysis(column) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'Page Views Per Visit' has many outliers. \n- Let's look quantiles to confirm this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"leads[column].quantile(np.linspace(0.75,1,30)).plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There is a sudden jump between 99th percentile and maximum value. \n- let's cap the values to 99th percentile to avoid skewing the analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capping `Page Views Per Visit` values to 99th percentile \ncap = leads[column].quantile(.99)\ncondition = leads[column] > cap \nleads.loc[condition, column] = cap ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_vars = ['TotalVisits', 'Page Views Per Visit', 'Total Time Spent on Website']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TotalVisits vs A free copy of Mastering The Interview"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[8,8])\nsns.barplot(x=continuous_vars[0], y = 'A free copy of Mastering The Interview', data=leads, hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- One can see that the proportion of leads with high Total Visits to the website also like a free copy of Mastering The Interview. \n- Incidentally, these are leads with higher conversion rate. \n- More convertable leads are being attracted by the website through providing 'A free copy of Mastering The Interview'."},{"metadata":{},"cell_type":"markdown","source":"### Lead Source vs Country "},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.barplot(x='Lead Source', y = 'Country', hue='Converted', data=leads)\nleads.groupby(['Country','Lead Source'])['Converted'].value_counts(normalize=True)\\\n.unstack()\\\n   .plot( \n    layout=(2,2),\n    figsize=(14,12), kind='barh', stacked=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most leads from India through Reference Sources have very high conversion rate. \n- Leads from outside of India from other Lead sources do not convert at all."},{"metadata":{},"cell_type":"markdown","source":"### Occupation vs City"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"x = \"What is your current occupation\"\ny = 'City'\n\nleads.groupby([x,y])['Converted'].value_counts(normalize=True)\\\n.unstack()\\\n   .plot( \n    layout=(2,2),\n    figsize=(14,12), kind='barh', stacked=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Working Professionals in Other cities of Maharashtra have higher conversion rates compared to those from Mumbai , Thane and other cities.\n- BusinessMen from Mumbai and Thane & Outskirts are poor leads in comparison to Tier 2 and Other cities . "},{"metadata":{},"cell_type":"markdown","source":"### Last Activity vs Country"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"x = \"Country\"\ny = 'Last Activity'\n\nleads.groupby([x,y])['Converted'].value_counts(normalize=True)\\\n.unstack()\\\n   .plot( \n    layout=(2,2),\n    figsize=(14,12), kind='barh', stacked=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- SMS and Emails are more favourable for conversion over Website Visits outside of India.\n- Leads from outside of India who click email links have higher conversion rate compared those from India. "},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Mapping Binary Variables to 0 / 1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_var = ['Do Not Email', 'A free copy of Mastering The Interview']\nleads[binary_var] = leads[binary_var].replace({'Yes' : 1, 'No' : 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Indicator Variables"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"categoricalCol = ['Lead Origin', 'Lead Source','Last Activity', 'Country', 'Specialization',\n       'What is your current occupation', 'City'] \n\nprint('Levels in Each Cateogrical Variable\\n')\nfor col in sorted(categoricalCol) : \n    print(col, leads[col].unique(), '\\n')\n\n# Creating dummy variables\nleadOriginDummies = pd.get_dummies(leads['Lead Origin'], drop_first=True)\nleadSourceDummies = pd.get_dummies(leads['Lead Source'], drop_first=True)\nlastActivityDummies = pd.get_dummies(leads['Last Activity'], drop_first=True)\ncountryDummies = pd.get_dummies(leads['Country'] ,drop_first=True)\nspecDummies = pd.get_dummies(leads['Specialization'],drop_first=True)\noccupationDummies = pd.get_dummies(leads[ 'What is your current occupation'],drop_first=True)\ncityDummies = pd.get_dummies(leads[ 'City'],drop_first=True)\n\n# adding dummy variables to leads dataframe\nleads = pd.concat([leads, leadOriginDummies,leadSourceDummies,lastActivityDummies, countryDummies, specDummies, occupationDummies, cityDummies], axis=1)\n\n# dropping categorical columns \nleads.drop(columns = categoricalCol, inplace=True)\n\n\nprint('Final Columns')\nleads.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top Correlations\ndef correlation(dataframe) : \n    cor0=dataframe.corr()\n    type(cor0)\n    cor0.where(np.triu(np.ones(cor0.shape),k=1).astype(np.bool))\n    cor0=cor0.unstack().reset_index()\n    cor0.columns=['VAR1','VAR2','CORR']\n    cor0.dropna(subset=['CORR'], inplace=True)\n    cor0.CORR=round(cor0['CORR'],2)\n    cor0.CORR=cor0.CORR.abs()\n    cor0.sort_values(by=['CORR'],ascending=False)\n    cor0=cor0[~(cor0['VAR1']==cor0['VAR2'])]\n    return pd.DataFrame(cor0.sort_values(by=['CORR'],ascending=False))\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"#Correlations for Converted Leads \nconvertedCondition= leads['Converted']==1\nprint('Correlations for Converted Leads')\ncorrelation(leads[convertedCondition])[1:30:2].style.background_gradient(cmap='GnBu').hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Conversions of leads from other lead origins and the ones through reference have similar conversion behaviour."},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"#Correlations for un-Converted Leads \nunconvertedCondition=leads['Converted']==0\nprint('Correlations for Non-Converted Leads')\ncorrelation(leads[unconvertedCondition])[1:30:2].style.background_gradient(cmap='GnBu').hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above, `Unknown Occupation` and `Unemployed` are highly correlated for non-converted leads. \n- This might mean that unemployed leads and leads with unknown occupation have the same conversion behaviour. "},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = leads.pop('Converted')\nX = leads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardizing Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler()\n\n# fitting and transforming train set\nX_train[continuous_vars] = scaler.fit_transform(X_train[continuous_vars])\n\n# Transforming test set for later use\nX_test[continuous_vars] = scaler.transform(X_test[continuous_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"### Recurvise Feature Elimination "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"\nprint('No of features : ', len(X_train.columns)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Currently, the dataset has 49 features. \n- We shall follow a mixed feature elimination approach. \n- We could use Recursive Feature Elimination for coarse elimination to 25 columns\n- This is followed by manual elimination of features with high p-value / VIF. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# RFE \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nminFeatures = 25\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select=minFeatures)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Columns selected by RFE : \nRFE_features = pd.DataFrame( {'feature' : X_train.columns, 'rank' : rfe.ranking_, 'support' : rfe.support_})\ncondition = RFE_features['support'] == True \nrfe_features = RFE_features[condition].sort_values(by='rank',ascending=True )\nprint('Features selected by RFE\\n')\ntab(rfe_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfeFeatures = rfe_features['feature'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manual Feature Elimination\n#### Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Multicollinearity \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(X) :\n    df = sm.add_constant(X)\n    vif = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n    vif_frame = pd.DataFrame({'vif' : vif[0:]},index = df.columns).reset_index()\n    tab(vif_frame.sort_values(by='vif',ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 1\nimport statsmodels.api as sm \nfeatures = rfe_features['feature'].values\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Unemployed` has the highest VIF. let's drop this feature. "},{"metadata":{},"cell_type":"markdown","source":"#### Model 2 "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Model 2 : Removing `Unemployed`\ncolumn_to_remove = 'Unemployed'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Other Lead Origins` has a very high VIF.\n- Let's drop this variable"},{"metadata":{},"cell_type":"markdown","source":"#### Model 3"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Model 3 : Removing `Other Lead Origins`\ncolumn_to_remove = 'Other Lead Origins'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Housewife` has a high p-value and hence the coefficient is insignificant. let's drop the same. "},{"metadata":{},"cell_type":"markdown","source":"#### Model 4"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 4 : Removing `Housewife`\ncolumn_to_remove = 'Housewife'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Student` has a p-value higher than 0.05 and the highest among all p-values. Let's drop this feature."},{"metadata":{},"cell_type":"markdown","source":"#### Model 5"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 5 : Removing `Student`\ncolumn_to_remove = 'Student'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Tier II Cities` has a p-value higher than confidence level and the highest among all the p - values. \n- Let's remove this feature"},{"metadata":{},"cell_type":"markdown","source":"#### Model 6"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 6 : Removing `Tier II Cities`\ncolumn_to_remove = 'Tier II Cities'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n- `Page Views Per Visit` has a high p-value. Let's eliminate this. "},{"metadata":{},"cell_type":"markdown","source":"#### Model 7"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 7 : Removing `Page Views Per Visit`\ncolumn_to_remove = 'Page Views Per Visit'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Media and Advertising` has a high p-value. let's drop this feature"},{"metadata":{},"cell_type":"markdown","source":"#### Model 8"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 8 : Removing `Media and Advertising`\ncolumn_to_remove = 'Media and Advertising'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This model has a feature `Email Link Clicked` with high p-value of 0.059. Let's drop this feature."},{"metadata":{},"cell_type":"markdown","source":"#### Model 9 "},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 9 : Removing `Email Link Clicked`\ncolumn_to_remove = 'Email Link Clicked'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All coefficients are significant / low p-value\n- For further elimination , let's use the magnitude of coefficient as the weight/importance of the variable. Higher values are more important than lower values. \n- By this reasoning, `TotalVisits` has the least coefficient. Let'd drop this. "},{"metadata":{},"cell_type":"markdown","source":"#### Model 10 "},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 10 : Removing `TotalVisits`\ncolumn_to_remove = 'TotalVisits'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm1 = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nlogm1 = logm1.fit()\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Other Lead Sources` has high p-value. Let's drop this variable.  "},{"metadata":{},"cell_type":"markdown","source":"#### Model 11 - Final Model"},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Model 11 : Removing `Other Lead Sources`\ncolumn_to_remove = 'Other Lead Sources'\nfeatures = X_train.columns[X_train.columns !=column_to_remove]\nX_train = X_train[features]\nlogm_final = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\nlogm_final = logm_final.fit()\nprint(\"VIF for X_train\")\nvif(X_train)\nlogm_final.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above, the features that remain are statistically significant and donot show any multi collinearity. \n- Hence, we could use Model 11 is our final model. "},{"metadata":{},"cell_type":"markdown","source":"## Final Features"},{"metadata":{"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"finalFeatures = X_train.columns.values\nprint('The Final Feature for Modelling are :', finalFeatures)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{},"cell_type":"markdown","source":"###  Predictions on Train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\ny_train_pred = logm_final.predict(X_train_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Actual Conversions vs Conversion Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a data frame with converted vs converted probabilities\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ntab(y_train_pred_final.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predictions with cut off = 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ntab(y_train_pred_final.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix for Train Set     \n\n| $\\frac{Predicted}{Actual}$ | Not Converted | Converted |\n| --- | --- | --- | \n| Not Converted | 3462 | 455 |\n| Converted | 699 | 1693 |\n"},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy of the Model "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\naccuracy = metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\nprint('Accuracy on Train set : ', round(100*accuracy,3),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Metrics beyond simple accuracy"},{"metadata":{"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsePositiveRate = FP/(FP + TN)\npositivePredictivePower = TP/(TP +FP )\nnegativePredictivePower = TN/(TN + FN)\nprint('sensitivity / Recall: ', round(100*sensitivity,3),'%')\nprint('specificity : ',  round(100*specificity,3),'%')\nprint('False Positive Rate : ',  round(100*falsePositiveRate,3),'%')\nprint('Precision / Positive Predictive Power : ',  round(100*positivePredictivePower,3),'%')\nprint('Negative Predictive Power : ',  round(100*negativePredictivePower,3),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting ROC Curve."},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding Optimal Cutoff Point"},{"metadata":{},"cell_type":"markdown","source":"- Optimal cutoff probability is that prob where we get balanced sensitivity and specificity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ntab(y_train_pred_final.head())","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\ntab(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various cutoff probabilities.\n\nfig,ax = plt.subplots()\nfig.set_figwidth(30)\nfig.set_figheight(10)\nplots=['accuracy','sensi','speci']\nax.set_xticks(np.linspace(0,1,50))\nax.set_title('Finding Optimal Cutoff')\nsns.lineplot(x='prob',y=plots[0] , data=cutoff_df,ax=ax)\nsns.lineplot(x='prob',y=plots[1] , data=cutoff_df,ax=ax)\nsns.lineplot(x='prob',y=plots[2] , data=cutoff_df,ax=ax)\n\nax.set_xlabel('Probabilites')\nax.set_ylabel('Accuracy,Sensitivity,Specificity')\nax.legend([\"Accuracy\",'Sensitivity','Specificity'])\n# cutoff_df.plot.line(, figure=[10,10])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  From the curve above, 0.36 is the optimum cutoff probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.36 else 0)\n\ntab(y_train_pred_final.head())","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\naccu = metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\nprint('Accuracy on Train set at Optimum Cut Off : ', round(100*accu,3),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsePositiveRate = FP/(FP + TN)\npositivePredictivePower = TP/(TP +FP )\nnegativePredictivePower = TN/(TN + FN)\nprint('sensitivity / Recall: ', round(100*sensitivity,3),'%')\nprint('specificity : ',  round(100*specificity,3),'%')\nprint('False Positive Rate : ',  round(100*falsePositiveRate,3),'%')\nprint('Precision / Positive Predictive Power : ',  round(100*positivePredictivePower,3),'%')\nprint('Negative Predictive Power : ',  round(100*negativePredictivePower,3),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ROC curve for cut off probability of 0.36\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision and Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the confusion matrix again\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Precision :TP / TP + FP\n- Recall :TP / TP + FN"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print('Precision :', confusion[1,1]/(confusion[0,1]+confusion[1,1]))\nprint('Recall :', confusion[1,1]/(confusion[1,0]+confusion[1,1]))","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Doing the same using the sklearn.\nfrom sklearn.metrics import precision_score, recall_score\nprint('Precision : ', precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))\nprint('Recall :', recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision and Recall Tradeoff\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The cut off point from precision-recall curve is ~0.4. \n- Note that we have used the cut off obtained from 'Sensitivity-Specificity' trade off to predict conversions in this analysis. "},{"metadata":{},"cell_type":"markdown","source":"### Predictions on Test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test[finalFeatures])\ny_test_pred = logm_final.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Actual Conversions vs Conversion Probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicted conversions vs actual conversions and customer ID\ny_test_predictions = pd.DataFrame({'Converted' :y_test, 'Conversion Probability' : y_test_pred, 'CustID' : y_test.index})\ntab(y_test_predictions.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions with optimal cut off = 0.35\ncutoff=0.36\ny_test_predictions['Predicted'] = y_test_predictions[\n 'Conversion Probability'\n].map(lambda x : 1 if x > cutoff else 0 ) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test_predictions['Converted'], y_test_predictions['Predicted'])\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy "},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print('Accuracy on Test set : ', round(100*(TP + TN)/(TP + TN + FP + FN),3),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Metrics Beyond Simple Accuracy "},{"metadata":{"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"sensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsePositiveRate = FP/(FP + TN)\npositivePredictivePower = TP/(TP +FP )\nnegativePredictivePower = TN/(TN + FN)\nprint('sensitivity / Recall: ', round(100*sensitivity,3),'%')\nprint('specificity : ',  round(100*specificity,3),'%')\nprint('False Positive Rate : ',  round(100*falsePositiveRate,3),'%')\nprint('Precision / Positive Predictive Power : ',  round(100*positivePredictivePower,3),'%')\nprint('Negative Predictive Power : ',  round(100*negativePredictivePower,3),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ROC curve for cut off probability of 0.364\ndraw_roc(y_test_predictions['Converted'],y_test_predictions['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Note the AUC is 0.79 on the test test"},{"metadata":{},"cell_type":"markdown","source":"## Lead Scoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging final predictions with leads dataset\nconversionProb = pd.concat([y_test_predictions['Conversion Probability'],y_train_pred_final['Converted_Prob']],axis=0)\nconversionProb = pd.DataFrame({'Conversion Probability' : conversionProb}, index=conversionProb.index)\nleads = pd.concat([leads,conversionProb],axis=1)\nleads['Prospect ID'] = prospect_ids\nleads['Lead No'] = lead_no\nleads['Converted'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying prediction accuracy\nleads['Predicted'] = leads['Conversion Probability'].map(lambda x : 1 if x > 0.36 else 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(leads['Converted'], leads['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nacc = metrics.accuracy_score(leads['Converted'], leads['Predicted'])\nprint('Accuracy : ', round(100*acc,3),'%')\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsePositiveRate = FP/(FP + TN)\nfalseNegativeRate = FN/(FP + TP)\npositivePredictivePower = TP/(TP +FP )\nnegativePredictivePower = TN/(TN + FN)\nprint('sensitivity : ', round(100*sensitivity,3),'%')\nprint('specificity : ',  round(100*specificity,3),'%')\nprint('False Positive Rate : ',  round(100*falsePositiveRate,3),'%')\nprint('False Negative Rate : ',  round(100*falseNegativeRate,3),'%')\nprint('Positive Predictive Power / Precision : ',  round(100*positivePredictivePower,3),'%')\nprint('Negative Predictive Power : ',  round(100*negativePredictivePower,3),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ROC curve\ndraw_roc(leads['Converted'], leads['Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Lead Scores \nleads['Lead Score'] = leads['Conversion Probability']*100\ntab(leads[['Prospect ID','Lead No','Lead Score']].sort_values(by='Lead Score', ascending=False)[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score Sheet for X Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the following to generate a sheet containing lead information provided by the company and corresponding scores \nleads.to_csv('lead_scores.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KS Statistic"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gain Chart \ny_test_predictions = y_test_predictions.sort_values(by='Conversion Probability', ascending=False)\ny_test_predictions['decile'] = pd.qcut(y_test_predictions['Conversion Probability'],10,labels=range(10,0,-1))\ny_test_predictions['Converted'] = y_test_predictions['Converted'].astype('int')\ny_test_predictions['Un Converted'] = 1 - y_test_predictions['Converted']\ntab(y_test_predictions.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.pivot_table(data=y_test_predictions,index=['decile'],values=['Converted','Un Converted','Conversion Probability'],\n                     aggfunc={'Converted':[np.sum],\n                              'Un Converted':[np.sum],\n                              'Conversion Probability' : [np.min,np.max]})\ndf1 = df1.reset_index()\ndf1.columns = ['Decile','Max Prob', 'Min Prob','Converted Count','Un Converted Count']\ndf1 = df1.sort_values(by='Decile', ascending=False)\ndf1['Total Leads'] = df1['Converted Count'] + df1['Un Converted Count']\ndf1['Conversion Rate'] = df1['Converted Count'] / df1['Un Converted Count']\nconverted_sum = df1['Converted Count'].sum()\nunconverted_sum = df1['Un Converted Count'].sum()\ndf1['Converted %'] = df1['Converted Count'] / converted_sum\ndf1['Un Converted %'] = df1['Un Converted Count'] / unconverted_sum\ntab(df1.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['ks_stats'] = np.round(((df1['Converted Count'] / df1['Converted Count'].sum()).cumsum() -(df1['Un Converted Count'] / df1['Un Converted Count'].sum()).cumsum()), 4) * 100\ntab(df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Max KS Statistic is 59.76 for 5th decile\n- This model discriminates between Converted and Non-converted leads well since KS Statistic in 4th decile (58.11) is greater than 40%. Hence, this is a reasonable good model. "},{"metadata":{},"cell_type":"markdown","source":"## Gain Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Cum Conversion %'] = np.round(((df1['Converted Count'] / df1['Converted Count'].sum()).cumsum()), 4) * 100\ntab(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Base %'] = np.arange(10,110,10)\ndf1 = df1.set_index('Decile')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tab(df1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"### Gain chart \nplot_columns =['Base %','Cum Conversion %']\nplt.plot(df1[plot_columns]);\nplt.xticks(df1.index);\nplt.title('Gain chart');\nplt.xlabel('Decile')\nplt.ylabel('Cummulative Conversion %')\nplt.legend(('Our Model','Random Model'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Instead of pursuing leads randomly, pursuing the top 40% leads scored by the model would let the sales team reach 80% of leads likely to convert."},{"metadata":{},"cell_type":"markdown","source":"## Lift Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Lift'] = df1['Cum Conversion %'] / df1['Base %']\ndf1['Baseline'] = 1\ntab(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lift chart \nplot_columns =['Lift', 'Baseline']\nplt.plot(df1[plot_columns]);\nplt.xticks(df1.index);\nplt.title('Lift chart');\nplt.xlabel('Decile')\nplt.ylabel('Lift')\nplt.legend(('Our Model','Random Model'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The model outperforms a random model by alteast 2 times in identifying the top 40% potentially convertible leads. \n- As opposed to 10% conversions from 10% leads pursued randomly, pursuing the top 10% leads scored by this model would lead to 24% conversions. "},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"A logistic regression model is created using lead features. To arrive at the list of features which significantly affect conversion probability, a mixed feature elimination approach is followed. 25 most important features are obtained through Recursive Feature Elimination and then reduced to 15 via p-value / VIF approach. The dataset is randomly divided into train and test set. (70 - 30 split).   \n  \nThe final relationship between log Odds of Conversion Probability and lead features is    \n  \n`logOdds(Conversion Probability)` = -0.6469 - 1.5426 * `Do Not Email` -1.2699 * `Unknown Occupation` -0.9057 * `No Specialization` -0.8704 * `Hospitality Management` - 0.6584 * `Outside India` + 1.7923 * `SMS Sent` + 1.1749 * `Other Last Activity` + 2.3769 * `Working Professional` - 0.8614 * `Olark Chat Conversation` + 5.3886 * `Welingak Website` + 3.0246 * `Reference`  + 1.1876 * `Olark Chat` -1.0250 * `Landing Page Submission` + 1.1253 * `Total Time Spent on Website` + 0.6106 * `Email Opened`   \n  \n  where `Total Time Spent on Website` is standardized to $\\mu=0,\\sigma=1$   \n    \nInterpreting Top 6 features affecting Conversion Probability :   \n- A lead from `Welingak Website` has 5.4 times higher odds of conversion than those from `Google`. \n- Leads through `Reference` have 3 times higher log odds of conversion than those from `Google`.\n- Leads from `Working Professional` have 2.38 times higher log odds of conversion than those from `Businessman`.\n- Leads with `SMS Sent` have 1.8 times higher log odds of conversion than those with no SMS sent.\n- Leads with `Do Not Email` have 1.5 times lesser log odds of conversion compared to leads who would like email updates.\n- Leads with `Unknown Occupation` have 1.27 times lesser log odds of conversion compared to those from `Businessman`.\n\nLead Scores :  \n- Score sheet can be generated by running [this](#Score-Sheet-for-X-Education) cell.\n  \nAt an optimum cut-off probability of 0.36, model performance is as follows.   \n\nModel Performance on Training Set :   \n\n- Accuracy : 81.7%\n- Sensitivity / Recall:  80.393 %\n- Specificity :  81.772 %\n- Precision / Positive Predictive Power :  72.924 %\n- False Positive Rate :  18.228 %\n- AUC Score : 0.81\n\nModel Performance for Test Set :   \n\n- Accuracy : 79.593 %\n- Sensitivity / Recall :  77.605 % \n- Specificity : 80.81% \n- Precision / Positive Predictive Power :  71.224 %\n- False Positive Rate :  19.19 %\n- AUC Score : 0.79\n\nKS statistic :   \n- Max KS Statistic is 59.76 for 5th decile\n- This model discriminates between Converted and Non-converted leads well since KS Statistic in 4th decile (58.11) is greater than 40%. Hence, this is a reasonably good model. \n\nGain : \n- Inside of pursuing leads randomly, pursuing the top 40% leads scored by the model would let the sales team reach 80% of leads likely to convert.\n\nLift : \n- The model outperforms a random model by alteast 2 times in identifying the top 40% potentially convertible leads. \n- As opposed to 10% conversions from 10% leads pursued randomly, pursuing the top 10% leads scored by this model would lead to 24% conversions. \n     \n       \n       \nNote : \n- Incorrect data types have been corrected \n- Columns with high missing values have been dropped. \n- Columns which do not explain variability in the model have been dropped. \n- Columns with sales teams notes like `Tags` where the classes are not mutually exclusive have been dropped.\n- Features with low missing values have been imputed with the most frequent values. \n- Categories in a feature with less than 1% contribution have been grouped together to reduce the number of levels. \n- Inconsistencies in Categories have been corrected. \n- 97.5 % of the leads provided by the company have been used for analysis. \n- Class imbalance = 0.6\n- Indicator variables have been created for all categorical variables with the first category as the reference. \n- Continuous variables have been standardized $\\mu : 0 , \\sigma = 1$ before modelling."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}