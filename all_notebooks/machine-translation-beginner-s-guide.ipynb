{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Neural Machine Translation\n\nIn this notebook, we're going to perform a Machine Translation (MT) using RNN Model. As part of this MT, we'll train the model to learn ENGLISH-FRENCH sentence pairs from the [dataset](https://www.kaggle.com/devicharith/language-translation-englishfrench) and once trained & ready, our model will accept ENGLISH & translate it to FRENCH.\n\nAs always, I will try to keep the notebook clean, well commented & organized. Please do consider it to **UPVOTE** if you find it helpful :-)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import collections\nfrom collections import Counter\n\nimport helper\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tabulate import tabulate\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"url = '../input/language-translation-englishfrench/eng_-french.csv'\n\ndata = pd.read_csv(url, header='infer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total Records\nprint(\"Total Records: \", data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for Null/Missing Values\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming Columns\ndata = data.rename(columns={\"English words/sentences\":\"Eng\", \"French words/sentences\":\"Frn\" })","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Randomly Show a English > French sentence\nx = np.random.randint(1, data.shape[0])\nprint(\"--- Random English - French Sentence --- \\n\"\n      \"English Sentence/Word: \", data.Eng[x], \"\\n\"\n      \"French Sentence/Word: \", data.Frn[x]\n     )","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Function for word count\ndef word_count (txt):\n    return len(txt.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the Word Count Function to Eng & French Columns\ndata['Eng_Count'] = data['Eng'].apply(lambda x: word_count(x))\ndata['Frn_Count'] = data['Frn'].apply(lambda x: word_count(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( '{} English Words'.format(data['Eng_Count'].sum()) ) \nprint('{} French Words'.format(data['Frn_Count'].sum()) )\n      ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, subplot_titles=(\"English\",\"French\"))\n\nfig.add_trace(\n    go.Histogram(x=data['Eng_Count'],histfunc='sum',opacity =0.8,showlegend=False,text='Eng'), row=1,col=1)\n\nfig.add_trace(\n    go.Histogram(x=data['Frn_Count'],histfunc='sum', opacity =0.8,showlegend=False,text='Frn'), row=1,col=2)\n\nfig.update_layout(height=600, width=800, title_text=\"Words Distribution\")\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Process\n\n**Tokenization** - For a neural network to predict on text data, it first has to be turned into data it can understand. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s). Here we're going to use Kera's **Tokenizer** function to turn each sentence into sequence of words\n\n**Padding** - When batching the sequence of token'd words together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length. Here we're going to use Kera's **Pad_Sequences** function\n\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Tokenize Function\ndef tokenize(x):\n    x_tk = Tokenizer(char_level = False)\n    x_tk.fit_on_texts(x)\n    return x_tk.texts_to_sequences(x), x_tk\n    #return x_tk\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Padding Function\ndef pad(x, length=None):\n    if length is None:\n        length = max([len(sentence) for sentence in x])\n    return pad_sequences(x, maxlen = length, padding = 'post')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vocabulary Size & Sequence Length (Complete Dataset)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenize English text & determine English Vocab Size \neng_seq, eng_tok = tokenize(data['Eng'])\neng_vocab_size = len(eng_tok.word_index) + 1\nprint(\"Complete English Vocab Size: \",eng_vocab_size)\n\n#Tokenize French text & determine French Vocab Size \nfrn_seq, frn_tok = tokenize(data['Frn'])\nfrn_vocab_size = len(frn_tok.word_index) + 1\nprint(\"Complete French Vocab Size: \",frn_vocab_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sequence Length (Complete Dataset) \neng_len = max([len(sentence) for sentence in eng_seq])\nfrn_len = max([len(sentence) for sentence in frn_seq])\n\nprint(\"English Sequence Length: \",eng_len,\"\\n\",\n      \"French Sequence Length: \",frn_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Split\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train (90%) and test set (10%)\ntrain_data, test_data = train_test_split(data, test_size=0.1, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Train & Test Data\n\nThe Training & Test Data contains some unwanted columns & also needed a reset of index","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop Columns\ntrain_data = train_data.drop(columns=['Eng_Count', 'Frn_Count'],axis=1)\ntest_data = test_data.drop(columns=['Eng_Count', 'Frn_Count'],axis=1)\n\n#Re-Index\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing the Training & Test Data\n\n**Note: For the purposes of this notebook/tutorial we're going to consider *English Sentences as Feature/Input* & *French Sentences as Target*.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Tokenization --\n\n# Training Data\ntrain_X_seq, train_X_tok = tokenize(train_data['Eng'])\ntrain_Y_seq, train_Y_tok = tokenize(train_data['Frn'])\n\ntrain_eng_vocab = len(train_X_tok.word_index) + 1\ntrain_frn_vocab = len(train_Y_tok.word_index) + 1\n\n# Testing Data\ntest_X_seq, test_X_tok = tokenize(test_data['Eng'])\ntest_Y_seq, test_Y_tok = tokenize(test_data['Frn'])\n\ntest_eng_vocab = len(test_X_tok.word_index) + 1\ntest_frn_vocab = len(test_Y_tok.word_index) + 1\n\n\n# -- Padding --\n\n#Training Data\ntrain_X_seq = pad(train_X_seq)\ntrain_Y_seq = pad(train_Y_seq)\n\n#Testing Data\ntest_X_seq = pad(test_X_seq)\ntest_Y_seq = pad(test_Y_seq)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Tabulate the Vocab Size\ntab_data = [[\"Train\", train_eng_vocab, train_frn_vocab],[\"Test\",test_eng_vocab,test_frn_vocab]]\nprint(tabulate(tab_data, headers=['Dataset','Eng Vocab Size','Frn Vocab Size'], tablefmt=\"pretty\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model\n\n### Model - RNN with Word Embedding\n### Architecture - LSTM\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Model\n\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps, btch_size):\n    \n    model = Sequential()\n    model.add(Embedding(in_vocab, btch_size, input_length=in_timesteps, mask_zero=True))\n    \n    model.add(LSTM(btch_size))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(btch_size, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile Parameters\nbatch_size = 64   #batch size\nlr = 1e-3          #learning rate\n\n#Model\nmodel = define_model(eng_vocab_size, frn_vocab_size, eng_len, frn_len, batch_size)\n\n#Compile Model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer = Adam(lr))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model\n\nWe'll train our model for 10 epochs and with a batch size of 64 with a validation split of 10%. 90% of the data will be used for training the model and the rest for evaluating it.\n\nWe will also use the ModelCheckpoint() function to save the model with the lowest validation loss.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fn = 'model.h1.MT'\nepoch = 2\nval_split = 0.1\n\n#Checkpoint\ncheckpoint = ModelCheckpoint(fn, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n#Train\nhistory = model.fit(train_X_seq, train_Y_seq,\n                    epochs=epoch, batch_size=batch_size, validation_split = val_split, callbacks=[checkpoint], \n                    verbose=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,8)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.title(\"Train vs Validation - Loss\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making Prediction\npredictions = model.predict(test_X_seq[1:6])[0]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def to_text(logits, tokenizer):\n\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = ''\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(to_text(predictions, frn_tok))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}