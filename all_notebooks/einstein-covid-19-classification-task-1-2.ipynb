{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Notebook description"},{"metadata":{},"cell_type":"markdown","source":"This notebook has been created for the Covid-19 prediction challenge.\n\nThe goal of this notebook is to present a methodology for using exams and patient data to create a predictive model for evaluating whether the patient has COVID-19 and if and what type of unit of treatment they will require.\n\nThe model approach was:\n1. Missing values: We first removed data-points that did not have any information other than the age quantile. We do that because if we try to include those data point's we will most likely not give a significant weight to the test results, and therefore hurt their predictive power. We could try to restrict our model to very few columns that have a higher % filled and then run our predictions, this would most likely generate a model that has more generalization power, but less accuracy when dealing with patients that have richier information.\n2. Correlated Features: We dropped features with more than 90% correlation (kept just one of the features)\n3. Feature selection: We did not apply any feature selection methodology, as we are assuming that all medical information could be relevant, but looking forward we should use the permutation importances results to decide which columns to keep\n4. Model architecture: For task1 we used ensemble methods with a randomized grid search, using a 10-fold stratified cross validation method. This was done to select the best params, but the real model performance is evaluated over an iteration of 500 random train test splits. For the task2 we used a similar method, but treated the problem as a multiclass classification one, where we used both the direct model and a One Vs All method\n5. Model deployment: To avoid overfit we suggested using Bagging classifier with 400 estimators with max samples of 0.9 (this way, the model performance should be close to the reported one)\n\nIt's important to note that the whole notebook has a number of parameters that should be altered according to the data input."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Data engineering libraries\nimport numpy as np\nimport pandas as pd\n\n# Python default libraries\nimport os\nimport warnings\nimport itertools\nimport joblib\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# Statistical functions\nfrom scipy import stats\n\n# imbalanced learn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\n# Model selection\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning libraries\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Metrics libraries\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score\n\n# Interpretability libraries\nfrom sklearn.inspection import permutation_importance\n\n# Notebook settings\n%config Completer.use_jedi = False\n%matplotlib inline\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Variable definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set random state to allow reproducible results\nrs = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Auxiliary functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function to divide the dataset into different train-test splits and assess the average performance of the model\ndef mean_performance(features_dataset, target_dataset, number_of_splits, model, percentage_test=0.2, labels=None):\n    # Define empty lists to serve as output\n    labels = [0, 1] if labels is None else sorted(labels)\n    metrics = ['precision', 'recall', 'f1', 'auc']\n    output_dict = dict()\n    for label in labels:\n        for m in metrics:\n            output_dict[m + '_class_' + str(label)] = list()\n        \n    # initialize model\n    model = model.estimator.set_params(**model.best_params_)\n    \n    for n in range(1, number_of_splits + 1):\n        # Random train test split\n        X_train, X_test, y_train, y_test = train_test_split(features_dataset, target_dataset, test_size=percentage_test, stratify=target_dataset)\n        \n        # Take the best model and fit it to the training set\n        model.fit(X_train, y_train)\n\n        # Make predictions\n        predictions = model.predict(X_test)\n        probas = model.predict_proba(X_test)\n\n        # Calculate performance metrics\n        precision, recall, f1, support = precision_recall_fscore_support(y_test, predictions, labels=labels)\n        \n        # Break the metrics by class and append results\n        dummies = pd.get_dummies(y_test)\n        for label in labels:\n            if label not in dummies.columns:\n                dummies[label] = 0\n            auc = roc_auc_score(dummies[label], probas[:, label])\n            \n            output_dict['precision_class_' + str(label)].append(precision[label])\n            output_dict['recall_class_' + str(label)].append(recall[label])\n            output_dict['f1_class_' + str(label)].append(f1[label])\n            output_dict['auc_class_' + str(label)].append(auc)\n       \n    # calculate the average results\n    avg = dict()\n    for label in labels:\n        for m in metrics:\n            avg['avg_' + m + '_class_' + str(label)] = np.mean(output_dict[m + '_class_' + str(label)])\n    \n    return avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function to calculate the permutated feature importance across different train-test splits\ndef permutated_feature_importance(features_dataset, target_dataset, input_columns, number_of_splits, \n                                  model, score, percentage_test=0.2):\n    # Define output dataframe\n    output_feature = pd.DataFrame()\n    std_feature = pd.DataFrame()\n    \n    # Create column with the name of the feature\n    output_feature['FEATURE'] = input_columns\n    std_feature['FEATURE'] = input_columns\n    \n    # initialize model\n    model = model.estimator.set_params(**model.best_params_)\n    \n    for n in range(1, number_of_splits + 1):\n        # Stratified train test split\n        X_train, X_test, y_train, y_test = train_test_split(features_dataset, target_dataset, test_size=percentage_test,\n                                                    stratify=target_dataset)\n        # Take the best model and fit it to the training set\n        model.fit(X_train, y_train)\n        \n        # Dictionary of permutation\n        permutated_dict = permutation_importance(model, features_dataset, target_dataset, scoring=score, random_state=rs)\n        output_feature[n] = permutated_dict['importances_mean']\n        std_feature[n] = permutated_dict['importances_std']\n    \n    # Calculate average importance and its std across all splits\n    output_feature['AVG_IMPORTANCE_' + str(number_of_splits)] = output_feature.mean(axis=1)\n    std_feature['AVG_STD_' + str(number_of_splits)] = std_feature.mean(axis=1)\n    \n    # Merge std avg with final output frame\n    output_feature = output_feature.merge(std_feature[['FEATURE', 'AVG_STD_' + str(number_of_splits)]], on=['FEATURE'])\n    \n    # Sort values by average importance\n    output_feature = output_feature.sort_values(by=['AVG_IMPORTANCE_' + str(number_of_splits)], ascending=False)\n        \n    return output_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_est_grid(grid, m):\n    \"\"\"\n    Get the equivalent grid dictionary for a One vs All estimator\n    :param grid: (dict) original grid dictioanry\n    :param m   : (string) model name\n    return: (dict) grid for One vs All\n    \"\"\"\n    grid2 = dict()\n    for k in grid:\n        if m in k:\n            grid2[k.replace(m + '__', m + '__estimator__')] = grid[k]\n        else:\n            grid2[k] = grid[k]\n    return grid2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to calculate correlation between categorical variables\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2 / n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n    rcorr = r - ((r - 1) ** 2)/(n - 1)\n    kcorr = k - ((k - 1) ** 2)/(n - 1)\n    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_covid = pd.read_excel('/kaggle/input/covid19/dataset.xlsx')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Dataset basic information"},{"metadata":{},"cell_type":"markdown","source":"We begin by checking basic data information, such as header and the shape of data, just to get an initial expectation of data challanges we may be facing"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_covid.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note that we have a couple of columns related to Coronavirus test (e.g. Coronavirus HKU1, Coronavirus 229E, ...). However, even if a patient test positive in one of these, it does not necessarily mean that they have SARS-Cov-2. The opposite also happens (patients can test negative in Coronavirus tests but test positive for SARS-Cov-2)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_covid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Nulls evaluation"},{"metadata":{},"cell_type":"markdown","source":"We check the top columns in terms of null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = (raw_covid.isnull().sum()/raw_covid.shape[0]*100).sort_values(ascending=False).reset_index()\nmissing.columns = ['VARIABLE', 'PERC_MISSING']\nmissing.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing['QUANTILE_MISSING'] = pd.cut(missing['PERC_MISSING'], 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y='QUANTILE_MISSING', data=missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(raw_covid, sparkline=False)\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\nThere are a lot of null entries in most of the columns. From the looks of it, it seems that most of the null entries are because the patient did not do that specific test (e.g. calculate platelets, red blood cells, etc.). \n\nTherefore, we will not use impute the nulls with mean, median or mode to not produce bias and false data. Instead we will:\n- Drop columns that have 100% null entries (Mycoplasma pneumonia, Urine - Sugar, Partial thromboplastin time (PTT), Prothrombin time (PT), Activitiy and D-Dimer columns. Since they have 100% null entries, they have no prediction power\n- For the remaining columns, we will impute values to indicate that they were not tested, guaranteeing that they will not mix with the ones that have real data. This does not necessarily mean that we will use these columns in our final model, which is a job that has to be done during model building / feature selection phase"},{"metadata":{},"cell_type":"markdown","source":"## Data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_covid.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\nChecking each column dtype with the snippet of the data, we found that a couple of columns have different dtype than they should have:\n- Urine - pH: due to some entries states as \"Não Realizado\", this column is considered as an object type when it should be float\n- Urine - Leukocytes: there are 9 entries that have a value of \"<1000\" in this column. Due to this fact, this column is considered as an object type when it should be float\n\n We will correct these columns dtypes and their entries later on this notebook"},{"metadata":{},"cell_type":"markdown","source":"##  Data Statistics"},{"metadata":{},"cell_type":"markdown","source":"### Numerical columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns = raw_covid.select_dtypes(exclude='object').columns\nprint('There are', len(numerical_columns), 'numerical columns in the dataset')\n\nraw_covid[numerical_columns].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n    \nSince the features are all normalized it's hard to tell whether it exists or not outliers in this table. In theory, we should flag an outlier data point's that have a test result outside a feasable range (for instance, have an hematocrit above 100%), but since we don't have the original values we will assume that this data treatment has been performed previously"},{"metadata":{},"cell_type":"markdown","source":"### Categorical Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = raw_covid.select_dtypes(include='object').columns\nprint('There are', len(categorical_columns), 'categorical columns in the dataset')\n\nraw_covid[categorical_columns].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 5))\nsns.countplot(x = 'SARS-Cov-2 exam result', data = raw_covid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_covid['SARS-Cov-2 exam result'].value_counts(normalize = True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n    \nWe have a really an imbalanced dataset: 90.1% of the cases have negative results while only 9.9% are positive.\nThis means that we will probably have to use balancing methods to obtain better model performances"},{"metadata":{},"cell_type":"markdown","source":"### Number of unique entries for each column "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [c for c in raw_covid.columns if 'Patient ID' not in c]:\n    print(col, ':', raw_covid[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = raw_covid.corr()\nplt.figure(figsize=(20, 8))\n\n# Heatmap of correlations\nsns.heatmap(corr, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=False, vmax=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# melt the matrix columns to get pair-wise correlation table\ncorr = pd.melt(corr.reset_index(), id_vars='index', value_name='corr')\n    \n# remove rows with the same variable\ncorr = corr[corr['index'] != corr['variable']]\n    \n# drop duplicates on pairwise columns\ncorr = corr.loc[\n    pd.DataFrame(\n        np.sort(corr[['index', 'variable']], 1), index=corr.index\n    ).drop_duplicates(keep='first').index\n]\n\n# sort values by absolute correlation values\ncorr['corr'] = corr['corr'].abs()\ncorr.sort_values(by='corr', ascending=False, inplace=True)\n\n# show top n rows\ncorr.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of columns to exclude from the correlation analysis\nexclude_columns = [\n    'Patient ID', \n    'SARS-Cov-2 exam result',\n    'Patient addmited to regular ward (1=yes, 0=no)',\n    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n    'Patient addmited to intensive care unit (1=yes, 0=no)',\n    \n    # raises an error because of the number of null values\n    'Urine - Esterase', 'Urine - Aspect', 'Urine - pH','Urine - Hemoglobin',\n    'Urine - Bile pigments', 'Urine - Ketone Bodies', 'Urine - Nitrite', \n    'Urine - Density', 'Urine - Urobilinogen', 'Urine - Protein', 'Urine - Sugar',\n    'Urine - Leukocytes', 'Urine - Crystals', 'Urine - Red blood cells', \n    'Urine - Hyaline cylinders', 'Urine - Granular cylinders', 'Urine - Yeasts',\n    'Urine - Color'\n]\n\ncols = [c for c in categorical_columns if c in raw_covid.columns and c not in exclude_columns]\n\n# calculate the correlation for each pair of variables\ncorr = list()\nfor c1, c2 in list(itertools.combinations(cols, 2)):\n    corr.append([c1, c2, cramers_v(raw_covid[c1].values, raw_covid[c2].values)])\n\n# show the table of correlations    \ncorr = pd.DataFrame(data=corr, columns=['col1', 'col2', 'corr'])\ncorr.sort_values(by='corr', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n    \nWe can see that there is of features that are highly correlated with each other (perfect positive or negative correlations), meaning that we will probably just keep one version of each variable. We will not apply the same proccess for the categorial features, given that they have a lower correlation score. Further in this notebook we will add criterias to do so"},{"metadata":{},"cell_type":"markdown","source":"## Outliers "},{"metadata":{},"cell_type":"markdown","source":"We initially analyzed outliers trough univariate analysis (boxplots and barplots), but we could not really tell if the values indicated as outliers could be considered so. And this is because of 2 facts:\n\n- A lot of features have null entries. Therefore, a small amount of people did the tests and we can not really tell what would happen if more people tested (i.e. would the outliers remain outliers?)\n- We thought about searching for standard values for each one of the tests in the dataset to see if the entries could be considered outliers. However, the data was standardized to have a mean of zero and a unit standard deviation, making it impossible to evaluate the original values\n\nDue to this fact, we will leave the dataset as is and not treat potential outliers"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns to exclude from the process\nexclude_columns = [\n    'Patient ID',\n    'SARS-Cov-2 exam result',\n    'Patient addmited to regular ward (1=yes, 0=no)',\n    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n    'Patient addmited to intensive care unit (1=yes, 0=no)'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create copy of the data"},{"metadata":{},"cell_type":"markdown","source":"This step is done to maintain the raw data intouchable. If we need to recurr to the original data, we do not need to import it again"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_covid = raw_covid.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treat dtypes"},{"metadata":{},"cell_type":"markdown","source":"In this section we will treat the inputted errors discovered during the EAD process as well as the dtypes of the 2 columns with errors:\n- Urine - pH\n- Urine - Leukocytes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Urine - pH\ncleaned_covid.loc[cleaned_covid['Urine - pH'] == 'Não Realizado', 'Urine - pH'] = '-9'\ncleaned_covid.loc[cleaned_covid['Urine - pH'] == 'not_done', 'Urine - pH'] = '-9'\ncleaned_covid['Urine - pH'] = cleaned_covid['Urine - pH'].astype(np.float)\n\n# Urine - Leukocytes\n# We will input the <1000 entries with the midpoint 500. Note that there is no entry with this specific value\ncleaned_covid.loc[cleaned_covid['Urine - Leukocytes'] == '<1000', 'Urine - Leukocytes'] = '500'\ncleaned_covid.loc[cleaned_covid['Urine - Leukocytes']=='not_done', 'Urine - Leukocytes'] = '-9'\ncleaned_covid['Urine - Leukocytes'] = cleaned_covid['Urine - Leukocytes'].astype(float)\n\n# Urine - Crystals\nrep = {'á': 'a', '-': 'Minus', '+': 'Plus'}\nfor key, value in rep.items():\n    cleaned_covid['Urine - Crystals'] = cleaned_covid['Urine - Crystals'].str.replace(key, value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selection of Patients with/without tests"},{"metadata":{},"cell_type":"markdown","source":"In this section we will break the dataframe into 2 groups:\n\n- Dataframe containing patients who did not do all the tests\n- Dataframe containing patients who had to go trough all the tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns related to tests\ntests_columns = [c for c in cleaned_covid.columns if c not in exclude_columns and c not in ['Patient age quantile']]\n\n# Dataframe of patients without any test result\ncleaned_covid_no_test = cleaned_covid[cleaned_covid[tests_columns].isnull().all(axis=1)].copy()\ncleaned_covid_no_test.dropna(how='all', axis='columns', inplace=True)\n\n# Dataframe of patients with test results\ncleaned_covid_test = cleaned_covid[~(cleaned_covid['Patient ID'].isin(cleaned_covid_no_test['Patient ID']))].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_covid_test['SARS-Cov-2 exam result'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop all null columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_covid.dropna(how='all', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop columns with lot's of nulls "},{"metadata":{},"cell_type":"markdown","source":"Selecting what is the \"allowed percentage of nulls\" is not an easy task, and there is no magic number to select it. Normally we would drop columns with 25%-30% of missing values, but that depends on how much predictive power the variable has. Depending on that, even columns with 50%-70% of nulls could be kept. Above those values the standard practice would be to request that the business area would collect the information. In our case it's not possible, and it's hard to tell whether each variable has or not great predictive power because that could be dependent on a combination of factors and tests. Because of that we can either procceed to perform a rigorous series of analysis to understand how a set of test combinations could influence a test result and verify data-driven insights with a specilist. For simplicity, in our case, we will drop variables that could result in overfit."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the amount of columns dropped by the fill percentage required\ntask1_ratio = cleaned_covid_test.count() / cleaned_covid_test.shape[0]\ntask2_ratio = cleaned_covid_test[cleaned_covid_test['SARS-Cov-2 exam result'] == 'positive']\ntask2_ratio = task2_ratio.count() / task2_ratio.shape[0]\nratio = pd.concat([task1_ratio, task2_ratio], axis='columns')\nres = list()\nfor thrs in np.linspace(0.01, 0.3, 50):\n    res.append([thrs, ratio[(ratio[0] < thrs) & (ratio[1] < thrs)].shape[0]])\nfig = plt.figure(figsize=(16,4))\nax = fig.gca()\nax.set_xticks(np.linspace(0.01, 0.3, 20))\nplt.scatter(np.array(res)[:, 0], np.array(res)[:, 1])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply column removal\nthrs = 0.05\ntask1_ratio = cleaned_covid_test.count() / cleaned_covid_test.shape[0]\ntask2_ratio = cleaned_covid_test[cleaned_covid_test['SARS-Cov-2 exam result'] == 'positive']\ntask2_ratio = task2_ratio.count() / task2_ratio.shape[0]\nratio = pd.concat([task1_ratio, task2_ratio], axis='columns')\ncleaned_covid_test.drop(columns=ratio[(ratio[0] < thrs) & (ratio[1] < thrs)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = cleaned_covid.drop(columns=exclude_columns).corr()\ncorr = pd.melt(corr.reset_index(), id_vars='index', value_name='corr')\ncorr = corr[corr['index'] != corr['variable']]\ncorr = corr.loc[pd.DataFrame(np.sort(corr[['index', 'variable']], 1), index=corr.index).drop_duplicates(keep='first').index]\ncorr['corr'] = corr['corr'].abs()\nres = list()\nfor thrs in np.linspace(0.7, 1, 32):\n    x = corr[corr['corr'] > thrs]\n    drop_corr = list()\n    while x.shape[0] > 0:\n        for c in x['index'].append(x['variable']).unique():\n            v1 = x[(x['variable'] == c) | (x['index'] == c)]\n            \n            if v1.shape[0] > 0:\n                c2 = v1['index'].values[0] if v1['index'].values[0] != c else v1['variable'].values[0]\n                v2 = x[(x['variable'] == c2) | (x['index'] == c2)]\n                \n                drop = c if (v1.shape[0] > v2.shape[0]) and (cleaned_covid[c].count() < cleaned_covid[c2].count()) else c2\n                drop_corr.append(drop)\n                x = x[(x['index'] != drop)& (x['variable'] != drop)]\n    res.append((thrs, cleaned_covid.shape[1] - len(drop_corr)))\nplt.plot(np.array(res)[:, 0], np.array(res)[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stablish correlation threshold to drop variables\nthrs = 0.9\n\n# calculate correlation matrix\ncorr = cleaned_covid_test.drop(columns=exclude_columns).corr()\n\n# transform columns into rows\ncorr = pd.melt(corr.reset_index(), id_vars='index', value_name='corr')\n\n# remove correlation of same variable\ncorr = corr[corr['index'] != corr['variable']]\ncorr['corr'] = corr['corr'].abs()\n\n# drop pair-wise duplicates\ncorr = corr.loc[\n    pd.DataFrame(\n        np.sort(corr[['index', 'variable']], 1), index=corr.index\n    ).drop_duplicates(keep='first').index\n]\n\n# select pairs above threshold\nx = corr[corr['corr'] > thrs].sort_values('corr', ascending=False)\ndrop_corr = list()\n\n# for each pair\nwhile x.shape[0] > 0:\n    for c in x['index'].append(x['variable']).unique():\n        # verify number of ocurrencces of a feature in the table\n        v1 = x[(x['variable'] == c) | (x['index'] == c)]\n        \n        if v1.shape[0] > 0:\n            c2 = v1['index'].values[0] if v1['index'].values[0] != c else v1['variable'].values[0]\n            v2 = x[(x['variable'] == c2) | (x['index'] == c2)]\n\n            # if the first features have more occurences than the second, that means that the first variable\n            # can be explained by multiple variables, therefore we will prefer the second which adds more\n            # variance to the dataset\n            drop = c if (v1.shape[0] > v2.shape[0]) and (cleaned_covid[c].count() < cleaned_covid[c2].count()) else c2\n            drop_corr.append(drop)\n            x = x[(x['index'] != drop)& (x['variable'] != drop)]\n\n# drop the selected columns\ncleaned_covid_test.drop(drop_corr, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Impute missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns with null entries\nnull_columns = [c for c in cleaned_covid_test.columns if cleaned_covid_test[c].isnull().sum() != 0]\n\n# List of columns to be filled with -9\ncolumns_num_fill = [c for c in null_columns if c in cleaned_covid_test.select_dtypes(exclude='object').columns]\n\n# List of columns to be filled with \"not_done\"\ncolumns_cat_fill = [c for c in null_columns if c in cleaned_covid_test.select_dtypes(include='object').columns]\n\n# Impute missing values\nfor col in columns_num_fill:\n    cleaned_covid_test[col].fillna(-9, inplace=True)\n    \nfor col in columns_cat_fill:\n    cleaned_covid_test[col].fillna('not_done', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treat categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SARS-Cov-2 exam result\ncleaned_covid_no_test.loc[cleaned_covid_no_test['SARS-Cov-2 exam result']=='negative', 'SARS-Cov-2 exam result'] = 0\ncleaned_covid_no_test.loc[cleaned_covid_no_test['SARS-Cov-2 exam result']=='positive', 'SARS-Cov-2 exam result'] = 1\ncleaned_covid_no_test['SARS-Cov-2 exam result'] = cleaned_covid_no_test['SARS-Cov-2 exam result'].astype(np.int)\n\ncleaned_covid_test.loc[cleaned_covid_test['SARS-Cov-2 exam result']=='negative', 'SARS-Cov-2 exam result'] = 0\ncleaned_covid_test.loc[cleaned_covid_test['SARS-Cov-2 exam result']=='positive', 'SARS-Cov-2 exam result'] = 1\ncleaned_covid_test['SARS-Cov-2 exam result'] = cleaned_covid_test['SARS-Cov-2 exam result'].astype(np.int)\n\n# One hot encode categorical columns\ncategorical = [c for c in cleaned_covid_test.select_dtypes(include='object').columns if 'Patient ID' not in c]\nencoded_covid_test = pd.get_dummies(cleaned_covid_test[categorical], drop_first=True)\ncleaned_covid_test.drop(categorical, axis=1, inplace=True)\ncleaned_covid_test = pd.concat([cleaned_covid_test, encoded_covid_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature and target split"},{"metadata":{},"cell_type":"markdown","source":"### Task 1: Predict confirmed COVID-19 cases among suspected cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create features and target for dataframe with no tests\nfeatures_no_test_task1 = cleaned_covid_no_test.drop(exclude_columns, axis = 1)\ntarget_no_test_task1 = cleaned_covid_no_test['SARS-Cov-2 exam result']\n\n# Create features and target for dataframe with tests\nfeatures_covid_task1 = cleaned_covid_test.drop(exclude_columns, axis = 1)\ntarget_covid_task1 = cleaned_covid_test['SARS-Cov-2 exam result']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 2: Predict admission to general ward, semi-intensive unit or intensive care unit"},{"metadata":{"trusted":true},"cell_type":"code","source":"units = cleaned_covid_test[cleaned_covid_test['SARS-Cov-2 exam result'] == 1]\nfeatures_covid_task2 = units.drop(exclude_columns, axis = 1)\n\ntarget_cols = [\n    'Patient addmited to regular ward (1=yes, 0=no)', 'Patient addmited to semi-intensive unit (1=yes, 0=no)', \n    'Patient addmited to intensive care unit (1=yes, 0=no)'\n]\ntarget_covid_task2 = units[target_cols[0]] + units[target_cols[1]] * 2 + units[target_cols[2]] * 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\nfrom IPython.display import SVG\ndt = Pipeline([('smt', SMOTE(0.5, random_state=rs)), ('dt', DecisionTreeClassifier(random_state=rs, max_depth=3))])\ndt.fit(features_covid_task1, target_covid_task1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_feat = pd.DataFrame(dt.named_steps['dt'].feature_importances_, index=features_covid_task1.columns, columns=['feat_importance'])\ndt_feat.sort_values('feat_importance').tail(8).plot.barh()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = Source(export_graphviz(dt.named_steps['dt'], out_file=None, feature_names=features_covid_task1.columns, filled=True))\nSVG(graph.pipe(format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- A lot of features do not add any predictive power, indicating that tree algorithms will be highly dependent on very few features\n- We did not apply any feature selection methodology, as we are assuming that all medical information could be relevant, but looking forward we should use the permutation importance's results to decide which columns to keep\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Task 1: Predict confirmed COVID-19 cases among suspected cases"},{"metadata":{},"cell_type":"markdown","source":"## Model Building "},{"metadata":{},"cell_type":"markdown","source":"When we were dealing with this problem, one of the questions that we end up having was how we should perform our train-test split in a proper manner (i.e. have a test set that reflects people who will end up at the hospital in the future). \n\nSince we have no way to know the answer for this question (i.e. to know if people who will go to the hospital reflects the overall population or if people who goes are biased ones - e.g. more worried about their health than usual people), we end up thinking about several approaches to really determine the performance of our final model:\n\n1) Run randomized train-test split and create a model and validate its performance on the single test set: as mentioned earlier, this specific approach could result in a test set that does not reflect the future patients of the hospital and our performance would be biased\n\n2) Run several train-test splits and create different models for each split: this approach would end up being too much time consuming since the number of models would be equal to the number of splits we end up choosing (e.g. 50 train-test splits means 50 model building pipeline)\n\n3) Run a stratified k-fold cross validation with the entire dataset and choose the model (i.e. algorithm and its parameters) with the best average performance. Create several train-test splits and assess the average performance of the model across the different splits\n\nWe decided to go forward we our approach number 3 since it is fast and realiable at the same time"},{"metadata":{},"cell_type":"markdown","source":"### Model instantiation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define param grid\nsmt_grid = {\n    'smt__sampling_strategy': [0.2, 0.3, 0.4, 0.5],\n    'smt__k_neighbors' : [2, 3, 5],\n    'smt__random_state': [rs]\n}\n\nrf_grid = {\n    'rf__n_estimators' : [int(x) for x in np.linspace(100, 2000, 20)],\n    'rf__max_features' : ['auto', 'sqrt', 'log2'],\n    'rf__min_samples_leaf': [2, 5, 10], # we set the minimum of samples leaf to minimize overfit \n    'rf__min_samples_split': [5, 10, 15], # we set the minimum of samples split to minimize overfit\n    'rf__max_depth': [5, 8, 15], # we limited the max depth to 15 given the risk of overfit\n    'rf__random_state' : [rs]\n}\nrf_grid.update(smt_grid)\n\nxgb_grid = {\n    'xgb__loss' : ['ls', 'lad', 'huber', 'quantile'],\n    'xgb__n_estimators': [int(x) for x in np.linspace(100, 2000, 20)],\n    'xgb__learning_rate': np.linspace(0.01, 0.1, 10),\n    'xgb__max_depth': range(3, 10),\n    'xgb__subsample': [0.8, 0.85, 0.9, 0.95, 1],\n    'xgb__max_features': ['auto', 'sqrt'],\n    'xgb__min_samples_leaf': [2, 5, 10], # we set the minimum of samples split to minimize overfit\n    'xgb__min_samples_split': [5, 10, 15, 30], # we limited the max depth to 15 given the risk of overfit\n    'xgb__random_state': [rs]\n}\nxgb_grid.update(smt_grid)\n\nlgbm_grid ={\n    'lgbm__num_leaves': stats.randint(6, 50), \n    'lgbm__min_child_samples': stats.randint(100, 500), \n    'lgbm__min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n    'lgbm__subsample': stats.uniform(loc=0.2, scale=0.8), \n    'lgbm__colsample_bytree': stats.uniform(loc=0.4, scale=0.6),\n    'lgbm__reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n    'lgbm__reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n    'lgbm__random_state': [rs]\n}\nlgbm_grid.update(smt_grid)\n\n# list models to run\n# NOTE: Since the lgbm model is much faster to run, we are going to use it for now, but those lines should be uncommented\n# when thinking of a model deployment\nmodels_task1 = [\n    ('RF', 'rf', RandomForestClassifier, rf_grid),\n    ('XGB', 'xgb', XGBClassifier, xgb_grid),\n    ('LGBM', 'lgbm', LGBMClassifier, lgbm_grid),\n]\n\n# dictionary of results\nresults_task1 = dict()\n\n# set the metric to select the best model\nMODEL_SELECTION_METRIC_TASK1 = 'recall'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Tunning"},{"metadata":{},"cell_type":"markdown","source":"Note: when the estimator inside RandomizedSearchCV is classifier, the default cross validation method is a stratified k-fold cross validation. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.fit_transform) for more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"# go through the models\nfor m, n, c, g in models_task1:\n    print(m)\n\n    # instantiate pipeline\n    p = Pipeline([('smt', SMOTE(random_state=rs)), (n, c())])\n\n    # run a grid search\n    results_task1[m] = RandomizedSearchCV(\n        p, g, cv=10, scoring=MODEL_SELECTION_METRIC_TASK1, verbose=1, n_jobs=-1, n_iter=300, random_state=rs, refit=False\n    )\n    results_task1[m].fit(features_covid_task1.values, target_covid_task1.values)\n\n    # print out the model performance\n    print('Best %s %s score:' % (m, MODEL_SELECTION_METRIC_TASK1), results_task1[m].best_score_)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model real performance valuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 100)\n\n# select the model with the highest score\nmodels = [results_task1[m] for m in results_task1]\nscores = [results_task1[m].best_score_ for m in results_task1]\ni = np.argmax(scores)\nbest_model_task1 = models[i]\n\navg = mean_performance(\n    features_covid_task1.values, target_covid_task1.values, 500, \n    model=best_model_task1, percentage_test=0.1,\n)\nfor k in avg:\n    print(k + ':', '%.2f' % avg[k])\nprint('-' * 100)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model interpretability"},{"metadata":{},"cell_type":"markdown","source":"To interpret the model, we will recurr only to global interpretability (for local interpretability, one can recurr to several libraries such as LIME, more specifically, we will recurr to permutated feature importance. The basic idea of this technique can be found here"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp_task1 = permutated_feature_importance(\n    features_covid_task1.values, target_covid_task1, features_covid_task1.columns, 50, best_model_task1, score = MODEL_SELECTION_METRIC_TASK1\n) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='AVG_IMPORTANCE_50', y='FEATURE', data=interp_task1.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling conclusions:**\n\n- We initially created one single model for the entire dataset (for both patients with and without test results information). However, that specific model gave poor performance across different algorithms, feature selection and class balancing techniques. We did not showed the results for the global model in this final version of the notebook\n- When breaking the dataset between patients with vs without test information, our model for the dataframe with test information performed better (confirming our initial hypothesis). However, the patients without information have only one single column (Patients age quantile) and, therefore, also resulted in poor performance models (not showed in this notebook)\n\n**Feature importance conclusions:**\n\n- Looking at the feature importance table generated above, we can conclude that, in this dataset, the most important features for our global model were: (1) Patient age quantile, (2) Leukocytes and (3) Platelets\n- We are not familiar with the health industry and its terms / meanings so we cannot conclude if the features that give tests information make sense or not in helping to predict SARS-Covid-2. However, having the patient age quantile as the most important feature makes sense **when thinking only in modeling terms**, since it is the column that has no null entries and had no imputed values.\n- It is also possible to encounter features with negative feature importance. In these cases, the predictions with the shuffled data happened to be more accurate than the real data. This happens when the feature did not matter or due to random chance that caused the predictions on shuffled data to be more accurate. Again, **we cannot be sure if this makes sense due to our lack of knowledge in the health field**\n\n**General conclusions:**\n\n- Dealing with 2 models (1 for a dataframe with test information and 1 for no test information) leads to better results, but it also means that the hospital needs to test patients before having any prediction output which may not be ideal since it means more costs, more demand for overall tests\n- Applying one single model for the entire dataset means that we do not necessarily need to run any priori tests in patients. However, the model performance is worse: we have lower recall (i.e. higher false negatives - telling people they do not have covid when in fact they have) and lower f1 score (harmonic mean of the precision and recall)"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Task 2: Predict admission to general ward, semi-intensive unit or intensive care unit"},{"metadata":{},"cell_type":"markdown","source":"## Model Building "},{"metadata":{},"cell_type":"markdown","source":"For the task 2 we will follow a similar methodology to task 1, but we are going change the problem to a multiclass classification and also use both an OneVsRest and the model itself"},{"metadata":{},"cell_type":"markdown","source":"### Model instantiation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# list models to run\n# NOTE: Since the lgbm model is much faster to run, we are going to use it for now, but those lines should be uncommented\n# when thinking of a model deployment\nmodels_task2 = [\n    ('OVA_RF', 'rf', lambda: OneVsRestClassifier(RandomForestClassifier()), get_est_grid(rf_grid, 'rf')),\n    ('RF', 'rf', RandomForestClassifier, rf_grid),\n    ('OVA_XGB', 'xgb', lambda: OneVsRestClassifier(XGBClassifier()), get_est_grid(xgb_grid, 'xgb')),\n    ('XGB', 'xgb', XGBClassifier, xgb_grid),\n    ('OVA_LGBM', 'lgbm', lambda: OneVsRestClassifier(LGBMClassifier()), get_est_grid(lgbm_grid, 'lgbm')),\n    ('LGBM', 'lgbm', LGBMClassifier, lgbm_grid)\n]\n\n# dictionary of results\nresults_task2 = dict()\n\n# set the metric to select the best model\nMODEL_SELECTION_METRIC_TASK2 = 'recall_weighted'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Tunning"},{"metadata":{},"cell_type":"markdown","source":"Note: when the estimator inside RandomizedSearchCV is classifier, the default cross validation method is a stratified k-fold cross validation. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.fit_transform) for more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"# go through the models\nfor m, n, c, g in models_task2:\n    print(m)\n\n    # instantiate pipeline\n    p = Pipeline([('smt', SMOTE(random_state=rs)), (n, c())])\n\n    # we change the sampling strategy based on the fact that we have a multi-class classification problem\n    g['smt__sampling_strategy'] = ['not majority']\n    \n    # we also limit the number of knn given that we may not have enough sample depending on the random split\n    g['smt__k_neighbors'] = [2]\n    \n    # run a grid search\n    # NOTE: we reduced the number of iterations just to deploy the notebook faster. But looking forward this number should be 200~300\n    results_task2[m] = RandomizedSearchCV(\n        p, g, cv=10, scoring=MODEL_SELECTION_METRIC_TASK2, verbose=1, n_jobs=-1, n_iter=300, random_state=rs, refit=False\n    )\n    results_task2[m].fit(features_covid_task2.values, target_covid_task2.values)\n\n    # print out the model performance\n    print('Best %s %s score:' % (m, MODEL_SELECTION_METRIC_TASK2), results_task2[m].best_score_)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model real performance valuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 100)\n\n# select the model with the highest score\nmodels = [results_task2[m] for m in results_task2]\nscores = [results_task2[m].best_score_ for m in results_task2]\ni = np.argmax(scores)\nbest_model_task2 = models[i]\n\navg = mean_performance(\n    features_covid_task2.values, target_covid_task2.values, 300, \n    model=best_model_task2, percentage_test=0.1, labels=list(target_covid_task2.unique())\n)\nfor k in avg:\n    print(k + ':', '%.2f' % avg[k])\nprint('-' * 100)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model interpretabiliy "},{"metadata":{"trusted":true},"cell_type":"code","source":"interp_task2 = permutated_feature_importance(\n    features_covid_task2.values, target_covid_task2, features_covid_task2.columns, 50, best_model_task2, score = MODEL_SELECTION_METRIC_TASK2\n) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='AVG_IMPORTANCE_50', y='FEATURE', data=interp_task2.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling conclusions:**\n\n- We decided to approach this problem using 2 methods: one-vs-all and multiclass modeling.\n- The one-vs-all model seems to have a better performance than the standard multiclass\n\n**Feature importance conclusions:**\n\n- Looking at the permutated feature importance table generated above, we can notice that the top 3 features were: (1) Proteina C reativa mg/dL, (2) Neutrophils and (3) Mean corpuscular hemoglobin concentration (MCHC). However, **we are not familiar with the health industry and its terms / meanings so we cannot conclude if the features that give tests information make sense or not in helping to predict admissions to general ward, semi intensive and intensive care unit**\n- We can also noticed that the columns do not have a high permutated feature importance on average, meaning that there is no column that outlies as the \"most important\" one. Basically, due to the small gap in average feature importance across several features, each feature have a little contribution to the final prediction\n- It is also possible to encounter features with negative feat\n"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Model Deployment "},{"metadata":{},"cell_type":"markdown","source":"For the model deployment we will select the best model for each task and run their training with the whole dataset. To avoid overfit, we will put the best within a bagging regressor selecting a given % of samples from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = 400\nmax_samples = 0.9\n\n# instantiate the bagging classifier\nmodel1 = best_model_task1.estimator.set_params(**best_model_task1.best_params_)\nbag1 = BaggingClassifier(model1, n_estimators=n_estimators, max_samples=max_samples)\n\nmodel2 = best_model_task2.estimator.set_params(**best_model_task2.best_params_)\nbag2 = BaggingClassifier(model2, n_estimators=n_estimators, max_samples=max_samples)\n\n# train and export model\nbag1.fit(features_covid_task1.values, target_covid_task1.values)\njoblib.dump(bag1, 'model_task1.pkl', compress=9)\n\n# train and export model\n# NOTE: We had to comment the second model because there are very few positive examples depending on the class\n# because of that we cannot run the bag with SMOTE. We either have to change our sampler or increase the number of data points\n# bag2.fit(features_covid_task2.values, target_covid_task2.values)\n# joblib.dump(bag2, 'model_task2.pkl', compress=9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}