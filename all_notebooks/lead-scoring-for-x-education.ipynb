{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n\n\n# Data display coustomization\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_data_dict = pd.read_excel('/kaggle/input/leads-dataset/Leads Data Dictionary.xlsx', skiprows=2)\nleads_data_dict.drop(leads_data_dict.columns[0], axis=1,inplace=True)\nleads_data_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore = pd.read_csv('/kaggle/input/leads-dataset/Leads.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### There are lot of rows in various categorical columns which have value 'Select' values present in the dataset. `Select` corresponds to the user having not made any selection. So we can replace it with NaN.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore = leadscore.replace('Select', np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Before moving ahead  with EDA, we will define some helper functions which will be used frequently in the rest of the analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### This function will generate a table of features, total NULL values, and %age of NULL values in it.\ndef findNullValuesPercentage(dataframe):\n    totalNullValues = dataframe.isnull().sum().sort_values(ascending=False)\n    percentageOfNullValues = round((dataframe.isnull().mean()).sort_values(ascending=False),2)\n    featuresWithPrcntgOfNullValues = pd.concat([totalNullValues, percentageOfNullValues], axis=1, keys=['Total Null Values', 'Percentage of Null Values'])\n    return featuresWithPrcntgOfNullValues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### this function will create BarPlot for our visualization.\n\ndef createCountPlot(keyVariable, plotSize):\n    fig, axs = plt.subplots(figsize = plotSize)\n    plt.xticks(rotation = 90)\n    dataframe = leadscore.copy()\n    dataframe[keyVariable] = dataframe[keyVariable].fillna('Missing Values')\n    ax = sns.countplot(x=keyVariable, data=dataframe)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}'.format(height/len(dataframe) * 100),\n                ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#This function will just drop the list of features(inplace) provided to it and print them.\ndef dropTheseFeatures(features):\n    print('Dataset shape before dropping the features {}'.format(leadscore.shape))\n    print('*****------------------------------------------*****')\n    for col in features:\n        print('Removing the column {}'.format(col))\n        leadscore.drop(col, axis=1, inplace=True)\n    print('*****------------------------------------------*****')\n    print('Dataset shape after dropping the features {}'.format(leadscore.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function will genrate a table which is populated with feature name and the %age of count of unique values in it.\ndef genarateUniqueValuePercentagePlot(features):\n    cols=4\n    rows = len(features)//cols +1\n    fig = plt.figure(figsize=(16, rows*5))\n    for plot, feature in enumerate(features):\n        fig.add_subplot(rows,cols,plot+1)\n        ax = sns.countplot(x=leadscore[feature], data=leadscore) \n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(leadscore) * 100),\n                ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Function to generate heatmaps\ndef generateHeatmaps(df, figsize):\n    plt.figure(figsize = figsize)        # Size of the figure\n    sns.heatmap(df.corr(),annot = True, annot_kws={\"fontsize\":7})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the name suggests this function will plot AUC-ROC curve.\ndef plot_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs)\n                                            #, drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getRegressionMetrics(actual,predicted):\n    from sklearn.metrics import precision_score, recall_score\n    m={}\n    confusion = metrics.confusion_matrix(actual, predicted )\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    m['sensitivity']=TP / float(TP+FN)\n    m['specificity']=TN / float(TN+FP)\n    m['recall']=recall_score(actual, predicted)\n    m['precision']=precision_score(actual, predicted)\n    m['accuracy']=metrics.accuracy_score(actual, predicted)\n    m['F1-score']=metrics.f1_score(actual, predicted, average='weighted')\n    \n    print(confusion)\n    for metric in m:\n        print(metric + ': ' + str(round(m[metric],2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.  Performing EDA on the Lead Score Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### We will perform some exploratory data analysis and understand the data better by following the below steps:\n\n- Checking the shape, columns, datatypes etc. of the dataset\n- Assessing out of place values\n- Checking for duplicate values\n- Checking for null values\n- Dropping unnecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We will check the 'Prospect ID' & for 'Lead Number' columns for each datapoint to check for any duplicates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if any duplicated value is present in the ID and Lead Number columns\nprint(sum(leadscore.duplicated('Prospect ID')) == 0)\nprint(sum(leadscore.duplicated('Lead Number')) == 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The Prospect ID and Lead Number are all unique values and not required for the model building.\n> We will drop one of them, since Lead Number seems be simply an index number so we will keep it and remove\nthe prospectID","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTheseFeatures(['Prospect ID'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### A feature with only one unique value is not useful for model building because this feature has zero variance. So we will drop all such clumns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numUniquesInFeatures = leadscore.nunique().sort_values()\nnumUniquesInFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropFeaturesWithSingleVal=[]\nfor feature in numUniquesInFeatures.index:\n#     print(feature, numUniquesInFeatures[feature])\n    if numUniquesInFeatures[feature] == 1:\n        dropFeaturesWithSingleVal.append(feature)\ndropFeaturesWithSingleVal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTheseFeatures(dropFeaturesWithSingleVal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Now we check the features with 2 unique values\n- A free copy of Mastering The Interview              \n- Newspaper Article                                   \n- Search                                              \n- Through Recommendations                             \n- X Education Forums                                  \n- Converted (Target Variable)                                     \n- Do Not Call                                         \n- Do Not Email                                        \n- Newspaper                                           \n- Digital Advertisement                               ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"genarateUniqueValuePercentagePlot(['A free copy of Mastering The Interview', 'Newspaper Article', 'Search','Through Recommendations',\n             'X Education Forums', 'Converted', 'Do Not Call', 'Do Not Email', 'Newspaper', 'Digital Advertisement'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From the abve plots, we can see that  `A free copy of Mastering The Interview` and `Converted (Target variable)`, all other features are higly skewed. These variables will not help in the model buidling so we will drop these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropHighySkewedFeatures = ['Newspaper Article', 'Search','Through Recommendations',\n             'X Education Forums', 'Do Not Call', 'Do Not Email', 'Newspaper', 'Digital Advertisement']\ndropTheseFeatures(dropHighySkewedFeatures)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######  Now check the NULL values in remaining features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We can see from the above table that `Lead Profile` and `How did you hear about X Education` columns have 74% and above missing values. Also assuming a cut-off of 45% the `Asymmetrique` score and Index features are also on higher side. Imputing these features will not be a good idea, because it can make the model biased.   So we decide to drop these columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropHighMissingValuesFeatues = ['How did you hear about X Education', 'Lead Profile', 'Lead Quality', \n                                'Asymmetrique Profile Score','Asymmetrique Activity Score','Asymmetrique Profile Index',\n                                'Asymmetrique Activity Index']\ndropTheseFeatures(dropHighMissingValuesFeatues)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Handling the `Score Variables`. These variables are the ones which were not present with the sales team before making the call. After making the calls the sales team assigned their values to each lead after their discussion. So, in the real scenario these will not be availabe at the time of model building. So we will drop these columns. The score variables in this datasets are:\n- Tags\n- Lead Quality\n- Lead Profile\n- Asymmetrique Activity Index\n- Asymmetrique Profile Index\n- Asymmetrique Activity Score\n- Asymmetrique Profile Score ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### From the above list, we have already dropped all the features in ealier steps except `Tag` variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTheseFeatures(['Tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Next we will deal with the reamianing columns with high NULL values. These are:\n\n###### `City`\n###### `Specialization`\n###### `What matters most to you in choosing a course` \n###### `What is your current occupation`\n###### `Country`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('City', (10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Here we will impute the Missing Values with mode , ie, `Mumbai` in this case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['City'].fillna('Mumbai', inplace=True)\n\ncreateCountPlot('City', (10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### So we can see from the above plot that most of the leads are coming from Mumbai city, how ever the conversion rate is considerably low.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### Treating Specialization ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('Specialization', (15,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###### Here we observe that, the specializations realted to `Finance, HR and  Marketing Management` have most  of the leads and reamaining  catergories share a comparably less percentage. \n###### So we will take the following steps for the imputations.\n\n- impute the `Missing Values` with `Finance, HR and  Marketing Management`, each of them equally.\n- `Business Administration (4.36%)` and `Operations Management (5.44%)` share a considerable share. So we will keep them as it is.\n- Combine remaining Management specialization in to a sigle category,`Other Managements`\n- The specializations with less than 4% share, into a single category as `Other Specializations`\n\n###### This will  help us in reducing the complexity of the model later, by keeping the count of dummy variables low.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Specialization'].value_counts(normalize=True, dropna=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# impute the Missing Values with Finance, HR and  Marketing Management, each of them equally.\nleadscore['Specialization'].iloc[:1000].fillna('Human Resource Management', inplace=True)\nleadscore['Specialization'].iloc[1001:2000].fillna('Marketing Management', inplace=True)\nleadscore['Specialization'].iloc[2000:].fillna('Finance Management', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Specialization'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.Specialization.replace(to_replace=['Supply Chain Management',\n       'IT Projects Management', \n       'Marketing Management',\n       'Retail Management',\n       'Hospitality Management',\n       'Healthcare Management'], value='Other Management', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.Specialization.replace(to_replace=[\n       'Media and Advertising',\n       'Travel and Tourism', \n       'Banking, Investment And Insurance', 'International Business',\n       'E-COMMERCE',\n       'Services Excellence',\n       'Rural and Agribusiness',\n       'E-Business'], value='Others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Specialization'].value_counts(normalize=True, dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('Specialization', (10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('What matters most to you in choosing a course', (15,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### For the above feature also the data seems to be skewed, so we assume that almost all the leads wan to join the academy for Better career prospects. So we wil remove this feature as well, later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('What is your current occupation', (10,7.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### For the current occupation column we will impute the missing values with mode, ie, `Unemployed`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['What is your current occupation'].fillna('Not Specified', inplace=True)\n\nleadscore['What is your current occupation'] = leadscore['What is your current occupation'].replace(['Student', 'Housewife','Businessman'], 'Other')\n\nleadscore['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('What is your current occupation', (10,7.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We see that the lead conversion rate is high in case of Working Profesionals. Also most of the leads are `Unemployed`. Other categories in occupation are negligible.\n\n###### Now we take a look a Country column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (20,4))\nplt.xticks(rotation = 90)\nsns.countplot('Country', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The country plot also seems to be skewed. After imputing the Missing Values with mode, `India`, it will become 97%. So we will drop this as well. \n\n##### Dropping `What matters most to you in choosing a course` and `Country` in this step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTheseFeatures(['What matters most to you in choosing a course','Country'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###### Now we have only few NULL values left. So we will drop those rows with NULLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Finally we get the all cleaned dataset with 9074 rows and 12 features.\n\n### We will perform some univariate and bivariate analysis ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### Starting with `Last Notable Activity` ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_data_dict[(leads_data_dict['Variables']=='Last Activity') | (leads_data_dict['Variables']=='Last Notable Activity')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From the data dictionary  we see that `Last Notable Activity` is defined very similar to `Last Activity`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (12,4))\nplt.xticks(rotation = 90)\nsns.countplot('Last Notable Activity', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  The `Last Notable Activity` is  last  activity performed by the student. It is not relevant for our modelling purpose. So we will drop it here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTheseFeatures(['Last Notable Activity'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We will perform the some analysis and cleaning operation for `Last Activity`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(leadscore['Last Activity'].value_counts(normalize=True, ascending=False), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Last Activity'] = leadscore['Last Activity'].replace([           \n                                                                'Form Submitted on Website',       \n                                                                'Unreachable',                     \n                                                                'Unsubscribed',                    \n                                                                'Had a Phone Conversation',        \n                                                                'View in browser link Clicked',    \n                                                                'Approached upfront',              \n                                                                'Email Received',                  \n                                                                'Email Marked Spam',               \n                                                                'Resubscribed to emails',          \n                                                                'Visited Booth in Tradeshow'], 'Miscellaneous')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncreateCountPlot('Last Activity', (7, 5))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### A free copy of Mastering The Interview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['A free copy of Mastering The Interview'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### For this column we will just replace Yes:1, No:0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['A free copy of Mastering The Interview'].replace({'Yes':1, 'No':0}, inplace=True)\nleadscore['A free copy of Mastering The Interview'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Analyzing City columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('City', (7, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['City'] = leadscore['City'].replace(['Thane & Outskirts', 'Other Metro Cities', 'Other Cities',\n       'Other Cities of Maharashtra', 'Tier II Cities'], 'Not Mumbai Cities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('City', (7, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('What is your current occupation', (10, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(leadscore['What is your current occupation'], hue=leadscore.Converted)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###### `Working Professional` are can be a important feature to since their conversion rate is very high.\n###### However the datasets contains mostly Unemployed leads","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (15, 7.5))\nsns.countplot(leadscore['Specialization'], hue=leadscore.Converted)\naxs.set_xticklabels(axs.get_xticklabels(),rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###### We see that  leads who have done specialization in  `Management` specially in `FInance Management` have higher number of leads as well as leads converted. This variable is highly significant and will help in model building.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"findNullValuesPercentage(leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### analyzing the trend of `Page Views Per Visit' using boxplot ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('Page Views Per Visit', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We see that there are many outliers in the higher side of the data. \n###### We will remove theese the outliers by capping using soft range capping.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = leadscore['Page Views Per Visit'].quantile(0.05) #---- lower range taken\nq4 = leadscore['Page Views Per Visit'].quantile(0.95) #----- higher range taken\n\nleadscore['Page Views Per Visit'][leadscore['Page Views Per Visit']<=q1] = q1 #----- capping of lower range \nleadscore['Page Views Per Visit'][leadscore['Page Views Per Visit']>=q4] = q4 #----- capping of higher range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('Page Views Per Visit', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=leadscore.Converted,y=leadscore['Page Views Per Visit'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###### From above plots we see that Median for converted and not converted leads are almost same. So We cannot say anything about the lead conversion based on Page Views ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### Analyzing Total Time Spent on Website","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Total Time Spent on Website'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   ###### The above summary shows that the total time is given in minutes. We will convert these to Hours first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(leadscore['Total Time Spent on Website'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Total Time Spent on Website'] = leadscore['Total Time Spent on Website'].apply(lambda x: round((x/60), 2))\nsns.distplot(leadscore['Total Time Spent on Website'], )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=leadscore.Converted, y=leadscore['Total Time Spent on Website'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The above plot tells that  Leads spending more time on the website are more likely to be converted.\n###### Therefore, we can suggest to the company to make the website more reliable and attractable for the leads, so that they spend more time on the website.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y = 'TotalVisits', x = 'Converted', data = leadscore)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q3 = leadscore.TotalVisits.quantile(0.95)\nleadscore = leadscore[(leadscore.TotalVisits <= Q3)]\nQ1 = leadscore.TotalVisits.quantile(0.05)\nleadscore = leadscore[(leadscore.TotalVisits >= Q1)]\nsns.boxplot(y=leadscore['TotalVisits'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='TotalVisits', x='Converted', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From above plots we see that Median for converted and not converted leads are almost same. So We cannot say anything about the lead conversion based on TotalVisits\n###### Analyzing Lead Source and Lead Origin","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"createCountPlot('Lead Source', (10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Replace the ctaegories with lower count with `Others` category to reduce the dummy variable. This is avoid the model complexity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Lead Source'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Lead Source'] = leadscore['Lead Source'].replace(['blog', 'Pay per Click Ads', \n                                                'bing', 'Social Media','WeLearn', 'Click2call', 'Live Chat', \n                                                'welearnblog_Home', 'youtubechannel', 'testone', 'Press_Release', 'NC_EDM'], 'Other')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Lead Source'] = leadscore['Lead Source'].replace('google', 'Google')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Source', hue='Converted', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From above plot we see that most of the leads are coming from sources like, `Olark Chat`, `Organic Search` , `Direct Traffic` and `Google` However their conversion rate is low. But the conversion rate for `Reference` and `Welingak WebSite` is quite high. So we should definitely consider these Lead Source in our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Lead Origin'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Origin', hue='Converted', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From the `Lead Origin` plot, we can see that most of the leads were identified from Landing Page submission and then by APIs. \n###### However the conversion rate of `Lead Add Form` is comparitably good. \n##### So we will drop only the `Lead Import` which is insignificant in this case, and it will unnecessarily create extra dummy variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop all the rows with `Lead Import` as Lead Origin\nleadscore.drop(leadscore[leadscore['Lead Origin'] == 'Lead Import'].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (10, 5))\nsns.countplot('Lead Origin', hue='Converted', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Lead Add form have higest Conversion rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (15, 5))\nsns.countplot('Last Activity', hue='Converted', data=leadscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### SMS sent has highest conversion rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nleadscore.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore_corr = leadscore[['Lead Origin', 'Lead Source', 'Converted', 'TotalVisits',\n       'Total Time Spent on Website', 'Page Views Per Visit', 'Last Activity',\n       'Specialization', 'What is your current occupation', 'City',\n       'A free copy of Mastering The Interview']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generateHeatmaps(leadscore_corr, (12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Based on the above heatmap we can say that `Page Views Per Visit` and `Total Visit` are correlated, Othere than these, there aren't any highly correlated features. \n###### Finally we have completed our analysis using visualization . Now we start the Data preparation with the cleansed dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Creating dummy variables for all categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feature =  leadscore.select_dtypes(include=['object']).columns\ncategorical_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### For the above 7 categorical columns, dummy encoding is required.\n###### We will create the dummies and drop the catergory which has the least frequency in each column\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['Specialization'], prefix  = 'Specialization')\ndummy = dummy.drop(['Specialization_Business Administration'], 1)\nleads_dummified = pd.concat([leadscore, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['Lead Source'], prefix  = 'Lead_Source')\ndummy = dummy.drop(['Lead_Source_Facebook'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['Last Activity'], prefix  = 'Last_Activity')\ndummy = dummy.drop(['Last_Activity_Email Link Clicked'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leadscore['Lead Origin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['Lead Origin'], prefix = 'Lead_Origin', drop_first=True)\n# dummy = dummy.drop(['Lead_Origin_Lead Add Form'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['What is your current occupation'], prefix = 'Occupation')\ndummy = dummy.drop(['Occupation_Other'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(leadscore['City'], prefix = 'City')\ndummy = dummy.drop(['City_Not Mumbai Cities'], 1)\nleads_dummified = pd.concat([leads_dummified, dummy], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization',\n       'What is your current occupation', 'City'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Test and Train Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\nlead_df_train,lead_df_test=train_test_split(leads_dummified,train_size=0.7,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = lead_df_train.drop(['Converted','Lead Number'], axis=1)\ny_train = lead_df_train['Converted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Scaling the numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_cols=X_train.select_dtypes(include=['float64', 'int64']).columns\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Now we will start with model building using the stats model and select top 15 significant features using the RFE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\nleads_reg = LogisticRegression()\n\nrfe = RFE(leads_reg, 15)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Check the columns reatained by rfe using rfe support and ranking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_selected_features = X_train.columns[rfe.support_]\nrfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel1 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model1.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_selected_features = rfe_selected_features.drop('Lead_Source_Organic Search', 1)\nrfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model2.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_selected_features = rfe_selected_features.drop('Lead_Source_Reference', 1)\nrfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel3 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model3.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrfe_selected_features = rfe_selected_features.drop('Last_Activity_Miscellaneous', 1)\nrfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[rfe_selected_features])\nmodel4 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nresult = model4.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### SInce all the p-values are less as ecpexted, so we can check the VIF for multicolinearity among the variables. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_selected_features].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_selected_features].values, i) for i in range(X_train[rfe_selected_features].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### VIF for all the variables are also less than 3. SO we can go ahead with this model with these features. \n\n###### Now we can proceed to derive the predictions, probabilities and  LeadScore on the trainin data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_selected_features = rfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = result.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = leadscore['Lead Number']\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Find the lead score and add a `Lead Score` column to the above table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We will add a new column `Predicted Hot Lead` assuming  threshold value as 0.5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Calaculate the Lead score based on the `Lead_Score_Prob` and generate the confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\ny_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score'].astype(int)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead )\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### With the help of the confusion matrix, we will calculate the below metrics, which will be used late for model evaluation later.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### 1. Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy for the Model 4 is {}%'.format(round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead),2)*100 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion_matrix[1,1] # true positive \nTN = confusion_matrix[0,0] # true negatives\nFP = confusion_matrix[0,1] # false positives\nFN = confusion_matrix[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity = round((TP / float(TP+FN)),2)\nspecificity = round((TN / float(TN+FP)),2)\n\nprint('Sensitivity is {}% and Specificity is {}%'.format(sensitivity*100, specificity*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######  This value of Sensitivity does not fullfill our objective ,as the CEO of the company has given a ballpark of the target lead conversion rate to be around 80%. This implies that the threshold value we chose ealier`[50%]` is not correct. Thus we need a better threshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The ROC AUC value, we got, is 0.80. This  indicates the we have  a good  model and is capable of distinguising the classes.\n\n###### Now we will find the prediction on the train data using this model and also calculate the cut-off threshold values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificty', 'Precision'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    preci = cm1[1,1]/(cm1[1,1]+cm1[0,1])   #TP/TP+FP\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci, preci]   \nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.set_context('paper')\n\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificty'], figsize=(10,6))\nplt.xticks(np.arange(0,1,step=.05), size=8)\nplt.yticks(size=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### As you can see, at about a threshold of 0.36, the curves of accuracy, sensitivity and specificity intersect, and they all take a value of around 80%. With this threshold we will predict the Hot Lead again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_off = 0.36","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: 1 if x > cut_off else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getRegressionMetrics(y_train_pred_final.Converted,y_train_pred_final.Predicted_Hot_Lead)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The above confusion matrix shows better results than the previously calculated one, using 0.5 as cutoff.\n###### Our  business requirement of getting the sensitvity value above 80% is also achieved in this model. Also the accuracy and F1 score is pretty good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(8, 4), dpi=100, facecolor='w', edgecolor='k', frameon='True')\nplt.title('Precision vs Recall')\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The precision vs recall tradeoff value from the above graph is at 0.41\n#### From the precision-recall graph above, we get the optimum threshold value as close to .41. However our business requirement here is to have Lead Conversion Rate around 80%.  \n\n#### This is already achieved with our earlier threshold  value of 0.36. So we will stick to this value.\n###### Now making predcitions on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df_test[num_cols] = scaler.transform(lead_df_test[num_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The below features were selected for the model in the train step","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_selected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = lead_df_test[rfe_selected_features]\ny_test = lead_df_test[['Lead Number', 'Converted']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = result.predict(X_test_sm)\ny_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coverting it to df\ny_pred_df = pd.DataFrame(y_test_pred)\n# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n# Remove index for both dataframes to append them side by side \ny_pred_df.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Append y_test_df and y_pred_df\ny_pred_final = pd.concat([y_test_df, y_pred_df],axis=1)\n# Renaming column \ny_pred_final= y_pred_final.rename(columns = {0 : 'Lead_Score_Prob'})\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['Predicted_Hot_Lead'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > cut_off else 0)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Find metrics of the test data resutls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"getRegressionMetrics(y_pred_final.Converted,y_pred_final.Predicted_Hot_Lead)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### So we getting almost similar results for test data as well, with 1% of deviation\n###### Now we will add the Lead Score to each leads","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)\ny_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### FInally we get the final result table of the test data, which contains the Leads Score for each leads, based on which we can determine whether a lead is HotLead or Cold Lead.\n###### Generate leadscore for the whole dataset, which was created after the EDA, before train_test_split. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Scaling the cleaned dataset again ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_dummified[num_cols] = scaler.transform(leads_dummified[num_cols])\nleads_dummified.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_lead_sm = sm.add_constant(leads_dummified[rfe_selected_features])\ncleaned_predicted = result.predict(cleaned_lead_sm)\ncleaned_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_lead_score_df = leadscore.copy()\nfinal_lead_score_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_lead_score_df['Lead Score']=round(cleaned_predicted*100,2)\nfinal_lead_score_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hot_leads = final_lead_score_df.sort_values(by='Lead Score',ascending=False)[['Lead Number','Lead Score']]\nhot_leads[hot_leads['Lead Score']>36] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The above list is showing the list of customers having score more than 36 .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_lead_score_df['Is_Hot_Lead'] = final_lead_score_df['Lead Score'].map(lambda x: 1 if x > 36 else 0)\nfinal_lead_score_df.sort_values(by='Lead Score',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above are top 10 leads which have high score and hence they have high chance of convertion\n#### Now we will determine the importance or the selected features\n###### Getting the coefficients from the final model summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff = result.params[1:]\ncoeff","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### To understand this better, we will find relative coefficients of the features. This will help us in camparing the features better.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_relevance = 100.0 * (coeff / coeff.max())\nfeature_relevance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_idx = np.argsort(feature_relevance,kind='quicksort',order='list of str')\nsorted_idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Now, we will plot the above data in a bar plot, to visulaize it better","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(10,6))\nax =  fig.add_subplot(1, 1, 1)\nax.barh(pos, feature_relevance[sorted_idx], align='center', color = 'tab:blue',alpha=0.8)\nax.set_yticks(pos)\nax.set_yticklabels(np.array(rfe_selected_features)[sorted_idx], fontsize=12)\nax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Fromt the above plot and data, we can find out the TOP  3 features, using the feature_relevance dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\npd.DataFrame(feature_relevance).reset_index().sort_values(by=0,ascending=False).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### After trying several models, we finally chose this Model, becuase it fullfilled the below criteria\n\n-  <font color = blue> All variables have p-value < 0.05.\n-  All the features have very low VIF values, meaning, there is hardly any muliticollinearity among the features. This is also evident from the heat map as well.\n-  The overall accuracy of 79% at a probability threshold of <bold> 0.36 </bold> on the test dataset is also acceptable.</font>\n\n###### <font color = blue> We can also tweak the  probability threshold value with in turn will decrease or increase the Sensitivity and increase or decrease the Specificity of the model, based on the business requirements. </font>\n    \n###### <font color = blue> High Sensitivity ensures that the leads who are likely to be convertted are correctly predicted where as high Specificity will ensure that leads that are on the cut-off of the probability of getting converted or not are not selected. </font>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The top three variables which contributed the most in lead conversion are: \n1. Lead Origin\n1. What is the Current Occupation? \n1. LeadSource.\n\n\n> The categories in each variable which are important are:\n1. Working Professionals (in Current Occupation)\n1. Lead Add Form (Lead Origin) and \n1. Welingak Website (in Lead Source).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### X Education has a period of 2 months every year during which they hire some interns. The sales team has around 10 interns allotted to them. So, during this phase, they wish to make the lead conversion more aggressive. So, they want almost all of the potential leads (i.e. the customers who have been predicted as 1 by the model) to be converted and hence, want to make phone calls to as much of such people as possible. Suggest a good strategy they should employ at this stage.\n\n> Since the resources in the sales team has increased, they can target for as much leads as possible. The cutoff criteria of Hot Leads can be lowered. We can target for 90% sensitivity which is at Lead score 20. At this point the accuracy is also 75% which is considerably good. At this cut off the salesperson can first target the top Hot Leads, which have more chances to get converted.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### Similarly, at times, the company reaches its target for a quarter before the deadline. During this time, the company wants the sales team to focus on some new work as well. So, during this time, the companys aim is to not make phone calls unless its extremely necessary, i.e. they want to minimize the rate of useless phone calls.\n\n> In this case, as there is a restriction on calling the leads, the sales team should not call, those leads which have very less chance of conversion. They should try to reach those customers which have more chance.\nHere, we should avoid false positive count as much as possible. That means we should target to achieve high precision. Precision is the positive predictive value or the fraction of the positive predictions that are actually positive. So, the sales team should focus on the leads having score more than 65 (as at lead score 60 the precision is 82 and at 70 the precision is ~82%, at this cutoff the accuracy is ~80%) to achieve more than 85% precision.\n\n>Also, for the remaining leads with low Lead Score, since the company doesnt aim to make phone calls, they can use other methods of reaching the customers like, automated emails and SMSes. By this strategy, we can reach to most of the customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}