{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification of Stack Overflow questions\n\nThis notebook presents different applications of Machine Learning methods in attempt to classify Stack Overflow-questions into three categories: HQ, LQ_EDIT and LQ_CLOSE. \n\nThe approaches used are Naive Bayes (NB) on a vectorized input of body and body + title and a combination of NB and k-Nearest Neighbors (kNN) as a means of trying to improve the predictions done by NB. "},{"metadata":{},"cell_type":"markdown","source":"### Imports\nThis project uses numpy to handle matrices and vectors, pandas to handle dataframes from the dataset, pyplot to plot results, re to handle regex-strings used to clean the data.\nImports from Scikit-learn are TFidVectorizer used to extract features from text, MultinomialNB which is a naive bayes-approach with more than 2 possible outputs or predictions (unlike BernoulliNB which only handles binary predictions) and KNeighborsClassifier which is the kNN-model used in the project."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset\nThe dataset used is [60k Stack Overflow Questions with Quality Rating](http://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate). This dataset comes split into a set of training data and validation data, in sizes of respectively 45.000 entries and 15.000 entries. The validation-set is kept separately to maintain a common ground for comparing models and results. \n\nThe three possible classifications, LQ_CLOSE, LQ_EDIT and HQ are mapped to integeres 0-2, and the columns with id and creationdate are dropped as these are not relevant for the task at hand. \n\nA concatenation of Title and Body is created separately for use when comparing the accuracy between NB-models with additional information. \n\nAs seen in the target value distribution, there is an equal amount of each classification in the dataset, meaning that a random guess-approach will result in about 33.3% accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv\")\nvalid = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv\")\n\ndata=train.drop(['Id', 'CreationDate'], axis=1)\nvalid = valid.drop(['Id', 'CreationDate'], axis=1)\ndata['Y']=train['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT':1, 'HQ':2})\nvalid['Y']=valid['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT':1, 'HQ':2})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge title and body to assess whether or not including Title gives better performance\ntitle_body_data = data.copy()\ntitle_body_data['text'] = title_body_data['Title'] + ' ' + title_body_data['Body']\ntitle_body_data = title_body_data.drop(['Title', 'Body', 'Tags'], axis=1)\ntitle_body_valid = valid.copy()\ntitle_body_valid['text'] = title_body_valid['Title'] + ' ' + title_body_valid['Body']\ntitle_body_valid = title_body_valid.drop(['Title', 'Body', 'Tags'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create target value distribution-diagram\nlabels = ['High quality', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nvalid_values = [len(valid[valid['Y'] == 2]), len(valid[valid['Y'] == 0]), len(valid[valid['Y'] == 1])]\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(16,9))\nfig.patch.set_facecolor('white')\nrects1 = ax.bar(x - width/2, values, width, label='Training data')\nrects2 = ax.bar(x + width/2, valid_values, width, label='Validation data')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Data size')\nax.set_title('Target value distribution')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\nplt.show()\nfig.savefig('target_value.pdf')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The question body contains a lot of HTML-tags which do not bring any context into whether a question is good or not, so a cleaning-function is applied to remove all HTML-tags with the content inside of it. Special characters are changed to whitespace as to keep the text clean and separated for use in the vectorizer. \n\nAfter some trial and error, it was found that removing the HTML-tags gave worse results than just removing the special characters < and >, so the cleaning-function had to be changed. \n\nThe column containing tags had a format of &lt;Tag1>&lt;Tag2>, which meant that cleaning the data ended with replacing '><' with a whitespace and removing < and >."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regex-strings to remove HTML-tags and other symbols not associated with text or code.\ndef clean_data(text):\n    text = text.lower()\n    # text = re.sub(r'<[^>]*>',' ', text)\n    text = re.sub(r'[^(a-zA-Z0-9)\\s]','', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove <, > and ><\ndef clean_tags(text):\n    text = text.lower()\n    text = re.sub(r'><',' ', text)\n    text = re.sub(r'[<>]','', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions\nBefore diving into the models themselves, there is a need to create some functions that can be applied across multiple models to keep the results consistent and enable a more functional and clean approach in general. \n\n### K-fold cross-validation\nThe k_fold_datasets-function is used to create training and validation datasets from the original training dataset, and returns train_x and valid_x containing the column specified with parameter key, otherwise the returned train_x and valid_x contains the parameters contained in keys. A clean-function is applied if clean=True, which is meant to remove special characters that might skew the results from the vectorizer. \n\nThe amount of parameters passed is a result of keeping the function as general as possible to handle both input with text and input with predicted classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to return training and validation sets based on k-fold cross validation\n# n defines the iteration number (n>0 and n<=k)\n# key defines which column to classify based on\n# keys defines the different columns present in the data\n# clean_fn defines the function to clean the input\ndef k_fold_datasets(n,data,k, key, keys, clean_fn, clean=True):\n    assert n>0\n    size = len(data)\n    # Make sure that no single questions are left outside of dataset\n    assert size//k == size/k\n    lower = int(size/k)*(n-1)\n    upper = int(size/k)*n\n    # Training Sets\n    train = pd.concat([data[:lower],data[upper:]], keys=keys)\n    if clean:\n        trainX = train[key].apply(clean_fn)\n    else:\n        trainX = train.copy().drop(['Y'], axis=1)\n    trainY = train['Y'].values\n\n    # Validation Sets\n    valid = data[lower:upper]\n    if clean:\n        validX = valid[key].apply(clean_fn)\n    else:\n        validX = valid.copy().drop(['Y'], axis=1)\n    validY = valid['Y'].values\n    \n    return trainX, trainY, validX, validY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization\nThe vectorizer used to create a representation of the text in the dataset is a [Term Frequency Inverse Document Frequency (TF-IDF) vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). The theory behind this vectorizer is further explained in the report for the project."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Key and Keys are used when creating concats of different columns\nvectorizer = TfidfVectorizer()\ndef vectorized_data(data, n, k, key, keys, clean_fn = clean_data):\n    trainX, trainY, validX, validY = k_fold_datasets(n, data, k, key, keys, clean_fn)\n    \n    trainX = vectorizer.fit_transform(trainX)\n    validX = vectorizer.transform(validX)\n    return trainX, trainY, validX, validY\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Averages and validaton\nWhen tuning hyperparameters the average accuracy is calculated from the accuracy per iteration with k-fold cross-validation. \n\nThe function validated_results_nb uses the provided validation set to calculate the overall score with optimal alpha"},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_nb(nb_model, k, dataset, key=\"Body\", keys=[\"Title\", \"Body\", \"Tags\", \"Y\"]):\n    average = 0\n    for i in range(1,k+1):\n        # Create k-fold training and testing sets\n        trainX, trainY, validX, validY = vectorized_data(dataset,i, k, key, keys)\n        nb_model.fit(trainX, trainY)\n        score = nb_model.score(validX, validY)\n        average+=score\n    average = average / k\n    return average","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validated_results_nb(train, valid, alpha, key):\n    trainX = vectorizer.fit_transform(train[key].apply(clean_data))\n    trainY = train['Y'].values\n    validX = vectorizer.transform(valid[key].apply(clean_data))\n    validY = valid['Y'].values\n    nb = MultinomialNB(alpha=alpha)\n    nb.fit(trainX, trainY)\n    score = nb.score(validX, validY)\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementation\nAs to ensure that results are consistent, the training data is randomized before every full run. This means that within one run the data is the same, but the data set might be randomized between full runs with hyperparameter-tuning, meaning that the optimal value for alpha might vary by a few decimals. \n\nA list of possible alpha-values is used with grid-search to determine the optimal alpha with a margin of 0.05.\n\nThe amount of iterations with k-fold is set to 6."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomize data\ndata = data.sample(frac=1)\nalphas = [x/20 for x in range(1,21)]\nk_fold = 6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question body, Naive Bayes\nIterate over the alpha-values provided and calculate the average score. Save the highest average score and corresponding alpha for use in validation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test hyperparameters of NB with only body as predictor\nbest_average_body = 0\nbest_alpha_body = -1\nfor a in alphas:\n    nb = MultinomialNB(alpha=a)\n    average = average_nb(nb, k_fold, data)\n    print(\"a=%.2f: %.3f\" % (a, average))\n    if average>best_average_body:\n        best_average_body = average\n        best_alpha_body = a\nprint(\"Best average: %.3f \\n Best alpha: %.2f\" % (best_average_body, best_alpha_body))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question Title and Body, Naive Bayes\nSame approach as the previous one but with title appended to the question body. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test hyperparameters of NB with body and title as predictor\nbest_average = 0\nbest_alpha = -1\nfor a in alphas:\n    nb = MultinomialNB(alpha=a)\n    average = average_nb(nb, k_fold, title_body_data, key='text', keys=['text', 'Y'])\n    print(\"a=%.2f: %.3f\" % (a, average))\n    if average>best_average:\n        best_average = average\n        best_alpha = a\nprint(\"Best average: %.3f \\n Best alpha: %.2f\" % (best_average, best_alpha))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation, Naive Bayes\nThe two optimal alphas found in the previous two steps are used to train two Naive Bayes-models on the full training set and validated with the provided validation set as to have the same grounds for comparing different models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score on validation set with body as predictor and alpha=%.2f: %.3f\" \n      % (best_alpha_body, validated_results_nb(data, valid, best_alpha_body, 'Body')))\nprint(\"Score on validation set with body and title as predictor and alpha=%.2f: %.3f\" \n      % (best_alpha, validated_results_nb(title_body_data, title_body_valid, best_alpha, 'text')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question Title, Body and Tags separately, Naive Bayes and kNN\nThe best alpha for prediction based on question body is reused to train a Naive Bayes-model, trained on a dataset with a size defined by the parameter 'split', and then used to predict the rest of the dataset and provided validation set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB(alpha=best_alpha_body)\n\ndef predict_nb(split):\n    cols = [\"Title\", \"Body\", 'Tags']\n    # Corresponding cleaning functions to each column\n    clean_fns = [clean_data, clean_data, clean_tags]\n    predicted_data = data[:45000//split].copy()\n    for i in range(len(cols)):\n        trainX, trainY, predictX, predictY = vectorized_data(data, 1, k=split, key=cols[i], keys=[\"Title\", \"Body\", 'Tags', \"Y\"], clean_fn = clean_fns[i])\n        # Swap predict and train to get smaller than n/2 dataset for NB\n        nb.fit(trainX, trainY)\n        print(\"%s: %.3f\" % (cols[i], nb.score(predictX, predictY)))\n        predicted_data[cols[i]] = nb.predict(predictX)\n        predicted_data['Y'] = predictY\n    return predicted_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to measure average performance across k-fold cross-validation, used in hyperparameter-tuning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_knn(knn_model, k, dataset, key=\"Body\", keys=[\"Title\", \"Body\", \"Tags\", \"Y\"]):\n    average = 0\n    for i in range(1,k+1):\n        # Create k-fold training and testing sets\n        trainX, trainY, validX, validY = k_fold_datasets(i, dataset, k, key, keys, clean_fn=None, clean=False)\n        knn_model.fit(trainX, trainY)\n        score = knn_model.score(validX, validY)\n        average+=score\n    average = average / k\n    return average","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The split is set to 3, which gives NB 30 000 entries for training and then make prediction on the remaining 15 000 entries.\n\nThe format of the prediction done by NB is shown below when running."},{"metadata":{"trusted":true},"cell_type":"code","source":"split=3\nprint(\"Predicted data accuracy:\")\npredicted_data = predict_nb(split)\npredicted_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable k_list contain the different values for k, and is a combination of arbitrarily chosen numbers and functions based on the size of the dataset. \n\nAll possible k-values in k_list is iterated over and the average score from k-fold cross-validation is calculated. Best k is saved for use in validation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_size = int(len(predicted_data)*(k_fold-1)/k_fold)\nk_list = [1, 10, 100, int(np.log(pred_size)), \n          int(np.sqrt(pred_size)), int(pred_size/2), \n          pred_size]\nprint(k_list)\n\nprint(\"kNN hyperparameters accuracy:\")\nbest_average_knn = 0\nbest_k_knn = -1\nfor k_param in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k_param, p=2)\n    average = average_knn(knn, k_fold, predicted_data)\n    print(\"k=%d: %.3f\" % (k_param, average))\n    if average>best_average_knn:\n        best_average_knn = average\n        best_k_knn = k_param\nprint(\"Best average: %.3f \\n Best k: %d\" % (best_average_knn, best_k_knn))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation, NB and kNN\nThe final code-block is a bit cluttered, so a step-by-step description of the purpose is provided:\n1. Initialization of NB and kNN with optimal hyperparameters from earlier.\n2. Split the data in partitions of 30 000 entries for training NB, 15 000 for training kNN based on predictions from NB and provided validation set of 15 000 entries, also for predicting with NB and validating kNN. \n3. Iterate over Title, Body and Tags:\n    3.1. Vectorize input data in given column from all three partitions so that NB can handle it. \n    3.2. Train NB on first partition of 30 000 entries on given column\n    3.3. Predict training data for kNN and validation data for kNN on given column\n4. Train kNN on training data with predictions from NB.\n5. Calculate accuracy on validation set with predictions from NB.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validation of knn-model\ncols = [\"Title\", \"Body\", 'Tags']\nnb = MultinomialNB(alpha=best_alpha_body)\nknn = KNeighborsClassifier(n_neighbors=best_k_knn, p=2)\n# Corresponding cleaning functions to each column\nclean_fns = [clean_data, clean_data, clean_tags]\ntraining_data = data[15000:].copy()\npredicted_data = data[:15000].copy()\npredicted_valid = valid.copy()\nfor i in range(len(cols)):\n    trainX = vectorizer.fit_transform(training_data[cols[i]].apply(clean_fns[i]))\n    trainY = training_data['Y'].values\n    predictX = vectorizer.transform(predicted_data[cols[i]].apply(clean_fns[i]))\n    predictY = predicted_data['Y'].values\n    validX = vectorizer.transform(predicted_valid[cols[i]].apply(clean_fns[i]))\n    validY = predicted_valid['Y'].values\n    nb.fit(trainX, trainY)\n    predicted_data[cols[i]] = nb.predict(predictX)\n    predicted_data['Y'] = predictY\n    predicted_valid[cols[i]] = nb.predict(validX)\n    predicted_valid['Y'] = validY\n    print(\"%s: %.3f\" % (cols[i], nb.score(validX, validY)))\nknn.fit(predicted_data.drop(['Y'], axis=1),predicted_data.drop(['Body', 'Title', 'Tags'], axis=1))\nscore = knn.score(predicted_valid.drop(['Y'], axis=1), predicted_valid.drop(['Body', 'Title', 'Tags'], axis=1))\nprint(score)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}