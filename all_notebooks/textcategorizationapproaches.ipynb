{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing required packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nfrom collections import Counter\nfrom textblob import TextBlob\nimport re\nimport nltk\nimport xgboost as xgb\nimport numpy as np\nimport collections\nimport operator\nimport itertools\nimport os\n\n\nfrom nltk.corpus import stopwords\nimport nltk.stem.snowball\nst = nltk.stem.snowball.SnowballStemmer('english')\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nstemmer = SnowballStemmer('english')\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, accuracy_score ,confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn import utils\n\nfrom yellowbrick.text import FreqDistVisualizer\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.models import Doc2Vec\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim import models\nfrom gensim.models.doc2vec import TaggedDocument\nimport pyLDAvis\nimport pyLDAvis.gensim as gensimvis\n\nfrom itertools import islice\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import utils\n\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\nimport spacy\nfrom spacy_langdetect import LanguageDetector\nnlp = spacy.load('en')\n\nfrom wordcloud import WordCloud \n\nimport chart_studio.plotly.plotly as py\nimport plotly.graph_objs as go\nimport chart_studio\n\n## Offline mode\nfrom plotly.offline import init_notebook_mode, iplot\nchart_studio.tools.set_credentials_file(username='avinashok', api_key='SZBL9GagG9yPYCwfSuDc')\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Variable Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Directories\ndataDirectory = \"../input/\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading Data file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading data file  \ndf = pd.read_csv(dataDirectory+'telecomagentcustomerinteractiontext/'+'CustomerInteractionData.csv')\n\n#Randomly shuffling data\ndf = df.sample(len(df))\n\n#Copying the Raw comment column for Future Use\ndf['CustomerInteractionText'] = df['CustomerInteractionRawText']\n\n#Checking the initial 5 rows\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Objects Used\ncommentTextColumn = 'CustomerInteractionText'\nagentAssignedColumn = 'AgentAssignedTopic'\nlocationID = 'LocationID'\ncallDuration = 'CallDurationSeconds' #In Seconds\nagentID = 'AgentID'\ncustomerID = 'CustomerID'\nrawText = 'CustomerInteractionRawText'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Data Quality"},{"metadata":{},"cell_type":"markdown","source":"By doing a Data Quality Check we are:\n\n- Identifying a unique column which can be assigned as a Primary Key column (incase it wasn't specified initially.)\n- Making sure all the columns are having the required datatypes\n- Calculating the Null Values, Duplicate values, etc. (Maximum, Minimum, Range in case its a numeric column.).\n- Creating visualizations to understand the Data Quality level of input data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying Primary Key\n\nprint(\"Number of Columns in the Dataset = \" + str(len(df.columns)))\n\nuniqueColumns=[]\nfor col in df.columns.to_list():\n    if len(df[str(col)].unique())==df.shape[0]:\n        uniqueColumns.append(col)\nif len(uniqueColumns)==1:\n    primaryKeyColumn = str(uniqueColumns[0])\n    print(\"Primary Key = \"+primaryKeyColumn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Datatypes of each columns in the dataset\nprint(df.dtypes)\n\n# Specifying datatypes we want for each column\ndataTypeDictionary = {\n    primaryKeyColumn: 'int64',\n    commentTextColumn: 'object',\n    rawText:'object',\n    agentAssignedColumn: 'object',             \n    locationID: 'int64',\n    callDuration: 'int64',\n    agentID: 'object',\n    customerID: 'object'\n    \n }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicatesCount = {}\nfor col in df.columns.to_list():\n    duplicatesCount[col] = [((df.duplicated(col).sum()/len(df))*100),100-((df.duplicated(col).sum()/len(df))*100)]\n\nnullCounter = {}\nfor col in df.columns.to_list():\n    count = 0\n    for cell in df[str(col)]:\n        if cell=='?' or cell==\"\":   # or len(str(cell))==1\n            count= count +1\n    nullCounter[col]=[float(count/len(df))*100,100-float(count/len(df))*100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataQualityCheck(checkName, columnName):\n    if checkName == \"Null Values\":\n        # create data\n        names='Null Values', 'Non Null Values',\n        size=np.array(nullCounter[columnName])\n        print(\"Null Values Data Quality Check for \"+str(columnName))\n        def absolute_value(val):\n            a  = np.round(val/100.*size.sum(), 0)\n            return a\n        # Create a circle for the center of the plot\n        my_circle=plt.Circle( (0,0), 0.7, color='white')\n\n        # Custom colors --> colors will cycle\n        plt.pie(size, labels=names, colors=['red','green'],autopct=absolute_value)\n        p=plt.gcf()\n        p.gca().add_artist(my_circle)\n        plt.show();\n    elif checkName ==\"Duplicates\": \n                # create data\n        names='Duplicate Values', 'Unique Values Values',\n        size=np.array(duplicatesCount[columnName])\n        print(\"Duplicate Value Data Quality check for \"+str(columnName))\n        def absolute_value(val):\n            a  = np.round(val/100.*size.sum(), 0)\n            return a\n        # Create a circle for the center of the plot\n        my_circle=plt.Circle( (0,0), 0.7, color='white')\n\n        # Custom colors --> colors will cycle\n        plt.pie(size, labels=names, colors=['red','green'],autopct=absolute_value)\n        p=plt.gcf()\n        p.gca().add_artist(my_circle)\n        plt.show();\n    elif checkName == \"Details\":\n        print(\"Details of the Column: \\n \")\n        print(\"Original Datatype should be \"+dataTypeDictionary[columnName]+\"\\n\")\n        print(\"Datatype in the data is \"+str(df[str(columnName)].dtypes)+\"\\n\")\n    elif checkName == \"Range\":\n        if str(df[str(columnName)].dtypes)=='int64' or str(df[str(columnName)].dtypes)=='datetime64[ns]':\n            print(\"Maximum Value is \"+str(df[str(columnName)].max())+\" \\n \")\n            print(\"Minimum Value is \"+str(df[str(columnName)].min()))\n        else:\n            print(\"Since the Datatype of column \"+str(columnName)+\" is not numeric in the given data, Range cannot be calculated.\")\n    \n    \ndef dQexecute(columnName):\n    print(\"\\n Name of the Column \"+str(columnName)+\"\\n \\n\")\n    dataQualityCheck(\"Details\",columnName)\n    dataQualityCheck(\"Null Values\",columnName)\n    dataQualityCheck(\"Duplicates\",columnName)\n    dataQualityCheck(\"Range\",columnName)\n    print(\"*****************\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df.columns.to_list():\n    dQexecute(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agent Assigned Topic Distribution Analysis"},{"metadata":{},"cell_type":"markdown","source":"For visualizing Topic distribution, we use the package [plotly](https://plot.ly/) which makes the bar charts more interactive."},{"metadata":{"trusted":true},"cell_type":"code","source":"z = {}\nuniqueTopics = list(df[agentAssignedColumn].unique())\nfor i in uniqueTopics:\n    z[i]=i\n\ndata = [go.Bar(x = df[agentAssignedColumn].map(z).unique(),y = df[agentAssignedColumn].value_counts().values,\n        marker= dict(colorscale='Jet',color = df[agentAssignedColumn].value_counts().values),text='Number of Calls for this reason')]\n\nlayout = go.Layout(title='Reasonwise Call Distribution')\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='TelecomCallDistribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyzing the Word Count for each Reasons"},{"metadata":{},"cell_type":"markdown","source":"- We can check average Word count for each reasons. At times taking Median word count might also be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['totalwords'] = df[commentTextColumn].str.split().str.len()\ndef reasonCodeLevelWordCount(reasonCode, parameter):\n    dfReasonCodeSubset = df[df[agentAssignedColumn]==reasonCode]\n    if parameter == 'mean':\n        return float(dfReasonCodeSubset.describe()['totalwords'][1])\n    elif parameter == 'median':\n        return float(dfReasonCodeSubset.describe()['totalwords'][5])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Mean Word Count\nreasonCodeDict = {}\nfor topic in uniqueTopics:\n    reasonCodeDict[str(topic)]=float(reasonCodeLevelWordCount(topic, 'mean'))\nplt.figure(figsize=(20,20))\nplt.title(\"Mean Word Frequency for each Topic\", fontdict=None, loc='center')\nplt.bar(reasonCodeDict.keys(), reasonCodeDict.values(), width = 0.1  , color='g')\nplt.show()\n\nprint(\"\\n\\n ******************** \\n\\n \")\n\n#Median Word Count (Optional)\nreasonCodeDict = {}\nfor topic in uniqueTopics:\n    reasonCodeDict[str(topic)]=float(reasonCodeLevelWordCount(topic, 'median'))\nplt.figure(figsize=(20,20))\nplt.title(\"Median Word Frequency for each Topic\", fontdict=None, loc='center')\nplt.bar(reasonCodeDict.keys(), reasonCodeDict.values(), width = 0.1  , color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Token (vocabulary) Frequency Distribution Before Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Visualizing frequency distribution of top words helps us in two ways:\n\n- Determine which words needs to be removed & devising a process flow of cleaning the text.\n- Comparing the Frequency distribution \"Before\" & \"After\" Text Preprocessing gives us a visual idea about how well we are cleaning the data for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df[commentTextColumn])\nfeatures   = vectorizer.get_feature_names()\nplt.figure(figsize=(12,8))\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nfor label in visualizer.ax.texts:\n    label.set_size(20)\nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"In Text Preprocessing, we are performing multiple Natural Language Processing techniques. The idea is to get noise-free standardized text which can be taken for used readily for Modelling. Steps of Preprocessing largely depends on the dataset we are dealing with. In this case the Text Preprocessing comprises of the following steps:\n\n\n- Object Standardization: Since the free text we have were written by Call Centre agents, they use a lot of domain specific Short Hands, abbreviations, etc. which are not present in any standard lexical dictionaries. But then, at certain times, they write the full form or complete word. Object Standardization strike a balance between these & use a same form of word through out the dataset.\n\n\n- Named Entity Recognition(NER): Detecting names of 'Person', 'Location', 'Product' etc. and removing them from the dataset as they don't specifically add much value to our final model. (Note: There are cases where such entities are important & should be retained in some format.). We use [SpaCy](https://spacy.io/usage/linguistic-features) for NER.\n\n\n- Other Language Identification: Since the data we have comes from different locations, there could be a possibility of other languages present in the data. We use `spacy_langdetect` for identifying them and removing them.\n\n\n- Alphanumeric removal: Since this is Telecom Call centre data, there are a lot of alphanumeric words involved. For eg: 3G, 20MBps, agentID: '4ap2X', etc. We can keep it or remove it as per requirement. \n\n\n- Text Cleaning: Next we remove all the special characters, ASCII characters, numbers, single character words, etc.\n\n\n- Autocorrection: The agent comments might have a lot of spelling mistakes, typos etc.  We try to correct atleast some of them using `TextBlob` Autocorrection.\n\n\n- Lexicon Normalization: We also normalize multiple representations of a word to its etymological root form. For eg: 'wanted', 'wanting', 'want' becomes > 'want'\n\nAfter this we perform the following operations on the dataset:\n\n- Remove null comments.\n- Remove duplicate comments (keeping its first occurance.)\n- Remove comments with just one word."},{"metadata":{},"cell_type":"markdown","source":"### Object Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nabbrevationDictionary = {'Cus': 'customer', 'cus': 'customer',\n                        'Xferred':'transferred', 'xferred': 'transferred'} \n\n#Function to Standardize Text\ndef objectStandardization(input_text):\n    words = str(input_text).split() \n    new_words = [] \n    for word in words:\n        word = re.sub('[^A-Za-z0-9\\s]+', ' ', word) #remove special characters\n        if word.lower() in abbrevationDictionary:\n            word = abbrevationDictionary [word.lower()]\n        new_words.append(word) \n    new_text = \" \".join(new_words) \n    return new_text\n\ndf[commentTextColumn] = df[commentTextColumn].apply(objectStandardization)\n\nprint(df[commentTextColumn].head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Named Entity Recognition"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Function to extract Names of persons, organizations, locations, products etc. from the dataset\ndef entityCollector(df):\n    listOfNames = []\n    for index, row in df.iterrows():\n        doc = nlp(row[str(commentTextColumn)])\n        fil = [(i.label_.lower(), i) for i in doc.ents if i.label_.lower() in [\"person\", \"gpe\", \"product\"]] # Extracts Person Names, Organization Names, Location, Product names\n        if fil:\n            listOfNames.append(fil)\n        else:\n            continue\n    flat_list = [item for sublist in listOfNames for item in sublist]\n    entityDict = {}\n    for a, b in list(set(flat_list)): \n        entityDict.setdefault(a, []).append(b)\n    return entityDict\n\nentityDict = entityCollector(df)\n\nprint(\"\\n Types of entities present in the data are: \"+\", \".join(list(entityDict.keys()))+\" \\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a custom Stop Word List"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor entity in list(entityDict.keys()):\n    entityDict[entity] = [str(i) for i in entityDict[entity]]\n\nignoreWords = []\nfor key in entityDict.keys():\n    ignoreWords.append(entityDict[key])\nignoreWords = [item for sublist in ignoreWords for item in sublist]\n\nprint(\"Number of words in Custom Stopword list = \"+str(len(ignoreWords)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identify the Language Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef languageDistribution(df):\n    nlp = spacy.load(\"en\")\n    nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n    df['language']=''\n    language = []\n    for index, row in df.iterrows():\n        text = row[str(commentTextColumn)]\n        doc = nlp(text)\n        language.append(str(doc._.language['language']))\n    df['language'] = language\n    return df\n\ndf = languageDistribution(df)\nlangDict = df.groupby('language')[str(primaryKeyColumn)].nunique().to_dict()\n\notherLanguagesList = list(langDict.keys()).remove('en')\nprint(\"Some sample other language texts: \\n\")\nfor lang in list(langDict.keys()):\n    print(str(df[df['language']==str(lang)].values.tolist()[0]))\n\n##Plot a Pie Distribution of Language Distribution\nbin_percent = pd.DataFrame(df['language'].value_counts(normalize=True) * 100)\nplot = bin_percent.plot.pie(y='language', figsize=(10, 10), autopct='%1.1f%%')\n\n#Dropping only the row with Spanish text\ndf = df.drop(df[df['language'] == 'es'].index);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identify Words which contains numbers in it"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Function to extract Alpha numeric words\ndef alphanumericExtractor(input_text):\n    words = str(input_text).split()\n    alphanumericWordlist = []\n    for word in words:\n        word = re.sub('[^A-Za-z0-9\\s]+', '', word.lower()) #remove special characters\n        word = re.sub(r'[^\\x00-\\x7F]+',' ', word) # remove ascii\n        if not word.isdigit() and any(ch.isdigit() for ch in word):\n            alphanumericWordlist.append(word)\n        else:\n            continue\n    return alphanumericWordlist\n\n#Function to get the frequency of Alphanumeric words in the data\ndef alphanumericFrequency(df, commentTextColumnName):\n    alphanumericWordsList = []\n    for index, row in df.iterrows():\n        if alphanumericExtractor(row[str(commentTextColumnName)]):\n            alphanumericWordsList.append(alphanumericExtractor(row[str(commentTextColumnName)]))\n        else:\n            continue\n    flat_list = [item for sublist in alphanumericWordsList for item in sublist]\n    counts = Counter(flat_list)\n    countsdict = dict(counts)\n    return countsdict\n\n# Final list of alphanumeric words\nalphanumericWordFreqDict = alphanumericFrequency(df, commentTextColumn)\n    \n# To plot the distribution\ntotalWordcount = len(alphanumericWordFreqDict)\n\ntopWordCount = input('How many top words do you want? maximum= '+str(totalWordcount)+ ' \\n ')\n# topWordCount = totalWordcount\n\nalphanumericWordFreqDictTop = dict(sorted(alphanumericWordFreqDict.items(), key=operator.itemgetter(1), reverse=True)[:int(topWordCount)])\nprint(alphanumericWordFreqDictTop)\n\nplt.figure(figsize=(20,20))\nplt.title('Frequency of AlphaNumeric Words in the Dataset', fontdict=None, loc='center')\nplt.bar(alphanumericWordFreqDictTop.keys(), alphanumericWordFreqDictTop.values(), width = 0.1  , color='b');\nplt.show();\n\n#Updating Custom stopword list with Alphanumeric words\nignoreWords = ignoreWords + list(alphanumericWordFreqDict.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(newDesc):\n    newDesc = re.sub('[^A-Za-z\\s]+', '', newDesc) #remove special characters\n    newDesc = re.sub(r'[^\\x00-\\x7F]+','', newDesc) # remove ascii\n    newDesc = ' '.join( [w for w in newDesc.split() if len(w)>1] )  \n    newDesc = newDesc.split()\n    cleanDesc = [str(w) for w in newDesc if w not in ignoreWords] #remove entity names, alphanumeric words\n    return ' '.join(cleanDesc)\n\ndf[commentTextColumn] = df[commentTextColumn].apply(clean_text)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrection"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef textAutocorrect(df, columnName):\n    df[str(columnName)] = df[str(columnName)].apply(lambda txt: ''.join(TextBlob(txt).correct()))\n    return True\n\ntextAutocorrect(df, commentTextColumn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"stops = nlp.Defaults.stop_words\ndefault_stopwords = stopwords.words('english')\ncustomStopWords = {'PRON', 'pron'}\nstops.update(set(default_stopwords))\nstops.update(set(customStopWords))\n\ndef normalize(comment, lowercase, remove_stopwords):\n    if lowercase:\n        comment = comment.lower()\n    comment = nlp(comment)\n    lemmatized = list()\n    for word in comment:\n        lemma = word.lemma_.strip()\n        if lemma:\n            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n                lemmatized.append(lemma)\n    normalizedSentence = \" \".join(lemmatized)\n    normalizedSentence = re.sub('[^A-Za-z\\s]+', '', normalizedSentence)  # remove special characters\n    normalizedSentence = normalizedSentence.split()\n    cleanDesc = [str(w) for w in normalizedSentence if w not in stops] #remove PRON\n    return \" \".join(cleanDesc)\n\ndf[commentTextColumn] = df[commentTextColumn].apply(normalize, lowercase=True, remove_stopwords=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Null Comments\ndef removeNullValueCommentText(df, columnName):\n    initialLength = len(df)\n    df = df[pd.notnull(df[columnName])]\n    finalLength = len(df)\n    print(\"\\n Number of rows with Null Value in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    return df\ndf = removeNullValueCommentText(df, commentTextColumn)\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing duplicate comments keeping the first one\ndef removeDuplicateComments(df, columnName, agentAssignedColumn):\n    initialDf = df.copy()\n    initialLength = len(initialDf)\n    finalDf = df.drop_duplicates(subset=[columnName], keep='first')\n    finalLength = len(finalDf)\n    print(\"\\n Number of rows with duplicate comments in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    print(\"\\n The Level 3 Reason Codes for the dropped rows are given below: \\n\")\n    droppedDF = initialDf[~initialDf.apply(tuple,1).isin(finalDf.apply(tuple,1))]\n    print(droppedDF[agentAssignedColumn].value_counts())\n    return finalDf,droppedDF\n\ndf,droppedDF = removeDuplicateComments(df, commentTextColumn, agentAssignedColumn)\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing comments with just one word. (Like #CALL?)\ndef removingShortComments(df, columnName, agentAssignedColumn, numberOfWords = 1):\n    initialDf = df.copy()\n    initialLength = len(initialDf)\n    finalDf = df[~(df[str(columnName)].str.split().str.len()<(int(numberOfWords)+1))]\n    finalLength = len(finalDf)\n    print(\"\\n Number of rows with short comments in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    print(\"\\n The Level 3 Reason Codes for the dropped rows are given below: \\n\")\n    droppedDF = initialDf[~initialDf.apply(tuple,1).isin(finalDf.apply(tuple,1))]\n    print(droppedDF[agentAssignedColumn].value_counts())\n    return finalDf,droppedDF\n\ndf,droppedDF = removingShortComments(df, commentTextColumn, agentAssignedColumn)\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Token Frequency Distribution After Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df[commentTextColumn])\nfeatures   = vectorizer.get_feature_names()\nplt.figure(figsize=(12,8))\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nfor label in visualizer.ax.texts:\n    label.set_size(20)\nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Distribution Analysis for each Topic"},{"metadata":{},"cell_type":"markdown","source":"This is very important step in analyzing which set of words contribute to each topics. This gives us an idea about how different models would perform and helps us improve the previous steps. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordFrequency(reasonCode):\n    return (df[df[agentAssignedColumn]==str(reasonCode)][commentTextColumn].str.split(expand=True).stack().value_counts())\n\n\ndef wordFrequencyListPlot(reasonCode, plot = False):\n    wordFreqDict = df[df[agentAssignedColumn]==str(reasonCode)][commentTextColumn].str.split(expand=True).stack().value_counts().to_dict()\n    wordFreqDictMostCommon = dict(collections.Counter(wordFreqDict).most_common(10)) #Considering only Top 10 words\n    print(list(wordFreqDictMostCommon.keys()))\n    if plot == True:\n        plt.title(str(reasonCode), fontdict=None, loc='center')\n        plt.bar(wordFreqDictMostCommon.keys(), wordFreqDictMostCommon.values(), width = 0.1  , color='b');\n        plt.figure(figsize=(10,10))\n        plt.show();\n    return list(wordFreqDictMostCommon.keys())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for reasoncode in uniqueTopics:\n    print(reasoncode)\n    wordFrequencyListPlot(reasoncode, plot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generating WordCloud for each reason to have a visual representation of texts corresponding to each topic. We can also have an overall wordcloud."},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordCloudGenerator(df, reasonCode, save = False):\n    dfReasonCodeSubset = df[df[agentAssignedColumn]==reasonCode]\n    wordcloud = WordCloud(max_words=50,background_color='white',max_font_size = 50,width=100, height=100).generate(' '.join(dfReasonCodeSubset[commentTextColumn]))\n    plt.imshow(wordcloud)\n    plt.title(str(reasonCode), fontdict=None, loc='center')\n    plt.figure(figsize=(50,50))\n    plt.axis(\"off\")\n    plt.show();\n    if save:\n        plt.savefig('wordCloud'+str(reasonCode)+'.png', bbox_inches='tight')\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for topic in uniqueTopics:\n    wordCloudGenerator(df, topic) #,save = True , if you want to save the Word Clouds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature extractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(df[agentAssignedColumn].values)\nX_train, X_test, y_train, y_test = train_test_split(df[commentTextColumn].values, y,stratify=y,random_state=42, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tf-idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3),use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words = 'english')\n# Fit and transform Tf-idf to both training and test sets\ntfidf.fit(list(X_train) + list(X_test))\nX_train_tfidf =  tfidf.transform(X_train) \nX_test_tfidf = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"countvec = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), stop_words = 'english', binary=True)\n# Fit and transform CountVectorizer to both training and test sets\ncountvec.fit(list(X_train) + list(X_test))\nX_train_countvec =  countvec.transform(X_train) \nX_test_countvec = countvec.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Classification - Supervised Approach"},{"metadata":{},"cell_type":"markdown","source":"Now that we have cleaned text and its corresponding labels, we can start modelling step. \nFirst we use a `Supervised Learning` approach. The methods we use in this approach are:\n\n- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression): We use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier): Here also, we use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): We use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- [Random Forest Classifier](https://en.wikipedia.org/wiki/Random_forest) : Here also, we use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- [XGBoost Classifier](https://en.wikipedia.org/wiki/Gradient_boosting): We use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- [Support Vector Machines(SVC)](https://en.wikipedia.org/wiki/Support-vector_machine) : Here also, we use a pipeline approach of both [Count Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF Vectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n- Combination of [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) & Logistic Regression: For using this, you'll need to download the file `GoogleNews-vectors-negative300.bin.gz` from [this location](https://code.google.com/archive/p/word2vec/)\n- Combination of [Doc2Vec](https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html) & Logistic Regression: Refer [this link](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) for a better understanding.\n- Bag-Of-Words with Keras Sequential Model\n- [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)\n- Heuristic/Rule Based Approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Number of Words in the column \"+commentTextColumn+\" is \"+str(df[commentTextColumn].apply(lambda x: len(x.split(' '))).sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[commentTextColumn]\ny = df[agentAssignedColumn]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlogreg = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', LogisticRegression(n_jobs=1, C=1e5)),])\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nnb = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB(alpha=0.01)),])\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochastic Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nsgd = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-2, random_state=42, max_iter=5, tol=None)),])\nsgd.fit(X_train, y_train)\n\ny_pred = sgd.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', RandomForestClassifier(max_features='sqrt',n_estimators=1000, max_depth=3,random_state=0)),])\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', xgb.XGBClassifier(n_jobs=1,max_depth=3,learning_rate=0.01,n_estimators=1000)),])\nxgboost.fit(X_train, y_train)\ny_pred = xgboost.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machines(SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n\nsvc = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', SVC(gamma='scale', decision_function_shape='ovo'))])\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the misclassified"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating Logistic Regression, since it has comparatevely better accuracy(83%)\nlogregclf = LogisticRegression(n_jobs=1, C=0.5)\nlogregclf.fit(X_train_tfidf, y_train)\ny_pred = logregclf.predict(X_test_tfidf)\nprint(\"---Misclassified Examples---\")\nfor x, y, y_hat in zip(X_test, lbl_enc.inverse_transform(y_test), lbl_enc.inverse_transform(y_pred)):\n    if y != y_hat:\n        print(f'Cleaned Comment: {x} | Original Topic: {y} | Predicted Topic: {y_hat}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learning Curve of the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\nX, y = X_train_tfidf, y_train\ntitle = \"Learning Curves (Logistic Regression)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X, y, (0.5, 1.01), cv=cv, n_jobs=10)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining Word2Vec & Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nwv = gensim.models.KeyedVectors.load_word2vec_format(dataDirectory+\"/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\", binary=True)\nwv.init_sims(replace=True)\nlist(islice(wv.vocab, 13030, 13050))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The common way is to average the two word vectors. BOW based approaches which includes averaging.\ndef word_averaging(wv, words):\n    all_words, mean = set(), []\n    \n    for word in words:\n        if isinstance(word, np.ndarray):\n            mean.append(word)\n        elif word in wv.vocab:\n            mean.append(wv.syn0norm[wv.vocab[word].index])\n            all_words.add(wv.vocab[word].index)\n\n    if not mean:\n        logging.warning(\"cannot compute similarity with no input %s\", words)\n        # FIXME: remove these examples in pre-processing\n        return np.zeros(wv.vector_size,)\n\n    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n    return mean\n\ndef  word_averaging_list(wv, text_list):\n    return np.vstack([word_averaging(wv, post) for post in text_list ])\n\ndef w2v_tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text, language='english'):\n        for word in nltk.word_tokenize(sent, language='english'):\n            if len(word) < 2:\n                continue\n            tokens.append(word)\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.3, random_state = 42)\n\ntest_tokenized = test.apply(lambda r: w2v_tokenize_text(r[commentTextColumn]), axis=1).values\ntrain_tokenized = train.apply(lambda r: w2v_tokenize_text(r[commentTextColumn]), axis=1).values\n\nX_train_word_average = word_averaging_list(wv,train_tokenized)\nX_test_word_average = word_averaging_list(wv,test_tokenized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg = logreg.fit(X_train_word_average, train[agentAssignedColumn])\ny_pred = logreg.predict(X_test_word_average)\n\nprint('accuracy %s' % accuracy_score(y_pred, test[agentAssignedColumn]))\nprint(classification_report(test[agentAssignedColumn], y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining Doc2Vec & Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doc2vec, taking the linear combination of every term in the document creates a random walk with bias process in the word2vec space.\ndef label_sentences(corpus, label_type):\n    \"\"\"\n    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n    a dummy index of the post.\n    \"\"\"\n    labeled = []\n    for i, v in enumerate(corpus):\n        label = label_type + '_' + str(i)\n        labeled.append(TaggedDocument(v.split(), [label]))\n    return labeled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df[commentTextColumn], df[agentAssignedColumn], random_state=0, test_size=0.3)\nX_train = label_sentences(X_train, 'Train')\nX_test = label_sentences(X_test, 'Test')\n\nall_data = X_train + X_test\nall_data[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])\n\nfor epoch in range(30):\n    model_dbow.train([x for x in tqdm(all_data)], total_examples=len(all_data), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vectors(model, corpus_size, vectors_size, vectors_type):\n    \"\"\"\n    Get vectors from trained doc2vec model\n    :param doc2vec_model: Trained Doc2Vec model\n    :param corpus_size: Size of the data\n    :param vectors_size: Size of the embedding vectors\n    :param vectors_type: Training or Testing vectors\n    :return: list of vectors\n    \"\"\"\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in range(0, corpus_size):\n        prefix = vectors_type + '_' + str(i)\n        vectors[i] = model.docvecs[prefix]\n    return vectors\n\ntrain_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\ntest_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n\nlogreg = LogisticRegression(n_jobs=1, C=1e9)\nlogreg = logreg.fit(train_vectors_dbow, y_train)\ny_pred = logreg.predict(test_vectors_dbow)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag-Of-Words with Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(len(df) * .7)\nprint (\"Train size: %d\" % train_size)\nprint (\"Test size: %d\" % (len(df) - train_size))\n\ntrain_comments = df[commentTextColumn][:train_size]\ntrain_topics = df[agentAssignedColumn][:train_size]\n\ntest_comments = df[commentTextColumn][train_size:]\ntest_topics = df[agentAssignedColumn][train_size:]\n\nmax_words = 500\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntokenize.fit_on_texts(train_comments) # only fit on train\nx_train = tokenize.texts_to_matrix(train_comments)\nx_test = tokenize.texts_to_matrix(test_comments)\n\nencoder = LabelEncoder()\nencoder.fit(train_topics)\ny_train = encoder.transform(train_topics)\ny_test = encoder.transform(test_topics)\n\nnum_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\nepochs = 10\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n\nscore = model.evaluate(x_test, y_test,batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 500\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 15\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df[commentTextColumn].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = tokenizer.texts_to_sequences(df[commentTextColumn].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nY = pd.get_dummies(df[agentAssignedColumn]).values\nprint('Shape of label tensor:', Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nbatch_size = 5\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n\naccr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='test')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test with an unseen comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_complaint = ['Avinash want to cancel service.']\nseq = tokenizer.texts_to_sequences(new_complaint)\npadded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\npred = model.predict(padded)\nlabels = uniqueTopics\nprint(pred, labels[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heuristic/Rule Based Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeFrequentOccuringWords(wordList):\n    commonWords = ['customer', 'want', 'plan', 'month', 'new', 'inquire', 'check']\n    wordList = [word for word in wordList if word not in commonWords]\n    return ','.join(wordList)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labelsKeywords = pd.DataFrame(columns = ['Label', 'Fuzzy Words', 'Strict Words'])\n\nlabelColumnList = list(df[agentAssignedColumn].value_counts().to_dict())\nstrictWordList = ['']\nfuzzyWordList = ['']\nfor reasoncode in labelColumnList:\n    fuzzyWordList.append(','.join(removeFrequentOccuringWords(wordFrequencyListPlot(reasoncode)).split(',')[-2:]))\n    strictWordList.append(','.join(removeFrequentOccuringWords(wordFrequencyListPlot(reasoncode)).split(',')[:6]))\n\nlabelsKeywords['Label'] = ['Others'] + labelColumnList\nlabelsKeywords['Fuzzy Words'] = fuzzyWordList\nlabelsKeywords['Strict Words'] = strictWordList\nlabelsKeywords.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_label(TrainingWords, StrictWords, text_in):   \n    clnTxt = [st.stem(u.lower()) for u in  text_in.split()]        \n    if(len(clnTxt)==0):\n        clnTxt =  'Others'     \n    NL = len(TrainingWords)\n    probV = [0.0 for k in range(NL)]\n    CM_A = []\n    for j in range(NL):\n        TrSet = set([ st.stem(u.lower())  for u in TrainingWords[j].split()])\n        SWords = set([ u.lower().strip() for u in StrictWords[j].split(\",\")])  if len(StrictWords[j]) else []\n        matching =  [s for s in SWords  if ((s in text_in) and len(set(s.split()).intersection(text_in.split()))==len(s.split())) ] \n        cm = list(TrSet.intersection(clnTxt))\n        if len(matching)>0:\n            probV[j] = 1\n            CM_A.append(matching)\n            break\n        else:                \n            if len(cm):\n                CM_A.append(cm)\n            else:\n                CM_A.append([])\n            probV[j] = (len(cm)/float(len(clnTxt)))\n    return({'idx':np.argmax(probV), 'maxV':np.max(probV), 'wa':CM_A[np.argmax(probV)]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nTestData = df.copy()\ntest_buffer = TestData.iloc[:,1]\n\nTrainingData = labelsKeywords.copy()\n\nfuzzy_word_list = TrainingData['Fuzzy Words']\nstrict_word_list = TrainingData['Strict Words']\ntrain_buffer = [fuzzy_word_list[i]+\" \"+strict_word_list[i] for i in range(len(TrainingData))]\n\nLabels = [a for a in TrainingData['Label']]\n\nTestData['New Topic'] = 0.0\nTestData['Conf'] = 0.0\nTestData['Matching words'] = [\"\" for i in range(len(TestData))]\n\nfor i in range(len(test_buffer)):\n    BL = find_best_label(train_buffer, TrainingData['Strict Words'], test_buffer[i])\n    TestData['New Topic'][i] = Labels[BL['idx']]\n    TestData['Conf'][i] = BL['maxV']\n    TestData['Matching words'][i] = \", \".join(BL['wa'])    \n    \nTestData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"originalTopic = TestData[agentAssignedColumn]\nnewTopic = TestData['New Topic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Classification - Unsupervised Approach"},{"metadata":{},"cell_type":"markdown","source":"We also try an `Unsupervised Learning` approach. [Latent Dirichlet allocation(LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is used to get the topic clusters. We use two types of Word Corpuses:\n- Bag of Words\n- TF-IDF\n\n\nIn trying to incorporate the context of sentences at times, we also try two word gram models:\n- Unigram Word distribution\n- Bigram Word distribution"},{"metadata":{},"cell_type":"markdown","source":"## Topic Modelling using LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gensim Preprocessing\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result\n\nprocessed_docs = df[commentTextColumn].map(preprocess)\nprocessed_docs[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LDA Using Bag-of-Words"},{"metadata":{},"cell_type":"markdown","source":"- Unigram Bag of Words "},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n#Modelling Step\nNUM_TOPICS = len(uniqueTopics) #10 here\nprint(\"\\n Number of topics = \"+str(NUM_TOPICS)+\" \\n\")\nsizeDf=str(len(df))\n\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes()\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\nlda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary, workers = 2, passes = 5, iterations = 100, eval_every=5)\n\n# # Saving the Model\n# modelFileName = 'bowGensimModel'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_bow.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlda_display = gensimvis.prepare(lda_model_bow, bow_corpus, dictionary, sort_topics = False)\n\n# #If you want to save the visualization\n# pyLDAvis.save_html(lda_display, 'bowGensimModel'+str(NUM_TOPICS)+'.html')\n\npyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n# Based on the Dirichlet Equations, these could be a possible Topic Assignments\nTopicList= {0:'Roaming Plans',\n1:'Others',\n2:'Add New Line',\n3:'Cancel Service',\n4:'Data Usage',\n5:'Poor Connectivity',\n6:'High Bill',\n7:'Deactivate',\n8:'Change Plans',\n9:'Port Out'}\n\n\ndf['topicNumDistributionColumn'] = lda_model_bow.get_document_topics(bow_corpus, minimum_probability=0.1)\n\ntopiclist = df['topicNumDistributionColumn'].tolist()\nnewTopic = []\nfor element in topiclist: \n    newTopic.append(str(sorted(element, key=lambda x: -x[1])[-1:]))\n    \ndf['NewTopic'] = newTopic\n\n\n\ntopicExpansionInNumberPrimary = []\nfor item in df['NewTopic']:\n    itemToList = re.findall(r'\\d+', item)\n    if len(itemToList)==3:\n        topicExpansionInNumberPrimary.append(TopicList[int(itemToList[0])])\n    elif len(itemToList)==0:\n        topicExpansionInNumberPrimary.append(None)\n    else:\n        topicExpansionInNumberPrimary.append(None)\ndf['NewTopic'] = topicExpansionInNumberPrimary\n\noriginalTopic = df[agentAssignedColumn]\nnewTopic = df['NewTopic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Bi gram Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_docs = df[commentTextColumn].to_list()\ntoken_ = [doc.split(\" \") for doc in processed_docs]\nbigram = Phrases(token_, min_count=1, threshold=2,delimiter=b' ')\n\n\nbigram_phraser = Phraser(bigram)\n\nbigram_token = []\nfor sent in token_:\n    bigram_token.append(bigram_phraser[sent])\n\n#now you can make dictionary of bigram token \ndictBigram = gensim.corpora.Dictionary(bigram_token)\n# dictBigram.filter_extremes(no_above=0.5, keep_n=100000)\n\n#Convert the word into vector, and now you can use from gensim \nbow_corpus_bigram = [dictBigram.doc2bow(text) for text in bigram_token]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_bow_bigram = gensim.models.LdaMulticore(bow_corpus_bigram, num_topics=NUM_TOPICS, id2word=dictBigram, workers = 2, passes = 5, iterations = 100, eval_every=5)\n\n# # Saving the Model\n# modelFileName = 'bowGensimModelBigram'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow_bigram.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_bow_bigram.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LDA Using TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"- Unigram TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n#Modelling Step\nNUM_TOPICS = len(uniqueTopics) #10 here\nprint(\"\\n Number of topics = \"+str(NUM_TOPICS)+\" \\n\")\nsizeDf=str(len(df))\n\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes()\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=NUM_TOPICS, id2word=dictionary, passes=5, workers=2, iterations = 100, eval_every=5)\n# # Saving the Model\n# modelFileName = 'tfidfGensimModel'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlda_display = gensimvis.prepare(lda_model_tfidf, corpus_tfidf, dictionary, sort_topics = False)\n\n# #If you want to save the visualization\n# pyLDAvis.save_html(lda_display, 'tfidfGensimModel'+str(NUM_TOPICS)+'.html')\n\npyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n# Based on the Dirichlet Equations, these could be a possible Topic Assignments\nTopicList= {0:'High Bill',\n1:'Roaming Plans',\n2:'Others',\n3:'Change Plans',\n4:'Cancel Service',\n5:'Poor Connectivity',\n6:'Add New Line',\n7:'Port Out',\n8:'Data Usage',\n9:'Deactivate'}\n\n\ndf['topicNumDistributionColumn'] = lda_model_tfidf.get_document_topics(corpus_tfidf, minimum_probability=0.1)\n\ntopiclist = df['topicNumDistributionColumn'].tolist()\nnewTopic = []\nfor element in topiclist: \n    newTopic.append(str(sorted(element, key=lambda x: -x[1])[-1:]))\n    \ndf['NewTopic'] = newTopic\n\n\n\ntopicExpansionInNumberPrimary = []\nfor item in df['NewTopic']:\n    itemToList = re.findall(r'\\d+', item)\n    if len(itemToList)==3:\n        topicExpansionInNumberPrimary.append(TopicList[int(itemToList[0])])\n    elif len(itemToList)==0:\n        topicExpansionInNumberPrimary.append(None)\n    else:\n        topicExpansionInNumberPrimary.append(None)\ndf['NewTopic'] = topicExpansionInNumberPrimary\n\noriginalTopic = df[agentAssignedColumn]\nnewTopic = df['NewTopic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Bigram TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_docs = df[commentTextColumn].to_list()\ntoken_ = [doc.split(\" \") for doc in processed_docs]\nbigram = Phrases(token_, min_count=1, threshold=2,delimiter=b' ')\n\n\nbigram_phraser = Phraser(bigram)\n\nbigram_token = []\nfor sent in token_:\n    bigram_token.append(bigram_phraser[sent])\n\n#now you can make dictionary of bigram token \ndictBigram = gensim.corpora.Dictionary(bigram_token)\n# dictBigram.filter_extremes(no_above=0.5, keep_n=100000)\n\n#Convert the word into vector, and now you can use from gensim \ncorpus_bigram = [dictBigram.doc2bow(text) for text in bigram_token]\n\ntfidf_model_bigram = models.TfidfModel(corpus_bigram)\ncorpus_tfidf_bigram = tfidf_model_bigram[corpus_bigram]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_tfidf_bigram = gensim.models.LdaMulticore(corpus_tfidf_bigram, num_topics=NUM_TOPICS, id2word=dictBigram, passes=5, workers=2, iterations = 100, eval_every=5)\n# # Saving the Model\n# modelFileName = 'tfidfGensimModelBigram'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_tfidf_bigram.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some Useful References:"},{"metadata":{},"cell_type":"markdown","source":"- [Analytics Vidya](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/) for Preprocessing\n- [Blog post](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) by Susan Li.\n- [Analytics Vidya](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/) for Topic Modelling.\n- [Machine Learning Plus](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}