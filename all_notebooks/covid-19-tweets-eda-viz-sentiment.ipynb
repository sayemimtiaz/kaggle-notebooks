{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This notebook contains EDA,visualization and sentiment analysis on Covid-19 tweets"},{"metadata":{},"cell_type":"markdown","source":"Downloaded tweets with the hashtag #covid19\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load all the required libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy             as np\nimport pandas            as pd\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport plotly.graph_objs as go\nimport plotly.express    as px \nimport nltk\nimport re\nimport string\n\nfrom scipy.stats import norm\nfrom wordcloud   import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = set(STOPWORDS)\n\nfrom textblob import TextBlob\nimport re\nfrom collections import Counter\n\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom IPython.display import Markdown as md\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/covid19-tweets/covid19_tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look at the dataset we have :**\n\nuser_name - contains the user name of that person.\n\nuser_location - contains the user's loaction from where he/she has tweeted.\n\nuser_description - It contains the user description on Tweeter\n\nuser_created - It contains the user id created time and date.\n\nuser_followers - It conatins the followers of users\n\nuser_friends - It contains the user's friends on Tweeter\n\nuser_favourites - It conatins user's favourites on Tweeter.\n\nuser_verified - User is verified or not ( True / False )\n\ndate - Date of Tweet\n\ntext - Text of Tweet he/she has tweeted.\n\nhashtags - how many hashtags his/her tweet have.\n\nsource - It contains the source of that.\n\nis_retweet - Any retweets it have or not ( True / False )."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dtypes of features\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Description of the dataset\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of rows and columns in the dataset\nprint(\"There are {} rows and {} columns in the dataset.\".format(data.shape[0],data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{},"cell_type":"markdown","source":"### 1. Word cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \",\".join(review for review in data.text if 'COVID' not in review and 'https' not in review and 'Covid' not in review)\nwordcloud = WordCloud(max_words=200, colormap='Set3',background_color=\"black\").generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.title('Prevalent words in tweets',fontsize=19)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Number of Tweets by location(Top 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nsns.barplot(data[\"user_location\"].value_counts().values[0:10],\n            data[\"user_location\"].value_counts().index[0:10]);\nplt.title(\"Top 10 Countries with maximum tweets\",fontsize=14)\nplt.xlabel(\"Number of tweets\",fontsize=14)\nplt.ylabel(\"Country\",fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Heatmap representation of missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17, 5))\nsns.heatmap(data.isnull(), cbar=True, cmap='Paired_r')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Bar plot of unique values in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_values_funct(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['Features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['Uniques'] = uniques\n    return unique_dataframe\n\nudf = unique_values_funct(data)\n\nf, ax = plt.subplots(1,1, figsize=(10,5))\nsns.barplot(x=udf['Features'], y=udf['Uniques'], alpha=0.8)\nplt.title('Bar plot for unique values in each column', fontsize=14)\nplt.ylabel('Unique values', fontsize=14)\nplt.xlabel('Columns', fontsize=14)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Distribution of words in text column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"num of words in text\"] = data[\"text\"].apply(lambda x: len(x))\nplt.figure(figsize=(10,7))\nsns.kdeplot(data[\"num of words in text\"])\nplt.title(\"Distribution of words in text column\")\nplt.xlabel(\"Number of words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Users with maximum tweets(Top 20)"},{"metadata":{"trusted":true},"cell_type":"code","source":"username_count = data['user_name'].value_counts().reset_index().rename(columns={\n    'user_name':'tweet_count','index':'user_name'})\n\nplt.figure(figsize=(8, 10))\nsns.barplot(y='user_name',x='tweet_count',data=username_count.head(20))\ny=username_count['tweet_count'].head(20)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Users with maximum tweets',weight='bold', size=13)\nplt.ylabel('Username', size=12, weight='bold')\nplt.xlabel('TweetCount', size=12, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Plot verified users account"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(x =\"user_verified\",data=data, palette=\"Set2\")\nplt.title(\"Verified user accounts or not ?\")\nplt.xticks([False,True],['Unverified','Verified'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Plot platform with maximum number of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsrc = data['source'].value_counts().sort_values(ascending=False)\nsource = src.head(10)\nsource.plot.bar(color=['red', 'green', 'blue', 'black','cyan','pink','purple','violet','yellow','orange'])\nplt.title('Platform with maximum number of tweets',size=13)\nplt.xlabel('User Platform',size=13)\nplt.ylabel('Tweet Count',size=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.1> 5 Most Tweet Sources used in India"},{"metadata":{"trusted":true},"cell_type":"code","source":"pla = data['source'][data['user_location'] == 'India'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in India', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.2> 5 Most Tweet Sources used in USA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pla = data['source'][data['user_location'] == 'United States'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in USA', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.3> 5 Most Tweet Sources used in Switzerland"},{"metadata":{"trusted":true},"cell_type":"code","source":"pla = data['source'][data['user_location'] == 'Switzerland'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in Switzerland', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.4> 5 Most Tweet Sources used in Australia"},{"metadata":{"trusted":true},"cell_type":"code","source":"pla = data['source'][data['user_location'] == 'Australia'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in Australia', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.5> 5 Most Tweet Sources used in United Kingdom"},{"metadata":{"trusted":true},"cell_type":"code","source":"pla = data['source'][data['user_location'] == 'United Kingdom'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in United Kingdom', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Plot top 5 hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_tags=data['hashtags'].value_counts().sort_values(ascending=False)\nplt.figure(figsize=(8,8))\nexplode = (0, 0.1, 0, 0,0.01) \n\ntop_tags[0:5].plot(kind = 'pie',title = 'Top 5 hashtags',autopct='%1.1f%%',shadow=True,explode = explode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10. Day with most number of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"date\"] = pd.to_datetime(data[\"date\"])\ndata[\"Month\"] = data[\"date\"].apply(lambda x : x.month)\ndata[\"day\"] = data[\"date\"].apply(lambda x : x.dayofweek)\ndmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\ndata[\"day\"] = data[\"day\"].map(dmap)\nplt.title(\"Day with maximun tweets\")\nsns.countplot(data[\"day\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment Analysis on Covid19 Tweets"},{"metadata":{},"cell_type":"markdown","source":"To peform sentiment analysis we need labeled dataset. The data can be downloaded from here: https://www.kaggle.com/surajkum1198/twitterdata"},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df = pd.read_csv('/kaggle/input/twitterdata/finalSentimentdata2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df['sentiment'].nunique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nsns.heatmap(senti_df.isnull(), cbar=True, cmap='Paired_r')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we dont have any missing/NaN value in our dataset."},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n"},{"metadata":{},"cell_type":"markdown","source":"Now let us preprocess text using some NLP tchniques like:\n\n1. converting to lowercase\n2. remove text in square brackets,\n3. remove links,\n4. remove punctuation\n5. remove words containing numbers\n6. Removing Punctuation\n7. Removing stopwords\n8. Stemming\n9. Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\npunc=string.punctuation\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    \n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    \n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    #Removing stopwords\n    text=\" \".join([word for word in str(text).split() if word not in stop_words])\n    \n    #Stemming\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    \n    #Lemmatization\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\nsenti_df['text'] = senti_df['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing emojis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nsenti_df['text']=senti_df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correct spelling of incorrect words"},{"metadata":{},"cell_type":"markdown","source":"If you are using high configuration machine then only perform this step of correcting incorrect words."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n!pip install textblob\n\nfrom textblob import TextBlob\n\ndef correct_bytextblob(sent):\n    return str(TextBlob(sent).correct())\n\nsenti_df['text'] = senti_df['text'].apply(lambda x: correct_bytextblob(x))\n\nsenti_df.to_csv('clean_tweets.csv',index=False)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(senti_df,test_size = 0.2,random_state=0,stratify = senti_df.sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing"},{"metadata":{},"cell_type":"markdown","source":"I've checked vectorizing using TF-IDF technique but its not performing well so going ahead with CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.text.values)\nX_valid = vectorizer.transform(valid.text.values)\n\ny_train = train.sentiment.values\ny_valid = valid.sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the classes are almost balanced, we dont need to perform any sampling technique."},{"metadata":{},"cell_type":"markdown","source":"## ML model building"},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the model test accuracy in descending order"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost'],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy,]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}