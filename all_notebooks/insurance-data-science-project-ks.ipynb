{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction & motivations\n\n* Background & context\n> Traditionally, pricing actuaries have utilised statistical/mathematical methods based on well-founded theories in order to predict the severity and likelihood of claims on policies that have  been underwritten; these predictions would then iteratively feed into the underwriting/actuarial/claims feedback cycle that underpins insurance as we know it today. However, in recent years actuarial methods have evolved to incorporate more modern/state-of-the-art techniques through the application of data science (DS) and machine learning (ML), in order to develop a much more sophisticated approach to reserving and pricing across a variety of lines of business.\n>\n> Modern-day actuarial pricing is typically performed via the industry-standard use of generalised linear models (GLMs) in order to produce a predictive mapping between the risk factors of each policyholder and their predicted loss cost - this can be achieved via the categorical encoding of these risk factors in order to produce predictions of both claim severities and frequencies, which can be easily translated into interpretable insurance tariff plans.\n>\n> More information on implementing GLMs for insurance pricing (predicting the likelihood/severity of insurance claims) can be found at the following websites: [Poisson regression and non-normal loss](https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html#) and [Tweedie regression on insurance claims](https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html).\n>\n> However, within the last decade a number of ML methods (e.g. decision trees) have also shown great promise and versatility in this area, approaching the common actuarial challenges of predicting claim likelihoods/severities (and many others) with both computational efficiency and accuracy. For more information, see here: [Boosting insights in insurance tariff plans with tree-based machine learning](https://www.researchgate.net/publication/332631030_Boosting_insights_in_insurance_tariff_plans_with_tree-based_machine_learning).\n\n* What is this project about?\n> The aim of this project is to provide a practical overview of the general DS/ML workflow, which is becoming an increasingly popular framework upon which modern-day actuarial pricing methods are being built. Accurate pricing is currently one of the most crucial challenges that businesses across the insurance industry are facing, where providing accurate estimates of loss costs is vital to ensuring prudent insurance portfolio management and maintaining successful financial performance, especially as the sector is currently experiencing a swelling volume of claims - for example, regarding business interruption - as one of the major consequences of COVID-19.\n>\n> As a case study, we will introduce some supervised ML techniques for predicting claim severities on a French motor third-party liability (MTPL) insurance dataset. Whilst this case study may not necessarily be entirely relevant to the most pressing of present-day actuarial pricing challenges, the aim is for this project to serve as an advocate for adopting the wider use of advanced analytical/computational approaches within the insurance industry, such that more effort can be devoted to preparing it for the future ahead.\n\n\n* What will be discussed/shown in the project?\n> In this project, we will consider how to pre-process and encode an MTPL insurance dataset, how to select important risk features from the dataset, and how to train/test a range of ML regressors to predict claim severities based on these selected risk features."},{"metadata":{},"cell_type":"markdown","source":"# Code initialisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import key modules that will be used throughout the project.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # graphs/plotting\nimport seaborn as sns\n\n# Check to ensure that both CSV files are held in the correct (input) directory.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Import the datasets into dataframes, and perform a merge to join them together"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load the CSV data files into Pandas dataframes.\n\nMTPL_filepath = \"/kaggle/input/fremtpl-french-motor-tpl-insurance-claims/\"\n\nprint(\"Now loading MTPLfreq.\")\nMTPLfreq = pd.read_csv(MTPL_filepath+\"freMTPLfreq.csv\")\nprint(\"MTPLfreq was loaded.\\n\")\n\nprint(\"Now loading MTPLsev.\")\nMTPLsev = pd.read_csv(MTPL_filepath+\"freMTPLsev.csv\")\nprint(\"MTPLsev was loaded.\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for total amount of claims paid in original DataFrame, prior to merging MTPLfreq with MTPLsev.\n\nprint(sum(MTPLsev['ClaimAmount']))\n\n# Aggregate the claim amounts by PolicyID, prior to merging MTPLfreq with MTPLsev.\n\nMTPLsev_grp = MTPLsev.groupby(['PolicyID'])[['ClaimAmount']].agg('sum').reset_index()\n\n# Perform an outer merge between MTPLfreq/MTPLsev, based on PolicyID, then reset the index back to PolicyID (this is dropped during merging).\n\ndf_merged = pd.merge(MTPLfreq, MTPLsev_grp, how='outer', on='PolicyID').fillna(0).set_index('PolicyID')\n\n# Check for the total amount of claims paid in new DataFrame, after merging MTPLfreq with MTPLsev.\n\nprint(sum(df_merged['ClaimAmount']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Review and understand the meaning of the data (columns), then assign the features/targets to their own dataframes"},{"metadata":{},"cell_type":"markdown","source":"**Understanding the columns and their datatypes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_merged.columns)\nprint('\\n')\nprint(df_merged.dtypes)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the code above, we can see that we have a variety of datatypes within our dataframe - the columns with `object` dtype contain non-numerical (character) data, which will need to be pre-processed in order for these to be machine-interpretable.\n\nThis will be explained in further detail later on."},{"metadata":{},"cell_type":"markdown","source":"**First 5 rows of the MTPL dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_merged.head())\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the code above, we can see a list of the first few rows within the merged dataframe - here, the PolicyID is set as the index, and each row lists out the values of each column (risk feature) for the first 5 policyholders within the dataset.\n\nMore information on the nature/meaning of these rows/columns can be found at the following pages: [freMTPL - French Motor TPL Insurance Claims Data](https://www.kaggle.com/karansarpal/fremtpl-french-motor-tpl-insurance-claims)."},{"metadata":{},"cell_type":"markdown","source":"**How many policyholders have made zero claims? How will this affect the choice of model that we use to estimate claim severity?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"policies_no_claims = len(df_merged.loc[df_merged['ClaimNb'] == 0].index)\nall_policies = len(df_merged.index)\n\npct_pols_no_clm = round((policies_no_claims/all_policies)*100, 2) \n\nprint(str(pct_pols_no_clm)+\"% of policyholders have not made any claims.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a vast majority of the policyholders have not made any claims whatsoever, this implies that `ClaimAmount` will have a distribution that peaks at zero, but also features right-skewness/tailing to account for positive (total) claim amounts with exponentially decaying probability.\n\nHence, the linear regression model that we use/choose will need to be able to sufficiently account for both of these characteristics of the distribution of `ClaimAmount`."},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Generate additional features based on interactions/transformations of existing variables"},{"metadata":{},"cell_type":"markdown","source":"Here, we create two new variables based on existing features - these are `ClaimFreq` and `ClaimSev`, which represent the frequency and severity of a policyholder's claim/s, respectively, in units of policy exposure. However, one of these features is not appropriate for use in model training - we will discuss this further in Step 4 below."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged['ClaimFreq'] = df_merged['ClaimNb'] / df_merged['Exposure']\n\ndf_merged['ClaimSev'] = df_merged['ClaimAmount'] / df_merged['Exposure']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Review and consider any sources of data leakage\n\nNext, we will consider where data leakage may be likely to creep into the model fitting process, by reviewing the features/columns to confirm their meanings and evaluate whether any of them can introduce bias into the model training process. In this project, the following two factors are considered to be relevant:\n\n* Target leakage\n> This can occur when the predictors/features include or refer to data/information that will not be available at the time of making predictions. In this project, any features that are derived from the target that we wish to predict (e.g. `ClaimSev`) are not suitable - this is because `ClaimSev` itself depends on `ClaimAmount`, which we will not know at the time of prediction.\n>\n> Therefore, in order to avoid target leakage, we will drop the `ClaimSev` column when assigning the features to their own dataframe in Step 5.\n>\n> On a separate note, while using the `ClaimNb`/`ClaimFreq` column/s could also be considered as leaking data into the model training process (as we will not know how many claims a policyholder will make), it is worth mentioning that these can be and are, in practice, provided as predictions instead (i.e. how many claims a policyholder is _expected_ to make) in order to later determine the pure risk premium for each policyholder/insured. In this project, for the sake of simplicity we assume that the `ClaimNb`/`ClaimFreq` values in the test dataset are predictions that have been generated in a prior exercise - we are aiming to predict the total loss for each policyholder, rather than the loss amount per individual claim.\n\n* Train-test contamination\n> This can occur when data that are used to train the model/s are subsequently used to make predictions, which will lead to the introduction of bias in the model evaluation process; once trained, the model will appear to perform extremely well against the (in-sample) test dataset, but will be worse at generalising to unseen/out-of-sample data.\n>\n> Hence, using `train_test_split()` to split our dataset into training/test samples is vital for avoiding train-test contamination of the model training process."},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Perform a train-test split of the dataset"},{"metadata":{},"cell_type":"markdown","source":"Next - we will create separate DataFrames that will store the features and target variables. These are then supplied to the `sklearn` function `train_test_split()` in order to split the data into training/test subsets, for the reason outlined above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign the target variable to its own dataframe.\ny_full = df_merged.ClaimAmount\n\n# Assign the features to their own dataframe. Also, remove ClaimSev, to prevent data leakage when predicting ClaimAmount.\nX_full = df_merged.drop(['ClaimAmount', 'ClaimSev'], axis=1)\n\nprint(y_full.head())\n\nprint(X_full.head())\n\n# Perform a train-test split to obtain the training and test data as separate dataframes.\nfrom sklearn.model_selection import train_test_split\n\n# We will set the size of the X/y training datasets to be 80% of the original (full) X/y datasets, via the train_size/test_size parameters\nX_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Encode categorical variables as numeric inputs for use in ML modelling"},{"metadata":{},"cell_type":"markdown","source":"**Label Encoding**\n> Here, we label-encode the **Power** column such that it changes each (ordinal) text-based label to a numerical value which is machine-interpretable, for later use in feature scaling as well as model fitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Make a copy of the training/validation feature subsets to avoid changing any original data.\ncopy_X_train = X_train.copy()\ncopy_X_valid = X_valid.copy()\n\n# Apply a label encoder to the 'Power' column (i.e. encoding of ordinal variable).\nlabel_encoder = LabelEncoder()\n\ncopy_X_train['Power'] = label_encoder.fit_transform(X_train['Power'])\ncopy_X_valid['Power'] = label_encoder.transform(X_valid['Power'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-Hot Encoding**\n> We also one-hot encode the **Brand**, **Gas** and **Region** columns, such that these categories are converted to numerical and machine-interpretable values that can be supplied to each regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Initialise a one-hot encoder to columns that contain categorical data.\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = ['Brand', 'Gas', 'Region']\n\n## We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented\n## in the training data, and setting sparse=False ensures that the encoded columns are returned as a numpy array\n## (instead of a sparse matrix).\n\n# Use the one-hot encoder to transform the categorical data columns. \nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(copy_X_train[OH_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(copy_X_valid[OH_cols]))\n\n# One-hot encoding removes the index; re-assign the original index.\nOH_cols_train.index = copy_X_train.index\nOH_cols_valid.index = copy_X_valid.index\n\n# Add column-labelling back in, using the get_feature_names() function. \nOH_cols_train.columns = OH_encoder.get_feature_names(OH_cols)\nOH_cols_valid.columns = OH_encoder.get_feature_names(OH_cols)\n\n# Create copies that only include numerical feature columns (these will be replaced with one-hot encoded versions).\ncopy_X_train_no_OH_cols = copy_X_train.drop(OH_cols, axis=1)\ncopy_X_valid_no_OH_cols = copy_X_valid.drop(OH_cols, axis=1)\n\n# Concatenate the one-hot encoded columns with the existing numerical feature columns.\nX_train_enc = pd.concat([copy_X_train_no_OH_cols, OH_cols_train], axis=1)\nX_valid_enc = pd.concat([copy_X_valid_no_OH_cols, OH_cols_valid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data scaling - normalisation**\n> Next, we perform min-max scaling on the encoded dataset, such that all features lie between 0 and 1 - this is so that, when training any of the regression models, all features will have variances with the same order of magnitude as each other. Thus, no single feature will dominate the objective function and prohibit the model from learning from other features correctly as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Initialise the MinMaxScaler model, then fit it to the (encoded) training feature dataset.\nMM_scaler = MinMaxScaler()\nMM_scaler.fit(X_train_enc)\n\n# Fit the scaler, then normalise/transform both the training and validation feature datasets.\nX_train_scale = pd.DataFrame(MM_scaler.transform(X_train_enc),\n                             index=X_train_enc.index,\n                             columns=X_train_enc.columns)\n\nX_valid_scale = pd.DataFrame(MM_scaler.transform(X_valid_enc),\n                             index=X_valid_enc.index,\n                             columns=X_valid_enc.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparison of X_train_scale and X_valid_scale**\n> Here, we check to ensure that all feature values are now numerically encoded and are between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify minimum value of all features in X_train_scale:\n\nX_train_scale.min(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify maximum value of all features in X_train_scale:\n\nX_train_scale.max(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify minimum value of all features:\n\nX_valid_scale.min(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify maximum value of all features:\n\nX_valid_scale.max(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 7: Explore the original dataset to obtain descriptive statistics"},{"metadata":{},"cell_type":"markdown","source":"Here, we use the `pd.describe()` function to obtain descriptive statistics of the original dataset (prior to preprocessing)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_merged.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will generate pairplots between the targets and features to understand the relationships between them and discover whether there are any trends/correlations within the data. To do this, we will use the `seaborn.pairplot()` function as a high-level interface to plot the pairwise relationships in the `df_merged` dataset.\n\nFirst, we define two separate lists of x-variables that we will produce pairplots with, depending on which y-variable we choose."},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_pairplot_x_vars_A = ['ClaimNb', 'Power', 'CarAge', 'DriverAge', 'Brand', 'Gas', 'Density']\n\ndesc_pairplot_x_vars_B = ['Power', 'CarAge', 'DriverAge', 'Brand', 'Gas', 'Density']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we supply 3 values to this function's parameters:\n\n* `data` - positional argument:\n> Here, we supply `df_merged` as a `pandas.DataFrame` object, where each column is a variable and each row is an observation.\n\n* `x_vars` - keyword argument:\n> Here, we provide the lists of variable names (defined above) as the columns/x-axes of the pairplot figures in order to make the non-square plots.\n\n* `y_vars` - keyword argument:\n> Here, we provide a single variable name (e.g. `ClaimNb`) as the row/y-axis for each pairplot figure."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot 1 - Exposure vs. x_vars.\n\ndesc_pairplot_1 = sns.pairplot(df_merged, x_vars=desc_pairplot_x_vars_A, y_vars='Exposure')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ClaimNb: The trend in this graph implies that policyholders with higher claim frequencies tend to be covered on policies with shorter exposure periods (this view may, however, be affected by renewals/binders).\n* Power: The trend in this graph implies that cars covered by policies with longer exposure periods tend to be less powerful, although there appear to be some high-powered cars (e.g. categories `k`/`n`) that go against this pattern.\n* CarAge: The trend in this graph implies that a vast majority of the cars insured have 1-year exposure periods, with a wider spread of policy exposure periods in the range of years lower than 25 (i.e. relatively young cars).\n* DriverAge: This graph shows a wide distribution of exposure periods across drivers of different ages, with a range of middle-aged policyholders that are insured with exposure periods longer than 1 year.\n* Brand: This graph does not show any clear correlation or trend between the exposure period of the MTPL policy and the brand of the car that is insured.\n* Gas: This graph does not show any clear correlation or trend between the exposure period of the MTPL policy and the fuel type of the car that is insured.\n* Density: This graph shows a wide distribution of policy exposure periods, which appears to decrease as the population density of the area that the policyholder inhabits increases; however, this distribution does feature some observations at higher values of Density."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot 2 - ClaimNb vs. x_vars.\n\ndesc_pairplot_2 = sns.pairplot(df_merged, x_vars=desc_pairplot_x_vars_B, y_vars='ClaimNb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Power: The trend in this graph implies that policyholders who are in the upper quartile of claim frequency (i.e. N > 3) tend to drive less powerful/average cars, however there does not appear to be any general correlation between car power and claim frequency, among the majority of other policyholders.\n* CarAge: The trend in this graph implies that a majority of the variance in ClaimNb is shown across cars that are younger than 50; the number of claims per policyholder tends to decrease with the age of the car.\n* DriverAge: This graph shows a wide distribution of claim frequencies across drivers of different ages, with no clear trend between the age of the policyholder and the number of claims that they have made.\n* Brand: This graph does not show any clear correlation or trend between the number of claims made and the brand of the car owned by the (insured) policyholder. \n* Gas: This graph also does not show any clear correlation between the number of claims made and the fuel type of the car that is insured.\n* Density: This graph shows the general trend that the number of claims per policyholder decreases as the population density (of the town/city that they live in) increases."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot 3 - ClaimFreq vs. x_vars (i.e. accounting for policy exposure weighting).\n\ndesc_pairplot_3 = sns.pairplot(df_merged, x_vars=desc_pairplot_x_vars_B, y_vars='ClaimFreq')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Power: The trend in this graph implies that ClaimFreq tends to decrease as the power of the car increases, however there are also some small peaks in ClaimFreq for certain categories representing high-powered cars (e.g. k/l).\n* CarAge: This graph shows that a vast majority of the variance in ClaimFreq is shared across policyholders who own relatively young cars (CarAge < 25), but also implies that ClaimFreq decreases as the age of the car increases.\n* DriverAge: This graph shows that claim frequency also decreases as the age of the policyholder increases, however a vast majority of the variance in ClaimFreq can be captured between the values of 0-100 (across all driver ages).\n* Brand: There appears to be no clear trend or correlation between the frequency of claims and the brand of the car owned by the policyholder.\n* Gas: This graph also does not show any clear correlation between ClaimFreq and the fuel type of the policyholder's car.\n* Density: This graph displays a negative correlation between the frequency of claims made by the policyholder and the population density of the city that they live in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot 4 - ClaimAmount vs. x_vars.\n\ndesc_pairplot_4 = sns.pairplot(df_merged, x_vars=desc_pairplot_x_vars_B, y_vars='ClaimAmount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Power: This graph shows that there is an overall negative correlation between the power of the car owned by the policyholder and the total value of claims made by them, however there are two major outliers at lower car powers - this is generally an exception to the rule.\n* CarAge: Similarly to the power of the car, there is an overall negative correlation between the age of the car and the total value of claims made by the policyholder, however there are some outliers for relatively new cars.\n* DriverAge: This graph shows that total claim amounts tend to be higher at younger ages (between 20-40), however there is also a smaller group of drivers between 60-80 that are responsible for non-trivial total claim amounts; major outliers can also be seen for two newer drivers (DriverAge ~ 20).\n* Brand: Whilst there are a handful of brands that are responsible for higher-than-normal claim amounts, these are in the vast minority of policyholders - the overall trend is that there is no clear correlation between the brand of the car and the total claim amount.\n* Gas: This graph also does not show any clear correlation between ClaimAmount and the fuel type of the policyholder's car, however there are two distinct outliers within the 'Regular' category.\n* Density: This graph displays a negative correlation between the total value of claims made by the policyholder and the population density of the city that they live in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot 5 - ClaimSev vs. x_vars (i.e. accounting for policy exposure weighting).\n\ndesc_pairplot_5 = sns.pairplot(df_merged, x_vars=desc_pairplot_x_vars_B, y_vars='ClaimSev')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Power: This graph shows that there is an overall negative correlation between the power of the car owned by the policyholder and the exposure-weighted total severity of claims made, however as before there are two major outliers at lower car powers.\n* CarAge: Similarly to the power of the car, there is an overall negative correlation between the age of the car and the exposure-weighted total severity of claims made by the policyholder, however there are some outliers for relatively new (albeit slightly used) cars.\n* DriverAge: This graph shows that exposure-weighted claim severities tend to be higher at younger ages (between 20-35), however there is also a smaller group of drivers between 45-55 that are responsible for non-trivial total claim amounts; some major outliers can also be seen for two very new drivers (DriverAge ~ 20).\n* Brand: Whilst there are a handful of brands that are responsible for higher-than-normal claim severities, these are in the vast minority of policyholders - the overall trend is that there is no clear correlation between the brand of the car and the total claim amount.\n* Gas: This graph also does not show any clear correlation between ClaimSeverity and the fuel type of the policyholder's car, however there are two distinct outliers within the 'Regular' category.\n* Density: This graph displays a negative correlation between the total value of claims made by the policyholder and the population density of the city that they live in."},{"metadata":{},"cell_type":"markdown","source":"* What distribution does ClaimAmount have?\n> ClaimAmount has a positive continuous distribution (non-negative, for this dataset) which is centred at 0 (a majority of policyholders do not make any claims). Hence, using an ordinary linear regression model that treats the response variable's distribution as normal/Gaussian would not be appropriate, due to the asymmetry in the probability distribution of ClaimAmount as described.\n\n* How will the distribution of this response variable affect the choice of regressors used for modelling claim severity?\n> As claim severity will need to be modelled via asymmetric/skewed distributions, we will need to consider regression approaches using generalised linear models which allow for response variables to have distributions that are non-normal, as well as other regressors that are capable of generalising in an agnostic manner (i.e. these do not require the underlying distribution of the response variable to be pre-defined). These methods are considered in further detail within Step 9."},{"metadata":{},"cell_type":"markdown","source":"# Step 8: Perform feature selection via L1 regularisation"},{"metadata":{},"cell_type":"markdown","source":"Next, we will perform feature selection via L1 (lasso) regularisation, in order to reduce the number of features that are used for fitting each of the models - this is done in order to prevent overfitting. To do this, we add a regularisation term (containing the L1 norm) to the standard loss function that is to be minimised, such that:\n\n> $\\text{Loss} = \\text{Error}(y,\\hat{y}) + \\lambda \\displaystyle\\sum_{i=1}^{N} |w_i|$ \n\nWhere:\n* $y$ is the true value/severity of the claim\n* $\\hat{y}$ is the claim value/severity predicted by the model\n* $\\lambda > 0$ is the regularisation parameter that determines the strength of regularisation to be applied to the loss function\n* $w_i$ is the weight of feature $i$\n\nThis modified loss function is then subsequently minimised in order to produce the parameters of the Lasso linear regression model. Features that are less significant in producing the Lasso model will have their weights/importances decreased towards 0 - these \"unimportant\" features can then be removed from the set of inputs/features that are supplied to the models we will use later on."},{"metadata":{},"cell_type":"markdown","source":"To perform this for our dataset, we will use the `Lasso()` class from `sklearn.linear_model`, fit it to our scaled training data, before assigning it to a variable called `lasso`. This class requires us to specify the following parameters:\n\n* `alpha` represents the constant that multiplies the L1 term (i.e is equivalent to $\\lambda$)\n* `random_state` sets the random number seed and is used for reproducibility purposes. Here, we set this value to 1.\n* `max_iter` represents the maximum number of iterations that the underlying solvers are allowed to take, in order to converge."},{"metadata":{},"cell_type":"markdown","source":"Next, we will pass `lasso` to the `SelectFromModel` class, before assigning it to a new variable called `model` - we specify `prefit=True` to ensure that the meta-transformer should expect a prefit model to be passed directly to it.\n\nThen, we apply the `.transform()` method in order to reduce the scaled training dataset down to the features that were selected by the Lasso (regression) model.\n\nFinally, we create a new dataframe `selected_features` which holds all 'important' features from the original set of columns in `X_train_l1` as their original (scaled) values, but sets values of 0 for every other feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n# Establish the Lasso (L1) Regularisation model that will perform feature selection.\nlasso = Lasso(alpha=5e-5, random_state=1, max_iter=1e+6).fit(X_train_scale, y_train)\nmodel = SelectFromModel(lasso, prefit=True)\n\nX_train_l1 = model.transform(X_train_scale)\n\nselected_features = pd.DataFrame(model.inverse_transform(X_train_l1),\n                                index=X_train_scale.index,\n                                columns=X_train_scale.columns)\n\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns = selected_features.columns[selected_features.var() != 0]\n\nprint(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_L1reg = selected_features.drop(selected_features.columns[selected_features.var() == 0], axis=1)\n\nprint(X_train_L1reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The X_valid dataframe is truncated such that only the L1-selected features are used for validation purposes.\nX_valid_L1reg = X_valid_scale[selected_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 9: Define the regressors/models used"},{"metadata":{},"cell_type":"markdown","source":"In this project, we aim to predict the target (ClaimAmount) using the following linear regression approaches:\n\n1. Random Forest Regression\n\n**Random Forest Regression** works by training multiple decision trees, based on the random sampling (with replacement) of a training dataset. Input features from an unseen dataset can then be supplied to each trained decision tree in order to generate a prediction, which is subsequently averaged across all predictions to produce a final regression output; averaging across all predictions has the benefit of reducing overfitting to any given random sample within the training set.\n\n2. Poisson (GLM) Regression\n\n**Poisson Regression** works in a very similar way to ordinary linear regression, except here we assume that the response variable (ClaimAmount) has a Poisson distribution - thus forming one of two generalised linear models (GLMs) that we will use in this project:\n\n> $Y_i \\stackrel{iid}{\\sim} Pois(\\lambda) $\n\nThis model is chosen purely as the first example of a GLM with a positive continuous distribution that can be used to predict claim severity, although there are better alternatives to use.\n\n3. Tweedie (GLM) Regression\n\n**Tweedie Regression**, like Poisson regression, also assumes that the response variable follows a non-normal distribution - in this case, we assume that ClaimAmount has a Tweedie distribution:\n\n> $Y = \\displaystyle\\sum_{i=1}^{T} X_i, T \\sim Pois(\\lambda), X_i \\stackrel{iid}{\\sim} Ga(\\alpha,\\gamma), T \\perp X_i $\n\nWhere $Y$ is the aggregate claim amount for a covered risk, $T$ is the number of reported claims and $X_i$ is the insurance payment for the $i_{th}$ claim.\n\nHowever, the Tweedie distribution is special in that it is an example of a compound Poisson-Gamma distribution, which means that the distribution shows a mix between both Poisson and Gamma form. The Poisson component helps to account for the large positive mass at zero (i.e. where ClaimAmount is 0, as most policyholders do not make any claims), however the Gamma component allows for a continuous, positively skewed, tail-shaped distribution associated with exponentially decaying probability density (i.e. higher claim severities can also be accounted for).\n\n4. XGBoost (eXtreme Gradient Boosting) Regression\n\n**XGBoost Regression** works similarly to random forest regression, however it is based on an iterative process of gradually reducing the error between predicted & true values. This is achieved by building a new decision tree to fit on the pseudo-residuals of the previous tree, allowing the algorithm to \"learn\" and iteratively refine the regression model until the objective loss function is sufficiently minimised. This can be performed using gradient descent optimisation algorithms; XGBoost is one example of a gradient descent method, and is widely implemented due to its effectiveness in this context."},{"metadata":{},"cell_type":"markdown","source":"# Step 10: Perform cross-validation to obtain the optimal set of hyperparameters for each model"},{"metadata":{},"cell_type":"markdown","source":"Here, we will perform 5-fold cross-validation in order to optimise one of each models' hyperparameters. These are:\n\n`RandomForestRegressor`\n* `n_estimators` represents the number of decision trees that are implemented by the random forest regressor. ***We will aim to optimise this hyperparameter.***\n* `random_state` sets the random number seed and is used for reproducibility purposes. Here, we set this value to 1.\n* `n_jobs` represents the number of calculations to run in parallel; setting a value of -1 means that all processors will be used.\n\n`PoissonRegressor`\n* `alpha` represents the constants that multiplies the penalty term, thus determining the strength of regularisation for the Poisson GLM used. ***We will aim to optimise this hyperparameter.***\n* `max_iter` represents the maximal number of iterations for the PoissonRegressor's solver.\n\n`TweedieRegressor`\n* `power` determines the underlying target value's distribution - using a value between 1 and 2 produces a compound Poisson-Gamma distribution.\n> As a pure Gamma distribution's probability density is not defined at x=0, we set this value to 1.8 such that the target's compound distribution shows more Gamma form than Poisson. This is another hyperparameter that could potentially be optimised for simultaneously, via grid-search methods.\n* `alpha` represents the constants that multiplies the penalty term, thus determining the strength of regularisation for the Tweedie GLM used. ***We will aim to optimise this hyperparameter.***\n* `max_iter` represents the maximal number of iterations for the TweedieRegressor's solver.\n\n`XGBRegressor`\n* `n_estimators` represents the number of gradient boosted trees implemented by the eXtreme Gradient Boosting (XGB) regressor; this is equivalent to the number of boosting rounds. ***We will aim to optimise this hyperparameter.***\n* `learning_rate` refers to the boosting learning rate/step size of the XGB regressor - this value is between 0 and 1.\n* `random_state` sets the random number seed and is used for reproducibility purposes. Here, we set this value to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the regression models from sklearn/xgboost.\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.linear_model import TweedieRegressor\nfrom xgboost import XGBRegressor\n\n# Import the cross_val_score function from sklearn.\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define scoring functions for each method.\n\ndef get_score_RF(n_estimators):\n    model_RF = RandomForestRegressor(n_estimators=n_estimators, random_state=1, n_jobs=-1)\n    \n    scores_RF = -1 * cross_val_score(model_RF, X_train_L1reg, y_train,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\n    return scores_RF.mean()\n\n\ndef get_score_PGLM(alpha):\n    model_PGLM = PoissonRegressor(alpha=alpha, max_iter=500)\n    \n    scores_PGLM = -1 * cross_val_score(model_PGLM, X_train_L1reg, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_absolute_error')\n    \n    return scores_PGLM.mean()\n\n\ndef get_score_TGLM(alpha):\n    model_TGLM = TweedieRegressor(power=1.8, alpha=alpha, max_iter=500)\n    \n    scores_TGLM = -1 * cross_val_score(model_TGLM, X_train_L1reg, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_absolute_error')\n    \n    return scores_TGLM.mean()\n\n\ndef get_score_XGB(n_estimators):\n    model_XGB = XGBRegressor(n_estimators=n_estimators,\n                               learning_rate=0.01,\n                               random_state=1)\n    \n    scores_XGB = -1 * cross_val_score(model_XGB, X_train_L1reg, y_train,\n                                     cv=5,\n                                     scoring='neg_mean_absolute_error')\n    \n    return scores_XGB.mean()\n\n\n## Create empty dictionaries which will be used to store the scoring results for each method.\n\nresults_RF = {}\nresults_PGLM = {}\nresults_TGLM = {}\nresults_XGB = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Obtain 8 scores for the RandomForestRegressor model.\n\nfor i in range(1, 9):\n    results_RF[100*i] = get_score_RF(100*i)\n    print(\"results_RF{} recorded\".format(i))\n\nprint(\"RF done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Obtain 8 scores for the PoissonRegressor model.\n\nfor i in range(1, 9):\n    results_PGLM[round(0.2*i, 2)] = get_score_PGLM(round(0.2*i, 2))\n    print(\"results_PGLM{} recorded\".format(i))\n\nprint(\"PGLM done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Obtain 8 scores for the TweedieRegressor model.\n\nfor i in range(1, 9):\n    results_TGLM[round(0.01*i, 2)] = get_score_TGLM(round(0.01*i, 2))\n    print(\"results_TGLM{} recorded\".format(i))\n\nprint(\"TGLM done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Obtain 8 scores for the XGBRegressor model.\n\nfor i in range(1, 9):\n    results_XGB[5*i] = get_score_XGB(5*i)\n    print(\"results_XGB{} recorded\".format(i))\n    \nprint(\"XGB done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Determine the optimal hyperparameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_n_estimators_best = min(results_RF, key=results_RF.get)\nprint(RF_n_estimators_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PGLM_alpha_best = min(results_PGLM, key=results_PGLM.get)\nprint(PGLM_alpha_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TGLM_alpha_best = min(results_TGLM, key=results_TGLM.get)\nprint(TGLM_alpha_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_n_estimators_best = min(results_XGB, key=results_XGB.get)\nprint(XGB_n_estimators_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 11: Train (fit) the models to the entire training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the optimised regression models that will be used.\n\nmodel_RF_opt = RandomForestRegressor(n_estimators=RF_n_estimators_best, random_state=1, n_jobs=-1)\n\nmodel_PGLM_opt = PoissonRegressor(alpha=PGLM_alpha_best, max_iter=500)\n\nmodel_TGLM_opt = TweedieRegressor(power=1.8, alpha=TGLM_alpha_best, max_iter=500)\n\nmodel_XGB_opt = XGBRegressor(n_estimators=XGB_n_estimators_best, learning_rate=0.01, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the optimised models to the full (pre-processed) training dataset.\n\nmodel_RF_opt.fit(X_train_L1reg, y_train)\nprint(\"model_RF_opt trained\")\n\nmodel_PGLM_opt.fit(X_train_L1reg, y_train)\nprint(\"model_PGLM_opt trained\")\n\nmodel_TGLM_opt.fit(X_train_L1reg, y_train)\nprint(\"model_TGLM_opt trained\")\n\nmodel_XGB_opt.fit(X_train_L1reg, y_train)\nprint(\"model_XGB_opt trained\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 12: Generate a unique set of predictions for each model"},{"metadata":{},"cell_type":"markdown","source":"The next step is to generate predictions of ClaimAmount for each policyholder within the pre-processed validation dataset; this is done using the `.predict()` function for each of the optimised models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the trained models to generate unique sets of predicted y-values i.e. ClaimAmount.\n\npreds_RF = model_RF_opt.predict(X_valid_L1reg)\npreds_PGLM = model_PGLM_opt.predict(X_valid_L1reg)\npreds_TGLM = model_TGLM_opt.predict(X_valid_L1reg)\npreds_XGB = model_XGB_opt.predict(X_valid_L1reg)\nprint(\"All predictions generated\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 13: Assess the chosen models' performance, using validation data"},{"metadata":{},"cell_type":"markdown","source":"In order to evaluate and rank the models based on their regression performances, an appropriate scoring metric should be used. One common example of this is to calculate the Mean Absolute Error (MAE) for each of the fitted models against the validation data, which can then be ranked in order to determine the model with the lowest MAE, which is deemed to be the best model in terms of accuracy and goodness of fit: \n\n> $\\text{MAE} = \\frac{1}{n} \\displaystyle\\sum_{t=1}^{n} |e_t|$ \n\nAlternatively, the Root Mean Squared Error (RMSE) can be derived for each fitted model against the validation data:\n\n> $\\text{RMSE} = \\sqrt {\\frac{1}{n} \\displaystyle\\sum_{t=1}^{n} {e_t^2}}$ \n\nHowever, in order to calculate the RMSE of a model, each prediction error must be squared before they are averaged together; this means that larger errors/outliers are more strongly penalised than smaller errors. Therefore, as the vast majority (~96%) of policyholders within the `freMTPL` dataset have not made any claims whatsoever, we do not wish to heavily penalise each of the models based on any severe claims/outliers that are incorrectly predicted, as this would increase the risk of overfitting each model to these outliers (i.e. by encouraging the model to predict large claims more frequently).\n\nHence, we will use the `mean_absolute_error()` function from `sklearn.metrics` to calculate the MAE score, comparing each model's predictions against the validation dataset's (true) values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Calculate the Mean Absolute Error metric for each set of predicted y-values.\n\nMAE_RF = mean_absolute_error(y_valid, preds_RF)\nMAE_PGLM = mean_absolute_error(y_valid, preds_PGLM)\nMAE_TGLM = mean_absolute_error(y_valid, preds_TGLM)\nMAE_XGB = mean_absolute_error(y_valid, preds_XGB)\nprint(\"All MAE scores calculated\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 14: Evaluate the models' performances"},{"metadata":{},"cell_type":"markdown","source":"Finally, we can determine the most effective model as the one that has obtained the lowest MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collect all MAE scores in a single dictionary.\n\nMAE_results = {'RF': MAE_RF,\n                'PGLM': MAE_PGLM,\n                'TGLM': MAE_TGLM,\n                'XGB': MAE_XGB}\n\nprint(MAE_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the model with the smallest MAE.\n\nbest_model = min(MAE_results, key=MAE_results.get)\nprint(best_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most logical next step would be to iteratively refine the model chosen above, by continually re-training it such that an optimal fit to the _test_ data is achieved.\n\nHere, we have only optimised each model's fit to the _training_ data before generating (and comparing) a single set of predictions for each of these regressors; we would need to repeat this for multiple variations of the model (ideally for each regressor, as well) - i.e. with different hyperparameter configurations - which would then be tested and finally ranked based on their performance against _test_ (\"live\") data."},{"metadata":{},"cell_type":"markdown","source":"# Step 15: Review for areas of improvement"},{"metadata":{},"cell_type":"markdown","source":"Whilst this project has aimed to give an overview of the DS workflow and an example of how it can be applied within insurance, it does not endeavour to provide an all-encompassing perspective on how to apply ML techniques for actuarial pricing purposes; there are many areas in which this project could be improved or reconsidered.\n\nFor example, in terms of the **approach taken to model/predict claim severity**:\n\n* In this project, we have only considered determining the total loss amount per policyholder based on their risk characteristics, and did not determine the average loss amount per claim. It may be possible to model this alternate scenario by deriving additional features beforehand, e.g. max & min ClaimAmounts per policyholder, rather than aggregating ClaimAmount up to policyholder-level.\n\n* Also, for each ClaimAmount prediction we have simply used the number of claims that was provided within the training/test datasets, rather than generating our own predictions beforehand (i.e. we have skipped the first step in the frequency-severity method of modelling losses) - furthermore, in traditional actuarial pricing we only wish to predict the total loss for policyholders who are expected to make at least 1 claim; policyholders who do not make a claim do not incur a loss for the insurer. As a result, we would only need to use non-zero claim amounts for model training/testing purposes - this would allow the inclusion of a Gamma-based GLM approach to predict individual/average claim severities, which may have provided a more suitable comparison to the Tweedie-based GLM approach, than the Poisson regressor that we had used in this project.\n\n* Here, we have only trained each model to the data once (via cross-validation), and tested it once; in a real-world situation, each model would be re-trained and re-tested until an optimal fit to the _test_ data is achieved. This was not explored within this project, for the sake of brevity, however this is relatively simple to perform (and automate).\n\nIn terms of **data pre-processing**:\n\n* There will be other, better, alternatives for how to perform categorical encoding of the relevant features, instead of using either label/one-hot encoding; whilst these methods were originally chosen based on the non-/ordinality of each respective column that was encoded, these may still not necessarily be the most appropriate methods to use - for instance, whilst one-hot encoding was performed for the non-ordinal categorical columns `Gas`, `Brand` and `Region`, each of these varied significantly in cardinality (number of unique values per column) - using one-hot encoding is generally not recommended for categories of high cardinality (e.g. `Region`).\n\nIn terms of **hyperparameter optimisation**:\n\n* One potential improvement could be to perform hyperparameter optimisation via grid-search methods, in order to iterate through all (possible) parameter values and find the model associated with the global minimum of its hyperdimensional loss function surface (loss surface), instead of optimising a single parameter whilst holding all other parameters constant as shown earlier above. This would result in obtaining a genuinely optimised set of models, for each regression method, that correspond with the global minima of their respective loss surfaces.\n\n* Whilst not considered in depth during this project for the sake of brevity, this can be achieved using the `GridSearchCV()` method within `sklearn.model_selection`, which uses the following parameters:\n> `estimator` - the model/estimator that we wish to find the optimal set of hyperparameters for (e.g. RandomForestRegressor, TweedieRegressor).\n>\n> `param_grid` - the dictionary of parameter names/settings to try as values; for example, when optimising a TweedieRegressor, this could be `{power : [1.7, 1.8, 1.9], alpha: [0.1, 0.2, 0.3]}`, although in practice a much wider range (grid) of values & parameters can be searched across.\n>\n> `scoring` - the scoring method that we wish to use in order to measure the performance of each model iteration (for each set of hyperparameters); for example, this could also be `'neg_mean_absolute_error'`.\n\nLastly, in terms of **feature selection**:\n\n* More stringent feature selection could be performed in order to further reduce the number of features that are supplied to each model during the training process, in order to reduce the likelihood of overfitting to the training dataset as a result.\n\n* This can be done by increasing the value of `alpha` within the `Lasso`/L1 regularisation model, in order to restrict the number of features that are kept with non-zero coefficients within the regression model - however, this would likely require an additional hyperparameter optimisation exercise of the L1 regularisation model itself, in order to establish a suitable compromise between being able to fit to the data's features/trends and being able to generalise to unseen data as well."},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements"},{"metadata":{},"cell_type":"markdown","source":"In addition to the sources cited earlier above, I would also like to thank [Arthur Charpentier](https://freakonometrics.github.io/index.html) for publishing the `freMTPL` insurance datasets that was considered in this project.\n\nAdditional information regarding the dataset that was used throughout this project, in addition to a variety of other actuarial datasets, can be found [here](http://cas.uqam.ca/pub/web/CASdatasets-manual.pdf)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}