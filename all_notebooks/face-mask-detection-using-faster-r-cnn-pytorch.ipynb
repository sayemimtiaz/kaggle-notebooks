{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Face-Mask Detection using Faster R-CNN (PyTorch) à¼¼ ã¤ â—•_â—• à¼½ã¤","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport pandas as pd\nimport numpy as np\n\nfrom PIL import Image\n\nimport torch\nimport torchvision\nimport torchvision.transforms as T\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import DataLoader, Dataset\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDIR_INPUT = \"/kaggle/input/face-mask-detection-dataset/\"\nDIR_IMAGES = DIR_INPUT + \"Medical mask/Medical mask/Medical Mask/images/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Loading Dataset\n\ndf = pd.read_csv(DIR_INPUT + \"train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Dataset ðŸ“Š","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Null Values, Unique Values\n\nunq_values = df[\"name\"].unique()\nprint(\"Total Records: \", len(df))\nprint(\"Unique Images: \",len(unq_values))\n\nnull_values = df.isnull().sum(axis = 0)\nprint(\"\\n> Null Values in each column <\")\nprint(null_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Total Classes\n\nclasses = df[\"classname\"].unique()\nprint(\"Total Classes: \",len(classes))\nprint(\"\\n> Classes <\\n\",classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualizing Class Distribution\n\nplt.figure(figsize=(14,8))\nplt.title('Class Distribution', fontsize= 20)\nsns.countplot(x = \"classname\", data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualise Random Images with BBox ðŸ•µï¸â€","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Function to plot image\n\ndef plot_img(image_name):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (14, 14))\n    ax = ax.flatten()\n    \n    bbox = df[df['name'] == image_name]\n    img_path = os.path.join(DIR_IMAGES, image_name)\n    \n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image /= 255.0\n    image2 = image\n    \n    ax[0].set_title('Original Image')\n    ax[0].imshow(image)\n    \n    for idx, row in bbox.iterrows():\n        x1 = row['x1']\n        y1 = row['y1']\n        x2 = row['x2']\n        y2 = row['y2']\n        label = row['classname']\n        \n        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 3)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, (255,0,0), 2)\n    \n    ax[1].set_title('Image with Bondary Box')\n    ax[1].imshow(image2)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Pass any image name as parameter\n\nplot_img(\"3845.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see, column headers (bbox co-ordinates) in 'train.csv' are messed up ðŸ˜, lets clean it! ðŸ˜‹**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cleaning Column Headers - x2 <-> y1 (mismatched)\ndf.rename(columns = {'x2' : 'y1', 'y1' : 'x2'}, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Re-plot same image\n\nplot_img(\"3845.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks good! ðŸ˜‰**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preparing Dataset for Training ðŸ“‚","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Class <-> Int\n\n_classes = np.insert(classes, 0, \"background\", axis=0)        # adding a background class for Faster R-CNN\nclass_to_int = {_classes[i] : i for i in range(len(_classes))}\nint_to_class = {i : _classes[i] for i in range(len(_classes))}\nprint(\"class_to_int : \\n\",class_to_int)\nprint(\"\\nint_to_class : \\n\",int_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating Data (Labels & Targets) for Faster R-CNN\n\nclass FaceMaskDetectionDataset(Dataset):\n    \n    def __init__(self, dataframe, image_dir, mode = 'train', transforms = None):\n        \n        super().__init__()\n        \n        self.image_names = dataframe[\"name\"].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.mode = mode\n        \n    def __getitem__(self, index: int):\n        \n        #Retrive Image name and its records (x1, y1, x2, y2, classname) from df\n        image_name = self.image_names[index]\n        records = self.df[self.df[\"name\"] == image_name]\n        \n        #Loading Image\n        image = cv2.imread(self.image_dir + image_name, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        if self.mode == 'train':\n            \n            #Get bounding box co-ordinates for each box\n            boxes = records[['x1', 'y1', 'x2', 'y2']].values\n\n            #Getting labels for each box\n            temp_labels = records[['classname']].values\n            labels = []\n            for label in temp_labels:\n                label = class_to_int[label[0]]\n                labels.append(label)\n\n            #Converting boxes & labels into torch tensor\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n\n            #Creating target\n            target = {}\n            target['boxes'] = boxes\n            target['labels'] = labels\n\n            #Transforms\n            if self.transforms:\n                image = self.transforms(image)\n\n\n            return image, target, image_name\n        \n        elif self.mode == 'test':\n\n            if self.transforms:\n                image = self.transforms(image)\n\n            return image, image_name\n    \n    def __len__(self):\n        return len(self.image_names)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Transform for Train & Valid\n\n## Using Albumentations\n#def get_transform():\n    #return A.Compose([\n        #ToTensorV2(p=1.0)\n    #], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n## Using torchvision.transforms - without Augmentation!\ndef get_transform():\n    return T.Compose([T.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preparing data for Train & Validation\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n#Dataset object\ndataset = FaceMaskDetectionDataset(df, DIR_IMAGES, transforms = get_transform())\n\n\n# split the dataset in train and test set - using 80% for training, 20% for validation\nindices = torch.randperm(len(dataset)).tolist()\ntrain_dataset = torch.utils.data.Subset(dataset, indices[:-866])\nvalid_dataset = torch.utils.data.Subset(dataset, indices[-866:])\n\n\n#Preparing data loaders\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size = 4,\n    shuffle = True,\n    num_workers = 4,\n    collate_fn = collate_fn\n)\n\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size = 4,\n    shuffle = True,\n    num_workers = 4,\n    collate_fn = collate_fn\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model - Resnet50 (Faster R-CNN) ðŸ”¨","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Utilize GPU if available\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create / load model\n\n#Faster - RCNN Model - pretrained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = len(class_to_int)\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Model for Training - Define learning parameters ðŸ“","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preparing model for training\n\n#Retriving all trainable parameters from model (for optimizer)\nparams = [p for p in model.parameters() if p.requires_grad]\n\n#Defininig Optimizer\n#optimizer = torch.optim.Adam(params, lr = 0.0001)\noptimizer = torch.optim.SGD(params, lr = 0.005, momentum = 0.9)\n\n#LR\n#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n#Load pre-trained model\ncheckpoint = torch.load(\"../input/face-mask-detection-trained-weights/fmd_frcnn_e23.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.to(device)\n\n#No of epochs\nepochs = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now comes everbody's favorite part ðŸ˜‹, let's train it!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training model xD\n\n# Training code is commented for submission, as it takes 11 mins for 1 epoch '-'\n\n\"\"\"\nitr = 1\ntotal_train_loss = []\n\nfor epoch in range(epochs):\n    \n    start_time = time.time()\n    train_loss = []\n    \n    #Retriving Mini-batch\n    for images, targets, image_names in train_data_loader:\n        \n        #Loading images & targets on device\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        #Forward propagation\n        out = model(images, targets)\n        losses = sum(loss for loss in out.values())\n        \n        #Reseting Gradients\n        optimizer.zero_grad()\n        \n        #Back propagation\n        losses.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n        optimizer.step()\n        \n        #Average loss\n        loss_value = losses.item()\n        train_loss.append(loss_value)\n        \n        if itr % 25 == 0:\n            print(f\"\\n Iteration #{itr} loss: {out} \\n\")\n\n        itr += 1\n    \n    #lr_scheduler.step()    \n    \n    epoch_train_loss = np.mean(train_loss)\n    total_train_loss.append(epoch_train_loss)\n    print(f'Epoch train loss is {epoch_train_loss:.4f}')\n\n    \n    time_elapsed = time.time() - start_time\n    print(\"Time elapsed: \",time_elapsed)\n    \n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': epoch_train_loss\n            }, \"checkpoint.pth\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training Details**\n\n* Architecture = ResNet50\n* Method = Faster R-CNN\n* Pretrained Weights = MS_COCO\n* Learning Rate = 0.05\n* Optimizer = SGD with momentum 0.9\n* Epochs = 25\n* Avg Time per Epoch (80% Train Data) = 690 sec\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Loss Details (After 30 Epochs of Training)**\n\n* cls_loss     =  0.0466\n* reg_loss     =  0.0125\n* obj_loss     =  0.0040\n* rpn_reg_loss =  0.0120\n* Overall      =  0.0546","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model on Validation Data ðŸ”§","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### This code may take upto 3 mins (on GPU) to execute, commented purposely!\n\n\"\"\"\nitr = 1\nv_loss = []\n\nstart_time = time.time()\n\nfor images, targets, image_names in valid_data_loader:\n        \n    #Loading images & targets on device\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n    #Forward propagation\n    out = model(images, targets)\n    losses = sum(loss for loss in out.values())\n        \n    #Average loss\n    loss_value = losses.item()\n    v_loss.append(loss_value)\n\nval_loss = np.mean(v_loss)\nprint(f'Val loss is {val_loss:.4f}')\n \ntime_elapsed = time.time() - start_time\nprint(\"Time elapsed: \",time_elapsed)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission ðŸ“„","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating submission df\n\nsubmission = pd.DataFrame(columns = [\"name\", \"x1\", \"x2\", \"y1\", \"y2\", \"classname\"])\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 0 - 1800 (Test Images)\n\nimages = sorted(os.listdir(DIR_IMAGES))\ntest_images = images[ : 1698]\n\n#Use submission.csv (from dataset) as temp\ntest_df = pd.read_csv(DIR_INPUT + \"submission.csv\")\ntest_df = test_df.drop_duplicates(subset='name', keep=\"first\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preparing Training Data\n\n#Test Dataset\ntest_dataset = FaceMaskDetectionDataset(test_df, DIR_IMAGES, mode = 'test', transforms = get_transform())\n\n#Test data loader\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=1,\n    drop_last=False,\n    collate_fn=collate_fn\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Results, appending it to submission df ðŸ“","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Results - may take few mins, please wait!\n\nthreshold = 0.5\nmodel.eval()\n\nfor images, image_names in test_data_loader:\n\n    #Forward ->\n    images = list(image.to(device) for image in images)\n    output = model(images)\n    \n    #Converting tensors to array\n    boxes = output[0]['boxes'].data.cpu().numpy()\n    scores = output[0]['scores'].data.cpu().numpy()\n    labels = output[0]['labels'].data.cpu().numpy()\n    \n    #Thresholding\n    boxes_th = boxes[scores >= threshold].astype(np.int32)\n    scores_th = scores[scores >= threshold]\n    \n    # int_to_class - labels\n    labels_th = []\n    for x in range(len(labels)):\n        if scores[x] > threshold:\n            labels_th.append(int_to_class[labels[x]])\n    \n    #Appending results to csv\n    for y in range(len(boxes_th)):\n        \n        #Bboxes, classname & image name\n        x1 = boxes_th[y][0]\n        y1 = boxes_th[y][1]\n        x2 = boxes_th[y][2]\n        y2 = boxes_th[y][3]\n        class_name = labels_th[y]\n        \n        #Creating row for df\n        row = {\"name\" : image_names[0], \"x1\" : x1, \"x2\" : x2, \"y1\" : y1, \"y2\" : y2, \"classname\" : class_name}\n        \n        #Appending to df\n        submission = submission.append(row, ignore_index = True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}