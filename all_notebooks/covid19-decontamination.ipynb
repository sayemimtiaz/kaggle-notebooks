{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n# \"What do we know about the persistence of the virus and the decontamination?\"\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=563\n\n\n# Results and discussion\n\nThe following results were manually selected to highlight research and news articles that are relevant to the topic:\nAdaption of the emergency department decontamination room for airway management during COVID-19 nan\thttps://doi.org/10.1016/j.ajem.2020.04.031\n* US ECOLOGY PROVIDING **DECONTAMINATION** RESPONSE TO CORONAVIRUS (COVID 19) OUTBREAK. BOISE, Idaho, March 19, 2020 — US Ecology, Inc. (“US Ecology” or “the Company”), in response to the COVID-19 Coronavirus outbreak, offers a comprehensive range of decontamination response and disposal services to customers, providing safe solutions for all disinfecting and decontamination needs to combat the spread of COVID-19. Safety. Service. Professional Standards. An unwavering commitment to safety, unequaled service and the highest professional standards has made US Ecology an industry leader with a proven track record spanning over 65 years. With a nationwide footprint of disposal facilities and service centers in place, US Ecology stands ready to serve new and existing customers. US Ecology’s highly-trained professionals are currently providing COVID-19 decontamination services for a wide range of customers including retail establishments, government agencies, cruise lines, restaurants, as well as insurance, transportation and other companies across the country. The Company’s experts safely handle hazardous materials and infectious waste streams on a daily basis while performing many services, and have extensive experience with managing decontamination responses to biohazards and infectious disease outbreaks with the current COVD-19 outbreak as well Ebola, H1N1 and other past health crises. US Ecology’s broad range of decontamination services include biological and infectious response, cleanup and disposal, blood borne pathogen cleanup and infectious waste management. US Ecology’s decontamination procedures meet or exceed the recommendations and work practices established by the Center for Disease Control (CDC), Occupational Safety and Health Administration (OSHA), World Health Organization (WHO) and other agencies. At each site, crew members follow stringent protocols to establish highly defined and controlled support, entry, decontamination and hot zones to ensure the safe entry/exit of response personnel and the protection of non-response personnel from affected areas. Decontamination procedures are executed by highly-trained personnel wearing the highest-grade personal protective equipment (PPE) and using only approved and highly effective hospital-grade disinfecting agents. To enlist US Ecology’s Decontamination Response and Disposal Services for the Coronavirus (COVID-19) please call the Emergency Response hotline at (800) 899-4672, contact a sales representative, or visit: https://www.usecology.com/contact-us. ABOUT US ECOLOGY, INC. US Ecology, Inc. is a leading provider of environmental services to commercial and government entities. The company addresses the complex waste management and response needs of its customers offering treatment, disposal and recycling of hazardous, non-hazardous and radioactive waste, leading emergency response and standby services, and a wide range of complementary field and industrial services. US Ecology’s focus on safety,\thttps://www.globalbankingandfinance.com/category/news/us-ecology-providing-decontamination-response-to-coronavirus-covid-19-outbreak/\n*\tEffectiveness of Ultraviolet-C Light and a High-Level Disinfection Cabinet for **Decontamination** of N95 Respirators BACKGROUND: Shortages of personal protective equipment (PPE) including N95 respirators are an urgent concern in the setting of the global COVID-19 pandemic. Decontamination of PPE could be useful to maintain adequate supplies, but there is uncertainty regarding the efficacy of decontamination technologies. METHODS: A modification of the American Society for Testing and Materials standard quantitative carrier disk test method (ASTM E-2197-11) was used to examine the effectiveness of 3 methods, including ultraviolet-C (UV-C) light, a high-level disinfection cabinet that generates aerosolized peracetic acid and hydrogen peroxide, and dry heat at 70°C for 30 minutes. We assessed the decontamination of 3 commercial N95 respirators inoculated with methicillin-resistant Staphylococcus aureus (MRSA) and bacteriophages MS2 and Phi6; the latter is an enveloped RNA virus used as a surrogate for coronaviruses. Three and 6 log(10) reductions on respirators were considered effective for decontamination and disinfection, respectively. RESULTS: UV-C administered as a 1-minute cycle in a UV-C box or a 30-minute cycle by a room decontamination device reduced contamination but did not meet criteria for decontamination of the viruses from all sites on the N95s. The high-level disinfection cabinet was effective for decontamination of the N95s and achieved disinfection with an extended 31-minute cycle. Dry heat at 70°C for 30 minutes was not effective for decontamination of the bacteriophages. CONCLUSIONS: UV-C could be useful to reduce contamination on N95 respirators. However, the UV-C technologies studied did not meet pre-established criteria for decontamination under the test conditions used. The high-level disinfection cabinet was more effective and met criteria for disinfection with an extended cycle.\thttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7192214/\n*\tAerosolized Hydrogen Peroxide **Decontamination** of N95 Respirators, with Fit-Testing and Virologic Confirmation of Suitability for Re-Use During the COVID-19 Pandemic In response to the current urgent demand for N95 respirators by healthcare workers responding to the COVID-19 pandemic, with particular emphasis on needs within local medical systems, we initiated an N95 decontamination study using aerosolized hydrogen peroxide or aHP (7% H2O2 solution), via the Pathogo Curis® (Curis) decontamination system. The study has thus far included 10 cycles of respirator decontamination, with periodic qualitative and quantitative fit testing to verify ongoing respirator integrity through the decontamination process, and support a statistical evaluation of successful respirator fit. In addition, we have conducted virologic testing of respirator surfaces and materials to demonstrate a rigorous verification of decontamination. Given that the current pandemic entails a respiratory viral pathogen, it is critical to address these aspects of respirator safety for reuse. These measures are intended to provide a foundation for a suitable decontamination process, which maintains N95 function, and supports safe respirator reuse by healthcare providers. Current results from both respirator fit testing and virologic testing indicate that the process is effective on the basis of zero failure rate on fit-testing of selected respirators, and on complete decontamination of multiple virus species by aHP treatment, comparable to that observed with commercial spore-based biological indicators of sterilization.\thttps://doi.org/10.1101/2020.04.17.20068577\n*\tThe Importance of Fit-Testing in **Decontamination** of N95 Respirators: A Cautionary Note nan\thttps://doi.org/10.1016/j.jaad.2020.05.008\n*\tMethods for Virus Recovery from Solids KEYWORDS Efficiency of recovery; sludge samples; soil samples; elution; concentration; **decontamination**.\thttps://doi.org/10.1016/b978-0-08-026401-1.50031-2\n\n* Neurotropic Coronavirus Infections Neurotropic strains of the mouse hepatitis virus (MHV) cause a range of diseases in infected mice ranging from mild encephalitis with clearance of the virus followed by demyelination to rapidly fatal encephalitis. This chapter discusses the structure, life cycle, transmission, and pathology of neurotropic coronaviruses, as well as the immune response to coronavirus infection. Mice infected with neurotropic strains of MHV have provided useful systems in which to study processes of virus- and immune-mediated demyelination and virus clearance and/or **persistence** in the CNS, and the mechanisms of virus evasion of the immune system.\thttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153443/\n\n# Highlights and suggestions for improvements\n\nThe advantages of this work includes:\n- Use of news content for greater information coverage. For example, the https://www.globalbankingandfinance.com/category/news/us-ecology-providing-decontamination-response-to-coronavirus-covid-19-outbreak/ resource was obtained from the news dataset.\n- Accurate results\n- Data table fromatting with clickable URLs\n- Simple data pipeline to understand and re-use: Users can easily enter a query and find relevant documents by simply calling\n> q='decontamination'\n<br>\n> search(q, 'decontamination')\n- Results are summarized as a WordCloud image\n\nSuggestions for improvements:\n- To speed up the process and minimize computation, only the title and part of the abstract / news content are used in the analysis. Analyzing full-text may reveal additional information.\n- Results can be fed into a document summarization algorithm so results can be more focussed even further\n- Further improvements in how data is presented with a more advanced user interface can be beneficial\n\n\n# Methodology\nData sources:\n- [CORD-19 Research Papers](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n- [COVID-19 Public Media Dataset](https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset)\n\nTo build the corpus, the title and the content from both data soucres are combined (only the first 3,000 characters are used). The task questions were used as search queries for relevant documents in the corpus. For each document in the corpus and search query pair, the word vectors are retrieved from the GoogleNews word embeddings and the cosine distance is calculated. The top documents that match each query are shown in this notebook together with a summary of results presented as a WordCloud image. Additional results are available as CSV files.\n\nI experimented with using the Word Mover's Distance measure as an alternative similarity measure to cosine distance however, it proved to be a lot slower and it was difficult to objectively decide if the results were better than cosine distance. I also tried using LDA (latent dirichlet allocation) to identify the major topics in the results but I decided to use WordClouds instead because it is more visually appealing and more keywords are highlighted.\n\n\nI hope that the findings from this notebook can help inform researchers and curious minds about the ongoing COVID-19 research. I want to thank the researchers, competition organizers, Kaggle and the dataset providers for making this work possible.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nimport io\nimport gc\nimport re\n\nimport logging\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\n\n# pandas settings\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 1000)\nplt.rcParams['figure.figsize'] = [12, 8]\n\n\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk import word_tokenize\ndownload('punkt')  # Download data for tokenizer.\n\nplt.rcParams['figure.figsize'] = [12, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data\n\nGather title and abstract from COVID19 articles and titles from news upto a maximum number of characters.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 3000   # 3000 chars\n\nresearch = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nresearch['title_abstract'] = [str(research.loc[i,'title']) + ' ' + str(research.loc[i,'abstract']) for i in research.index ]\nresearch['source'] = 'research'\nresearch\n\nnews = pd.read_csv('/kaggle/input/covid19-public-media-dataset/covid19_articles.csv')\ndel news[\"Unnamed: 0\"]\nnews['source'] = 'news'\nnews['title_abstract'] = [ news.loc[i,'title'] + '. ' + news.loc[i,'content'][:(MAX_LEN-len(news.loc[i,'title']))] for i in news.index  ]\nnews\n\ndata = pd.concat([research[['title_abstract','source', 'url']], news[['title_abstract', 'source', 'url']]]).rename(columns={'title_abstract':'title'}).drop_duplicates().reset_index(drop=True)\n\nprint('News:',news.shape)\nprint('Research:',research.shape)\nprint('Combined data:',data.shape)\n\ndel research\ndel news\ngc.collect()\n\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Title similarity search and Topic Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim word embeddings\n# https://www.kaggle.com/raymishra/sentence-similarity-match\n# https://radimrehurek.com/gensim/models/fasttext.html\n# https://radimrehurek.com/gensim/models/keyedvectors.html\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nfilepath = \"../input/gnewsvector/GoogleNews-vectors-negative300.bin\"\n\n\nfrom gensim.models import KeyedVectors\nwv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n\n#extracting words7 vectors from google news vector\nembeddings_index = {}\nfor word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n    coefs = np.asarray(vector, dtype='float32')\n    embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper functions\n\ndef preprocess(doc):\n#     doc = re.sub(r'[\\W\\d]+',' ',doc)  # Remove numbers and punctuation.\n    doc = doc.lower()  # Lower the text.\n    doc = word_tokenize(doc)  # Split into words.\n    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n    return doc\n\ndef avg_feature_vector(sentence, model, num_features):\n#     words = sentence.lower().split()\n#     words = preprocess(sentence)\n    words = simple_preprocess(sentence)\n    #feature vector is initialized as an empty array\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in embeddings_index.keys():\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n\nfrom scipy.spatial import distance\ndef calc_dist_cosine(s1, target, max_dist=0.5):\n    ret = []\n    for t in tqdm(target):\n        tv = avg_feature_vector(t,model= embeddings_index, num_features=300)\n        qv = avg_feature_vector(q,model= embeddings_index, num_features=300)\n        dist = distance.cosine(tv, qv)\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n\n\n# wv_from_bin.init_sims(replace=True)  # Normalizes the vectors in the word2vec class before calculating wmdistance\ndef calc_dist_wm(s1, target, max_dist=5.0):\n    \"\"\"\n    Word mover distance. Slower than cosine similarity.\n    https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n    \"\"\"\n    ret = []\n    for t in tqdm(target):\n#         print(t)\n        dist = wv_from_bin.wmdistance(preprocess(s1), preprocess(t))\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n       \ndef calc_dist(s1, target):\n    \"\"\"\n    Dist interface\n    \"\"\"\n    return calc_dist_cosine(s1, target)\n#     return calc_dist_wm(s1, target)\n\n# usage\n# s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\n# s2_afv = avg_feature_vector('Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number',model= embeddings_index, num_features=300)\n# cos = distance.cosine(s1_afv, s2_afv)\n# print(cos)\n# calc_dist_wm('Why the second proforma does not coincide with the first, what has changed', ['Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA\n# https://www.kaggle.com/monsterspy/topic-modeling-with-lda\n# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n\nfrom gensim.models import ldamodel\nimport gensim.corpora\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nstop.update(['href','br'])\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\nnum_topics = 5\n\ndef train_lda(data_text):\n    train_ = []\n    for i in range(len(data_text)):\n        train_.append([word for word in tokenizer.tokenize(data_text[i].lower()) if word not in stop])\n\n    id2word = gensim.corpora.Dictionary(train_)\n    corpus = [id2word.doc2bow(text) for text in train_]    # Term Document Frequency\n    lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n    return lda\n\ndef get_lda_topics(model, num_topics, topn=5):\n    word_dict = {};\n    for i in range(num_topics):\n        words = model.show_topic(i, topn = topn);\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nThe topics include virus, cov, rna, protein, antibody, cells, infection which are research-based words in our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lda = train_lda(data.title.values.tolist())\n# lda_all_titles = get_lda_topics(lda, num_topics)\n# lda_all_titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_clickable(link):\n    # target _blank to open new window\n    return f'<a target=\"_blank\" href=\"{link}\">{link}</a>'\n# df.style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(q, out_prefix=\"result\"):\n    res = calc_dist(q, data.title)\n    res.to_csv(f'result_{out_prefix}.csv', index=False)\n\n    # second iteration using word distance\n#     res2 = calc_dist_wm(q, res.title)\n#     res2.to_csv(f'result_{out_prefix}_wmd.csv', index=False)\n\n#     lda = train_lda(res.title.values.tolist())\n#     lda_res = get_lda_topics(lda, num_topics)\n#     print(lda_res)\n    \n    topn = 20\n    wc = WordCloud(background_color='white', stopwords=stop_words).generate(' '.join(res.title.values.tolist()[:topn]).lower())\n    plt.imshow(wc)\n    plt.axis('off')\n\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decontamination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q='decontamination'\nres = search(q, 'decontamination')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Persistence of the virus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q='persistence of the virus'\nres = search(q, 'persistence')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}