{"cells":[{"metadata":{"id":"sW1iZa70Vn1O"},"cell_type":"markdown","source":"## Introduction ##\n\nFor this study (done for an online data science course), I generated a data set with features that affect national life expectancy,\nbased on information from [WHO (World Health Organization)][who]. The servers are found at the\n[GHO (Global Health Observatory)][whodb] and the [UNESCO Databases of Resources on Education][unesco_ed].\n\nThe data set is similar (but not identical) to a previously used [dataset in Kaggle][kag_ds].\nHowever, that Kaggle set has bad data, where it is not clear how much (if any) of the information is valid.\nHaving that kind of set with good data would be better.\n\n[who]: https://www.who.int\n[whodb]: https://www.who.int/gho/database/en/\n[unesco_ed]: https://en.unesco.org/themes/education/databases\n[kag_ds]: https://www.kaggle.com/kumarajarshi/life-expectancy-who\n"},{"metadata":{"id":"eY-kl5x5Vn1S"},"cell_type":"markdown","source":"This is a list of the features in the data set.\n\n|Field|Description|\n|---:|:---|\n|country|Country|\n|country_code|Three letter identifier of a country\n|region|Global region of the country\n|year|Year|\n|life_expect|Life expectancy at birth (years)|\n|life_exp60|Life expectancy at age 60 (years)|\n|adult_mortality|Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)|\n|infant_mort|Death rate up to  age 1|\n|age1-4mort|Death rate between ages 1 and 4|\n|alcohol|Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)|\n|bmi|Mean BMI (kg/m^2) (18+) (age-standardized estimate)|\n|age5-19thinness|Prevalence of thinness among children and adolescents, BMI \\< (median - 2 s.d.) (crude estimate) (%)|\n|age5-19obesity|Prevalence of obesity among children and adolescents, BMI \\> (median + 2 s.d.) (crude estimate) (%)|\n|hepatitis|Hepatitis B (HepB) immunization coverage among 1-year-olds (%)|\n|measles|Measles-containing-vaccine first-dose (MCV1) immunization coverage among 1-year-olds (%)|\n|polio|Polio (Pol3) immunization coverage among 1-year-olds (%)|\n|diphtheria|Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)|\n|basic_water|Population using at least basic drinking-water services|\n|doctors|Medical doctors (per 10,000)|\n|hospitals|Total density per 100 000 population: Hospitals|\n|gni_capita|Gross national income per capita (PPP int. $)|\n|gghe-d|Domestic general government health expenditure (GGHE-D) as percentage of gross domestic product (GDP) (%)|\n|che_gdp|Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)|\n|une_pop|Population (thousands)|\n|une_infant|Mortality rate, infant (per 1,000 live births)|\n|une_life|Life expectancy at birth, total (years)|\n|une_hiv|Prevalence of HIV, total (\\% of population ages 15-49)|\n|une_gni|GNI per capita, PPP (current international \\$)|\n|une_poverty|Poverty headcount ratio at \\$1.90 a day (PPP) (\\% of population)|\n|une_edu_spend|Government expenditure on education as a percentage of GDP (\\%)|\n|une_literacy|Adult literacy rate, population 15+ years, both sexes (\\%)|\n|une_school|Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes|\n\nThe feature names that start with \"une_\" are from the UNESCO database. The other features are from the GHO database."},{"metadata":{"id":"CEwbwMwyVn1W","outputId":"367b71d6-61b2-4ec0-81e2-d89b44a77f01","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport statsmodels.api as sm\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\nfrom sklearn import linear_model, ensemble\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom statsmodels.tools.eval_measures import mse, rmse\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport time\n%matplotlib inline\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\n\n# suppress warnings about \"value is trying to be set on a copy of a slice from a DataFrame\"\npd.options.mode.chained_assignment = None  # default='warn'\n\n# function to visualize the predictions compared to the test set\n# also print out several fit metrics\n# only using RMS error, but including other metrics to be on the safe side\n\ndef fitter_metrics(y_actual, y_pred):\n    plt.scatter(y_actual, y_pred)\n    plt.plot(y_actual, y_actual, color=\"red\")\n    plt.xlabel(\"true values\")\n    plt.ylabel(\"predicted values\")\n    plt.title(\"Life expectancy: true and predicted values\")\n    plt.show()\n\n    print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_actual, y_pred)))\n    print(\"Mean squared error of the prediction is: {}\".format(mse(y_actual, y_pred)))\n    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_actual, y_pred)))\n    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_actual - y_pred) / y_actual)) * 100))\n\n# using a random number seed for the train-test split\n# set it here, in case we want to re-run with a different seed number\nrand_seed = 173\n\n# *** read in file from Kaggle ***\nwho_df = pd.read_csv('/kaggle/input/who-national-life-expectancy/who_life_exp.csv')\n\n# *** otherwise read from local version of file ***\n#who_df = pd.read_csv('who_life_exp.csv')\n\nprint('Data set has {} countries for the years {} to {}\\n'.format(who_df['country_code'].nunique(),\n                                                                  who_df['year'].min(),who_df['year'].max()))\n\nprint(who_df.info())","execution_count":null,"outputs":[]},{"metadata":{"id":"1LxzSRWUVn1m"},"cell_type":"markdown","source":"## Target Feature ##\n\nBelow are the life expectancy (at birth) vs. year for the 183 countries, grouped into regions.\nThe goal of this analysis is to look for which features affect the national life expectancy.\n\nOverall we can see the life expectancy is gradually increasing for most countries.\nThere are some countries that show dips and drops.\nI did want to look at two unusual country plots to see why they are different than the others.\n(Other dips or odd trends were not investigated in this analysis.)\n\nIn the Eastern Mediterranean region, the plot for one country has the life expectancy over 70 until 2011,\nwhen it dropped to around 60 and stayed there. The country is Syria, and the drop in life expectancy\nreflects the [Syrian civil war][syria_war] that began in 2011 and is still going on.\n\n[syria_war]: https://en.wikipedia.org/wiki/Syrian_civil_war"},{"metadata":{"id":"XNx_FrFJVn1p","outputId":"93a6a0f1-65ab-4a03-a650-4b2c18498bd6","trusted":true},"cell_type":"code","source":"region_names = who_df['region'].unique()\nfig, axs = plt.subplots(2, 3)\nfig.set_size_inches(14.0, 8.0)\nfor ireg, region in enumerate(region_names):\n    ix = ireg//3\n    iy = ireg%3\n    axs[ix, iy].set_title(region)\n    temp_df = who_df[who_df['region'] == region]\n    for country in temp_df['country'].unique():\n        axs[ix, iy].plot(temp_df[temp_df['country']==country].year, temp_df[temp_df['country']==country].life_expect)\n    axs[ix, iy].set_xlabel(\"year\")\n    axs[ix, iy].set_ylabel(\"life expectancy\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"-kem7BIzP4ml"},"cell_type":"markdown","source":"The other country is Haiti in the \"Americas\" region. It has a data point from 2010 is\nsignificantly lower than the years around it.\nOn 12 Jan 2010, a magnitude 7 earthquake which struck Port-au-Prince, and\ndestroyed most of the medical and treatment facilities. The loss of life that\nyear affected the mortality rates, and the life expectancy for the years after\n2010 remained slightly lower than 2009. (If you are interested in learning more\nabout Haiti and life expectancy for that country, \nvisit [The Borgen Project web site][haiti_borgen].)\n\nI decided to remove the Haiti data point for 2010, as an outlier.\nThe models/fits used in this study were predicting a life expectancy around 63,\nabout the same as 2009 and 2011. (The actual value was 36.2 for the year 2010.)\n\nAlthough I was interested in making a model that would account for disasters,\nthe values for that year would need to be sensitive enough to show the effect.\nFor Haiti, the features for \"basic water\", number of hospitals, and\nnumber of doctors for the year 2010 do not reflect the damage done by the earthquake.\n\n[haiti_borgen]: https://borgenproject.org/top-10-facts-about-life-expectancy-in-haiti/"},{"metadata":{"id":"P4UV3TlQP4mo","trusted":true},"cell_type":"code","source":"# This identified which country had the 2010 dip in the Americas plot\n#who_df[(who_df['region'] == \"Americas\") & (who_df['life_expect'] < 65)].head(30)\n\n# print(who_df[who_df['country_code'] == 'HTI'].head(20))\n\nwho_df = who_df[~((who_df['country_code'] == 'HTI') & (who_df['year'] == 2010))]","execution_count":null,"outputs":[]},{"metadata":{"id":"Eur4KEpHVn3W"},"cell_type":"markdown","source":"## Comment on Morality Features ##\n\nI have seen examples of notebooks that used \"adult mortality\" when fitting for life expectancy.\n\nThis should not be done, unless the study is about how the life expectancy was calculated.\n- The life expectancy is calculated using the death rate at a particular age, and integrating the expectation for years lived over all possible years.\n- The adult mortality is calculated using the death rate at a particular age, and integrating the number of deaths over a range years.\n\nIt is no surprise that two quantities that are both calculated from yearly death rates are highly correlated.\nWith only 3 mortality features (infant, child, and adult), a linear fit can get an R<sup>2</sup> of 0.972 for life expectancy.\nIf we add quadratic terms to the fit function to account for non-linearity, the\nR<sup>2</sup> increases to 0.985. (The predictions were consistently low for the lowest and highest\nlife expectancies if we don't include the quadratic terms.)\n\nThose features are still interesting for other studies. Although it is beyond the scope of this analysis,\na future study could look to see if a low life expectancy is being driven by deaths in a particular age group,\nor the different age group mortality rates are affected by different features."},{"metadata":{"id":"BvrzH70qVn3Y","outputId":"88d77be8-16a7-4d37-c848-39c763856460","trusted":true},"cell_type":"code","source":"# We haven't done any data cleaning, so make sure to remove null values\nX = who_df[['life_expect', 'infant_mort', 'age1-4mort', 'adult_mortality']].copy()\nX = X.dropna(axis=0)\n\nY = X['life_expect']\nX_m = X[['adult_mortality', 'infant_mort', 'age1-4mort']]\nX_m['infant2'] = X_m['infant_mort'].pow(2)\nX_m['youth2'] = X_m['age1-4mort'].pow(2)\nX_m['adult2'] = X_m['adult_mortality'].pow(2)\nX_m = sm.add_constant(X_m)\nX_train, X_test, y_train, y_test = train_test_split(X_m, Y, test_size = 0.2, random_state = rand_seed)\n\n# We fit an OLS model using statsmodels\nresults_ols = sm.OLS(y_train, X_train).fit()\n\n# We print the summary results.\nprint(results_ols.summary())","execution_count":null,"outputs":[]},{"metadata":{"id":"u3ZuKh_sVn3d","outputId":"ef819451-174e-43d6-8871-acb8564e6e7a","trusted":true},"cell_type":"code","source":"# We are making predictions here\ny_ols = results_ols.predict(X_test)\n\nfitter_metrics(y_test, y_ols)","execution_count":null,"outputs":[]},{"metadata":{"id":"MYzcBJBVP4nN"},"cell_type":"markdown","source":"For simplicity, I will drop the UNESCO life expectancy feature, and use the GHO values.\nI don't know enough about the methodologies, so I arbitrarily picked one version.\n\nThey are similar, but not identical. For example, the GHO data has a sharp drop for Haiti in 2010, but the UNESCO data does not.\nDetermining the reasons for those differences is beyond the scope of this analysis.\n\nThe other features that use mortality rates directly are removed from the data set.\nThat includes dropping the life expectancy after age 60."},{"metadata":{"id":"x7JWsGtzP4nO","outputId":"2c95df2e-831f-4c7b-b89b-9b3766ebae2b","trusted":true},"cell_type":"code","source":"plt.scatter(who_df[\"life_expect\"], who_df[\"une_life\"])\nplt.plot(who_df[\"life_expect\"], who_df[\"life_expect\"], color=\"red\")\nplt.xlabel('GHO life expect')\nplt.ylabel('UNESCO life expect')\nplt.show()\n\ndel who_df['une_life']\ndel who_df['adult_mortality']\ndel who_df['infant_mort']\ndel who_df['age1-4mort']\ndel who_df['une_infant']\ndel who_df['life_exp60']","execution_count":null,"outputs":[]},{"metadata":{"id":"3PrHK7aRVn14"},"cell_type":"markdown","source":"## Data Cleaning: missing values ##\n\nFor the initial examination of the database, I made some choices about which rows or columns to drop\ndue to missing values. My aim was to keep as many of the countries and features as possible. These will not\nbe the right choices for all analyses, but it is a starting point.\n\nThere are missing values for the vaccine features. Those are for Timor-Leste and the years 2000 and 2001.\nIt became the first new sovereign state of the 21st century in 2002. Given that historical information,\nI will remove those years from that country.\n\nFor the \"alcohol\" feature, there are 70 missing values.\nThe following countries are missing data for a subset of years:\n- South Sudan (SSD, all years)\n- Sudan (SDN, 2000-2010)\n- Serbia (SRB, 2000-2005)\n- Montenego (MNE, 2000-2005)\n- Canada (CAN, 2000-2004)\n- Afghanistan (AFG, 2000-2004)\n\nSouth Sudan gained independence from Sudan in 2011. With the missing alcohol information and my lack\nof knowldege about how the data before 2011 was generated, it will be easier to drop Sudan and\nSouth Sudan from this study entirely.\n\nThe union of Serbia and Montenego dissolved in 2006. Given that historical information, I will remove\nthe years 2000-2005 from those countries, rather than worry about what method was used to handle the\ndata in the years when those two countries were unified.\n\nFor Canada and Afghanistan, those two countries will be handled later, when we are using interpolation to fill in missing values in general."},{"metadata":{"id":"bT5sOgO0Vn17","scrolled":true,"outputId":"3d5e1a0d-4102-48cf-dd95-9456a290ee0e","trusted":true},"cell_type":"code","source":"clean_df = who_df.copy()\n\n#print(\"NaN for polio\")\n#print(clean_df[clean_df['polio'].isnull()].head(50))\n\n# missing vaccine information for 2000 and 2001\nindices = clean_df[(clean_df['country_code'] == 'TLS') & (clean_df['year'] < 2002)].index\nclean_df.drop(indices , inplace=True)\n\n#print(\"NaN for alcohol\")\n#print(clean_df[clean_df['alcohol'].isnull()].head(50))\n\nfor country in clean_df['country_code'].unique():\n    num_na = clean_df[clean_df['country_code'] == country]['alcohol'].isnull()\n    if (num_na.any()):\n        print(\"  for feature \\\"alcohol\\\" \",country,\" is missing data for\", num_na.sum(),\"years\")\n\nindices = clean_df[((clean_df['country_code'] == 'SSD') | (clean_df['country_code'] == 'SDN'))].index\nclean_df.drop(indices , inplace=True)\n\nindices = clean_df[((clean_df['country_code'] == 'SRB') | (clean_df['country_code'] == 'MNE')) &\n                 (clean_df['year'] < 2006)].index\nclean_df.drop(indices , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"_jQx2fa8Vn2G"},"cell_type":"markdown","source":"For the gross national income (GNI) per capita, we have values from both the GHO and UNESCO databases.\nAlthough they have similar trends, there are clear differences. For reasons unknown to me, the GHO data is missing GNI for Argentina, while the UNESCO data is missing Cuba.\n\nNorth Korea, Somolia, and Syria are missing GNI values from both sources,\nso I will drop those countries.\n\nSince GHO set does not have data for the years 2014-2016, I will use the UNESCO GNI, and fill in missing\nvalues if available from GHO data.\n\nAt first, I was planning on using a scale factor when using filling in the missing values, but there\nis not a consistent trend. (It may be useful to investigate the differences in the databases in some\nfuture study.)\nTo keep things simple, I will use a 1.0 scale factor."},{"metadata":{"id":"uSs4GpYnVn2I","outputId":"7631328d-7d25-4db3-cd4f-a0cc17fcaf08","trusted":true},"cell_type":"code","source":"#print(\"NaN for gni ppp\")\n#print(clean_df[clean_df['une_gni'].isnull()].head(50))\n\nindices = clean_df[((clean_df['country_code'] == 'SOM') | (clean_df['country_code'] == 'SYR') | (clean_df['country_code'] == 'PRK'))].index\nclean_df.drop(indices , inplace=True)\n\nplt.scatter(clean_df[\"gni_capita\"], clean_df[\"une_gni\"])\nplt.plot(clean_df[\"gni_capita\"], clean_df[\"gni_capita\"], color=\"red\")\nplt.xlabel('GHO GNI per capita')\nplt.ylabel('UNESCO GNI per capita')\nplt.show()\n\nclean_df['gni_scale'] = (clean_df['une_gni'] / clean_df['gni_capita'])\nplt.scatter(clean_df[\"gni_capita\"], clean_df[\"gni_scale\"])\nplt.plot([0.0, 122000.0], [1.0, 1.0], color=\"red\")\nplt.xlabel('GHO GNI per capita')\nplt.ylabel('UNESCO GNI / GHO GNI')\nplt.show()\n\nprint('Mean UNESCO/ GHO value for GNI > 40K : ', clean_df[clean_df['gni_capita'] > 40000]['gni_scale'].mean())\nprint('Mean UNESCO/ GHO value for GNI > 80K : ', clean_df[clean_df['gni_capita'] > 80000]['gni_scale'].mean())\n\n# Not using a scale factor\n# use GHO value when the UNESCO value is missing; possible that both are null\nclean_df['une_gni'] = np.where(clean_df['une_gni'].notnull(), clean_df['une_gni'], clean_df['gni_capita'])\ndel clean_df['gni_scale']\ndel clean_df['gni_capita']","execution_count":null,"outputs":[]},{"metadata":{"id":"n_GsvaAYVn2g"},"cell_type":"markdown","source":"For the remaining missing values, I will use interpolation. Missing values before the earliest entry will use\nthe earliest value, values after the last entry will use that value, and a linear extrapolation for missing\nvalues between available entries. Most of the features are relatively stable by country. If a different\nanalysis is being done, a better solution for missing values may be required.\n\nThe features \"gghe-d\" and \"che_gdp\" (health expenditures) only have two countries with no data, so those countries\nwill be dropped. Montenegro is missing info for gghe-d and che_gdp; Albania is missing che_gdp."},{"metadata":{"id":"Q2jnPWYlVn2i","outputId":"8a837c39-6ecf-4af9-c175-f3949563beaf","trusted":true},"cell_type":"code","source":"# to interpolate the missing values\nclean_df = clean_df.groupby('country').apply(lambda group: group.interpolate(method='linear', limit_direction='both'))\n\n# Montenegro is missing info for gghe-d and che_gdp; Albania is missing che_gdp\nclean_df = clean_df[~((clean_df['country_code'] == 'ALB') | (clean_df['country_code'] == 'MNE'))]\n\ncountry_list = clean_df['country_code'].unique()\ncolumn_list = list(clean_df.columns)\n\ngone_all = dict()\ngone_some = dict()\n\nfor col in column_list:\n    for country in country_list:\n        num_na = clean_df[clean_df['country_code'] == country][col].isnull()\n        if (num_na.all()):\n            gone_all[col] = gone_all.get(col, 0) + 1\n        if (num_na.any()):\n            gone_some[col] = gone_some.get(col, 0) + 1\n    if col in gone_some:\n        print(\"Feature\",col,\"has\",gone_all[col],\"countries with no data, \",gone_some[col],\"with some missing data.\")","execution_count":null,"outputs":[]},{"metadata":{"id":"q6dFBrZcP4nv"},"cell_type":"markdown","source":"The correlation table/heat map is useful to get an overview of the features, but should be used carefully.\nTwo variables can have a zero Pearson correlation coefficient, and still have a strong non-linear correlation."},{"metadata":{"id":"9wva2SXWVn2-","outputId":"87368123-86b5-4fec-be96-15904be2c27e","trusted":true},"cell_type":"code","source":"#print(clean_df.corr())\n\nplt.figure(figsize=(20,10))\nsn.heatmap(clean_df.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NFHfbW2VP4n0"},"cell_type":"markdown","source":"The 3 vaccination features are highly correlated with each other (0.92-0.97). Instead of keeping \"measles\", \"diphtheria\", and \"polio\", I will use Principal Component Analysis (PCA) to make one \"vaccination\" variable to replace those 3 features."},{"metadata":{"id":"xm5KvA5pP4n1","outputId":"8531d9e3-bf40-4be7-a3b9-86e4d16c1f0e","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\n# Standardizing the features\nX = clean_df[['measles', 'polio', 'diphtheria']]\nX = StandardScaler().fit_transform(X)\n\n# want the principal component vector to be positively correlated with increasing vaccination rates\n# for this particular fit by the code, that requires multiplying by -1\nprincipalComponents = pca.fit_transform(X)\nclean_df['vaccination'] = -principalComponents\n\nprint('Variance accounted for by pca:', pca.explained_variance_ratio_)\n\nplt.figure(figsize=(12,4))\nplt.subplot(1, 3, 1)\nplt.scatter(clean_df['vaccination'], clean_df['measles'])\nplt.xlabel('vac pca')\nplt.ylabel('measles')\n\nplt.subplot(1, 3, 2)\nplt.scatter(clean_df['vaccination'], clean_df['polio'])\nplt.xlabel('vac pca')\nplt.ylabel('polio')\n\nplt.subplot(1, 3, 3)\nplt.scatter(clean_df['vaccination'], clean_df['diphtheria'])\nplt.xlabel('vac pca')\nplt.ylabel('diphtheria')\n\nplt.tight_layout()\nplt.show()\n\ndel clean_df['measles']\ndel clean_df['polio']\ndel clean_df['diphtheria']","execution_count":null,"outputs":[]},{"metadata":{"id":"UavbFK-ZVn3F"},"cell_type":"markdown","source":"I plotted the distribution of the life expectancy, the remaining features, \nand the scatter plot of\nlife expectancy as a function of those features.\n\nWe don't necessarily expect these features to have a normal distribution.\nFor example, populations for each nation should be largely\nrandom. Some of the features are a percentage of the population, which cannot exceed 100%.\nThis kind of limitation needs to be kept in mind if we are looking at metrics where the\nfit assumes normality of the features.\n\n(Seven of the features still have countries with missing information. This is the reason for the warning message from the code when plotting \"hepatitis\".)"},{"metadata":{"id":"UlwnK4JGVn3G","outputId":"e4243bd6-3f43-4890-8a2a-65b47f0f5c4b","trusted":true},"cell_type":"code","source":"target_feature = \"life_expect\"\n\nnum_features = list(clean_df.columns)\nnum_features.remove(target_feature)\nnum_features.remove('country')\nnum_features.remove('country_code')\nnum_features.remove('region')\nnum_features.remove('year')\n\nprint(\"Target feature:\",target_feature)\nprint(\"Numeric  features:\",num_features)\n\nprint(\"Plotting target feature:\",target_feature)\nplt.hist(clean_df[target_feature])\nplt.show()\n\nfor feat in num_features:\n    print(\"Plotting feature:\",feat)\n    plt.figure(figsize=(12,4))\n    plt.subplot(1, 2, 1)\n    plt.hist(clean_df[feat])\n    plt.xlabel(feat)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(clean_df[feat], clean_df[target_feature])\n    plt.xlabel(feat)\n    plt.ylabel(\"life expectancy\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"IhQRum4mVn3M"},"cell_type":"markdown","source":"Looking at the scatter plot distributions, 3 features\nshowed noticable non-linear correlations: young age obsesity, number of doctors (per capita),\nand GNI. Based on the shape of the scatter plot distributions, I decided\nto try using the logarithm of those values. The transformed variables appear (visually) to have\na more linear relationship with life expectancy.\n\n(Later on, I will use linear regression to see how the fit is affected,\nbefore and after the log transformation on those 3 features.)"},{"metadata":{"id":"1-SvlqGjVn3O","outputId":"fd25a90c-a776-4104-8760-aa8d2244e69a","trusted":true},"cell_type":"code","source":"no_log_df = clean_df.copy()\n\nfeature_log = ['age5-19obesity', 'doctors', 'une_gni']\n\nfor feat in feature_log:\n    clean_df[feat] = np.log1p(clean_df[feat])\n\n    print(\"Plotting feature:\",feat)\n    plt.figure(figsize=(12,4))\n    plt.subplot(1, 2, 1)\n    plt.hist(clean_df[feat])\n    plt.xlabel(feat)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(clean_df[feat], clean_df[target_feature])\n    plt.xlabel(feat)\n    plt.ylabel(\"life expectancy\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GvoTTvuKVn2r"},"cell_type":"markdown","source":"For now, I will drop features that have countries with missing values, to use the most countries.\nThe result is data with 11 features from 176 countries.\n\n(Later on, I will see what changes if I try to keep more features, and drop the countries with missing values.)\n\nI am also not using country, country code, year, and region information.\nThere are factors that may depend on the \"year\", like monetary inflation or technology advancements,\nbut those effects are too complicated to include in this initial study."},{"metadata":{"id":"n_e_ihB-Vn2t","outputId":"efaa20a6-e9ee-4715-f9a1-b979f2aaa53c","trusted":true},"cell_type":"code","source":"# make a copy before dropping any more features and countries\n# this will save some work later on, when I want to try a\n# different selection of features\n\ndf_before_remove = clean_df.copy()\n\n# drop hepatitis, hospitals, hiv, poverty, spend edu, literacy, school\nremove_list = ['hepatitis', 'hospitals', 'une_hiv', 'une_poverty', 'une_edu_spend', 'une_literacy', 'une_school']\nfor col_name in remove_list:\n    del clean_df[col_name]\n    del no_log_df[col_name]\n    num_features.remove(col_name)\n\nprint('Remaining features:',len(num_features), num_features)\nprint(\"\\n\",clean_df['country_code'].nunique(),\"countries to be analyzed\")\n\n# drop remaining NaN rows; should be zero, but running it just in case I missed a stray value somewhere\nclean_df = clean_df.dropna(axis=0)\nno_log_df = no_log_df.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"tmN0hBkzVn3h"},"cell_type":"markdown","source":"## Comment on Cross Validation and Overfitting ##\nI am making a note here as a reminder to myself about a potential fitting problem to be avoided in the future.\nIt might prove useful as an example for others learning data science as well.\n\nI am using 80\\% of the data for model training, comparing different learning models, and tuning of hyperparameters. The remaining 20\\% will be used to test the final model fit.\n\nWhile searching for examples of cross validation on the Internet, there are times when it is acceptable to use 100\\% of the data during cross validation. That is equivalent to trying multiple train-test selections. This\nshould only be done when the model and hyperparameters are already fixed before looking at the data.\n\nEarly while working on this analysis, I made the mistake of running \"cross_val_score\" without running a function like \"train_test_split\" or the dataframe method \"sample(frac=1)\" beforehand. The function \"cross_val_score\" does not make random selections when dividing the data. (If we run \"cross_val_score\" with \"CV=5\" (for example), it will take the first 20\\% of the rows for a test set, the rows from 21\\% to 40\\% on the next iteration as a test set, and so forth.) For this data, where the entries are clustered by country and region, this will lead to massive overfitting and poor prediction results. (The reason for\nthe poor predictions was not immediately obvious to me,\nsince the code ran without errors.)"},{"metadata":{"id":"6T6kfFpFVn3h"},"cell_type":"markdown","source":"## Model Fit Selection ##\n\nI tried multiple models to fit the life expectancy data, with default hyperparameters for each model. After a promising model\nwas selected (based in part on the root-mean-square (RMS) errors and processing time), I \noptimized the hyperparameters for that particular model."},{"metadata":{"id":"X72L192lVn3i","trusted":true},"cell_type":"code","source":"# create a table of the model metrics, to allow easier comparison\nmodel_perform = pd.DataFrame(columns = ['model', 'rms_error', 'time'])\n\nX = clean_df[num_features]\ny = list(clean_df[target_feature])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rand_seed)\n\n# standardize the features before fitting\n# not all models require this, but it will make life easier to do it for all of them\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train), columns=num_features)\nX_test_sc = pd.DataFrame(sc.transform(X_test), columns=num_features)","execution_count":null,"outputs":[]},{"metadata":{"id":"JtzYsvvcgwiQ"},"cell_type":"markdown","source":"For each model, I use \"cross_val_score\" from the sklearn library. The score is the negative root mean squared error. The fitter tries to increase the score, which results in a lower (non-negative) RMS error.\n\nBelow, I am running the statsmodel OLS fit, to get the nicely formatted fit results text output. (I ended up not using it, but I wanted that information available in case I wanted to see it.)"},{"metadata":{"id":"6p3ctE_7P4oY","outputId":"250691ad-453f-495d-9b33-bc04b75987cf","trusted":true},"cell_type":"code","source":"# fit using statsmodels OLS\nX_train_sc2 = sm.add_constant(X_train_sc)\nresults_ols = sm.OLS(y_train, X_train_sc2).fit()\n\n# We print the summary results.\nprint(results_ols.summary())\n\n# We are making predictions here\nX_test_sc2 = sm.add_constant(X_test_sc)\ny_ols = results_ols.predict(X_test_sc2)\n\nfitter_metrics(y_test, y_ols)","execution_count":null,"outputs":[]},{"metadata":{"id":"HycNfVELVn3m"},"cell_type":"markdown","source":"### Model Fitting: OLS ###\nThe ordinary least squares fit is not great (RMS error 3.62), but it does show that this set of 11 features\nhas some predictive power, even with the simple assumption of linearity.\n\nThis model fit would be improved if we had more usable features (to account for more of the variance), or if we\nhad a theoretical function to use in the fit that accounted for non-linear relationships."},{"metadata":{"id":"Ax6QS4s5Vn3m","scrolled":true,"outputId":"a94e26f4-b233-4a26-db63-089196b5c27a","trusted":true},"cell_type":"code","source":"model = linear_model.LinearRegression()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'OLS (before log transform)', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"9SMra2XyVn3r"},"cell_type":"markdown","source":"The data includes the 3 features that were log transformed.\nI wanted to see how the OLS fit does with the untransformed features. The fit below shows that not using the logarithm values results in a poorer fit (RMS error 3.97)."},{"metadata":{"id":"Dwi0mgyDVn3s","outputId":"fa68a7d3-8130-4dc4-b695-38c0071da27e","trusted":true},"cell_type":"code","source":"y2 = no_log_df[target_feature]\nX2 = no_log_df[num_features]\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state = 173)\n\nmodel = make_pipeline(StandardScaler(), linear_model.LinearRegression())\nscore = cross_val_score(model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X2_train, y2_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'OLS (after log transform)', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y2_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"NaGsm3MDVn3w"},"cell_type":"markdown","source":"### Model Fitting: ElasticNet ###\n\nAn alternative to OLS is [Elastic Net][elastic_net], which the sklearn documentation states is a \"linear regression with combined L1 and L2 priors as regularizer\".\nThe default model is not as good as OLS (RMS 4.20). (I assume that some tweaking of the hyperparamters would fix that, but I did not do that here.)\n\n[elastic_net]: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"},{"metadata":{"id":"4kCmpXyCVn3x","outputId":"1f74cec8-0bbc-46ed-9807-b556c27cd897","trusted":true},"cell_type":"code","source":"model = linear_model.ElasticNet()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'ElasticNet', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"3yndL9F6Vn30"},"cell_type":"markdown","source":"### Model Fitting: Huber ###\nThe description on the sklearn web site mentions the [Huber model][huber_reg] is a \"linear regression model that\nis robust to outliers.\" For this dataset, the default hyperparameters results in a fit comparable\nto OLS (RMS 3.68).\n\n[huber_reg]: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html"},{"metadata":{"id":"HXB7PAYKVn31","outputId":"8c461011-47e9-4903-9415-708eadc6f1b3","trusted":true},"cell_type":"code","source":"# Failed to converge with default of max_iter=100\nmodel = linear_model.HuberRegressor(max_iter=1200)\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'Huber Linear', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"gyDpaHZDVn35"},"cell_type":"markdown","source":"### Model Fitting: K Nearest Neighbors ###\nThe regression method based on [k-nearest neighbors][knn_method] is an improvement over the OLS fit (RMS 1.48).\n\nSince the life expectancy\nand features on a national level change slowly over time (in general), and each country has up to 17 year of entries, it is not surprising this model does well on this data. I ended up not chosing this model, as I am concerned that the fit does well on existing data, but perhaps not as well on predicting new results.\n\n[knn_method]: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"},{"metadata":{"id":"9KiUDYYnVn36","outputId":"ddb708cd-2639-4052-e011-00b6c433befa","trusted":true},"cell_type":"code","source":"# default is n_neighbors=5\nmodel = KNeighborsRegressor()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'KNeighbors', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"pfr5XvIUVn4F"},"cell_type":"markdown","source":"### Model Fitting: Support Vector ###\nThe documentation for [Support Vector Machine methods][svr_method] mentions that the variables have to be scaled before\nused in the model fit. In this particular situation, I found out that not standardizing the features results in model predictions\nthat are always near 70.\n\nThe RMS is better than the OLS fit (2.81), but we can see that for life expectancy below 50 years, the\npredictions (with default hyperparameters) are overestimated.\n\n[svr_method]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"},{"metadata":{"id":"O62BIqX-Vn4G","outputId":"bfc75904-371d-467d-8c66-a1c00f395d9e","trusted":true},"cell_type":"code","source":"# \nmodel = SVR()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\n\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'Support Vector', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"LOKDHIaLVn3-"},"cell_type":"markdown","source":"### Model Fitting: Random Forest ###\nThe last two models that I tried use ensemble-based methods. The [Random Forest Regressor][rfr_method] by default has an unlimited depth, and nodes are expanded until all leaves are pure.\n\nThe results are good (RMS error 1.51), but this is probably close to the best result possible for this model. The other models have default hyperparameters set to limit the execution time of the code, so this is an apple-to-orange kind of comparison.\n\n[rfr_method]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"},{"metadata":{"id":"1io6pmTbVn3_","outputId":"318dbdd8-07af-408d-ae14-712f42979295","trusted":true},"cell_type":"code","source":"# The default value of n_estimators changed from 10 to 100 in version 0.22\n\nmodel = ensemble.RandomForestRegressor()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'Random Forest', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"qiIvmAB0Vn4C"},"cell_type":"markdown","source":"I wanted to see how this model performs with a limit on the depth, which will speed up the processing and make it more like the other models I used in this section.\n\nThe results with a depth of 4 are a better than OLS (RMS error 3.23), but it doesn't make any predictions below 50 years. As the depth increases, the model predictions improve at the lowest\nand highest life expectancy values."},{"metadata":{"id":"u9iYmMQaVn4D","outputId":"23d9f61b-aa18-4d08-9290-a29a11031fb5","trusted":true},"cell_type":"code","source":"# The default value of n_estimators changed from 10 to 100 in version 0.22\n\nmodel = ensemble.RandomForestRegressor(max_depth=4)\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'Random Forest (depth 4)', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"nQoaELfTVn4I"},"cell_type":"markdown","source":"### Model Fitting: Gradient Boosting ###\nThe last model I tried is [Gradient Boosting Regressor][gbr_method]. It is more of a \"black-box\" algorithm (less intuition about how to improve the fit), but it tends to perform well on machine learning problems.\n\n[gbr_method]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"},{"metadata":{"id":"hzm78Q8zVn4J","outputId":"78bc3213-2517-40cb-f412-c2c7b4708525","trusted":true},"cell_type":"code","source":"# default n_estimators=100, max_depth=3\n\nmodel = ensemble.GradientBoostingRegressor()\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\nelapse_time = time.time()\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\nelapse_time = time.time() - elapse_time\nmodel_perform = model_perform.append({'model': 'Gradient Boost', 'rms_error': -score.mean(), 'time': elapse_time}, ignore_index=True)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"bI3ny5ImuGe5"},"cell_type":"markdown","source":"## Optimizing the Gradient Boost Model ##\n\nI decided to use Gradient Boosting as the model to fit this data set. Even with default hyperparameters, it had one of the lower RMS errors. Later on, we will see that the optimized hyperparameters results in a better fit than even the KNeighbors (unlimited depth) result."},{"metadata":{"id":"53wmMUANuIQ8","outputId":"4b82b79d-6da7-441e-a8a5-9d5ef8c9aea1","trusted":true},"cell_type":"code","source":"print(model_perform.round(decimals=3).head(20))","execution_count":null,"outputs":[]},{"metadata":{"id":"NTCYC6pNVn4L"},"cell_type":"markdown","source":"The default hyperparameters for GBR are depth of 3 and 100 estimators.\n\nI ran GridSearchCV to see if more resources will improve the model predictions.\nThe model seems stable over the range of hyperparameters used, so there is some\nflexibility on what parameters to use.\n\nI choose a depth of 6 and 300 estimators. While not the absolute best test score rank,\nthe test scores are similar (top four are all 0.978 for mean score), and chosing\n300 estimators is faster than the best rank (depth 6, 500 estimators)."},{"metadata":{"id":"cAeq6eGPVn4L","outputId":"3c52aecf-dc36-4738-c031-980138d16915","trusted":true},"cell_type":"code","source":"params = {'n_estimators': [100, 200, 300, 500],\n          'max_depth': [3, 4, 5, 6, 7]}\n\nmodel = ensemble.GradientBoostingRegressor()\nclf = GridSearchCV(model, params)\nclf.fit(X_train_sc, y_train)\n\noptimize_df = pd.DataFrame.from_dict(clf.cv_results_)\ndel optimize_df['params']\n#rank_col = optimize_df.pop(\"rank_test_score\")\n#optimize_df = optimize_df.insert(1, rank_col.name, rank_col)\noptimize_df.round(decimals=3).head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"eQ7COliKVn4P"},"cell_type":"markdown","source":"Using the chosen hyperparameters instead of the default improves the RMS errors from\n2.26 to 1.34."},{"metadata":{"id":"G8hQXzxtVn4P","outputId":"8578a0d8-0310-484e-ca34-f08b1600caa9","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 300,\n          'max_depth': 6}\n\nmodel = ensemble.GradientBoostingRegressor(**params)\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\ny_pred = cross_val_predict(model, X_train_sc, y_train, cv=5)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"uApctGVPulDA"},"cell_type":"markdown","source":"The GBR code does return scores for the features to indicate relative importance in the fit. For this fit, \"basic_water\" has a much higher \"importance\" than the other 10 features.\n\nHowever, I suspect the features are have enough correlation that the model could have arrived at other solutions with similar results. If I use GBR with \"basic_water\" removed (with 10 features), we see that order of features is not preserved. For example, \"doctors\" is now the most important feature, rather than the previous second-best feature of GNI."},{"metadata":{"id":"DfrMskLQVn4T","outputId":"41e0a39b-13cf-4457-a913-d04fb3bc6cc8","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 300,\n          'max_depth': 6}\n\n# Initialize and fit the model.\nclf = ensemble.GradientBoostingRegressor(**params)\nclf.fit(X_train_sc, y_train)\n\nfeature_importance = clf.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance depth 6')\n\nparams = {'n_estimators': 300,\n          'max_depth': 6}\n\nX_no_h2o = X_train_sc.copy()\ndel X_no_h2o['basic_water']\n\nclf = ensemble.GradientBoostingRegressor(**params)\nclf.fit(X_no_h2o, y_train)\n\nfeature_importance = clf.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_no_h2o.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance w/o basic_water')\n\nplt.subplots_adjust(left=0.5, right=1.1)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zr1_tqziwkWd"},"cell_type":"markdown","source":"Using GBR with only 10 features does increase the RMS error from 1.34 to 1.44. \"basic_water\" is important, just not as overwhelmingly important as we might naively think based on the plot of relative importance."},{"metadata":{"id":"jv7POrKrP4pV","outputId":"92d245b8-1aeb-44db-b5ac-0de659a3e542","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 300,\n          'max_depth': 6}\n\nmodel = ensemble.GradientBoostingRegressor(**params)\nscore = cross_val_score(model, X_no_h2o, y_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\ny_pred = cross_val_predict(model, X_no_h2o, y_train, cv=5)\n\nfitter_metrics(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"kp11zpFPVn4T"},"cell_type":"markdown","source":"I was curious about the\neffect of changing the depth on the GBR. While \"basic_water\" stays on top, the importance of the other features\nshift in order and relative magnitude.\n\n(Future analyses might want to explore the correlation between the features, or try PCA in more detail.)"},{"metadata":{"id":"v5q8m-8LP4pZ","outputId":"268c2fc4-b2e2-461e-8e3b-97cc4fb36a12","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 300,\n          'max_depth': 4}\n\n# Initialize and fit the model.\nclf = ensemble.GradientBoostingRegressor(**params)\nclf.fit(X_train_sc, y_train)\nscore = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='neg_root_mean_squared_error')\n\n\nfeature_importance = clf.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance depth 4')\n\nparams = {'n_estimators': 300,\n          'max_depth': 8}\n\nclf = ensemble.GradientBoostingRegressor(**params)\nclf.fit(X_train_sc, y_train)\n\nfeature_importance = clf.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance depth 8')\n\nplt.subplots_adjust(left=0.5, right=1.1)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"F0oFlbgSVn4W"},"cell_type":"markdown","source":"## Additional Features ##\nI want to check how the results change if we can include more features, at the cost of removing countries that don't have that information. That will require reducing the number of countries from 176 to 61, but increases the available features from 11 to 18.\n\nKeep in mind that we only had 130 entries for \"hospitals\", so most of the information for that feature was interpolated from one or two entries per country. (I won't be exploring in detail the effects of the features that have sparse entries.)"},{"metadata":{"id":"o5rb-aktVn4W","outputId":"8ff11785-20f6-4679-a570-3b956fb47d0e","trusted":true},"cell_type":"code","source":"# use the copy before we dropped features and countries\nclean_df = df_before_remove.copy()\n\nmore_features = list(clean_df.columns)\nmore_features.remove(target_feature)\n\n#remove_list = ['hepatitis', 'une_hiv', 'une_poverty', 'une_edu_spend', 'une_literacy',\n#               'gni_capita', 'une_pop']\n#for col_name in remove_list:\n#    del clean_df[col_name]\n\n# drop remaining NaN rows\nclean_df = clean_df.dropna(axis=0)\n\nprint(clean_df['country_code'].nunique(),\"countries to be analyzed\")\nlist_use_country = clean_df['country_code'].unique()\n\n# make another dataframe with the coutries we just dropped\nunclean_df = df_before_remove.copy()\nunclean_df = unclean_df[~unclean_df.country_code.isin(list_use_country)]\n\n# drop remaining NaN rows\n#unclean_df = unclean_df.dropna(axis=0)\n\nprint(unclean_df['country_code'].nunique(),\"countries excluded\")\n\nmore_features.remove('country')\nmore_features.remove('country_code')\nmore_features.remove('region')\nmore_features.remove('year')\n\nprint(\"Target feature:\",target_feature)\nprint(\"Larger set of numeric  features:\",len(more_features), more_features)","execution_count":null,"outputs":[]},{"metadata":{"id":"uUQx_yQCVn4Z"},"cell_type":"markdown","source":"Using the 61 countries that have at least some information for the 18 features, the fit accuracy improved.\nThe RMS error is 0.92 for the training set. (Anything near 1.0 suggests that we are near the limit for improving the fit model.)"},{"metadata":{"id":"35lpVoSFVn4Z","outputId":"a831ced8-a214-4da2-b7b4-936cc7027a9d","trusted":true},"cell_type":"code","source":"Y3 = clean_df[target_feature]\nX3 = clean_df[more_features]\n\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, Y3, test_size = 0.2, random_state = rand_seed)\n\nparams = {'n_estimators': 300,\n          'max_depth': 6}\n\nmodel = make_pipeline(StandardScaler(), ensemble.GradientBoostingRegressor(**params))\nscore = cross_val_score(model, X3_train, y3_train, cv=5, scoring='neg_root_mean_squared_error')\nprint('Array of cross_val_score results:',score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\ny_pred = cross_val_predict(model, X3_train, y3_train, cv=5)\n\nfitter_metrics(y3_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"CXGQFG7lVn4c"},"cell_type":"markdown","source":"I was curious if the selection of those 61 countries, rather than adding 7 more features, might have been the reason for the improvement.\n\nUsing the set of 11 features used earlier, the fit of the 61 countries has an RMS error of 1.30, similar to the earlier result (with 171 countries), and higher than the 18-feature fit (RMS 0.92)."},{"metadata":{"id":"yZ8j0NSlVn4d","outputId":"a5376d73-c91d-4caa-cab9-036a67075a1b","trusted":true},"cell_type":"code","source":"Y4 = clean_df[target_feature]\nX4 = clean_df[num_features]\n\nX4_train, X4_test, y4_train, y4_test = train_test_split(X4, Y4, test_size = 0.2, random_state = rand_seed)\n# \nparams = {'n_estimators': 300,\n          'max_depth': 6}\n\nmodel = make_pipeline(StandardScaler(), ensemble.GradientBoostingRegressor(**params))\nscore = cross_val_score(model, X4_train, y4_train, cv=5, scoring='neg_root_mean_squared_error')\nprint(score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\ny_pred = cross_val_predict(model, X4_train, y4_train, cv=5)\n\nfitter_metrics(y4_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"TZwOTzS2Vn4f"},"cell_type":"markdown","source":"I then redid the fit (with 11 features) with the 115 countries that were excluded from the 18-feature fit. The RMS error is 1.14.\n\nI don't have the time to explore this further. If some of the 7 additional features are not important for the GBR model, then those could be excluded in order to include more countries in the fit.\n\nFuture analyses might try an unsupervised classification, to test if some countries have different factors that influence the life expectancy. (Rather than trying a single global fit with all countries, we might want to try a model with fits on two distinct subsets of countries.)"},{"metadata":{"id":"MSXWdo_gVn4g","outputId":"707f0fd9-685e-4520-a0e3-b48752b0ba5c","trusted":true},"cell_type":"code","source":"Y5 = unclean_df[target_feature]\nX5 = unclean_df[num_features]\n\nX5_train, X5_test, y5_train, y5_test = train_test_split(X5, Y5, test_size = 0.2, random_state = rand_seed)\n# \nparams = {'n_estimators': 300,\n          'max_depth': 6}\n\nmodel = make_pipeline(StandardScaler(), ensemble.GradientBoostingRegressor(**params))\nscore = cross_val_score(model, X5_train, y5_train, cv=5, scoring='neg_root_mean_squared_error')\nprint(score)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() ))\n\ny_pred = cross_val_predict(model, X5_train, y5_train, cv=5)\n\nfitter_metrics(y5_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"_W8AgkGWVn4l"},"cell_type":"markdown","source":"## Final Model Fit ##\nI used the Gradient Boosting Regressor fit on the set of 176 countries with 11 features. The reserved test data is then run with the trained model.\n\nThe RMS error is 1.2, indicating a reasonably good fit, with no obvious bias when comparing the predictions to the true values. The RMS error is comparable to the errors seen with cross-validation with the training data. (If the model was overfitting, I would expect the error to be significantly higher for the test data.)\n\nThe data set has a small selection of features available from the WHO data servers. The model predicts the national life expectancies well with only 11 features, and without using features that depend directly on mortality rates.\n\nThis model suggests that countries with lower life expectancy are best served to improve access to water (potable and sanitation), doctors (basic health care), and overall income (GNI per capita). More study of the feature correlations would be needed to see what other features are important.\n\nEven this basic analysis shows there is a lot that can be investigated in this data set. If you find this interesting,\nyou are encouraged to look at the GHO and UNESCO data servers, which have thousands of variables."},{"metadata":{"id":"GmYEqwLqVn4m","outputId":"d73905e9-b555-400a-ced2-41b13f7fb239","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 300,\n          'max_depth': 6}\n\n# Initialize and fit the model.\nclf = ensemble.GradientBoostingRegressor(**params)\nclf.fit(X_train_sc, y_train)\n\npredict_test = clf.predict(X_test_sc)\nfitter_metrics(y_test, predict_test)\n\nfeature_importance = clf.feature_importances_\n\n# Make importances relative to max importance.\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_test.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}