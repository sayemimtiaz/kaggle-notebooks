{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom datetime import timedelta\n\n# Data Visualization Liraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom IPython.display import display, Markdown\n\n#hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\npyo.init_notebook_mode()\n\n#display max columns of pandas dataframe\npd.set_option('display.max_columns', None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Datsets:"},{"metadata":{},"cell_type":"markdown","source":"# Comparison of COVID19 data - Global Vs India"},{"metadata":{},"cell_type":"markdown","source":"# Global COVID19 Data Analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cov_dash = pd.read_csv('../input/uncover/UNCOVER_v4/UNCOVER/johns_hopkins_csse/johns-hopkins-covid-19-daily-dashboard-cases-over-time.csv')\ncov_dash_country = pd.read_csv('../input/uncover/UNCOVER_v4/UNCOVER/johns_hopkins_csse/johns-hopkins-covid-19-daily-dashboard-cases-by-country.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cov_dash_country.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Field description\n\n* Country_Region - The name of the Country\n* Last_Update - The most recent date the file was pushed\n* Lat - Latitude\n* Long_ - Longitude\n* Confirmed - Aggregated confirmed case count for the state\n* Deaths - Aggregated Death case count for the state\n* Recovered - Aggregated Recovered case count for the state\n* Active - Aggregated confirmed cases that have not been resolved (Active = Confirmed - Recovered - Deaths)\n* Incident_Rate - confirmed cases per 100,000 persons\n* Mortality_Rate - Number recorded deaths * 100/ Number confirmed cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper Function - Missing data check\ndef missing_data(data):\n    missing = data.isnull().sum()\n    available = data.count()\n    total = (missing + available)\n    percent = (data.isnull().sum()/data.isnull().count()*100).round(4)\n    return pd.concat([missing, available, total, percent], axis=1, keys=['Missing', 'Available', 'Total', 'Percent']).sort_values(['Missing'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data check\nmissing_data(cov_dash_country)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping off columns with 100% missing values and unnecessary ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"cov_dash_country = cov_dash_country.drop(['people_tested','people_hospitalized','iso3'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filtering out the rows with issing latitude and longitude data."},{"metadata":{"trusted":true},"cell_type":"code","source":"cov_dash_country[cov_dash_country.lat.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It seems both these data points belong to cruise ships and not necessarily associated with any country\n* Hence dropping these rows temporarily"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_country = cov_dash_country.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Global Reported Cases till Date**\nTotal number of confirmed cases, deaths reported, revoveries and active cases all across the world"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_country.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.DataFrame(covid_country[[\"confirmed\",\"deaths\",\"recovered\",\"active\"]].sum()).transpose()\nnew_df['mortality_rate'] = covid_country['mortality_rate'].mean()\nnew_df['incident_rate'] = covid_country['incident_rate'].mean()\nnew_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check out below link**\n- [Using Python to create a world map from a list of country names](https://towardsdatascience.com/using-python-to-create-a-world-map-from-a-list-of-country-names-cd7480d03b10)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a world map to show distributions of users \nimport folium\nfrom folium.plugins import MarkerCluster\n#empty map\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\n#for each coordinate, create circlemarker of user percent\nfor i in range(len(covid_country)):\n        lat = covid_country.iloc[i]['lat']\n        long = covid_country.iloc[i]['long']\n        radius=5\n        popup_text = \"\"\"Country : {}<br>\n                    Confimed : {}<br>\n                    Deaths : {}<br>\n                    Recovered : {}<br>\"\"\"\n        popup_text = popup_text.format(covid_country.iloc[i]['country_region'],\n                                   covid_country.iloc[i]['confirmed'],\n                                       covid_country.iloc[i]['deaths'],\n                                       covid_country.iloc[i]['recovered']\n                                   )\n        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n#show the map\nworld_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.choropleth(covid_country, locations=\"country_region\",\n                    color=covid_country[\"confirmed\"], \n                    hover_name=\"country_region\", \n                    hover_data=[\"deaths\"],\n                    locationmode=\"country names\")\n\nfig.update_layout(title_text=\"Confirmed Cases Heat Map (Log Scale)\")\nfig.update_coloraxes(colorscale=\"blues\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 countries with highest confirmed cases\ncovid_country_top20=covid_country.sort_values(\"confirmed\",ascending=False).head(20)\n\nfig = px.bar(covid_country_top20, \n             x=\"country_region\",\n             y=\"confirmed\",\n             orientation='v',\n             height=800,\n             title='Top 20 countries with COVID19 Confirmed Cases',\n            color='country_region')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 countries with highest deaths\ncovid_country_top20=covid_country.sort_values(\"deaths\",ascending=False).head(20)\nfig = px.bar(covid_country_top20, \n             x=\"country_region\",\n             y=\"deaths\",\n             orientation='v',\n             height=800,\n             title='Top 20 countries with COVID19 Deaths',\n            color='country_region')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 countries with highest active cases\ncovid_country_top20=covid_country.sort_values(\"active\",ascending=False).head(20)\nfig = px.bar(covid_country_top20, \n             x=\"country_region\",\n             y=\"active\",\n             orientation='v',\n             height=800,\n             title='Top 20 countries with COVID19 Active Cases',\n            color='country_region')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 countries with highest recovered cases\ncovid_country_top20=covid_country.sort_values(\"recovered\",ascending=False).head(20)\nfig = px.bar(covid_country_top20, \n             x=\"country_region\",\n             y=\"recovered\",\n             orientation='v',\n             height=800,\n             title='Top 20 countries with COVID19 Recovered Cases',\n            color='country_region')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr= covid_country.corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr,cmap=\"YlGnBu\",annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not significant relatioship here based on above pearson's correlation matrix"},{"metadata":{},"cell_type":"markdown","source":"# India COVID19 Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#age_group = pd.read_csv('../input/covid19-in-india/AgeGroupDetails.csv')\nindia_covid19 = pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n#hospital_beds = pd.read_csv('../input/covid19-in-india/HospitalBedsIndia.csv')\n#individual_details = pd.read_csv('../input/covid19-in-india/IndividualDetails.csv')\n#ICMR_details = pd.read_csv('../input/covid19-in-india/ICMRTestingLabs.csv')\n#ICMR_labs = pd.read_csv('../input/covid19-in-india/ICMRTestingLabs.csv')\nstate_testing = pd.read_csv('../input/covid19-in-india/StatewiseTestingDetails.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latlong = pd.read_csv('../input/latlong/LatLong.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the unique entries for each state"},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19['State/UnionTerritory'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\nFrom above unique results it can be found that some of the state names are repeated with special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\nindia_covid19.rename(columns={'State/UnionTerritory': 'State', 'Cured': 'Recovered'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19['State'] = india_covid19['State'].replace({\"Nagaland#\": \"Nagaland\",\"Jharkhand#\":\"Jharkhand\",\"Madhya Pradesh#\":\"Madhya Pradesh\",\n                                                        \"Chandigarh\":\"Punjab\", \"Cases being reassigned to states\":\"Other\", \"Unassigned\":\"Other\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19['State'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19['Confirmed'] = pd.to_numeric(india_covid19['Confirmed'], errors='coerce')\nindia_covid19['Confirmed']=india_covid19['Confirmed'].fillna(0)\nindia_covid19['Confirmed']=india_covid19['Confirmed'].astype('int')\nindia_covid19['Deaths'] = pd.to_numeric(india_covid19['Deaths'], errors='coerce')\nindia_covid19['Deaths']=india_covid19['Deaths'].fillna(0)\nindia_covid19['Deaths']=india_covid19['Deaths'].astype('int')\nindia_covid19['Recovered'] = pd.to_numeric(india_covid19['Recovered'], errors='coerce')\nindia_covid19['Recovered']=india_covid19['Recovered'].fillna(0)\nindia_covid19['Recovered']=india_covid19['Recovered'].astype('int')\nindia_covid19['ConfirmedIndianNational']= india_covid19['ConfirmedIndianNational'].replace(\"-\", 0)\n#india_covid19['ConfirmedIndianNational']=india_covid19['ConfirmedIndianNational'].fillna(0)\nindia_covid19['ConfirmedIndianNational']=india_covid19['ConfirmedIndianNational'].astype('int')\nindia_covid19['ConfirmedForeignNational']= india_covid19['ConfirmedForeignNational'].replace(\"-\", 0)\n#india_covid19['ConfirmedForeignNational']=india_covid19['ConfirmedForeignNational'].fillna(0)\nindia_covid19['ConfirmedForeignNational']=india_covid19['ConfirmedForeignNational'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning up mixed date formats in Date column\n\n# new data frame with split value columns \nnew = india_covid19[\"Date\"].str.split(\"/\", n = 2, expand = True) \n  \n# making separate first name column from new data frame \nindia_covid19[\"Day\"]= new[0] \nindia_covid19['Day']=india_covid19['Day'].astype('int') \n# making separate last name column from new data frame \nindia_covid19[\"Month\"]= new[1]\nindia_covid19['Month']=india_covid19['Month'].astype('int') \n# making separate last name column from new data frame \nindia_covid19[\"Year\"]= 2020\n#india_covid19['Year']=india_covid19['Year'].astype('int') \nindia_covid19.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid19.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping original date column and creating a new cleaned date column\nindia_covid19 = india_covid19.drop(['Date'],axis = 1)\nindia_covid19['Date'] = india_covid19['Year'].map(str) + '-' + india_covid19['Month'].map(str) + '-' + india_covid19['Day'].map(str)\nindia_covid19 = india_covid19.drop(['Year','Month','Day'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing data types to datetime format\nindia_covid19[\"Date\"]=pd.to_datetime(india_covid19[\"Date\"],format='%Y%m%d', errors='ignore')\nindia_covid19[\"Time\"]=pd.to_datetime(india_covid19[\"Time\"], format='%H%M', errors='ignore')\nindia_covid19.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid_final = india_covid19.merge(latlong)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid_final.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"india_covid_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statewise COVID19 statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstatewise = pd.pivot_table(india_covid_final, values=['Confirmed','Deaths','Recovered'], index='State', aggfunc='max')\nstatewise['Recovery Rate'] = statewise['Recovered']*100 / statewise['Confirmed']\nstatewise['Mortality Rate'] = statewise['Deaths']*100 /statewise['Confirmed']\nstatewise = statewise.sort_values(by='Confirmed', ascending= False)\nstatewise.style.background_gradient(cmap='YlOrRd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = pd.pivot_table(india_covid_final, values=['Confirmed','Deaths','Recovered','Latitude','Longitude'], index='State', aggfunc='max')\nstate['Recovery Rate'] = state['Recovered']*100 / state['Confirmed']\nstate['Mortality Rate'] = state['Deaths']*100 /state['Confirmed']\nstate = state.sort_values(by='Confirmed', ascending= False)\nstate.reset_index(level=0, inplace=True)\nstate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#empty map\nindia_map= folium.Map(location=[21, 78], zoom_start=5,tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(india_map)\n#for each coordinate, create circlemarker of user percent\nfor i in range(len(state)):\n        lat = state.iloc[i]['Latitude']\n        long = state.iloc[i]['Longitude']\n        radius=5\n        popup_text = \"\"\"State : {}<br>\n                    Confimed : {}<br>\n                    Deaths : {}<br>\n                    Recovered : {}<br>\"\"\"\n        popup_text = popup_text.format(state.iloc[i]['State'],\n                                   state.iloc[i]['Confirmed'],\n                                       state.iloc[i]['Deaths'],\n                                       state.iloc[i]['Recovered']\n                                   )\n        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n#show the map\nindia_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statewise COVID19 Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\nstate_testing['TotalSamples']=state_testing['TotalSamples'].fillna(0)\nstate_testing['TotalSamples']=state_testing['TotalSamples'].astype('int')\nstate_testing['Positive']=state_testing['Positive'].fillna(0)\nstate_testing['Positive']=state_testing['Positive'].astype('int')\nstate_testing['Negative']=state_testing['Negative'].fillna(0)\n#state_testing['Negative']=state_testing['Negative'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_testing = pd.pivot_table(state_testing, values=['TotalSamples','Positive','Negative'], index='State', aggfunc='max')\nstatewise_testing['Positive_Case_Rate'] = statewise_testing['Positive']*100 / statewise_testing['TotalSamples']\nstatewise_testing['Positive_Case_Rate']=round(statewise_testing['Positive_Case_Rate'].astype('int'),2)\nstatewise_testing = statewise_testing.sort_values(by='TotalSamples', ascending= False)\nstatewise_testing.style.background_gradient(cmap='YlOrRd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It's no brainer that Maharshtra tops this list in all aspect\n* Considering the number of cases Punjab has done a great job when it comes to recovery rate\n* Similarly in terms of mortality rate West Bengal and Gujarat seem to have higher numbers\n* Tamilnadu, Maharshtra, Andhra Pradesh and Rajastan respectively had ramped up their testing capabilities\n* Though being second in the country w.r.t. total confirmed cases Tamilnadu seem to have established the healthcare facility to a higher level which is evident from the lower mortality rate (0.6%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"testing=state_testing.groupby('State')['TotalSamples'].max().sort_values(ascending=False).reset_index()\nfig = px.bar(testing, \n             x=\"TotalSamples\",\n             y=\"State\", \n             orientation='h',\n             height=800,\n             title='Statewise Testing',\n            color='State')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#india_covid_final.to_csv('newcovid.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,10))\nfigure = px.line(india_covid_final, x='Date', y='Confirmed', color='State')\nfigure.update_xaxes(rangeslider_visible=True)\npyo.iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering Exercise"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score,silhouette_samples\nstd=StandardScaler()\n#pd.set_option('display.float_format', lambda x: '%.6f' % x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=statewise[[\"Mortality Rate\",\"Recovery Rate\"]]\n#Standard Scaling since K-Means Clustering is a distance based alogrithm\nX=std.fit_transform(X) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss=[]\nsil=[]\nfor i in range(2,10):\n    clf=KMeans(n_clusters=i,init='k-means++',random_state=64)\n    clf.fit(X)\n    labels=clf.labels_\n    centroids=clf.cluster_centers_\n    sil.append(silhouette_score(X, labels, metric='euclidean'))\n    wcss.append(clf.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,5))\nx=np.arange(2,10)\nax1.plot(x,wcss,marker='o')\nax1.set_xlabel(\"Number of Clusters\")\nax1.set_ylabel(\"Within Cluster Sum of Squares (WCSS)\")\nax1.set_title(\"Elbow Method\")\nx=np.arange(2,10)\nax2.plot(x,sil,marker='o')\nax2.set_xlabel(\"Number of Clusters\")\nax2.set_ylabel(\"Silhouette Score\")\nax2.set_title(\"Silhouette Score Method\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K=4 seem to be the best number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_final=KMeans(n_clusters=4,init='k-means++',random_state=32)\nclf_final.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise[\"Clusters\"]=clf_final.predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_summary = statewise.sort_values(by='Clusters', ascending= False)\ncluster_summary.style.background_gradient(cmap='Purples').format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table1 = pd.pivot_table(statewise, values=['Confirmed', 'Deaths','Recovered'], \n                       index=['Clusters'], aggfunc=np.sum)\ntable1.style.background_gradient(cmap='Greens').format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table2 = pd.pivot_table(statewise, values=['Recovery Rate','Mortality Rate'], \n                       index=['Clusters'], aggfunc=np.mean)\ntable2.style.background_gradient(cmap='Blues').format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise[\"ClusterNo\"] = statewise[\"Clusters\"].astype(str)\nfig = px.scatter(statewise, x=\"Recovery Rate\", y=\"Mortality Rate\", color=\"ClusterNo\",\n                 size='Deaths', hover_data=['Confirmed','Deaths','Recovered'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Day wise summary\ndf = india_covid_final.copy()\ndf['Date'] = pd.to_datetime(df['Date'],format='%Y/%m/%d')\nindia_covid_date = pd.pivot_table(df, values=['Confirmed','Deaths','Recovered'], index='Date', aggfunc='sum')\nindia_covid_date['Recovery Rate'] = india_covid_date['Recovered']*100 / india_covid_date['Confirmed']\nindia_covid_date['Mortality Rate'] = india_covid_date['Deaths']*100 /india_covid_date['Confirmed']\nindia_covid_date.reset_index(level=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', india_covid_date.shape[0]+1)\nindia_covid_date\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,10))\n\n# Plot \nfig = px.line(india_covid_date, x='Date', y='Confirmed')\n\n# Add one more plot\nfig.add_scatter(x=india_covid_date['Date'], y=india_covid_date['Recovered'], mode='lines')\n\n# Add one more plot\nfig.add_scatter(x=india_covid_date['Date'], y=india_covid_date['Deaths'], mode='lines')\n\n# Show plot w/ range slider\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,10))\n\n# Plot \nfig = px.line(india_covid_date, x='Date', y='Recovery Rate')\n\n# Show plot w/ range slider\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,10))\n\n# Plot \nfig = px.line(india_covid_date, x='Date', y='Mortality Rate')\n\n# Show plot w/ range slider\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#india_covid_final.to_csv('newcovid.csv',index=False)\n#india_covid_date.to_csv('covdate.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COVID19 India Predictions"},{"metadata":{},"cell_type":"markdown","source":"## Facebook's Prophet Model Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fb_data = india_covid_date.copy()\nfb_confirm = fb_data[['Date', 'Confirmed']]\nfb_confirm = fb_confirm.rename(columns={'Date': 'ds',\n                        'Confirmed': 'y'})\n\nfb_confirm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time Series Forecasting with Prophet\n# set the uncertainty interval to 95% (the Prophet default is 80%)\nmy_model = Prophet(interval_width=0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model.fit(fb_confirm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataframe\n# Prophet provides the make_future_dataframe helper function\nfuture_dates = my_model.make_future_dataframe(periods=2, freq='MS')\nfuture_dates.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = my_model.predict(future_dates)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ds: the datestamp of the forecasted value\n* yhat: the forecasted value of our metric (in Statistics, yhat is a notation traditionally used to represent the predicted values of a value y)\n* yhat_lower: the lower bound of our forecasts\n* yhat_upper: the upper bound of our forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model.plot(forecast,\n              uncertainty=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation:-\n* The infection trend is exponential and evident india has not yet reached the peak point\n* Towards weekend and beginning of the week the confirmed cases rate seem to be high\n* Movement of people during weekend and week beginning. This could contribute this rise\n* Social distancing not followed in some parts of the country as evident from various web sources and news channels"},{"metadata":{},"cell_type":"markdown","source":"# Pandemic Prediction Model - India(SIR)\n\n- Based on a article published by [Analytics Vidhya](https://medium.com/analytics-vidhya/how-to-predict-when-the-covid-19-pandemic-will-stop-in-your-country-with-python-d6fbb2425a9f)\n#### Helper Functions:\n- Reference Link here [TS UTILS](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/time_series/ts_utils.py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Run all below code before using parametric or logistic prediction\n! pip install pmdarima\n! pip install arch\n## for data\nimport pandas as pd\nimport numpy as np\n\n## for plotting\nimport matplotlib.pyplot as plt\n\n## for stationarity test\nimport statsmodels.api as sm\n\n## for outliers detection\nfrom sklearn import preprocessing, svm\n\n## for autoregressive models\nimport pmdarima\nimport statsmodels.tsa.api as smt\nimport arch\n\n## for deep learning\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\n\n## for prophet\nfrom fbprophet import Prophet\npd.plotting.register_matplotlib_converters()\n\n## for parametric fit\nfrom scipy import optimize\n\n\n\n###############################################################################\n#                         TS ANALYSIS                                         #\n###############################################################################\n'''\nPlot ts with rolling mean and 95% confidence interval with rolling std.\n:parameter\n    :param ts: pandas Series\n    :param window: num for rolling stats\n'''\ndef plot_ts(ts, plot_ma=True, plot_intervals=True, window=30, figsize=(15,5)):\n    rolling_mean = ts.rolling(window=window).mean()\n    rolling_std = ts.rolling(window=window).std()\n    plt.figure(figsize=figsize)\n    plt.title(ts.name)\n    plt.plot(ts[window:], label='Actual values', color=\"black\")\n    if plot_ma:\n        plt.plot(rolling_mean, 'g', label='MA'+str(window), color=\"red\")\n    if plot_intervals:\n        #mean_absolute_error = np.mean(np.abs((ts[window:] - rolling_mean[window:]) / ts[window:])) * 100\n        #deviation = np.std(ts[window:] - rolling_mean[window:])\n        #lower_bound = rolling_mean - (mean_absolute_error + 1.96 * deviation)\n        #upper_bound = rolling_mean + (mean_absolute_error + 1.96 * deviation)\n        lower_bound = rolling_mean - (1.96 * rolling_std)\n        upper_bound = rolling_mean + (1.96 * rolling_std)\n        #plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n        #plt.plot(lower_bound, 'r--')\n        plt.fill_between(x=ts.index, y1=lower_bound, y2=upper_bound, color='lightskyblue', alpha=0.4)\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.show()\n        \n\n\n'''\nTest stationarity by:\n    - running Augmented Dickey-Fuller test wiht 95%\n    - plotting mean and variance of a sample from data\n    - plottig autocorrelation and partial autocorrelation\n'''\ndef test_stationarity_acf_pacf(ts, sample=0.20, maxlag=30, figsize=(15,10)):\n    with plt.style.context(style='bmh'):\n        ## set figure\n        fig = plt.figure(figsize=figsize)\n        ts_ax = plt.subplot2grid(shape=(2,2), loc=(0,0), colspan=2)\n        pacf_ax = plt.subplot2grid(shape=(2,2), loc=(1,0))\n        acf_ax = plt.subplot2grid(shape=(2,2), loc=(1,1))\n        \n        ## plot ts with mean/std of a sample from the first x% \n        dtf_ts = ts.to_frame(name=\"ts\")\n        sample_size = int(len(ts)*sample)\n        dtf_ts[\"mean\"] = dtf_ts[\"ts\"].head(sample_size).mean()\n        dtf_ts[\"lower\"] = dtf_ts[\"ts\"].head(sample_size).mean() + dtf_ts[\"ts\"].head(sample_size).std()\n        dtf_ts[\"upper\"] = dtf_ts[\"ts\"].head(sample_size).mean() - dtf_ts[\"ts\"].head(sample_size).std()\n        dtf_ts[\"ts\"].plot(ax=ts_ax, color=\"black\", legend=False)\n        dtf_ts[\"mean\"].plot(ax=ts_ax, legend=False, color=\"red\", linestyle=\"--\", linewidth=0.7)\n        ts_ax.fill_between(x=dtf_ts.index, y1=dtf_ts['lower'], y2=dtf_ts['upper'], color='lightskyblue', alpha=0.4)\n        dtf_ts[\"mean\"].head(sample_size).plot(ax=ts_ax, legend=False, color=\"red\", linewidth=0.9)\n        ts_ax.fill_between(x=dtf_ts.head(sample_size).index, y1=dtf_ts['lower'].head(sample_size), y2=dtf_ts['upper'].head(sample_size), color='lightskyblue')\n        \n        ## test stationarity (Augmented Dickey-Fuller)\n        adfuller_test = sm.tsa.stattools.adfuller(ts, maxlag=maxlag, autolag=\"AIC\")\n        adf, p, critical_value = adfuller_test[0], adfuller_test[1], adfuller_test[4][\"5%\"]\n        p = round(p, 3)\n        conclusion = \"Stationary\" if p < 0.05 else \"Non-Stationary\"\n        ts_ax.set_title('Dickey-Fuller Test 95%: '+conclusion+' (p-value: '+str(p)+')')\n        \n        ## pacf (for AR) e acf (for MA) \n        smt.graphics.plot_pacf(ts, lags=maxlag, ax=pacf_ax, title=\"Partial Autocorrelation (for AR component)\")\n        smt.graphics.plot_acf(ts, lags=maxlag, ax=acf_ax, title=\"Autocorrelation (for MA component)\")\n        plt.tight_layout()    \n   \n\n\n'''\nDefferenciate ts.\n:parameter\n    :param ts: pandas Series\n    :param lag: num - diff[t] = y[t] - y[t-lag]\n    :param order: num - how many times it has to differenciate: diff[t]^order = diff[t] - diff[t-lag] \n    :param drop_na: logic - if True Na are dropped, else are filled with last observation\n'''\ndef diff_ts(ts, lag=1, order=1, drop_na=True):\n    for i in range(order):\n        ts = ts - ts.shift(lag)\n    ts = ts[(pd.notnull(ts))] if drop_na is True else ts.fillna(method=\"bfill\")\n    return ts\n\n\n\n'''\n'''\ndef undo_diff(ts, first_y, lag=1, order=1):\n    for i in range(order):\n        (24168.04468 - 18256.02366) + a.cumsum()\n        ts = np.r_[ts, ts[lag:]].cumsum()\n    return ts\n\n\n\n'''\nRun Granger test on 2 series\n'''\ndef test_2ts_casuality(ts1, ts2, maxlag=30, figsize=(15,5)):\n    ## prepare\n    dtf = ts1.to_frame(name=ts1.name)\n    dtf[ts2.name] = ts2\n    dtf.plot(figsize=figsize, grid=True, title=ts1.name+\"  vs  \"+ts2.name)\n    plt.show()\n    ## test casuality (Granger test) \n    granger_test = sm.tsa.stattools.grangercausalitytests(dtf, maxlag=maxlag, verbose=False)\n    for lag,tupla in granger_test.items():\n        p = np.mean([tupla[0][k][1] for k in tupla[0].keys()])\n        p = round(p, 3)\n        if p < 0.05:\n            conclusion = \"Casuality with lag \"+str(lag)+\" (p-value: \"+str(p)+\")\"\n            print(conclusion)\n        \n\n\n'''\nDecompose ts into\n    - trend component = moving avarage\n    - seasonality\n    - residuals = y - (trend + seasonality)\n:parameter\n    :param s: num - number of observations per season (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n'''\ndef decompose_ts(ts, s=250, figsize=(20,13)):\n    decomposition = smt.seasonal_decompose(ts, freq=s)\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    residual = decomposition.resid   \n    fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, sharey=False, figsize=figsize)\n    ax[0].plot(ts)\n    ax[0].set_title('Original')\n    ax[0].grid(True) \n    ax[1].plot(trend)\n    ax[1].set_title('Trend')\n    ax[1].grid(True)  \n    ax[2].plot(seasonal)\n    ax[2].set_title('Seasonality')\n    ax[2].grid(True)  \n    ax[3].plot(residual)\n    ax[3].set_title('Residuals')\n    ax[3].grid(True)\n    return {\"trend\":trend, \"seasonal\":seasonal, \"residual\":residual}\n\n\n\n'''\nFind outliers using sklearn unsupervised support vetcor machine.\n:parameter\n    :param ts: pandas Series\n    :param perc: float - percentage of outliers to look for\n:return\n    dtf with raw ts, outlier 1/0 (yes/no), numeric index\n'''\ndef find_outliers(ts, perc=0.01, figsize=(15,5)):\n    ## fit svm\n    scaler = preprocessing.StandardScaler()\n    ts_scaled = scaler.fit_transform(ts.values.reshape(-1,1))\n    model = svm.OneClassSVM(nu=perc, kernel=\"rbf\", gamma=0.01)\n    model.fit(ts_scaled)\n    ## dtf output\n    dtf_outliers = ts.to_frame(name=\"ts\")\n    dtf_outliers[\"index\"] = range(len(ts))\n    dtf_outliers[\"outlier\"] = model.predict(ts_scaled)\n    dtf_outliers[\"outlier\"] = dtf_outliers[\"outlier\"].apply(lambda x: 1 if x==-1 else 0)\n    ## plot\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.set(title=\"Outliers detection: found \"+str(sum(dtf_outliers[\"outlier\"]==1)))\n    ax.plot(dtf_outliers[\"index\"], dtf_outliers[\"ts\"], color=\"black\")\n    ax.scatter(x=dtf_outliers[dtf_outliers[\"outlier\"]==1][\"index\"], y=dtf_outliers[dtf_outliers[\"outlier\"]==1]['ts'], color='red')\n    ax.grid(True)\n    plt.show()\n    return dtf_outliers\n\n\n\n'''\nInterpolate outliers in a ts.\n'''\ndef remove_outliers(ts, outliers_idx, figsize=(15,5)):\n    ts_clean = ts.copy()\n    ts_clean.loc[outliers_idx] = np.nan\n    ts_clean = ts_clean.interpolate(method=\"linear\")\n    ax = ts.plot(figsize=figsize, color=\"red\", alpha=0.5, title=\"Remove outliers\", label=\"original\", legend=True)\n    ts_clean.plot(ax=ax, grid=True, color=\"black\", label=\"interpolated\", legend=True)\n    plt.show()\n    return ts_clean\n\n\n\n###############################################################################\n#                 MODEL DESIGN & TESTING - FORECASTING                        #\n###############################################################################\n'''\nSplit train/test from any given data point.\n:parameter\n    :param ts: pandas Series\n    :param exog: array len(ts) x n regressors\n    :param test: num or str - test size (ex. 0.20) or index position (ex. \"yyyy-mm-dd\", 1000)\n:return\n    ts_train, ts_test, exog_train, exog_test\n'''\ndef split_train_test(ts, exog=None, test=0.20, plot=True, figsize=(15,5)):\n    ## define splitting point\n    if type(test) is float:\n        split = int(len(ts)*(1-test))\n        perc = test\n    elif type(test) is str:\n        split = ts.reset_index()[ts.reset_index().iloc[:,0]==test].index[0]\n        perc = round(len(ts[split:])/len(ts), 2)\n    else:\n        split = test\n        perc = round(len(ts[split:])/len(ts), 2)\n    print(\"--- splitting at index: \", split, \"|\", ts.index[split], \"| test size:\", perc, \" ---\")\n    \n    ## split ts\n    ts_train = ts.head(split)\n    ts_test = ts.tail(len(ts)-split)\n    if plot is True:\n        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=True, figsize=figsize)\n        ts_train.plot(ax=ax[0], grid=True, title=\"Train\", color=\"black\")\n        ts_test.plot(ax=ax[1], grid=True, title=\"Test\", color=\"black\")\n        ax[0].set(xlabel=None)\n        ax[1].set(xlabel=None)\n        plt.show()\n        \n    ## split exog\n    if exog is not None:\n        exog_train = exog[0:split] \n        exog_test = exog[split:]\n        return ts_train, ts_test, exog_train, exog_test\n    else:\n        return ts_train, ts_test\n    \n\n\n'''\nEvaluation metrics for predictions.\n:parameter\n    :param dtf: DataFrame with columns raw values, fitted training values, predicted test values\n:return\n    dataframe with raw ts and forecast\n'''\ndef utils_evaluate_forecast(dtf, title, plot=True, figsize=(20,13)):\n    try:\n        ## residuals\n        dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n        dtf[\"error\"] = dtf[\"ts\"] - dtf[\"forecast\"]\n        dtf[\"error_pct\"] = dtf[\"error\"] / dtf[\"ts\"]\n        \n        ## kpi\n        residuals_mean = dtf[\"residuals\"].mean()  #errore medio nel training\n        residuals_std = dtf[\"residuals\"].std()    #standard dev dell'errore nel training\n        error_mean = dtf[\"error\"].mean()   #errore medio nel test\n        error_std = dtf[\"error\"].std()     #standard dev dell'errore nel test\n        mae = dtf[\"error\"].apply(lambda x: np.abs(x)).mean()  #mean absolute error\n        mape = dtf[\"error_pct\"].apply(lambda x: np.abs(x)).mean()  #mean absolute error %\n        mse = dtf[\"error\"].apply(lambda x: x**2).mean() # mean squared error\n        rmse = np.sqrt(mse)  #root mean squared error\n        \n        ## intervals\n        dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*residuals_std\n        dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*residuals_std\n        dtf[\"pred_int_low\"] = dtf[\"forecast\"] - 1.96*error_std\n        dtf[\"pred_int_up\"] = dtf[\"forecast\"] + 1.96*error_std\n        \n        ## plot\n        if plot==True:\n            fig = plt.figure(figsize=figsize)\n            fig.suptitle(title, fontsize=20)   \n            ax1 = fig.add_subplot(2,2, 1)\n            ax2 = fig.add_subplot(2,2, 2, sharey=ax1)\n            ax3 = fig.add_subplot(2,2, 3)\n            ax4 = fig.add_subplot(2,2, 4)\n            ### training\n            dtf[pd.notnull(dtf[\"model\"])][[\"ts\",\"model\"]].plot(color=[\"black\",\"green\"], title=\"Model\", grid=True, ax=ax1)      \n            ax1.set(xlabel=None)\n            ### test\n            dtf[pd.isnull(dtf[\"model\"])][[\"ts\",\"forecast\"]].plot(color=[\"black\",\"red\"], title=\"Forecast\", grid=True, ax=ax2)\n            ax2.fill_between(x=dtf.index, y1=dtf['pred_int_low'], y2=dtf['pred_int_up'], color='b', alpha=0.2)\n            ax2.fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3)     \n            ax2.set(xlabel=None)\n            ### residuals\n            dtf[[\"residuals\",\"error\"]].plot(ax=ax3, color=[\"green\",\"red\"], title=\"Residuals\", grid=True)\n            ax3.set(xlabel=None)\n            ### residuals distribution\n            dtf[[\"residuals\",\"error\"]].plot(ax=ax4, color=[\"green\",\"red\"], kind='kde', title=\"Residuals Distribution\", grid=True)\n            ax4.set(ylabel=None)\n            plt.show()\n            print(\"Training --> Residuals mean:\", np.round(residuals_mean), \" | std:\", np.round(residuals_std))\n            print(\"Test --> Error mean:\", np.round(error_mean), \" | std:\", np.round(error_std),\n                  \" | mae:\",np.round(mae), \" | mape:\",np.round(mape*100), \"%  | mse:\",np.round(mse), \" | rmse:\",np.round(rmse))\n        \n        return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"conf_int_up\", \n                    \"forecast\",\"error\",\"pred_int_low\",\"pred_int_up\"]]\n    \n    except Exception as e:\n        print(\"--- got error ---\")\n        print(e)\n    \n\n\n'''\nGenerate dates to index predictions.\n:parameter\n    :param start: str - \"yyyy-mm-dd\"\n    :param end: str - \"yyyy-mm-dd\"\n    :param n: num - length of index\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n'''\ndef utils_generate_indexdate(start, end=None, n=None, freq=\"D\"):\n    if end is not None:\n        index = pd.date_range(start=start, end=end, freq=freq)\n    else:\n        index = pd.date_range(start=start, periods=n, freq=freq)\n    index = index[1:]\n    print(\"--- generating index date --> start:\", index[0], \"| end:\", index[-1], \"| len:\", len(index), \"---\")\n    return index\n\n\n\n'''\nPlot unknown future forecast.\n'''\ndef utils_plot_forecast(dtf, zoom=30, figsize=(15,5)):\n    ## interval\n    dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n    dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*dtf[\"residuals\"].std()\n    dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*dtf[\"residuals\"].std()\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n    \n    ## entire series\n    dtf[[\"ts\",\"forecast\"]].plot(color=[\"black\",\"red\"], grid=True, ax=ax[0], title=\"History + Future\")\n    ax[0].fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3) \n          \n    ## focus on last\n    first_idx = dtf[pd.notnull(dtf[\"forecast\"])].index[0]\n    first_loc = dtf.index.tolist().index(first_idx)\n    zoom_idx = dtf.index[first_loc-zoom]\n    dtf.loc[zoom_idx:][[\"ts\",\"forecast\"]].plot(color=[\"black\",\"red\"], grid=True, ax=ax[1], title=\"Zoom on the last \"+str(zoom)+\" observations\")\n    ax[1].fill_between(x=dtf.loc[zoom_idx:].index, y1=dtf.loc[zoom_idx:]['conf_int_low'], \n                       y2=dtf.loc[zoom_idx:]['conf_int_up'], color='b', alpha=0.3)\n    plt.show()\n    return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"forecast\",\"conf_int_up\"]]\n\n\n\n###############################################################################\n#                           RANDOM WALK                                       #\n###############################################################################\n'''\nGenerate a Random Walk process.\n:parameter\n    :param y0: num - starting value\n    :param n: num - length of process\n    :param ymin: num - limit\n    :param ymax: num - limit\n'''\ndef utils_generate_rw(y0, n, sigma, ymin=None, ymax=None):\n    rw = [y0]\n    for t in range(1, n):\n        yt = rw[t-1] + np.random.normal(0,sigma)\n        if (ymax is not None) and (yt > ymax):\n            yt = rw[t-1] - abs(np.random.normal(0,sigma))\n        elif (ymin is not None) and (yt < ymin):\n            yt = rw[t-1] + abs(np.random.normal(0,sigma))\n        rw.append(yt)\n    return rw\n        \n\n \n'''\nSimulate Random Walk from params of a given ts: \n    y[t+1] = y[t] + wn~(0,Ïƒ)\n'''\ndef simulate_rw(ts_train, ts_test, figsize=(15,10)):\n    ## simulate train\n    diff_ts = ts_train - ts_train.shift(1)\n    rw = utils_generate_rw(y0=ts_train[0], n=len(ts_train), sigma=diff_ts.std(), ymin=ts_train.min(), ymax=ts_train.max())\n    dtf_train = ts_train.to_frame(name=\"ts\").merge(pd.DataFrame(rw, index=ts_train.index, columns=[\"model\"]), how='left', left_index=True, right_index=True)\n    \n    ## test\n    rw = utils_generate_rw(y0=ts_train[-1], n=len(ts_test), sigma=diff_ts.std(), ymin=ts_train.min(), ymax=ts_train.max())\n    dtf_test = ts_test.to_frame(name=\"ts\").merge(pd.DataFrame(rw, index=ts_test.index, columns=[\"forecast\"]), \n                                                 how='left', left_index=True, right_index=True)\n    \n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=\"Random Walk Simulation\")\n    return dtf\n\n\n\n'''\nForecast unknown future.\n:parameter\n    :param ts: pandas series\n    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n    :param zoom: for plotting\n'''\ndef forecast_rw(ts, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n    ## fit\n    diff_ts = ts - ts.shift(1)\n    sigma = diff_ts.std()\n    rw = utils_generate_rw(y0=ts[0], n=len(ts), sigma=sigma, ymin=ts.min(), ymax=ts.max())\n    dtf = ts.to_frame(name=\"ts\").merge(pd.DataFrame(rw, index=ts.index, columns=[\"model\"]), \n                                       how='left', left_index=True, right_index=True)\n    \n    ## index\n    index = utils_generate_indexdate(start=ts.index[-1], end=end, n=pred_ahead, freq=freq)\n    \n    ## forecast\n    preds = utils_generate_rw(y0=ts[-1], n=len(index), sigma=sigma, ymin=ts.min(), ymax=ts.max())\n    dtf = dtf.append(pd.DataFrame(data=preds, index=index, columns=[\"forecast\"]))\n    \n    ## plot\n    dtf = utils_plot_forecast(dtf, zoom=zoom)\n    return dtf\n    \n\n\n###############################################################################\n#                        AUTOREGRESSIVE                                       #\n###############################################################################\n'''\nFits Holt-Winters Exponential Smoothing: \n    y[t+i] = (level[t] + i*trend[t]) * seasonality[t]\n:parameter\n    :param ts_train: pandas timeseries\n    :param ts_test: pandas timeseries\n    :param trend: str - \"additive\" (linear), \"multiplicative\" (non-linear)\n    :param seasonal: str - \"additive\" (ex. +100 every 7 days), \"multiplicative\" (ex. x10 every 7 days)\n    :param s: num - number of observations per seasonal (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n    :param alpha: num - the alpha value of the simple exponential smoothing (ex 0.94)\n:return\n    dtf with predictons and the model\n'''\ndef fit_expsmooth(ts_train, ts_test, trend=\"additive\", seasonal=\"multiplicative\", s=None, alpha=0.94, figsize=(15,10)):\n    ## checks\n    check_seasonality = \"Seasonal parameters: No Seasonality\" if (seasonal is None) & (s is None) else \"Seasonal parameters: \"+str(seasonal)+\" Seasonality every \"+str(s)+\" observations\"\n    print(check_seasonality)\n    \n    ## train\n    #alpha = alpha if s is None else 2/(s+1)\n    model = smt.ExponentialSmoothing(ts_train, trend=trend, seasonal=seasonal, seasonal_periods=s).fit(smoothing_level=alpha)\n    dtf_train = ts_train.to_frame(name=\"ts\")\n    dtf_train[\"model\"] = model.fittedvalues\n    \n    ## test\n    dtf_test = ts_test.to_frame(name=\"ts\")\n    dtf_test[\"forecast\"] = model.predict(start=len(ts_train), end=len(ts_train)+len(ts_test)-1)\n    \n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=\"Holt-Winters (\"+str(alpha)+\")\")\n    return dtf, model\n\n\n\n'''\nFits SARIMAX (Seasonal ARIMA with External Regressors):  \n    y[t+1] = (c + a0*y[t] + a1*y[t-1] +...+ ap*y[t-p]) + (e[t] + b1*e[t-1] + b2*e[t-2] +...+ bq*e[t-q]) + (B*X[t])\n:parameter\n    :param ts_train: pandas timeseries\n    :param ts_test: pandas timeseries\n    :param order: tuple - ARIMA(p,d,q) --> p: lag order (AR), d: degree of differencing (to remove trend), q: order of moving average (MA)\n    :param seasonal_order: tuple - (P,D,Q,s) --> s: number of observations per seasonal (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n    :param exog_train: pandas dataframe or numpy array\n    :param exog_test: pandas dataframe or numpy array\n:return\n    dtf with predictons and the model\n'''\ndef fit_sarimax(ts_train, ts_test, order=(1,0,1), seasonal_order=(0,0,0,0), exog_train=None, exog_test=None, figsize=(15,10)):\n    ## checks\n    check_trend = \"Trend parameters: No differencing\" if order[1] == 0 else \"Trend parameters: d=\"+str(order[1])\n    print(check_trend)\n    check_seasonality = \"Seasonal parameters: No Seasonality\" if (seasonal_order[3] == 0) & (np.sum(seasonal_order[0:3]) == 0) else \"Seasonal parameters: Seasonality every \"+str(seasonal_order[3])+\" observations\"\n    print(check_seasonality)\n    check_exog = \"Exog parameters: Not given\" if (exog_train is None) & (exog_test is None) else \"Exog parameters: number of regressors=\"+str(exog_train.shape[1])\n    print(check_exog)\n    \n    ## train\n    model = smt.SARIMAX(ts_train, order=order, seasonal_order=seasonal_order, exog=exog_train, enforce_stationarity=False, enforce_invertibility=False).fit()\n    dtf_train = ts_train.to_frame(name=\"ts\")\n    dtf_train[\"model\"] = model.fittedvalues\n    \n    ## test\n    dtf_test = ts_test.to_frame(name=\"ts\")\n    dtf_test[\"forecast\"] = model.predict(start=len(ts_train), end=len(ts_train)+len(ts_test)-1, exog=exog_test)\n    \n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    title = \"ARIMA \"+str(order) if exog_train is None else \"ARIMAX \"+str(order)\n    title = \"S\"+title+\" x \"+str(seasonal_order) if np.sum(seasonal_order) > 0 else title\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=title)\n    return dtf, model\n\n\n    \n'''\nFind best Seasonal-ARIMAX parameters.\n:parameter\n    :param ts: pandas timeseries\n    :param exog: pandas dataframe or numpy array\n    :param s: num - number of observations per seasonal (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n:return\n    best model\n'''\ndef find_best_sarimax(ts, seasonal=True, stationary=False, s=1, exog=None,\n                      max_p=10, max_d=3, max_q=10,\n                      max_P=10, max_D=3, max_Q=10):\n    best_model = pmdarima.auto_arima(ts, exogenous=exog,\n                                     seasonal=seasonal, stationary=stationary, m=s, \n                                     information_criterion='aic', max_order=20,\n                                     max_p=max_p, max_d=max_d, max_q=max_q,\n                                     max_P=max_P, max_D=max_D, max_Q=max_Q,\n                                     error_action='ignore')\n    print(\"best model --> (p, d, q):\", best_model.order, \" and  (P, D, Q, s):\", best_model.seasonal_order)\n    return best_model.summary()\n\n\n\n'''\nFits GARCH (Generalized Autoregressive Conditional Heteroskedasticity):  \n    y[t+1] = m + e[t+1]\n    e[t+1] = Ïƒ[t+1] * wn~(0,1)\n    ÏƒÂ²[t+1] = c + (a0*ÏƒÂ²[t] + a1*ÏƒÂ²[t-1] +...+ ap*ÏƒÂ²[t-p]) + (b0*eÂ²[t] + b1*e[t-1] + b2*eÂ²[t-2] +...+ bq*eÂ²[t-q])\n:parameter\n    :param ts: pandas timeseries\n    :param order: tuple - ARIMA(p,d,q) --> p:lag order (AR), d:degree of differencing (to remove trend), q:order of moving average (MA)\n'''\ndef fit_garch(ts_train, ts_test, order=(1,0,1), seasonal_order=(0,0,0,0), exog_train=None, exog_test=None, figsize=(15,10)):\n    ## train\n    arima = smt.SARIMAX(ts_train, order=order, seasonal_order=seasonal_order, exog=exog_train, enforce_stationarity=False, enforce_invertibility=False).fit()\n    garch = arch.arch_model(arima.resid, p=order[0], o=order[1], q=order[2], x=exog_train, dist='StudentsT', power=2.0, mean='Constant', vol='GARCH')\n    model = garch.fit(update_freq=seasonal_order[3])\n    dtf_train = ts_train.to_frame(name=\"ts\")\n    dtf_train[\"model\"] = model.conditional_volatility\n    \n    ## test\n    dtf_test = ts_test.to_frame(name=\"ts\")\n    dtf_test[\"forecast\"] = model.forecast(horizon=len(ts_test))\n\n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    title = \"GARCH (\"+str(order[0])+\",\"+str(order[2])+\")\" if order[0] != 0 else \"ARCH (\"+str(order[2])+\")\"\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=title)\n    return dtf, model\n\n\n\n'''\nForecast unknown future.\n:parameter\n    :param ts: pandas series\n    :param model: model object\n    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n    :param zoom: for plotting\n'''\ndef forecast_arima(ts, model, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n    ## fit\n    model = model.fit()\n    dtf = ts.to_frame(name=\"ts\")\n    dtf[\"model\"] = model.fittedvalues\n    dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n    \n    ## index\n    index = utils_generate_indexdate(start=ts.index[-1], end=end, n=pred_ahead, freq=freq)\n    \n    ## forecast\n    preds = model.forecast(len(index))\n    dtf = dtf.append(preds.to_frame(name=\"forecast\"))\n    \n    ## plot\n    dtf = utils_plot_forecast(dtf, zoom=zoom)\n    return dtf\n\n\n\n###############################################################################\n#                            RNN                                              #\n###############################################################################\n'''\nPlot loss and metrics of keras training.\n'''\ndef utils_plot_keras_training(training):\n    metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,3))\n    \n    ## training\n    ax[0].set(title=\"Training\")\n    ax11 = ax[0].twinx()\n    ax[0].plot(training.history['loss'], color='black')\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss', color='black')\n    for metric in metrics:\n        ax11.plot(training.history[metric], label=metric)\n    ax11.set_ylabel(\"Score\", color='steelblue')\n    ax11.legend()\n    \n    ## validation\n    ax[1].set(title=\"Validation\")\n    ax22 = ax[1].twinx()\n    ax[1].plot(training.history['val_loss'], color='black')\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Loss', color='black')\n    for metric in metrics:\n        ax22.plot(training.history['val_'+metric], label=metric)\n    ax22.set_ylabel(\"Score\", color=\"steelblue\")\n    plt.show()\n    \n    \n    \n'''\nPreprocess a ts partitioning into X and y.\n:parameter\n    :param ts: pandas timeseries\n    :param s: num - number of observations per seasonal (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n    :param scaler: sklearn scaler object - if None is fitted\n    :param exog: pandas dataframe or numpy array\n:return\n    X, y, scaler\n'''\ndef utils_preprocess_ts(ts, s, scaler=None, exog=None):\n    ## scale\n    if scaler is None:\n        scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n    ts_preprocessed = scaler.fit_transform(ts.values.reshape(-1,1)).reshape(-1)        \n    \n    ## create X,y for train\n    ts_preprocessed = kprocessing.sequence.TimeseriesGenerator(data=ts_preprocessed, \n                                                               targets=ts_preprocessed, \n                                                               length=s, batch_size=1)\n    lst_X, lst_y = [], []\n    for i in range(len(ts_preprocessed)):\n        xi, yi = ts_preprocessed[i]\n        lst_X.append(xi)\n        lst_y.append(yi)\n    X = np.array(lst_X)\n    y = np.array(lst_y)\n    return X, y, scaler\n\n\n\n'''\nGet fitted values.\n'''\ndef utils_fitted_lstm(ts, model, scaler, exog=None):\n    ## scale\n    ts_preprocessed = scaler.fit_transform(ts.values.reshape(-1,1)).reshape(-1) \n    \n    ## create Xy, predict = fitted\n    s = model.input_shape[-1]\n    lst_fitted = [np.nan]*s\n    for i in range(len(ts_preprocessed)):\n        end_ix = i + s\n        if end_ix > len(ts_preprocessed)-1:\n            break\n        X = ts_preprocessed[i:end_ix]\n        X = np.array(X)\n        X = np.reshape(X, (1,1,X.shape[0]))\n        fit = model.predict(X)\n        fit = scaler.inverse_transform(fit)[0][0]\n        lst_fitted.append(fit)\n    return np.array(lst_fitted)\n\n\n\n'''\nPredict ts using previous predictions.\n'''\ndef utils_predict_lstm(ts, model, scaler, pred_ahead, exog=None):\n    ## scale\n    s = model.input_shape[-1]\n    ts_preprocessed = list(scaler.fit_transform(ts[-s:].values.reshape(-1,1))) \n    \n    ## predict, append, re-predict\n    lst_preds = []\n    for i in range(pred_ahead):\n        X = np.array(ts_preprocessed[len(ts_preprocessed)-s:])\n        X = np.reshape(X, (1,1,X.shape[0]))\n        pred = model.predict(X)\n        ts_preprocessed.append(pred)\n        pred = scaler.inverse_transform(pred)[0][0]\n        lst_preds.append(pred)\n    return np.array(lst_preds)\n\n\n\n'''\nFit Long short-term memory neural network.\n:parameter\n    :param ts: pandas timeseries\n    :param exog: pandas dataframe or numpy array\n    :param s: num - number of observations per seasonal (ex. 7 for weekly seasonality with daily data, 12 for yearly seasonality with monthly data)\n:return\n    generator, scaler \n'''\ndef fit_lstm(ts_train, ts_test, model, exog=None, s=20, figsize=(15,5)):\n    ## check\n    print(\"Seasonality: using the last\", s, \"observations to predict the next 1\")\n    \n    ## preprocess train\n    X_train, y_train, scaler = utils_preprocess_ts(ts_train, scaler=None, exog=exog, s=s)\n    \n    ## lstm\n    if model is None:\n        model = models.Sequential()\n        model.add( layers.LSTM(input_shape=X_train.shape[1:], units=50, activation='relu', return_sequences=False) )\n        model.add( layers.Dense(1) )\n        model.compile(optimizer='adam', loss='mean_absolute_error')\n    \n    ## train\n    print(model.summary())\n    training = model.fit(x=X_train, y=y_train, batch_size=1, epochs=100, shuffle=True, verbose=0, validation_split=0.3)\n    utils_plot_keras_training(training)\n    \n    dtf_train = ts_train.to_frame(name=\"ts\")\n    dtf_train[\"model\"] = utils_fitted_lstm(ts_train, training.model, scaler, exog)\n    dtf_train[\"model\"] = dtf_train[\"model\"].fillna(method='bfill')\n    \n    ## test\n    preds = utils_predict_lstm(ts_train[-s:], training.model, scaler, pred_ahead=len(ts_test), exog=None)\n    dtf_test = ts_test.to_frame(name=\"ts\").merge(pd.DataFrame(data=preds, index=ts_test.index, columns=[\"forecast\"]),\n                                                 how='left', left_index=True, right_index=True)\n    \n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=\"LSTM (memory:\"+str(s)+\")\")\n    return dtf, training.model\n\n\n\n'''\nForecast unknown future.\n:parameter\n    :param ts: pandas series\n    :param model: model object\n    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n    :param zoom: for plotting\n'''\ndef forecast_lstm(ts, model, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n    ## fit\n    s = model.input_shape[-1]\n    X, y, scaler = utils_preprocess_ts(ts, scaler=None, exog=None, s=s)\n    training = model.fit(x=X, y=y, batch_size=1, epochs=100, shuffle=True, verbose=0, validation_split=0.3)\n    dtf = ts.to_frame(name=\"ts\")\n    dtf[\"model\"] = utils_fitted_lstm(ts, training.model, scaler, None)\n    dtf[\"model\"] = dtf[\"model\"].fillna(method='bfill')\n    \n    ## index\n    index = utils_generate_indexdate(start=ts.index[-1], end=end, n=pred_ahead, freq=freq)\n    \n    ## forecast\n    preds = utils_predict_lstm(ts[-s:], training.model, scaler, pred_ahead=len(index), exog=None)\n    dtf = dtf.append(pd.DataFrame(data=preds, index=index, columns=[\"forecast\"]))\n    \n    ## plot\n    dtf = utils_plot_forecast(dtf, zoom=zoom)\n    return dtf\n\n\n\n###############################################################################\n#                           PROPHET                                           #\n###############################################################################\n'''\nFits prophet on Business Data:\n    y = trend + seasonality + holidays\n:parameter\n    :param dtf_train: pandas Dataframe with columns 'ds' (dates), 'y' (values), 'cap' (capacity if growth=\"logistic\"), other additional regressor\n    :param dtf_test: pandas Dataframe with columns 'ds' (dates), 'y' (values), 'cap' (capacity if growth=\"logistic\"), other additional regressor\n    :param lst_exog: list - names of variables\n    :param freq: str - \"D\" daily, \"M\" monthly, \"Y\" annual, \"MS\" monthly start ...\n:return\n    dtf with predictons and the model\n'''\ndef fit_prophet(dtf_train, dtf_test, lst_exog=None, model=None, freq=\"D\", figsize=(15,10)):\n    ## setup prophet\n    if model is None:\n        model = Prophet(growth=\"linear\", changepoints=None, n_changepoints=25, seasonality_mode=\"multiplicative\",\n                yearly_seasonality=\"auto\", weekly_seasonality=\"auto\", daily_seasonality=\"auto\",\n                holidays=None)\n    if lst_exog != None:\n        for regressor in lst_exog:\n            model.add_regressor(regressor)\n    \n    ## train\n    model.fit(dtf_train)\n    \n    ## test\n    dtf_prophet = model.make_future_dataframe(periods=len(dtf_test), freq=freq, include_history=True)\n    \n    if model.growth == \"logistic\":\n        dtf_prophet[\"cap\"] = dtf_train[\"cap\"].unique()[0]\n    \n    if lst_exog != None:\n        dtf_prophet = dtf_prophet.merge(dtf_train[[\"ds\"]+lst_exog], how=\"left\")\n        dtf_prophet.iloc[-len(dtf_test):][lst_exog] = dtf_test[lst_exog].values\n    \n    dtf_prophet = model.predict(dtf_prophet)\n    dtf_train = dtf_train.merge(dtf_prophet[[\"ds\",\"yhat\"]], how=\"left\").rename(columns={'yhat':'model', 'y':'ts'}).set_index(\"ds\")\n    dtf_test = dtf_test.merge(dtf_prophet[[\"ds\",\"yhat\"]], how=\"left\").rename(columns={'yhat':'forecast', 'y':'ts'}).set_index(\"ds\")\n    \n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, title=\"Prophet\")\n    return dtf, model\n    \n\n\n'''\nForecast unknown future.\n:parameter\n    :param ts: pandas series\n    :param model: model object\n    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n    :param zoom: for plotting\n'''\ndef forecast_prophet(dtf, model, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n    ## fit\n    model.fit(dtf)\n    \n    ## index\n    index = utils_generate_indexdate(start=dtf[\"ds\"].values[-1], end=end, n=pred_ahead, freq=freq)\n    \n    ## forecast\n    dtf_prophet = model.make_future_dataframe(periods=len(index), freq=freq, include_history=True)\n    dtf_prophet = model.predict(dtf_prophet)\n    dtf = dtf.merge(dtf_prophet[[\"ds\",\"yhat\"]], how=\"left\").rename(columns={'yhat':'model', 'y':'ts'}).set_index(\"ds\")\n    preds = pd.DataFrame(data=index, columns=[\"ds\"])\n    preds = preds.merge(dtf_prophet[[\"ds\",\"yhat\"]], how=\"left\").rename(columns={'yhat':'forecast'}).set_index(\"ds\")\n    dtf = dtf.append(preds)\n    \n    ## plot\n    dtf = utils_plot_forecast(dtf, zoom=zoom)\n    return dtf\n\n\n\n###############################################################################\n#                    PARAMETRIC CURVE FITTING                                 #\n###############################################################################\n'''\nFits a custom function.\n:parameter\n    :param X: array\n    :param y: array\n    :param f: function to fit (ex. logistic: f(X) = capacity / (1 + np.exp(-k*(X - midpoint)))\n                                or gaussian: f(X) = a * np.exp(-0.5 * ((X-mu)/sigma)**2)   )\n    :param kind: str - \"logistic\", \"gaussian\" or None\n    :param p0: array or list of initial parameters (ex. for logistic p0=[np.max(ts), 1, 1])\n:return\n    optimal params\n'''\ndef fit_curve(X, y, f=None, kind=None, p0=None):\n    ## define f(x) if not specified\n    if f is None:\n        if kind == \"logistic\":\n            f = lambda p,X: p[0] / (1 + np.exp(-p[1]*(X-p[2])))\n        elif find == \"gaussian\":\n            f = lambda p,X: p[0] * np.exp(-0.5 * ((X-p[1])/p[2])**2)\n    \n    ## find optimal parameters\n    model, cov = optimize.curve_fit(f, X, y, maxfev=10000, p0=p0)\n    return model\n    \n\n\n'''\nPredict with optimal parameters.\n'''\ndef utils_predict_curve(model, f, X):\n    fitted = f(X, model[0], model[1], model[2])\n    return fitted\n\n\n\n'''\nPlot parametric fitting.\n'''\ndef utils_plot_parametric(dtf, zoom=30, figsize=(15,5)):\n    ## interval\n    dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n    dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*dtf[\"residuals\"].std()\n    dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*dtf[\"residuals\"].std()\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n    \n    ## entire series\n    dtf[\"ts\"].plot(marker=\".\", linestyle='None', ax=ax[0], title=\"Parametric Fitting\", color=\"black\")\n    dtf[\"model\"].plot(ax=ax[0], color=\"green\", label=\"model\", legend=True)\n    dtf[\"forecast\"].plot(ax=ax[0], grid=True, color=\"red\", label=\"forecast\", legend=True)\n    ax[0].fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3)\n   \n    ## focus on last\n    first_idx = dtf[pd.notnull(dtf[\"forecast\"])].index[0]\n    first_loc = dtf.index.tolist().index(first_idx)\n    zoom_idx = dtf.index[first_loc-zoom]\n    dtf.loc[zoom_idx:][\"ts\"].plot(marker=\".\", linestyle='None', ax=ax[1], color=\"black\", \n                                  title=\"Zoom on the last \"+str(zoom)+\" observations\")\n    dtf.loc[zoom_idx:][\"model\"].plot(ax=ax[1], color=\"green\")\n    dtf.loc[zoom_idx:][\"forecast\"].plot(ax=ax[1], grid=True, color=\"red\")\n    ax[1].fill_between(x=dtf.loc[zoom_idx:].index, y1=dtf.loc[zoom_idx:]['conf_int_low'], \n                       y2=dtf.loc[zoom_idx:]['conf_int_up'], color='b', alpha=0.3)\n    plt.show()\n    return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"forecast\",\"conf_int_up\"]]\n\n\n\n'''\nForecast unknown future.\n:parameter\n    :param ts: pandas series\n    :param f: function\n    :param model: list of optim params\n    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n    :param zoom: for plotting\n'''\ndef forecast_curve(ts, f, model, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n    ## fit\n    fitted = utils_predict_curve(model, f, X=np.arange(len(ts)))\n    dtf = ts.to_frame(name=\"ts\")\n    dtf[\"model\"] = fitted\n    \n    ## index\n    index = utils_generate_indexdate(start=ts.index[-1], end=end, n=pred_ahead, freq=freq)\n    \n    ## forecast\n    preds = utils_predict_curve(model, f, X=np.arange(len(ts)+1, len(ts)+1+len(index)))\n    dtf = dtf.append(pd.DataFrame(data=preds, index=index, columns=[\"forecast\"]))\n    \n    ## plot\n    utils_plot_parametric(dtf, zoom=zoom)\n    return dtf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For parametric fitting\nfrom scipy import optimize\ndf_new = india_covid_date.copy()\ndtf = df_new[['Date','Confirmed']]\ndtf = dtf.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create new cases column\n# new cases(t) = total(t) â€” total(t-1)\n## create new cases column\ndtf[\"new\"] = dtf[\"Confirmed\"] - dtf[\"Confirmed\"].shift(1)\ndtf[\"new\"] = dtf[\"new\"].fillna(method='bfill')\ndtf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model design\nThe model is a function of the independent variable and one or more coefficients (or parameters). The error represents random variations in the data that follow a specific probability distribution (usually Gaussian). The objective of curve fitting is to find the optimal combination of parameters that minimize the error. Here we are dealing with time series, therefore the independent variable is time. In mathematical terms:\n\n* y = f(time) + error"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Various functions with random parameters\n\n'''\nLinear function: f(x) = a + b*x\n'''\ndef f(x):\n    return 1 + 28000*x\n\ny_linear = f(x=np.arange(len(dtf)))\n'''\nExponential function: f(x) = a + b^x\n'''\ndef f(x):\n    return 1 + 1.066**x\n\ny_exponential = f(x=np.arange(len(dtf)))\n'''\nLogistic function: f(x) = a / (1 + e^(-b*(x-c)))\n'''\ndef f(x): \n    return 6800000 / (1 + np.exp(-0.3*(x-150)))\n\ny_logistic = f(x=np.arange(len(dtf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13,5))\nax.scatter(dtf[\"Confirmed\"].index, dtf[\"Confirmed\"].values, color=\"black\")\nax.plot(dtf[\"Confirmed\"].index, y_linear, label=\"linear\", color=\"red\")\nax.plot(dtf[\"Confirmed\"].index, y_exponential, label=\"exponential\", color=\"green\")\nax.plot(dtf[\"Confirmed\"].index, y_logistic, label=\"logistic\", color=\"blue\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Exponential model seem to fit the model better but the drawback here is it caanot be exponential foreever as the infection rate cannot go beyong the total population\n* There has to be a saturation and dipping point.\n* So guassian function can be used to explain this case"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dtf.to_csv('param.csv',index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Guassian function with random parameters\n\n'''\nGaussian function: f(x) = a * e^(-0.5 * ((x-Î¼)/Ïƒ)**2)\n'''\ndef f(x):\n    return 100000 * np.exp(-0.015 * ((x-225)/6)**2)\n\ny_gaussian = f(x=np.arange(len(dtf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13,5))\nax.bar(dtf[\"new\"].index, dtf[\"new\"].values, color=\"black\")\nax.plot(dtf[\"new\"].index, y_gaussian, color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import optimize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Function\n# https://docs.scipy.org/doc/scipy/reference/optimize.html\n'''\nFunction to fit. In this case logistic function:\n    f(x) = capacity / (1 + e^-k*(x - midpoint) )\n'''\ndef f(X, c, k, m):\n    y = c / (1 + np.exp(-k*(X-m)))\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit\nmodel_l = fit_curve(X=np.arange(len(dtf[\"Confirmed\"])), y=dtf[\"Confirmed\"].values, f=f, p0=[np.max(dtf[\"Confirmed\"]), 1, 1])\nmodel_l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Forecast - Logistic Function\npreds = forecast_curve(dtf[\"Confirmed\"], f, model_l, pred_ahead=30, end=None, freq=\"D\", zoom=14, figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 120 days forecase to observe the peak/saturation point\npreds90 = forecast_curve(dtf[\"Confirmed\"], f, model_l, pred_ahead=120, end=None, freq=\"D\", zoom=14, figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nFunction to fit. In this case gaussian function:\n    f(x) = a * e^(-0.5 * ((x-Î¼)/Ïƒ)**2)\n'''\ndef f(X, a, b, c):\n    y = a * np.exp(-0.4 * ((X-b)/c)**2)\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = fit_curve(X=np.arange(len(dtf[\"new\"])), y=dtf[\"new\"].values, f=f, p0=[1, np.mean(dtf[\"new\"]), np.std(dtf[\"new\"])])\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Forecast\npreds = forecast_curve(dtf[\"new\"], f, model, pred_ahead=120, end=None, freq=\"D\", zoom=15, figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 90 days forecast to observe the recovery phase\npreds90p = forecast_curve(dtf[\"new\"], f, model, pred_ahead=270, end=None, freq=\"D\", zoom=14, figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SARS Analysis for comparison\n* This is to compare SARS data with COVID19\n* Work in progress"},{"metadata":{"trusted":true},"cell_type":"code","source":"sars = pd.read_csv(\"../input/sars-outbreak-2003-complete-dataset/sars_2003_complete_dataset_clean.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\nsars.rename(columns={'Cumulative number of case(s)': 'Total_Cases', 'Number of deaths': 'Deaths', 'Number recovered':'Recovered'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sars['Country'] = sars['Country'].replace({\"Hong Kong SAR, China\": \"HongKong\",\"Taiwan, China\":\"Taiwan\",\"Republic of Ireland\":\"Ireland\",\n                                                        \"Republic of Korea\":\"Korea\", \"Macao SAR, China\":\"Macao\", \"Russian Federation\":\"Russia\",\"Viet Nam\":\"Vietnam\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sars.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country level aggregation\nsars_country = pd.pivot_table(sars, values=['Total_Cases', 'Deaths', 'Recovered'], index='Country', aggfunc='sum')\nsars_country = sars_country.sort_values(by='Total_Cases', ascending= False)\nsars_country.style.background_gradient(cmap='YlOrRd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Day wise summary\nsars_df = sars.copy()\nsars_df['Date'] = pd.to_datetime(sars_df['Date'],format='%Y/%m/%d')\nsars_df_date = pd.pivot_table(sars_df, values=['Total_Cases', 'Deaths', 'Recovered'], index='Date', aggfunc='sum')\nsars_df_date['Recovery Rate'] = sars_df_date['Recovered']*100 / sars_df_date['Total_Cases']\nsars_df_date['Mortality Rate'] = sars_df_date['Deaths']*100 /sars_df_date['Total_Cases']\nsars_df_date.reset_index(level=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create new cases column\n# new cases(t) = total(t) â€” total(t-1)\n## create new cases column\nsars_new = sars_df_date[['Date','Total_Cases']]\nsars_new = sars_new.set_index('Date')\nsars_new[\"new\"] = sars_new[\"Total_Cases\"] - sars_new[\"Total_Cases\"].shift(1)\nsars_new[\"new\"] = sars_new[\"new\"].fillna(method='bfill')\nsars_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot \nfig = px.bar(sars_new, x=sars_new.index, y='new')\n# Show plot w/ range slider\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,10))\n\n# Plot \nfig = px.line(sars_df_date, x='Date', y='Total_Cases')\n\n# Add one more plot\nfig.add_scatter(x=sars_df_date['Date'], y=sars_df_date['Recovered'], mode='lines')\n\n# Add one more plot\nfig.add_scatter(x=sars_df_date['Date'], y=sars_df_date['Deaths'], mode='lines')\n\n# Show plot w/ range slider\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Params:\n    def __init__(self, c, n, sigma, gamma, r_zero):\n        self.c = c\n        self.N = n\n        self.sigma = sigma\n        self.gamma = gamma\n        self.r_zero = r_zero","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions - Calculations\n\ndef seir_function(t, y, params):\n    \"\"\"\n    dS / dt = -beta * S * I / N\n    dE / dt = +beta * S * I / N - sigma * E\n    dI / dt = +sigma * E - gamma * I + c * R * I / N\n    dR / dt = gamma * I - c * R * I / N\n    yprime = [dS / dt  dE / dt dI / dt   dRdt]\n    input:\n      t current time\n      y vector of current soln values\n      y(1) = S, y(2) = E, y(3) = I, y(4) = R\n    parameters in \"params\"\n      beta, N, sigma, gamma, c, R_zero_array(table of values)\n    output: (col vector)\n      yprime(1) = dS / dt\n      yprime(2) = dE / dt\n      yprime(3) = dI / dt\n      yprime(4) = dR / dt\n    \"\"\"\n    R_zero_array = params.r_zero\n    \n    min_t = np.min(R_zero_array[:, 0])\n    max_t = np.max(R_zero_array[:, 0])\n    t_val = max(min_t, min(t, max_t))\n    \n    R_zero = np.interp(t_val, R_zero_array[:, 0], R_zero_array[:, 1])\n    \n    gamma = params.gamma\n    \n    beta = R_zero * gamma\n    \n    N = params.N\n    sigma = params.sigma\n    c = params.c\n    \n    S = y[0]\n    E = y[1]\n    I = y[2]\n    R = y[3]\n    \n    yprime = np.zeros(4)\n    \n    yprime[0] = -beta * S * I / N\n    yprime[1] = +beta * S * I / N - sigma * E\n    yprime[2] = +sigma * E - gamma * I + c * R * I / N\n    yprime[3] = gamma * I - c * R * I / N\n    return yprime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', india_covid_date.shape[0]+1)\nindia_covid_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThis code is a python version of the oringal Matlab/Octave code from Peter Forsyth\n(see: https://cs.uwaterloo.ca/~paforsyt/SEIR.html)\nparameters.py for SEIR model\nS = susceptible population\nE = Exposed (infected, not yet infectious)\nI = Infectious (now can infect others)\nR = Removed (got sick, now recovered and immune, or died :( )\nN = total population = (S + E + I + R)\nnote: added cRI/N term:  disease\nmutates, can cause reinfection, or immunity lost\nThis assumes that mutated form jumps to Infected population\nCan also assume that mutated form jumps to Exposed population\nFor now, we assume c=0 (no mutation has been observed)\ndS/dt = -beta*S*I/N\ndE/dt = +beta*S*I/N - sigma*E\ndI/dt = +sigma*E -gamma*I + c*R*I/N\ndR/dt = gamma*I -c*R*I/N\nthis file passes seir_function in the calculations module to the ode solver\node systen is specified in the calculations module\n\"\"\"\nimport numpy as np\n#import parameters as parameters\nfrom scipy import integrate\n#from calculations_module import seir_function\nimport matplotlib.pyplot as plt\n\n#Reference:\n# https://towardsdatascience.com/infection-modeling-part-1-87e74645568a\n# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_India\n# https://sites.me.ucsb.edu/~moehlis/APC514/tutorials/tutorial_seasonal/node4.html\n# https://www.idmod.org/docs/hiv/model-seir.html\n\nS_0 = 168690 #India, excluding initial infected, exposed population,\n\nI_0 = 3  # initial infected\n\nE_0 = 27. * I_0  # initial exposed\n\nR_0 = 3  # initial recovered (not to be confused with R_zero, below)\n# initially, no one has recovered\n\nc = 0.0  # no mutation (yet)\n# maybe later...still a mystery\n\n\n\nN = S_0 + I_0 + E_0 + R_0  # N = total population\n\nsigma = 1. / 5.1  # https://doi.org/10.1056/NEJMoa2001316 (2020).\n\ngamma = 1. / 18.  # https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/newsÃ¢â‚¬â€œwuhan-coronavirus\n\"\"\"\n R_zero = number of people infected by each infectious person\n          this has nothing to do with \"R\" = removed above\n          or R_0 (initial value of recovered)\n          but is common terminology (confusing, but usual notation)\n     time dependent, starts offf large, than drops with\n         time due to public health actions (i.e. quarantine, social distancing)\n    R_zero > 1, cases increase\n    R_zero < 1 cases peak and then drop off \n      R_zero declining with time https://www.nature.com/articles/s41421-020-0148-0\n      beta = R_zero*gammma (done in \"seir.m\" )\n \n     table of:   time(days)  R_zero\n                  ....     ....\n                  ....     ....\n                  ....     ....\n       linearly interpolate between times\n       Note: this is different from Wang et al (2020), which assumes\n             piecewise constant values for R_zero\n\"\"\"\nr_zero_array = np.zeros([6, 2])\nr_zero_array[0, :] = [0.0,  3.0]# t=0 days    R_zero = 3.0\nr_zero_array[1, :] = [20.0,  2.6]# t = 60 days R_zero = 2.6\nr_zero_array[2, :] = [70.0,  1.9]# t = 70 days R_zero = 1.9\nr_zero_array[3, :] = [84.0,  1.0]# t = 84 days R_zero = 1.0\nr_zero_array[4, :] = [90.0,  .50]# t = 90 days R_zero = .50\nr_zero_array[5, :] = [1000, .50]# t = 1000 days R_zero =.50\n\nparams = Params(c, N, sigma, gamma, r_zero_array)\n\nt_0 = 0\ntspan = np.linspace(t_0, 181, 180)  # time in days\n\ny_init = np.zeros(4)\ny_init[0] = S_0\ny_init[1] = E_0\ny_init[2] = I_0\ny_init[3] = R_0\n\n\ndef seir_with_params(t, y):\n    return seir_function(t, y, params)\n\n\nr = integrate.ode(seir_with_params).set_integrator(\"dopri5\")\nr.set_initial_value(y_init, t_0)\ny = np.zeros((len(tspan), len(y_init)))\ny[0, :] = y_init  # array for solution\nfor i in range(1, 180):\n    y[i, :] = r.integrate(tspan[i])\n    if not r.successful():\n        raise RuntimeError(\"Could not integrate\")\n\n\nfig, axes = plt.subplots(ncols=2)\naxes[0].plot(tspan, y[:, 0], color=\"b\", label=\"S: susceptible\")\naxes[1].plot(tspan, y[:, 1], color=\"r\", label=\"E: exposed\")\naxes[0].set(xlabel=\"time (days)\", ylabel=\"S: susceptible\")\naxes[1].set(xlabel=\"time (days)\", ylabel=\"E: exposed\")\n\naxes[0].legend()\naxes[1].legend()\nplt.show()\n\nfig, axes = plt.subplots(ncols=2)\naxes[0].plot(tspan, y[:, 2], color=\"b\", label=\"I: infectious\")\naxes[1].plot(tspan, y[:, 3], color=\"r\", label=\"R: recovered\")\naxes[0].set(xlabel=\"time (days)\", ylabel=\"I: infectious\")\naxes[1].set(xlabel=\"time (days)\", ylabel=\"R: recovered\")\naxes[0].legend()\naxes[1].legend()\nplt.show()\n\ntotal_cases = y[:, 1] + y[:, 2] + y[:, 3]\ntotal_cases_active = y[:, 1] + y[:, 2]\n\nfig, ax = plt.subplots()\nax.plot(tspan, total_cases, color=\"b\", label=\"E+I+R: Total cases\")\nax.plot(tspan, total_cases_active, color=\"r\", label=\"E+I: Active cases\")\nax.set(xlabel=\"time (days)\", ylabel=\"Patients\", title='Cumulative and active cases')\nplt.legend()\nplt.show()\n\nnsteps = np.size(tspan)\nS_end = y[nsteps - 1, 0]\nE_end = y[nsteps - 1, 1]\nI_end = y[nsteps - 1, 2]\nR_end = y[nsteps - 1, 3]\n\ntotal = S_end + E_end + I_end + R_end\n\nfrom datetime import datetime, timedelta\nspecific_date = datetime(2020, 1, 30)\nnew_date = specific_date + timedelta(tspan[nsteps-1])\n\n\nprint('time (days): % 2d' %tspan[nsteps-1])\n\nprint('total population: % 2d' %total)\n\nprint('initial infected: % 2d' %I_0)\n\nprint('total cases (E+I+R) at t= % 2d : % 2d' %(tspan[nsteps-1], E_end + I_end + R_end))\n\nprint('Recovered at t=  % 2d : % 2d \\n' %(tspan[nsteps-1], R_end))\nprint('Infected (infectious) at t= % 2d : % 2d \\n' %(tspan[nsteps-1],I_end))\nprint('Exposed (non-infectious) at t= % 2d : % 2d \\n ' %(tspan[nsteps-1], E_end))\nprint('Susceptable at t= % 2d : % 2d \\n ' %(tspan[nsteps-1], S_end))\nprint ('Data for t= ' ,new_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_newcurve(X, y, f=None, kind=None, p0=None):\n    ## define f(x) if not specified\n    if f is None:\n        if kind == \"logistic\":\n            f = lambda p,X: p[0] / (1 + np.exp(-p[1]*(X-p[2])))\n        elif find == \"gaussian\":\n            f = lambda p,X: p[0] * np.exp(-0.5 * ((X-p[1])/p[2])**2)\n    \n    ## find optimal parameters\n    model, cov = optimize.curve_fit(f, X, y, maxfev=10000, p0=p0)\n    return model\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n### Global\n* Countries or economies which open with a delay (smart/data driven) might bounce back fast\n* Economies which open sooner might boom for a while but will face the wrath of second wave of COVID19\n  * Second wave can adversely affect the economy as well as the population/skillsets\n  * Possibly might end up in L-shaped economic recovery [Recession Shapes](https://en.wikipedia.org/wiki/Recession_shapes)\n\n### India\n* Based on the predictions of logistic and parametric curve fitting models India might reach the plateau somewhere between end of June to mid July\n* These predictions are based on data collected on data and can change based on government policies and actions\n* Strict social distancing will help lower the transmission and help recover the economy in a healthy manner\n\n\n### Few possible developments during/post COVID19\n* Health monitoring devices for people of all economic background\n* Remote/Online education for rural population\n* Staggered operation of work place/manufacturing sector\n  * 4 days of work with fixed group may be extra work hours with pay and then 10-14 days of quarantine\n  * All this with less workforce at any given point\n  * Since the incubation period of COVID19 virus is 4-5 days after which it enters transmission/infectious phase\n* Tools/creative ways to follow strict social distancing in public places (Restaurants, Markets, Malls, Museums etc.)\n* Door delivery of essentials - more local players"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}