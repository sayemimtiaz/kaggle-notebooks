{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Logistic Regression (and some Linear Regression)\nThis is a somewhat longer introduction to Logistic Regression and how it is connected to Deep Learning.\n\nThis notebook is also available as a [blog post](https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88) and on [Github](https://github.com/dennisbakhuis/Tutorials/tree/master/Logistic_Regression).\n\nIf you are new to Python, here is a course I wrote to get started with Python:\\\n[https://python-10-minutes-a-day.rocks](https://python-10-minutes-a-day.rocks)\n\n### A short overview of the topics we will be discussing:\n1. Link between neural networks and logistic regression\n2. One step back: linear regression\n3. From linear to (binary) logistic regression\n4. Round up"},{"metadata":{},"cell_type":"markdown","source":"# 1. Link between neural network and logistic regression\nWhen we hear or read about deep learning we generally mean the sub-field of machine learning using artificial neural networks (ANN). These computing systems are quite successful in solving complex problems in various fields, examples are, image recognition, language modelling, and speech recognition. While the name ANN implies that they are related to the inner workings of our brain, the truth is that they mainly share some terminology. An ANN generally consists of multiple interconnected layers, which on itself are build using neurons (also called nodes). An example is shown in figure 1.\n\n![Neural Network](https://github.com/dennisbakhuis/Tutorials/raw/master/Neural_Network/assets/NeuralNetwork.png)\n*Figure 1: Example of a typical neural network, borrowed from my follow up article on neural networks.*"},{"metadata":{},"cell_type":"markdown","source":"In this example, we have one input layer, consisting of four individual inputs nodes. This input layer is â€˜fully connectedâ€™ to the first hidden layer, i.e. Fully connected means that each input is connected to each node. The first hidden layer is again fully connected to another â€˜hiddenâ€™ layer. The term hidden indicates that we are not directly interact with these layers and these are kind of obscured to the user. The second hidden layer is on its turn fully connected two the final output layer, which consists of two nodes. So in this example we feed the model four inputs and we will receive two outputs.\n\nLetâ€™s now focus on a single neuron from our previous example. This neuron still is connected to all inputs, also called features. Using these features, the neuron calculates a single response (or output). A diagram of such a system is depicted in figure 2.\n\n![Logistic unit](https://github.com/dennisbakhuis/Tutorials/raw/master/Logistic_Regression/assets/logisticunit.png)\n*Figure 2: single neuron with four input features. The neuron has two operations: a linear part and the activation function.*\n\nThe input features are named ð‘“Â¹, ð‘“Â², ð‘“Â³, and ð‘“â´ and are all connected to the single neuron. This neuron executes two operations. The first operation is a multiplication of the features with a weight vector ð‘Š and adding the bias term ð‘. The second operation is a so called activation function, indicated here by ðœŽ. The output of the neuron is a probability between zero and one. The single neuron acts like a small logistic regression model and therefore, an ANN can be seen as a bunch of interconnected logistic regression models stacked together. While this idea is pretty neat, the underlying truth is a bit more subtle. There are many different architectures for ANNs and they can use various building blocks that act quite different than in this example.\n\nThe linear operation in our single neuron is nothing more than a linear regression. Therefore, to understand logistic regression, the first step is to have an idea how linear regression works. The next section will show a step by step example as a recap."},{"metadata":{},"cell_type":"markdown","source":"# 2.1 One step back: linear regression\n## a) What is a linear regression again?\nLinear regression in its simplest form (also called simple linear regression), models a single dependent variable ð‘¦ using a single independent variable ð‘¥. This may sound daunting, but was this means is that we want to solve the following equation:\n```\nð‘¦ = að‘¥ + ð‘\n```\n\nIn the context of machine learning, ð‘¥ represents our input data, ð‘¦ represents our output data, and by solving we mean to find the best weights (or parameters), represented by ð‘¤ and ð‘ in the linear regression equation. A computer can help us find the best values for ð‘¤ and ð‘, to have the closest match for ð‘¦ using the input variable ð‘¥.\n\nFor the next examples, let us define the following values for x and y:\n```\nð‘¥ = [âˆ’2,âˆ’1,0,1,2,3,4,5]\nð‘¦ = [âˆ’3,âˆ’1,1,3,5,7,9,11]\n```\n\nThe values for ð‘¥ and ð‘¦ have a linear relation so we can use linear regression to find (or fit) the best weights and solve the problem. Maybe, by staring long enough at these values, we can discover the relation, however it is much easier to use a computer to find the answer.\n\nIf you have stared long enough or just want to know the answer, the relation between ð‘¥ and ð‘¦ is the following:\n```\nð‘¦ = 2ð‘¥ + 1\n```\n\nIn the next section we will use Tensorflow to create our single neuron model and try to â€˜solveâ€™ the equation."},{"metadata":{},"cell_type":"markdown","source":"## b) An implementation in Tensorflow\n\nBefore we start with Tensorflow, we should first organize our input data (ð‘¥) and output data (ð‘¦). For this we are going to use Numpy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nX = np.array([-2, -1,  0,  1,  2,  3,  4,  5], dtype=np.float)\nY = np.array([-3, -1,  1,  3,  5,  7,  9, 11], dtype=np.float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next step, we will import Tensorflow. It is always a good practice to check which version we are using:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can create a model using Keras, which is now a part of Tensorflow. To do this, we will use the Sequential class, which can stack various layers â€˜sequentiallyâ€™ after each other. We use the Dense class from Keras to create a â€˜fully connectedâ€™ layer, which consists of a single neuron (unit)."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1, input_shape=[1]),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Dense function is used to create layers of many fully connected neurons (logistic units). The parameter units is used to set the amount of neurons. We only use a single unit and therefore, we will set it to one. As this is the first â€˜layerâ€™ of our model, we need to tell Tensorflow what shape it can expect as an input. This is only necessary for the first layer.\n\nNow that we have defined the model, we need to use the Compile() method to configure the model for training. The method requires at least two parameters, a loss function and an optimizer. The loss function is a measure for how well the model predicts the actual value. For this example we will use the mean squared error (average of the squared difference between the predicted and the actual value of ð‘¦). The â€˜learning algorithmâ€™ will try to minimize the loss my adjusting (optimizing) the parameters (weights and bias) for each step. The optimizer defines a method to perform this Optimizing step and a common method is Gradient Descent, or in our case Stochastic Gradient Descent (SGD). This method will become more clear in the next section where we will implement it in plain Numpy."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='sgd', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will use the Fit() method to let the algorithm learn the best parameters. While Tensorflow has many smart ways to address this problem, what more or less is happing, are the following steps:\n1. calculate the prediction $\\hat y$ using the current weights of the model\n2. calculate the loss of the current values\n3. calculate the gradient of the loss function with respect to the parameters ($W$ and $b$)\n4. adjust the weights (optimize) using the gradient.\n5. repeat for the number of epochs, i.e. the number of times to go through the provided examples (dataset).\n\nIf it is not yet clear what this means, it does not really matter yet. It will become more clear in section 2C.\n\nThis step will generate a lot of output. Just notice that the loss is indeed decreasing and 'close' the output cell by clicking to the left of this cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X, Y, epochs=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can use our model to 'predict' values it has never seen. This is sometimes also called inference. Let's try the value of 12. We know that it should be 25. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict([12.0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why is the value not exactly 25?\\\nThe model calculates the difference between the actual value and the predicted value and creeps slowly towards the actual value. Running the fit procedure longer will get you closer to 25.\n\nThis was not that hard, but it might feel dark Jedi force. Therefore, in the next section we will implement this algorithm in plain Python (with the help of Numpy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2C) What's actually happening 'under the hood'\nIn the previous section we gave a rough overview what Tensorflow is doing under the hood:\n\n1. calculate the prediction $\\hat y$ using the current weights of the model\n2. calculate the loss of the current values\n3. calculate the gradient of the loss function with respect to the parameters ($W$ and $b$)\n4. adjust the weights (optimize) using the gradient.\n5. repeat for the number of epochs, i.e. the number of times to go through the provided examples (dataset).\n\nWe will no code exactly this and hopefully come to a similar result as Tensorflow.\n\nFirst we define the model parameters. These are the weights $W$, which is just a single value, because we only have a single input. Also we need to define the bias term $b$."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(2020) # to make reproducible results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W = 0.01 * np.random.randn(1) # In deep learning it is important to initialize the weights randomly. For our example, 0 would suffice.\nb = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I called the parameters $W$ and $b$ to match their corresponding official neural network terms: weights and bias. We are however, still calculating the exact same thing as the linear regression problem:\n\\begin{equation}\ny = W x + b\n\\end{equation}\n\nNext we will define a function that makes a prediction using our current model parameters. In deep learning terms, this is called forward pass. The variable of the predicted value is generally name $\\hat y$."},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward(X, W, b):\n    yhat = W * X + b\n    return yhat ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function is named forward and uses the input vector $X$ and multiplies it with the weight parameter $W$ and adds the bias term $b$. Exactly as we decribed in the equation.\n\nNow we can test the function with the current parameters. Again, we will input a value of 12.0 but of course, it will return gibberish as the weights are randomly initialized."},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = forward(12.0, W, b)\nyhat[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed not quite right, but we still have to train our model. Before we can do that we need to calculate the current loss. As a loss we used the mean squared error of the predicted value $\\hat y$ and the actual value $y$.\n\n\\begin{equation}\nLoss = \\frac{1}{m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n\\end{equation}\n\nHopefully the math does not scare you, but if you take the time, it is not that hard. The variabel $m$ here is the amount of examples (points in the dataset). Our $X$ holds eight values and therefore, $m=8$. When we have a $1/m$ followed by as sum ($\\sum$) over $m$ is an average of all the values that are $inside$ the summation. Here, we take the average over all $(y - \\hat{y})^2$ which is the difference between the actual value and the predicted value $\\hat y$, squared. The square is important because negative difference and a positve differnce would cancel each other out if we would not square the difference. Now that we fully understand the *mean squared error*, we can implement it in code:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(yhat, Y):\n    m = len(yhat)\n    loss = 1/m * np.sum(yhat - Y)**2\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see what the loss is between our previously calculated value, we can do this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss(yhat, 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, our weights are pretty much off and we need to update them. To do that we need to first calculate the gradients $\\delta$Loss/$\\delta W$ and $\\delta$Loss/$\\delta b$. Maybe your differential skills are a bit rusty. The trick is to apply the product rule. We can ignore the sums as these are linear:\n\n\\begin{equation}\n\\frac{\\delta}{\\delta W} Loss =  \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta W} (y - \\hat{y})^2 \\\\\nU = (y - \\hat{y}) = (y - (Wx + b)) \\\\\n\\frac{\\delta}{\\delta W} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta U} U^2 \\frac{\\delta}{\\delta W} -(W x + b) \\\\\n\\frac{\\delta}{\\delta W} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  -2x(y - \\hat{y})\n\\end{equation}\n\nFor $\\delta$Loss/$\\delta b$ we only need to repeat the last step, with respect to $b$:\n\\begin{equation}\n\\frac{\\delta}{\\delta b} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta U} U^2 \\frac{\\delta}{\\delta b} -(W x + b) \\\\\n\\frac{\\delta}{\\delta b} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  -2(y - \\hat{y})\n\\end{equation}\n\nWe will implement this in the 'backward pass' function. As variable names get a bit long, we will just call them $dW$ and $db$."},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward(X, Y, yhat):\n    m = len(yhat)\n    dW = 1/m * np.sum( -2 * X * (Y - yhat))\n    db = 1/m * np.sum( -2 * (Y - yhat))\n    return (dW, db)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we can get the gradient of our previous test example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"(dW, db) = backward(12.0, 25, yhat)\n(dW, db)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last function we need, before we can compose our training loop is the update function. This function will 'optimize' our weights one step. This is the actual gradient descent in which we subtract (descent) the gradient from our current weights. Gradient descent is defined as follows:\n\n\\begin{equation}\nW = W - \\alpha \\delta W\\\\\nb = b - \\alpha \\delta b\n\\end{equation}\n\nHere we have a new parameter $\\alpha$ which is called the learning rate. We will set it to 0.01. Our code for the update function is as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(W, b, dW, db, learning_rate = 0.01):\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n    return (W, b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To update our curent model paramters we simply do:"},{"metadata":{"trusted":true},"cell_type":"code","source":"(W, b) = update(W, b, dW, db)\n(W, b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, we have updated our weights for the first time. To improve the weights, we have to repeat this process many times. For this we will write a loop:"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iterations = 500\n\n# a reset for W and b\nnp.random.seed(2020)\nW = 0.01 * np.random.randn(1)\nb = 0\n\nfor i in range(num_iterations):\n    yhat = forward(X, W, b)\n    l = loss(Y, yhat)\n    dW, db = backward(X, Y, yhat)\n    W, b = update(W, b, dW, db)\n    if i % 100 == 0:\n        print(l)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are in luck and the loss, i.e. the difference between our model prediction and the actual value, is decreasing. How would we now predict, when we input a value of 12.0?"},{"metadata":{"trusted":true},"cell_type":"code","source":"forward(12.0, W, b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope that the magic box in Tensorflow is now a bit more clear. In the next section we will use our new knowledge for binary logistic regression."},{"metadata":{},"cell_type":"markdown","source":"# 3) From Linear Regression to Binary Logistic Regression\n## a) What is the difference?\nThe differences between linear regressions and a logistic regressions are not major. There are two differences from the previous code we created. First, our linear regression model only had a single feature, which we inputted with $x$, meaning that we only had a single weight. In logistic regression, you generally input more than one feature, and each will have its own weight. This will change the previous simple multiplication to a matrix multiplication (dot product). Secondly, we will add a so called *activation function* to map this value between 0 or 1. Let's remind ourselves again of our simple model:\n![Logistic unit](https://github.com/dennisbakhuis/Tutorials/raw/master/Logistic_Regression/assets/logisticunit.png)\n\nBy convention (from what I have understood) in Tensorflow, the input vector has columns for features, and rows for examples. If we would have 2 datapoints the input matrix would look like this:\n\n\\begin{equation}\nX = \\left( \n\\begin{matrix}\nf^1_1 & f^2_1 & f^3_1 & f^4_1 \\\\\nf^1_2 & f^2_2 & f^3_2 & f^4_2 \n\\end{matrix} \\right)\n\\end{equation}\n\nThe superscript shows the feature number, the subscript indicates the example.\n\nEach of these inputs are associated with their own weights. The node itself has two explict operations. The first is the dot product between the weights vector and the input vector. The second is the sigmoid funtion. The weight vector $W$ in this example has four weights:\n\n\\begin{equation}\nX = \\left( \n\\begin{matrix}\nW^1 \\\\\nW^2 \\\\\nW^3 \\\\\nW^4\n\\end{matrix} \\right)\n\\end{equation}\n\nIn the node we first compute the linear part:\n\n\\begin{equation}\nZ = W X + b\n\\end{equation}\n\nFor our example this will look like this:\n\n\\begin{equation}\nZ = \\left( \n\\begin{matrix}\nW^1 \\\\\nW^2 \\\\\nW^3 \\\\\nW^4\n\\end{matrix} \\right) \\left( \n\\begin{matrix}\nf^1_1 & f^2_1 & f^3_1 & f^4_1 \\\\\nf^1_2 & f^2_2 & f^3_2 & f^4_2 \n\\end{matrix} \\right) + b = \\left(\n\\begin{matrix}\nW^1 f^1_1 + W^2 f^2_1 + W^3 f^3_1 + W^4 f^4_1 + b\\\\\nW^1 f^1_1 + W^2 f^2_1 + W^3 f^3_1 + W^4 f^4_1 + b\n\\end{matrix}\n\\right) = \\left( \n\\begin{matrix}\nz_1 \\\\\nz_2 \n\\end{matrix} \\right)\n\\end{equation}\n\nNotice that the result is only a single value for each example.\n\nFor all values of $x$, which can be from $-\\infty$ to $+\\infty$ (all real numbers), the sigmoid function maps $x$ between 0 and 1. Only values for $x$ close to zero matter, as these are in the 'linear regime'. Very large, or very small are only clipped to 1 and 0 respectively. The mathematical definition of the sigmoid is:\n\n\\begin{equation}\nA = \\frac{1}{1 + e^{-Z}}\n\\end{equation}\n\nThis is all there is to the logistic unit. The sigmoid function gives the node its non-linear character. Many of these units together can do almost magical things. First we will make our first logistic regression model in Tensorflow."},{"metadata":{},"cell_type":"markdown","source":"## b) An implementation in Tensorflow\nBefore we can start we first need some data to do a logistic regression. I downloaded the titantic dataset from Azeem Bootwala from Kaggle to have a play for this example. It can be downloaded from here:\\\nhttps://www.kaggle.com/azeembootwala/titanic"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/titanic//train_data.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, always inspect the columns and data types:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I expect that Azeem did not save the set using index=False option and therefore we have a 'Unnamed: 0' column. This one is redundant with our current index so we can remove it. Also the PassengerId is not useful for our model, lets remove that column too."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Unnamed: 0', 'PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets observe some random examples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Azeem already did some preprocessing. The target variable $Y$ is 'Survived', all other columns are features.\\\nA short description:\n- Sex: 0 or 1 -> male or female\n- age: value rescaled between 0 and 1\n- fare: ticket price rescaled between 0 and 1\n- Pclass_1 .. Pclass_3: One-hot encoded Passenger class\n- family size: rescaled value between 0 and 1 of family size.\n- title_1 .. title_4: mr, mrs, master, miss one-hot encoded\n- emb_1 .. emb_3: Embark location one-hot encoded. \n\nIn total we will have 14 features.\n\nFor this example, the data will suffice and I will not go into detail on how this data has become what it is. Honestly, I do not know myself and have just downloaded it ;-).\n\nLets put these variables in the format we defined before ($X$ and $Y$)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df['Survived'].to_numpy()\nX = df.iloc[:,1:].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable $Y$ is the label if they survived and we had 792 examples. So the shape will be $(m, 1)$ where $m$ = 792:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numpy omits the 1 for the single column as it is redundant. \n\nFor X, we expect a shape of ($m$, 14):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the data, lets create the model in Tensorflow:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1, input_shape=[14], activation='sigmoid'),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model in Tensorflow is very similar to our linear regression model. The input has changed from 1 to 14 features and we added an activation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To configure the model, we again do a compile. For this example I changed the loss into 'binary_cross_entropy'. This is another loss function which works better for binary logistic regression problems. If you are interested in the inner workings I recommend wikipedia. \\\nI also added the metric 'accuracy' to calculate the accuracy for each epoch. This should improve as we train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.fit(X, Y, epochs=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, this was all to training a binary logistic classifier in Tensorflow. We achieved an accuracy of ~80% which is not too bad for the effort we put in. \n\nWe can extract the weights $W$ and the bias $b$ from our model. These values we can later compare to our own implementation of the logistic regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"W_tf, b_tf = [x.numpy() for x in model.weights]\nW_tf, b_tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also plot the loss if we like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.xkcd()\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(np.arange(500), train_history.history['loss'], 'b-', label='loss')\nxlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't mind the nice XKCD wobble :-). It is quite impressive how few steps are required to get to such a result. In the final section we will unveal the Dark Jedi arts being performed by Tensorflow."},{"metadata":{},"cell_type":"markdown","source":"## c) What is actually happening?\nWell, the steps required are still the same:\n1. forward pass\n2. calculate loss\n3. backward pass\n4. update weights\n5. repeat\n\nAl steps have minor changes. The forward pass will be a more general dot product and we need to add the activation function. This final activation functions gives a value between 0 and 1, we need to round this to an actual value of 0 or 1. The loss function is binary cross entropy, which is of course different from the mean square error. The backward pass will calculate the gradient of the loss function with respect to $W$ and $b$. As our loss function changed, we will have different differentials. The update weights function is unchanged, the final loop is also very similar.\n\nIn Numpy, and in math in general as far as I know, the dot product needs the shapes of the vectors (and matrices) to be compatible:\n\n\\begin{equation}\nX Y = X_{i,j} Y_{j, k}\n\\end{equation}\n\nThis means that the number of columns of $X$ must be equal to the number of rows in $Y$. To make ourselves a bit easier, we will Transpose our input vector $X$ and flip the vector. This will result that the rows will be features and the columns will be examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.T\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets first get two rows from our dataset to do test calculations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtry = X[ :, :2]\nYtry = Y[:2]\nXtry.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ytry","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets define our weights. As we have 14 features, our vector $W$ will have 14 values. The bias is a constant for the whole node, and is only a single value."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(2020)\nW = 0.01 * np.random.randn(14)\nb = 0\nW.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we need to calculate the sigmoid function:\n\\begin{equation}\nA = \\frac{1}{1 + e^{-Z}}\n\\end{equation}\nlets create it in Python:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(Z):\n    A = 1 / (1 + np.exp(-Z))\n    return A\n\nsigmoid(np.array([-100, 0, 0.1, 1000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets redefine our forward function to do the dot product and the activation function. The forward pass is defined in two steps:\n1. $Z = W X + b$\n2. $\\sigma(Z)$\n\nNote that $W X$ is a dot product."},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward(X, W, b):\n    Z = np.dot(W.T, X) + b\n    A = sigmoid(Z)\n    return A\n\nA = forward(Xtry, W, b)\nA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have actual predictions, we can write our loss function to measure how well our predictions are. This is done with the binary cross entropy, for which I will simply give the equation. \n\\begin{equation}\nloss = -\\frac{1}{m}\\sum_{i=1}^{m}y\\log(A)+(1-y)\\log(1-A)\n\\end{equation}\n\nIt might happen for the first iteration that we try to calculate a $\\log(0)$ which is of course not defined. To avoid the warning, we will add a tiny value to our loss. As it is super small, the difference is not noticable, but does help suppress the warning. One less warning a day keeps the ...."},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(A, Y, epsilon = 1e-15):\n    m = len(A)\n    l = -1/m * np.sum( Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n    return l\n\nloss(A, Ytry)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next is the backwards pass. For this, We would need to differentiate the Loss function with $W$ and $b$. Not to  bore you guys, I have provided these functions:\n\\begin{equation}\n \\frac{\\partial loss}{\\partial W} = \\frac{1}{m} \\sum_{i=1}^m  X(A - Y)^T \\\\\n \\frac{\\partial loss}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A - y)\n\\end{equation}\nIn Python, this looks like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward(X, Y, A):\n    m = len(yhat)\n    dW = 1/m * np.dot(X, (A - Y).T)\n    db = 1/m * np.sum(A - Y) \n    return (dW, db)\n\n(dW, db) = backward(Xtry, Ytry, A)\ndW, db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost there, next we need to update the weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(W, b, dW, db, learning_rate = 0.01):\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n    return (W, b)\n\nupdate(W, b, dW, db)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To compare the results, we can calculate the accuracy. However our activation function returns a probability between 0 and 1. By definition, values <= 0.5 are rounded to 0 and values > 0.5 are rounded to 1. This is slightly different from the round function so we will make our own function for this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def roundValue(A):\n    return np.uint8( A > 0.5)\n\nyhat = roundValue(A)\nyhat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can calculate the accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(yhat, Y):\n    return round(np.sum(yhat==Y) / len(yhat) * 1000) / 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now finally lets implement the final loop:"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iterations = 8000\nlr = 0.01\n\n# Lets just reset W and b\nnp.random.seed(2020)\nW = 0.01 * np.random.randn(14)\nb = 0\n\nlosses, acces = [], []\nfor i in range(num_iterations):\n    A = forward(X, W, b)\n    l = loss(Y, A)\n    yhat = roundValue(A)\n    acc = accuracy(yhat, Y)\n    dW, db = backward(X, Y, A)\n    W, b = update(W, b, dW, db, learning_rate=lr)\n    losses.append(l)\n    acces.append(acc)\n    if i % 1000 == 0:\n        print('loss:', l, f'\\taccuracy: {accuracy(yhat, Y)}%') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot the loss function of our function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(np.arange(len(losses)), losses, 'b-', label='loss')\nxlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(np.arange(len(acces)), acces, 'b-', label='accuracy')\nxlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Round up\nWell, this was it for the tutorial on a Logistic regression. Hopefully you got an idea on how Logistic regression work and that Tensorflow is not only black magic. I found it a great exercise to write these from scratch and as you have seen, it is not that difficult either.\n\nPlease let me know if you have any comments or suggestions."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}