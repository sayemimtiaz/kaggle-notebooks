{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Prediction - Project\n\n1. Let's doooo this. This is my first official data science mini weekend project attempt. It is supposed to be a fun learning process. Let's see how capable this human is. "},{"metadata":{},"cell_type":"markdown","source":"### Library Importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import RobustScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n#from sklearn.preprocessing import CategoricalEncoder as ce\n#!pip install --upgrade category_encoders\n#from feature_engine import categorical_encoders as ce\n#from feature_engine import missing_data_imputers as mdi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Data Import\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_failure_data_file_path = '../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\nheart_failure_data = pd.read_csv(heart_failure_data_file_path) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Observe and Understand Data\n\nSo apparently this data has no mismatched or missing data (according to the page where I found this data)\n\nQuick real world breakdown and understanding of data. (might not be 100% accurate as i might have miss interpreted what the Dr. who explained this stuff said to me)\n\nThis task is not about the english. So don't get bothered by language errors. :)\n\n* **Age:** obviously older you are more likely you are of heart failure. longer exposure to bad habits, body start breaking dwon naturally etc.\n\n* **Anaemia:** heart failure is basically defined to occur when your heart/heart pump cannot support your body. So if you have anaemia you carry less oxygen, now if you superimpose that with another problem then you reach heart failure faster. So anaemia is like a booster towards heart failure. It worsens the heart problem. \n\n* **creatinine_phosphokinase:**The CPK enzyme is released when cardiac muscle gets destroyed/damaged. So if you have heart failure the CPK enzyme count should be more. Testing for this is tricky though because apparently they use molecular mass to determine what enzyme it is. This allows for error. So just because you have a high count doesnt mean you necesarily have heart failure. This test is still more accurate than the serum_creatinine test.\n\n* **diabetes:**\n\n* **ejection fraction:** under 40% means you are having a systolic failure. ejection fraction is the ratio of blood into and out of your heart. Obviously the smaller the ratio the less blood your body pumps out. This is a problem ofcoarse. (There is also preserved ejection ratio which will be discussed in high blood pressure) - YOU CAN STILL BE HAVING A HEART FAILURE EVEN IF THIS IS NORMAL. if its not normal you are definitely having a heart issue. \n\n* **high blood pressure:** This is a form of preserved ejection ratio or diastolic failure. basically something happens where your muscle wall thickens on the one side. This means less volume of blood can go into your heart. So although the ratio of blood in and out stays the same, the volume in and out decreases.\n\n* **Platelets:** more plateletes more likely you are to clot, especially when youre older. e.g your valves dont work as well, so there is turbulance which leads to clotting of plateletes. \n\n* **serum_creatinine:** enzyme that is released when there is a damage in muscle tissue. (not specific to heart). This test is more likely for error. Could indicate heart failure but doesnt necessarily do so. If the person consumes creatinine supplements it affect the test results.\n\n* **serum_sodium:** When you have allot of sodium in your blood it coulf be an indication of low blood pressure. When you have low blood pressure your bodu tries to fix that and it absorbs water and sodium into your blood. \n\n* **sex:** not a medical reasoning thats obvious\n\n* **smoking:**\n\n* **time:**\n\n* **death event:** are these just general death events or death events due to heart failure. I am going to make a mega assumption that these are indeed deaths due to heart failure otherwise I don't think I am on a level that can solve this problem otherwise. \n\n\nOkay so this is all the theoretical trend and knowledge data. But there is no reason why there isnt a correlation that exists that has not been understood yet. This is where Machine Learning helps. But for my starting point I am going to try create a ML model that works with the knwoledge I have."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_failure_data.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3:Organise Data\ncredit needs to go where its due: I worked through  [this](https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c)  blog as guidance.\n"},{"metadata":{},"cell_type":"markdown","source":"Checking correlation - high correkation is not ideal"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cheching correlarion between our features ---- im not sure how its checking --- but it seems as if our features hoe very low correlations.\n#Our highest correlation is deat_event and time.\n#it seems that the whole point of checking for correlations is so that if there is a high correlation you can actually remove those features. \nheart_failure_data_copy = heart_failure_data.copy()\ncorr_matrix = heart_failure_data_copy.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#ALthough we were told there are no missing values we can still check it incase\nfor var in heart_failure_data.columns:\n    print(var, 'percent of missing values', heart_failure_data[var].isnull().mean().round(3))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"unique stuff\n\nThe data seems to be unique across the features minus ofcoarse the boolean styled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#unique values\nprint('The unique values')\nfor var in heart_failure_data.columns:\n    print(var, heart_failure_data[var].unique(), '\\n')\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"variability is good, a lack of variability means the feature isnt as useful"},{"metadata":{"trusted":true},"cell_type":"code","source":"   \n#variability test\n#there is no feature that has a category that holds the majority (90%), so all of the features have enough variability to keep them\nfor var in heart_failure_data.columns:\n    print(var,'\\n', heart_failure_data[var].value_counts()/len(heart_failure_data))\n    \n# So it looks as if we did not need to delete any features from these tests","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cardinality of Categorical and Discrete Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_failure_features = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\ncontinuous_features = ['age', 'time']\ndiscrete_features = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\ncategorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n\nfor var in heart_failure_features :\n    print(var, 'has', heart_failure_data[var].nunique(), 'unique categories')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distributions and Outliers\n\nThere is allot to talk about and understand here, basically if your data has a skewed distribution you can fix it with different transformations. But how does that help? ... need to explore that idea further to properly understand it"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in continuous_features:\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    fig = heart_failure_data.boxplot(column=var)\n    fig.set_title('outliers')\n    fig.set_ylabel(var)\n    \n    plt.subplot(1,2,2)\n    fig = heart_failure_data[var].hist(bins=20)\n    fig.set_ylabel('number of cases')\n    fig.set_xlabel(var)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalanced Target Distribution\n\napparently certain ML algorithms such as trees tend to bias their predictions towards the majority answer. Lets see where our answer biases towards and how we can possible prevent a bias prediction algorithm. \n\n* \"Oversampling is a technique which attempts to add random copies of the minority class to the dataset until the imbalance is eliminated\". [[reference quote]](https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c)\n* \"Undersampling is the opposite of oversampling as it entails removing majority class observations\"[[reference quote]](https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c) - disadvantage is you dont want to remove data\n* \"synthetic minority oversampling technique (SMOTE) uses the KNN algorithm to generate new observations to eliminate the imbalance.\" [[reference quote]](https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c)"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_failure_data['DEATH_EVENT'].value_counts()/len(heart_failure_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ## Step 4: Preprocessing Pipilenine\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_failure_data[discrete_features] = heart_failure_data[discrete_features].astype('O')\nX = heart_failure_data[heart_failure_features]\ny = heart_failure_data.DEATH_EVENT\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Create Model\n\n(If I were to redo anything, it is this section. I did not properly understand everything I implemented)\n\nWhat I think I should have:\n\ny: DEATH_EVENT\n\nX: age, anaemia, creatinine_phosphokinase, ejection_fraction, high_blood_pressure, platelets, smoking.\n\ni think time can play a role in a way where it can weight certain sections. (e.g if all the people who died soon afer the checkup had high_blood_pressure than there is good chance that the next person will too. I just dont know how the model works and if it would consider something like that.)\n\nTHIS IS A BOOLEAN CLASSIFICATION PROBLEM, HOW DOES THIS AFFECT YOUR WORK?"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))\nmodels.append(('rf_classifer', RandomForestClassifier(random_state=42)))\nmodels.append(('bayes', GaussianNB()))\nmodels.append(('gbc', GradientBoostingClassifier()))\nbase_model_train = []\nbase_model_test = []\nfor name, classifier in models:\n    scores = cross_val_score(classifier, X_train, y_train, cv=5, scoring='recall')\n    base_model_train.append(scores.mean().round(4))\n    print(scores)\n    print('{}: Avg CV recall using all features on training data: {}'.format(name, scores.mean().round(4)))\n    \n    classifier.fit(X_train, y_train)\n    y_preds = classifier.predict(X_test)\n    test_recall = recall_score(y_test, y_preds, average='binary')\n    test_class = classification_report(y_test, y_preds)\n    cnf_matrix = confusion_matrix(y_test, y_preds)\n    base_model_test.append(test_recall.round(4))\n    print('{}: Recall w/all features on test data {}:'.format(name, test_recall.round(4)))\n    print(test_class)\n    print(cnf_matrix)\n    print('-------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\nX_train_std_sm, y_train_sm = sm.fit_resample(X_train, y_train)\nmodel1_train = []\nmodel1_test = []\nfor name, classifier in models:\n    pipeline = make_pipeline(classifier)\n    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n    model1_train.append(scores.mean().round(4))\n    print(scores)\n    print('{}: Avg CV Recall w/All Reatures: {}'.format(name, scores.mean().round(4)))\n    \n    classifier.fit(X_train_std_sm, y_train_sm)\n    y_preds = classifier.predict(X_test)\n    test_recall = recall_score(y_test, y_preds)\n    test_class = classification_report(y_test, y_preds)\n    cnf_matrix = confusion_matrix(y_test, y_preds)\n    model1_test.append(test_recall.round(4))\n    print('{}: Recall w/All Features on test data {}:'.format(name, test_recall.round(4)))\n    print(test_class)\n    print(cnf_matrix)\n    print('-------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))\nmodels.append(('rf_classifer', RandomForestClassifier(random_state=42)))\nmodels.append(('bayes', GaussianNB()))\nmodels.append(('gbc', GradientBoostingClassifier()))\nmodel2_train = []\nmodel2_test = []\nfor name, classifier in models:\n    scores = cross_val_score(classifier, X_train, y_train, cv=5, scoring='recall')\n    model2_train.append(scores.mean().round(3))\n    print(scores)\n    print('{}: Avg CV Recall on RF Features: {}'.format(name, scores.mean().round(3)))\n    \n    classifier.fit(X_train, y_train)\n    y_preds = classifier.predict(X_test)\n    test_recall = recall_score(y_test, y_preds, average='binary')\n    test_class = classification_report(y_test, y_preds)\n    cnf_matrix = confusion_matrix(y_test, y_preds)\n    model2_test.append(test_recall.round(3))\n    print('{}: Recall w/RF features on test data {}:'.format(name, test_recall.round(3)))\n    print(test_class)\n    print(cnf_matrix)\n    print('-------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodels = []\nmodels.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))\nmodels.append(('rf_classifer', RandomForestClassifier(random_state=42)))\nmodels.append(('bayes', GaussianNB()))\nmodels.append(('gbc', GradientBoostingClassifier()))\nsm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n#skf = StratifiedKFold(n_splits=5)\nmodel3_train = []\nmodel3_test = []\nfor name, classifier in models:\n    pipeline = make_pipeline(classifier)\n    scores = cross_val_score(pipeline, X_train, y_train, scoring='recall')\n    model3_train.append(scores.mean().round(3))\n    print(scores)\n    print('{}: Avg CV Recall w/RF Reatures: {}'.format(name, scores.mean().round(3)))\n    \n    classifier.fit(X_train, y_train)\n    y_preds = classifier.predict(X_test)\n    test_recall = recall_score(y_test, y_preds, average='binary')\n    test_class = classification_report(y_test, y_preds)\n    cnf_matrix = confusion_matrix(y_test, y_preds)\n    model3_test.append(test_recall.round(3))\n    print('{}: Recall w/RF on test data {}:'.format(name, test_recall.round(3)))\n    print(test_class)\n    print(cnf_matrix)\n    print('-------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = ['Log_Regression', 'Random_Forest', 'Naive_Bayes', 'Gradient_Boosting_clf']\nidx = ['All_Feat_Imbalance_Train', 'All_Feat_Imbalance_Test','All_Feat_Smote_Train',\n'All_Feat_Smote_Test','RF_Imbalance_Train', \n'RF_Imbalance_Test','RF_Smote_Train','RF_Smote_Test']\ncombined_results = pd.DataFrame([base_model_train,base_model_test,\nmodel1_train, model1_test, model2_train,\nmodel2_test, model3_train, model3_test],\ncolumns=classifiers, index=idx)\ntest_results = pd.DataFrame([base_model_test, model1_test, model2_test, model3_test], columns=classifiers, index=idx[1:8:2])\nprint(test_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.lineplot(data=test_results[['Log_Regression', 'Random_Forest',\n                             'Naive_Bayes', 'Gradient_Boosting_clf']])\nplt.xlabel('Iterations', fontsize=17, labelpad=15)             \nplt.ylabel('Recall Scores', fontsize=17, labelpad=15)\nplt.title('Classifier Recall Scores on Test Data',fontsize=25, pad=15)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}