{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom textblob import TextBlob\n\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nimport re, string, unicodedata\n#import inflect\n\nimport spacy\n#nltk.download('stopwords')\n\nimport nltk.data\n\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\n#from __future__ import unicode_literals\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**----------------------------- Read data as a CSV file ---------------------------------------**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Amazon_Meta_Data = pd.read_csv('../input/amazon-reviews-unlocked-mobile-phones/Amazon_Unlocked_Mobile.csv',nrows=1000, encoding='utf-8')\n\n#------- Counting words in reviews ---------\nword_counts = []\nfor review in Amazon_Meta_Data['Reviews']:\n    count=0\n    for word in str(review).split():\n        count +=1\n    word_counts.append(count)\nAmazon_Meta_Data['words_counts'] = word_counts\n\n# Discarding rows having less then 5 words of review.\nAmazon_Meta_Data=Amazon_Meta_Data.loc[Amazon_Meta_Data['words_counts'] >=5]\n\nAmazon_Meta_Data.head()\n#Amazon_Meta_Data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** ---------------------------  converting product names to code -----------------------------**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Reviews = Amazon_Meta_Data['Reviews']\nBrand_Name = Amazon_Meta_Data['Brand Name'].str.upper()\nbrand_list =list(set(Brand_Name))\nunique_products=list(set(Amazon_Meta_Data['Product Name']))\nunique_rating = list(set(Amazon_Meta_Data['Rating']))\n\n#**unique products in dataset and total reviews about each product in dataset.**\nproduct_count = []\nprodName = []\nprodcode=[]\ncount =0\nfor prod in unique_products:\n    count=count+1\n    product_count.append(len(Amazon_Meta_Data.loc[Amazon_Meta_Data['Product Name'] == prod]))\n    prodName.append(prod)\n    prodcode.append(count)\n    \nproducts_code = pd.DataFrame()\n\nproducts_code['Product Name'] = prodName\nproducts_code['product code'] = prodcode\ncount=0\nproduct_code= []\n\nfor pr in products_code['Product Name']:\n    for prod in Amazon_Meta_Data['Product Name']:\n        if(prod==pr):\n            product_code.append(count)\n    count=count+1\n    \n#df.insert(loc=idx, column='A', value=new_col)\n\nAmazon_Meta_Data.insert(loc=1, column='Product Code', value=product_code)"},{"metadata":{},"cell_type":"markdown","source":"**Data Analysis to get information from dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Reviews = Amazon_Meta_Data['Reviews']\nBrand_Name = Amazon_Meta_Data['Brand Name'].str.upper()\nbrand_list =list(set(Brand_Name))\nunique_products=list(set(Amazon_Meta_Data['Product Name']))\nunique_rating = list(set(Amazon_Meta_Data['Rating']))\n\n#**unique products in dataset and total reviews about each product in dataset.**\nproduct_count = []\nfor prod in unique_products:\n    product_count.append(len(Amazon_Meta_Data.loc[Amazon_Meta_Data['Product Name'] == prod]))\n    \n\n#**Detailed information About dataset**\n# print('total Reviews    : ',len(Reviews))\n# print('total Brands     : ',len(Brand_Name.value_counts()))\n# print('total Products   : ',len(product_count))\n# print('total Brands with their counts are \\n',Brand_Name.value_counts())\n\n\n#**Ratings count according to the product**\nrating=list(set(Amazon_Meta_Data['Rating']))\nfor rating in unique_rating:\n    globals()['rating_list_%s' % rating] =[]\nfor prod in unique_products:\n    for rating in unique_rating:\n        globals()['rating_list_%s' % rating].append(len(Amazon_Meta_Data.loc[(Amazon_Meta_Data['Product Name'] == prod) & (Amazon_Meta_Data['Rating']==rating)]))\n\n        \nproducts_dataset = pd.DataFrame()\nproducts_dataset['Product Name'] = unique_products\nproducts_dataset['Total_Reviews'] = product_count\nproducts_dataset['1_Rating'] = rating_list_1\nproducts_dataset['2_Rating'] = rating_list_2\nproducts_dataset['3_Rating'] = rating_list_3\nproducts_dataset['4_Rating'] = rating_list_4\nproducts_dataset['5_Rating'] = rating_list_5\n\n# products_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **============================Task 2==========================**"},{"metadata":{},"cell_type":"markdown","source":"****Text cleaning and sentiment Functions****"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef replace_numbers(words):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    #words = replace_numbers(words)\n    words = remove_stopwords(words)\n    return words\n\n#Steemming and Lemmatization\ndef stem_and_lemmatize(words):\n    stems = stem_words(words)\n    lemmas = lemmatize_verbs(words)\n    return stems, lemmas\n\n#stems, lemmas = stem_and_lemmatize(words)\n#print('Stemmed:\\n', stems)\n#print('\\nLemmatized:\\n', lemmas)\n\n\n# ---------------   Cleaning   ------------------\ndef clean_text(text):\n    wording = nltk.word_tokenize(text)\n    words = normalize(wording)\n    string_text = ' '.join(words)\n    return string_text\n\n# ---------------   Sentiment   ------------------\ndef get_text_sentiment(text):\n    # create TextBlob object of passed text \n    analysis = TextBlob(clean_text(text)) \n    # set sentiment \n    if analysis.sentiment.polarity > 0: \n        return 'positive'\n    elif analysis.sentiment.polarity == 0: \n        return 'neutral'\n    else: \n        return 'negative'    \n    \n#---------------------- TextBlob Feature Extractions -----------------\n#Function to extract features from text\ndef textBlob_feature_extraction(text): \n        blob = TextBlob(text)\n        return blob.noun_phrases\n    \n    \n#----------------------  extract Sentences  -------------------------    \ndef sentances(text):\n    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n    return sent_detector.tokenize(text.strip())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------  Text Cleaning & Sentiment Extraction --------------------\ncleaned_reviews =[]\nsentiment= []\nfor reviews in Amazon_Meta_Data['Reviews']:\n    cleaned_reviews.append(clean_text(reviews))\n    sentiment.append(get_text_sentiment(reviews))\n    \nfeatures_Dataset = pd.DataFrame()\nfeatures_Dataset['Product Name'] = Amazon_Meta_Data['Product Name']\nfeatures_Dataset['Reviews'] = Amazon_Meta_Data['Reviews']\nfeatures_Dataset['Cleaned_Reviews'] = cleaned_reviews\nfeatures_Dataset['Sentiment'] = sentiment\n\n\n# Extracting Features from each review using TextBlob & Spacy *\n    \n#--------------------- Text Blob -------------------------\nfeatures = []\nfor reviews in features_Dataset['Cleaned_Reviews']:\n    features.append(textBlob_feature_extraction(reviews))\n#adding Extracted features to dataset\nfeatures_Dataset[\"TextBlob_Features\"] = features\n\n#--------------------- Spacy -----------------------------\nnlp = spacy.load('en')\n\nfeature_spacy = []\nfor review in nlp.pipe(features_Dataset['Cleaned_Reviews']):\n    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n    feature_spacy.append(','.join(chunks))\n\nfeatures_Dataset['Spacy_features']= feature_spacy\n\n#---------------------------------------------------------\nfeatures_Dataset.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Count of extracted features of each review collected By TextBlob **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------- Features Counts ------------------\n#function to extract Features With their counts\ncounts = {}\ndef feature_count(feature):\n    if  feature in counts:\n        counts[feature] += 1\n    else:\n        counts[feature] = 1\n#Extracting feature count form feartures\nfor feature_list in features_Dataset['TextBlob_Features']:\n    for feature in feature_list:\n        feature_count(feature)\n\n#-----------------Distinct Features Counts ------------------\n#dataframe of features and their count    \ndist_feature =[]\ndist_feature_count= []\nfor key, value in counts.items() :\n    dist_feature.append(key)\n    dist_feature_count.append(value)\n    \n#----------------- Features Counts Dataset ------------------\nfeature_TextBlob = pd.DataFrame()\nfeature_TextBlob['Feature Name'] =dist_feature\nfeature_TextBlob['Feature Count'] =dist_feature_count\n\n#----------------- Features Details ------------------\n#discarding feature that occure less the 3 in the entire dataset\nprint('TextBlob features extracted from dataset are : ' ,len(feature_TextBlob))\nfeature_TextBlob =feature_TextBlob.loc[feature_TextBlob['Feature Count'] >=3]\nprint('TextBlob features left after discarding those who occure less then three times in dataset are : ' ,len(feature_TextBlob))\n#feature_TextBlob\n\n#--------------------  Downlaodable CVS of Features and their count --------------------\nfrom IPython.display import HTML\nfeature_TextBlob.to_csv('feature_TextBlob.csv', index=False)\n\ndef create_download_link(title = \"Download features and counts as CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='feature_TextBlob.csv')\n##===========================================================================================================\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**+++++++++++++++++++++     Aspects Polarities     +++++++++++++++++++++**"},{"metadata":{"trusted":true},"cell_type":"code","source":"module_two = pd.DataFrame()\nmodule_two[\"Products\"] = Amazon_Meta_Data[\"Product Name\"]\nmodule_two[\"Reviews\"] = Amazon_Meta_Data[\"Reviews\"]\nmodule_two = module_two.loc[(module_two['Products'] == '4 Inch Touch Screen Cell Phone Unlocked, Android Unlocked Gsm Smartphones Dual Camera Dual Sim Dual Standby No Contract (Blue)')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_sent= []\nsentences = []\nfor reviews in module_two[\"Reviews\"]:\n    for sent in sentances(reviews):\n        sentiment_sent.append(get_text_sentiment(sent))\n        sentences.append(sent)\n\n\nprod1_dataset = pd.DataFrame()\nprod1_dataset[\"Reviews_sentences\"] =  sentences\nprod1_dataset[\"sentiment\"] = sentiment_sent\n\nprod1_dataset.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_reviews =[]\nsentiment= []\nfor reviews in prod1_dataset[\"Reviews_sentences\"]:\n    cleaned_reviews.append(clean_text(reviews))\n    sentiment.append(get_text_sentiment(reviews))\n\n\nNew_df = pd.DataFrame()\nNew_df['Reviews_sentences'] = prod1_dataset[\"Reviews_sentences\"]\nNew_df['Cleaned_Reviews_sentences'] = cleaned_reviews\nNew_df['Sentiment'] = sentiment\n\n\nNew_df.head()\n\n# Extracting Features from each review using TextBlob **\nfeatures = []\nfor reviews in New_df['Cleaned_Reviews_sentences']:\n    features.append(textBlob_feature_extraction(reviews))\n#adding Extracted features to dataset\nNew_df[\"TextBlob_Features\"] = features\nNew_df.head(10)\n#dataset.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_reviews = New_df.loc[(New_df['Sentiment'] == 'positive')]\npositive_reviews.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------- Features Counts ------------------\n#function to extract Features With their counts\ncounts = {}\ndef feature_count(feature):\n    if  feature in counts:\n        counts[feature] += 1\n    else:\n        counts[feature] = 1\n#Extracting feature count form feartures\nfor feature_list in positive_reviews['TextBlob_Features']:\n    for feature in feature_list:\n        feature_count(feature)\n\n#-----------------Distinct Features Counts ------------------\n#dataframe of features and their count    \ndist_feature =[]\ndist_feature_count= []\nfor key, value in counts.items() :\n    dist_feature.append(key)\n    dist_feature_count.append(value)\n    \n#----------------- Features Counts Dataset ------------------\npositive_reviews_features = pd.DataFrame()\npositive_reviews_features['Feature Name'] =dist_feature\npositive_reviews_features['Feature Count'] =dist_feature_count\n\n#----------------- Features Details ------------------\n#discarding feature that occure less the 3 in the entire dataset\nprint('TextBlob features extracted from dataset are : ' ,len(positive_reviews_features))\n##positive_reviews_features =positive_reviews_features.loc[positive_reviews_features['Feature Count'] >=3]\n#print('TextBlob features left after discarding those who occure less then three times in dataset are : ' ,len(positive_reviews_features))\n#feature_TextBlob\n\npositive_reviews_features.head(10)\n##===========================================================================================================","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_reviews = New_df.loc[(New_df['Sentiment'] == 'negative')]\nnegative_reviews.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------- Features Counts ------------------\n#function to extract Features With their counts\ncounts = {}\ndef feature_count(feature):\n    if  feature in counts:\n        counts[feature] += 1\n    else:\n        counts[feature] = 1\n#Extracting feature count form feartures\nfor feature_list in negative_reviews['TextBlob_Features']:\n    for feature in feature_list:\n        feature_count(feature)\n\n#-----------------Distinct Features Counts ------------------\n#dataframe of features and their count    \ndist_feature =[]\ndist_feature_count= []\nfor key, value in counts.items() :\n    dist_feature.append(key)\n    dist_feature_count.append(value)\n    \n#----------------- Features Counts Dataset ------------------\nnegative_reviews_features = pd.DataFrame()\nnegative_reviews_features['Feature Name'] =dist_feature\nnegative_reviews_features['Feature Count'] =dist_feature_count\n\n#----------------- Features Details ------------------\n#discarding feature that occure less the 3 in the entire dataset\n#print('TextBlob features extracted from dataset are : ' ,len(negative_reviews_features))\n#print('TextBlob features left after discarding those who occure less then three times in dataset are : ' ,len(negative_reviews_features))\n#feature_TextBlob\n\nnegative_reviews_features.head(10)\n##===========================================================================================================\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_count = []\nfor prod in unique_products:\n    product_count.append(len(Amazon_Meta_Data.loc[Amazon_Meta_Data['Product Name'] == prod]))\nAspects_polarity_DB = pd.DataFrame()\nAspects_polarity_DB['Product Name'] =unique_products\nAspects_polarity_DB['Total Reviews'] =product_count\nprint('total products Before descarding less then 5 reviews products',len(Aspects_polarity_DB))\nAspects_polarity_DB=Aspects_polarity_DB.loc[Aspects_polarity_DB['Total Reviews'] >=5]\nprint('total products after descarding less then 5 reviews products',len(Aspects_polarity_DB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}