{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Basic Understaning about NEURAL STYLE TRANSFER algorithm"},{"metadata":{},"cell_type":"markdown","source":"****We all have used different camera filters that are in built in our smartphone or sometimes we ahve installed different filter apps to make our pictures look more glossy and artistic. But, have you ever wondered how these filters actually works? How this artistic looks comes?****\n\n****All this is because of an intelligent style transfer algorithm....****<br>\n****Now, What is Style Transfer??****<br>\nStyle transfer is the technique of reconstructing images in the style of another image.<br> There are varoius research papers on this topic that how neural networks could be used to generate artistic style images, each author stating their own innovative way of optimization and loss function to create a artistic image generating neural network.<br>\n****After going through various works on Neural Style Transfer and how they could be constructed, In this Kernel I have explained part by part how a basic Neural style transfer is actually constructed ****"},{"metadata":{},"cell_type":"markdown","source":"![Style transfer image](https://cdn-images-1.medium.com/max/1600/1*kOQOZxBDNw4lI757soTEyQ.png)\n\n**Source - [Towards Data Science](https://towardsdatascience.com/real-time-video-neural-style-transfer-9f6f84590832)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport os\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nimport matplotlib.pyplot as plt\nfrom keras.applications import vgg19\nfrom keras.models import Model\n#from keras import optimizers\nfrom scipy.optimize import fmin_l_bfgs_b\n#from keras.applications.vgg19 import VGG19\n#vgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#vgg19 = VGG19(include_top = False, weights=vgg19_weights)\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"StylePath = '../input/best-artworks-of-all-time/images/images/'\nContentPath = '../input/image-classification/validation/validation/travel and adventure/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the path for Base Content Image and the Style image respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_image_path = ContentPath+'13.jpg'\nstyle_image_path = StylePath+'Pablo_Picasso/Pablo_Picasso_92.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimensions of the generated picture.\nwidth, height = load_img(base_image_path).size\nimg_nrows = 400\nimg_ncols = int(width * img_nrows / height)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This function is used to Preprocess the image with help of VGG19.\n**VGGNet** was invented by **VGG (Visual Geometry Group)** from University of Oxford, Though VGGNet was the 1st runner-up, not the winner of the ILSVRC (ImageNet Large Scale Visual Recognition Competition) 2014 in the classification task, which has significantly improvement over ZFNet (The winner in 2013) and AlexNet (The winner in 2012). And GoogLeNet is the winner of ILSVLC 2014, I will also talk about it later.) Nevertheless, VGGNet beats the GoogLeNet and won the localization task in ILSVRC 2014.<br><br>\n**VGG19** is a model, with weights pre-trained on **ImageNet**.**ImageNet**, is a dataset of over 15 millions labeled high-resolution images with around 22,000 categories. ILSVRC uses a subset of ImageNet of around 1000 images in each of 1000 categories. In all, there are roughly 1.3 million training images, 50,000 validation images and 100,000 testing images."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_image(image_path):\n    from keras.applications import vgg19\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.title(\"Base Image\",fontsize=20)\nimg1 = load_img(ContentPath+'13.jpg')\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.title(\"Style Image\",fontsize=20)\nimg1 = load_img(StylePath+'Pablo_Picasso/Pablo_Picasso_92.jpg')\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get tensor representations of our images\n\nbase_image = K.variable(preprocess_image(base_image_path))\nstyle_reference_image = K.variable(preprocess_image(style_image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.image_data_format()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### this will contain our generated image\n\nThink of **Variable** in tensorflow as a normal variables which we use in programming languages. We initialize variables, we can modify it later as well. Whereas **placeholder** doesnâ€™t require initial value. Placeholder simply allocates block of memory for future use. Later, we can use feed_dict to feed the data into placeholder. By default, placeholder has an unconstrained shape, which allows you to feed tensors of different shapes in a session."},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will contain our generated image\nif K.image_data_format() == 'channels_first':\n    combination_image = K.placeholder((1,3,img_nrows, img_ncols))\nelse:\n    combination_image = K.placeholder((1,img_nrows, img_ncols,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine the 3 images into a single Keras tensor\ninput_tensor = K.concatenate([base_image,\n                              style_reference_image,\n                              combination_image\n                              ], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the VGG19 model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the VGG19 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nfrom keras.applications.vgg19 import VGG19\nvgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = VGG19(input_tensor=input_tensor,\n              include_top = False,\n              weights=vgg19_weights)\n#model = vgg19.VGG19(input_tensor=input_tensor,\n#                    weights='imagenet', include_top=False)\nprint('Model loaded.')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Athough Vgg19 is basically used for Classification purpose, but here our objective is not to classify rather our objective is to transform a image, so we do not need all the layers of vgg19, we have specially excluded those layers which are used for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Content layer where will pull our feature maps\ncontent_layers = ['block5_conv2'] \n\n# Style layer we are interested in\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1',\n                'block5_conv1'\n               ]\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\nprint(outputs_dict['block5_conv2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The content Loss\nGiven a chosen content layer **l**, the content loss is defined as the Mean Squared Error between the feature map **F** of our content image **C** and the feature map **P** of our generated image **Y**.\n![Coontent Loss](https://cdn-images-1.medium.com/max/800/1*1YfGhmzBw7EK3e8CRpZbuA.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\ndef get_content_loss(base_content, target):\n    return K.sum(K.square(target - base_content))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Style Loss\nTo do this at first we need to, calculate the **Gram-matrix**(a matrix comprising of correlated features) for the tensors output by the style-layers. The Gram-matrix is essentially just a matrix of dot-products for the vectors of the feature activations of a style-layer.<br><br>\nIf an entry in the Gram-matrix has a value close to zero then it means the two features in the given layer do not activate simultaneously for the given style-image. And vice versa, if an entry in the Gram-matrix has a large value, then it means the two features do activate simultaneously for the given style-image. We will then try and create a mixed-image that replicates this activation pattern of the style-image.\nIf the feature map is a matrix **F**, then each entry in the Gram matrix **G** can be given by:\n![Gram Matrix](https://cdn-images-1.medium.com/max/800/1*5xx9KmhVb59Mxe_buOwHBA.png)\nThe loss function for style is quite similar to out content loss, except that we calculate the Mean Squared Error for the Gram-matrices instead of the raw tensor-outputs from the layers.\n![Style loss](https://cdn-images-1.medium.com/max/800/1*PuYveCM2BlgFfjUCr6I_Ng.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# the gram matrix of an image tensor (feature-wise outer product)\ndef gram_matrix(input_tensor):\n    assert K.ndim(input_tensor)==3\n    #if K.image_data_format() == 'channels_first':\n    #    features = K.batch_flatten(input_tensor)\n    #else:\n    #    features = K.batch_flatten(K.permute_dimensions(input_tensor,(2,0,1)))\n    #gram = K.dot(features, K.transpose(features))\n    channels = int(input_tensor.shape[-1])\n    a = tf.reshape(input_tensor, [-1, channels])\n    n = tf.shape(a)[0]\n    gram = tf.matmul(a, a, transpose_a=True)\n    return gram#/tf.cast(n, tf.float32)\n\ndef get_style_loss(style, combination):\n    assert K.ndim(style) == 3\n    assert K.ndim(combination) == 3\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_nrows*img_ncols\n    return K.sum(K.square(S - C))#/(4.0 * (channels ** 2) * (size ** 2))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## In the below four cells I have tried out my own way of extracting features from the style layers and content layers, but it didn't worked out successfully. So you can try out thse codes after modifying them, I hope it would work for you"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get output layers corresponding to style and content layers \n#style_outputs = [model.get_layer(name).output for name in style_layers]\n#content_outputs = [model.get_layer(name).output for name in content_layers]\n#model_outputs = style_outputs + content_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the style and content feature representations from our model  \n#style_features = [style_layer[0] for style_layer in model_outputs[:num_style_layers]]\n#content_features = [content_layer[1] for content_layer in model_outputs[num_style_layers:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#style_output_features = model_outputs[:num_style_layers]\n#content_output_features = model_outputs[num_style_layers:]\n# Accumulate style losses from all layers\n# Here, we equally weight each contribution of each loss layer\n#weight_per_style_layer = 1.0 / float(num_style_layers)\n#loss = K.variable(0.0)\n#style_score = 0\n#content_score = 0\n    \n#for target_style, comb_style in zip(gram_style_features, style_output_features):\n#    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n# Accumulate content losses from all layers \n#weight_per_content_layer = 1.0 / float(num_content_layers)\n#for target_content, comb_content in zip(content_features, content_output_features):\n#    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n\n#style_score *= style_weight\n#content_score *= content_weight\n\n# Get total loss\n#loss = style_score + content_score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content_weight=0.025 \nstyle_weight=1.0\n# combine these loss functions into a single scalar\nloss = K.variable(0.0)\nlayer_features = outputs_dict['block5_conv2']\nbase_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nprint('Layer Feature for Content Layers :: '+str(layer_features))\nprint('Base Image Feature :: '+str(base_image_features))\nprint('Combination Image Feature for Content Layers:: '+str(combination_features)+'\\n')\nloss += content_weight * get_content_loss(base_image_features,\n                                      combination_features)\n\nfeature_layers = ['block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1']\nfor layer_name in feature_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    print('Layer Feature for Style Layers :: '+str(layer_features))\n    print('Style Image Feature :: '+str(style_reference_features))\n    print('Combination Image Feature for Style Layers:: '+str(combination_features)+'\\n')\n    sl = get_style_loss(style_reference_features, combination_features)\n    loss += (style_weight / len(feature_layers)) * sl\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features are extracted from each layer in style layers and content layers and their overall loss is calculated from it..."},{"metadata":{},"cell_type":"markdown","source":"### This deprocess_image function is used return the original format of the Final image  after transformation which could be easily read and displayed by Matplotlib."},{"metadata":{"trusted":true},"cell_type":"code","source":"def deprocess_image(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((3, img_nrows, img_ncols))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculation of gradient with respect to loss.."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the gradients of the generated image wrt the loss\ngrads = K.gradients(loss, combination_image)\ngrads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = [loss]\nif isinstance(grads, (list,tuple)):\n    outputs += grads\nelse:\n    outputs.append(grads)\nf_outputs = K.function([combination_image], outputs)\nf_outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Athough there are various optimizers but we have used L-BFGS optimizer in this case, I have also gone through research papers where they have used ADAM optimizer to optimize the loss and get the final image.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\nx_opt = preprocess_image(base_image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loss_and_grads(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((1, 3, img_nrows, img_ncols))\n    else:\n        x = x.reshape((1, img_nrows, img_ncols, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype('float64')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype('float64')\n    return loss_value, grad_values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The purpose of this **Evaluator** class is to avoid the error **'numpy.ndarray' object is not callable error with optimize.minimize** while running the L-BFGS optimizer for loss minimization.<br><br>\nYou should pass the function itself to minimize, instead of a evaluated value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = Evaluator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this segment we run the code upto a given iteration. Although I would not recommend you to use **maxiter** parameter in **fmin_l_bfgs_b** to set the number of iterations.  Rarther use iteration in for loop to get better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"iterations=400\n# Store our best result\nbest_loss, best_img = float('inf'), None\nfor i in range(iterations):\n    print('Start of iteration', i)\n    x_opt, min_val, info= fmin_l_bfgs_b(evaluator.loss, \n                                        x_opt.flatten(), \n                                        fprime=evaluator.grads,\n                                        maxfun=20,\n                                        disp=True,\n                                       )\n    print('Current loss value:', min_val)\n    if min_val < best_loss:\n        # Update best loss and best image from total loss. \n        best_loss = min_val\n        best_img = x_opt.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Final Image**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save current generated image\nimgx = deprocess_image(best_img.copy())\nplt.imshow(imgx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nplt.title(\"Base Image\",fontsize=20)\nimg_base = load_img(base_image_path)\nplt.imshow(img_base)\n\nplt.subplot(5,5,1+1)\nplt.title(\"Style Image\",fontsize=20)\nimg_style = load_img(style_image_path)\nplt.imshow(img_style)\n\nplt.subplot(5,5,1+2)\nplt.title(\"Final Image\",fontsize=20)\nplt.imshow(imgx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function **Run_Style_Transfer** is nothing but combination of all the above code that is discussed in the above cells part by part. It returns the final image after style transfer between two images."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_image_instantiator(image_path,img_nrows,img_ncols):\n    from keras.applications import vgg19\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Run_StyleTransfer(base_image_path, style_image_path):\n    \n    width, height = load_img(base_image_path).size\n    img_nrows = 400\n    img_ncols = int(width * img_nrows / height)\n    \n    base_image = K.variable(preprocess_image_instantiator(base_image_path,img_nrows,img_ncols))\n    style_reference_image = K.variable(preprocess_image_instantiator(style_image_path,img_nrows,img_ncols))\n    \n    if K.image_data_format() == 'channels_first':\n        combination_image = K.placeholder((1,3,img_nrows, img_ncols))\n    else:\n        combination_image = K.placeholder((1,img_nrows, img_ncols,3))\n        \n    input_tensor = K.concatenate([base_image,\n                                  style_reference_image,\n                                  combination_image\n                                  ], axis=0)\n    from keras.applications.vgg19 import VGG19\n    vgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    model = VGG19(input_tensor=input_tensor,\n                  include_top = False,\n                  weights=vgg19_weights)\n    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n    \n    content_weight=0.025 \n    style_weight=1.0\n    # combine these loss functions into a single scalar\n    loss = K.variable(0.0)\n    layer_features = outputs_dict['block5_conv2']\n    base_image_features = layer_features[0, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    #print('Layer Feature for Content Layers :: '+str(layer_features))\n    #print('Base Image Feature :: '+str(base_image_features))\n    #print('Combination Image Feature for Content Layers:: '+str(combination_image_features))\n    loss += content_weight * get_content_loss(base_image_features,\n                                          combination_features)\n\n    feature_layers = ['block1_conv1', 'block2_conv1',\n                      'block3_conv1', 'block4_conv1',\n                      'block5_conv1']\n    for layer_name in feature_layers:\n        layer_features = outputs_dict[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        #print('Layer Feature for Style Layers :: '+str(layer_features))\n        #print('Style Image Feature :: '+str(style_reference_features))\n        #print('Combination Image Feature for Style Layers:: '+str(combination_features))\n        sl = get_style_loss(style_reference_features, combination_features)\n        loss += (style_weight / len(feature_layers)) * sl\n        \n    grads = K.gradients(loss, combination_image)\n    \n    outputs = [loss]\n    if isinstance(grads, (list,tuple)):\n        outputs += grads\n    else:\n        outputs.append(grads)\n    f_outputs = K.function([combination_image], outputs)\n    \n    x_opt = preprocess_image(base_image_path)\n    \n    evaluator = Evaluator()\n    iterations=200\n    # Store our best result\n    best_loss, best_img = float('inf'), None\n    for i in range(iterations):\n        #print('Start of iteration', i)\n        x_opt, min_val, info= fmin_l_bfgs_b(evaluator.loss, \n                                            x_opt.flatten(), \n                                            fprime=evaluator.grads,\n                                            maxfun=20,\n                                            disp=True,\n                                           )\n        #print('Current loss value:', min_val)\n        if min_val < best_loss:\n            # Update best loss and best image from total loss. \n            best_loss = min_val\n            best_img = x_opt.copy()\n    imgx = deprocess_image(best_img.copy())\n    \n    return imgx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_image_path_1 = '../input/image-classification/images/images/travel and  adventure/Places365_val_00005821.jpg'\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nplt.title(\"Base Image\",fontsize=20)\nimg_base = load_img(base_image_path_1)\nplt.imshow(img_base)\n\nstyle_image_path_1 = '../input/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_96.jpg'\nplt.subplot(5,5,1+1)\nplt.title(\"Style Image\",fontsize=20)\nimg_style = load_img(style_image_path_1)\nplt.imshow(img_style)\n\nplt.subplot(5,5,1+2)\nimgg = Run_StyleTransfer(base_image_path_1, style_image_path_1)\nplt.title(\"Final Image\",fontsize=20)\nplt.imshow(imgg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_image_path_2 = '../input/image-classification/images/images/travel and  adventure/Places365_val_00005982.jpg'\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nplt.title(\"Base Image\",fontsize=20)\nimg_base = load_img(base_image_path_2)\nplt.imshow(img_base)\n\nstyle_image_path_2 = '../input/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_24.jpg'\nplt.subplot(5,5,1+1)\nplt.title(\"Style Image\",fontsize=20)\nimg_style = load_img(style_image_path_2)\nplt.imshow(img_style)\n\nplt.subplot(5,5,1+2)\nimga = Run_StyleTransfer(base_image_path_2, style_image_path_2)\nplt.title(\"Final Image\",fontsize=20)\nplt.imshow(imga)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_image_path_3 = '../input/image-classification/images/images/travel and  adventure/Places365_val_00005752.jpg'\nplt.figure(figsize=(30,30))\nplt.subplot(5,5,1)\nplt.title(\"Base Image\",fontsize=20)\nimg_base = load_img(base_image_path_3)\nplt.imshow(img_base)\n\nstyle_image_path_3 = '../input/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_83.jpg'\nplt.subplot(5,5,1+1)\nplt.title(\"Style Image\",fontsize=20)\nimg_style = load_img(style_image_path_3)\nplt.imshow(img_style)\n\nplt.subplot(5,5,1+2)\nimgy = Run_StyleTransfer(base_image_path_3, style_image_path_3)\nplt.title(\"Final Image\",fontsize=20)\nplt.imshow(imgy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n* In our case the Final images formed are not totally perfect, because the Style image does not totally blend with the Base content image.\n* It could be improved through icreasing the number of iteration, or by trying out a different syle transfer algorithm which could preseve the edges of the base image, or by trying out with different optimizer to minimize gradient and loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}