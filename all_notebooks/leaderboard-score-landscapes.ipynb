{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Leaderboard Score Landscapes\n\nThis notebook shows traces of public/private submission scores for the whole leaderboard, for each Kaggle competition.\n\nLeaderboards are based on ranks of scores, but the distribution of scores themselves can help show the nature of a competition.\n\n#### Color Scheme\n\n - <font color=red>red</font> Private Scores\n - <font color=blue>blue</font> Public Scores\n - <font color=#3a3>green</font> Top Public Scores (a different ordering of teams)\n\nMedal zones are are marked with a dotted line. (When there is no public LB, or for recent kernel competitions - only the <font color=red>red</font> line of private LB scores is shown.)\n\nVery often, the gold medal solutions will score very much better by the competition metric... A sharp gradient in the <font color=red>red</font> line.\n\nA sharp gradient in the <font color=#3a3>green</font> line indicates heavy public LB overfitting (see [Mercedes-Benz Greener Manufacturing][13]).\n\nWhen the different lines match up really well it tends to indicate a competition with very large test set (see [Avito Duplicate Ads Detection][14]).\n\nWhen the red and blue lines go flat (horizontally) it indicates identical scores - generally from shared public submissions (public kernels, see [Google Analytics Customer Revenue Prediction][9] where the zero benchmark won medals!)\n\nThis is also yet another way to indicate **shake-up**: the amount the blue line dances around the green line! (For example, 100th place finisher's public score vs 100th best public score.)\n\n\nSome notable entries:\n\n### Strong Wins/Golds\n - [Rossmann Store Sales][3] @gertjac's outstanding solo win with few submissions, from a bungalowpark :)\n - [Expedia Hotel Recommendations][4] @idle_speculation's legendary solo win (with ***ONE*** submission!)\n - [Porto Seguroâ€™s Safe Driver Prediction][5] @mjahrer's de-noising auto encoder solution that is in a league of it's own.\n - [Homesite Quote Conversion][6] obvious shelf where big teams pushed for gold places.\n - [NFL Big Data Bowl 2020][11] Zoo win again, with a striking margin\n - [Liverpool ION Switching][10] extreme outlier top score from team that found a flaw in the data preparation.\n\n### Public Kernels Winning Medals\n - [Recruit Restaurant Visitor Forecasting][7] long trail of identical bronze scores.\n - [TalkingData AdTracking Fraud Detection Challenge][8] last day share of silver-worthy CSV file.\n\n\n(This notebook is adapted from [Winning Team Submission Traces][1] which shows the submissions scores over time for the winning team in each competition.)\n\n\n### Revisions\n\nVersion 9 creates two plots per competition: one for medalists and one for all teams. Some of the 'global' plots have outliers that squash the range, making most of the field look like one flat line. Excluding outliers is a balance between omitting some teams from view and keeping the plot interesting! This will be fixed later. It's useful to have this version to refer back to.\n\n [1]: https://www.kaggle.com/jtrotman/winning-team-submission-traces\n [2]: https://www.kaggle.com/jtrotman/blender-medal-counts\n [3]: #rossmann-store-sales\n [4]: #expedia-hotel-recommendations\n [5]: #porto-seguro-safe-driver-prediction\n [6]: #homesite-quote-conversion\n [7]: #recruit-restaurant-visitor-forecasting\n [8]: #talkingdata-adtracking-fraud-detection\n [9]: #ga-customer-revenue-prediction\n [10]: #liverpool-ion-switching\n [11]: #nfl-big-data-bowl-2020\n [12]: #walmart-recruiting-trip-type-classification\n [13]: #mercedes-benz-greener-manufacturing\n [14]: #avito-duplicate-ads-detection\n [15]: #planet-understanding-the-amazon-from-space\n "},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"%matplotlib inline\nimport gc, os, sys, time\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\nfrom IPython.display import HTML, display\n\nIN_DIR = os.path.join('..', 'input', 'meta-kaggle')\n\ndef read_csv_filtered(csv, col, values):\n    dfs = [df.loc[df[col].isin(values)]\n           for df in pd.read_csv(os.path.join(IN_DIR, csv), chunksize=100000, low_memory=False)]\n    return pd.concat(dfs, axis=0)\n\ncomps = pd.read_csv(os.path.join(IN_DIR, 'Competitions.csv')).set_index('Id')\ncomps = comps.query(\"HostSegmentTitle != 'InClass'\")\nidx = comps.EvaluationAlgorithmName.isnull()\ncomps.loc[idx, 'EvaluationAlgorithmName'] = comps.loc[idx, 'EvaluationAlgorithmAbbreviation']\n\ncomps['EvaluationLabel'] = comps.EvaluationAlgorithmAbbreviation\nidx = comps.EvaluationLabel.str.len() > 30\ncomps.loc[idx, 'EvaluationLabel'] = comps.loc[idx, 'EvaluationLabel'].str.replace(r'[^A-Z\\d\\-]', '')\n\ncomps['DeadlineDate'] = pd.to_datetime(comps.DeadlineDate)\ncomps['EnabledDate'] = pd.to_datetime(comps.EnabledDate)\ncomps['DeadlineDateText'] = comps.DeadlineDate.dt.strftime('%c')\ncomps['EnabledDateText'] = comps.EnabledDate.dt.strftime('%c')\ncomps['Year'] = comps.EnabledDate.dt.year\ncomps['RewardQuantity'].fillna('', inplace=True)\n\n# Read teams - for LB ranks\nteams = read_csv_filtered('Teams.csv', 'CompetitionId', comps.index).set_index('Id')\nteams = teams.dropna(subset=['PublicLeaderboardSubmissionId', 'PrivateLeaderboardSubmissionId'])\n\n# Read submissions - to get scores\nsubs = read_csv_filtered('Submissions.csv', 'TeamId', teams.index).set_index('Id')\nsubs['SubmissionDate'] = pd.to_datetime(subs.SubmissionDate)\n\nasfloats = ['PublicScoreLeaderboardDisplay',\n            'PublicScoreFullPrecision',\n            'PrivateScoreLeaderboardDisplay',\n            'PrivateScoreFullPrecision',]\n\nsubs[asfloats] = subs[asfloats].astype(float)\nsubs = subs.query('not IsAfterDeadline').copy()\nsubs['CompetitionId'] = subs.TeamId.map(teams.CompetitionId)\n\n# values some competitions use as invalid scores\nfor bad in [99, 999999]:\n    for c in asfloats:\n        idx = (subs[c] == bad)\n        subs.loc[idx, c] = subs.loc[idx, c].replace({bad: np.nan})\n\n# Map scores to teams\n# Beware: submission IDs are read as floats - should read as object & discard missing\nteams['PublicScore'] = teams.PublicLeaderboardSubmissionId.map(subs.PublicScoreFullPrecision)\nteams['PrivateScore'] = teams.PrivateLeaderboardSubmissionId.map(subs.PrivateScoreFullPrecision)\nteams['Medal'].fillna(0, inplace=True)\n\nscore_cols = ['PublicScore', 'PrivateScore']\n\n# The Random Number Grand Challenge looked like fun!\nidx = teams.PublicScore > 1e99\nteams.loc[idx, score_cols] = np.nan\n\n# Mercedes-Benz Greener Manufacturing looked like fun!\nidx = (teams.PublicScore < -7e7)\nteams.loc[idx, ['PublicScore']] = np.nan\n\n# Ordering for groupby\ncomp_id_order = comps.DeadlineDate.rank(method='first', ascending=False)\ndisplay_order = teams.CompetitionId.map(comp_id_order)"},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"plt.rc(\"figure\", figsize=(18, 12))\nplt.rc(\"font\", size=14)\nplt.rc(\"axes\", xmargin=0.01)\nplt.rc(\"axes\", edgecolor='#606060')\n\n\ndef find_range(scores):\n    scores = sorted(scores)\n    n = len(scores)\n    max_i = n - 1\n    for i in range(n // 2, n):\n        best = scores[:i]\n        if len(best):\n            m = np.mean(best)\n            s = np.std(best)\n            if s != 0:\n                z = (scores[i] - m) / s\n                if abs(z) < 3:\n                    max_i = i\n    return scores[0], scores[max_i]\n\n\ndef get_range(df):\n    comp_id = df.iloc[0].CompetitionId\n    c = comps.loc[comp_id]\n\n    mul = -1 if c.EvaluationAlgorithmIsMax else 1\n    a, b = find_range(df.PublicScore.dropna().values * mul)\n    A, B = find_range(df.PrivateScore.dropna().values * mul)\n\n    A = min(a, A) * mul\n    B = max(b, B) * mul\n\n    R = (B - A)\n    B += R / 20\n    A -= R / 20\n    return min(A, B), max(A, B)"},{"cell_type":"markdown","metadata":{},"source":"# Outlier Score Detection\n\nThis is work in progress...\n\nIf you want an explanation of this please comment and I'd be happy to write one :)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"NBUCKETS = 8\nthresholds = np.arange(NBUCKETS + 1) / NBUCKETS\n\nthresholds = {\n    # EvaluationAlgorithmIsMax, best score on left\n    True  : thresholds[::-1],\n    # not EvaluationAlgorithmIsMax, best score on left\n    False : thresholds\n}\n\nthresholds"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"quantiles = {}\nfor cid, sub_df in teams.groupby('CompetitionId'):\n    c = comps.loc[cid]\n    thres = thresholds[c.EvaluationAlgorithmIsMax]\n    vs = sub_df.PrivateScore.dropna().quantile(thres).values\n    quantiles[cid] = (vs - vs.min()) / (vs.max() - vs.min())"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"quan_df = pd.Series(quantiles).apply(pd.Series).add_prefix('q')\nquan_df.shape"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"quan_df.describe().T"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"quan_df.index = comps.reindex(quan_df.index).Title"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"mag = (quan_df @ thresholds[True]) / quan_df.sum(1)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"idx = np.argsort(mag)"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"cmap = 'jet'"},{"cell_type":"markdown","metadata":{},"source":"Hard to show all the labels with **Seaborn**..."},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"sns.heatmap(quan_df.iloc[idx], cmap=cmap);"},{"cell_type":"markdown","metadata":{},"source":"But **Pandas** can."},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"with pd.option_context(\"display.max_rows\", len(quan_df)):\n    # need round(2) AND set_precision(2)\n    display(quan_df.iloc[idx].round(2).style.background_gradient(axis=None, cmap=cmap).set_precision(2))"},{"cell_type":"markdown","metadata":{},"source":"# The Competitions"},{"cell_type":"code","execution_count":13,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"medal_colors = ['Gold', 'Silver', 'Chocolate']\n\nrank_cols = ['PublicLeaderboardRank', 'PrivateLeaderboardRank']\n\ntop_cols = [\n    'TeamName', 'ScoreFirstSubmittedDate', 'LastSubmissionDate',\n    'PublicLeaderboardRank', 'PublicScore', 'PrivateScore'\n]\n\n\nfor i, (comp, sub_df) in enumerate(teams.groupby(display_order)):\n        \n    comp_id = sub_df.iloc[0].CompetitionId\n    c = comps.loc[comp_id]\n\n    ranked = sub_df.dropna(subset=rank_cols).copy()\n    if ranked.shape[0] < 1:\n        continue\n\n    ranked[rank_cols] = ranked[rank_cols].astype(int)\n    public_redundant = ranked.eval('PublicLeaderboardRank==PrivateLeaderboardRank').all()\n    public_redundant = public_redundant or (ranked.PublicScore.var() == 0)\n    abs_diff = ranked.eval('abs(PublicLeaderboardRank-PrivateLeaderboardRank)')\n    shakeup = (abs_diff / len(ranked)).mean()\n    \n    # use LB rank to sort, then we don't even need to know EvaluationAlgorithmIsMax!\n    df = ranked.set_index('PrivateLeaderboardRank')\n    df = df.sort_index()\n\n    if not public_redundant:\n        pub = ranked.set_index('PublicLeaderboardRank')\n        pub = pub.sort_index()\n    else:\n        pub = None\n\n    markup = (\n        '<h1 id=\"{Slug}\">{Title}</h1>'\n        '<p>'\n        'Type: {HostSegmentTitle} &mdash; <i>{Subtitle}</i>'\n        '<br/>'\n        '<a href=\"https://www.kaggle.com/c/{Slug}/leaderboard\">Leaderboard</a>'\n        '<br/>'\n        'Dates: <b>{EnabledDateText}</b> &mdash; <b>{DeadlineDateText}</b>'\n        '<br/>'\n        '<b>{TotalTeams}</b> teams; <b>{TotalCompetitors}</b> competitors; '\n        '<b>{TotalSubmissions}</b> submissions'\n        '<br/>'\n        'Leaderboard percentage: <b>{LeaderboardPercentage}</b>'\n        '<br/>'\n        'Evaluation: <a title=\"{EvaluationAlgorithmDescription}\">{EvaluationAlgorithmName}</a>'\n        '<br/>'\n        'Reward: <b>{RewardType}</b> {RewardQuantity} [{NumPrizes} prizes]'\n        '<br/>').format(**c)\n\n    tmp = df.loc[[1, 2, 3, 4, 5], top_cols].copy()\n    tmp.columns = tmp.columns.str.replace(r'([a-z])([A-Z])', r'\\1<br/>\\2')\n    markup += 'Top Five: '\n    markup += tmp.to_html(index_names=False,\n                          notebook=True,\n                          escape=False,\n                          na_rep='')\n    display(HTML(markup))\n\n    vc = df.Medal.value_counts()\n    title = c.Title\n    if str(c.Year) not in title:\n        title += f' [{c.Year}]'\n    if 1 in vc and 2 in vc and 3 in vc:\n        title += f' - {vc[1]} gold; {vc[2]} silver; {vc[3]} bronze'\n    if shakeup > 0:\n        title += f' - {shakeup:.3f} shake-up'\n\n    mthres = np.where(df.Medal.diff())[0]\n\n    mdl = df.query('Medal!=0')\n    if len(mdl) < 1:\n        # no medals; show top 10%\n        n = int(np.ceil(len(df) / 10))\n        mdl = df.head(n)\n\n    ############################## medalists\n    plt.subplot(2, 1, 1)\n\n    if pub is not None:\n        pub.head(len(mdl))['PublicScore'].plot(color='Green',\n                                               label='Best public')\n        mdl['PublicScore'].plot(color='Blue', label='Public score', alpha=0.3)\n\n    mdl['PrivateScore'].plot(color='Red', label='Private score')\n\n    if len(mthres) == 4:\n        xmin = 0.5\n        for color, xval in zip(medal_colors, mthres[1:]):\n            plt.axvspan(xmin, xmax=xval + 0.5, color=color, alpha=0.2)\n            xmin = xval + 0.5\n\n    plt.xlim(left=1)\n    plt.title(title)\n    plt.legend()\n    plt.ylabel(c.EvaluationLabel)\n    plt.grid(True, axis='x')\n\n    ############################## global\n    plt.subplot(2, 1, 2)\n\n    if pub is not None:\n        pub['PublicScore'].plot(color='Green', label='Best public')\n        df['PublicScore'].plot(color='Blue', label='Public score', alpha=0.3)\n\n    ax = df['PrivateScore'].plot(color='Red', label='Private score')\n\n    if len(mthres) == 4:\n        xmin = 0.5\n        for color, xval in zip(medal_colors, mthres[1:]):\n            plt.axvspan(xmin, xmax=xval + 0.5, color=color, alpha=0.2)\n            xmin = xval + 0.5\n\n    bottom, top = get_range(df)\n    plt.ylim(bottom, top)\n    plt.xlim(left=1)\n    plt.legend()\n    plt.ylabel(c.EvaluationLabel)\n    plt.grid(True, axis='x')\n\n    ############################## end\n    plt.tight_layout()\n    plt.show()"},{"cell_type":"markdown","metadata":{},"source":"____\n\n# Conclusions\n\nGenerally, the gold, silver, bronze thresholds do a really good job, it is rare that a publicly shared solution gets a medal.\n\nPlotting the distribution of marathon running times shows spikes: people push harder to hit a new landmark time like sub three hours. It's the same here, the lure of a competition medal is strong!"},{"cell_type":"code","execution_count":14,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"_ = \"\"\"\nRe-run to include recent competitions:\n\n    2021-06-23 | Slug:iwildcam2021-fgvc8\n    2021-06-28 | Slug:coleridgeinitiative-show-us-the-data\n    2021-07-05 | Slug:tabular-playground-series-jun-2021\n    2021-08-10 | Slug:google-smartphone-decimeter-challenge\n    2021-08-11 | Slug:commonlitreadabilityprize\n\n\n\"\"\""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":2}