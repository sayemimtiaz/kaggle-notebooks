{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Fugue\n\n[Fugue](https://github.com/fugue-project/fugue) was just open sourced. It unifies core interfaces for computing, and makes your code run on Pandas, Spark and Dask without change.\n\nYou can write Fugue workflows using [programming interface](https://fugue-tutorials.readthedocs.io/en/latest/tutorials/dag.html) or [Fugue SQL](https://fugue-tutorials.readthedocs.io/en/latest/tutorials/sql.html) based on the project needs. In this notebook, I am going to use Fugue SQL which is an enriched SQL language, it can use , it can run on different backends as well.\n\nIn this particular case, you see 0 dependency on Fugue and Spark, you only see native python code and SQL, but they run on Fugue and Spark\n\n\n# Acknowledgement\n\nI find [Manch Hui](https://www.kaggle.com/manchunhui)'s [notebook](https://www.kaggle.com/manchunhui/us-presidential-election-sentiment-analysis) and [Maksym Shkliarevskyi](https://www.kaggle.com/maksymshkliarevskyi)'s [notebook](https://www.kaggle.com/maksymshkliarevskyi/us-election-eda-sentiment-analysis) are very helpful. Thank you!\n\n\n# Objective\n\nWe try to:\n\n* do basic data exploration and sentiment analysis using existing popular tools\n* demonstrate a compute framework agnostic and scale agnostic way for data analytics\n* show SQL mindset/approach is also great for science work, especially when you can chain them together\n* show that native python + SQL is powerful enough and Fugue can invisibly orchestrate them\n\nWe DO NOT try to:\n\n* get the best quality result, which requires significant effort on data cleaning and fine tuning\n* compete with pandas on speed. For this dataset, pandas may be faster, but the Fugue approach can handle significantly larger scale data with no code change, and the performance will still be optimal, this is what pandas can't do\n\n\n# Setup environment\n\nYou only need to install the [fuggle](https://github.com/fugue-project/fuggle) package (Fugue for Kaggle users), and call `setup` with a backend (this will also enable the highlight of Fugue SQL in side cells). And in this notebook, we use spark."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"!pip install fuggle>=0.0.6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuggle import setup, Plot, PlotBar, PlotBarH, PlotLine\n\nsetup(\"spark\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge Data\n\nIt will be much simpler if we can merge the multiple csv files into one parquet file for the following setups to use, benefits:\n* Reading csv is normally slower than reading parquet\n* When saving to parquet, all columns data types become explicity, so the following steps will have no concern\n\nBecause this Kaggle dataset is designed more for pandas to process locally, so we use pandas to convert the data.\n\n**Notice:** the dataset quality is questionable. For example we need to firstly conflate `United States` and `United States of America`"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom time import sleep\nimport pandas as pd\n\n\ndef load_raw_data() -> pd.DataFrame:\n    dfs = []\n    for hashtag in [\"donaldtrump\",\"joebiden\"]:\n        df = pd.read_csv(f\"../input/us-election-2020-tweets/hashtag_{hashtag}.csv\", lineterminator='\\n')\n        df[\"hashtag\"] = hashtag # add hashtag column\n        dfs.append(df)\n        print(df.shape)\n    all_df = pd.concat(dfs)\n    for col in [\"created_at\", \"user_join_date\", \"collected_at\"]:\n        all_df[col] = pd.to_datetime(all_df[col]).astype('datetime64[us]')\n    for col in [\"tweet_id\",\"likes\",\"retweet_count\",\"user_id\",\"user_followers_count\"]:\n        all_df[col] = all_df[col].astype(int)\n    all_df[\"country\"] = all_df[\"country\"].replace(\"United States of America\", \"United States\")\n    return all_df.reset_index(drop=True)\n\ndf = load_raw_data()\ndf.to_parquet(\"/kaggle/working/tweets_raw.parquet\")\n# here we also save 5% of sample data into another parquet file\ndf.sample(frac=0.05, replace=False, random_state=1).to_parquet(\"/kaggle/working/tweets_sample.parquet\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's use Fugue SQL on Spark (and we are using 4 cores Kaggle provides) to load the raw parquet and save to smaller partitions. This has performance benefit for spark execution -- the more partitions you have, the better load balance but the worse overhead. As you can see, the following SQL code is a mix of standard SQL and enriched syntax."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\nLOAD \"/kaggle/working/tweets_raw.parquet\"\nDROP COLUMNS collected_at\nSELECT DISTINCT * WHERE tweet_id>0  # dedup and remove invalid records\nSAVE AND USE PREPARTITION 16 OVERWRITE \"/kaggle/working/tweets.parquet\"\nPRINT ROWCOUNT\nSELECT country, COUNT(*) AS ct GROUP BY country ORDER BY ct DESC\nPRINT ROWS 100\nSELECT * WHERE country IS NULL\nPRINT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see there are many records without location information. **Analytics based on location may not be accurate for this dataset**. But for demonstration (of Fugue) purpose, we assume it's alright.\n\nNow let's get some simple stats from the data. You start to see some standard SQL. Using Spark backend, they are executed as Spark SQL."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/tweets.parquet\"\n\nSELECT hashtag, country, COUNT(*) AS ct \n    FROM df\n    WHERE country IS NOT NULL\n    GROUP BY hashtag, country\n    ORDER BY ct DESC\nPRINT TITLE \"by country\"\n\nSELECT hashtag, state, COUNT(*) AS ct \n    FROM df\n    WHERE country = \"United States\"\n    GROUP BY hashtag, state\n    ORDER BY ct DESC\nPRINT TITLE \"by state\"\n\nSELECT *, DATE_TRUNC(\"Hour\", created_at) AS ts FROM df\nSELECT ts, SUM(IF(hashtag=\"donaldtrump\",0,1)) AS biden, SUM(IF(hashtag=\"donaldtrump\",1,0)) AS trump GROUP BY ts\n# In fuggle, there are visualization extensions such as PlotLine, they are simple and quick but may not be beautiful\n# You can write your own exetension for specific cases and you can make the charts amazing\nOUTPUT USING PlotLine(x=\"ts\", order_by=[\"ts\"], title=\"hourly tweets\", logy=true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing (data cleaning)\n\nWe need to normalize the tweet text, remove special characters, stop words and lemmatize them. For this part, I borrowed the idea from [Manch Hui](https://www.kaggle.com/manchunhui)'s great [notebook](https://www.kaggle.com/manchunhui/us-presidential-election-sentiment-analysis).\n\nSo the first question is, for a local pandas dataframe how we do it? In the following code `lemmatize` is to do everything we mentioned given a single piece of text. `lemmatize_tweet` is a wrapper function dealing with a pandas dataframe input. And you can see we wrote some assertion as simple unit tests in notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport unicodedata\nfrom typing import Iterable, Dict, Any, List\n\ndef lemmatize(text):\n    filtered_sent=\"\"\n    wnl = nltk.stem.WordNetLemmatizer()\n    stopwords = nltk.corpus.stopwords.words('english')\n    text = (unicodedata.normalize('NFKD', text)\n            .encode('ascii', 'ignore')\n            .decode('utf-8', 'ignore')\n            .lower())\n    text = re.sub(r'https?.+|[^(a-zA-Z)(0-9)\\s]',' ',text)\n    words = text.split()\n    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n\nassert ['trump', 'word', 'c', 'dd', '123'] == lemmatize(\"#trump:a,wOrds.c      .dd. 123 https://asdasdf, http://asdasdf,\")\n\n# schema: *-tweet+words:[str]\ndef lemmatize_tweet(df:pd.DataFrame) -> pd.DataFrame:\n    df[\"words\"] = df[\"tweet\"].apply(lemmatize)\n    return df.drop([\"tweet\"],axis=1)\n\n\ntdf = pd.DataFrame([[\"abc def,g http3\"]],columns=[\"tweet\"])\nlemmatize_tweet(tdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Actually, `lemmatize_tweet` is already a [Fugue extension](https://fugue-tutorials.readthedocs.io/en/latest/tutorials/extensions.html). The comment is called schema hint, it tells Fugue that the output will be the input excluding `tweet` column and plus a `words` column whose type is an array of strings.\n\nNow let's use it in Fugue SQL to generate another file `tweets_lem.parquet`."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\nLOAD \"/kaggle/working/tweets.parquet\"\nSELECT tweet_id, FIRST(tweet) AS tweet GROUP BY tweet_id\nTRANSFORM USING lemmatize_tweet\nSAVE AND USE OVERWRITE \"/kaggle/working/tweets_lem.parquet\"\nPRINT ROWCOUNT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute Sentiment Polarities\n\nAgain, the idea of computing polarities is borrowed from [Manch Hui](https://www.kaggle.com/manchunhui)'s [notebook](https://www.kaggle.com/manchunhui/us-presidential-election-sentiment-analysis) and [Maksym Shkliarevskyi](https://www.kaggle.com/maksymshkliarevskyi)'s [notebook](https://www.kaggle.com/maksymshkliarevskyi/us-election-eda-sentiment-analysis). I will compute both VADER and TextBlob scores.\n\nFirst of all let's consider the problem with a local pandas dataframe input"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n# input_has: words:[str]\n# schema: *,vader_score:double,textblob_score:double-words\ndef compute_polarities(df:pd.DataFrame) -> pd.DataFrame:\n    sid = SentimentIntensityAnalyzer()\n    text = df[\"words\"].apply(lambda w: \" \".join(w))\n    df[\"vader_score\"] = text.apply(lambda t: sid.polarity_scores(t)[\"compound\"])\n    df[\"textblob_score\"] = text.apply(lambda t: TextBlob(t).sentiment.polarity)\n    return df.drop([\"words\"],axis=1)\n\ntdf = pd.DataFrame([[\"i really love it\".split(\" \"),1]], columns=[\"words\",\"a\"])\ncompute_polarities(tdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the schema hint of `compute_polarities`, we know two things:\n\n* the input data must contain `words` column, if not, an error will raise before execution\n* the output will remove `words` and add two score columns\n\nAgain, it is just a native python function like you normally do, just add some hints using comments, and also add a small test after you write the code. Now let's compute using Fugue SQL"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\nTRANSFORM (LOAD \"/kaggle/working/tweets_lem.parquet\") USING compute_polarities\nSAVE OVERWRITE \"/kaggle/working/tweets_polarities.parquet\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Polarity Analysis\n\nThe following scripts are to analyze the polarities from different perspectives.\n\n## Polarity on locations\n\nThe following script computes likes weighted average polarities based on US states, and country"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\ndf = LOAD \"/kaggle/working/tweets.parquet\"\npolarity = LOAD \"/kaggle/working/tweets_polarities.parquet\"\n\ndata = \n    SELECT hashtag, state, country, vader_score, textblob_score, likes\n    FROM df INNER JOIN polarity ON df.tweet_id = polarity.tweet_id\n    \nSELECT \n        state,\n        SUM(IF(hashtag=\"donaldtrump\",NULL,vader_score*likes))/SUM(IF(hashtag=\"donaldtrump\",0,likes)) AS biden,\n        SUM(IF(hashtag=\"donaldtrump\",vader_score*likes,NULL))/SUM(IF(hashtag=\"donaldtrump\",likes,0)) AS trump\n    FROM data\n    WHERE country = \"United States\" AND state != \"Guam\"\n    GROUP BY state\n    \nOUTPUT USING PlotBarH(x=\"state\", order_by=\"trump desc\", height=2.0)\n\n\nSELECT \n        country,\n        SUM(IF(hashtag=\"donaldtrump\",NULL,vader_score*likes))/SUM(IF(hashtag=\"donaldtrump\",0,likes)) AS biden,\n        SUM(IF(hashtag=\"donaldtrump\",vader_score*likes,NULL))/SUM(IF(hashtag=\"donaldtrump\",likes,0)) AS trump,\n        COUNT(*) AS ct\n    FROM data\n    GROUP BY country\n    \nSELECT * ORDER BY ct DESC LIMIT 10\nOUTPUT USING PlotBar(x=\"country\", y=[\"biden\",\"trump\"], order_by=\"ct desc\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polarity on time"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%fsql\ndf = SELECT * FROM (LOAD \"/kaggle/working/tweets.parquet\") WHERE country = \"United States\"\npolarity = LOAD \"/kaggle/working/tweets_polarities.parquet\"\n\ndata = \n    SELECT DATE_TRUNC(\"Day\", created_at) AS ts, hashtag, likes, country, vader_score, textblob_score\n    FROM df INNER JOIN polarity ON df.tweet_id = polarity.tweet_id\n    \nSELECT \n        ts,\n        SUM(IF(hashtag=\"donaldtrump\",NULL,textblob_score*likes))/SUM(IF(hashtag=\"donaldtrump\",0,likes)) AS biden,\n        SUM(IF(hashtag=\"donaldtrump\",textblob_score*likes,NULL))/SUM(IF(hashtag=\"donaldtrump\",likes,0)) AS trump\n    FROM data\n    GROUP BY ts\n    \nOUTPUT USING PlotLine(x=\"ts\", order_by=\"ts\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}