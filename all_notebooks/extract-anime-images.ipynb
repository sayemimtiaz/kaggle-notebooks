{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ![Anime World](https://cache.desktopnexus.com/thumbseg/2305/2305029-bigthumbnail.jpg)Welcome to the world of Anime. This code requieres the dataset I created [Anime characters](https://www.kaggle.com/shanmukh05/anime-names-and-image-generation) . You can fork the notebook and can use to extract any other parts of the above mentioned site. Let's dive in."},{"metadata":{},"cell_type":"markdown","source":"The following cells of code will help you to extract the links to each anime character present in [Anime and Manga](https://myanimelist.net/character.php) website."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing the requirements\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom urllib.request import Request, urlopen\nimport time\nimport os\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What I am doing is  othing but web scraping. This notebook is also helpful for learning the web scraping."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_link1 = \"https://myanimelist.net/character.php?letter=\"   # base path for every letter (A-Z)\nbase_link2 = \"&show=\"             # base path for every sub pages of given letter (50 anime characters per page)\nletters_ls = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\nbase_links = [base_link1 + i for i in letters_ls]\n\n# these are the number of pages per each alphabet. \n#For exmaple the anime characters with starting letter \"Z\" are present in 7 pages \n#(50 per each page and last page may not contain 50 images)\nnum_dict = {}\nnum_dict[\"Z\"] = 7\nnum_dict[\"Y\"] = 67\nnum_dict[\"X\"] = 2\nnum_dict[\"W\"] = 22\nnum_dict[\"V\"] = 13\nnum_dict[\"U\"] = 23\nnum_dict[\"T\"] = 122\nnum_dict[\"S\"] = 186\nnum_dict[\"R\"] = 29\nnum_dict[\"Q\"] = 2\nnum_dict[\"P\"] = 12\nnum_dict[\"O\"] = 62\nnum_dict[\"N\"] = 74\nnum_dict[\"M\"] = 146\nnum_dict[\"L\"] = 26\nnum_dict[\"K\"] = 206\nnum_dict[\"J\"] = 16\nnum_dict[\"I\"] = 61\nnum_dict[\"H\"] = 115\nnum_dict[\"G\"] = 29\nnum_dict[\"F\"] = 41\nnum_dict[\"E\"] = 20\nnum_dict[\"D\"] = 26\nnum_dict[\"C\"] = 31\nnum_dict[\"B\"] = 32\nnum_dict[\"A\"] = 103\n\nprint(sum(list(num_dict.values())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This block will give you links to each page of each letter (ie., 7 links for \"Z\" letter)"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_links = []\n\nfor num,letter in enumerate(letters_ls):\n    for i in range(num_dict[letter]):\n        if i==0:\n            final_links.append(base_links[num])\n        else:\n            final_links.append(base_links[num]+base_link2+str(i*50))\nlen(final_links)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On running below block you will get final links ie., each link contains details of one anime character. Basically I used \n\n> time.sleep()\n\nto  make sure that there is no much traffic from our side during the web scraping. As we are going through a lot of pages. it is better to use the sleep to make sure that we do not get the [Error 403](https://stackoverflow.com/questions/16627227/http-error-403-in-python-3-web-scraping/31758803)."},{"metadata":{"trusted":true},"cell_type":"code","source":"a_tags = []\ntime.sleep(180)\nfor i,link in enumerate(final_links):\n    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n         'Referer': 'https://cssspritegenerator.com',\n         'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n         'Accept-Encoding': 'none',\n         'Accept-Language': 'en-US,en;q=0.8',\n         'Connection': 'keep-alive'}\n    req = Request(link , headers=headers)\n    webpage = urlopen(req).read()\n    soup = BeautifulSoup(webpage, \"html.parser\")\n    links = soup.findAll(\"a\")\n    for l in links:\n        ls = []\n        if l.img:\n            a_tags.append(l)\n    if i%150==0 and i!=0:\n        print(\"Completed {} links and waiting for 300 seconds\".format(i))\n        time.sleep(300)\n        print(\"Waiting completed, going for next set of 150 links\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the above code gives you the output as a csv file containg the links to every anime character present in [Anime and Manga](https://myanimelist.net/character.php). Below code helps you to get the names and images of the anime characters using the above links."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code gives you the actual links form a tag that we got from previous csv file\nanime_links = []\nfor link in a_tags:\n    if \"onclick\" in str(link):\n        a_tags.remove(link)\n    else:\n        anime_links.append(link.attrs[\"href\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new folder where we store the images\n\nif os.path.isdir(\"./dataset/\") == False:\n    os.mkdir(\"./dataset/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_links = pd.read_csv(\"../input/anime-names-and-image-generation/anime_links.csv\")\nanime_links = list(df_links[\"Link\"])\nlen(anime_links)     #72992","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following block gives you final images stored in dataset folder (./dataset). You can also extract names of characters form name of each anime image."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nanime_names = []\n\nfor i,src in enumerate(anime_links):\n    src_new = src.encode('ascii', 'ignore').decode('ascii')       #there are some names where there are characters other than ascii. To reduce complexity iam excluding those\n    if src_new != src:\n        continue\n    req = Request(src , headers=headers)\n    webpage = urlopen(req).read()\n    soup = BeautifulSoup(webpage, \"html.parser\")\n    links = soup.findAll(\"a\")\n    for link in links:\n        if link.img and \"border\" not in str(link) and \"onclick\" not in str(link):  # actual image links\n            split= str(link.img).split(\"\\\"\")\n            img_src = split[5]\n            name = split[1]\n            \n            if img_src.split(\".\")[-1] == \"jpg\" and len(name.split(\"/\"))==1:\n                split = str(link.img).split(\"\\\"\")\n                response = requests.get(img_src, headers=headers)\n                image_name = \"./dataset/\"+name + \".jpg\"\n                file = open(image_name, \"wb\")\n                file.write(response.content)\n                file.close()\n\n    if i%120==0 and i!=0:  # downloading 120 images and waiting for 4 min and then the loop continues.\n        print(\"Completed {} links and waiting for 240 seconds\".format(i))\n        time.sleep(240)\n        print(\"Waiting completed, going for next set of 120 links\\n\\n\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}