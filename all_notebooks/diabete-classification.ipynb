{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diabete Classification"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# for visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold # helps us split the data\nfrom sklearn import metrics # import the metrics we will use\nimport warnings\n\n\n\n# import the models we will be using\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the data"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/pyms-diabete/diabete.csv\")\ndf.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the data :{}\".format(df.shape)) # shape of aour data\ndf.info() # gives us a general view of the data types","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can already see that there are no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.profile_report() # general overview of the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['diabete']) # distribution of our target variable\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Closer look of the count\nprint(df['diabete'].value_counts(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation\n\nLet's take a look at the correlation between the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df.corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(df_corr, cmap='icefire', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 6 features which are more or less correlated with the target.\nSince we don't categorical features we can directly feed the data into our algorithm.\nNext, we are going to take a closer look at the interaction between each feature by using the pairplot function from seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,18))\nsns.pairplot(data=df, hue='diabete')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some inapropriate values by looking at this plot. After some research I found out the minimal value for each feature.\nIt's not possible to have a 0 bmi(should be >=1) nor 0 tension(should be  >= 40)\nnor a thickness equal to 0 (should be >5 depending on the sex)\n\nFrom this plot, we can see that the vast majority of peaople suffering from Diabete have: -pregnant,\n-higher BMI, higher g and two more other(confirmed by the correlation matrix)"},{"metadata":{},"cell_type":"markdown","source":"## Directly feed the unpreprocessed data into an algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data\nX = df.drop('diabete',1)\ny = df[['diabete']]\n\n\n'''we can definitely split the whole data into train, val and test sets. However, we are going to use Stratified\nKFold class from sklearn which will generate random index from the data and preserve the percentage of samples\nfor each class'''\n\n# use the stratify parameter in order to preserve the percentage of the class in each fold\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=42)\n\n# print(\"Train size-\\tX: {}\\ty: {}\".format(X_train.shape, y_train.shape))\n# print(\"Valid size-\\tX: {}\\ty: {}\".format(X_val.shape, y_val.shape))\n# print(\"Test size-\\tX: {}\\ty: {}\".format(X_test.shape, y_test.shape))\n\n\ndef validation_and_test_score(model, X_train=X_train, y_train=y_train, X_test=X_test,\n                              y_test=y_test, epochs=5):\n    '''Helper function'''\n    # validation strategy using stratified KFold\n    kf = StratifiedKFold(n_splits = epochs, shuffle = True, random_state = 42) \n    # y_oof = np.zeros(X_train.shape[0]).astype('object')\n    pred = np.zeros(X_test.shape[0]).astype('object')\n    i = 0\n    val_score = []\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = model\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_proba = clf.predict_proba(X_vl)[:, 1]\n        # y_oof[val_idx] = y_pred_proba\n        val_score.append(metrics.roc_auc_score(y_vl, y_pred_proba))\n        print(\"Val AUC Fold # \" + str(i) + \": \" + str(val_score[i]))\n        pred += clf.predict_proba(X_test)[:, 1] / epochs\n        i += 1 \n    print(\"Validation AUC (mean) :\\t{}\".format(np.mean(val_score)))\n    print(\"Test AUC :\\t{}\".format(metrics.roc_auc_score(y_test, pred)))\n    \n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"without any data preprocessing(**raw data**), we got an AUC score of **0.8261** on the test set and **0.8244** on the validation set which are not that bad. Let's see if we can get a better one. <br> \nSince we are talking about health here, it is really important to se how the model behave, let's see the confusion matrix for more details"},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conf_matrix(model, X_test=X_test, y_test=y_test):\n    '''Helper function'''\n    y_pred = model.predict(X_test)\n    print(metrics.confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(log_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"84 are classified as really negative <br>\n26 are classified as really positive<br>\n16 are classified as positive but are actually negative<br>\n28 are classified as negative but are actually positive<br>\n\nThere are 28 people classified as non diabetic but actually are. This is not acceptable as diabete is konwn to be one of the most dangerous diseases. We can try to tweak the threashold depending on the result we want."},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_feats(df_entry):\n    df = df_entry.copy()\n    \n    # apply log transformation to skewed features (did not work)\n    # df['logPregnant'] = df['Pregnant'].apply(np.log1p)\n    # df['logInsuline-2H'] = df['Insuline-2H'].apply(np.log1p)\n        #     df['logPedigree'] = df['Pedigree'].apply(np.log) # use log because minimal value > 0\n#     df['logTriceps'] = df['Triceps'].apply(np.log1p)\n    \n    #categorise feature to better capture their meaning\n    df['categBloodPressure'] = pd.cut(df['tension'], bins=5, labels=[\"normal\", \"elevated\", \"high1\",\n                                                                      \"high2\", \"hyper\"])\n    df['categAge'] = pd.cut(df['age'], bins=7, labels=['20s','30s','40s','50s','60s','70s','80s'],\n                            include_lowest=True)\n    bmiBins = [1, 18.5, 24.9, 29.9, np.inf]\n    df['catBMI'] = pd.cut(df['bmi'], bins=bmiBins, labels=[\"under\",\"normal\",\"over\", \"obese\"])\n\n    res_df = pd.get_dummies(df, columns=['categBloodPressure','catBMI','categAge'], drop_first=True)\n#     res_df = pd.concat([df, dummies], axis=1)\n    \n    cols_to_drop = ['tension', 'bmi', 'age']\n    res_df = res_df.drop(cols_to_drop, axis=1)\n    return res_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = add_feats(df)\nprint(new_df.shape)\nnew_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feed the data after feature engineering into an algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_df.drop('diabete',1)\ny = new_df[['diabete']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With feature engineering our AUC on the test set went from **0.8261** to **0.8390** which is not a big boost but still good."},{"metadata":{},"cell_type":"markdown","source":"## Outlier detection\n\nThere are many ways to detect outliers in a dataset.\nHere we are going to tackle the outliers in the original DataFrame and see if it can improve our previous performance."},{"metadata":{},"cell_type":"markdown","source":"#### List of the most correlated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"out_df = df.copy() # Original DataFrame containing outliers\ncorr_df = out_df.corr()\nselected_features = corr_df[corr_df['diabete']>=0.13].index.drop('diabete').tolist()\nprint(\"Selected Features by the correlation Test: \\n\")\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Box plot of the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"axis = []\nfigure = plt.figure(figsize=(20,20))\nx=1\nfor c in selected_features:\n    axis.append(figure.add_subplot(4,4,x))\n    sns.boxplot(y=c, x='diabete', data=out_df, ax=axis[-1])\n    x+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to define a helper function which will help us detect and delete or replace the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_delete_outlier(f, df, delete_out=False, replace_out=False):\n    '''Helper function to detecte outliers'''\n    res_df = None\n    s = df[f] #the feature\n    q25, q75 = np.percentile(s, 25), np.percentile(s, 75)\n    inter_range = q75 - q25 #interquartile range\n    step = 1.5*inter_range\n    lower, upper = q25 - step, q75 + step\n    outliers = [x for x in s if x < lower or x > upper] # list of outlier\n    isOutlier = [True if x < lower or x > upper else False for x in s] # mask\n    \n    if delete_out==True:\n        res_def = df.drop(df[(s < lower) | (s > upper)].index) # drop the outliers\n    elif replace_out==True:\n        df.loc[isOutlier, f] = df[f].median() #locate outliers and replace them with the median\n        res_def = df.copy()\n    return res_def, outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feed the cleaned original data into an algorithm"},{"metadata":{},"cell_type":"markdown","source":"#### a- delete the outliers\nAfter deleting the outliers, We are going to feed the cleaned original data(witout feature engineering) into a model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#detecte and delete outliers in original df\ndf_with_out = out_df.copy()\nfor c in selected_features:\n    res_df, _ = detect_delete_outlier(c, df_with_out, delete_out=True)\nprint(\"featured df shape: {}\".format(df.shape))\nprint(\"cleaned df shape: {}\".format(res_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = res_df.drop('diabete',1)\ny = res_df[['diabete']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By deleting the ouliers, we got an AUC of **0.8023** on the test set an **0.8299** on the validation set, it indicates an overfitting model.<br>\nCompared to **0.8261** on the test set obtained by using the original raw data."},{"metadata":{},"cell_type":"markdown","source":"#### b- replace the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#detecte and replace outliers in original df\ndf_with_out = out_df.copy()\nfor c in selected_features:\n    res_df, _ = detect_delete_outlier(c, df_with_out, replace_out=True)\nprint(\"featured df shape: {}\".format(df.shape))\nprint(\"cleaned df shape: {}\".format(res_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = res_df.drop('diabete',1)\ny = res_df[['diabete']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By replacing the ouliers, we got an AUC of **0.8125** on the test set an **0.8266** on the validation set<br>\nCompared to **0.8261** on the test set obtained by using the original raw data."},{"metadata":{},"cell_type":"markdown","source":"So far the best AUC we got was around **0.8390** after feature engineering.<br>\n\nMaybe we should try to tackle these instances considered as outliers one by one. For example by replacing the 0 with the appropriate values.<br>\n**bmi, tension, thickness, glucose** shoud not take a 0 a value"},{"metadata":{},"cell_type":"markdown","source":"## Rectify the inappropriate values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcopy = df.copy()\ndfcopy['bmi'] = dfcopy['bmi'].apply(lambda x: 1 if x < 1 else x)\ndfcopy['tension'] = dfcopy['tension'].apply(lambda x: 40 if x < 40 else x)\ndfcopy['thickness'] = dfcopy['thickness'].apply(lambda x: 5.5 if x < 5.5 else x )\ndfcopy.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Directly feed this rectified data to an algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfcopy.drop('diabete',1)\ny = dfcopy[['diabete']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we first used the **raw** data earlier, we got an AUC of **0.8261** on the test set, and **now** we obteind **0.83** on the test set, without any other preprocessing."},{"metadata":{},"cell_type":"markdown","source":"Let's apply **feature engineering** to this rectified data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df_2 = add_feats(dfcopy)\n\nX = new_df_2.drop('diabete',1)\ny = new_df_2[['diabete']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nvalidation_and_test_score(log_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, by applying feature engineering on the new dataset we went from **0.8390(feature engineering on unrectified raw data)** to **0.8422**.\nFinally, let's take a glimpse at the confusion matrix of our last classifier."},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = log_reg.predict(X_test)\nprint(metrics.confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that:<br>\n85 people are classified as really negative (84 before)<br>\n30 people are classified as really positive (26 before)<br>\n15 are classified as positive but are actually negative (16 before)<br>\n24 are classified as negative but are actually positive (28 before)<br>\n\nWe can see that only by rectifying inappropriate values, the model classified 24 people to be non diabetic but actually are (28 before). Of course 24 people are still many but this shows the importance of diving into the values of the features we are given.\nWe could do a more specific adjustment to the values of our features if we had access to the gender of each person.\nWe can try many things in order to improve this result. For example, fine tune the model by tweaking the hyperparameters or tweak the threashold depending on the result we want."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}