{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# **Hypertension Risk Factors Table**\n\n![](https://sportslogohistory.com/wp-content/uploads/2018/09/georgia_tech_yellow_jackets_1991-pres-1.png)\n"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"###################### LOAD PACKAGES ##########################\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import PorterStemmer\nfrom IPython.core.display import display, HTML\nimport pandas as pd\nimport numpy as np\nimport functools\nimport re\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n!pip install bert-extractive-summarizer\nfrom summarizer import Summarizer\n\nimport torch\nfrom transformers import *\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint (df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (df.shape)\ndf.head()\n\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\n\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\n\nfor index, row in df.iterrows():\n    if ';' not in row['sha'] and os.path.exists('/kaggle/input/CORD-19-research-challenge/'+row['full_text_file']+'/'+row['full_text_file']+'/pdf_json/'+row['sha']+'.json')==True:\n        with open('/kaggle/input/CORD-19-research-challenge/'+row['full_text_file']+'/'+row['full_text_file']+'/pdf_json/'+row['sha']+'.json') as json_file:\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            keyword_list=['TB','incidence','age']\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n\n            df.loc[index, 'abstract'] = body.lower()\n\ndf=df.drop(['full_text_file'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.head()\nprint (df.shape)\n\ndf_master=pd.read_csv('../input/aipowered-literature-review-csvs/kaggle/working/Risk Factors/Hypertension.csv')\n#usecols=[\"Date\",\"Study\",\"Study Link\",\"Journal\",\"Days\",\"Range (Days)\",\"Sample (n)\",\"Study Type\"]\ndf_master.drop(df_master.columns[df_master.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\ndf_master.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# BERT pretrained question answering module\ndef answer_question(question,text, model,tokenizer):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    answer=answer.replace(\" ##\", \"\")\n    answer=answer.replace(\" · \", \"·\")\n    answer=answer.replace(\" . \", \".\")\n    answer=answer.replace(\" , \", \",\")\n    if '[SEP]'in answer or '[CLS]' in answer or answer=='':\n        answer='unk'\n        \n    return answer\n\n# custom sentence score\ndef score_sentence_prob(search,sentence,focus):\n    keywords=search.split()\n    sent_parts=sentence.split()\n    word_match=0\n    missing=0\n    for word in keywords:\n        word_count=sent_parts.count(word)\n        word_match=word_match+word_count\n        if word_count==0:\n            missing=missing+1\n    percent = 1-(missing/len(keywords))\n    final_score=abs((word_match/len(sent_parts)) * percent)\n    if missing==0:\n        final_score=final_score+.05\n    if focus in sentence:\n        final_score=final_score+.05\n    return final_score\n\ndef extract_data(text,word):\n    extract=''\n    text=text.replace(\"(\", \"parena\")\n    text=text.replace(\")\", \"parenb\")\n    if word in text:\n        res = [i.start() for i in re.finditer(word, text)]\n        extracted=text[res[0]:res[0]+75]\n        extracted=extracted.replace(\"parena\",\"(\")\n        extracted=extracted.replace(\"parenb\",\")\")\n        split_string = extracted.split(\")\", 2)\n        extract = split_string[0]+\")\"\n    return extract\n\n\ndef process_question(df,search,focus,df_master):\n    df_table = pd.DataFrame(data=None, columns=df_master.columns)\n    all_words=focus+' '+search\n    # focuses to make sure the exact phrase in text\n    #df1 = df[df['abstract'].str.contains(focus)]\n    # focus to make sure all words in text\n    df1=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in all_words))]\n    for index, row in df1.iterrows():\n        sentences = row['abstract'].split('. ')\n        pub_sentence=''\n        hi_score=0\n        study=''\n        hi_study_score=0\n        for sentence in sentences:\n            if len(sentence)>75 and focus in sentence:\n                rel_score=score_sentence_prob(search,sentence,focus)\n                if rel_score>.06:\n                    sentence=sentence.capitalize()\n                    if sentence[len(sentence)-1]!='.':\n                        sentence=sentence+'.'\n                    pub_sentence=pub_sentence+' '+sentence\n                    if rel_score>hi_score:\n                        hi_score=rel_score\n                \n        if pub_sentence!='':\n            text=row['abstract'][0:1000]\n            \n            question='how many patients or cases were in the study, review or analysis?'\n            sample=answer_question(question,text,model,tokenizer)\n            sample=sample.replace(\"#\", \"\")\n            sample=sample.replace(\" , \", \",\")\n            if sample=='19' or sample=='' or '[SEP]'in sample:\n                sample='unk'\n            if len(sample)>50:\n                sample='unk'\n            sample=sample.replace(\" \", \"\")\n            \n            question='what type or kind of review study analysis model was used?'\n            design=answer_question(question,text,model,tokenizer)\n            design=design.replace(\" ##\", \"\")\n            if '[SEP]'in design or '[CLS]' in design or design=='':\n                design='unk'\n            \n            #shorter = pub_sentence[0:1000]\n            ### get sever numbers\n            #question='what is the '+focus+' HR OR RR AOR hazard odds ratio ()?'\n            #severe=answer_question(question,text,model,tokenizer)\n            severe=''\n            stat_data=[\"parenaodd\",\"parenaor\",\"parenahr\",\"parenaar\"]\n            for statdat in stat_data:\n                statdat=focus+' '+statdat\n                extract=extract_data(row['abstract'],statdat)\n                severe=severe+' '+extract\n            \n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            score=hi_score\n            journal=row[\"journal\"]\n            if journal=='':\n                journal=row['full_text_file']\n            linka='https://doi.org/'+link\n            linkb=title\n            final_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,linkb)\n            #author_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,authors[0]+' et al.')\n            #sentence=pub_sentence+' '+author_link\n            sentence=pub_sentence\n            #sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'</p>'\n            to_append = [row['publish_time'],title,linka,journal,severe,'-',\n       '-', '-', '-',\n       '-', '-', '-',\n       '-', '-', '-',\n       '-', '-', '-',\n       '-',design,sample,'-']\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    df_table=df_table.sort_values(by=['Date'], ascending=False)\n    to_append = df_master.columns\n    df_length = len(df_table)\n    df_table.loc[df_length] = to_append\n    return df_table\n###################### MAIN PROGRAM ###########################\n\n\n### focus quesiton with single keyword\nkeywords = ['hypertension']\n#keywords = ['hypertension','diabetes','heart disease','gender','copd','smoking','age','stroke','cerbrovascular','cancer','kidney disease','drinking','tuberculosis','obesity']\n#'diabetes','heart disease','male gender','copd','smoking','age','stroke','cerbrovascular','cancer','kidney disease','drinking','tuberculosis','bmi'\n\nq=0\n\n# loop through the list of questions\nfor keyword in keywords:\n    # limit results to severe risk factors\n    search_words = 'risk factors sever comorbid'\n    \n    # get best sentences\n    df_table=process_question(df,search_words,keyword,df_master)\n    df_answers=df_table\n        \n    display(HTML('<h3>'+search_words+'</h3>'))\n    \n    df_table_show = df_table.append(df_master,sort=False)\n    df_table_show = df_table_show.drop_duplicates(subset='Title', keep=\"last\")\n    \n    df_table_display=HTML(df_table_show.to_html(escape=False,index=True))\n    display(df_table_display)\n    \n    df_table_show.to_csv('Hypertension.csv', index = False)\nprint ('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}