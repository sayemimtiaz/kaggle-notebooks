{"cells":[{"metadata":{"_uuid":"90d1583d56ca43d1b9c4448190c2c1a2f807df11"},"cell_type":"markdown","source":"## Movie Recommender System using Text Mining"},{"metadata":{"_uuid":"31fcb2bc425d4a407541ab434bd6d4f33fc5bd43"},"cell_type":"markdown","source":"In this modern era, all the companies have started using recommender systems more and more to enhance their customer experience. \nThere are two basic types of recommender systems algorithm namely Collabrative filtering based and content-based filtering (also known as pensonality-based approach). Both these approaches are used heavily. \nI will be discussing the former approach that is the content-based approach. Let me discuss few examples of this approach first-\n\n\n• Pandora uses the properties of songs and artist (around 400 and so variables) to create different types of stations for users like pop artist and pop songs are listed under the same station.\n\n\n• Content based systems are also used by e-commerce companies to suggest items to the user based on filters set by the user."},{"metadata":{"_uuid":"53e7af0b500dba18d1e241bdc0c76bdd6655ecc4"},"cell_type":"markdown","source":"Now, that we have some rough sketch on what a *content based system* does. Lets talk about the algorithm on a high level and understand how does this work. So basically, we have to create different profiles for the pool of items among which we need to make the recommendations and we create a profile based on user's choice or previous items they have rated highly. Once this is done, we compare these in n-dimensional space and compute the similarity between them and recommend those items to the user which are the most similar to the user's profile."},{"metadata":{"_uuid":"9aa54ccc4f86e796355584b4e1a6395d91e53bd8"},"cell_type":"markdown","source":"#### Motivation\nMotivation behind this movie recommender system is say a user searches for a movie that he likes then it is upto which movies should we recommend to him that he can look at after that movie. This is the similar approach that is used by big companies like Netflix, IMDB for recommending movies based on user choice. A similar approach is used by amazon to show suggested items in their catalog once a user clicks and opens a particular item. This helps companies to show users only those items that are relevant to them."},{"metadata":{"_uuid":"a820f78515095b02107ecd8b41875af68da6cc8f"},"cell_type":"markdown","source":"<img src='http://blog.allmyfaves.com/wp-content/uploads/2012/07/foundd-what-movie-should-I-watch-tonight.jpg'>"},{"metadata":{"_uuid":"9c9d4f481e94802919d4a7afb23a5fd39887840c"},"cell_type":"markdown","source":"#### Methodology\n\nSo my methodology for this tutorial is based on the same algorithm.\n1. I have used a movie dataset where user profile will be created by using user's selection.\n2. We compare these user's selection with all the movies in the dataset.\n3. We compare these profiles(vectors) using cosine similarities. (To make these profiles we will have to process our dataset into data that will be required for making vectors)\n\nTo create the user profiles and the item profiles I will be using keywords from the movie's overview (which are calculated using Tf-Idf) and similarly, among the tags for each I have calculated the top tags that represent that movie. I will also be using features like movie ratings, actors, director etc to create profiles."},{"metadata":{"_uuid":"593f0989907413b84abd05c481b831d224a5f747"},"cell_type":"markdown","source":"###### Let's get started"},{"metadata":{"_uuid":"6b1016c785bcafc15ac853f4938fa89507619adf"},"cell_type":"markdown","source":"First step is importing required libraries\n1. We have imported pandas for basic dataframe manipulations\n2. We have imported numpy for array manipulation\n3. We have imported matplotlib for initial visualization\n4. We have imported csv to import csv files "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"359430c99adb6adbf542afcc60ce3f48bf1154be"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#Library used for initial visualization\nimport matplotlib.pyplot as plt \n#Library used for initial visualization\nimport csv \nimport ast\nimport re\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fb3877554f17fb0a3adb1668cabce9feb21f75a"},"cell_type":"markdown","source":"We will now load and merge the two datasets that we have one dataset is credits dataset that contains information like cast, crew information. Second dataset is movies dataset which contains information genres, language, overview, vote_average."},{"metadata":{"trusted":true,"_uuid":"07588bbc1eef72ae279320770275f755e3397476","collapsed":true},"cell_type":"code","source":"credits_csv=pd.read_csv('../input/tmdb_5000_credits.csv')#change file path\nmovies_csv=pd.read_csv('../input/tmdb_5000_movies.csv')#change file path\ndata=pd.merge(credits_csv, movies_csv, left_on='movie_id', right_on='id')\n#To drop columns that have been repeated\ndata=data.drop(['id','title_x'],axis=1) \n#Renaming particular columns\ndata.rename(columns={'title_y':'title'}, inplace=True) \n#Descriptive Statistics on the numberical data\ndata.describe() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8e0c9651f740418f79de8ce07d29b434ea3102"},"cell_type":"markdown","source":"Checking outliers in numberical variables"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c4bc1e40092b56d947cedecc7322c2503ecaccb"},"cell_type":"code","source":"data.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d99de307d49cc7ac3b01904924db523f6c43e50"},"cell_type":"markdown","source":"Checking for null values"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"bb77def2898fce788a7349c2017bf66054be8851"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"804309cc2f6886be059545977cc868a9059c0c58"},"cell_type":"markdown","source":"We check the datatype for each of the column in the dataset."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1f3c169e9beebed8d7c4b9eb96d7182690161b2d"},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"064444f36e34998bfdb5edcc8f4335787d0100a3"},"cell_type":"markdown","source":"We have to process the types for columns that contain JSON type data(lists, dictionaries) because while reading the files they are just loaded as strings (represented as object types). We have to convert them to their appropriate types."},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"46a1c335279c1e7580d06f8035e15ec4e70cd72a"},"cell_type":"code","source":"#This cell should be run only once because the values once converted are no longer string type \ndata[\"genres\"]=data[\"genres\"].apply(ast.literal_eval)\ndata[\"spoken_languages\"]=data[\"spoken_languages\"].apply(ast.literal_eval)\ndata[\"cast\"]=data[\"cast\"].apply(ast.literal_eval)\ndata[\"crew\"]=data[\"crew\"].apply(ast.literal_eval)\ndata[\"keywords\"]=data[\"keywords\"].apply(ast.literal_eval)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ad847d82ec8e6ec076d0548def9e2b8385fef7b"},"cell_type":"markdown","source":"Following libraries are required for text manipulation and text mining of the overview and keywords in the movie.\nStemmer is used to extract only the stem of a word."},{"metadata":{"_uuid":"feea99be0b2c35fb25368906a7037e2143fd134c"},"cell_type":"markdown","source":"After looking at the whole dataset I have decided to use only the following columns:-\n\n1. Overview (Description of the movie can suggest the type of movie and give us the keywords in a movie for example say we have Superman, the movie is based on a comic so most likely comic will be a keyword in the overview)\n\n2. Genres (Genre is very indicative of the taste of the user and thus must be used)\n\n3. Keywords (These are keywords pre existing in the data that can be said to be tags representing a movie)\n\n4. Actors (A person watches a movie if he likes a particular actor and that is why we need to include main actors but not all actors as supporing actors are not much of interest)\n\n5. Director (People often go for watching a movie based on the director as a director who has a good reputation in the industry is more sort after)\n\n6. Language (Language which the movie is indicate of a person's native language)\n\n7. voting_average (The score of the movie and how much it has been rated overall)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"759776b98e8987a114bd00ff8b7bb219cc2ef10f"},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd6f80c9c40103005c32f7e54af445a602061c54"},"cell_type":"markdown","source":"This following functions takes in a document and calculates the term-frequencies and returns the dictionary with frequencies for each word in the document.\n\nThough the TF formula used here is different from what we studied in the class which was just calculating the total count of each word in each document.\n\nHere rather we have documents(overviews) which are very different in lengths so TF alone will not be indicative enough. Thus we normalize them by calculating (1+log(word_count)) and then normalizing these across all the documents by dividing this by vector length.\n\nReference:- \nhttps://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/\n\nThe formula used here is :- (1+np.log10(word_count))/vector_length)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5af1a8aef29bfedb8697e596261dc1e0c2bebc85"},"cell_type":"code","source":"from collections import Counter\ndef tf(overview):\n    vector_length=0\n    overview_words=re.sub(\"[^\\w'-]\",\" \", str(overview).lower()).split()\n    stemmed_words=list()\n    for word in overview_words:\n        stemmed_words.append(ps.stem(word))\n    overview_words=Counter(stemmed_words) \n    \n    words_dicts=dict()\n    for word,count in overview_words.items():\n        vector_length+=((1+np.log10(count))**(2))\n    vector_length=vector_length**(0.5)\n    for word,count in overview_words.items():\n        words_dicts.update({ps.stem(word):((1+np.log10(count))/vector_length)})\n    return words_dicts\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c5349e6f68dbd6152aa738510a31b758b781ae3"},"cell_type":"markdown","source":"The following function is used to calculate idf from a set of documents that is passed using a list and we calculate idf for each term.\n\nThe motivation behind idf(inverse document frequency) is that all the words in a document are not necessarily useful in modelling the topics for that document.\n\nIn the function we use a loop to iteratre over each document and split it into words and stem the words that is extracting only the root of that word because words can be in different forms. Then we use idf formula to calculate idf for each unique word in the set of documents.\n\nFormula used for calculating the IDF is :- np.log10(number of documents/number of documents containg that word)\n\n"},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"b612106c4ec2b72e9eb4b6d23bee47d310791791"},"cell_type":"code","source":"#pass a list of documents\ndef idf(idf_data): \n    idf_dict=dict()\n    for docs in idf_data:\n        doc_words=re.sub(\"[^\\w'-]\",\" \", str(docs).lower()).split()\n        stemmed_words=list()\n        for word in doc_words:\n            stemmed_words.append(ps.stem(word))\n        \n        doc_words=list(set(stemmed_words))\n        for word in doc_words:\n            if ps.stem(word.lower()) not in idf_dict.keys():\n                idf_dict.setdefault(ps.stem(word.lower()), 1)\n            else:\n                idf_dict[ps.stem(word.lower())]+=1\n    for key,value in idf_dict.items():\n        idf_dict[key]=np.log10(len(idf_data)/value)\n    return idf_dict\n       \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff31de0af72b5b292310f4cea2c25a98e74f7ffe"},"cell_type":"markdown","source":"#### Now we start processing our data based on our requirements\n"},{"metadata":{"_uuid":"43474bffb8e5269020b513f4ab6185515cec4478"},"cell_type":"markdown","source":"We are processing following columns that we discussed to be using initially."},{"metadata":{"_uuid":"119de4c16686fe74aa399f10315b7dc4920d1a8e"},"cell_type":"markdown","source":"Now we have to process our data to extract actors from the dataset we will be using the main actors in a movie to do so we will only be using the main three actors of the movie which are represented in the cast of the movie as actor with order 0,1 and 2.\nWe create a dataframe that has the top three actors in each movie."},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"7712cc68147014ec4e86d5e721e775100a682d4f"},"cell_type":"code","source":"actors=list()\n\nfor i in range(0,len(data.index)):\n    actors.append([d['name'].strip() for d in data['cast'][i] if d['order'] == 0 or d['order'] == 1 or d['order'] == 2])\n    \nlabels=['actor1','actor2','actor3','actor4','actor5']\nactors_df=pd.DataFrame.from_records(actors,columns=labels,exclude=['actor4','actor5'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"73a30d17e87ef16091ba0ce08188a459a9241003"},"cell_type":"markdown","source":"We apply TF-IDF on overview and make a new dataframe with only top 5 important words in each document."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"aa459c77f4573a7cd88835851e6a739529672597"},"cell_type":"code","source":"tfidf_column=[]\nidf_dict=idf(data[\"overview\"])\nfor overview in data[\"overview\"]:        \n    tfidf_dict=tf(overview)\n    for key,value in tfidf_dict.items():\n        tfidf_dict[key]=value*idf_dict[key]\n    tfidf_dict=sorted(tfidf_dict, key=tfidf_dict.get, reverse=True)[:5]\n    tfidf_column.append(tfidf_dict)\nimportantwords_df=pd.DataFrame(tfidf_column,columns=[\"Key1\",\"Key2\",\"Key3\",\"Key4\",\"Key5\"])\nimportantwords_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf61670b6ecc446f303b746eacadeb386f85efb"},"cell_type":"markdown","source":"Now, for looking at the genres of the movie we are going to make a dataframe that gets all the genres from the movies and creates a dummy variable that gives 0 or 1 respective to if that movie corresponds to that genre or not."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"15c0de0298a2c8ed7efebf0789c8b6252e67ada1"},"cell_type":"code","source":"genre_list=[]\nfor genre in data[\"genres\"]:\n    for d in genre:\n        if d['name'] not in genre_list:\n            genre_list.append(d['name'])\n\nall_movies=[]\nfor genre in data[\"genres\"]:\n    movie_genres=dict()\n    for gen in genre_list:\n        movie_genres.setdefault(gen, 0)\n    for d in genre:\n        movie_genres[d['name']]=1\n    all_movies.append(movie_genres)\ngenres_df=pd.DataFrame(all_movies)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c9c44660c3e557f5b4740c3701ba56ed476fa4"},"cell_type":"markdown","source":"We extract the Director from the list of crew members and create a new column that has the director of each movie."},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"88903826640cca7446a8f29b1578f760365cae59"},"cell_type":"code","source":"directors_list=list()\nfor crew in data[\"crew\"]:\n    director_flag=0\n    for d in crew:\n        if d['job']==\"Director\":\n            directors_list.append({\"Director\":d['name']})\n            director_flag=1\n            break\n    if director_flag==0:\n        directors_list.append({\"Director\":''})\n\ndirectors_df=pd.DataFrame(directors_list)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"899591a5ba64ad29167f5465c1dc3f87727fcfa1"},"cell_type":"markdown","source":"Now for the list of keywords that exists in our movie dataset already are basically the search tags that represent the movie and the we extract the keywords that have the highest tf-idf among the list of words as those keywords are the most unique tags for each movie."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"68690189e4bc8f8457168b86ca08d9dc224bbba3"},"cell_type":"code","source":"keywords_column=list()\nfor movie_keywords in data[\"keywords\"]:\n    keywords=''\n    for d in movie_keywords:\n        keywords=keywords+ps.stem(d['name'])+' '\n    keywords_column.append(keywords.strip())\n    \nkeywords_df=pd.DataFrame(keywords_column)\n\ntfidf_column2=[]\nidf_dict2=idf(keywords_df[0])\nfor docs in keywords_df[0]:        \n    tfidf_dict=tf(docs)\n    for key,value in tfidf_dict.items():\n        tfidf_dict[key]=value*idf_dict2[key]\n    tfidf_dict=sorted(tfidf_dict, key=tfidf_dict.get, reverse=True)[:5]\n    tfidf_column2.append(tfidf_dict)\nkeywords_5df=pd.DataFrame(tfidf_column2,columns=[\"Keyword1\",\"Keyword2\",\"Keyword3\",\"Keyword4\",\"Keyword5\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acabc37fa17f53ec3515459bb2642d7fdf67bfd7"},"cell_type":"markdown","source":"We concatenate all the columns that we have processed and create a new result dataset that we will use to make the user and item profiles."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"98c234a125d45c5f712c8af9afa86d29027f574f"},"cell_type":"code","source":"#Concatenating\nresult=pd.concat([data,actors_df,directors_df,genres_df,importantwords_df,keywords_5df],axis=1)\n#Normalizing vote_average to change the range to 0 to 1\nresult[\"vote_average\"]=result[\"vote_average\"]*0.1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05579bf3a07b9ae41e02017cbc82750cff5a3628"},"cell_type":"markdown","source":"### User selection"},{"metadata":{"_uuid":"a6c48778c2a507f5f6d57d909017ed54f847867d"},"cell_type":"markdown","source":"This is the user selection that we need use to create the user profile."},{"metadata":{"_uuid":"71c843cebc00fb949f2c0a8e8aa89987829ea3de"},"cell_type":"markdown","source":"### You can make the user selection multiple times but you need to run the next cell too with it."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e36e221d517614c53a80712803d84d9d83cbcc46"},"cell_type":"code","source":"import ipywidgets as widgets\n\nuser_movie=widgets.Dropdown(\n    options=sorted(list(result[\"title\"])),\n    description='Please choose a movie:',\n    disabled=False,\n    value='2 Fast 2 Furious'\n)\nuser_movie","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6f68d37b194423fefec4772446a7fa457b03780"},"cell_type":"markdown","source":"Above is a dropdown when you run the notebook you can choose the movie using this drop down. \n\nI have run the notebook and selecting the movie as **\"2 Fast 2 Furious\"**"},{"metadata":{"_uuid":"b62083a5867ecc13821eec531e98e66ba667cbce"},"cell_type":"markdown","source":"### Steps Followed:-\n\n1. We create a user profile and first the row from the dataset that corresponds to users selected movie.\n2. We use original_language, actors, director, keys(from overview) and keywords and code 1 or 0 for each movie.\n3. For the user profile each of the above is 1.\n4. Then we drop irrelevant columns.\n5. We subset the data to remove the user's selection from the original dataset.\n6. We compute the cosine similarities between to dataset(one dataset represents user profile and other represents item profiles)\n7. We sort the cosine similarities by lowest to highest.\n8. We pick the last 5.\n9. Recommend the 5 movies we picked"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"65b935d8495e7691213c650cf7febc821256a495"},"cell_type":"code","source":"import sklearn.metrics.pairwise\nprint(\"Movie selected is \"+user_movie.value)\nuser_profile=result.loc[result['title']==str(user_movie.value)]\n\nuser_profile=user_profile.drop(['cast','crew','popularity', 'budget', 'genres', 'homepage', 'keywords','original_title','overview', 'production_companies','production_countries','release_date', 'revenue', 'runtime', 'spoken_languages', 'status','tagline','vote_count'],axis=1)\nactor1=user_profile['actor1'].tolist()\nactor2=user_profile['actor2'].tolist()\nactor3=user_profile['actor3'].tolist()\noriginal_language=user_profile['original_language'].tolist()\nDirector=user_profile['Director'].tolist()\nKey1=user_profile['Key1'].tolist()\nKey2=user_profile['Key2'].tolist()\nKey3=user_profile['Key3'].tolist()\nKey4=user_profile['Key4'].tolist()\nKey5=user_profile['Key5'].tolist()\nKeyword1=user_profile['Keyword1'].tolist()\nKeyword2=user_profile['Keyword2'].tolist()\nKeyword3=user_profile['Keyword3'].tolist()\nKeyword4=user_profile['Keyword4'].tolist()\nKeyword5=user_profile['Keyword5'].tolist()\n\n\nactor_list=[actor1[0],actor2[0],actor3[0]]\nkey_list=[Key1[0],Key2[0],Key3[0],Key4[0],Key5[0]]\nkeyword_list=[Keyword1[0],Keyword2[0],Keyword3[0],Keyword4[0],Keyword5[0]]\n\n\nuser_profile['actor1'] = np.where(user_profile.actor1.isin(actor1),1,0)\nuser_profile['actor2'] = np.where(user_profile.actor2.isin(actor2),1,0)\nuser_profile['actor3'] = np.where(user_profile.actor3.isin(actor3),1,0)\nuser_profile['original_language'] = np.where(user_profile.original_language.isin(original_language),1,0)\nuser_profile['Director'] = np.where(user_profile.Director.isin(Director),1,0)\nuser_profile['Key1'] = np.where(user_profile.Key1.isin(Key1),1,0)\nuser_profile['Key2'] = np.where(user_profile.Key2.isin(Key2),1,0)\nuser_profile['Key3'] = np.where(user_profile.Key3.isin(Key3),1,0)\nuser_profile['Key4'] = np.where(user_profile.Key4.isin(Key4),1,0)\nuser_profile['Key5'] = np.where(user_profile.Key5.isin(Key5),1,0)\nuser_profile['Keyword1'] = np.where(user_profile.Keyword1.isin(Keyword1),1,0)\nuser_profile['Keyword2'] = np.where(user_profile.Keyword2.isin(Keyword2),1,0)\nuser_profile['Keyword3'] = np.where(user_profile.Keyword3.isin(Keyword3),1,0)\nuser_profile['Keyword4'] = np.where(user_profile.Keyword4.isin(Keyword4),1,0)\nuser_profile['Keyword5'] = np.where(user_profile.Keyword5.isin(Keyword5),1,0)\n\nitem_profiles=result.drop(['cast','crew','popularity', 'budget', 'genres', 'homepage', 'keywords','original_title','overview', 'production_companies','production_countries','release_date', 'revenue', 'runtime', 'spoken_languages', 'status','tagline','vote_count'],axis=1)\nitem_profiles=item_profiles.loc[~(data['original_title']==str(user_movie.value))]\nitem_profiles['actor1'] = np.where(item_profiles.actor1.isin(actor_list),1,0)\nitem_profiles['actor2'] = np.where(item_profiles.actor2.isin(actor_list),1,0)\nitem_profiles['actor3'] = np.where(item_profiles.actor3.isin(actor_list),1,0)\nitem_profiles['original_language'] = np.where(item_profiles.original_language.isin(original_language),1,0)\nitem_profiles['Director'] = np.where(item_profiles.Director.isin(Director),1,0)\nitem_profiles['Key1'] = np.where(item_profiles.Key1.isin(key_list),1,0)\nitem_profiles['Key2'] = np.where(item_profiles.Key2.isin(key_list),1,0)\nitem_profiles['Key3'] = np.where(item_profiles.Key3.isin(key_list),1,0)\nitem_profiles['Key4'] = np.where(item_profiles.Key4.isin(key_list),1,0)\nitem_profiles['Key5'] = np.where(item_profiles.Key5.isin(key_list),1,0)\n\nitem_profiles['Keyword1'] = np.where(item_profiles.Keyword1.isin(keyword_list),1,0)\nitem_profiles['Keyword2'] = np.where(item_profiles.Keyword2.isin(keyword_list),1,0)\nitem_profiles['Keyword3'] = np.where(item_profiles.Keyword3.isin(keyword_list),1,0)\nitem_profiles['Keyword4'] = np.where(item_profiles.Keyword4.isin(keyword_list),1,0)\nitem_profiles['Keyword5'] = np.where(item_profiles.Keyword5.isin(keyword_list),1,0)\n\n\nitem_profiles\nx=user_profile.drop(['title','movie_id'],axis=1)\ny=item_profiles.drop(['title','movie_id'],axis=1)\n\n\narr = sklearn.metrics.pairwise.cosine_similarity(x,y, dense_output=True)\narr_index=arr.argsort()\narr_index=arr_index.ravel()\nfinal_df=pd.DataFrame(arr_index).tail(5)\nfinal_df\nitem_profiles=item_profiles.iloc[final_df[0]]\nitem_profiles=item_profiles.iloc[::-1]\nitem_profiles","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0549eedc4a302e37dfe6a4221ab44562a7698e77"},"cell_type":"code","source":"print(\"Recommended Movies for \"+ user_movie.value + \": \\n\\n\"+'\\n'.join(list(item_profiles[\"title\"])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07791c8ee088d42f2227bb5dddf291cb1fada93f"},"cell_type":"markdown","source":"Play Around and try other movies choose different movies from the dropdown."},{"metadata":{"collapsed":true,"_uuid":"e582876069a7ae36ef3e925fe326c64657fd8f03"},"cell_type":"markdown","source":"## References\n\n1. Content Based Recommendations | Stanford University. (2018). YouTube. Retrieved 31 March 2018, from https://www.youtube.com/watch?v=2uxXPzm-7FY\n\n2. Engines, B., Engines, B., & Das, S. (2018). Beginners Guide to learn about Content Based Recommender Engine. Analytics Vidhya. Retrieved 31 March 2018, from https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/\n\n3. Lecture 16.2 — Recommender Systems | Content Based Recommendations — [ Andrew Ng ]. (2018). YouTube. Retrieved 31 March 2018, from https://www.youtube.com/watch?v=9siFuMMHNIA\n\n4. Overview of Recommender Systems | Stanford University. (2018). YouTube. Retrieved 31 March 2018, from https://www.youtube.com/watch?v=1JRrCEgiyHM&t=627s\n\n5. Recommender system. (2018). En.wikipedia.org. Retrieved 31 March 2018, from https://en.wikipedia.org/wiki/Recommender_system"},{"metadata":{"_uuid":"ca5e7dc612a1d9383e9e7d9095d66e45b6fd5443"},"cell_type":"markdown","source":"## Further reads\n\nFurther resources for better understanding of recommender systems:-\n\nA good read to know more about the other most widely used recommendation algorithm Collaborative Filtering http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf .This paper talks about amazon's use of item-to-item collaborative filtering.\n\nIf you want to deep dive into the topic and understand different algorithms more thoroughly. I recommend this course https://www.coursera.org/specializations/recommender-systems.\n\nAnother great tutorial if you want to look at another example of content based recommender systems.\nhttps://www.themarketingtechnologist.co/a-recommendation-system-for-blogs-content-based-similarity-part-2/\n\nA good read on hybrid recommender systems\nhttps://www.hindawi.com/journals/mpe/2015/145636/\n"},{"metadata":{"collapsed":true,"_uuid":"28284da76c9075ebdf7a005a11e7e9844927fafe"},"cell_type":"markdown","source":"## Conclusion\n\nRecommender systems are very widely used in the industry right now and is one of the must have skills on your resume. For the other most widely used recommender systems and you can go through the some of the readings I suggested above and look at the problems related to recommender systems like cold start problem(The algorithm we have discussed in this tutorial can address this problem). And read how both these algorithms are used in a hybrid algorithm to solve some of these problems.\nApart from this content based recommender system does not have any particular fixed metric for evaluation, but in the case of Collaborative filtering we can evaluate our model using users previous ratings and dividing our data into training and test sets. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1}