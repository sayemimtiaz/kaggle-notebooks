{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# This is for storing data file path\nimportdata = '' # '' meaning for define data type(String) \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        importdata = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T04:41:46.602286Z","iopub.execute_input":"2021-06-23T04:41:46.602648Z","iopub.status.idle":"2021-06-23T04:41:46.616119Z","shell.execute_reply.started":"2021-06-23T04:41:46.602617Z","shell.execute_reply":"2021-06-23T04:41:46.615099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import Counter, deque\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:41:46.617579Z","iopub.execute_input":"2021-06-23T04:41:46.618036Z","iopub.status.idle":"2021-06-23T04:42:26.68199Z","shell.execute_reply.started":"2021-06-23T04:41:46.617989Z","shell.execute_reply":"2021-06-23T04:42:26.68096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(importdata)\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:26.683515Z","iopub.execute_input":"2021-06-23T04:42:26.683749Z","iopub.status.idle":"2021-06-23T04:42:26.710699Z","shell.execute_reply.started":"2021-06-23T04:42:26.683726Z","shell.execute_reply":"2021-06-23T04:42:26.710041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"print(data.info())  # find Nan, number of columns object(5)\nprint(data.describe())  # check the values\npd.set_option('display.max_columns', 6)  # display all columns (object +1 = 6)\nprint(data.head())\nBottle = data.groupby(['Bottle_name']).size()\nprint(Bottle)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:26.71179Z","iopub.execute_input":"2021-06-23T04:42:26.712121Z","iopub.status.idle":"2021-06-23T04:42:26.735746Z","shell.execute_reply.started":"2021-06-23T04:42:26.712096Z","shell.execute_reply":"2021-06-23T04:42:26.735113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"data['Brand'].value_counts().plot(kind='barh')\nplt.show()\n\nplt.subplot(1, 2, 1)\n# opt = [\"Yamazaki\"] # this is for isin(), if I need to specify more, I cam write opt = ['Yamazaki', 'banrd name']\nx = data.loc[data['Brand'].isin([\"Yamazaki\"])] # extract all values (rows) which specified by using isin() function\nx['Bottle_name'].value_counts().plot(kind='bar')\nplt.title(\"Yamazaki and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\ny = data.loc[data['Brand'].isin([\"Hibiki\"])]\ny['Bottle_name'].value_counts().plot(kind='bar', color='green')\nplt.title(\"Hibiki and Bottle_Name\")\nplt.show()\n\nplt.subplot(1, 2, 1)\nz = data.loc[data['Brand'].isin([\"Nikka\"])]\nz['Bottle_name'].value_counts().plot(kind='bar', color='orange')\nplt.title(\"Kikka and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\nw = data.loc[data['Brand'].isin([\"Hakushu\"])]\nw['Bottle_name'].value_counts().plot(kind='bar', color='red')\nplt.title(\"Hakushu and Bottle_Name\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:26.736669Z","iopub.execute_input":"2021-06-23T04:42:26.737065Z","iopub.status.idle":"2021-06-23T04:42:27.599718Z","shell.execute_reply.started":"2021-06-23T04:42:26.73703Z","shell.execute_reply":"2021-06-23T04:42:27.598889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the Texts","metadata":{}},{"cell_type":"code","source":"corpus = [] # corpus will only get in the end all the clean reviews\nfor i in range(len(data)):\n    review = re.sub('[^a-zA-Z0-9_]', ' ', data['Review_Content'][i]) # for  remove all punctuation\n    review = review.lower() # all the capital letters were turned into lowercase\n    # review = review.split() # split different words\n    words = word_tokenize(review) # split words\n    ps = PorterStemmer() # loved -> love remove ed\n    stop = set(stopwords.words('english'))\n    stop.update(('Yamazaki', 'hibiki', 'it', 'whisky', 'whiskey', 'bottle')) # update stop words\n    review = [ps.stem(word) for word in words if not word in stop]\n    review = ' '.join(review) # adding space between each word of review\n    corpus.append(review)\n\ncount = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\nprint(array)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:27.60073Z","iopub.execute_input":"2021-06-23T04:42:27.600983Z","iopub.status.idle":"2021-06-23T04:42:29.083565Z","shell.execute_reply.started":"2021-06-23T04:42:27.600959Z","shell.execute_reply":"2021-06-23T04:42:29.082655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Positive, Negative, Neutral word list","metadata":{}},{"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\npos_word_list = []\nneu_word_list = []\nneg_word_list = []\n\nfor word in count:\n    if (sid.polarity_scores(word)['compound']) >= 0.5:\n        pos_word_list.append(word)\n    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n        neg_word_list.append(word)\n    else:\n        neu_word_list.append(word)\n\nprint('Positive :', pos_word_list)\nprint('Neutral :', neu_word_list)\nprint('Negative :', neg_word_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:29.084904Z","iopub.execute_input":"2021-06-23T04:42:29.085208Z","iopub.status.idle":"2021-06-23T04:42:29.314501Z","shell.execute_reply.started":"2021-06-23T04:42:29.085169Z","shell.execute_reply":"2021-06-23T04:42:29.31356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP","metadata":{}},{"cell_type":"code","source":"def prev_and_next(input_list):\n    CURRENT = input_list\n    PREV = deque(input_list)\n    PREV.rotate(-1)\n    PREV = list(PREV)\n    NEXT = deque(input_list)\n    NEXT.rotate(1)\n    NEXT = list(NEXT)\n    return zip(PREV, CURRENT, NEXT)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:29.316406Z","iopub.execute_input":"2021-06-23T04:42:29.316653Z","iopub.status.idle":"2021-06-23T04:42:29.321079Z","shell.execute_reply.started":"2021-06-23T04:42:29.316629Z","shell.execute_reply":"2021-06-23T04:42:29.320155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding a column ('Sentiment') for NLP","metadata":{}},{"cell_type":"code","source":"data['Sentiment'] = 0\n\npositiveList = pd.read_csv('/kaggle/input/positive-words/positive.txt') # import all positive words from online\nnegativelist = pd.read_csv('/kaggle/input/negative-words/nega.txt')\n\npos = positiveList.iloc[:, 0].values # for lemmatizer use: converting all words to the single distinct list 'awesome','good'\nneg = negativelist.iloc[:, 0].values # converting all words to the single distinct list 'bad','hate'\n\nindex = 0 # accessing all rows from 1\n\nlemmatizer = WordNetLemmatizer() # modeling and lemmatization that convert plural to singular ex) Feet -> Foot\n\nps = PorterStemmer()  # loved -> love remove ed\nstop = set(stopwords.words('english'))\nstop.remove('not') # for checking Not + any positive words: Not Like\nstempos = [ps.stem(word) for word in pos if not word in stop] # for positiveList Ps use: this is for the review which will applied stemming in the loop\nstemnega = [ps.stem(word) for word in neg if not word in stop] # this is for the review which will applied stemming in the loop\n\ncorpus = [] # collecting all stemming and lemmatazed words\nfor i in data['Review_Content']:\n    review = re.sub('[^a-zA-Z]', ' ', i)  # for  remove all punctuation\n    review = review.lower()  # all the capital letters were turned into lowercase\n    words = word_tokenize(review) # split words\n\n    lemWords = [lemmatizer.lemmatize(w) for w in words] # for words lemmatizer\n\n    # This loop will stop when itr finds one of the positive or negative words (lemmatization) in a sentence.\n    for itr in lemWords: # check a word all in lemWords(h)\n        if itr in pos:   # check h in pos\n            data.at[index, 'Sentiment'] = 1 # found positive word first\n            break # finish loop\n        elif itr in neg: # check h in neg\n            data.at[index, 'Sentiment'] = 0 # found negative word first\n            break\n\n    review = [ps.stem(word) for word in lemWords if not word in stop] # for NLP by lemWords PorterStemmer\n\n    # I assumed that sentences that have any negative word first are already negative.\n    # This loop point is for finding 'Not + any positive words' such as \"NOT LIKE\"\n    if (data.at[index, 'Sentiment'] == 1): # for find next has negative word    'It's okay but I do not like it'\n        for previous, current, next in prev_and_next(review): # assume the next words are any positive words\n            if current == 'not':\n                if next in stempos: # stempos is stemmed as review needs to be stemmed for NLP\n                    data.at[index, 'Sentiment'] = 0\n                    break # no longer this loop needs to search\n\n    index += 1 # index is increasing\n    review = ' '.join(review)\n    corpus.append(review) # stores stemmed and lemmatized words here\n\nsentiment = data['Sentiment'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\nprint(sentiment)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:29.32271Z","iopub.execute_input":"2021-06-23T04:42:29.323055Z","iopub.status.idle":"2021-06-23T04:42:33.375805Z","shell.execute_reply.started":"2021-06-23T04:42:29.323018Z","shell.execute_reply":"2021-06-23T04:42:33.37498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis for NLP","metadata":{}},{"cell_type":"code","source":"count = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\narray1d = np.array(array[:, -1], dtype='i') # i = int\n\n# describe outlier\nprint(np.median(array1d))\nprint(np.mean(array1d))\nprint(np.std(array1d))  # for check the array type\nprint(array1d[(array1d>np.quantile(array1d, 0.1)) & (array1d<np.quantile(array1d, 0.9))].tolist())\n\n# visualizing outlier\nplt.boxplot(array1d)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:33.376999Z","iopub.execute_input":"2021-06-23T04:42:33.377284Z","iopub.status.idle":"2021-06-23T04:42:33.492444Z","shell.execute_reply.started":"2021-06-23T04:42:33.377253Z","shell.execute_reply":"2021-06-23T04:42:33.491599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Removing Words from the Above Steps","metadata":{}},{"cell_type":"code","source":"removewords = [word for word in count if (count[word] > 50) & (not word in stempos and not word in stemnega)]\n\nremoveIndex = []\nfor word in removewords:\n    if word in corpus:\n        removeIndex.append(corpus.index(word))\n        corpus.remove(word)\ndata = data.drop(removeIndex)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:33.495372Z","iopub.execute_input":"2021-06-23T04:42:33.495617Z","iopub.status.idle":"2021-06-23T04:42:33.770692Z","shell.execute_reply.started":"2021-06-23T04:42:33.495592Z","shell.execute_reply":"2021-06-23T04:42:33.769807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Bag of Words model","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer()\nx = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values\n\nmax_words = int(len(x[0] * 0.9))\ncv = CountVectorizer(max_features=max_words)\nx = cv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:33.771804Z","iopub.execute_input":"2021-06-23T04:42:33.772079Z","iopub.status.idle":"2021-06-23T04:42:33.893603Z","shell.execute_reply.started":"2021-06-23T04:42:33.772054Z","shell.execute_reply":"2021-06-23T04:42:33.892794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying NLP","metadata":{}},{"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n\n# Training the Naive Bayes model on the Training set\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T04:42:33.894671Z","iopub.execute_input":"2021-06-23T04:42:33.89492Z","iopub.status.idle":"2021-06-23T04:42:33.992733Z","shell.execute_reply.started":"2021-06-23T04:42:33.894897Z","shell.execute_reply":"2021-06-23T04:42:33.991827Z"},"trusted":true},"execution_count":null,"outputs":[]}]}