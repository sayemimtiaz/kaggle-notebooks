{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as pi\nimport seaborn as sn\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nfrom scipy.stats import randint as sp_randint\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans \n\n# Preprocessing libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\n\n# Models\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Description and the probelm space\n\n- The dataset is for the compresive strength of the cement with below parameters\n\n    - Cement\n    - Blast\n    - Fly Ash\n    - Water\n    - Super Plasticizer\n    - Coarse Aggregate\n    - Fine Aggregate\n    - Age\n    - Concrete Compressive Strength"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the display layout for the coding environment\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }</style>\"))\n\nsn.set(font_scale=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset analysis for Cemenet Concrete\n\n- Loading the dataset and checking out the dimensions, shape and the type of the parameters\n- Analysing the each attributes for the anomilies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nprint('Listing files in the folder')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_data = pd.read_csv(\"/kaggle/input/regression-with-neural-networking/concrete_data.csv\")\n\nconcrete_data.columns=['cement','slag','ash','water','superplastic','coarseagg','fineagg','age','strength']\n\nprint(\"\\nDimensions of the data\")\n\nprint(\"Shape of the data :{0}\".format(concrete_data.shape))\nprint(\"Size of the data :{0}\".format(concrete_data.size))\nprint(\"nDim of the data :{0}\".format(concrete_data.ndim))\nprint(\"Shape x * y :{0}\".format(concrete_data.shape[0]*concrete_data.shape[1]))\n\nprint(\"\\nData types of the Data\")\nprint(concrete_data.info())\n\nprint(\"\\nData elements from file\")\nconcrete_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for the missing or the null values\n\n- The numeric columns does not have invalid characters only the NaN \n- Checking the number of 0's in the data that could also be a null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the types and the missing values of the data\n\nprint(\"\\nList of Null values in each column\\n\")\nprint(concrete_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset does not have any null values, but need to verify if 0.0 is acutally replaced null value\n\nconcrete_columns =  concrete_data.columns\n\nprint('\\nZeros in each of the attribute\\n')\nfor col in concrete_columns:\n    print(f\"Total 0's in Column[{col}] : {(concrete_data[col]==0).sum()}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Dataset Analysis\n\n- total 9 attributes, the data types are already numeric. Hence no conversion of datatypes was required\n- All the attributes are linear in nature, there are good number of data values for each attributes\n- There are no null values in any of the attributes, hence null value treatment not required\n- All the attribute are relevant, no need to drop any of the attributes\n- **'strength'** is the Target variable, will do detailed analysis on that attribute later\n\n\n#### Complexties in the data set\n\n- Values of the attributes are in different units, hence needs to be converted to the common type by preprocessors\n- Some of the attributes have zero values, it might be a default value hence leaving it un touched for now\n- There could be some outliers in the data, have to do some outlier processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define functions to identify the number of outliers\n\ndef Identify_Outliers(data_column):\n    \n    dataFrame = pd.DataFrame(data_column)\n    \n    Quar1 = dataFrame.quantile(0.25)  \n    Quar3 = dataFrame.quantile(0.75)  \n    \n    IQR = Quar3 - Quar1\n    \n    return ((dataFrame < (Quar1-1.5*IQR)) | (dataFrame> (Quar3+1.5*IQR))).sum()\n    \n\nprint(\"\\n Number of outliers in each attribute \\n\")\nfor (columnName, columnData) in concrete_data.iteritems(): \n    print(Identify_Outliers(concrete_data[columnName]))\n\ndata_columns = concrete_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the outliers\n\nfig, axis = plt.subplots(2, 3, figsize=(25, 12), sharex=False)\n\naxis[0,0].set_title(\"Slag\")\nsn.boxplot(concrete_data[\"slag\"],color='green',orient='h',ax=axis[0,0]);\n\naxis[0,1].set_title(\"water\")\nsn.boxplot(concrete_data[\"water\"],color='green',orient='h',ax=axis[0,1])\n\naxis[1,0].set_title(\"superplastic\")\nsn.boxplot(concrete_data[\"superplastic\"],color='green',orient='h',ax=axis[0,2])\n\naxis[1,1].set_title(\"fineagg\")\nsn.boxplot(concrete_data[\"fineagg\"],color='orange',orient='h',ax=axis[1,0])\n\naxis[1,1].set_title(\"age\")\nsn.boxplot(concrete_data[\"age\"],color='orange',orient='h',ax=axis[1,1])\n\naxis[1,1].set_title(\"ash\")\nsn.boxplot(concrete_data[\"ash\"],color='orange',orient='h',ax=axis[1,2])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Outliers Analysis\n\n- Included the 'ash' attribute for the comparison of non-outliers\n- The oultiers on these datasets are nominal as compared to the number of data\n- The 'age' column has the higher number of outlier, more detailed analysis required before correcting the outliers\n- Modifiying the outliers will impact the data quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do a scatter plot to understand the outliers in detail with the target variable\n\nfig, axis = plt.subplots(1, 4, figsize=(25, 8), sharex=False)\n\naxis[0].set_title(\"Super Plastic\")\nsn.scatterplot(x='strength' ,y='superplastic',data=concrete_data,ax=axis[0]);\n\naxis[1].set_title(\"Water\")\nsn.scatterplot(x='strength' ,y='water',data=concrete_data,ax=axis[1]);\n\naxis[2].set_title(\"Age\")\nsn.scatterplot(x='strength' ,y='age',data=concrete_data,ax=axis[2]);\n\naxis[2].set_title(\"Fly Ash\")\nsn.scatterplot(x='strength' ,y='ash',data=concrete_data, palette=['orange'],  ax=axis[3]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Outlier Analysis\n\n- The outliers for the attributes **'Slag' , 'Fine Aggergate'** looks to be nominal hence not altering those attributes\n- Done a scatter plot to look into the attribute which has the higher number of outliers **'Super Plastic' , 'Water' , 'Age'**\n- The outliers for the **'Water'** looks to be more even a little of the values in lower and uper whiskers, leaving this attribue intact\n- The oultiers for the **'Super Plastic'** is little spread across, the impact could be mainly of 0 values. We will experiment modifying the 0's\n- The **'ash'** attribute looks to be similar to the **'Super Plastic'** the difference is the upper whisker spread little lower \n- The outlier treatment needed for the **'age'** attribute as it looks to be more spread across, treating the upper value alone would be better\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# treating the outliers of the 'age' and 'superplastic'\n\nage_IQR = concrete_data[\"age\"].quantile(.75)-concrete_data[\"age\"].quantile(.25)\nage_upperWhisker = concrete_data[\"age\"].quantile(.75) + (age_IQR*1.5)\n\nprint(f'\\n Qunatile Q3(75): {age_upperWhisker}')\n\nconcrete_outlierremoved_data = concrete_data.copy()\n\nconcrete_outlierremoved_data['superplastic'] = concrete_outlierremoved_data['superplastic'].replace({0:concrete_outlierremoved_data['superplastic'].median()})\nconcrete_outlierremoved_data['superplastic'] = pi.where(concrete_outlierremoved_data['superplastic']>20,20,concrete_outlierremoved_data['superplastic'])\n\nconcrete_outlierremoved_data['age'] = pi.where(concrete_outlierremoved_data['age']>age_upperWhisker,age_upperWhisker,concrete_outlierremoved_data['age'])\n\n# Do a histogram to see the difference\n\nfig, axis = plt.subplots(2, 2, figsize=(25, 8), sharex=False)\n\naxis[0,0].set_title('Super Plastic')\nsn.distplot(concrete_data['superplastic'],color='blue',ax=axis[0,0]);\n\naxis[0,1].set_title(\"Super Platic\")\nsn.distplot(concrete_outlierremoved_data['superplastic'],color='blue',ax=axis[0,1]);\n\naxis[0,0].set_title('age')\nsn.distplot(concrete_data['age'],color='orange',ax=axis[1,0]);\n\naxis[0,1].set_title(\"age\")\nsn.distplot(concrete_outlierremoved_data['age'],color='orange',ax=axis[1,1]);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the outlier treatment\n\n- Removing of the outliers on the **'superplastic'** seems to normalise the data and reduce the skew\n- Treating the outliers to upper whisker has reduced the skewness of the data\n- Need to do univarite analysis for the other attribute before finalising on the outlier treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nFive point summary for the attributes')\nconcrete_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Data: ', concrete_data.shape)\n\nprint('\\nMedian for the data')\nprint(concrete_data.median())\n\nprint('\\nMode for the data')\nprint(concrete_data.mode())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nSkewing of the data\\n')\n\nfor col_name in concrete_columns:\n    print('{0} Parameter is Right Skewed:   {1}'.format(col_name,concrete_data[col_name].mean() > concrete_data[col_name].median()))\n    \nprint('\\n{0} Parameter is Left Skewed: {1}'.format('water',concrete_data['water'].mean() < concrete_data['water'].median()))\nprint('{0} Parameter is Left Skewed: {1}'.format('superplastic',concrete_data['superplastic'].mean() < concrete_data['superplastic'].median()))\nprint('{0} Parameter is Left Skewed: {1}'.format('fineagg',concrete_data['fineagg'].mean() < concrete_data['fineagg'].median()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Five point analysis\n\n- The difference between the median and the max values are greater for the attributes **'cement' , 'slag' , 'ash' , 'age'**\n- The above differene causes the skewness of the attributes, most of these attribute are rightly skewed\n- Some of the attributes have 0 as modes as the 0's are more in these attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Display_DistPlot(col,axis_rad,color):\n    axis_rad.set_title(col)\n    sn.distplot(concrete_data[col],color=color,ax=axis_rad);\n\nsn.set(font_scale=1.5)\n\nfig, axis = plt.subplots(2, 4, figsize=(30, 15), sharex=False)\n\nDisplay_DistPlot('cement',axis[0,0],'blue')\nDisplay_DistPlot('slag',axis[0,1],'blue')\nDisplay_DistPlot('ash',axis[0,2],'blue')\nDisplay_DistPlot('water',axis[0,3],'blue')\n\nDisplay_DistPlot('superplastic',axis[1,0],'orange')\nDisplay_DistPlot('coarseagg',axis[1,1],'orange')\nDisplay_DistPlot('fineagg',axis[1,2],'orange')\nDisplay_DistPlot('age',axis[1,3],'orange')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Univariate Analysis\n\n- The attributes **'cement , 'Coarse Agg' , 'Fine Agg'** looks to be more normalise with little dip in the curve\n- The **'slag' , 'ash' , 'superplastic'** has additional gaussians due the 0's\n- The **'age'** attribute has multiple gaussians and its rightly skewed"},{"metadata":{},"cell_type":"markdown","source":"## Correlation or BiVariate Analysis\n\n- The correlaation matrix with heat map gives the visualisation of the atrribute relation\n- The threshold valued heat map gives the better clarity on the relations\n- The pairplot shows the detailed view of the relation between each of the attributes\n- Followed by swarm or violin plot will share some details on the selecte attribute impacts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the Correlation between the variables\n\nfig, axis = plt.subplots(2, 1, figsize=(30, 20), sharex=False)\n\nsn.set(font_scale=1.5)\n\nsn.heatmap(concrete_data.corr(), mask=pi.triu(concrete_data.corr()),annot_kws={\"size\": 14}, annot=True,fmt='.3f',ax=axis[0]);\n\ncorr_thresold=0.1\nconcrete_corr_threshold=concrete_data.corr()>corr_thresold\n\nsn.heatmap(concrete_corr_threshold,mask=pi.triu(concrete_data.corr()), annot_kws={\"size\": 16},annot=True,fmt='d',ax=axis[1]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngraph=sn.pairplot(concrete_data,kind='scatter',x_vars=concrete_columns,y_vars=concrete_columns,diag_kind='kde');\n\ngraph.fig.set_size_inches(35,35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Analysis\n\n- **'Cement'** and **'Strength'** shows a postivie correlation with the data scattered on the right\n- **'slag' , 'ash' , 'water' , 'superplastic' , 'coarseagg' , 'fineagg'** have multiple gussians, will do PCA analysis to form clusters\n- **'age'** shows a quite a number of gaussians\n- **'strength'** correlation with the different attribute is more scattered, which suggest that any of the feature need not to be removed\n- **'water'** and **'superplastic'** shows a negative correlation, need a study with the swarm plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(2, 3, figsize=(30, 20), sharex=False)\n\nsn.scatterplot(data=concrete_data,x='water',y='superplastic',hue='strength', ax=axis[0,0]);\nsn.scatterplot(data=concrete_data,x='water',y='fineagg',hue='strength',ax=axis[0,1]);\nsn.scatterplot(data=concrete_data,x='age',y='superplastic',hue='strength',ax=axis[0,2]);\n\nsn.scatterplot(data=concrete_data,x='cement',y='superplastic',hue='strength',ax=axis[1,0]);\nsn.scatterplot(data=concrete_data,x='cement',y='coarseagg',hue='strength',ax=axis[1,1]);\nsn.scatterplot(data=concrete_data,x='cement',y='fineagg',hue='strength',ax=axis[1,2]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Analysis\n\n- As discussed earlier 'water' and superplastic have negative correlation, moreover more the water lesser the superplastic and strength\n- 'cement' and 'coarseagg' has interesting correlation higher cement and mid less to higher the coarse the stronger it gets, even with higher coarseagg with lesser cement the strength is always lower\n- The age is little strange, higher age value also shows lower superplastic and strength\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying the minimum values for the attributes\n\nprint('\\nUniques values of the \"slag\"\\n')\nprint(pi.sort(concrete_data['slag'].unique()))\n\nprint('\\nUniques values of the \"ash\"\\n')\nprint(pi.sort(concrete_data['ash'].unique()))\n\nprint('\\nUniques values of the \"superplastic\"\\n')\nprint(pi.sort(concrete_data['superplastic'].unique()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data set for the analysis\n\n- Making the values to the common units for the analysis\n- Not removing any of the features or attribues as it all looks good\n- There are 0 values in the attribue hence imputing it to the median values\n- The outliers does not seem to have impact hance leaving the outliers as it is for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the simple imputer to change the 0's to median\n\nimputer = SimpleImputer(missing_values=0,strategy='median')\nimputer.fit(concrete_data)\n\nconcrete_mod_data = pd.DataFrame(imputer.transform(concrete_data))\nconcrete_mod_data.columns=concrete_data.columns\n\nprint('\\nZeros in each of the attribute\\n')\nfor col in concrete_columns:\n    print(f\"Total 0's in Column[{col}] : {(concrete_mod_data[col]==0).sum()}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA Analysis - Composite Feature and Gaussian\n\n- Using PCA for the composite feature and dimensionality reduction\n- The PCA cluster as well helpful in testing the accuracy of the model\n- The cluster could be also analysed for any gussians as well as the prediction strength"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = concrete_mod_data.columns\n\n# pre-processing the split data using the preprocesser technique, standard processor\nscaler_model = preprocessing.StandardScaler()\nconc_standardscaled_array= scaler_model.fit_transform(concrete_mod_data)\n\nconc_standardscaled = pd.DataFrame(conc_standardscaled_array,columns=column_names)\n\nconc_stdscaled_x = conc_standardscaled.drop(['strength'],axis=1)\nconc_stdscaled_y = conc_standardscaled['strength']\n\nprint(f\"\\nScales Axis Shape: {conc_stdscaled_x.shape}\")\nconc_stdscaled_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ncomponents_selected=8\npca = PCA(n_components=components_selected)\npca.fit(conc_stdscaled_x)\n\nprint(f\"\\n The explained variance based on the {components_selected} components\\n\")\nprint(pca.explained_variance_ratio_)\n\n\ncumm_var_ratio = pd.Series(pi.cumsum(pca.explained_variance_ratio_))\nprint(f\"\\nThe cummulative variance ratio base on the {components_selected} components\\n\")\nprint(cumm_var_ratio)\n\n\nplt.bar(list(range(1,9)),pca.explained_variance_ratio_,alpha=1, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()\n\nplt.step(list(range(1,9)),pi.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components_selected=7\npca = PCA(n_components=components_selected)\npca.fit(conc_stdscaled_x)\n\nprint(f\"\\n The explained variance based on the {components_selected} components\\n\")\nprint(pca.explained_variance_ratio_)\n\n\ncumm_var_ratio = pd.Series(pi.cumsum(pca.explained_variance_ratio_))\nprint(f\"\\nThe cummulative variance ratio base on the {components_selected} components\\n\")\nprint(cumm_var_ratio)\n\n\nconc_transformed_x = pd.DataFrame(pca.transform(conc_stdscaled_x))\nconc_transformed_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph=sn.pairplot(conc_transformed_x,kind='scatter',diag_kind='kde');\n\ngraph.fig.set_size_inches(35,35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the analysis\n\n- All the transformed clusters are independent does not show any correlation\n- Identified 7 cluster features for the model prediction"},{"metadata":{},"cell_type":"markdown","source":"## Identifying the features for the model through the coeff\n- the pairplot visualisation helped in analysing the features\n- In order to identify the feature importance, using the Coeff function and filtering features based on the values"},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_mod_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conc_x=concrete_mod_data.drop(['strength'],axis=1)\nconc_y=concrete_mod_data['strength']\n\nx_train_feature,x_test_feature,y_train_feature,y_test_feature = train_test_split(conc_x,conc_y,test_size=0.3,random_state=80)\n\nsvm_model = svm.SVR(kernel='linear')\nsvm_model.fit(x_train_feature,y_train_feature)\n\nceof_data=pd.concat([pd.Series(svm_model.coef_[0]),pd.Series(x_train_feature.columns)],axis=1)\n\nprint(\"\\n Feature coeff\\n\")\nprint(ceof_data.sort_values(by=[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Feature selection\n\n- All features are showing good feature coeff, hence not removing any of the features"},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data for the Model\n\n- Normal approach to split it into 70:30, the same will be following\n- Model will be trained with regular as well as the PCA cluster data for accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Sclaing the source data\nconc_zscaled=concrete_mod_data.apply(zscore)\n\n# Standard data\nconc_zscaled_x=conc_zscaled.drop(['strength'],axis=1)\nconc_zscaled_y=conc_zscaled['strength']\n\nconc_zscaled_x.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA transformed data \n\nconc_stdscaled_y\nconc_transformed_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spliting the different data\n\nx_train_zscaled,x_test_zscaled,y_train_zscaled,y_test_zscaled = train_test_split(conc_zscaled_x,conc_zscaled_y,test_size=0.3,random_state=160)\n\n\n# PCA Cluster data split\n\nx_train_pca,x_test_pca,y_train_pca,y_test_pca = train_test_split(conc_transformed_x,conc_stdscaled_y,test_size=0.3,random_state=160)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Algorithms - SVM & Decession Tree Regressor Models\n\n- SVM and the Decession Tree looks to be best models for this data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvm_model = svm.SVR(gamma=.7,C=10)\nsvm_model.fit(x_train_zscaled,y_train_zscaled)\n\nsvm_predict_train = svm_model.predict(x_train_zscaled)\nsvm_predict_test = svm_model.predict(x_test_zscaled)\n\nsvm_train_accuracy = metrics.r2_score(y_train_zscaled,svm_predict_train)\nsvm_test_accuracy = metrics.r2_score(y_test_zscaled,svm_predict_test)\n\nprint(\"\\nAccuracy of the SVM Model\\n\")\nprint(f\"Train Accuracy: {svm_train_accuracy}\")\nprint(f\"Test  Accuracy: {svm_test_accuracy}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvm_model = svm.SVR(gamma=.6,C=10)\nsvm_model.fit(x_train_pca,y_train_pca)\n\nsvm_predict_train_pca = svm_model.predict(x_train_pca)\nsvm_predict_test_pca = svm_model.predict(x_test_pca)\n\nsvm_train_accuracy_pca = metrics.r2_score(y_train_pca,svm_predict_train_pca)\nsvm_test_accuracy_pca = metrics.r2_score(y_test_pca,svm_predict_test_pca)\n\nprint(\"\\nAccuracy of the SVM Model\\n\")\nprint(f\"Train Accuracy: {svm_train_accuracy_pca}\")\nprint(f\"Test  Accuracy: {svm_test_accuracy_pca}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_model = svm.SVR(gamma=.5,C=10)\nkfoldcross = KFold(n_splits=30, random_state=190,shuffle=True)\nscore = cross_val_score(svm_model, conc_zscaled_x, conc_zscaled_y, cv=kfoldcross)\n\nkfold_score=score.mean()\nprint(\"Kfold Crossvalidation score\")\nprint(f\"Accuracy: {kfold_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Model Tuning for SVM\n\n- Tuned the C and Alpha values for an accuracy of 79% in the test data\n- Extra performance Squezee through the Kfold yielded 87.9% accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"dectree_model = DecisionTreeRegressor(max_depth=14)\ndectree_model.fit(x_train_zscaled,y_train_zscaled)\n\ndectree_predict_train = dectree_model.predict(x_train_zscaled)\ndectree_predict_test = dectree_model.predict(x_test_zscaled)\n\ndectree_train_accuracy = metrics.r2_score(y_train_zscaled,dectree_predict_train)\ndectree_test_accuracy = metrics.r2_score(y_test_zscaled,dectree_predict_test)\n\nprint(\"\\nAccuracy of the Decission Tree Model\\n\")\nprint(f\"Train Accuracy: {dectree_train_accuracy}\")\nprint(f\"Test  Accuracy: {dectree_test_accuracy}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dectree_model = DecisionTreeRegressor(max_depth=22)\ndectree_model.fit(x_train_pca,y_train_pca)\n\ndectree_predict_train_pca = dectree_model.predict(x_train_pca)\ndectree_predict_test_pca = dectree_model.predict(x_test_pca)\n\ndectree_train_accuracy_pca = metrics.r2_score(y_train_pca,dectree_predict_train_pca)\ndectree_test_accuracy_pca = metrics.r2_score(y_test_pca,dectree_predict_test_pca)\n\nprint(\"\\nAccuracy of the Decission Tree Model\\n\")\nprint(f\"Train Accuracy: {dectree_train_accuracy_pca}\")\nprint(f\"Test  Accuracy: {dectree_test_accuracy_pca}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dectree_model = DecisionTreeRegressor(max_depth=50)\nkfoldcross = KFold(n_splits=40, random_state=190,shuffle=True)\nscore = cross_val_score(dectree_model, conc_zscaled_x, conc_zscaled_y, cv=kfoldcross)\n\nkfold_score=score.mean()\nprint(\"Kfold Crossvalidation score\")\nprint(f\"Accuracy: {kfold_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparam_dist = {\"max_depth\": range(20,50),\n              \"max_features\": sp_randint(7, 9),\n              \"min_samples_split\": range(5, 20),\n              \"min_samples_leaf\": range(5, 20),\n              \"criterion\": [\"mse\", \"mae\"]}\n\nrandomCV = RandomizedSearchCV(dectree_model, param_distributions=param_dist, n_iter=50) #default cv = 3\n\n#print(dectree_model.get_params().keys())\n\nrandomCV.fit(conc_zscaled_x, conc_zscaled_y)\nprint(randomCV.best_params_)\n\nprint(pi.sort(randomCV.cv_results_['mean_test_score']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Model Tuning for Decission Tree\n\n- Tuned the diffent parameters of the Decission tree the accuracy of 80% in the test data\n- Extra performance Squezee through the Kfold yielded 85% accuracy"},{"metadata":{},"cell_type":"markdown","source":"## Final Summary\n\n- The SVM Model was tuned using the Kfold to get an accuracy of 87.9% inspite of initial 79%\n- The Decission tree was tuned using the kfold to get an accuracy of 85% inspite of initial 80%\n- The Random Search CV was not able to push the model for greater accuracy, it only yielded 80% max"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}