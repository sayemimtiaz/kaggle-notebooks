{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Electric Vehicle (EV) Analysis and Temperature Prediction\n\nBeing able to predict the operating parameters of an electric vehicle motor is an important emerging task due to the huge demand of electric vehicles in todays society. \n\nThroughout this notebook, we'll take the measured parameters of a Permanent-Magnet Synchronous Motor (PMSM) and attempt to predict all four output temperatures (permanent magnetic, stator yoke, stator winding and stator tooth) as accurately as possible. This forms an interesting multi-output regression task, and we'll explore various feature engineering, models and hyper-parameter tuning accordingly. \n\n**Table of Contents:**\n\n1. [Imports](#imports)\n2. [Creation of Training / Test Splits](#training-splits)\n3. [EDA](#EDA)\n4. [Baseline Predictions](#baseline-predictions)\n5. [Feature Engineering](#feature-engineering)\n6. [Improved Splitting of Data](#data-splitting)\n7. [Updated Baseline after Feature Engineering](#updated-baseline)\n8. [Model Cross-Validation](#cross-validation)\n9. [Model Refinements and Evaluation](#model-refinements)\n10. [Test Set Predictions](#final-test-set-predictions)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n## 1. Import dependencies and dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, cross_validate\n\nfrom collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, BaggingRegressor, VotingRegressor\nfrom sklearn.linear_model import ElasticNet, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold, GroupShuffleSplit\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVR, LinearSVR\n\nfrom xgboost import XGBRegressor\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_df = pd.read_csv(\"/kaggle/input/electric-motor-temperature/pmsm_temperature_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"training-splits\"></a>\n## 2. Creation of our main training and test splits "},{"metadata":{},"cell_type":"markdown","source":"We want to allocate a proportion of the total data as our test set. We'll do this using the profile IDs, since we want to fairly test the generalisation performance of our models. If we leak information from various profile ids in the test set into the training set, our models will be assessed as over-optimistically good in terms of performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_profiles = data_df['profile_id'].value_counts().index.values\ntest_set_profiles = [31, 32, 41, 42, 51, 52, 61, 62, 71, 73]\ntrain_set_profiles = [id for id in full_profiles if id not in test_set_profiles]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = data_df.loc[data_df['profile_id'].isin(train_set_profiles)].copy()\ntest_df = data_df.loc[data_df['profile_id'].isin(test_set_profiles)].copy()\n\nlen(train_df['profile_id'].value_counts()), len(test_df['profile_id'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_profiles = test_df.loc[:, 'profile_id'].copy()\ntest_labels = test_df.loc[:, ['pm', 'stator_yoke', 'stator_tooth', 'stator_winding']].copy()\ntest_df.drop(['profile_id', 'pm', 'stator_yoke', 'stator_tooth', 'stator_winding'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add names to the index of each df\ntrain_df.index.name = 'timestep'\ntest_df.index.name = 'timestep'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 3. Initial Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 General trends of our data features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_df.loc[:, 'ambient'].plot(figsize=(16,4))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[:, 'coolant'].plot(figsize=(16,4))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our training data appears to be ordered and not shuffled, as can be seen from the relatively continuous series represented above for the two selected variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features within the data appears to have already been standardised with a zero mean and standard deviation of 1. This is unfortunate, since it makes manipulation of the electrical parameters and measured values into more useful and insightful ones a more difficult task."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['profile_id'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Exploring trends within an example Profile ID"},{"metadata":{},"cell_type":"markdown","source":"**Lets briefly explore the data associated with an example profile ID**"},{"metadata":{"trusted":true},"cell_type":"code","source":"EXAMPLE_ID = 10\n\nexample_profile = train_df.loc[train_df['profile_id'] == EXAMPLE_ID].copy()\nexample_profile['ambient'].plot(figsize=(16,7))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data is extremely noisy in its current form, which could be detrimental to our results for making temperature predictions. To overcome this, we could consider pre-processing strategies such as applying filters over the data to smooth it. A good strategy for this could be to apply one or more low-pass filters to the data. A good means of achieving this could be to apply an exponentially weighted moving average and standard deviation to each of our features, especially those that are particularly noisy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a range of spans to experiment with\nspans = [600, 1200, 2400, 4800]\n\nfor span in spans:\n    new_col = f\"ambient_exp_{span}\"\n    example_exp_ma = example_profile.ewm(span=span, adjust=False).mean()\n    example_profile[new_col] = example_exp_ma['ambient'].copy()\n    \nambient_cols = [x for x in example_profile.columns.values if x.startswith('ambient')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[ambient_cols].plot(figsize=(16,7))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By applying exponential weighted moving average functions as shown above, we can effectively reduce the noise within our dataset, which will hopefully help improve the performance of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a range of spans to experiment with\nspans = [600, 1200, 2400]\n\nchosen_col = 'coolant'\n\nfor span in spans:\n    new_col = f\"{chosen_col}_exp_{span}\"\n    example_exp_ma = example_profile.ewm(span=span, adjust=False).mean()\n    example_profile[new_col] = example_exp_ma[chosen_col].copy()\n    \nchosen_cols = [x for x in example_profile.columns.values if x.startswith(chosen_col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[chosen_cols].plot(figsize=(16,8))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Outflow Coolant Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the basic measured parameters, we want to be able to predict the temperatures at the rotor, stator yoke, stator tooth, and stator winding as part of the efforts in increasing Electric Vehicle (EV) efficiencies. It's not practical to be able to measure these in real time for commercialised vehicles, so being able to accurately predict these temperatures would be extremely valueble."},{"metadata":{"trusted":true},"cell_type":"code","source":"output_vars = ['pm', 'stator_yoke', 'stator_tooth', 'stator_winding']\n\nexample_profile[output_vars].plot(figsize=(16,6))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good thing is that for the example we've looked at, each of the four output dependent variables appear to be highly correlated. This should simply the process of making predictions for each of the variables given the operational motor data. When results are more correlated like this, it makes it easier for our multiple-output regression models to produce a good set of results from a unified model, rather than having to form individual models for each predicted output.\n\nLet's see how these output variables look after applying applying an exponentially weighted average to each variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_ma = example_profile[output_vars].ewm(span=500).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_ma.plot(figsize=(16,6))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Temperature (Exp Weighted Average)\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's assess the input features over time:"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_cols = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'torque', 'i_d', 'i_q']\n\nexample_profile[input_cols].plot(figsize=(18,8))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()\n\nexample_profile[input_cols].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Correlation of our features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_correlations = train_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(train_df_correlations, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is useful to understand the correlation some variables have relative to one-another. For instance, coolant looks like it is highly correlated to the stator yoke temperature. Since stator yoke temperature is also highly correlated to the other output variables, this could come in useful for making inferences of temperature given a set of input features.\n\nA key thing to keep in mind is that a correlation matrix such as that above will only capture linear correlations. When analysing the previous plots, its clear that many of our variables are correlated, but these are more complex relationships that may not be captured well be the pearson correlation coefficient, which the matrix above is based on."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sample = train_df.sample(100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.scatterplot(x='coolant', y='stator_yoke', data=training_sample, alpha=0.05)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a nice correlation between coolant and one of the dependent outputs - stator yoke."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nax = sns.regplot(x=\"i_d\", y=\"pm\", data=training_sample, scatter_kws={'alpha':0.01})\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nax = sns.regplot(x=\"i_q\", y=\"pm\", data=training_sample, scatter_kws={'alpha':0.02})\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets look at how the output variables are correlated to one-another:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.scatter_matrix(training_sample[output_vars], figsize=(16, 10), alpha=0.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of our output variables are highly correlated to one another, which makes sense; as the temperature of a given component tends to increase in the motor, so do all of the others on average. Again, as stated previously, this makes the multi-output regression problem slightly easier, since the different outputs we are trying to predict are all similar."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"baseline-predictions\"></a>\n## 4. Baseline predictions on the test set using default off-the-shelf models"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Basic Multiple-Output Linear Regression OLS "},{"metadata":{},"cell_type":"markdown","source":"Just so that the process of making a model and predicting on the dataset is familier, lets make some predictions on the test set using simple baseline models. We'll evaluate these models on the test set just to have an idea of what we should be aiming to beat."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[output_vars]\nX = train_df.drop(columns=output_vars + ['profile_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets train a normal multiple-output linear regression model on the data and see what results we obtain:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\n\nlr_scores = cross_val_score(lin_reg, X_train, y_train, \n                         scoring='neg_mean_squared_error', cv=10)\n\nlr_rmse_scores = np.sqrt(-lr_scores)\nprint(f\"Linear Regression RMSE: {lr_rmse_scores.mean():.5f} +/- {lr_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg.fit(X_train, y_train)\nval_preds = lin_reg.predict(X_test)\nprint(f\"Linear Regression RMSE on Validation Split: {np.sqrt(mean_squared_error(val_preds, y_test)):.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Examining the predictions made by our baseline for an example Profile ID"},{"metadata":{},"cell_type":"markdown","source":"Let's see what this model is like for reproducing a random profile from the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_4_idx = train_df[train_df['profile_id'] == 4].copy().index\nprofile_4_preds = lin_reg.predict(X.iloc[profile_4_idx])\nprofile_4_original = train_df.loc[profile_4_idx, output_vars]\nprofile_4_preds.shape, profile_4_original.shape\n\nnew_cols = [f\"{x}_pred\" for x in profile_4_original.columns]\nprofile_4_preds = pd.DataFrame(profile_4_preds)\nprofile_4_preds.columns = new_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_4_original[output_vars].plot(figsize=(16,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Output Temperature\", weight=\"bold\")\nplt.title(\"Original Output Temperatures - Profile 4\", weight='bold')\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_4_preds.plot(figsize=(16,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Output Temperature\", weight=\"bold\")\nplt.title(\"Predicted Output Temperatures - Profile 4\", weight='bold')\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in profile_4_original.columns:\n    mse = mean_squared_error(profile_4_original[column], profile_4_preds[column + '_pred'])\n    rmse = np.sqrt(mse)\n    print(f\"RMSE for {column}: {rmse:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too bad straight off the bat, however this should not be relied on heavily since many of the data instances above could have been within the chosen training set, and therefore our linear regression model may have simply memorised many of those points above. For true generalisation evaluation we would need to test this model on a profile it has never seen before.\n\nA good option for this would be to ensure our training and validation splits used to evaluate models contain unique profile IDs to one-another, so that the data we validate against is always new and unseen."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"feature-engineering\"></a>\n## 5. Feature Engineering and Improvements for better performance"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Addition of new features:\n\nFrom the provided electrical terms and measured data we have, we can combine these using various formulae to form new (and potentially insightful) features for our dataset. A key problem is that the data for this problem is already standardised, and unfortunately we have no access to the original dimensions or standardising parameters to reverse this process. Despite this, using the relationships defined below we can may still get some meaningful features from the standardised data, even if they are no longer mathematically sound products due to the standardised form its currently in.\n\n**Voltage and current magnitudes**\n\nWe can work out the magnitudes of the voltage and current vectors by calculating the norm, as shown below:\n\n$ v_{s} = \\displaystyle\\sqrt{v_{d}^{2} + v_{q}^{2}} $\n\n$ i_{s} = \\displaystyle\\sqrt{i_{d}^{2} + i_{q}^{2}} $\n\nWhere $ v_{s} $ is the voltage magnitude, $ v_{d} $ is the d-component of the voltage, and $ v_{q} $ is the q-component of the voltage.\n\n**Apparent Power**\n\nUsing these features, we can then determine electrical apparent power (magnitude of complex power), which is given by:\n\n$ \\vert S \\vert = 1.5 \\times v_{s} \\times i_{s} $\n\n**Effective Power**\n\nUsing the components of voltage and current we can also calculate effective power like so:\n\n$ P = \\big( v_{d} \\times v_{q} \\big) + \\big( i_{d} \\times i_{q} \\big) $\n\n**Additional Feature Combinations**\n\nWe'll also add some additional features, which represent the interaction between parameters of interest. This will include:\n\n- Motor speed and current magnitude: $ \\omega \\times i_{s} $\n\n\n- Motor speed and apparent power: $ \\omega \\times \\vert S \\vert $"},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm(row, col1, col2):\n    return np.linalg.norm([row[col1], row[col2]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_new_features(dataframe):\n    \"\"\" Calculate new electrical features for our dataset \"\"\"\n    extra_df = dataframe.copy()\n    \n    extra_df['voltage_s'] = extra_df.apply(norm, args=('u_d', 'u_q'), axis=1)\n    extra_df['current_s'] = extra_df.apply(norm, args=('i_d', 'i_q'), axis=1)\n    extra_df['apparent_power'] = 1.5 * extra_df['voltage_s'] * extra_df['current_s']\n    extra_df['effective_power'] = ((extra_df['u_d'] * extra_df['u_q']) +\n                                (extra_df['i_d'] * extra_df['i_q']))\n    extra_df['speed_current'] = extra_df['current_s'] * extra_df['motor_speed']\n    extra_df['speed_power'] = extra_df['current_s'] * extra_df['apparent_power']\n    \n    return extra_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = find_new_features(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile = train_df.loc[train_df['profile_id'] == EXAMPLE_ID].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()\n\nexample_profile[['voltage_s', 'current_s', 'apparent_power']].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that apparent power actually correlates well to the output variables - at least for this example profile ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars + ['apparent_power']].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets now standardise our apparent power variable with zero mean and a unit standard deviation, which should fit it to the output variable data better:"},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile['apparent_power'] = ((example_profile['apparent_power'] - \n                                     example_profile['apparent_power'].mean()) /\n                                     example_profile['apparent_power'].std())\n\nexample_profile[output_vars + ['apparent_power']].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars + ['effective_power']].plot(figsize=(18,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Ambient Temperature\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_var = 'apparent_power'\nalpha = 0.05\n\nplt.figure(figsize=(14,9))\n\nplt.subplot(221)\nax = sns.regplot(x=plot_var, y=\"pm\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(222)\nax = sns.regplot(x=plot_var, y=\"stator_yoke\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(223)\nax = sns.regplot(x=plot_var, y=\"stator_tooth\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(224)\nax = sns.regplot(x=plot_var, y=\"stator_winding\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Introduction of exponentially weighted averages"},{"metadata":{},"cell_type":"markdown","source":"Let's try applying a low pass filter (exponentially weighted averages are the equivalent of doing this) to our data and seeing how this impacts the relationship our input features have with the output features.\n\nLet's try with the apparent power variable and see how different variations of it correlate to the output variables when we apply averaging functions with different spans:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a range of spans to experiment with\nspans = [600, 1200, 2400, 4800]\n\nchosen_col = 'apparent_power'\n\nfor span in spans:\n    new_col = f\"{chosen_col}_exp_{span}\"\n    example_exp_ma = example_profile.ewm(span=span, adjust=False).mean()\n    example_profile[new_col] = example_exp_ma[chosen_col].copy()\n    \nchosen_cols = [x for x in example_profile.columns.values if x.startswith(chosen_col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[chosen_cols].plot(figsize=(16,5))\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Apparent Power (Standardised)\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = output_vars + chosen_cols\ncolor_dict = {}\n\nfor column in cols:\n    if column == 'apparent_power':\n        color_dict[column] = 'black'\n    elif column.startswith('apparent_power_exp'):\n        color_dict[column] = 'tab:blue'\n    else:\n        color_dict[column] = 'tab:red'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars + chosen_cols].plot(figsize=(16,7), \n                                                color=[color_dict.get(x, 'black') for x in cols])\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Standardised Feature Values\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its quite interesting how well the apparent power feature correlates to our output temperatures when we apply a low-pass filter (exponentially weighted average) to it. This certainly works well for this specific profile ID. It could be worth exploring others to make sure it also correlates well to those.\n\nIn all cases, it seems like the apparent power feature was well worth introducing into our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# translate apparent power variables by 1.5\nexample_profile[chosen_cols[1:]] = example_profile[chosen_cols[1:]] - 1.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_profile[output_vars + chosen_cols[1:]].plot(figsize=(16,7), \n                                                color=[color_dict.get(x, '#333333') for x in cols])\nplt.xlabel(\"Timestep (0.5s per step)\", weight=\"bold\")\nplt.ylabel(\"Standardised Feature Values\", weight=\"bold\")\nplt.tight_layout()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through a simple translation downwards, we can almost fit the apparent power plots onto the various temperature plots!\n\nIt will be worth fitting a new model with these variables and seeing how this improves performance relative to the baseline models!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_var = 'apparent_power_exp_2400'\nalpha = 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,9))\n\nplt.subplot(221)\nax = sns.regplot(x=plot_var, y=\"pm\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(222)\nax = sns.regplot(x=plot_var, y=\"stator_yoke\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(223)\nax = sns.regplot(x=plot_var, y=\"stator_tooth\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\n\nplt.subplot(224)\nax = sns.regplot(x=plot_var, y=\"stator_winding\", data=example_profile, scatter_kws={'alpha':alpha})\nplt.grid()\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = example_profile.corr()\ncorr_matrix['stator_yoke'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The relationships are not quite linear and are more complex, but they're definitely much better than before we added any features."},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Extending our data with the exponentially weighted average features \n\nLet's add these exponentially weighted averages to our training data and see how they perform when used to train a model. We'll automate this process through the definition of a class to do our pre-processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"class PMSMDataProcessor(BaseEstimator, TransformerMixin):\n    \"\"\" PMSM data processor and loader \"\"\"\n    \n    def __init__(self, add_exp_terms=True, spans=[600, 1200, 2400, 4800],\n                 output_vars=['pm', 'stator_tooth', 'stator_yoke', 'stator_winding']):\n        self.add_exp_terms = add_exp_terms\n        self.spans = [600, 1200, 2400, 4800]\n        self.output_vars = output_vars\n        self.training_cols = []\n        \n        \n    def fit(self, X, y=None):      \n        return self\n                \n    \n    def transform(self, X):\n        \"\"\" Create new features from our given data \"\"\"\n        \n        extra_df = self._find_new_features(X)\n        \n        # update training cols\n        self._update_train_cols(extra_df)\n        \n        # if selected add additional exponential terms\n        if self.add_exp_terms:\n            extra_df = self._find_exp_terms(extra_df).copy()\n            \n            # update training cols again to reflect new additions\n            self._update_train_cols(extra_df)\n            \n        return extra_df\n        \n        \n    def _find_norm(self, row, col1, col2):\n        return np.linalg.norm([row[col1], row[col2]])\n    \n    \n    def _update_train_cols(self, X):\n        \"\"\" Compile list of base columns used for training \"\"\"\n        self.training_cols = [x for x in X if x != 'profile_id' \n                              and x not in self.output_vars]\n    \n    \n    def _find_new_features(self, dataframe):\n        \"\"\" Calculate new electrical features for our dataset \"\"\"\n        \n        extra_cols = dataframe.copy()\n        \n        extra_cols['voltage_s'] = extra_cols.apply(self._find_norm, \n                                                   args=('u_d', 'u_q'), axis=1)\n        extra_cols['current_s'] = extra_cols.apply(self._find_norm, \n                                                   args=('i_d', 'i_q'), axis=1)\n        \n        extra_cols['apparent_power'] = 1.5 * extra_cols['voltage_s'] * extra_cols['current_s']\n        extra_cols['effective_power'] = ((extra_cols['u_d'] * extra_cols['u_q']) +\n                                         (extra_cols['i_d'] * extra_cols['i_q']))\n        \n        extra_cols['speed_current'] = extra_cols['current_s'] * extra_cols['motor_speed']\n        extra_cols['speed_power'] = extra_cols['current_s'] * extra_cols['apparent_power']\n    \n        return extra_cols\n    \n    \n    def _find_exp_terms(self, dataframe):\n        \"\"\" Add exponential terms to all features, using the class spans \"\"\"\n        \n        extended_df = dataframe.copy()\n        \n        # add new columns to each feature for each span\n        for span in self.spans:\n            exp_ma_df = dataframe.ewm(span=span, adjust=False).mean()\n            for column in self.training_cols:\n                new_col = f\"{column}_exp_{span}\"\n                extended_df[new_col] = exp_ma_df[column].copy()\n                \n        return extended_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Process data from scratch again:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_preprocessor = PMSMDataProcessor()\n\ntrain_df_extra = data_preprocessor.fit_transform(train_df)\ntest_df_extra = data_preprocessor.transform(test_df)\n\ny = train_df_extra[data_preprocessor.output_vars].copy()\nX = train_df_extra.drop(columns=data_preprocessor.output_vars)\n\nX.shape, y.shape, test_df_extra.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we dont want to use it as training data, we need to keep track of our profile IDs within the training set so that we can ensure appropriate splits of data between training and validation splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select profile id's as groups to split our folds on\ngroups = X.loc[:, 'profile_id'].copy()\nX.drop('profile_id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Standardise all features of our processed data\n\nAlthough our data was already standardised originally, since we've introduced new features we should standardise again to ensure the new columns are consistent relative to the original features.\n\nWe'll need to remove the profile IDs, standardise our features, and then add it in as a column again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX_std = pd.DataFrame()\nX_std[X.columns] = pd.DataFrame(std_scaler.fit_transform(X))\nX_std['profile_id'] = groups.values\ntest_std = std_scaler.transform(test_df_extra)\n\nX_std.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-splitting\"></a>\n## 6. Improved splitting of data - ensuring unique profile IDs between splits"},{"metadata":{},"cell_type":"markdown","source":"For a better and true appreciation of the generalisation performance of our model we need to ensure that the training, validation and test splits all contain different profile IDs. This ensures that the data is unseen from what we've trained our models on.\n\nDue to the requirement to know the profile_id of the training data, we need to keep track of the profile IDs corresponding to the training instances, which is stored in the profiles variable.\n\nWe have two options for grouped kfolds. We can either split without shuffling, using GroupKFold, or Shuffle our groups to get different subsets of groups each time (but still no like-groups in both splits, just as we need).\n\nWe'll use the shuffling option, since we want to be able to mix up the splits of profile_id we get between different folds, in order to help assess the true generalisation of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_kfold = GroupShuffleSplit(n_splits=6)\ngrouped_kfold.get_n_splits(X_std, y, groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To illustrate how this works, this kfold iterator returns the following splits:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in grouped_kfold.split(X_std, y, groups):\n    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n    train_inputs, val_inputs = X_std.iloc[train_index], X_std.iloc[test_index]\n    train_outputs, val_outputs = y.iloc[train_index], y.iloc[test_index]\n    \n    profiles_train = train_inputs['profile_id'].value_counts().index.values\n    profiles_test = val_inputs['profile_id'].value_counts().index.values\n    \n    print(f\"Profile IDs in Training set: \\n{profiles_train}\\n\")\n    print(f\"Profile IDs in Test set: \\n{profiles_test}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is good, we have a nice random subset of profile IDs for each fold, and in each case, the profile IDs within the training split are not present within the validation split. This ensures a much better evaluation of performance when testing our models."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"updated-baseline\"></a>\n## 7. Testing the baseline performance with improved splits and additional features"},{"metadata":{},"cell_type":"markdown","source":"Lets test this using our OLS linear regression model, and repeat it 10 times so that we can find the average performance across multiple attempts of cross-validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_rmse_scores = []\nmean_rmse_scores = []\n\nlr_reg = LinearRegression()\n\n# times to repeat the cross validation\nrepeats = 10\n\nfor i in tqdm(range(repeats)):\n    lr_scores = cross_val_score(lr_reg, X_std.drop('profile_id', axis=1), y, groups=groups,\n                                scoring='neg_mean_squared_error', cv=grouped_kfold)\n    rmse_scores = np.sqrt(-lr_scores)\n    lr_rmse_scores.append(rmse_scores)\n    mean_rmse_scores.append(rmse_scores.mean())\n\nlr_rmse_scores = np.array(lr_rmse_scores)\nmean_rmse_scores = np.array(mean_rmse_scores)\n\nprint(f\"Linear Regression OLS RMSE: {mean_rmse_scores.mean():.5f} +/- {mean_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results are much better than the original baseline models, which highlights the improvement that adding our extra features has resulted in. We could likely improve by filtering the output of our linear regression model, in order to smooth the output noise.\n\nWhat's interesting about this result is we might have expected performance to have dropped, due to the improvement in how we split our data. Clearly, the addition of more insightful features has helped overcome this and increase performance considerably. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cross-validation\"></a>\n## 8. Exploring a range of default models using Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"It will be heplful to explore a range of different models and test their performance against one-another accordingly, so that we can choose the best and then perform further fine-tuning.\n\nSince the training set is very large computation time can be a problem, especially if we are using techniques like cross-validation and grid-search. To counteract this, we will initially make a subsample of training data to make our comparisons on, which should greatly speed up this process. As above, we'll ensure we split training and validation so that profile ids are unique across splits:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_model_grouped_cross_validation(clf_tuple_list, X, y, groups, K_folds=6, random_seed=0, subsample=False,\n                                 score_type='neg_mean_squared_error', subsample_prop=0.1):\n    \"\"\" Find grouped cross validation scores, and print and return results \"\"\"\n    \n    model_names, model_scores = [], []\n    \n    # if selected, only use a small portion of data for speed\n    if subsample:\n        X, _, y, _ = train_test_split(X, y, training_size=subsample_prop, random_seed=random_seed)\n    \n    for name, model in clf_tuple_list:\n        grouped_kfold = GroupShuffleSplit(n_splits=K_folds)\n        cross_val_results = cross_val_score(model, X, y, groups=groups, \n                                            cv=grouped_kfold, scoring=score_type, n_jobs=-1)\n        model_names.append(name)\n        rmse_scores = np.sqrt(-cross_val_results)\n        model_scores.append(rmse_scores)\n        print(\"{0:<40} {1:.5f} +/- {2:.5f}\".format(name, rmse_scores.mean(), rmse_scores.std()))\n        \n    return model_names, model_scores\n\n\ndef boxplot_comparison(model_names, model_scores, figsize=(14, 10), score_type=\"RMSE\",\n                       title=\"K-Folds Cross-Validation Comparisons\"):\n    \"\"\" Boxplot comparison of a range of models using Seaborn and matplotlib \"\"\"\n    \n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title, fontsize=18)\n    ax = fig.add_subplot(111)\n    sns.boxplot(x=model_names, y=model_scores)\n    ax.set_xticklabels(model_names)\n    ax.set_xlabel(\"Model\", fontsize=16) \n    ax.set_ylabel(\"Model Score ({})\".format(score_type), fontsize=16)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to significant computation time and our large dataset, we'll only conduct this on a small sub-sample of the total data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trg_sub, _, y_trg_sub, _ = train_test_split(X_std, y, stratify=groups, test_size=0.99, random_state=12)\n\n# extract our training profiles from the training data prior to fitment\ntrg_profiles = X_trg_sub.loc[:, 'profile_id'].values\nX_trg_sub = X_trg_sub.drop('profile_id', axis=1)\n\nX_trg_sub.shape, y_trg_sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of classifiers to compare\nclf_list = [(\"Linear Regressor\", LinearRegression()),\n            (\"Ridge Regressor\", Ridge(alpha=250)),\n            (\"LASSO Regressor\", Lasso(alpha=0.1)),\n            (\"ElasticNet\", ElasticNet(alpha=0.01, l1_ratio=0.1)),\n            (\"Support Vector Regressor\", MultiOutputRegressor(LinearSVR())),\n            (\"Extra Trees Regressor\", ExtraTreesRegressor(n_estimators=10, bootstrap=True, max_depth=60, \n                                                          n_jobs=-1, min_samples_leaf=3, min_samples_split=5)),\n            (\"Random Forest Regressor\", RandomForestRegressor(n_estimators=10, bootstrap=True, max_depth=60, \n                                                              n_jobs=-1, min_samples_leaf=3, min_samples_split=5)),\n            (\"Multi-layer Perception\", MLPRegressor())]\n\n# obtain names and scores for each cross-validation, and print / plot results\nprint(\"K-Folds Cross-Validation Results on training set: \\n{}\".format('-'*60))\n%time model_names, model_scores = multi_model_grouped_cross_validation(clf_list, X_trg_sub, y_trg_sub, trg_profiles)\nboxplot_comparison(model_names, model_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, our linear regression models appear to be the best in this case. This has also been the case in the previous results we obtained, which suggests a simple, yet very effective solution to the problem of predicting real-time temperatures in our motors is using a linear regression OLS model.\n\nOur tree based methods took a long time to train above, so it could be worth reducing the size of our dataset further when exploring the optimisation of these methods."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-refinements\"></a>\n## 9. Exploring and tuning our individual regression models:\n\nLets explore linear regression and regularisation a bit more, since it is the best performing model from all of those above."},{"metadata":{},"cell_type":"markdown","source":"### 9.1 Exploration of a Ridge regressor and the alpha hyper-parameter:\n\nSince our ridge regressor was good, but not quite as good as standard unregularised linear regression, it could be worth exploring a range of hyper-parameters to see whether we can improve performance accordingly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_values = [0, 0.01, 0.1, 0.5, 1.0, 1.5, 3, 10, 20, 50, 100, \n                200, 250, 500, 750, 1000, 2000, 3000, 4000, 5000]\n\nridge_rmse_scores = []\nmean_ridge_rmse_scores = []\n\nfor alpha in tqdm(alpha_values):\n    ridge_reg = Ridge(alpha=alpha, fit_intercept=True)\n    ridge_scores = cross_val_score(ridge_reg, X, y, groups=groups,\n                                   scoring='neg_mean_squared_error', cv=grouped_kfold)\n    rmse_scores = np.sqrt(-ridge_scores)\n    ridge_rmse_scores.append(rmse_scores)\n    mean_ridge_rmse_scores.append(rmse_scores.mean())\n\nridge_rmse_scores = np.array(ridge_rmse_scores)\nmean_ridge_rmse_scores = np.array(mean_ridge_rmse_scores)\n\nprint(f\"Ridge RMSE: {mean_ridge_rmse_scores.mean():.5f} +/- {mean_ridge_rmse_scores.std():.5f}\")\n\n# calculate standard deviation to annotate on our plot\nridge_rmse_scores_std = ridge_rmse_scores.std(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot mean and standard deviation of cross-val scores\nplt.figure(figsize=(18,6))\nsns.lineplot(x=alpha_values, y=mean_ridge_rmse_scores)\nplt.fill_between(alpha_values, mean_ridge_rmse_scores - ridge_rmse_scores_std, \n                 mean_ridge_rmse_scores + ridge_rmse_scores_std, \n                 color='tab:blue', alpha=0.2)\nplt.semilogx(alpha_values, mean_ridge_rmse_scores + ridge_rmse_scores_std, 'b--')\nplt.semilogx(alpha_values, mean_ridge_rmse_scores - ridge_rmse_scores_std, 'b--')\nplt.ylabel(\"Cross-Validation RMSE (Average)\", weight='bold', size=14)\nplt.xlabel(\"Ridge Alpha Hyper-Parameter\", weight='bold', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2 Exploring LASSO regression in a similar way:\n\nLASSO tends to perform automatic feature reduction by setting the coefficient of low-importance features to zero during training. The extend to which this occurs is related to the hyper-parameter values we select at model creation. We'll explore a range of these similarly to how we did with Ridge regression above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# range of alpha values between 0 and 1\nalpha_values = np.logspace(-4, 1, 20)\n\nlasso_rmse_scores = []\nmean_lasso_rmse_scores = []\n\nfor alpha in tqdm(alpha_values):\n    lasso_reg = Lasso(alpha=alpha, fit_intercept=True, max_iter=1000, tol=0.5)\n    lasso_scores = cross_val_score(lasso_reg, X, y, groups=groups,\n                                   scoring='neg_mean_squared_error', cv=grouped_kfold)\n    rmse_scores = np.sqrt(-lasso_scores)\n    lasso_rmse_scores.append(rmse_scores)\n    mean_lasso_rmse_scores.append(rmse_scores.mean())\n\nlasso_rmse_scores = np.array(lasso_rmse_scores)\nmean_lasso_rmse_scores = np.array(mean_lasso_rmse_scores)\n\nprint(f\"LASSO RMSE: {mean_lasso_rmse_scores.mean():.5f} +/- {mean_lasso_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems our models struggle to converge if not given enough iterations. If our performance is good enough, it may be worth increasing this accordingly in order to maximise the potential performance of our LASSO models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate standard deviation to annotate on our plot\nlasso_rmse_scores_std = lasso_rmse_scores.std(axis=1)\n\n# plot mean and standard deviation of cross-val scores\nplt.figure(figsize=(18,6))\nsns.lineplot(x=alpha_values, y=mean_lasso_rmse_scores)\nplt.fill_between(alpha_values, mean_lasso_rmse_scores - lasso_rmse_scores_std, \n                 mean_lasso_rmse_scores + lasso_rmse_scores_std, \n                 color='tab:blue', alpha=0.2)\nplt.semilogx(alpha_values, mean_lasso_rmse_scores + lasso_rmse_scores_std, 'b--')\nplt.semilogx(alpha_values, mean_lasso_rmse_scores - lasso_rmse_scores_std, 'b--')\nplt.ylabel(\"Cross-Validation RMSE (Average)\", weight='bold', size=14)\nplt.xlabel(\"LASSO Alpha Hyper-Parameter\", weight='bold', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.3 Combining the effects of L1 and L2 regularisation together using Elastic Net:\n\nFor this we'll explore a range of alpha and l1_ratio parameters to see whether elastic net has potential or not:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# range of alpha values and l1 ratios for exploring\nalpha_values = [0.0001, 0.001, 0.01, 0.1, 1, 5, 10]\nl1_ratios = np.arange(0.0, 1.1, 0.1)\n\nresults_tuples = []\nen_rmse_scores = []\nmean_en_rmse_scores = []\n\nfor alpha in tqdm(alpha_values):\n    for l1 in l1_ratios:\n        \n        en_reg = ElasticNet(alpha=alpha, l1_ratio=l1, tol=0.5,\n                            fit_intercept=True, max_iter=1000)\n        en_scores = cross_val_score(en_reg, X, y, groups=groups,\n                                    scoring='neg_mean_squared_error', cv=grouped_kfold)\n        rmse_scores = np.sqrt(-en_scores)\n        en_rmse_scores.append(rmse_scores)\n        mean_en_rmse_scores.append(rmse_scores.mean())\n        \n        results_tuples.append((alpha, l1, rmse_scores.mean(), rmse_scores.std()))\n\nen_rmse_scores = np.array(en_rmse_scores)\nmean_en_rmse_scores = np.array(mean_en_rmse_scores)\n\nprint(f\"Elastic Net RMSE: {mean_en_rmse_scores.mean():.5f} +/- {mean_en_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# form dataframe from the results\nelasnet_results = pd.DataFrame(results_tuples)\nelasnet_results.columns = ['alpha', 'l1_ratio', 'mean', 'std']\n\n# plot results according to hyper-parameters and mean RMSE\nplt.figure(figsize=(16,6))\nsns.scatterplot(x='alpha', y='l1_ratio', hue='mean', \n                data=elasnet_results, s=150)\nplt.xlabel('ElasticNet Alpha Parameter', weight='bold', size=14)\nplt.ylabel('ElasticNet l1-ratio', weight='bold', size=14)\nplt.legend(loc='best')\nplt.xlim(-0.5, 13.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that regularisation really isn't helping our linear model perform any better on our dataset; The basic linear regression model without any regularisation tends to perform better on our temperature prediction task when compared to the regularised methods. This can be seen in the plot above, whereby the points towards the lower right of the plot (those more representing normal linear regression with no regularisation) perform much better. This suggests that we're not actually fitting our data enough, and therefore performing regularisation (preventing over-fitting) may actually be harmful rather than good. We could consider adding further features to produce a more complex model, but in this case the results will suffice."},{"metadata":{},"cell_type":"markdown","source":"### 9.4 Investigating performance of bootstrapped linear regression ensemble:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_bag_reg = BaggingRegressor(LinearRegression(), n_estimators=5, bootstrap=True)\n\nlr_bag_rmse_scores = []\nmean_lr_bag_rmse_scores = []\n\n# times to repeat the cross validation\nrepeats = 2\n\nfor i in tqdm(range(repeats)):\n    lr_bag_scores = cross_val_score(lr_bag_reg, X, y, groups=groups,\n                                scoring='neg_mean_squared_error', cv=grouped_kfold)\n    rmse_scores = np.sqrt(-lr_bag_scores)\n    lr_bag_rmse_scores.append(rmse_scores)\n    mean_lr_bag_rmse_scores.append(rmse_scores.mean())\n\nlr_bag_rmse_scores = np.array(lr_bag_rmse_scores)\nmean_lr_bag_rmse_scores = np.array(mean_lr_bag_rmse_scores)\n\nprint(f\"Lin Reg Bagging RMSE: {mean_lr_bag_rmse_scores.mean():.5f} +/- {mean_lr_bag_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.5 Exploration of tree based models:\n\nThe performance of our random forest and extra tree regressors were not actually that bad. With refinement, these could become good models to include within a final ensemble, and therefore its worth trying to optimise the performance if we can.\n\nIn order to do this, we'll take an even smaller stratified sample of training instances based on profile ID (due to higher time complexity with these methods), followed by unique grouping between training / validation of profile IDs, just like we did above. This will allow us to experiment and fine-tune on a much smaller subsample, which still has the benefit of having different profile IDs between our training and validation splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trg_sub, _, y_trg_sub, _ = train_test_split(X_std, y, stratify=groups, test_size=0.99, random_state=12)\ntrg_profiles = X_trg_sub.loc[:, 'profile_id'].values\n\nprint(f\"Number of profile IDs within sample: {X_trg_sub['profile_id'].value_counts().shape[0]}\\n\")\n\n# drop profile ID before training\nX_trg_sub = X_trg_sub.drop('profile_id', axis=1)\n\nprint(f\"Shapes of data: X = {X_trg_sub.shape}, y = {y_trg_sub.shape}\")\n\nsample_group_kfold = GroupShuffleSplit(n_splits=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good - our sample still retains instances from all 42 profile IDs after our stratified sampling technique. We can now perform grouped K-Folds on this in a similar way to above."},{"metadata":{},"cell_type":"markdown","source":"#### Exploration of Extra Trees Hyper-Parameters\n\nBelow we'll define a range of parameters to use with grid search for finding optimal hyper-parameters for our Extra Trees Regressor. Note - we'll not bother using the n_estimators parameter, since we know a larger number of estimators helps to reduce overfitting, so for the final model we'll use a large number of n_estimators. By using this during grid search we'll just increase the search time considerably for relatively little insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"et_param_grid = {'n_estimators' : [10], 'max_depth' : [5, 10, 20, 60], \n                 'min_samples_split' : [2, 5, 10, 20], 'min_samples_leaf' : [1, 3, 5, 10], \n                 'n_jobs': [-1], 'bootstrap' : [True]}\n\net_reg = ExtraTreesRegressor()\net_grid_search = GridSearchCV(et_reg, et_param_grid, cv=sample_group_kfold, scoring='neg_mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit to our grid search with grouped kfold\n%time et_grid_search.fit(X_trg_sub, y_trg_sub, groups=trg_profiles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display best parameters found during grid search\nbest_et_params = et_grid_search.best_params_\nbest_et_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change best params n_estimators to a higher a more appropriate value\nbest_et_params['n_estimators'] = 600\nbest_et_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameters found were: \n{'bootstrap': True,\n 'max_depth': 20,\n 'min_samples_leaf': 3,\n 'min_samples_split': 2,\n 'n_jobs': -1}"},{"metadata":{},"cell_type":"markdown","source":"Let's perform cross-validation again using this model and the optimal hyper-parameters, along with more estimators:"},{"metadata":{"trusted":true},"cell_type":"code","source":"et_reg = ExtraTreesRegressor(**best_et_params)\n\net_scores = cross_val_score(et_reg, X_trg_sub, y_trg_sub, groups=trg_profiles, \n                            scoring='neg_mean_squared_error', cv=sample_group_kfold)\n\net_rmse_scores = np.sqrt(-et_scores)\nprint(f\"Extra Trees Reg RMSE: {et_rmse_scores.mean():.5f} +/- {et_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of this model is not bad now! If we trained this on all of the training set, I expect this performance would be improved even further, especially when considering this model has only been trained on less than 1% of the total training set!\n\nIf we perform the same with Random Forest, the optimal parameters found are similar, although performance is slightly less than that found with the Extra Trees Regressor; Clearly the extra bias provided by Extra Trees lets it perform better to our data than that with a Random Forest.\n\nEither of these tree based models will be ideal to use in a final ensemble model for our regression problem. Extra Trees is preferable, since its much faster to train, and better performing in this case."},{"metadata":{},"cell_type":"markdown","source":"**Exploring the performance of a Gradient Boosting Regressor:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_reg = MultiOutputRegressor(GradientBoostingRegressor())\n\ngb_scores = cross_val_score(gb_reg, X_trg_sub, y_trg_sub, groups=trg_profiles, \n                            scoring='neg_mean_squared_error', cv=sample_group_kfold)\n\ngb_rmse_scores = np.sqrt(-gb_scores)\nprint(f\"Gradient Boosting Reg RMSE: {gb_rmse_scores.mean():.5f} +/- {gb_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly our gradient boosting regressor has high potential here, and could likely be trained to have better performance than our linear regression OLS model. The problem with this method however is the extremely long training time on the entire dataset, which could become problematic.\n\nIf we performed hyper-parameter optimisation on this, I have no doubt that it would be the highest performing model so far on our data."},{"metadata":{},"cell_type":"markdown","source":"### 9.6 Production of a stacking ensemble to combine the predictions of multiple models:\n\nA good strategy is to combine the results of multiple models together, which often results in a better overall performance than either of the standalone models produce. This assumes that each model chosen is relatively good in terms of performance, and the more diversity we have between each model used in the ensemble, the better the chance of our performance being better for a given task.\n\nJust to illustrate this concept, and produce a reasonable model, lets combine the benefits of a tree based model, Extra Trees Regressor, along with our multiple-output Linear Regression OLS mode (with no regularisation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"regressors = [('Bagged Linear Reg', BaggingRegressor(LinearRegression(), n_estimators=20, bootstrap=True)), \n              ('Extra Trees Reg', ExtraTreesRegressor(**best_et_params))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_reg = MultiOutputRegressor(VotingRegressor(estimators=regressors, n_jobs=-1))\n\nensemble_scores = cross_val_score(ensemble_reg, X_trg_sub, y_trg_sub, groups=trg_profiles, \n                            scoring='neg_mean_squared_error', cv=sample_group_kfold)\n\nensemble_rmse_scores = np.sqrt(-ensemble_scores)\nprint(f\"Ensemble Reg RMSE: {ensemble_rmse_scores.mean():.5f} +/- {ensemble_rmse_scores.std():.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, our ensemble is actually better performing than either model alone! This should suffice for a final submission and evaluation on the test set."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"final-test-set-predictions\"></a>\n## 10. Producing our models and making predictions on the final test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_preprocessor = PMSMDataProcessor()\n\ntrain_df_extra = data_preprocessor.fit_transform(train_df)\ntest_df_extra = data_preprocessor.transform(test_df)\n\ny = train_df_extra[data_preprocessor.output_vars].copy()\nX = train_df_extra.drop(columns=data_preprocessor.output_vars)\n\n# select profile id's as groups to split our folds on\ngroups = X.loc[:, 'profile_id'].copy()\nX.drop('profile_id', axis=1, inplace=True)\n\nX.shape, y.shape, test_df_extra.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX_std = pd.DataFrame()\nX_std[X.columns] = pd.DataFrame(std_scaler.fit_transform(X))\ntest_std = std_scaler.transform(test_df_extra)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et_opt_params = {'n_estimators' : 600, 'bootstrap': True, 'max_depth': 20, \n                 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_jobs': -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train final extra trees classifier\net_reg = ExtraTreesRegressor(**et_opt_params)\n%time et_reg.fit(X_std, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train final extra trees classifier\nbag_lr_reg = BaggingRegressor(LinearRegression(), n_estimators=20, bootstrap=True)\n%time bag_lr_reg.fit(X_std, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et_test_preds = et_reg.predict(test_std)\nprint(f\"Final Extra Trees RMSE on Test Set: {np.sqrt(mean_squared_error(et_test_preds, test_labels)):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_test_preds = bag_lr_reg.predict(test_std)\nprint(f\"Final Linear Regression RMSE on Test Set: {np.sqrt(mean_squared_error(lr_test_preds, test_labels)):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With both sets of predictions obtained from these different models, we can simply combine them into a final merged dataset by averaging their results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_preds = (et_test_preds + lr_test_preds) / 2.0\nprint(f\"Final Ensemble RMSE on Test Set: {np.sqrt(mean_squared_error(ensemble_preds, test_labels)):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an ensemble and compare final performance - we should expect a marginal improvement\nregressors = [('Bagged Linear Reg', BaggingRegressor(LinearRegression(), n_estimators=20, bootstrap=True)), \n              ('Extra Trees Reg', ExtraTreesRegressor(**et_opt_params))]\n\n# craete our final ensemble model\nensemble_reg = MultiOutputRegressor(VotingRegressor(estimators=regressors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Training final ensemble model on entire training set.\\n\")\n#ensemble_reg.fit(X_std, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_preds = ensemble_reg.predict(test_std)\n#print(f\"Final Ensemble RMSE on Test Set: {np.sqrt(mean_squared_error(test_preds, test_labels)):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Closing Remarks\n\nSo there we have it - a final prediction on our test set, which was completely unseen by the trained models due to the unique Profile IDs assigned to the test at the very beginning of this notebook. Our performance was pretty strong, especially when considering the relative simplicity of the models that we used in our final product.\n\nWe could definitely improve this model further, which could be done by one or more of the following strategies:\n\n- Production of a larger ensemble with more diversity.\n\n- Use of better models, such as gradient boosting regressors to produce our final model. This wasn't included within this notebook due to the significant training time of a gradient boosting model on the huge dataset provided. However, the basic concepts remain the same and this could easily be added into the above analysis.\n\n- Use of a neural network implementation with a better capacity to learn the complex relationships within our data.\n\n- Better exploitation of time-series within our data, which could be done by the use of models such as RNN LSTMs or CNNs to learn time-dependent patterns within our data. It's expected that if we did this well, it would result in the best overall performance when compared to any other classical model implementation. The downside of this is the inference time could be greater, and the training time could be significant due to model complexity.\n\nI hope you enjoyed this notebook - many thanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}