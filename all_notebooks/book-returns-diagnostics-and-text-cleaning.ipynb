{"cells":[{"metadata":{},"cell_type":"markdown","source":"***Overview***\n\nThe objective of this notebook was to investigate whether there are particular types of books and cities that have a high rate of books being returned.\n\nI have focused on cleaning the text representing book names and city names, and removing the anomalies, with the help of fuzzy string matching, regular expressions, string handling and data wrangling. \n\nAfter cleaning and merging the book and city names, exploratory data analysis was performed on the book return rate using bar plots and ecdf plots. The cleaned books and city names with highest orders were also visualized. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom fuzzywuzzy import fuzz, process\nimport tqdm.notebook as tq\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders = pd.read_csv('../input/gufhtugu-publications-dataset-challenge/GP Orders - 5.csv',encoding=\"UTF-8-SIG\")\norders.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Breaking down orders of multiple books into multiple orders, dropping columns that are not of interest and converting book and city names into uppercase strings."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['Book_O'] = orders['Book Name'].str.split('/')\norders = orders.explode('Book_O')\norders['Book_O'] = orders['Book_O'].apply(str)\norders['City_O'] = orders['City'].apply(str)\norders['Book_O'] = orders['Book_O'].str.upper()\norders['City_O'] = orders['City_O'].str.upper()\norders.drop([ 'Book Name','Order Date & Time','Order Number','Payment Method','Total items','Total weight (grams)'], axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(orders['Order Status'].value_counts())\norders = orders[ orders['Order Status'] != 'Cancelled' ].reset_index(drop=True)\nreturned = orders[ orders['Order Status']=='Returned'].reset_index(drop=True)\ntotalBooksNo = orders.shape[0]\nreturnedBooksNo = returned.shape[0]\nprint('Total Non-Cancelled Book Orders : ',totalBooksNo)\nprint('Returned Book Orders : ',returnedBooksNo)\nprint('Returned Books Percentage : ',round(returnedBooksNo*100/totalBooksNo,2),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origNoBooks = orders['Book_O'].nunique()\norigNoCities = orders['City_O'].nunique()\nprint('No of Unique Book Names : ',origNoBooks)\nprint('No of Unique City Names : ',origNoCities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def listSortPrint(lst,n=200):\n    lst.sort()\n    print(lst[:n])\n    print('_______________')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueCities = list(orders['City_O'].unique())\nprint('First 200 Unique Cities')\nlistSortPrint(uniqueCities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transliteration means converting alphabets from one language to another. I am using it for converting Urdu text into Roman Urdu for fuzzy matching and merging of city names and for finding Urdu words in book and city names. \n\nThe table for mapping is originally by Ahmed, T. (2009). \"Roman to Urdu Transliteration using word list\"-Conference of Language and Technology. The dictionary was written by Shan Khan in 2019."},{"metadata":{"trusted":true},"cell_type":"code","source":"def transliterate(string):\n    buck2uni = {\n            u\"\\u0627\":\"A\",\n            u\"\\u0627\":\"A\", \n            u\"\\u0675\":\"A\", \n            u\"\\u0673\":\"A\", \n            u\"\\u0630\":\"A\", \n            u\"\\u0622\":\"AA\", \n            u\"\\u0628\":\"B\", \n            u\"\\u067E\":\"P\", \n            u\"\\u062A\":\"T\", \n            u\"\\u0637\":\"T\", \n            u\"\\u0679\":\"T\", \n            u\"\\u062C\":\"J\", \n            u\"\\u0633\":\"S\", \n            u\"\\u062B\":\"S\", \n            u\"\\u0635\":\"S\", \n            u\"\\u0686\":\"CH\", \n            u\"\\u062D\":\"H\", \n            u\"\\u0647\":\"H\", \n            u\"\\u0629\":\"H\", \n            u\"\\u06DF\":\"H\", \n            u\"\\u062E\":\"KH\", \n            u\"\\u062F\":\"D\", \n            u\"\\u0688\":\"D\", \n            u\"\\u0630\":\"Z\", \n            u\"\\u0632\":\"Z\", \n            u\"\\u0636\":\"Z\", \n            u\"\\u0638\":\"Z\", \n            u\"\\u068E\":\"Z\", \n            u\"\\u0631\":\"R\", \n            u\"\\u0691\":\"R\", \n            u\"\\u0634\":\"SH\", \n            u\"\\u063A\":\"GH\", \n            u\"\\u0641\":\"F\", \n            u\"\\u06A9\":\"K\", \n            u\"\\u0642\":\"K\", \n            u\"\\u06AF\":\"G\", \n            u\"\\u0644\":\"L\", \n            u\"\\u0645\":\"M\", \n            u\"\\u0646\":\"N\", \n            u\"\\u06BA\":\"N\", \n            u\"\\u0648\":\"O\", \n            u\"\\u0649\":\"Y\", \n            u\"\\u0626\":\"Y\", \n            u\"\\u06CC\":\"Y\", \n\n            u\"\\u06D2\":\"E\", \n            u\"\\u06C1\":\"H\",\n            u\"\\u064A\":\"E\"  ,\n            u\"\\u06C2\":\"AH\"  ,\n            u\"\\u06BE\":\"H\"  ,\n            u\"\\u0639\":\"A\"  ,\n            u\"\\u0643\":\"K\" ,\n            u\"\\u0621\":\"A\",\n            u\"\\u0624\":\"O\",\n            u\"\\u060C\":\"\" #seperator ulta comma\n    }\n\n    for k, v in buck2uni.items():\n        string = string.replace(k, v)\n\n    return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['City'] = orders['City_O'].apply(transliterate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finding 3 letter city names for help in checking for city abbreviations being used instead of full city names."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueCities = list(orders['City'].unique())\nuniqueCities.sort()\nabbrevs = list(filter(lambda x: (len(x) == 3), uniqueCities))\nprint(abbrevs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After analyzing the unique city names it was found that there are many anomalies in the city names. Apart from spelling mistakes they often include words like CITY, DISTRICT and province names. Sometimes the address is included and city name abbreviations are used.\n\nI wanted to use a single name string for all orders from a particular city, to get accurate counts. The first step for that is to replace or remove those strings. I made a dictionary to replace the substrings in the column. Some full strings are also being replaced as their abbreviation is substring in other names. "},{"metadata":{"trusted":true},"cell_type":"code","source":"replD_subs = {'CITY':'','DISTRICT':'','DISST':'','DIST':'','TEHSEEL':'','TEHSIL':'',' TEH ':'','ZLA':'',\n              'VILLAGE':'','PUNJAB':'','SINDH':'','BALOCHISTAN':'','KPK':'','KHYBER PAKHTUNKHWA':'', 'PUR EAST':'PUR SHQI',\n              'CANNT':'','CANTT':'','SIND':'','PAKISTAN':'','CENTRAL':'','NORTH/':'','EAST':'','SOUTH/':'','WEST':'',\n              'BWN':'BAHAWAL NAGAR','BWP':'BAHAWAL PUR ','LHR':'LAHORE','BAHAWAL PUR':'BAHAWAL PUR  ',\n              'MZH':'MUZAFFAR GARH','FSD':'FAISALABAD','DGK':'DERA GHAZI',\n              'ISB':'ISLAMABAD',' AND ':''}\nfor k,v in tq.tqdm(replD_subs.items()):\n    orders['City'] = orders['City'].str.replace(k,v)\norders['City'].replace('KHI','KARACHI',inplace=True)\norders['City'].replace('HYD','HYDERABAD',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function is being applied to remove the numbers and whitespaces in start of string before the city name. It uses regex to search for A-Z characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def trimStart(ct):\n    res = None\n    temp = re.search(r'[A-Z]',ct,re.I) \n    if temp is not None: \n        res = temp.start()\n    else:\n        res = 0\n    if len(ct)>0: \n        return ct[res:]\n    else:\n        return ct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['City'] = orders['City'].apply(trimStart)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sometimes there are additional details in the city and book names. In city names there are district and province names and address details. Removing the string part after a particular length helps in merging similar city names. In books there can be additional details like specifying that it was free book etc, but those are rare and book names were often long, so I decided to do it only for cities."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['City'] = orders['City'].str[:14]\norders['Book'] = orders['Book_O'].str[:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correcNoCities = orders['City'].nunique()\nprint('No of Original Unique Cities : ',origNoCities)\nprint('No of Corrected Unique Cities : ',correcNoCities)\nprint( round(100-(correcNoCities*100/origNoCities),2),'% Reduction so far')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some cities with lot of addresses and extra details in the strings, and they are also important cities with high sales. So additional cleaning is being done to replace the whole string if the name is found in the string. More cities can also be cleaned like that. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanLongStr(stri):\n    ctLst = ['LAHORE','KARACHI','SIALKOT','MULTAN','ISLAMABAD']\n    for ct in ctLst:\n        if ct in stri:\n            return ct\n    return stri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['City'] = orders['City'].apply(cleanLongStr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correcNoCities = orders['City'].nunique()\nprint('No of Original Unique Cities : ',origNoCities)\nprint('No of Corrected Unique Cities : ',correcNoCities)\nprint( round(100-(correcNoCities*100/origNoCities),2),'% Reduction so far')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.sort_values(by=['City'],ascending=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function uses Fuzzy String Matching with Levenshtein Similarity to replace all similar strings in a dataframe column by the first occurence of a string in the same group.\n\nThe code for main idea was posted by Alperen on StackOverflow. However that solution had quadratic time complexity and took around 40 minutes to run on our data. I have modified and made it much faster by sorting the names and finding the start and end index of names starting with same letter, and then matching only with strings in the corresponding bins instead of all strings. This requires first letter to be same, which actually helps in making better matches in our case.\n\nThe time was reduced from 40 min to 2 min 24 sec. The algorithm still has O(n²) time complexity because each string is compared with strings in a bin, the number of which still depends on the number of strings. However the time is reduced by a factor close to number of alphabets, like the 17X reduction in time that I got."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fuzzyReplace(df,colName,thresh):\n    \n    strLst = list(df[colName])\n    strLst.sort()\n    indxD = {}\n    lastS = '*'\n    indxD[lastS]=[0]\n    for i,stri in enumerate(strLst):\n        if len(stri) > 0:\n            curS = stri[0]\n            if curS != lastS:\n                indxD[curS] = [i]\n                indxD[lastS].append(i)\n                lastS = curS\n    indxD[lastS].append(len(strLst))\n    \n    for i in tq.tqdm(range(len(strLst))):\n        if len(strLst[i]) > 0:\n            startLtr = strLst[i][0]\n            for j in range( indxD[startLtr][0], indxD[startLtr][1] ):\n                if i < j and fuzz.ratio(strLst[i], strLst[j]) >= thresh:\n                    strLst[j] = strLst[i]\n                    \n    return strLst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['City'] = fuzzyReplace(orders,'City',73)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of unique cities was reduced to nearly half by text cleaning and merging of city names "},{"metadata":{"trusted":true},"cell_type":"code","source":"correcNoCities = orders['City'].nunique()\nprint('No of Original Unique Cities : ',origNoCities)\nprint('No of Corrected Unique Cities : ',correcNoCities)\nprint( round(100-(correcNoCities*100/origNoCities),2),'% Reduction after Fuzzy Merging')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueCities = list(orders['City'].unique())\nprint('First 200 Unique Cities')\nlistSortPrint(uniqueCities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.sort_values(by=['Book'],ascending=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Book names are relatively clean with much less anomalies. However several types of issues were discovered after analyzing the unique book names. They include following:\n1. Some books with Urdu names have orders with their name in Urdu text as well as orders with the name in Roman Urdu.\n2. Some book names have both Urdu text and Roman Urdu text, while the separate Urdu and Roman Urdu versions exist too.\n3. Some book names have extra details about the books in some versions, often in parenthesis.\n4. Multiple versions of same books exist due to spelling mistakes too.\n\nThe names of the books are often very similar and sometimes they have parts. The number of book names with anomalies was relatively small and the chances for mistake were significant, so I decided to manually make a list of the book names to replace and merge, as it will be more reliable. I am utilizing functions for detection of book names likely to have anamolies for aiding me in the process of manually finding the names, as a semi-automated human-in-loop approach. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def hasEngUrdBoth(stri):\n    return transliterate(stri) != stri and re.search(r'[A-Z]',stri,re.I) is not None\n\ndef hasUrduOnly(stri):\n    return transliterate(stri) != stri and re.search(r'[A-Z]',stri,re.I) is None\n\ndef hasEnglish(stri):\n    return re.search(r'[A-Z]',stri,re.I) is not None\n\ndef hasParenthesis(stri):\n    return '(' in stri or ')' in stri","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that I'm only printing the top 200 now because the output becomes too big to show. I printed all during my analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('First 200 Book Names')\nlistSortPrint(list(orders['Book_O'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Book Titles with English and Urdu words')\nmixedBooks = list(orders[ orders['Book'].apply(hasEngUrdBoth) == True ]['Book'].unique())\nlistSortPrint(mixedBooks)\nprint('Book Titles with Parenthesis')\nlistSortPrint(list(orders[ orders['Book'].apply(hasParenthesis) == True ]['Book'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Book Titles in Urdu')\nurduBooks = list(orders[ orders['Book'].apply(hasUrduOnly) == True ]['Book'].unique())\nlistSortPrint(urduBooks)\nurduBooksTL = [ transliterate(bk) for bk in urduBooks ]\nprint('Book Titles in Urdu - Transliterated')\nlistSortPrint(urduBooksTL)\nenglishBooks = list(orders[ orders['Book'].apply(hasEnglish) == True ]['Book'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,bk in enumerate(urduBooksTL):\n    bestMatchesE = process.extract(bk,englishBooks,limit=3)\n    bestMatchesU = process.extract(urduBooks[i],mixedBooks,limit=3)\n    bestMatchesE = [ tpl for tpl in bestMatchesE if tpl[1] >= 80]\n    bestMatchesU = [ tpl for tpl in bestMatchesU  if tpl[1] >= 70]\n    \n    if len(bestMatchesE)>0 or len(bestMatchesU)>0:\n        print(i+1,' : ',urduBooks[i])\n        if len(bestMatchesE)>0:\n            print(bestMatchesE)\n        if len(bestMatchesU)>0:\n            print(bestMatchesU)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replD_subs = { 'ڈیٹا سائنس':'DATA SCIENCE','مشین لرننگ':'MACHINE LEARNING',\n              'ارفع کریم':'ARFA KARIM','ڈیٹا سائنس ۔ ایک تعارف':'DATA SCIENCE',\n              'ارطغرل غازی':'ERTUGRUL GHAZI','MOLO MASALI - مولو مصلی':'MOLO MASALI',\n              'SHAOOR شعور۔ علم سے آگہی کا سفر':'SHAOOR','SAFAR E HAJJ سفر حج':'SAFAR E HAJJ',\n              'JAVA  جاوا 2':'JAVA 2','(C++) ++سی':'(C++)','R KA TAARUF  آر کا تعارف':'R KA TAARUF',\n              'JUSTUJU KA SAFAR جستجو کا سفر':'JUSTUJU KA SAFAR (URDU)','JUSTJU KA SAFAR-1':'JUSTUJU KA SAFAR (URDU)',\n              'LINUX - AN INTRODUCTION  (RELEASE DATA - OCTOBER 3, 2020)':'LINUX - AN INTRODUCTION',\n              'ادھورے گناہ':'ADHORAY GUNNAH','JAN KA KHAMOSH ZAYAN - HIGH BLOOD PRESSURE' : 'JAN KA KHAMOSH ZAYAN',\n              'KULYAT MAKATEEB E IQBAL (4 VOLUMES COMPLETE)':'KULLYAT MAKATEEB-E-IQBAL (4 VOLUMES)','انٹرنیٹ سے پیسہ کمائیں؟- مستحقین زکواة':'انٹرنیٹ سے پیسہ کمائیں',\n              'IRTEQA SHAHEEN - ارتقاء شاہین':'IRTEQA SHAHEEN','HAR SHAYE KA NAZRIA - ہر شے کا نظریہ':'HAR SHAYE KA NAZRIA',\n              'BITCOIN BLOCKCHAIN AUR CRYPTO CURRENCY - FREE E-BOOK':'BIT COIN BLOCK CHAIN AUR CRYPTO CURRENCY',\n              'BIT COIN BLOCK CHAIN AUR CRYPTO CURRENCY بٹ کوائن، بلاک چین اور کرپٹو کرنسی':'BIT COIN BLOCK CHAIN AUR CRYPTO CURRENCY',\n              'HAZIR GHAYAB حاضر غائب':'HAZIR GHAYAB'\n             }\nfor k,v in tq.tqdm(replD_subs.items()):\n    orders['Book'].replace(k,v,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('First 200 Book Names after merging')\nlistSortPrint(list(orders['Book'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many book names were merged and fixed resulting in 1.3 % reduction, which will make the counts more accurate. However it is much less compared to city names, because book names are cleaner."},{"metadata":{"trusted":true},"cell_type":"code","source":"correcNoBooks = orders['Book'].nunique()\ncorrecNoCities = orders['City'].nunique()\nprint('No of Original Unique Books: ',origNoBooks)\nprint('No of Corrected Unique Books: ',correcNoBooks)\nprint( round(100-(correcNoBooks*100/origNoBooks),2),'% Reduction')\nprint('_'*10)\nprint('No of Original Unique Cities : ',origNoCities)\nprint('No of Corrected Unique Cities : ',correcNoCities)\nprint( round(100-(correcNoCities*100/origNoCities),2),'% Reduction')\nprint('_'*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Currently the City column contains mispelled City names because the first name in group is being used. It will be later replaced by joins."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.sample(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function is being used to generate a dataframes with the return percentage, completed, returned and total books. Return percentage is the percentage of a book being returned among the total uncancelled book orders."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getOrderStatusInfo(colName,orders,minObs=20):\n    col_status = orders.groupby([colName, \"Order Status\"])[\"Order Status\"].count().unstack().fillna(0).reset_index()\n    col_status['Total'] = col_status[ ['Completed','Returned'] ].sum(axis=1)\n    col_status['Return_Percentage'] = col_status['Returned'] / col_status['Total'] * 100\n    col_status.sort_values(by=['Return_Percentage'],ascending=False,inplace=True)\n    col_status = col_status[ (col_status['Total'] >= minObs) & (col_status['Returned'] > 0) ].reset_index(drop=True)\n    return col_status","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am also using a minimum number of observations when finding books without high return percentage,so we could find books and cities with some data to support that they have high likelihood of being returned. However I didn't keep it higher because there are many books and locations don't have much orders but shouldn't be excluded."},{"metadata":{"trusted":true},"cell_type":"code","source":"minObsB = 10\nminObsC = 5\nevidentReturnedBooks = getOrderStatusInfo('Book',orders,minObsB)\nevidentReturnedCities = getOrderStatusInfo('City',orders,minObsC)\nprint('No of Returned Book Titles with at least '+str(minObsB)+' Orders : ',evidentReturnedBooks.shape[0])\nprint('No of Returned Cities with at least '+str(minObsC)+' Orders : ',evidentReturnedCities.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evidentReturnedCities = pd.merge(evidentReturnedCities,orders[['City_O','City']].drop_duplicates(),\\\n                                 how='left',left_on='City', right_on='City').drop_duplicates(subset=['City'])\n\nevidentReturnedBooks = pd.merge(evidentReturnedBooks,orders[['Book_O','Book']].drop_duplicates(),\\\n                                 how='left',left_on='Book', right_on='Book').drop_duplicates(subset=['Book'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After joining, the Book_O column has the original book names, and the Book column has names after merging different versions of same book names.\n\nThe sorted return percentage of books along with their total orders can be observed here."},{"metadata":{"trusted":true},"cell_type":"code","source":"evidentReturnedBooks[:30]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After joining, the City_O column has the original city names, and the City column has names after merging different versions of same city names, which can have wrong spelling because they are random names from group of similar city names.\n\nThe sorted return percentage of cities along with their total orders can be observed here."},{"metadata":{"trusted":true},"cell_type":"code","source":"evidentReturnedCities[:40]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def barPlot(df,topN,catCol,valCol,title,units=''):\n    fig, ax = plt.subplots(figsize =(16, 16)) \n    ax.barh(df[catCol].head(topN),df[valCol].head(topN)) \n    ax.invert_yaxis()\n    for i in ax.patches: \n        plt.text(i.get_width()+0.25, i.get_y()+0.5,str(round((i.get_width()), 2))+units,\\\n                 fontsize = 10, fontweight ='bold', color ='grey') \n    ax.set_title(title, loc ='left') \n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The bar chart of Top 30 books according to return percentage and having at least 10 orders is being plotted.\n\nIt can be observed that there are many books with fairly high return percentage. There are some interesting observations. More than half of the people returned \"آپ کیسے کروڑ پتی بن سکتے ہیں\", a book about how to become rich. There could be other reasons for book returns, however I suspect that people had rather high hopes of riches and invested money for that but became disappointed with the content and wanted their money back. The other books with at least 30% return rate include two dark and sad books, which are translations and might have been too depressing for the readers, and then there is the famous \"A brief history of time\" by Stephen Hawking which some might have bought due to populariry but felt that it's a bit difficult and uninteresting for them.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"topN = 30\nbarPlot(evidentReturnedBooks,topN,'Book','Return_Percentage','Top '+str(topN)+' Return Percentages of Books',' %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bar chart of Top 30 City names or locations according to return percentage and having at least 5 orders is being plotted.\n\nIt can be observed that there are many cities/locations with high return percentage. \nThere are some interesting observations. There are some location, which seem rare like foreign cities, small towns and even particular addresses, having very high return percentage like 100%. I might have some confirmation bias but I suspect that those orders can be from the same person or people with some connection that are exploiting the return policy by ordering and returning again.\n\nThe small towns often have high return percentage, which makes sense considering that they might find the books more expensive. The orders are not too much from them so some of that might be by chance, however similar trend was observed for much higher minimum orders requirement."},{"metadata":{"trusted":true},"cell_type":"code","source":"topN = 30\nbarPlot(evidentReturnedCities,topN,'City_O','Return_Percentage','Top '+str(topN)+' Return Percentages of Cities',' %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ECDF plot for return percentages of those books show that around 75% books have return percentage under 13%, while the top 25% are spread between around 18% to 53.3%, with some big jumps represented by the flat portions. The median is around 8%.\n\nThis indicates that there is a considerable amount of books with unusually high return percentages, which maybe more likely to be returned due to their content, and some of them are outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.ecdfplot(data=evidentReturnedBooks, x=\"Return_Percentage\").set_title('ECDF for Return Percentages of Books')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ECDF plot for return percentages of those cities/locations show that nearly % books are have return percentage under 20%, while the top 20% are spread between around 22% to 100%, with big jumps represented by the flat portions. The median is around 10%.\nThis indicates that there is a considerable amount of cities/locations with unusually high return percentages, which maybe more likely to return. There are several locations with 100% return percentage, indicated by the vertical jump at the end.\n\nIt was a bit unexpected that a significant amount of particular cities/locations would have higher than usual and so high return percentage. My intuition was that books would appear to have more influence on return percentage, as compared to cities. The reason for that should be investigated and checked for possible exploitation of return policy by certain customers and demographic.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.ecdfplot(data=evidentReturnedCities, x=\"Return_Percentage\").set_title('ECDF for Return Percentages of Cities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cleaned and merged book and cities names are also being sorted according to total number of orders to get accurate estimate of top selling books and cities with highest orders."},{"metadata":{"trusted":true},"cell_type":"code","source":"evidentReturnedBooks.sort_values(by=['Total'],ascending=False,inplace=True)\nevidentReturnedCities.sort_values(by=['Total'],ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the top selling books are technical and other books by Zeeshan Usmani, with the book on how to earn from internet taking the top spot and significantly higher sales than all other books, probably because it is about something that a lot of people are interested in."},{"metadata":{"trusted":true},"cell_type":"code","source":"topN = 10\nbarPlot(evidentReturnedBooks,topN,'Book','Total','Top '+str(topN)+' Books by Total Orders')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can seen the most orders are from the big cities with most population, which makes sense. Islamabad has less population compared to the few below it but people there seem to be more educated and fond of books and learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"topN = 10\nbarPlot(evidentReturnedCities,topN,'City_O','Total','Top '+str(topN)+' Cities by Total Orders')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is potential for more analytics and improvements. Some possible things that can be tried include following:\n* The books can be divided into categories to see analytics for different types of books. Scrapping the Guftagu website for that might also be a good idea.\n* The cities and locations can also be divided into categories to see analytics for different types of areas. Some data of Pakistani cities can be found on Kaggle.\n* The city/location merging can be improved with some manual replacement.\n* Some hypothesis testing might be performed to verify the insights being indicated. \n* Some more data wrangling and exploration can be done to investigate things and find possible reasons.\n* Features other than books and cities can also be explored."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}