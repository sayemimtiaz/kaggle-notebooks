{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Table Of Content</h3>\n\n\n* [1. Import Libraries and Data Loading](#1)\n* [2. Data Preprocessing](#2)\n* [3. Feature Engineering](#3)\n* [4. Exploratory Data Analysis](#4)\n    * [4.1 Time Based Analysis](#4.1)\n    * [4.2 Synopsis Based Analysis](#4.2)\n* [5. Generating Synopses](#5)\n    * [5.1 Training a LSTM Network](#5.1)\n    * [5.2 Generated Synopses](#5.2)\n    \n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities</h3>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nfrom keras.layers import Dense,LSTM,Input,Dropout,SimpleRNN\nfrom keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import one_hot\nfrom tqdm.notebook import tqdm\nimport gc\nplt.rc('figure',figsize=(17,13))\nsns.set_context('paper',font_scale=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anime_df = pd.read_csv('/kaggle/input/top-10000-anime-movies-ovas-and-tvshows/Anime_Top10000.csv')\nanime_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing</h3>\n","metadata":{}},{"cell_type":"code","source":"\n# Remove all the special characters\nanime_df.Anime_Name\t             = anime_df.Anime_Name.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \nanime_df.Synopsis\t             = anime_df.Synopsis.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \n\n# Substituting multiple spaces with single space \nanime_df.Anime_Name              = anime_df.Anime_Name.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\nanime_df.Synopsis                = anime_df.Synopsis.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\n\n# Converting to Lowercase \nanime_df.Anime_Name              = anime_df.Anime_Name.str.lower() \nanime_df.Synopsis                = anime_df.Synopsis.str.lower() \n\n#Synopsis Sentiment Analysis\nsid = SIA()\nanime_df['sentiments']           = anime_df['Synopsis'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nanime_df['Positive Sentiment']   = anime_df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nanime_df['Neutral Sentiment']    = anime_df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nanime_df['Negative Sentiment']   = anime_df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nanime_df.drop(columns=['sentiments'],inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Feature Engineering</h3>\n","metadata":{}},{"cell_type":"code","source":"#Only TV-Shows\ntv_df                                                               = anime_df[anime_df.Anime_Episodes.str.contains('TV')].copy()\n\n#Extract Number Of Episodes\ntv_df['#_Episodes']                                                 = tv_df.Anime_Episodes.apply(lambda x: ''.join(re.findall(r'[0-9]*', x)))\ntv_df['#_Episodes']                                                 = tv_df['#_Episodes'].replace('',np.nan)\ntv_df.loc[tv_df['#_Episodes'].notna(),'#_Episodes']                 = tv_df[tv_df['#_Episodes'].notna()]['#_Episodes'].astype(np.int32)\n#Extract Air Years\ntv_df['Air_Years']                                                  = tv_df.Anime_Air_Years.apply(lambda x: ''.join(re.findall(r'[0-9 -]*', x)))\n\n#Start Year\ntv_df['Start_Year']                                                 = tv_df.Air_Years.apply(lambda x: x.split('-')[0].strip())\ntv_df['Start_Year']                                                 = tv_df['Start_Year'].astype(np.int32)\n#End Year\ntv_df['End_Year']                                                   = tv_df.Air_Years.apply(lambda x: x.split('-')[1].strip() if len(x.split('-')[1])>4 else 'Still Airing')\ntv_df.loc[(~tv_df['End_Year'].str.contains('Still')),'End_Year']    = tv_df[~tv_df['End_Year'].str.contains('Still')]['End_Year'].astype(np.int32)\n\ntv_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"plt.title('Distriubtion of Anime Ratings')\nsns.histplot(tv_df.Anime_Rating,kde=True,stat='probability',palette = cm.coolwarm(tv_df.Anime_Rating))\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: When looking at the distribution of rating in our sample of anime tv-shows we see that distribution is approximately normal and centered around a particular mean, but an interesting point to note is the multimodality, we have two modes around the mean which may indicate two underlying groups, in the probabilistic inference section we will further explore the two modes.","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =tv_df[['Anime_Rating','Start_Year','End_Year','#_Episodes','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =tv_df[['Anime_Rating','Start_Year','End_Year','#_Episodes','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: unfortunately, we see no significant correlations between our numeric features in both Pearson and Spearman correlation metrics.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"plt.title('Distriubtion Anime First Air Years')\nax = sns.barplot(x=tv_df.Start_Year.value_counts().sort_index().index,y=tv_df.Start_Year.value_counts().sort_index().values,palette=cm.coolwarm(tv_df.Start_Year.value_counts().sort_index().values))\nplt.xticks(rotation=-45,fontsize=11)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: Skimming the plot above, we can immediately observe that 2013-2019 were the years when the highest rating animes first aired.\nOlder animes may be rated lower in comparison to this interval, but it may be due to the trend of new animes being written and animated with the aim to the current viewer taste and trends where in the past, the opinion and taste of western countries had a lesser effect of anime writers as well as western countries not having a large industry of anime writes.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,6))\nplt.title('Distriubtion Anime First Air Years')\nax = sns.barplot(x=tv_df['#_Episodes'].value_counts().sort_index().index,\n                 y=tv_df['#_Episodes'].value_counts().sort_index().values,\n                 palette=cm.coolwarm(tv_df['#_Episodes'].value_counts().sort_index().values))\nplt.xticks(rotation=90,fontsize=10)\nplt.margins(x=0)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: In the plot above, we observe the distribution of the number of episodes in the anime TV show sample.\nInterestingly 12,13,24,25,26 episodes are the most common amount of episodes for an anime TV show.","metadata":{}},{"cell_type":"code","source":"year_mean_df = tv_df.groupby(by='Start_Year').mean()\n\nfig = go.Figure()\ntrace = go.Scatter(x=year_mean_df.index,y=year_mean_df.Anime_Rating,mode='lines+markers',name='Average Rating',line=dict(color='firebrick', width=4))\nRA = year_mean_df.Anime_Rating.rolling(5).mean()\nRS = year_mean_df.Anime_Rating.rolling(5).std()\n\nrunning_average = go.Scatter(x=RA.index,y=RA.values,mode='lines',name='Running Average',line_color='blue')\nrunning_average_postd = go.Scatter(x=RA.index,y=RA.values+RS.values,mode='lines',name='Running Average + 1 SD',line_color='green',line_dash='dot')\nrunning_average_mostd = go.Scatter(x=RA.index,y=RA.values-RS.values,mode='lines',name='Running Average - 1 SD',line_color='green',line_dash='dot', fill='tonexty')\n\nfig.add_trace(trace)\nfig.add_trace(running_average)\nfig.add_trace(running_average_postd)\nfig.add_trace(running_average_mostd)\n\nfig.update_layout(title='<b> Start Year Mean Anime Rating<b>',xaxis_title='<b>Year<b>',yaxis_title='<b>Average Rating<b>')\nfig.update_layout(hovermode=\"x unified\")\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: When looking at the average anime rating throughout our data frame's timeline, we see a rising average rating trend each year.\nAnother interesting observation is that the deviation in rating becomes narrower with time, meaning the population that rates the anime shows in our data becomes more consistent.\n\n","metadata":{}},{"cell_type":"code","source":"year_mean_df = tv_df.groupby(by='End_Year').mean()\n\nfig = go.Figure()\ntrace = go.Scatter(x=year_mean_df.index,y=year_mean_df.Anime_Rating,mode='lines+markers',name='Average Rating',line=dict(color='firebrick', width=4))\nRA = year_mean_df.Anime_Rating.rolling(5).mean()\nRS = year_mean_df.Anime_Rating.rolling(5).std()\n\nrunning_average = go.Scatter(x=RA.index,y=RA.values,mode='lines',name='Running Average',line_color='blue')\nrunning_average_postd = go.Scatter(x=RA.index,y=RA.values+RS.values,mode='lines',name='Running Average + 1 SD',line_color='green',line_dash='dot')\nrunning_average_mostd = go.Scatter(x=RA.index,y=RA.values-RS.values,mode='lines',name='Running Average - 1 SD',line_color='green',line_dash='dot', fill='tonexty')\n\nfig.add_trace(trace)\nfig.add_trace(running_average)\nfig.add_trace(running_average_postd)\nfig.add_trace(running_average_mostd)\n\nfig.update_layout(title='<b> End Year Mean Anime Rating<b>',xaxis_title='<b>Year<b>',yaxis_title='<b>Average Rating<b>')\nfig.update_layout(hovermode=\"x unified\")\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: Similarly to the average first-year rating, we see a climbing trend in the average rating, but in comparison to the first year, the end-year has a much lower deviation through time, i.e., constant variance nonconstant mean.\n","metadata":{}},{"cell_type":"code","source":"b_date_mean = tv_df.groupby(by='Start_Year').mean().reset_index()\nb_date_std = tv_df.groupby(by='Start_Year').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Yearly Average Positive Sentiment',  'Yearly Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig['layout']['xaxis2']['title'] = 'Start_Year'\nfig.update_layout(hovermode=\"x unified\")\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_date_mean = tv_df.groupby(by='Start_Year').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Yearly Deviation in Positive Sentiment',  'Yearly Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig['layout']['xaxis2']['title'] = 'Start_Year'\nfig.update_layout(hovermode=\"x unified\")\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Synopsis Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"f_data = tv_df.copy()\nplt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Lyrics',fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],label='Negative Sentiment',lw=2)\nsns.kdeplot(f_data['Neutral Sentiment'] ,label='Neutral Sentiment' ,color='orange' ,lw=2)\nsns.kdeplot(f_data['Positive Sentiment'],label='Positive Sentiment',color='tab:red',lw=2)\nplt.legend()\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Lyrics',fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],cumulative=True,label='Negative Sentiment',lw=2)\nsns.kdeplot(f_data['Neutral Sentiment'],cumulative=True,label='Neutral Sentiment' ,color='orange' ,lw=2)\nsns.kdeplot(f_data['Positive Sentiment'],cumulative=True ,label='Positive Sentiment',color='tab:red',lw=2)\nplt.xlabel('Sentiment Value')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Rating_Based = pd.cut(tv_df.Anime_Rating,10,labels=range(0,10)).to_frame()\nRating_Based['Range'],bins = pd.cut(tv_df.Anime_Rating,10,retbins=True)\nRating_Based['Syn'] = tv_df.Synopsis\n\nbins =[str(Rating_Based.query(f'Anime_Rating == {i}').Range.iloc[0]) for i in range(0,10)]\nSTP = list(STOPWORDS)\nSTP += ['written','mal','s','by','rewrite']\nfigure,axs = plt.subplots(2,5)\nsample = 9\n\nfor row in axs:\n    for ax in row:\n        ax.set_title('Rating Range +'+bins[sample],fontsize=13)\n        wc = WordCloud(background_color='white',width=200,height=200,stopwords=STP).generate(' '.join(Rating_Based.query(f'Anime_Rating == {sample}').Syn))\n        ax.imshow(wc)\n        sample-=1\n        ax.axis('off')\nplt.tight_layout()\nplt.show()\n#Rating_Based","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUMBER_OF_COMPONENTS = 500\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = tv_df.Synopsis.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Anime Synopsis Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbest_fearures = [[CVZ.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:500])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in tqdm(worddf.Word):\n    total_count = 0\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable X% of Variance<b>\",'<b>Appeared On X Synopsis<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\ndel best_fearures\ndel app\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Generating Synopses</h3>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Training a LSTM Network</h3>\n","metadata":{}},{"cell_type":"code","source":"\ngc.collect()\nsynopsis = tv_df.Synopsis[:tv_df.shape[0]//3]\nvocab = list(nltk.FreqDist(' '.join(synopsis).split(' ')).keys())\nvocab_size = len(vocab)\nword_index = { ch:i for i,ch in enumerate(vocab) }\nindex_word = { i:ch for i,ch in zip(word_index.values(),word_index.keys())}\nFTD = ' '.join(synopsis).split(' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"S = []  \nC = []\nstride = 10\nT = 25\nfor i in range(0, len(FTD) - T, stride):\n    S.append(FTD[i: i + T])\n    C.append(FTD[i + T])\nX = np.zeros((len(S), T, vocab_size), dtype='bool')\nY = np.zeros((len(S), vocab_size), dtype='bool')    \nfor i, seq in tqdm(enumerate(S)):\n    for t, char in enumerate(seq):\n        X[i, t, word_index[char]] = 1\n        Y[i, word_index[C[i]]] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_nn = Sequential()\nlstm_nn.add(Input((T,vocab_size)))\nlstm_nn.add(LSTM(128))\nlstm_nn.add(Dropout(0.2))\nlstm_nn.add(Dense(vocab_size,activation='softmax'))\n\nlstm_nn.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nlstm_nn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = lstm_nn.fit(X, Y, epochs=500, batch_size=128,verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'],'.-')\nplt.ylabel('loss',fontsize=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated = []\nfor itr in tqdm(range(0,10)):\n    start = np.random.randint(0, len(X)-1)\n    input_buffer = X[start] \n    generated_text = S[start].copy()\n\n    for i in (range(100)):\n        yhat = lstm_nn.predict(input_buffer[None,:])[0]\n        #ix = np.argmax(yhat)\n        ix = np.random.choice(range(vocab_size), p=yhat)\n\n        ch = index_word[ix]\n        generated_text += [ch]\n        input_buffer = np.r_[input_buffer[1:,:], np.zeros((1,vocab_size))]\n        input_buffer[-1,ix] = 1\n\n    generated.append(' '.join(generated_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Generated Synopses</h3>\n","metadata":{}},{"cell_type":"code","source":"for dx,i in enumerate(generated):\n    print('Generated Synopsis Example #',dx)\n    print(i,'\\n\\n')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}