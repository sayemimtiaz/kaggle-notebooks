{"cells":[{"metadata":{"_uuid":"f5ec2d083b7c783d39c0b3149f1fa96acbbff4af"},"cell_type":"markdown","source":"# Udacity PyTorch Scholarship Final Lab Challenge"},{"metadata":{"_uuid":"f52e52717905193b1eb76385c9be26207e0f624c"},"cell_type":"markdown","source":"**By [Droid(kaggle- droid021)](https://www.linkedin.com/in/v3nvince)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"002f25de7252b2f056743dafe141742d749afff3"},"cell_type":"code","source":"data_dir = '../input/flower_data/flower_data'\n\n# TODO: Define transforms for the training data and testing data\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\ntest_transforms = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + '/valid', transform=test_transforms)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba34d99dd8144a4252d6cceaa708a4d55ad528a"},"cell_type":"code","source":"model = models.densenet121(pretrained=True)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af3963b47925fafcc822de63c1681563424d9d15"},"cell_type":"code","source":"# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 512)),\n    ('relu', nn.ReLU()),\n    ('dropout', nn.Dropout(0.4)),\n    ('fc2', nn.Linear(512, 102)),\n    ('output', nn.LogSoftmax(dim=1))\n]))\n    \nmodel.classifier = classifier\nmodel.classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"543bd88ff970e12f4caea2d6631e82edf41a406d"},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f36e138047583ccceb04dfc84472675fbb99e111"},"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38590d3b2ba55205662df7368674e659c8f2fb84"},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nepochs = 50\n\nvalid_loss_min = np.Inf\n\n# Some lists to keep track of loss and accuracy during each epoch\nepoch_list = []\ntrain_loss_list = []\nval_loss_list = []\ntrain_acc_list = []\nval_acc_list = []\n# Start epochs\nfor epoch in range(epochs):\n    \n    #adjust_learning_rate(optimizer, epoch)\n    \n    # monitor training loss\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # Set the training mode ON -> Activate Dropout Layers\n    model.train() # prepare model for training\n    # Calculate Accuracy         \n    correct = 0\n    total = 0\n    \n    # Load Train Images with Labels(Targets)\n    for data, target in train_loader:\n        \n        if train_on_gpu:\n            data, target = data.to(device), target.to(device)\n        \n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        \n        if type(output) == tuple:\n            output, _ = output\n        \n        # Calculate Training Accuracy \n        predicted = torch.max(output.data, 1)[1]        \n        # Total number of labels\n        total += len(target)\n        # Total correct predictions\n        correct += (predicted == target).sum()\n        \n        # calculate the loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*data.size(0)\n    \n    # calculate average training loss over an epoch\n    train_loss = train_loss/len(train_loader.dataset)\n    \n    # Avg Accuracy\n    accuracy = 100 * correct / float(total)\n    \n    # Put them in their list\n    train_acc_list.append(accuracy)\n    train_loss_list.append(train_loss)\n    \n        \n    # Implement Validation like K-fold Cross-validation \n    \n    # Set Evaluation Mode ON -> Turn Off Dropout\n    model.eval() # Required for Evaluation/Test\n\n    # Calculate Test/Validation Accuracy         \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n\n\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n\n            # Predict Output\n            output = model(data)\n            if type(output) == tuple:\n                output, _ = output\n\n            # Calculate Loss\n            loss = criterion(output, target)\n            val_loss += loss.item()*data.size(0)\n            # Get predictions from the maximum value\n            predicted = torch.max(output.data, 1)[1]\n\n            # Total number of labels\n            total += len(target)\n\n            # Total correct predictions\n            correct += (predicted == target).sum()\n    \n    # calculate average training loss and accuracy over an epoch\n    val_loss = val_loss/len(test_loader.dataset)\n    accuracy = 100 * correct/ float(total)\n    \n    # Put them in their list\n    val_acc_list.append(accuracy)\n    val_loss_list.append(val_loss)\n    \n    # Print the Epoch and Training Loss Details with Validation Accuracy   \n    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n        epoch+1, \n        train_loss,\n        accuracy\n        ))\n    # save model if validation loss has decreased\n    if val_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        val_loss))\n        # Save Model State on Checkpoint\n        torch.save(model.state_dict(), 'umodel.pt')\n        valid_loss_min = val_loss\n    # Move to next epoch\n    epoch_list.append(epoch + 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9a9751f2cca1480f5a5586886b107d202e115ee"},"cell_type":"markdown","source":"## Links Here:  \n**Model State Checkpoint File: [umodel.pt](./umodel.pt)**   (Preferred)  "},{"metadata":{"trusted":true,"_uuid":"7eba135cff29e71eace2ea5fc4f602e52b8ffa0d"},"cell_type":"code","source":"model.load_state_dict(torch.load('umodel.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"460108459d896cf671c680a62ce300c5c0653124"},"cell_type":"code","source":"# Training / Validation Loss\nplt.plot(epoch_list,train_loss_list)\nplt.plot(val_loss_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training/Validation Loss vs Number of Epochs\")\nplt.legend(['Train', 'Valid'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1e4a168a3cb00c2b65b8ae2d201f326d5afa9c7"},"cell_type":"code","source":"# Train/Valid Accuracy\nplt.plot(epoch_list,train_acc_list)\nplt.plot(val_acc_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Training/Validation Accuracy\")\nplt.title(\"Accuracy vs Number of Epochs\")\nplt.legend(['Train', 'Valid'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd2bef52ac96c84fcbe3580ab82e459a434a7f61"},"cell_type":"code","source":"val_acc = sum(val_acc_list[:]).item()/len(val_acc_list)\nprint(\"Validation Accuracy of model = {} %\".format(val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d11d4774d1733a94a0cc1d5125830b62b40ad89e"},"cell_type":"code","source":"!git clone https://github.com/GabrielePicco/deep-learning-flower-identifier\n!pip install airtable\nimport sys\nsys.path.insert(0, 'deep-learning-flower-identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdd4df72e0b6d74cba415420cb6feb17c027c11d"},"cell_type":"code","source":"from test_model_pytorch_facebook_challenge import calc_accuracy\ncalc_accuracy(model, input_image_size=224, use_google_testset=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}