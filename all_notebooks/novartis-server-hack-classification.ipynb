{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Novartis Data science challenge :\n\nTo predict whether the server is hacked or not.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import accuracy_score # accuracy score\nfrom sklearn.metrics import recall_score   # recall score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#loading the train data\ntrain = pd.read_csv(\"/kaggle/input/novartis-data/Train.csv\")\n#Shape of train\nprint(train.shape) #printing the shape of train\nprint(train.describe()) #printing the statistics of train\nprint(train.info()) #printing the information of train\nprint(train.head()) #printing the first five rows of the train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the test data\ntest = pd.read_csv(\"/kaggle/input/novartis-data/Test.csv\")\n#Shape of train\nprint(test.shape) #printing the shape of train\nprint(test.describe())#printing the statistics of train\nprint(test.info()) #printing the information of train\nprint(test.head()) #printing the first five rows of the test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the Incident_ID and Date from train data\ntrain = train.drop(['INCIDENT_ID','DATE'], axis=1)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifying all the columns that has the null values in train data\nnull_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above code we can view that X_12 column has 182 null values,we will replace those null values with zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filled NaN values with \"0\" using fillna()\ntrain[\"X_12\"].fillna(0,inplace = True)\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifying all the columns that has the null values in test data\nnull_columns=test.columns[test.isnull().any()]\ntest[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filled NaN values with \"0\" using fillna()\ntest[\"X_12\"].fillna(0,inplace = True)\n#train[\"X_12\"].ffill(axis = \"rows\")\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can view in the train and test info that X_12 is float64 let's convert it into int64","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"X_12\"] = train[\"X_12\"].astype(np.int64)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"X_12\"] = test[\"X_12\"].astype(np.int64)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the duplicated rows from train data\nprint(\"Train shape before removing the duplicates :\" , train.shape)\ntrain.drop_duplicates(keep='first', inplace=True)\nprint(\"Train shape After removing the duplicates :\" , train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Skewness of the train data\ntrain.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis ON TRAIN DATA :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#Skewness of train data\nsns.distplot(train.skew(),color='blue',axlabel ='Skewness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target Variable Distribution:\n\nOur first step in Machine Learning should always be analyzing the target variable. MULTIPLE_OFFENSE is our given target/dependent variable. Let's analyse its distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(16,6))\nsns.violinplot(train['MULTIPLE_OFFENSE'])\nplt.show()\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['MULTIPLE_OFFENSE'].skew()))\nprint(\"Kurtosis: {}\".format(train['MULTIPLE_OFFENSE'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of training Mutiple Offence : {} \".format(len(train)))\nprint(\"Offense Rate {:.4}%\".format(train[\"MULTIPLE_OFFENSE\"].mean()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us visualize the Multiple Offense using pie chart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Pie Chart for the target variable\nlabels = ['Hacked', 'Genuine']\nplt.title('Multiple Offense')\ntrain['MULTIPLE_OFFENSE'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',shadow=True,labels=labels,fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram\ntrain.hist(figsize=(14,14))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot \nplt.subplots(figsize=(15, 6))\nsns.boxplot(data = train, orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a correlation heatmap\nsns.heatmap(train.corr(),annot=True, cmap='gist_ncar', linewidths=0.1)\nfig=plt.gcf()\nfig.set_size_inches(14,14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above correlation plot we can clearly say that X_2 and X_3 are highly correlated,this is to check correlation of X_1 to X_15 correlations along with Multiple Offense.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#High Correlation of X_2 and X_3 using joint plot\nsns.jointplot(train['X_2'],train['X_3'], kind=\"reg\", color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA MODELLING FOR PREDICTION :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.iloc[:,:-1]\ny_train = train[\"MULTIPLE_OFFENSE\"]\n#Dropping the Incident_ID and Date from test data\nX_test = test.drop(['INCIDENT_ID','DATE'], axis=1)\nprint(\"Shape of X_train : \",X_train.shape)\nprint(\"Shape of y_train : \",y_train.shape)\nprint(\"Shape of X_test : \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SMOTE:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Synthetic minority oversampling technique to balance the imbalanced data.\nprint('Before OverSampling, the shape of X_train: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of y_train: {} \\n'.format(y_train.shape))\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\nfrom imblearn.over_sampling import SMOTE\nsampler = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = sampler.fit_sample(X_train, y_train)\nprint('After OverSampling, the shape of X_train: {}'.format(X_train_sm.shape))\nprint('After OverSampling, the shape of y_train: {} \\n'.format(y_train_sm.shape))\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_sm==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_sm==0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above code we can cleary view that the target variable is well balanced.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spltting the data into train and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train_sm, y_train_sm ,test_size=0.3, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_val_sc   = sc.transform(X_val)\nX_test_sc  = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA MODELLING FOR PREDICTION :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Classification\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nsvc = SVC()\nsvc.fit(X_train_sc, y_train)\ny_pred = svc.predict(X_val_sc)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_svc = recall_score(y_pred, y_val)\nprint(\"Support Vector Classifier Accuracy Score:\",acc_svc)\nprint('Support Vector Classifier Recall Score:',recall_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train_sc, y_train)\ny_pred = gbk.predict(X_val_sc)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_gbk = recall_score(y_pred, y_val)\nprint(\"Gradient Boosting Classifier Accuracy Score:\",acc_gbk)\nprint('Gradient Boosting Classifier Recall Score:',recall_gbk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X_train_sc, y_train)\ny_pred = randomforest.predict(X_val_sc)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_randomforest = recall_score(y_pred, y_val)\nprint(\"Random Forest Classifier Accuracy Score:\",acc_randomforest)\nprint('Random Forest Classifier Recall Score:',recall_randomforest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train_sc, y_train)\ny_pred = decisiontree.predict(X_val_sc)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_decisiontree = recall_score(y_pred, y_val)\nprint(\"Decision Tree Accuracy Score:\",acc_decisiontree)\nprint('Decision Tree Recall Score:',recall_decisiontree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN or k-Nearest Neighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train_sc, y_train)\ny_pred = knn.predict(X_val_sc)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_knn = recall_score(y_pred, y_val)\nprint(\"KNN Classifier Accuracy Score:\",acc_knn)\nprint('KNN Classifier Recall Score:',recall_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machine', \n              'Random Forest', \n              'Decision Tree',\n              'Gradient Boosting Classifier',\n               'k-Nearest Neighbors Classifier'],\n    'Accuracy Score': [acc_svc,acc_gbk, acc_randomforest,acc_decisiontree,acc_knn],\n    'Recall Score'  :  [recall_svc,recall_gbk,recall_randomforest,recall_decisiontree,recall_knn]})\nmodels.sort_values(by='Accuracy Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#I have chosen Gradient Boosting classifier amongst all classifiers\ny_pred = gbk.predict(X_test_sc)\nsubmission_df = pd.DataFrame({'INCIDENT_ID':test['INCIDENT_ID'], 'MULTIPLE_OFFENSE':y_pred})\nsubmission_df.to_csv('Sample Submission GBK v1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}