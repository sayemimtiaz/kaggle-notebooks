{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhousing = pd.read_csv(\"../input/housing-handsonml-chapter2/housing.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \n#only in a Jupyter notebook\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From scratch methid to split data based on length\nimport numpy as np\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = split_train_test(housing, 0.2)\nprint(len(train_set), \"train +\", len(test_set), \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#better way to split data using hash values - this needs a distinct identifier column, for which index or \n#combination of lat and longitude can be used\n\nimport hashlib\n\ndef test_set_check(identifier, test_ratio, hash):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n\ndef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n    return data.loc[~in_test_set], data.loc[in_test_set]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id = housing.reset_index() # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best package for test set splitting\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creatinf 5 categories of meidam income for stratified splitting\nhousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stratified splitting\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the additionam median value category column that was added\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copying the train set to work, so that OG train set is not disturbed\nhousing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gepgraphical scatter plot\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting alpha for better visualisation\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The radius of each circle represents the district’s population and the color represents the price . Using predefined color map to map housing prices\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,) \n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix\ncorr_matrix = housing.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using pandas plot to certain more correlated attributes to check visually\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#zooming on the one with mediam income and house value\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating custom variables to check if they are better than the existing ones\n#One should always reiterating this step when some pattern is noticed\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing correlations again to check if customs performed better\n#bedrooms per room seems better clearly than just bedrooms or rooms\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making x and y - training and label datasets\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing value handling - three options\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1 - get rid of NA rows\nhousing.drop(\"total_bedrooms\", axis=1) # option 2 - get rid of the column itself\nmedian = housing[\"total_bedrooms\"].median() # option 3 - impute median values\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputer function to imputr median values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remiving the categorical value because imputer only works on Numerical values\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputed values\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming train set with imputed values\nX = imputer.transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the above numpy array to a dataframe\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding the categorial column - ocean proximity\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nhousing_cat = housing[\"ocean_proximity\"]\nhousing_cat_encoded = encoder.fit_transform(housing_cat)\nhousing_cat_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoder.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one hot encoding better than label, fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we need to reshape it:17\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot\n#output is a scipy matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#coverting to numpy array\nhousing_cat_1hot.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can apply both transformations (from text categories to integer categories, then from integer categories to one-hot vectors) in one shot using the LabelBinarizer class:\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n#this returns a dense NumPy array by default. You can get a sparse matrix instead by passing sparse_output=True to the LabelBinarizer constructor.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#Feature Scaling - very important. all columns should be on similar scale. Y does not need to be scaled\n'''\nTwo mwthods - minmax scaler and Standardization\nMin-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled\nso that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max\nminus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a\nfeature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.\nStandardization is quite different: first it subtracts the mean value (so standardized values always have a\nzero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike\nmin-max scaling, standardization does not bound values to a specific range, which may be a problem for\nsome algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However,\nstandardization is much less affected by outliers. For example, suppose a district had a median income\nequal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–\n0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called\nStandardScaler for standardization.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Self defined pipeline function\nfrom sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n        def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n            self.add_bedrooms_per_room = add_bedrooms_per_room\n        def fit(self, X, y=None):\n            return self # nothing else to do\n        def transform(self, X, y=None):\n            rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n            population_per_household = X[:, population_ix] / X[:, household_ix]\n            if self.add_bedrooms_per_room:\n                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n                return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n            else:\n                return np.c_[X, rooms_per_household, population_per_household]\n            \nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if we could feed a Pandas DataFrame directly into our pipeline, instead of having ,\n#to first manually extract the numerical columns into a NumPy array. There is nothing in Scikit-Learn to\n#handle Pandas DataFrames,19 but we can write a custom transformer for this task\n\n#self defined pipeline function\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Self defined label binarier class\nclass MyLabelBinarizer(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipeline code - basically all the preprocessing steps together - imputing, scaling, encoding for both numerical and categorical columns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy = \"median\")),\n                        ('attribs_adder', CombinedAttributesAdder()),\n                        ('std_scaler', StandardScaler()),\n                        ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\nfrom sklearn.pipeline import FeatureUnion\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n                         ('selector', DataFrameSelector(num_attribs)),\n                         ('imputer', SimpleImputer(strategy = \"median\")),\n                         ('attribs_adder', CombinedAttributesAdder()),\n                         ('std_scaler', StandardScaler()),\n                        ])\n\ncat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attribs)), \n                         ('label_binarizer', MyLabelBinarizer()),\n                        ])\n\nfull_pipeline = FeatureUnion(transformer_list = [(\"num_pipeline\", num_pipeline), \n                                                 (\"cat_pipeline\", cat_pipeline),\n                                                ])\n\n# And we can now run the whole pipeline simply:\n#to get the finally prepared dataset\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying linear regression on the transformed data\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeing some predictions on test values\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeing MSE of the linear regression model - not good - definitely underfitting- need to try a more complex model \nfrom sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeinf rmse\n#overfits excessively giving 0 rmse\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCross Validation :Cross-validation is a technique for evaluating ML models by training several ML models on subsets \nof the available input data and evaluating them on the complementary subset of the data. \nUse cross-validation to detect overfitting, ie, failing to generalize a pattern.\n\nYou use the k-fold cross-validation method to perform cross-validation. \nIn k-fold cross-validation, you split the input data into k subsets of data (also known as folds). \nYou train an ML model on all but one (k-1) of the subsets, and then evaluate the model on the subset that was not used for training. \nThis process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doing cross validation to avoid overfitting and beter analysyse train-test RMSE\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree results after cross validation - still performs poorly\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regression with cross validation - still performs poorly\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random forest regression = ensemble/combination of many decision trees to form a better model \nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\n#result metrics of the random forest model  - much better than linear regression and decision tree moseld -RMSE is so much less\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random forest with Cross validation\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#pickle to save models\nfrom sklearn.externals import joblib\njoblib.dump(my_model, \"my_model.pkl\")\n# and later...\nmy_model_loaded = joblib.load(\"my_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid search on random forest - to hyper parameter tune the model \n# and find the best parameter combination that gives the most accurate predictions\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing the best parameters\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the nest estimator model\ngrid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing all grid search models with their RMSEs\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"'''\nAlternatives to GridSearchCV is \n- Randomised Search CV where it picks random combinations to find bet values\n- Ensemble Methods\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing feature importances of all independent variables\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining feature importances with parameter names and listing in desecending order\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_one_hot_attribs = list(encoder.classes_)\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying the final best model that came from grid search\nfinal_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the final best RMSE - lowest found so far\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThe idea is to find the model with the right complexity and then keep hyperparameter tuning it to find the best model\n'''","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"'''\npresent your solution (highlighting what you have\nlearned, what worked and what did not, what assumptions were made, and what your system’s limitations\nare), document everything, and create nice presentations with clear visualizations and easy-to-remember\nstatements\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}