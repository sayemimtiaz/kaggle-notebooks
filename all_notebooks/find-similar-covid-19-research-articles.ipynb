{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport glob\nimport json\nimport re\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"**Goal**\n- With a large amount of literature and fast spreading of COVID-19. It's difficult for health care professionals figure out relevant research. \n- In this post, we will try to identify which topic is discussed in research. It also reduce number of articles which scientist has go through. \n- Research paper topic modelling is an unsupervised machine learning method which allow us to learn topic of articles in corpus"},{"metadata":{},"cell_type":"markdown","source":"*ok Lets go*\n- Because kaggle provided us lot of json file so we will load all json data to dataframe and drop abstract duplicate to make sure unique articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npath = '/kaggle/input/'\nall_json = glob.glob(f'{path}/**/*.json', recursive=True)\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ncovid_df = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\ncovid_df.drop_duplicates(['abstract'], inplace=True)\ncovid_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to clean-up the text by \n- Remove punctuation\n- Convert each text to lower case"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_df['body_text'] = covid_df['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ncovid_df['abstract'] = covid_df['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ncovid_df['body_text'] = covid_df['body_text'].apply(lambda x: lower_case(x))\ncovid_df['abstract'] = covid_df['abstract'].apply(lambda x: lower_case(x))\ncovid_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Because we only need body_text of the article so we will drop paper_id and abstract then save clean file, we will use it later"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = covid_df.drop([\"paper_id\", \"abstract\"], axis=1)\ntext.head()\ntext.to_csv('./clean_text.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Next we will import spacy. If you never installed spacy before then you have to install before import\n- If you are using anaconda then implement\n    - *conda install -c conda-forge spacy*\n- If you are not using anaconda and you want to install via pip then implement:\n    - *pip install -U spacy*\n- If you want to install from source then implement:\n    - *git clone https://github.com/explosion/spaCy\n    - *cd spaCy*\n    - *pip install -r requirements.txt*\n    - *python setup.py build_ext - inplace*\n- You can refer to this page for more option: https://spacy.io/usage\n- **Then what is spaCy ?**\n    - spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n    - If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n    - spaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning ([source](https://spacy.io/usage/spacy-101))\n- ok let's import spacy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nspacy.load('en')\nfrom spacy.lang.en import English\nparser = English()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We will use following function to clean our text and return list of tokens:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text):\n    lda_tokens = []\n    tokens = parser(text)\n    for token in tokens:\n        if token.orth_.isspace():\n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n        elif token.orth_.startswith('@'):\n            lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)\n    return lda_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use NLTK’s Wordnet to find the meanings of words, synonyms, antonyms, and more. In addition, we use WordNetLemmatizer to get the root word."},{"metadata":{},"cell_type":"markdown","source":"- We use NLTK Wordnet and WordNetLemmatizer to find the meaning of words such as synonyms, antonyms, etc. and also get the root word\n- Before that feel free to install nltk and download wordnet together with stopword\n    - *pip install - user -U nltk*\n    - *nltk.download('wordnet')*\n    - *nltk.download('stopwords')*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Filter out stop words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"en_stop = set(nltk.corpus.stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can define a function to prepare the text for topic modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_text_for_lda(text):\n    tokens = tokenize(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [get_lemma(token) for token in tokens]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Open up our data, read line by line, for each line, prepare text for LDA, then add to a list.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom random import randint\n\ntext_data = []\nwith open('./clean_text.csv') as f:\n    for line in f:\n        tokens = prepare_text_for_lda(line)\n        value = randint(0, 100)\n        if value==99:\n            text_data.append(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Latent Dirichlet Allocation (LDA) with Gensim**\n- What is Gensim ?\n    - Gensim = \"Generate Similar\". \n    - Gensim started off as a collection of various Python scripts for the Czech Digital Mathematics Library dml.cz in 2008, where it served to generate a short list of the most similar articles to a given article (source)\n- Install Gensim via anaconda\n    - conda install -c anaconda gensim\n- Install Gensim via pip\n    - pip install - upgrade gensim\n    \n**Then what is LDA**\n- In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox (source)\n- Ok, we will create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So we are trying to ask LDA to find 20 topics in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nNUM_TOPICS = 10\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All topic related to virus mechanism but research on difference way"},{"metadata":{},"cell_type":"markdown","source":"# pyLDAvis\n- pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n- Visualizing 20 topics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model10.gensim')\nimport pyLDAvis.gensim\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Saliency: a measure of how much the term tells you about the topic.\n- Relevance: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n- The size of the bubble measures the importance of the topics, relative to the data.\n- First, we got the most salient terms, means terms mostly tell us about what’s going on relative to the topics. We can also look at individual topic."},{"metadata":{},"cell_type":"markdown","source":"> When we have 10 or more topics, we can see certain topics are clustered together, this indicates the similarity between topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}