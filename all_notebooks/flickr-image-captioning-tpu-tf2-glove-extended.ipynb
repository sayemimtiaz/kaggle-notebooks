{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Neuron Engineer provided a great [notebook tutorial](https://www.kaggle.com/ratthachat/flickr-image-captioning-tpu-tf2-glove) that extends TensorFlow's own tutorial on [Image captioning with visual attention](https://www.tensorflow.org/tutorials/text/image_captioning). Neuron Engineer's additions include TPU training and upgrading the encoder from InceptionV3 to EfficientNet-B3. My notebook tutorial offers the additional extensions listed below:\n- Bleu Score metrics\n- Decoders\n    - Pure Sampling\n    - Top K Sampling\n    - Greedy Search\n    - Beam Search\n- Scheduled Sampling from https://arxiv.org/pdf/1506.03099.pdf\n- Early Stopping based off of validation bleu score\n"},{"metadata":{"id":"K2s1A9eLRPEj"},"cell_type":"markdown","source":"# Flickr Image Captioning with TF2 & TPU!!!\n***with bonus improvement on using Glove***\n\nImage Captioning is a very exciting problem where our model have to translate understanding of images to human readable sentences! However, although Kaggle has been a very rich source of shared codes, I found that there's extremely rare Kaggle kernels on this exciting domain where Vision meets NLP ... \n\n\n![Image Captioning](https://miro.medium.com/max/1400/1*6BFOIdSHlk24Z3DFEakvnQ.png)\n\nWhen I started learning Deep Learning and exploring Kaggle, this kind of tutorial kernel is the one I was looking for, but could not find practical resources at that time. I hope this tutorial to be a starting point and inspiration for those who want to learn this exciting field of Visio-Linguistic domain!!\n\nWith the release of Tensorflow 2 (the latest version now is TF2.2) and Kaggle free TPU + free GCS (Google Cloud Storage), we all now can access a computational **super power** easily than ever before. I think this is simply a revolution, and this super-feature finally allow laymen like us to attack complicated problems like Image Captioning to another level. All of these inspired me to work and share this tutorial kernel!\n\n\n## Reference and Improvement\nAt the moment when I was writing this kernel, the best source to learn image captioning is TF official tutorial : https://www.tensorflow.org/tutorials/text/image_captioning where not only we can learn how to do basic image captioning, we can also learn how to \n\n* How to implement **Attention mechanism** properly! Attention mechanism is a technique to allow us to focus on only-relevant spatial image features when generating each word.\n* How to construct **Keras subclass API** in contrast to normally usage of Function API\n* How to build **custom training loop** in Keras\n\nTo adapt this great tutorial to Kaggle we need to adjust code a bit. For those who are interested in to try this original tutorial in Kaggle, I already did all the jobs for you here : https://www.kaggle.com/ratthachat/image-captioning-by-effnet-attention-in-tf2-1\n\nIn this kernel, we will go beyond the original tutorial in many directions!!\n\n* ***From GPU to TPU*** : we will increase our computational power by *** > 10 folds*** using Kaggle recent TPU feature! With Tensorflow 2, this requires minimal code changing and requires us to transfer data to GCS, which again Kaggle has generously provided us for free! \n* ***Complexity of data***: we changes training data from COCO to Flickr which is more challenging due to quality of texts describing the images! To get decent model, we increase training data ***almost 20 folds*** from around 8K to 150K from the original tutorial, but all training from scratch to convergence can be done by less than 2 hours with the power of TF2 and TPU!!\n* ***Capability of models*** : we upgrade both CNN encoder (for images) and RNN decoder (for texts). On CNN, we use the current SOTA EfficientNet with \"Noisy-Students\" weights generously provided [here](https://github.com/qubvel/efficientnet) . For RNN, instead of learning word vectors from scratch, we adjusted the code to embed the pretrained Glove vectors directly . \n\nWith all these upgrades we got much better results than the original tutorial. (you can compared this kernel result with the original in Kaggle's link above)\n\n# 0. Outline and Pre-requisite\nThis tutorial consists of 3 main sections\n\n1. Setup everything (TPU/GPU and input pipeline)\n2. Build and train Model\n3. Predict and visualize image captions\n\nEach section consists of several sub-steps as described in each section . Before we can begin, install the latest tensorflow stable version (TF 2.2) and Qubvel's efficientnet . These two may be auto set up into Kaggle docker in the future, but for now we have to install it manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Try efficientnet in addition to InceptionV3 of the original example.\n#!pip install -q tensorflow==2.2 # fix TPU memory issue\n!pip install -q efficientnet\n\n#N_VOCABS = 20000 # all vocabs of flickr30k is around 18k, so we choose them all -- if training loss does not work well, change to 5K\nN_VOCABS = 18317 # shouldn't vocab size be constrained by embedding_matrix size? (18318, 300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I also provide the saved weights for this kernel, if you just want to play around image captioning, without model training, you can set the `USE_PREVIOUS_SAVE = True` . If you set it to be `False`, please remember to turn on your TPU where you have a 30 hours/week quota.\n\n**NOTE** In notebook version 7, I set this to true, and just use CPU to predict captions. If you want to use a TPU to train this kernel, please see version 6."},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_PREVIOUS_SAVE = False # Set to False if you want to train the model by yourself","execution_count":null,"outputs":[]},{"metadata":{"id":"Cffg2i257iMS"},"cell_type":"markdown","source":"# 1. Setup evertything\n\nWe have to prepare several steps of data pipeline. Note that we have two types of inputs : images and real-captions, and only one output type (predicted captions)\n\n1.1. Choose appropriate TF `strategy` for TPU/GPU, and Transfer dataset into Kaggle GCS in case of TPU\n\n1.2. Propcess Captions , adding `<start>` and `<end>`\n\n1.3. Build a list of images and corresponding captions (image-input and text-output)\n\n1.4. Setup text-input for RNN decoder\n\n - 1.4.1) Load pretrained word vectors (Glove) for each vocabulary\n - 1.4.2) Tokenize captioning to model's vocabulary also pad each caption to have the same length\n \n1.5. Split data into train and valid set\n\n1.6. Employ `tf.dataset` to construct appropriate input pipeline using data from steps 2.-4.\n - We can also add data transformation (e.g. image augmentation like H-flip) easily in this step"},{"metadata":{"id":"U8l4RJ0XRPEm","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nimport gc\nfrom glob import glob\nfrom PIL import Image\nimport pickle\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 TPU/GPU strategy and GCS transfer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn \nfrom tokenizers import ByteLevelBPETokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code will detect TPU or else use either GPU or CPU. With TF-Keras' power, all complex details of multi-GPUs or TPUs will hide under the hood in the variable `strategy` . In Kaggle, if you enable TPU, you should see REPLICAS = 8 (meaning 8 processors for TPU v3-8)"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"id":"b6qbGw8MRPE5"},"cell_type":"markdown","source":"### Upload Flickr into Kaggle GCS\n\nWe will use the [Flickr dataset](http://bryanplummer.com/Flickr30kEntities/) to train our model. Originally, the dataset contains more than 30,000 images, each of which has at least 5 different caption annotations. This great dataset is already in Kaggle's dataset; however, if we want to train our model using TPU, we have to upload this dataset into Kaggle GCS so that TPU can be most effectively access them. Note that TPU cannot directly access normal Kaggle dataset, so this step is a must.\n\nKaggle is so great that it provides free GCS for us, and easy API, see KaggleDatasets below. This will automatically select the best GCS zone with free of charges!"},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL_FLICKR_PATH = '/kaggle/input/flickr-image-dataset/flickr30k_images/'\nannotation_file = LOCAL_FLICKR_PATH + 'results.csv'\nLOCAL_IMG_PATH = LOCAL_FLICKR_PATH + 'flickr30k_images/'\n\n!ls {LOCAL_IMG_PATH} | wc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT: **. The following cell upload all Flickr images (8GB) into GCS . And it will take around 25 minutes!!!\n\nThe below cell will show an error (HTTP timeout), but actually the transferring process is still ongoing. If using TPU, you have to run this cell again by 25 minutes of time, until it returns the message \"yeah\" . At this point, you can turn off your TPU, wait, and come back again in 25-30 minutes until you can move on.\n\nIf you use GPU or CPU with option `USE_PREVIOUS_SAVE = True`, you can pass all these cells without problems, since you can access normal Kaggle dataset.\n\nAlso note that once the upload finish, it sometimes will remain in Kaggle GCS for a while; therefore, maybe if you play around this kernel in 2-3 days consecutively, you may see that you don't have to upload them again.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## This steps will take around 25 minutes offline ...\nif strategy.num_replicas_in_sync == 8:\n#     GCS_DS_PATH_FLICKR = KaggleDatasets().get_gcs_path('flickr8k-sau') # 2gb # 5 mins\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path('flickr-image-dataset') # 8gb # 20-25 mins\n    print('yeah')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Once the uploading finish, set up the correct paths for each GPU/TPU\n\nif strategy.num_replicas_in_sync == 8:\n    # print(GCS_DS_PATH_FLICKR)\n    # !gsutil ls $GCS_DS_PATH_FLICKR\n\n    print(GCS_DS_PATH)\n    !gsutil ls $GCS_DS_PATH\n    \n    FLICKR_PATH = GCS_DS_PATH + '/flickr30k_images/'\n    IMG_PATH = FLICKR_PATH + 'flickr30k_images/'\n    # less than 10sec\n    !gsutil ls {IMG_PATH} | wc\nelse: \n    FLICKR_PATH = LOCAL_FLICKR_PATH\n    IMG_PATH = LOCAL_IMG_PATH","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Processing Captions\nAfter finish preparing data, now we begin to process the captions for all images. First, we add the tag `<start>` and `<end>` to all captions to tell the model about the beginning and the end of each caption."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(annotation_file, delimiter='|') #\nprint(df.shape)\nprint(df.columns[2], df.columns[2] == ' comment') # The column name has the front space\ndf[' comment'].values[0]\ndf.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\nSTART_TOKEN = '<start> '\nEND_TOKEN = ' <end>'\n\n# tokenizer = ByteLevelBPETokenizer(lowercase=True)\n# tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_start_end(text):\n    return START_TOKEN + str(text) + END_TOKEN\n\ndf['comment'] = df[' comment'].progress_apply(add_start_end)\ndf.comment.values[:6]","execution_count":null,"outputs":[]},{"metadata":{"id":"aANEzb5WwSzg"},"cell_type":"markdown","source":"## 1.3 Lists of input images and corresponding captions\n\nNext, we begin to prepare data pipeline. Lets start first by building a list of input images and corresponding captions (image-input and text-output)\n\nNote that since each image have 5 captions, we simply have 5 duplicated image names in the `full_img_name_list`"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_img_name_list = [] # include gs path\n\nfor ii in tqdm_notebook(range(len(df))):\n    full_image_path = IMG_PATH + df.image_name.values[ii]\n    full_img_name_list.append(full_image_path)\n                        \nlen(full_img_name_list), full_img_name_list[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_captions_list = list(df.comment.values)\nprint(len(all_captions_list), all_captions_list[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Setup text-input for RNN decoder (text-input)\n\nHere, we have to tokenize the caption labels so that our RNN can use as input and predict as output (word by word). \n\nIn order to generate good sentence, one key is to have good word vectors, so here we choose to employ the pretrained Glove vectors to improve our captioning process\n\n### 1.4.1 Setup Glove word vectors\n\nthe below function `build_matrix` will return the required word vectors. This function will recieve list of words in the dataset's vocabulary as input. We will tokenize all captions and list all vocabs in the dataset in steps 1.4.2\n\nThis funciton is carefully designed to return meaningful word vectors as best as it could as it can fix many unknown words like two examples below :\n\n**Example1.** Note that as Gloves differentiate between upper and lower cases, sometimes Glove may not know word like `john` but it may know `John`. In this case if our dataset have the word `john` which can happen in informal writing, we still want it to have the `John`'s vector (instead of having a random one due to out-of-vocabulary) \n\n**Example2.** For another case, sometimes our dataset may contain some uncommon words like `deforestization` but Glove will not know this word. However, as this complex word may come from the root like `deforest` where Glove really knows, so it may be better to give `deforest`'s vector to this complex word instead of a pure random one.\n\nBoth examples are done automatically in the `build_matrix` function with this line : \n`for candidate in [word, word.lower(), word.upper(), word.capitalize(), ps.stem(word), lc.stem(word), sb.stem(word) ]:` .\n\nNote that all these sub-words mapping can be done automatically in modern decoder model like GPT-2. However, at this moment, GPT-2 cannot get the image input vectors, so unfortunately we cannot employ the GPT-2 super power into our captioning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\nfrom gensim.models import KeyedVectors\nimport gensim\ndef build_matrix(word_index, embedding_index, vec_dim):\n    \n    num_unk = 0\n    \n    emb_mean, emb_std = -0.0033470048, 0.109855264\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len(word_index) + 1,vec_dim))\n#     embedding_matrix = np.zeros((len(word_index) + 1, vec_dim))\n    for word, i in word_index.items():\n        known = False\n        for candidate in [word, word.lower(), word.upper(), word.capitalize(), \n                          ps.stem(word), lc.stem(word), sb.stem(word) ]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                known = True\n                break\n        if known == False: num_unk += 1\n    \n    print('number of unknown words is ', num_unk)\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# we actually have two choices of pretrained word vectors here : glove and word2vec. You are free to experiment what's best.\nEMBEDDING_FILES = [\n    '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim',\n    '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\n]\n\nglove_model = gensim.models.KeyedVectors.load(EMBEDDING_FILES[1], mmap='r') # here, we choose glove\ngensim_words = glove_model.index2word\nprint(len(gensim_words), gensim_words[:20])\n# How to use\nprint(glove_model['the'].shape)\n'the' in glove_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4.2 Tokenization\nHere, we do a usual keras text-processing : \n\n- tokenize all captions into list of words, and remember all vocabulary\n- convert list of words into list of \"word-index\"\n- pad each example in the list to have the same length\n\nNote that we can set the number of Vocab to be small to simplify the training process; however, in this kernel, we already employed pretrained word vector power, so we choose the full set of vocab (around 20K words) so that we can produce most natural sentences."},{"metadata":{"id":"HZfK8RhQRPFj","trusted":true},"cell_type":"code","source":"# Find the maximum length of any caption in our dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)","execution_count":null,"outputs":[]},{"metadata":{"id":"oJGE34aiRPFo","trusted":true},"cell_type":"code","source":"%%time\n# Choose the top_k words from the vocabulary\ntop_k = N_VOCABS \ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ') # note 'a'\ntokenizer.fit_on_texts(all_captions_list)\ntrain_seqs = tokenizer.texts_to_sequences(all_captions_list)\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make list from dict\ntokenizer.index2word = [tokenizer.index_word[ii] for ii in range(len(tokenizer.word_index)) ] \nprint(tokenizer.index2word[:20]) # see top-20 most frequent words\nprint(tokenizer.index2word[-20:]) # these all come to <unk>\nlen(tokenizer.index2word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.index_word.get(2000, tokenizer.word_index['<end>']))\nprint(tokenizer.index_word.get(19999, tokenizer.word_index['<end>']))\nprint(tokenizer.word_index['<end>'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_cap = np.array([len(text.split()) for text in all_captions_list])\nprint(len_cap.mean(), len_cap.std(), len_cap.max(), len_cap.min())\nmax_seq_len = int(np.percentile(len_cap,99.9))","execution_count":null,"outputs":[]},{"metadata":{"id":"0fpJb5ojRPFv","trusted":true},"cell_type":"code","source":"%%time\n# Create the tokenized vectors : list of word-indices\ntrain_seqs = tokenizer.texts_to_sequences(all_captions_list)\n\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen = max_seq_len, truncating='post')\n\n# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs) #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lenx = np.array([len(x) for x in cap_vector])\nprint(lenx.min(), lenx.mean(), cap_vector[0])\nprint(max_length)\nmax_length = np.min([max_seq_len, max_length])\nprint(max_length)","execution_count":null,"outputs":[]},{"metadata":{"id":"M3CD75nDpvTI"},"cell_type":"markdown","source":"## 1.5 Split the data into train and valid set\n\nNote that due to each image have 5 captions, we cannot splitting the data randomly. Here, to avoid knowledge leakage from train-set to valid-set, we must use GroupKFold to carefully make each image stays in the same set."},{"metadata":{"id":"iS7DDMszRPGF","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\n\n# 2.5% valid = 3975 captions = 795 images\n# GroupKFold is not randomized\nkf = GroupKFold(n_splits=40).split(X=full_img_name_list, groups=full_img_name_list) #2.5% valid\n#kf = GroupKFold(n_splits=2560).split(X=full_img_name_list, groups=full_img_name_list)\n\nfor ind, (tr, val) in enumerate(kf):\n    img_name_train = np.array(full_img_name_list)[tr] # np.array make indexing possible\n    img_name_val = np.array(full_img_name_list)[val]\n    \n    cap_train =  cap_vector[tr]\n    cap_val =  cap_vector[val]\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cap_train))\nprint(len(cap_val))\n\nprint(len(img_name_train))\nprint(len(img_name_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"XmViPkRFRPGH","trusted":true},"cell_type":"code","source":"print(img_name_train[:6],'\\n')\nprint(cap_train[:6],'\\n')\nlen(img_name_train), len(cap_train), len(img_name_val), len(cap_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"uEWM9xrYcg45"},"cell_type":"markdown","source":"## 1.6 Create data pipeline tf.data dataset \n\nFinally, we arrive the last step of data preparation : putting everything together in data pipeline.\n\nData pipeline includes reading images/texts from files or from memory, process / transform them (e.g. data augmentation), and feed them into the processors like GPU or TPU. Previously, in Keras & Tensorflow 1, `data_generator` is the method of choice when using Keras but there's cubersome API, and bottleneck on I/O & CPU.\n\nTo optimize training speed, efficient data pipeline is needed when you cannot store all your data in the memory.\n\nIn Tensorflow 2, data pipeline becomes very powerful and simple with `tf.dataset` API. `tf.dataset` not only much simplifies the old mechanism of `data_generator`, but it also address the bottleneck by using parallel machanism, prefetching, processing pipeline in GPU/TPU (instead of a slow CPU) and so on ... For details please see this [tutorial](https://www.youtube.com/watch?v=VeV65oVvoes) "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_size = (299, 299,3)\nAUTO = tf.data.experimental.AUTOTUNE\n\ndef decode_image(filename, label=None, image_size=(target_size[0],target_size[1])):\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n    \n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    image = (tf.cast(image, tf.float32) / 255.0)\n    image = (image - means) / stds # for qubvel EfficientNet\n    \n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below specific setting e.g. `drop_remainder = False` is proven to be effective in this tutorial. You can try experiment other options.\n\nOne important thing to note here is that, since we use custom training loop, it's important to convert from normal dataset into distributed dataset for TPU by `strategy.experimental_distribute_dataset()` function.\n\nPlease refer to this official [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) and this official [guideline](https://www.tensorflow.org/guide/distributed_training) for more details."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to define BATCH_SIZE for tf.dataset\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would like to calculate bleu score on validation data within our training loop in order to inform early stopping. Training the model requires that each image be associated with <font size=\"4\"><b>one</b></font> caption. However, calculation of bleu score requires that each image be associated with its respective <font size=\"4\"><b>group</b></font> of captions. Therefore, we must regroup the captions of our validation data in a specific format for creating a tensorflow dataset and calculating bleu score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install numpy_indexed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy_indexed as npi\ncombine_val = np.hstack((img_name_val.reshape(3975, 1),cap_val))\n#combine_val = np.hstack((img_name_val.reshape(65, 1),cap_val))\ngroup_val_cap = npi.group_by(combine_val[:, 0]).split(combine_val[:, 1:])\ngroup_val_img = npi.group_by(combine_val[:, 0]).split(combine_val[:, 0])[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(group_val_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create \n- <font size=\"4\"><b>train_dataset</b></font> for training the model\n- <font size=\"4\"><b>valid_dist_dataset</b></font> for keeping track of validation cross entropy loss (with teacher forcing) during training\n- <font size=\"4\"><b>valid_dataset_bleu</b></font> for keeping track of validation bleu score (without teacher forcing) during training\n- <font size=\"4\"><b>valid_dataset_beam</b></font> for calculating final validation bleu score (with a beam search decoder and without teacher forcing) after training. My current implementation of beam search decoder is too slow to be incorporated in the training loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset(batch_size = BATCH_SIZE):\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((img_name_train, cap_train))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .repeat() # \n        .shuffle(batch_size*8, reshuffle_each_iteration=True)\n        .batch(batch_size, drop_remainder=False)\n        .prefetch(AUTO)\n    )\n    return strategy.experimental_distribute_dataset(train_dataset)\n\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((img_name_val, cap_val))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .cache()\n    .prefetch(AUTO)\n)\n\nvalid_dist_dataset = strategy.experimental_distribute_dataset(valid_dataset)\n\n\nvalid_dataset_bleu = (\n    tf.data.Dataset\n    .from_tensor_slices((group_val_img))\n    .map(decode_image)\n    .batch(64, drop_remainder=False)\n    .cache()\n    .prefetch(AUTO)\n)\n\nvalid_dataset_beam = (\n    tf.data.Dataset\n    .from_tensor_slices((group_val_img))\n    .map(decode_image)\n    .batch(1, drop_remainder=False)\n    #.cache()\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"nrvoDphgRPGd"},"cell_type":"markdown","source":"# 2.Model\n\nLet me borrow the following model introduction from [official Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/image_captioning ) with little modification for our tutorial: \n\n\nThe model architecture for our Image Captioning model is inspired by the [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper.\n\n* In this example, we extract the features from the lower convolutional layer of EfficientNet-B3 giving us a vector of shape (10, 10, 1536).\n* We squash that to a shape of (100, 1536). Effectively, we change 2D data from CNN into 1D sequential data for RNN\n* This vector is then passed through the RNN Decoder (which consists of an attention mechanism to look back to this vector in every word prediction).\n* In prediction, RNN (here GRU) using knowledge state (already predicted words) together with original data attended over the image to predict the next word.\n\nGiven an image like the example below, our goal is to generate a caption such as \"a surfer riding on a wave\".\n\n![Man Surfing](https://tensorflow.org/images/surf.jpg)\n\n*[Image Source](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg); License: Public Domain*\n\n![Prediction](https://tensorflow.org/images/imcap_prediction.png)"},{"metadata":{"id":"Q3TnZ1ToRPGV","trusted":true},"cell_type":"code","source":"'''\n# Here, we define all relevant parameters for model building and training.\n# Feel free to change these parameters according to your system's configuration\n'''\nLR = 3e-4\nif strategy.num_replicas_in_sync == 1:\n    BATCH_SIZE = 1 # in the case of CPU, make thing small just to be able to fit memory\n\nBUFFER_SIZE = 1000\nembedding_dim = 300 #embedding_matrix.shape[1] # 300 for Glove\nunits = 512 # GRU hidden vector #\nvocab_size = top_k + 1 # +1 for <unk>\n\n# attention_features_shape = total number of local parts in images that Decoder can attend. For EfficientNet-B3, it's 10x10 (HxW) = 100\nattention_viz_dim = 10 # \nattention_features_shape = attention_viz_dim**2 \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that to define Attention block, Encoder, Decoder, we use Keras Subclass API, not the usual functional API. This Subclass API is more powerful as it can handle complex model like encoder-decoder & its training process."},{"metadata":{"id":"ja2LFTMSdeV3","trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"AZ7R1RxHRPGf","trusted":true},"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        \n        \n        self.cnn0 = efn.EfficientNetB3(weights='noisy-student', \n                                      input_shape=target_size, include_top=False)\n        \n        \n        # e.g. layers[-1].output = TensorShape([None, 10, 10, 1536]) for B3 (not global pooling)\n        self.cnn = tf.keras.Model(self.cnn0.input, self.cnn0.layers[-1].output) \n        self.cnn.trainable = False\n        \n        # shape after fc == (batch_size, attention_features_shape, embedding_dim) >> this is my mistake, should be hidden instead of embedding_dim\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n        \n    # here, x is img-tensor of target_size\n    def call(self, x):\n        x = self.cnn(x) # 4D\n        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]) ) # 3D\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I commented out implementation of a second gru layer and a bidirectional layer in the Decoder below. Feel free to uncomment the second gru layer implementation if you are interested in using it but I don't believe a bidirectional layer makes sense for a decoder. "},{"metadata":{"id":"V9UbGQmERPGi","trusted":true},"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_matrix, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n    \n    self.vocab_size = embedding_matrix.shape[0]\n    \n    # new interface of pretrained embedding weights : https://github.com/tensorflow/tensorflow/issues/31086\n    # see also : https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n    self.embedding = tf.keras.layers.Embedding(self.vocab_size, embedding_matrix.shape[1], \n                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), \n                                               trainable=False,\n                                               mask_zero=True)\n    \n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    \n    '''\n    self.gru2 = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    '''\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    \n    # I don't believe bidirectional layer makes sense for a Decoder\n    '''\n    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform'))\n    self.fc1 = tf.keras.layers.Dense(2*self.units)\n    '''\n    \n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n  \n  # x=sequence of words\n  # features=image's extracted features \n  # hidden=GRU's hidden unit\n  def call(self, x, features, hidden):\n    \n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    \n    # passing the concatenated vector to the GRU (bidirectional)\n    #output, state_1, state_2 = self.gru(x)\n    \n    # passing the concatenated vector to the GRU \n    output, state = self.gru(x)\n    #output, state = self.gru2(output)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n    \n    #'state' is 'hidden state'\n    #'x' are 'predictions'\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we construct the encoder , decoder objects as well as loss function. For TPU custom-training, we have to be careful to create them under  `with strategy.scope():` so that Tensorflow will distribute our training batch to each processor appropriately.\n\nYou can learn more on training strategy [here](https://www.youtube.com/watch?v=jKV53r9-H14&list=UU0rqucBdTuFTjJiefW5t-IQ&index=43)."},{"metadata":{"id":"Qs_Sr03wRPGk","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # tf.keras.backend.clear_session()\n    embedding_matrix = build_matrix(tokenizer.word_index, glove_model, embedding_dim)\n    print(embedding_matrix.shape) # if not use stop-stem trick, num of unknowns is 495 (vs. current 287)\n    \n    encoder = CNN_Encoder(embedding_dim)\n    decoder = RNN_Decoder(embedding_matrix, units, vocab_size)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none') \n    # Set reduction to `none` so we can do the reduction afterwards and divide by\n    # global batch size.\n\n    def loss_function(real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        loss_ = loss_object(real, pred)\n\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n        \n        # About why we use `tf.nn.compute_average_loss`, please check this tutorial\n        # https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\n#         loss_ = tf.reduce_mean(loss_)\n        loss_ = tf.nn.compute_average_loss(loss_, global_batch_size=BATCH_SIZE)\n        \n        return loss_","execution_count":null,"outputs":[]},{"metadata":{"id":"PHod7t72RPGn"},"cell_type":"markdown","source":"## 2.1 Define Training Step\n\nFor custom training, we have to define the `train_step` function which taking each training batch as input, and return loss as output. For TPU, we also have to make one extra-effort to call `strategy.run` (previously named `strategy.experimental_run_v2`) to divide the global batch into sub-batch for each TPU processor.\n\nIn `train_step`, here's what we do :\n\n* The encoder output, hidden state(initialized to 0) and the decoder input (which is the  `<start>` token) is passed to the decoder.\n* The decoder returns the predictions and the decoder hidden state.\n* The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n\nNote that we use \"teacher-forcing\" training where we input the correct caption for each input. This is different in actual inference stage, where we don't know the real caption, and have to input the previously-predicted word as input. Teacher-forcing make training easier, but will not be very robust. In some advanced research, some papers suggest to transitition from teacher-forcing to actual-inference training in the middle of training process. Along those lines, we have implemented linear scheduled sampling as explained in the paper https://arxiv.org/pdf/1506.03099.pdf.\n\nAfter that we define `valid_step` in a similar way.\n"},{"metadata":{"id":"sqgyz2ANKlpU","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    @tf.function\n    def train_step(img_tensor, target, ss_prob):\n        loss = 0\n\n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = decoder.reset_state(batch_size=target.shape[0])\n\n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n        with tf.GradientTape() as tape:\n            features = encoder(img_tensor)\n\n            for i in range(1, target.shape[1]):\n                # passing the features through the decoder\n                predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n                loss += loss_function(target[:, i], predictions)\n                \n                #linear scheduled sampling\n                #https://arxiv.org/pdf/1506.03099.pdf\n                #https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n                if tf.random.uniform(shape=(1,)) < ss_prob:\n                #if tf.random.uniform(shape=(1,)) < 100: # temporary line of code!!!!!!\n                    # using teacher forcing\n                    dec_input = tf.expand_dims(target[:, i], 1)\n                else:\n                    # no teacher forcing\n                    predicted_ids = tf.random.categorical(predictions, 1, dtype=tf.int32)\n                    predicted_ids = tf.reshape(predicted_ids, (-1,))\n                    dec_input = tf.expand_dims(predicted_ids, 1)\n\n        total_loss = (loss / int(target.shape[1]))\n\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n        gradients = tape.gradient(loss, trainable_variables)\n\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n        return loss, total_loss\n    \n    @tf.function\n    def distributed_train_step(inputs, ss_prob):\n\n        (images, labels) = inputs\n#         loss = strategy.experimental_run_v2(train_step, args=(images, labels))\n        loss = strategy.run(train_step, args=(images, labels, ss_prob))\n        \n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    valid_loss = tf.keras.metrics.Sum()\n    \n    @tf.function \n    def val_step(img_tensor, target, teacher_forcing=True):\n        loss = 0\n        batch = target.shape[0] # BATCH_SIZE//strategy.num_replicas_in_sync #\n        hidden = decoder.reset_state(batch_size= batch)\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch, 1)\n        features = encoder(img_tensor)\n      #   print(features.shape) # (BATCH_SIZE, IMG_FEAT_LEN, ENCODER_HID) = 64 100 256\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n\n            # use teacher forcing when calculating val CE loss\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n        avg_loss = (loss / int(target.shape[1]))\n        return loss, avg_loss\n    \n\n    @tf.function\n    def cal_val_loss(val_dataset):\n        # target.shape = (64,49) = (Per Replica BATCH_SIZE?, SEQ_LEN)\n        val_num_steps = len(img_name_val) // BATCH_SIZE\n        valid_data_iter = iter(val_dataset)\n        valid_loss.reset_states()\n        \n        total_loss = 0.0\n        for ii in tf.range(val_num_steps):\n            _, per_replica_val_loss = \\\n                strategy.run(val_step, args=next(valid_data_iter))\n            t_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_val_loss, axis=None)\n            total_loss += t_loss\n            \n        valid_loss.update_state(total_loss/val_num_steps)\n        return total_loss/val_num_steps\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are different types of decoding algorithms we can use for Natural Language Generation and thus image caption generation. Note when we use 'decoding' in this context we do not mean the RNN_Decoder as part of a encoder/decoder architecture. We are referring to decoding algorithms that handle the output of the RNN_Decoder. The medium article below offers a great overview of these decoding algorithms along with some nice visualizations. Implementations of \n- pure sampling method\n- top k sampling method\n- greedy method\n\nare below. Implementation of beam search decoder is given after the training loop to be used in final calculation of bleu score. My current implementation of beam search does not work with batch input, and thus is too slow to be incorporated in the training loop.\n\nhttps://medium.com/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pure_sampling_decoder(predictions):\n    predicted_ids = tf.random.categorical(predictions, 1, dtype=tf.int32)\n    predicted_ids = tf.reshape(predicted_ids, (-1,))\n    dec_input = tf.expand_dims(predicted_ids, 1)\n    return dec_input, predicted_ids\n\ndef top_k_sampling_decoder(predictions, k = 10):\n    top_k_predicted_probs, top_k_predicted_ids = tf.math.top_k(predictions, k = k, sorted = True)\n    sample_top_k_ids = tf.random.categorical(top_k_predicted_probs, 1, dtype=tf.int32)\n    predicted_ids = tf.gather_nd(top_k_predicted_ids, sample_top_k_ids, batch_dims = 1)\n    predicted_ids = tf.reshape(predicted_ids, (-1,))\n    dec_input = tf.expand_dims(predicted_ids, 1)\n    return dec_input, predicted_ids\n\ndef greedy_decoder(predictions):\n    predicted_probs, predicted_ids = tf.math.top_k(predictions, k = 1, sorted = True) \n    predicted_probs = tf.reshape(predicted_probs, (-1,))\n    predicted_ids = tf.reshape(predicted_ids, (-1,))\n    dec_input = tf.expand_dims(predicted_ids, 1)\n    return dec_input, predicted_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_step_bleu(img_tensor):\n    # img_tensor shape(batch_size, 299, 299, 3)\n    batch = img_tensor.shape[0] # BATCH_SIZE//strategy.num_replicas_in_sync #\n    hidden = decoder.reset_state(batch_size= batch)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch, 1)\n    features = encoder(img_tensor)\n  #   print(features.shape) # (BATCH_SIZE, IMG_FEAT_LEN, ENCODER_HID) = 64 100 256\n    sequence_predictions = []\n    for i in range(1, 47):\n        predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n        # no teacher forcing\n        dec_input, predicted_ids = pure_sampling_decoder(predictions)\n        #dec_input, predicted_ids = top_k_sampling_decoder(predictions)\n        #dec_input, predicted_ids = greedy_decoder(predictions)\n\n        sequence_predictions.append(predicted_ids)\n\n    sequence_predictions = tf.stack(sequence_predictions)\n    sequence_predictions = tf.transpose(sequence_predictions)\n    \n    # return sequence_predictions in order to calculate bleu score\n    return sequence_predictions ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Start training!!\n\nFor TPU training, the first epoch takes around 1hour. However, after that, with `tf.dataset.cache` power, it's just 3-4 mins /epoch so we will be able to finish 10 epochs training very fast!\n\nFor TPU, our batch_size is 64x8 =512. We have 154,624 captions to be trained, this will accomplish in 302 steps / epoch. For each 50 steps, we will calculate validation loss, and print out both training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run only if you don't want to train from scratch\nUSE_PREVIOUS_SAVE = True\n'''\nfor (batch, inputs) in tqdm_notebook(enumerate(train_dist_dataset)):\n    _, per_replica_train_loss = distributed_train_step(inputs)\n    break\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 30 # For TPU, 1st epoch takes 1hour, after that, with cache power, it's just 3-4 mins /epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Callback_EarlyStopping(MetricList, min_delta=0.1, patience=20, mode='min'):\n    #No early stopping for the first patience epochs \n    if len(MetricList) <= patience:\n        return False\n    \n    min_delta = abs(min_delta)\n    if mode == 'min':\n      min_delta *= -1\n    else:\n      min_delta *= 1\n    \n    #last patience epochs \n    last_patience_epochs = [x + min_delta for x in MetricList[::-1][1:patience + 1]]\n    current_metric = MetricList[::-1][0]\n    \n    if mode == 'min':\n        if current_metric >= max(last_patience_epochs):\n            print(f'Metric did not decrease for the last {patience} epochs.')\n            return True\n        else:\n            return False\n    else:\n        if current_metric <= min(last_patience_epochs):\n            print(f'Metric did not increase for the last {patience} epochs.')\n            return True\n        else:\n            return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm\ndef calc_bleu_score():\n    val_num_steps = len(group_val_img) // 64 + 1 # add 1 for the 'remainder' batch\n    valid_dataset_iter = iter(valid_dataset_bleu)\n    val_sequence_predictions = []\n    for i in tqdm(tf.range(val_num_steps)):\n        img_tensor = next(valid_dataset_iter)\n        sequence_predictions = val_step_bleu(img_tensor)\n        val_sequence_predictions.append(sequence_predictions)\n\n    list_of_hypotheses = np.concatenate(val_sequence_predictions, axis = 0) # list of hypotheses that corresponds to list of references.\n    new_hypotheses = []\n    for hypothesis in list_of_hypotheses:\n        new_hypothesis = []\n        for element in hypothesis:\n            if element == 4: #break after end token reached\n                break\n            new_hypothesis.append(element)\n        new_hypotheses.append(new_hypothesis)\n    list_of_hypotheses = new_hypotheses\n    \n    list_of_references = []\n    for references in group_val_cap:\n        references = references[:, 1:] #remove start token\n        new_references = []\n        for reference in references:\n            reference = reference.astype(np.int)\n            reference = np.delete(reference, np.where(reference == 0)) #remove pad tokens\n            reference = reference[:-1] #remove end token\n            new_references.append(list(reference))\n        list_of_references.append(new_references) # list of references for all sentences in corpus.\n\n    # calculate BLEU score\n    bleu_1 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(1.0, 0, 0, 0))\n    bleu_2 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.5, 0.5, 0, 0))\n    bleu_3 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.3, 0.3, 0.3, 0))\n    bleu_4 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    print('BLEU: %f' % corpus_bleu(list_of_references, list_of_hypotheses))\n    print('BLEU-1: %f' % bleu_1)\n    print('BLEU-2: %f' % bleu_2)\n    print('BLEU-3: %f' % bleu_3)\n    # BLEU-4 computes 1-gram through 4-gram scores and gives them equal weight to compute a final score\n    print('BLEU-4: %f' % bleu_4)\n    \n    return bleu_1, bleu_2, bleu_3, bleu_4\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run training loop below with 'USE_PREVIOUS_SAVE = True' if you plan to load weights further down. You may get error when loading weights if you haven't run the training loop. "},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    loss_plot = []\n    val_loss_plot = []\n    bleu_1_plot = []\n    bleu_2_plot = []\n    bleu_3_plot = []\n    bleu_4_plot = []\n    best_bleu_score = 0\n    start_epoch = 0\n    num_steps = len(img_name_train) // (BATCH_SIZE)\n    start = time.time()\n    total_loss = 0\n    epoch = 0\n    train_dist_dataset = get_training_dataset()\n    decrease_lr = 0\n    \n    if USE_PREVIOUS_SAVE: # if we use pretrained checkpoint, just end the train quickly\n        print('Use prev. save weights, so run for few epochs')\n        EPOCHS,num_steps = 1,1\n        \n    num_steps_accum = num_steps\n    print(num_steps, BATCH_SIZE, num_steps*BATCH_SIZE)\n    print(f'Starting lr: {optimizer.lr.numpy()}')\n    \n    calc_bleu_score()\n    \n    for (batch, inputs) in tqdm_notebook(enumerate(train_dist_dataset)): # by .repeat() this will indefinitely run\n            \n        if batch >= num_steps_accum:\n            epoch += 1\n            print('end of epoch ', epoch)\n            bleu_1, bleu_2, bleu_3, bleu_4 = calc_bleu_score()\n            bleu_1_plot.append(bleu_1)\n            bleu_2_plot.append(bleu_2)\n            bleu_3_plot.append(bleu_3)\n            bleu_4_plot.append(bleu_4)\n            \n            loss_plot.append(total_loss / num_steps_accum)    \n            print ('Epoch {} Loss {:.6f}'.format(epoch,total_loss/num_steps_accum))\n            print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n            print(f'New ss_prob = {1 - epoch/EPOCHS}')\n            \n            if num_steps_accum > num_steps*EPOCHS:\n                print('end of training!!')\n                break\n\n            num_steps_accum += num_steps\n            print('next numsteps ', num_steps_accum)\n            print('train_losses: ', loss_plot)\n            print('val_losses: ', val_loss_plot)\n            print('bleu_1_plot: ', bleu_1_plot)\n            print('bleu_2_plot: ', bleu_2_plot)\n            print('bleu_3_plot: ', bleu_3_plot)\n            print('bleu_4_plot: ', bleu_4_plot)\n            \n            stopEarly = Callback_EarlyStopping(bleu_4_plot, min_delta = 0.0001, patience = 3, mode = 'max')\n            if stopEarly:\n                if decrease_lr == 2:\n                    print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch, EPOCHS))\n                    print(\"Terminating training \")\n                    break\n                else:\n                    decrease_lr += 1\n                    print(f'Decrease lr from {optimizer.lr.numpy()} to {optimizer.lr.numpy()*0.3}')\n                    optimizer.lr.assign(optimizer.lr*0.3)\n            \n            if bleu_4 > best_bleu_score:\n                print('update best bleu score from %.4f to %.4f' % (best_bleu_score, bleu_4))\n                best_bleu_score = bleu_4\n                if not USE_PREVIOUS_SAVE:\n                    encoder.save_weights('encoder_best.h5')\n                    decoder.save_weights('decoder_best.h5')\n\n                \n        #linear scheduled sampling\n        #https://arxiv.org/pdf/1506.03099.pdf\n        #https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n        _, per_replica_train_loss = distributed_train_step(inputs, tf.convert_to_tensor(1 - epoch/EPOCHS))\n        t_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_loss,\n                         axis=None)\n            \n        total_loss += t_loss\n        \n        \n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, t_loss.numpy() ))\n            \n            if USE_PREVIOUS_SAVE:\n                break\n\n            val_loss = cal_val_loss(valid_dist_dataset) #this only works with TPU!!!!\n            val_loss_plot.append(val_loss)\n            \n            print(\"val_loss: \", val_loss.numpy())\n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if USE_PREVIOUS_SAVE:\n    %%time\n\nprint(total_loss, t_loss)\n\nplt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Train Loss')\nplt.title('Loss Plot')\nplt.show()\n\n# plt.plot(loss_plot)\nplt.plot(val_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Val Loss')\nplt.title('Loss Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(bleu_1_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Bleu 1 Score')\nplt.title('Bleu 1 Score')\nplt.show()\n\nplt.plot(bleu_2_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Bleu 2 Score')\nplt.title('Bleu 2 Score')\nplt.show()\n\nplt.plot(bleu_3_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Bleu 3 Score')\nplt.title('Bleu 3 Score')\nplt.show()\n\nplt.plot(bleu_4_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Bleu 4 Score')\nplt.title('Bleu 4 Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_plot = [x.numpy() for x in loss_plot]\nval_loss_plot_ = [x.numpy() for i, x in enumerate(val_loss_plot) if i%3 == 0][:len(loss_plot)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bleu = pd.DataFrame(list(zip(list(range(1, epoch + 1)), train_loss_plot, val_loss_plot_, bleu_1_plot, bleu_2_plot, bleu_3_plot, bleu_4_plot)), \n                       columns = ['Epoch', 'TrainLoss', 'ValLoss', 'Bleu1', 'Bleu2', 'Bleu3', 'Bleu4'])\ndf_bleu.to_csv('results.csv', index=False)\ndf_bleu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    encoder.load_weights('./encoder_best.h5')\n    decoder.load_weights('./decoder_best.h5') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_bleu_score()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beam Width of 3 may take about 4.5 hours for 795 images. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom operator import itemgetter\nimport pdb\n# BEAM SEARCH\ndef val_step_beam(img_tensor, beam_width = 3):\n    # img_tensor shape(batch_size, 299, 299, 3)\n    batch = img_tensor.shape[0]\n    hidden = decoder.reset_state(batch_size = 1)\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch, 1)\n    features = encoder(img_tensor)\n  #   print(features.shape) # (BATCH_SIZE, IMG_FEAT_LEN, ENCODER_HID) = 1 100 256\n    sequence_predictions = []\n    \n    # (log(1), initialize_of_zeros)\n    k_beam = [(tf.constant(0, dtype=tf.float32), dec_input, hidden, sequence_predictions)]\n    \n    for i in range(1, 47):\n        all_k_beams = []\n        \n        for log_prob, di, h, sp in k_beam:\n            prediction, h, _ = decoder(di, features, h)\n            \n            # there is no softmax in the decoder\n            prediction = tf.nn.softmax(prediction)\n\n            # do not use no teacher forcing with val CE loss but use with bleu score\n            # no teacher forcing\n            #predictions.shape = (1, 18318)\n            predicted_probs, predicted_ids = tf.math.top_k(prediction, k = beam_width, sorted = True)\n            predicted_probs = tf.reshape(predicted_probs, (beam_width, 1))\n            predicted_ids = tf.reshape(predicted_ids, (beam_width, 1))\n            for (predicted_prob, predicted_id) in zip(predicted_probs, predicted_ids):\n                predicted_prob = tf.reshape(predicted_prob, (-1,))\n                predicted_log_prob = tf.math.log(predicted_prob)\n                log_prob = tf.math.add(log_prob, predicted_log_prob)\n\n                predicted_id = tf.reshape(predicted_id, (-1,))\n                di = tf.expand_dims(predicted_id, 1)\n\n                # axis 1 = batch axis \n                # shape should be (max_length, 1)\n                sp.append(predicted_id)\n                # whereas target is (batch, max_length)\n            \n                all_k_beams.append((log_prob, di, h, sp))\n        \n        #k_beam = sorted(all_k_beams, key = lambda x: x[0])[-beam_width:]\n        k_beam = sorted(all_k_beams, key=itemgetter(0))[-beam_width:] #faster\n        \n    sequence_predictions =  k_beam[-1][3]\n\n    sequence_predictions = tf.stack(sequence_predictions)\n    sequence_predictions = tf.transpose(sequence_predictions)\n    \n    # return sequence_predictions in order to calculate bleu score\n    return sequence_predictions \n\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm\ndef calc_bleu_score():\n    val_num_steps = len(group_val_img)\n    valid_dataset_iter = iter(valid_dataset_beam)\n    val_sequence_predictions = []\n    for i in tqdm(tf.range(val_num_steps)):\n        img_tensor = next(valid_dataset_iter)\n        t0 = time.time()\n        sequence_predictions = val_step_beam(img_tensor)\n        t1 = time.time()\n        total = t1 - t0\n        print(total)\n        val_sequence_predictions.append(sequence_predictions)\n\n    #calculate bleu score\n    list_of_hypotheses = np.concatenate(val_sequence_predictions, axis = 0) # list of hypotheses that corresponds to list of references.\n    new_hypotheses = []\n    for hypothesis in list_of_hypotheses:\n        new_hypothesis = []\n        for element in hypothesis:\n            if element == 4: #break after end token reached\n                break\n            new_hypothesis.append(element)\n        new_hypotheses.append(new_hypothesis)\n    list_of_hypotheses = new_hypotheses\n    \n    list_of_references = []\n    for references in group_val_cap:\n        references = references[:, 1:] #remove start token\n        new_references = []\n        for reference in references:\n            reference = reference.astype(np.int)\n            reference = np.delete(reference, np.where(reference == 0)) #remove pad tokens\n            reference = reference[:-1] #remove end token\n            new_references.append(list(reference))\n        list_of_references.append(new_references) # list of references for all sentences in corpus.\n\n    # calculate BLEU score\n    bleu_1 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(1.0, 0, 0, 0))\n    bleu_2 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.5, 0.5, 0, 0))\n    bleu_3 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.3, 0.3, 0.3, 0))\n    bleu_4 = corpus_bleu(list_of_references, list_of_hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    print('BLEU: %f' % corpus_bleu(list_of_references, list_of_hypotheses))\n    print('BLEU-1: %f' % bleu_1)\n    print('BLEU-2: %f' % bleu_2)\n    print('BLEU-3: %f' % bleu_3)\n    # BLEU-4 computes 1-gram through 4-gram scores and gives them equal weight to compute a final score\n    print('BLEU-4: %f' % bleu_4)\n    \n    return bleu_1, bleu_2, bleu_3, bleu_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"calc_bleu_score()","execution_count":null,"outputs":[]},{"metadata":{"id":"xGvOcLQKghXN"},"cell_type":"markdown","source":"# 3. Predict Caption!\n\nSo now the real fun time begin!\n\n* The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n* Stop predicting when the model predicts the end token.\n* And store the attention weights for every time step.\n\nFirst let us define all helper functions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def show_image(image,figsize=None,title=None):\n    \n    if figsize is not None:\n        fig = plt.figure(figsize=figsize)\n        \n    if image.ndim == 2:\n        plt.imshow(image,cmap='gray')\n    else:\n        plt.imshow(image)\n        \n    if title is not None:\n        plt.title(title)\n        \ndef show_Nimages(imgs,scale=1):\n\n    N=len(imgs)\n    fig = plt.figure(figsize=(25/scale, 16/scale))\n    for i, img in enumerate(imgs):\n        ax = fig.add_subplot(1, N, i + 1, xticks=[], yticks=[])\n        show_image(img)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RCWpDtyNRPGs","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n    \n    try:\n        hidden = decoder.reset_state(batch_size=1)\n    except:\n        hidden = decoder.layers[-1].reset_state(batch_size=1)\n        \n    img_tensor_val = tf.expand_dims(decode_image(image), 0)\n#     print(img_tensor_val.shape)\n    features = encoder(img_tensor_val)\n#     print(features.shape)\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"id":"fD_y7PD6RPGt","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    \n    bits = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    temp_image = np.array(image)\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (attention_viz_dim, attention_viz_dim))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()\n    \n    return temp_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_all_captions(img_list, caps, rid):\n    orig = img_list[rid]\n    for rr in range(rid-5, rid+5):\n        image_name = img_list[rr]\n        if image_name == orig:\n            real_caption = ' '.join([tokenizer.index_word[i] for i in caps[rr] if i not in [0]])\n            print ('Real Caption:', real_caption)\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Prediction on Training set\n\nFirst, we can see how well our model learn from the training set. Does it overfit or not? \n\nRemember that we have 5 captions per image. To ensure that our model won't memorize some labels, we will print all captions associated with each image.\n\nNote that the ground-truth captions in Flickr are quite mostly quite non-simple. In contrast to other popular dataset like COCO where it can be much easier for our model.\n\nYou can also see on attention visualization that, by predicting each word in the image, our model most of the time focus on the correct part of the pictures. Amazing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# captions on the train set\nimgs = []\nfor ii in range(10):\n    rid = np.random.randint(0, len(img_name_train))\n    print_all_captions(img_name_train,cap_train,rid)\n    image = img_name_train[rid]\n    result, attention_plot = evaluate(image)\n    print ('Prediction Caption:', ' '.join(result))\n    img = plot_attention(image, result, attention_plot)\n    imgs.append(img)\n    if (ii+1) %2 == 0:\n        show_Nimages(imgs)\n        imgs = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Prediction on Validation Set\n\nThe real test for our model is here. Is it able to generalize to unknown images ?\nLet see it by yourself. Nevertheless, from the validation loss, we got, it seems that our model can explain each valid image as well as that of each training image.\n\nOverall, if we run this prediction many times (each times with 10 random new images), we can see that roughly our model understand many images concept like \"many people\", \"computer\", \"field\", \"dog\", \"girl\", \"mouth\", etc. or even action like \"leaning\", \"sitting\", \"jump\". etc. However, it does not know the grammar so the sentence it produces many time look quite strange!\n\nWe may be able to fix this by sending this incomplete sentence to the sentence-expert like GPT-2 and make it correct for us!"},{"metadata":{"id":"7x8RiPHe_4qI","trusted":true},"cell_type":"code","source":"# captions on the validation set\nimgs = []\nfor ii in range(10):\n    rid = np.random.randint(0, len(img_name_val))\n    print_all_captions(img_name_val,cap_val,rid)\n    image = img_name_val[rid]\n    result, attention_plot = evaluate(image)\n    print ('Prediction Caption:', ' '.join(result))\n    img = plot_attention(image, result, attention_plot)\n    imgs.append(img)\n    if (ii+1) %2 == 0:\n        show_Nimages(imgs)\n        imgs = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction on Custom Image"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"path = '../input/coco2017/val2017/val2017/000000000139.jpg'\nresult, attention_plot = evaluate(path)\nprint ('Prediction Caption:', ' '.join(result))\nimg = plot_attention(path, result, attention_plot)\nshow_Nimages([img])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}