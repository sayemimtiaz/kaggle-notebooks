{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Word Association Analysis\n\n    A. Louwe \n    https://www.github.com/louweal"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nIn this notebook we analyse word associations from online word association games. The word association game is a game frequently played on internet forums. In this game, each player posts a word that was his (first) association for the word in the previous post (e.g. user1 writes 'green', user2 posts 'grass', user3 posts 'tree' and so on). \n\nWe analyse these word associations to obtain new insights into structure of the mental lexicon and simply because it is fun to analyse this type of data. ;)"},{"metadata":{},"cell_type":"markdown","source":"## Dataset & Exploratory Data Analysis\nWe collected this data in 2017 using ScraPy (web scraping) from multiple sources. All data was shuffled and anonymized. \n\nThe dataset contains the following five columns:\n- _author_ : an numeric identifier for each user\n- _word1_ : the cue\n- _word2_ : the association\n- _source_ : abbreviation of the source name \n- _sourceID_ : an numeric identifier for the source"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # data visualization\nfrom collections import Counter\nimport os\nimport csv\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\ndf = pd.read_csv('/kaggle/input/wordgame/wordgame_20180814.csv') \npal = sns.color_palette(\"Paired\", 10) # color palette\ndf.sample(5, random_state=24)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of word associations in this dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word associations are scraped from the following sources:\n- AC : AspiesCentral (https://www.autismforums.com/forums/)\n- BC : BleepingComputer (https://www.bleepingcomputer.com/forums/)\n- CC : ClassicComics (https://classiccomics.org/)\n- ECF : E-CigaretteForum (https://www.e-cigarette-forum.com/)\n- GOG : Online game store (https://www.gog.com/forum)\n- LEF : Learn English Forum (https://learnenglish.vanillacommunity.com) \n- SAS : Social Anxiety Support (https://www.socialanxietysupport.com/forum/)\n- TF : The Fishy (https://forum.thefishy.co.uk/) \n- U2 : @U2 Forum (https://forum.atu2.com/)\n- WP : Wrong Planet (https://wrongplanet.net/forums/)\n\nMost data is (American) English and includes post from people from many different countries. LEF is a forum for people who want to learn English and therefore word associations from this source could contain more mistakes. Data from TF contains British English, since this is a forum for British football supporters. AC and WP are internet forums for people with autism, which might also effect the type of word associations. \n\nNumber of word association from each source:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sources_names = [\"AS\", \"BC\", \"CC\",\"ECF\",\"GOG\",\"LEF\",\"SAS\",\"TF\",\"U2\",\"WP\"] \n#plt.figure(figsize=(16, 4))\n\nsns.set(rc={'figure.figsize':(16,4)})\nax = sns.barplot(df.source.value_counts().sort_index(), sources_names, orient=\"h\", palette=pal)\nax.set_title(\"Sources\")\nax.set_xlabel('Word association count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of unique users:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.author.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of word association count per user:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 4))\nmedian_wa = df.author.value_counts().median()\nmean_wa = df.author.value_counts().mean()\nplt.axvline(median_wa, color='k', linestyle='dashed', linewidth=1)\nplt.text(median_wa+30, 1, \"median: \"+str(int(median_wa)))\nplt.axvline(mean_wa, color='k', linestyle='dashed', linewidth=1)\nplt.text(mean_wa+30, 3, \"mean: \"+str(int(mean_wa)))\ng= plt.hist(df.author.value_counts(), bins=50, color=\"orange\")\nax = plt.gca()\n#ax.set_xscale('log')\nax.set_yscale('log')\nplt.title('Distribution of number of word associations per user')\nplt.xlabel('Word assocation count per user')\nplt.ylabel('Frequency (logaritmic)')\nplt.gca()\nplt.xlim([1, 12000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Methodology"},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    # replace\n    x = x.replace('\\n',\" \") # (wrongplanet)\n    x = x.replace(\"`\", \"'\")\n    x = x.replace(\"$$\", \"$\") #allows $-signs but not multiple\n\n    #remove \n    for symbol in [\"_\",\"~\",\"^\",\"xd\"]:\n        x = x.replace(symbol, \"\")\n        \n    if(x.find(\"said:\") > 1):  #classic comics: removes posts containing quotes\n        return ''\n    \n    # remove everything following ...\n    for symbol in [\"quote:\",\"sent from my\",\"edited by\",\"posted via\",\n                   \"/\",\"(\",\",\",\"*\",\"\\u00a0\",\"--\",\"*\",'\"',\".\",\"!\",\"?\",\"=\",\"[\",\"{\",\":\",\";\",\">\",\"<\"]:\n        x = x.split(symbol)[0]\n\n    #remove leading characters\n    for symbol in [\"+\",\"-\",\"&\",\"'\",\" \"]:\n        x = x.lstrip(symbol)\n\n    #remove trailing characters\n    for symbol in [\"-m\", \"'\", \" \", '\\u00a0']:\n        x = x.rstrip(symbol)\n    return x\n\n# convert word columns to string\ndf['word1'] = df['word1'].astype(str) \ndf['word2'] = df['word2'].astype(str)\n\n# delete all words containing non-ascii characters\ndf['word1'] = df['word1'].apply(lambda x: bytes(x, 'utf-8').decode('ascii','ignore'))\ndf['word2'] = df['word2'].apply(lambda x: bytes(x, 'utf-8').decode('ascii','ignore'))\n\n#convert all to lowercase\ndf['word1'] = df['word1'].apply(lambda x: x.lower())\ndf['word2'] = df['word2'].apply(lambda x: x.lower())\n\n# clean data\ndf['word1'] = df['word1'].apply(lambda x: preprocess(x))\ndf['word2'] = df['word2'].apply(lambda x: preprocess(x))\n\n# replace empty word with np.NaN\ndf = df.replace('',np.NaN)\n\n# drop all pairs containing NaN values\ndf = df.dropna(axis=0, how='any').reset_index(drop=True)\n\n# cut off all data longer than 25 characters, since words are rarely longer \ndf['word1'] = df['word1'].apply(lambda x: x[:25] if len(x)>25 else x)\ndf['word2'] = df['word2'].apply(lambda x: x[:25] if len(x)>25 else x)\n\n# remove pairs with identical words\ndf = df[df.word1 != df.word2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nlong_string = ','.join(list(df['word2'].values))\n\nwordcloud = WordCloud(background_color=\"honeydew\", width=1000, height=300, max_words=5000, random_state=24)\nwordcloud.generate(long_string)\nwordcloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save cleaned/preprocessed data\ndf.to_csv(\"/kaggle/working/wordgame_clean.csv\", sep=',', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"### Word frequency"},{"metadata":{},"cell_type":"markdown","source":"Number of unique words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.drop_duplicates(['word2']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute term frequency (TF) feature\nnum_terms = len(df)\ndf['tf'] = df.groupby(['word2'])['word2'].transform('count')\ndf['rtf'] = df.tf.apply(lambda x: x*100/num_terms) # relative term frequency (percentage!)\nfrequent_words = df.drop_duplicates(subset=['word2']).sort_values(by=['tf'], ascending=False).head(20)\nfrequent_words = frequent_words[::-1] # reverse \n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_words.word2, frequent_words.rtf, color=\"orange\", alpha=0.4)\nfor i in range(0,len(frequent_words)):\n    ax.text(frequent_words.rtf.iloc[i], i, frequent_words.tf.iloc[i], ha='right', va='center')\n    ax.text(0.001, i, frequent_words.word2.iloc[i], ha='left', va='center') \nax.set_yticks([])\nplt.title(\"Most frequent words\")\nplt.xlabel('Relative term frequency')\nplt.ylabel('')\ntestplt = plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Zipfs law\nAccording to Zipfs law, the frequency of any word (in a large sample of words) is inversely\nproportional to its rank in the frequency. "},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = np.array(df.word1.value_counts())\ntokens = np.array(df.word1.value_counts().index)\nranks = np.arange(1, len(counts)+1)\nindices = np.argsort(-counts)\nfrequencies = counts[indices]\n\nplt.figure(figsize=(10, 6))\n#plt.plot([1, 10e2], [10e2, 1], color='grey', linestyle='--', linewidth=1) # adds a diagonal line to the plot\nplt.loglog(ranks, frequencies, marker=\".\", alpha=0.4, c=\"orange\")\nfor n in list(np.logspace(-0.5, np.log10(len(counts)), 25).astype(int))[:-1]:\n    dummy = plt.text(ranks[n], frequencies[n], \" \" + tokens[indices[n]], verticalalignment=\"bottom\", \n                     horizontalalignment=\"left\")\nplt.title(\"Zipfs plot\")\nplt.xlabel(\"Frequency rank\")\nplt.ylabel(\"Absolute frequency\")\nplt.xlim(1, 10e5)\nplt.ylim(1, 10e2)\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word association frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['pair'] = df.apply(lambda x: str(x.word1) + \":\" + str(x.word2), axis=1)\ndf['pf'] = df.groupby(['pair'])['pair'].transform('count')\ndf['rpf'] = df.pf.apply(lambda x: x*100/num_terms) # ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_pairs = df.drop_duplicates(subset=['pair']).sort_values(by=['pf'], ascending=False).head(20)\nfrequent_pairs = frequent_pairs[::-1]\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_pairs.word2, frequent_pairs.rpf, color=\"orange\", alpha=0.4)\nfor i in range(0,len(frequent_pairs)):\n    ax.text(frequent_pairs.rpf.iloc[i], i, frequent_pairs.pf.iloc[i], ha='right', va='center')\n    ax.text(0, i, frequent_pairs.pair.iloc[i], ha='left', va='center') \nax.set_yticks([])\nplt.title(\"Most frequent word associations\")\nplt.xlabel('Relative term frequency')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Syntactic Features\n#### Word length"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['len1'] = df['word1'].apply(lambda x:len(x))\ndf['len2'] = df['word2'].apply(lambda x:len(x))\ndf['ldiff'] = abs(df['len1'] - df['len2']) # length difference between word1 and word2 \n\nplt.figure(figsize=(16, 4))\nf = plt.hist(df.len2, bins=np.arange(0,25), color='orange') \nplt.axvline(df.len2.mean(), color='k', linestyle='dashed', linewidth=1)\nplt.text(df.len2.mean()+0.1, 1000, str(round(df.len2.mean(),2)))\nplt.title('Word length distribution')\nplt.xlabel('Word length')\nplt.ylabel('Frequency')\naxes = plt.gca()\naxes.set_xlim([0,27])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common affixes\nWord association can be either semantical (e.g. 'blue' -> 'sky') or syntactic (e.g. *'bowels' -> 'vowels'*). Here we find the common prefixes and suffixes (e.g. in the previous example the suffix is *'owels'*) and the length of these common affixes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find common prefix\ndf['prefix'] = df.apply(lambda r: os.path.commonprefix([r.word1, r.word2]), axis=1)\ndf['pl']= (df['prefix'].apply(lambda x: len(x))) # compute common prefix length\n\n# find common suffix\ndf['suffix'] = df.apply(lambda r: os.path.commonprefix([r.word1[::-1], r.word2[::-1]]), axis=1)\ndf['suffix'] = df['suffix'].apply(lambda x:x[::-1]) #un-reverse suffix\ndf['sl']= (df['suffix'].apply(lambda x: len(x))) # compute common affix length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_prefixes = pd.DataFrame(df.prefix.value_counts()[1::].head(20))\nfrequent_prefixes['pc'] = frequent_prefixes['prefix']\nfrequent_prefixes['prefix'] = frequent_prefixes.index\nfrequent_prefixes = frequent_prefixes.iloc[::-1] # reverse dataframe\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_prefixes.prefix, frequent_prefixes.pc, color=\"orange\", alpha=0.4)\nfor i in range(0, len(frequent_prefixes)):\n    ax.text(frequent_prefixes.pc.iloc[i]+10, i, frequent_prefixes.prefix.iloc[i]+\"-\", ha='left', va='center')\nax.set_yticks([])\nplt.title(\"Most frequent prefixes\")\nplt.xlabel('Prefix frequency')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_suffixes = pd.DataFrame(df.suffix.value_counts()[1::].head(20))\nfrequent_suffixes['sc'] = frequent_suffixes['suffix']\nfrequent_suffixes['suffix'] = frequent_suffixes.index\nfrequent_suffixes = frequent_suffixes.iloc[::-1] # reverse dataframe\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_suffixes.suffix, frequent_suffixes.sc, color=\"orange\", alpha=0.4)\nfor i in range(0, len(frequent_suffixes)):\n    ax.text(frequent_suffixes.sc.iloc[i]+10, i,  \"-\"+frequent_suffixes.suffix.iloc[i], ha='left', va='center')\nax.set_yticks([])\nplt.title(\"Most frequent suffixes\")\nplt.xlabel('Suffix frequency')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Levenstein Distance\n\nLevenshtein distance (also called Edit Distance) is a measure of the similarity between two strings. It is minimum the number of deletions, insertions, or substitutions required to transform the first string into the second."},{"metadata":{"trusted":true},"cell_type":"code","source":"import Levenshtein\ndf['edit'] = df.apply(lambda r:Levenshtein.distance(r.word1, r.word2), axis=1)\n# rough normalization of edit distances\ndf['norm_edit'] = df.apply(lambda r:r.edit/max(r.len1,r.len2), axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare df for annotation\neditdf = df.drop_duplicates(['pair'])\n\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\n\n# scatter all points (including duplicate pairs)\nax.scatter(df.pl, df.edit, alpha=0.1, s=50, c=\"orange\")\n\n# annotate unique points only\nfor i in range(0, len(editdf), 3009):\n    if editdf.pl.iloc[i] > 0 and editdf.edit.iloc[i] < 6:\n        ax.annotate(editdf.pair.iloc[i], (editdf.pl.iloc[i], editdf.edit.iloc[i]))\n        \nplt.title(\"Prefix lengths and edit distances\")\nplt.xlabel(\"Common prefix length\")\nplt.ylabel(\"Levenstein distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(df.sl, df.edit, alpha=0.1, s=50, c=\"orange\")\n\nfor i in range(0, len(editdf),3003):\n    if editdf.sl.iloc[i] > 1 and editdf.edit.iloc[i] < 10:\n        ax.annotate(editdf.pair.iloc[i], (editdf.sl.iloc[i], editdf.edit.iloc[i]))\nplt.title(\"Suffix lengths and edit distances\")\nplt.xlabel(\"Common suffix length\")\nplt.ylabel(\"Edit distance\")\n#ax.gca()\nplt.xlim([-1,25])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This concludes the syntactic feature extraction. In the next section we move on to the semantic features such as word similarity. :)"},{"metadata":{},"cell_type":"markdown","source":"## Semantic Features\nHere we extract features related to word meaning. We start with word embedding models."},{"metadata":{},"cell_type":"markdown","source":"### Word Embedding Model (Word2Vec)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the documents for model training\ncopydf = df.copy() # copy the data\ncopydf['word2'] = copydf.word2.apply(lambda x: [x]) \nclouddf = copydf.groupby(['word1']).agg({'word2':'sum'}).reset_index() # group by cue\n# add cue word also to word list, as many times as it has associations\nclouddf['document'] = clouddf.apply(lambda r: [r.word1]*len(r.word2) + r['word2'], axis=1)\n\n# create W2V model\nw2v_model = Word2Vec(clouddf.document, size=300, min_count=1, sg=1, workers=10)\n\n# create word corpus\ncorpus = list((dict(Counter(df.word1.tolist()).most_common()).keys()))[::-1]\n\n# add word vector to 4000 most frequent words (t-sne can't handle the full dataset)\ncorpusdf = pd.DataFrame(corpus[-4000::], columns=['word'])\ncorpusdf['wordvector'] = corpusdf.word.apply(lambda x: w2v_model.wv[x])\n\n# Compute reduced word-vectors using t-sne\ntsnemodel = TSNE(random_state=42).fit_transform(corpusdf['wordvector'].tolist()) \ncorpusdf['x'] = tsnemodel[:,0]\ncorpusdf['y'] = tsnemodel[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot \ncorpusdf = corpusdf[::-1] # reverse\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(16)\nsc = ax.scatter(corpusdf.y, corpusdf.x, marker='o', c=corpusdf.index, \n                cmap=plt.get_cmap('plasma'), s=180, alpha=0.1)\nfor j in range(0,4000,19): #len(corpusdf), 20):\n    ax.annotate(corpusdf.word.iloc[j], (corpusdf.y.iloc[j], corpusdf.x.iloc[j]), fontsize='9')\nplt.title(\"Reduced word vectors (W2V + t-SNE)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sim'] = df.apply(lambda r:w2v_model.wv.similarity(r.word1, r.word2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 3))\nfor i in [6,8,9]:\n    s = df[df['sourceID']==i]\n    plt.axvline(s.sim.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s.sim, label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Word similarity distribution')\nplt.xlabel(\"Word2Vec similarity\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre-trained Word Embedding model\nNext make a more semantically meaningful word embedding, that is not influenced by the word frequencies in the dataset, by using a pre-trained model that is trained on different data. We use this pre-trained Google News corpus word vector model: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.keyedvectors import KeyedVectors\n# this might take a while, and requires a computer with a decent amount of memory (16+ GB)\nw2v_news_model = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace spaces by underscores (word2vec convention)\ndf['word1w2v'] = df.word1.apply(lambda x:(x.replace(\" \", \"_\")))\ndf['word2w2v'] = df.word2.apply(lambda x:(x.replace(\" \", \"_\"))) \n# check if model contains words\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab)) \nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100/len(df)))\n\n# apply title capitilization to words not found in the model\ndf.loc[~df.inw2v1, 'word1w2v'] = df.loc[~df.inw2v1].word1w2v.apply(lambda x:x.title())\ndf.loc[~df.inw2v2, 'word2w2v'] = df.loc[~df.inw2v2].word2.apply(lambda x:x.title())\n# check if model contains the words (again)\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab))\nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100/len(df)))\n\n# uppercase words not found in the model\ndf.loc[~df.inw2v1, 'word1w2v'] = df.loc[~df.inw2v1].word1w2v.apply(lambda x:x.upper())\ndf.loc[~df.inw2v2, 'word2w2v'] = df.loc[~df.inw2v2].word2w2v.apply(lambda x:x.upper())\n# check if model contains the words (again)\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab))\nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100/len(df)))\n\n# check if both words in words association are found\ndf['inw2v'] = df.apply(lambda r:(r.inw2v1 & r.inw2v2), axis=1)\ndf = df.drop('inw2v1', 1) # remove\ndf = df.drop('inw2v2', 1) # remove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute similarity\ndf['news_sim'] = np.nan # similarity for unknown word pairs\ndf.loc[df.inw2v, 'news_sim'] = df.loc[df.inw2v].apply(lambda r:w2v_news_model.similarity(r.word1w2v, r.word2w2v), axis=1)\n# show pairs with highest similarity\ndf.drop_duplicates(subset=['pair']).sort_values(by=['news_sim'], ascending=False).head(5).pair.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove data without similarity scores for plotting\nw2vdf = df[df.inw2v==True] \n\nplt.figure(figsize=(16, 3))\nfor i in [6,8,9]: #[3,7]: #range(0,10): #[6,8,9]:\n    s = w2vdf[w2vdf['sourceID']==i]\n    plt.axvline(s.news_sim.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s.news_sim, label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Word similarity distribution (Google news vectors)')\nplt.xlabel(\"Word2Vec similarity\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n# create word corpus\ncorpus = list((dict(Counter(w2vdf.word1w2v.tolist()).most_common()).keys()))[::-1]\n\n# add word vector to 4000 most frequent words (t-sne can't handle the full dataset)\ncorpusdf = pd.DataFrame(corpus[-4000::], columns=['word'])\ncorpusdf['news_wordvector'] = corpusdf.word.apply(lambda x: w2v_news_model[x])\n\n# Compute reduced word-vectors using t-sne\ntsnemodel = TSNE(random_state=42).fit_transform(corpusdf['news_wordvector'].tolist()) \ncorpusdf['x'] = tsnemodel[:,0]\ncorpusdf['y'] = tsnemodel[:,1]\n\ncorpusdf = corpusdf[::-1] # reverse\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(16)\nsc = ax.scatter(corpusdf.y, corpusdf.x, marker='o', c=corpusdf.index, \n                cmap=plt.get_cmap('plasma'), s=180, alpha=0.1)\nfor j in range(0,4000,19): #len(corpusdf), 20):\n    ax.annotate(corpusdf.word.iloc[j], (corpusdf.y.iloc[j], corpusdf.x.iloc[j]), fontsize='9')\nplt.title(\"Reduced word vectors (W2V News vectors + t-SNE)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordNet\nWordNet is a lexical database for the English language. Here we use it to identify word pairs as synonyms, antonyms, hypernyms and hyponyms. We also identify nouns."},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfrom nltk.corpus import wordnet\n\ndef isSynonym(r):\n    synonyms = wordnet.synsets(r.word1)\n    lemmas = set(chain.from_iterable([w.lemma_names() for w in synonyms]))\n    return (r.word2 in lemmas)\n\ndef isAntonym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        for l in syn.lemmas():\n            if l.antonyms():\n                h.append(l.antonyms()[0].name().split('.')[0]) \n    return (r.word2 in h)\n\ndef isHypernym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        if syn.hypernyms():\n            h.append(syn.hypernyms()[0].name().split('.')[0])\n    return (r.word2 in h)\n\ndef isHyponym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        if syn.hyponyms():\n            h.append(syn.hyponyms()[0].name().split('.')[0]) \n    return (r.word2 in h)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify nouns \nnouns = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('n')} \ndf['noun1'] = df.word1.apply(lambda x:(x in nouns)) \ndf['noun2'] = df.word2.apply(lambda x:(x in nouns)) \n\n# identify synonyms\ndf['synonym'] = df.apply(isSynonym, axis=1)\n# identify antonyms\ndf['antonym'] = df.apply(isAntonym, axis=1)\n# identify hypernyms\ndf['hypernym'] = df.apply(isHypernym, axis=1)\n# identify hyponyms\ndf['hyponym'] = df.apply(isHyponym, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage of pairs with semantic relationship(s): "},{"metadata":{"trusted":true},"cell_type":"code","source":"dfsem = df[df.synonym | df.antonym | df.hypernym | df.hyponym]\nlen(dfsem)*100/len(df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# prepare for plotting\nsemdf = dfsem[~dfsem.news_sim.isnull()].drop_duplicates(['pair'])\n\n# some pairs multiple relationships! e.g. wound:hurt (last in this list defines its color)... \ncolumns = [semdf.synonym, semdf.antonym, semdf.hypernym, semdf.hyponym]\ncolors = [\"orange\", \"purple\", \"limegreen\", \"grey\"]\n\nsemdf['color'] = \"white\" # add column for colors\nfor i in range(0,4):\n    semdf.loc[columns[i], 'color'] = colors[i]\n    \n#plot \nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\n\n# hacking some labels into the plot\nfor i in range(0,4):\n    ax.scatter([], [], alpha=0.2, s=50, c=colors[i], label=columns[i].name)\n    \n# plot all points\nax.scatter(semdf.news_sim, semdf.pf, alpha=0.2, s=50, c=semdf.color, label=\"\")\n\n# draw mean line\nfor i in range(0,4):\n    ax.axvline(semdf[columns[i]].news_sim.mean(), color=colors[i], linestyle='dashed', linewidth=1)\n\n# annotate some points\nfor i in range(0, len(semdf), 5):\n    if semdf.pf.iloc[i] > 17:\n        ax.annotate(semdf.pair.iloc[i], (semdf.news_sim.iloc[i], semdf.pf.iloc[i]))\n        \nplt.title(\"Word associations with semantic relationships\")\nplt.xlabel(\"Similarity (news vectors)\")\nplt.ylabel(\"Pair frequency\")\nplt.legend(loc='upper right')\nplt.gca()\nplt.ylim(0,70)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save semantic features\ndf.to_csv(\"/kaggle/working/wordgame_semantics.csv\", sep=',', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Psycholinguistic features"},{"metadata":{},"cell_type":"markdown","source":"#### Age-of-Acquisition\nAge of Acquisition is a psycholinguistic variable referring to the age at which a word is typically learned. Source: http://crr.ugent.be/archives/806. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\n# read AoA data\nreader = csv.reader(open('/kaggle/input/wordgame/AoA_ratings_Kuperman.csv', 'r'))\n# create AoA dictionary\naoa_dict = {k:v for k,v in reader if v!='NA'}\n# add AoA feature to data\ndf['AoA'] = df['word2'].apply(lambda x:float(aoa_dict[x]) if x in aoa_dict else np.nan).astype('float') \n\nprint(\"Percentage of words with known Age-of-acquisition: \" + str(round(len(df[~df['AoA'].isnull()])/len(df)*100,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare a df for plotting\naoadf = df[~df.AoA.isnull()].drop_duplicates(['word2'])\n\n#plot \nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(aoadf.AoA, aoadf.tf, alpha=0.3, s=50, c=\"orange\")\nfor i in range(2, len(aoadf), 140):\n    if aoadf.tf.iloc[i] > 50 or aoadf.AoA.iloc[i] > 16:\n        ax.annotate(aoadf.word2.iloc[i], (aoadf.AoA.iloc[i], aoadf.tf.iloc[i]))\nplt.title(\"Age of acquisition vs. word frequency\")\nplt.xlabel(\"Age-of-acquisition\")\nplt.ylabel(\"Word frequency\")\nplt.gca()\nplt.ylim([0,700])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aoadf = df[~df.AoA.isnull()]\n\nplt.figure(figsize=(16, 5))\nfor i in [2,8,9]: #range(0,10,1):\n    s = aoadf[aoadf['sourceID']==i]\n    plt.axvline(s.AoA.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s['AoA'], label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Histogram of Age-of-acquisition scores')\nplt.xlabel(\"Age-of-acquisition\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Concreteness rating\nAnother phycholinguistic variable is the concreteness rating. (source: http://crr.ugent.be/archives/1330). Words like 'fire' have a high concreteness rating, whereas words like 'fail' and 'generic' have low concreteness ratings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# read concreteness data\nreader = csv.reader(open('/kaggle/input/wordgame/Concreteness_english.csv', 'r'))\n# create concreteness dictionary\nconc_dict = {k:v for k,v in reader if v!='NA'}\n# add concreteness feature to data\ndf['CR'] = df['word2'].apply(lambda x:conc_dict.get(x)).astype('float')\n# percentage of words with known concreteness rating\nlen(df[~df.CR.isnull()])/len(df)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare data for plotting\ncrdf = df[~df.CR.isnull()].drop_duplicates(['word2'])\n\n# remove rows with nan aoa's\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(crdf.CR, crdf.tf, alpha=0.3, s=50, c=\"orange\")\nfor i in range(0, len(crdf), 5):\n    if crdf.tf.iloc[i] > 200 and crdf.CR.iloc[i] < 4.9:\n        ax.annotate(crdf.word2.iloc[i], (crdf.CR.iloc[i], crdf.tf.iloc[i]))\nplt.title(\"Concreteness rating vs. word frequency\")\nplt.xlabel(\"Concreteness rating\")\n#plt.xticks([1.0,5.0])\nplt.ylabel(\"Word frequency\")\nplt.gca()\nplt.xlim([1,5.01])\nplt.ylim([0,700])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crdf = df[~df.CR.isnull()]\n\nplt.figure(figsize=(16, 5))\nfor i in [2,5,9]: #range(0,10,1):\n    s = crdf[crdf['sourceID']==i]\n    plt.axvline(s.CR.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s['CR'], alpha=0.5, label=s.iloc[0]['source'], color=pal[i])  \nplt.title('Histogram of Concreteness Ratings')\nplt.xlabel('Concreteness Rating')\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save psycholinguistic features\ndf.to_csv(\"/kaggle/working/wordgame_psycho.csv\", sep=',', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}