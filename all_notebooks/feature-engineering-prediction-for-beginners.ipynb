{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>End-to-Project 1: Housing Prices Model<h2> \n    \n<b>To Build a model of housing prices in California census data</b>\n\n\nDataset: Based on data from 1990 Califor\n\n<h2>Checklist for Machine Learning Projects:</h2>\n    \n\n1. Frame the problem and look at the big picture\n2. Get the data\n3. Explore the data to gain insights\n4. Prepare the data for Machine Learning Algorithms\n5. Explore many different model and shortlist the best ones\n6. Fine-tune model\n7. Present the solution\n8. Launch, monitor, and maintain the system","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Frame the Problem and Look at the Bigger Picture","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Datasets has following attributes for each block of group in California:\n- Population\n- Median Income\n- Median Housing Price\n- And More\n\n*Its important to know all the attributes and understand it\n\n\n<b>Step 1: Look at the bigger picture</b>\n\n- Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data\n- A block group typically has a population of 600 to 3,000 people \n- Let's call them districts\n\n<b>Objective</b>\n\nOur model should learn from this data and be able to predict the median housing price in any district\n\n<b> Questions to be Framed before starting the problem:</b>\n\n1. What exactly is the business objective?\n\n2. How does the company expect to use and benefit from this model? \n\n3. Do you want it for one time use or regular use?\n\n4. How are you solving the problem right now? It gives a reference performance, as well as insights on how to solve the problem.\n\n5. Is the Problem:\n- Supervised, Unsupervised or Reinforcement Learning?\n- Classification task, Regression task, or Something else?\n- Should we use batch learning or online learning techniques?\n\n<b> Above questions helps in determining:</b>\n\n1. How to frame the problem?\n\n2. Which algorithm to select?\n\n3. Performance measure to evaluate the model.\n\n4. Effort required to tweaking it.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n    \n<h3> For this prject: Answers of Questions Framed before starting the problem:</h3>\n\n<b>1. What exactly is the business objective?</b>\n\nThis model will be a part of data pipeline where the model will be used to predict the prices and fit into other system.\n\nThe model's outout will be feed to another machine learning system.\n\n\nUpstream components\n        \n        |\n        |       \n        V\n   \n   District Data \n        \n        |        \n        |        \n        V\n  \n  District Pricing - (Our Model will be used)\n        \n        |       \n        |        \n        V\n   \n   District Prices\n        \n        |        \n        |\n        V\n        \n   Investment Anaysis <--- Other Signals\n        \n        |        \n        |        \n        V\n   \n   Investments\n   \n<br>\n<br>\n\n\n<b>2. How does the company expect to use and benefit from this model?</b>\n\nIts will be making investmenst based on the investment anaysis provided by our model.\n\nThe system will determine:\n\n- Whether it is worth investing in a given area or now\n- Getting this right it critical as\n- It directly affects the revenue\n\n\n<b>3. Do you want it for one time use or regular use?</b>\n\nThe Model will be of one time use because the census data is collected every 10 years.\n\n\n<b>4. How is it solved right now?</b>\n\nDistrict housing prices are currently estmated manually by experts:\n\n- Team gethers up-to-date information about a district\n- Experts use complex rules to come up with an estimate\n- This is costly and time-consuming\n- Estimates are not great\n- Typical error rate is about 15%\n\n\n<b>5. Is the Problem:</b>\n<i>\n- Supervised, Unsupervised or Reinforcement Learning?\n- Classification task, Regression task, or Something else?\n- Should we use batch learning or online learning techniques?</i>\n\nThis is :\n- Supervised. Data has input(to train) and output(district median housing price).\n- Regression. Its a multivariate regression, as system will use multiple features.\n- Online or Batch can be figured out from the process, since the data comes every 10years this is a batch learning. And data is not so huge which can be easily fit in memory.\n\n\n<b>6. What is the Performance Measure?</b>\n\nRoot Mean Square Error\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Brief introduction to Statistical Inference</h2>\n\n\n1. Numerical:\n<ul>\n\n- Continuous: 1,2,34, 45 etc.\n\n- Discrete: Values are not part of a range\n</ul>\n\n\n2. Categorical:\n<ul> \n\n- Regular: Eg. Block Ids\n\n- Ordinal (Level have an inherent ordering. Priority in ordering): Eg. High, Low and Medium\n\n</ul>\n\n3. String: Sentiment Analysis like tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Random Process</h2>\n\nIn a random process we know what outcomes could happen, but we don't know which particular outcome will happen.\n\n<b>\n    \n    P(A) = Probability of event A\n    \n    0<=P(A)<=1\n    \n    In Random process:\n    P(A)=(Instances/Total Possiblilities)\n    \n</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Random Variable</h2>\n\n- Random variable is a variable:\n    - Value is unknown\n    - A fuction that assigns vaues to each of an experiment's outcomes\n    \n\n- Can be of multiple types depending upon the type of the quantity measured i.e.:\n    - Continuous\n    - Discrete\n    - Categorical etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Probability Distribution</h2>\n\n- A probability distribution assigns a:\n\n    - Probability to each of the possible outcomes of\n    - An random experiment, survey, or procedure of statistical inference\n    \n    - If we plot a histogram, the Probablity Distribution will form a 'Bell shaped Curve'\n    \n    \n<b>Bell Shaped Probability Distribution / Normal Distribution Curve can be of many types: </b>\n\n<b>By Plotting a Histogram</b>\n\n\n- <u>Normality - Skewness:</u>\n\nDistributions are skewed to the side of the long tail.\n\n    1. Left Skewed: Most instances towards left.\n    2. Symmetric: Most instances towards center.\n    3. Right Skewed: Most instances towards right.\n    \n<u>Here, in Normal Distribution:</u>\n\n<b>If Standard Deviation is low, curve is narrow,<br> if Standard Deviation is high, curve is spread out</b>\n\n<b>If Mean is high, the peak of the curve is high, <br>if mean is low peak of the curve is low</b>\n    \n- <u>Normality - Modal:</u>\n    1. Unimodal: Eg. Rolling of Two Dice.\n    2. Bimodal: Eg. ploting ages to two categories like students and teachers in school.\n    3. Uniform: Eg. In Credit Card number, ploting the last digit.\n    4. Multimodal: Eg. In a resturant, ploting the number of persons in various time instances of a day.\n\n<b>By Plotting a Box and Whisker Plot</b>\nhttps://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/stats-box-whisker-plots/v/reading-box-and-whisker-plots\n\nIt draws a box, such that it contains the 50% of the total data. It divides the Whisker Line into 4 Quartiles (Q1, Q2, Q3, and Q4)\n\nOrder the data in the Whisker Line from Min to Max and divide into 4 percentiles.\n\nThe Distance between the 1st quartile - Q1(25th percentile) of the data and 3rd quartile - Q3 (75th percentile)\n\nSpread of the central tendency, IQR = Q3 - Q1\n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Measure of Central Tendencies</h2>\nAlso known as point estimate. \n<br>*Sample statistics and Population Parameter.\n\n<b> General Types</b>\n\n    1. Mean: Arithmetic Average. Non Robust and Standard Deviation gets impacted with an outlier. And its Symmetric.\n    \n    \n    2. Median: Order the data and get the 50th Percentile. It is more stable(robust) than Mean, as its least effected by an outlier. Use IQR, box plot and Skewed.\n    \n    \n    3. Mode (Most Frequest Observation)\n\n<b>Quick Data Visualization for central tendencies.</b>\n\nmean < median : Left Skewed - Higher Probability of Occurance or Outlier on Right\n\nmean > median : Right Skewed - Higher Probability of Occurance or Outlier on Left\n\nmean = median : Symmetric - More Values near the center\n\n*based on bell curve of box curve mean is easily differentiable as compared to median.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Measure of Spread of Data</h2>\n\nVar = SD<sup>2</sup> = Sum<sup>n</sup><sub>i</sub> [(x<sub>i=1</sub> -  mean(x))<sup>2</sup>/ (n - 1)]\n   \n*We square to avoid canceling of the -ve and +ve values. \n\n*We square, to emphasize the differences from the central tendency, by increasing larger deviations more than smaller ones so that they are weighted more heavily.\n\n*Bigger Var means bigger deviation from the mean","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Normal Distribution:</h2>\n\n<b>If Standard Deviation is low, bell curve is narrow,<br> if Standard Deviation is high, bell curve is spread out</b>\n\n<b>If Mean is high, the peak of the bell curve is high, <br>if mean is low peak of the bell curve is low</b>\n\n<b>Total Population = Area under the bell curve</b>\n\n<b>Maximum Number of Observation = Height of Peak</b>\n\n<b>68-95-99.7% rule</b>:\nArea covered by Normal Distribution\n\n    - 68% of population: 1st standard deviation from top <br>(i.e. mean - sd to mean + sd) \n    - 95% of population: 2nd standard deviation from top <br>(i.e. mean - 2*sd to mean + 2*sd) \n    - 99.7% of population: 3rd standard deviation from top <br>(i.e. mean - 3*sd to mean + 3*sd)\n    \n    \n<b>Calculate Distance of Outlier from the Mean = (Value - Mean) /SD </b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Selecting Performance Measure - Root Mean Square Error (RMSE)</h2>\n\nThis performance measure, mostly fits for linear regression.\n\nWhen we draw the best fit line,\n\nRoot Mean Square Error = (1/Slope * Sum[(Actual Value - Predicted Value)<sup>2</sup>/Total Number of Observations])<sup>1/2<sup>\n\n<b>Predicted Values in Best-fit-line has the lowest Root Mean Square Error.</b>\n\n<b>Best fit line is the line which fits the predicted values of the linear regression and have the lowest distance from the actual values.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Get the Data\n\nDatabase Repository:\n1. UCI\n2. Kaggle\n3. Amazon\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Dataset Method 1: By Calling through function\n\n# HOUSING_PATH = 'Datasets/'\n\n# def load_housing_data(housing_path = HOUSING_PATH):\n#     csv_path = os.path.join(housing_path, 'housing.csv')\n#     return pd.read_csv(csv_path)\n\n# housing = load_housing_data()\n\nhousing = pd.read_csv('../input/dataset/housing.csv')\n    \nhousing.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Dataset Method 2: By Calling through function\n\n# housing = pd.read_csv('Datasets/housing.csv')\n# housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find the number of values in a categorical column\n\nhousing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation</b>\n\n#std - shows how desperse the values are\n\n<b>Points to note:</b>\n\n    1. 25% of the districts have a housng_median_age lower than 18\n\n    2. 50% of the districts have a housng_median_age lower than 29\n    \n    3. 75% of the districts have a housng_median_age lower than 37\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Plot a histogram\nWe can plot a histogram only on numericals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nhousing.hist(bins = 50 , figsize = (20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b>\n\n1. <u>Caped Data:</u> Data is caped after certain range of values so we can see sudden spike at the end of the graph for housing_median_age and median_housing_value.\n\n      After checking with the team, we got to know that the data has been scaled and capped at:\n    - 15 (actually 15.0001) for higher median incomes, and\n    - At 0.5 (actually 0.4999) for lower median incomes, and\n    - Capped Median age - 50\n    - Capped Median House value - 5000,00\n\n    Lesson: It is important to understand how your data was computed. ML model may never learn that prices never go beyond that limit, if we use it directly.\n    \n    <u>Solution:</u> \n    - Check with the team if this is the problem or not\n    - If team says they need precise predictions even beyond $500,000 then:\n        - Collect proper labels for districts whose labels were capped, Or\n        - Remove those labels from training as well as test dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Train and Test (Validation) Set\n\n- We split the data into two sets:\n    - Training Set 80%\n    - Test Set 20%\n- We Train the model on training set.\n- And Evaluate the performance of the model on the test set.\n\n<h3><u>Types of Bias</u></h3>\n<h3>Bias 1: Data Snooping bias</h3>\n\n- Brain is a an amazing pattern detecion system\n- Highly prone to overfitting\n- We may find interesting patterns in the test data\n- And select a particular Machine Larning Model\n- Our Model will be too optimistic\n- And will not perfor as well as expected \n\n\n<b>Solution</b> \n\n- We can avoid this Data Snooping Bias by creating the Data Split at the begining and use np.random.seed(42) to give a random seed during the spliting process so that the algorithm don't see the whole data\n\n- Or By creating own function\n\n<b>We will use train_test_split() from scklearn.model_selection </b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### How np.random.seed(42) works","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nprint(np.random.random())\nprint(np.random.rand())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It shows fixed the random state to one value so that the split function doesn't end up looking at all data and pick one, thereby making a purely random selection. \n\n- This also fixed the random selected value so that we don't get different split values everytime we run the function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#So here np.random.permutation creates an array of random integers under the given value.\n\nnp.random.permutation(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating own split function:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test(data, test_ratio):\n    shuffle_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data)*test_ratio)\n    test_indices = shuffle_indices[:test_set_size]\n    train_indices = shuffle_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will use train_test_split() from scklearn.model_selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nnp.random.seed(42)\ntrain_set, test_set = train_test_split(housing, test_size = 0.2, random_state=42)\n\nprint(len(train_set), ' - train\\n', len(test_set), ' - test')\n\ntest_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Bias 2: Sampling bias</h3>\n\n- Till now we have considered random sampling methods\n\n- Random Sampling method work fine if dataset is large enough. \n\n- Else we are at risk of introducing significt sampling bias.\n\n- We cannot pick a sample randomly, because sample must be a representation of all sets of data.\n\n\n- Types of Sampling Bias:\n    - Convenience Sample: Individuals who are easily accesible are more likely to be included in the sample.\n    - Voluntary response: Occurse when the sample have strong opinions on the issue.\n    - If only a (non-random) fraction of the randomly sampled people respond to a survey such that the sample is no longer representative of the population.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Solutions for Sampling Bias:\n\n1. Simple Random Sample (SRS):\n\n    Each case is equally likely to be selected. Eg. np.random.seed(4)\n    \n\n2. Stratified Sample:\n\n    Divide the population into homogenenous strata, then randomly sample from within each stratum. Here, each strata has same properties and then we randomly select from each of stratum.\n    \n\n3. Cluster Sample:\n\n    Divide the population clusters, randomly sample a few clusters, then randomly sample from within these clusters. Each cluster is a represntation of whole population.\n\n\n<b>What can be wrong when your algorithm is showing 99% accuracy?</b>\n\n<b>Problems:</b>\n1. Algorithm might be categorizing most of the things as the given category say 'cat'\n\n2. Or it can be a sampling bias i.e. the test sample which we choosed has most of the samples from same category.\n\n<b>Solution:</b>\n1. We mainly go for Stratified Sample.\n2. We will choose some other method of acuracy instead of root mean square.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Stratified Sampling\n\nA well conducted survey on 100 people in the US should:\n- Try to maintain the ratio of categories in the sample same as the main population.\n- Population is divided into Homogeneuos groups called strata\n- The Right number of instances is Sampled from each Stratum\n\n\n<b>Here in the Housing Problem</b>\n  1. We are advised that the housing_median_income is a very important attribute to predict median housing prices.\n  2. Then we have to do Stratified Sampling as it is a important feature.\n  3. The test set should be representative of various categories of median income.\n  4. We should not have too many strata, and each strata should be large enough.\n  \n  <b>Column - Feature, each feature is an attribute of our record. Every column represents a characteristics of datapoint.\n  \n  Rows - Instance. It represents a datapoint.</b>\n  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['median_income'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a Test Set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Creating 5 Stratas of the important column or feature </h3>\n\n\n<b>To reduce the range of datapoints, we first divide median_income feature by 1.5 so that:</b>\n1. We will not have too many strata\n2. And each stratum will be large enough\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ceil is used to round of the values in an array\nnp.ceil(housing['median_income']/1.5)\n\nhousing['income_cat'] = np.ceil(housing['median_income']/1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>After reducing the size of the important column and rounding off by using np.ceil(),we got 11 different stratas</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So in other to reduce the number of stratas, we Cape the data of 'income_cat' feature - the important column ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Labelling or Caping all the datapoint above 5 and make it 5.0 using .where function\n \nhousing['income_cat'].where(housing['income_cat']<5, 5.0, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Here, we successfully reduced the number of stratas from 11 to 5 Stratas.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Stratified Sampling \nUsing  StratifiedShuffleSplit from sklearn.model_selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified Sampling using Scikit-learn's StratifiedShuffleSplit Class\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 2, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n\n#Here .loc[] is used to access the rows based on the label names.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#income category proportion in test set generated with stratified sampling\n\nstrat_test_set['income_cat'].value_counts()/len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#income category proportion in full dataset\n\nhousing['income_cat'].value_counts()/len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets compare income category proportion in Stratified Sampling and Random Samplinga\n\ndef income_cat_proportions(data):\n    return data['income_cat'].value_counts()/len(data)\n\n\ntrain_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 42)\n\ncompare_props = pd.DataFrame({\n                'Overall': income_cat_proportions(housing),\n                'Stratified': income_cat_proportions(strat_test_set),\n                'Random': income_cat_proportions(test_set)\n                            }).sort_index()\n\ncompare_props['Rand.%error'] = 100 * compare_props['Random'] / compare_props['Overall'] - 100\ncompare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### While Creating Test Sample, StartifiedShuffle Split Sampling Gave Better better results than Trani_Test_Split, Random Split Sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_cat', axis = 1, inplace = True)\n    \nstrat_train_set    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we dropped the income_cat column which we used to create stratas so that it don't interfere during training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Explore the Data to gain insights\n\n- If the Trainingset is large Sample the training set to make manipulations easy and fast\n\n- Since our training set is small, we can directly work on full set.\n\n- Create a copy of traiing set first so that we can play with it without harming the training set\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind = 'scatter', x = 'latitude', y = 'longitude')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Obesrvation:</b>\n- Hard to see any particluar pattern\n- We can not visualize the places with high density points","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is why we have to reduce alpha(transparency) to 0.1\n\n# housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha ='0.1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611). Thanks to Wilmer Arellano for pointing it out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.4,\n             s = housing['population']/100, label = 'population', figsize = (10,7),\n             c = 'median_house_value', cmap = plt.get_cmap('jet'), colorbar = True,\n             sharex = False)\nplt.legend()\n\n#housing['population']/100 = Radius of the circle\n#cmap = plt.get_cmap('jet') here, jet is the rbg range of colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.image as mpimg\n# california_img = mpimg.imread('Images/california.png')\n# ax = housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', figsize =(10,7),\n#                   s= housing['population']/100, label = 'Population',\n#                   c = 'median_house_value', cmap = plt.get_cmap('jet'),\n#                   colorbar = True, alpha = 0.4,\n#                  )\n# plt.imshow(california_img, extent= [-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n# plt.ylabel('Latitude', fontsize = 14)\n# plt.xlabel('longitude', fontsize = 14)\n\n# prices = housing['median_house_value']\n# tick_values = np.linspace(prices.min(), prices.max(), 11)\n# cbar = plt.colorbar()\n# cbar.ax.set_yticklabels(['$%dk'%(round(v/1000)) for v in tick_values], fontsize = 14)\n# cbar.set_label( 'Median House Value', fontsize = 16)\n\n# plt.legend(fontsize = 16)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b>\n1. In Northern California the housing prices in coastal districts are not too high\n2. So its not a simple rule\n3. We have to take a look at the correlation\n\n<h3>Looking for correlation</h3> \n\nCorellation indicates the extend to which two or more variables fluctuate together\n\n<b>Types of Correlation:</b>\n1. When Corr() is 1, Perfect Positive Correlation.\n2. When Corr() is 0.9, High Positive Correlation.\n3. When Corr() is 0.5, Low Positive Correlation.\n4. When Corr() is -1, Perfect negative Correlation.\n5. When Corr() is -0.9, High negative Correlation.\n6. When Corr() is -0.5, Low negative Correlation.\n\nSince, the data is not too large we can compute the correlation using corr() method\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(corr_matrix, annot = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix['median_house_value']\ncorr_matrix['median_house_value'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b>\n\n1. Blocks in which median_house_value is higher, are the blocks where more median_income group of people is higher.\n\n2. Latitude is negatively correlated to the median_house_value.\n\n3. median_house_value is higher where total_rooms is higher.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#To plot a scater plot in entire housing DataFrame\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = ['median_house_value','median_income','total_rooms','housing_median_age', 'households', 'total_bedrooms','latitude' ]\n\nscatter_matrix(housing[attributes], figsize = (12,8))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observations:</b>\nHere, we can see median_income has most important correlation. So, let's plot them separately.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observations</b>\n1. Caped Data - At median_house_value 500000\n2. There few horizontal lines, we should remove corresponding districts to prevent algorithm from learning to reproduce these data quirks.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Attribute Combinations or Feature Engineering</h3>\n\n1. Our Mileage will vary with each project\n2. But the general ideas are similar\n3. Mostly done in second round of optimization\n4. It is the process of Combining attributes to form a new attribute which has strongest correlation.\n5. We may want to try out various attribute combinations\n    - Before preparing data for machine learning algorithms\n    \n    \n<b>Example:</b>\n1. total_rooms is not very useful if we don't know how many household are there. What about number of rooms per household?\n\n2. total_bedrooms is not useful. We want to compare to number of bedrooms per room\n\n3. Population per household alos seems an interesting attribute combination, number of people per household","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing ['rooms_per_household'] = housing['total_rooms']/housing['households']\nhousing ['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\nhousing ['population_per_household'] = housing['population']/housing['households']\nhousing.head()\nhousing.describe()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observations</b>\n1. median_house_value is lower for higher bedrooms_per_room","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind = 'scatter',\n             x = 'rooms_per_household',\n             y = 'median_house_value',\n             alpha = 0.2)\nplt.axis([0,5,0,520000])\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Prepare the data for Machine Learning Algorithms\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Data Cleaning\n- Let's revert to a clean (training set) strat_train_set \n\n- Drop the target value from training set. Here Target value is median_house_value\n \n- Seperate Independent and Dependent Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let’s revert to a clean training set\n\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set - Independent Features Dataset\nhousing_labels = strat_train_set[\"median_house_value\"].copy() #Dependent Features Dataset\n\n# Note drop() creates a copy of the data and does not affect strat_train_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_labels.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isn = housing.isnull()\nisn.any(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How to handle missing data:\n1. Get rid of corresponding districts where total_bedroom has missing values\n2. Get rid of the whole attribute\n3. Set the values to some value (zero, the mean, the median, etc.)\n4. We can fix the missng value by following methods of Pandas DataFrame:\n    - dropna(): drops missing values of a given subset = ['columnname']\n    - drop(): drops the entire given column\n    - fillna(): fills the missing value with the mean, mode or median","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let’s experiment with sample dataset for data cleaning\n\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head(100)\nsample_incomplete_rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Cleaning - Missing Values - sclearn Imputer Class","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Scikit Learn Provides inbuilt Impurter Class to take care of missing values\n- Create an instance of Imputer Class\n- Specify each attributes missing values should be replaced with the median of that attribute.\n- Strategy = Median","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Remove the Categorical Attributes before using Imputer</b>\n    \nHere, ocean_proximity feature of housing dataset has categorical attributes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Encoding Method 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nh = housing\nh.head()\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer([('on',OneHotEncoder(),[-1])], remainder = 'passthrough')\nh = ct.fit_transform(h)\nh.shape\nct","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Step1. Call imputer from sklearn.preprocessing\nRemove the Categorical Attributes before using Imputer.\n\n#### Step2. Fit the imputer Instance using fit()\nAfter calling fit(), imputer class computes the median of every attribute and stores the coputed medians in statistics_instance \n\n#### Step3. Transform","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the Categorical Attributes before using Imputer\n#housing_num - pandas table without categorical feature \n\nhousing_num = housing.drop('ocean_proximity', axis =1)\ntype(housing_num)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling the Imputer Class to fill missing\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting imputer instance using fit()\n\nimputer.fit(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this contains medians of all columns\n\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transform the training set into numpy array with no missing values using medians from the imputer.statistics_\n\nX = imputer.transform(housing_num)\nprint(type(X))\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Column Back to DataFrame from Numpy Array","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_tr = pd.DataFrame(X, columns = housing_num.columns)\nprint(type(housing_tr))\nhousing_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_tr.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How to Handle Text and Categorical Attributes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Most of ML works good with numerical values but not the strings or categorical values\n- Let's transform 'ocean_proximity' to numerical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat = housing['ocean_proximity']\nhousing_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ways to handle categorical values:\n\n<b>1. Pandas factorize() method - Ordinal:</b>\n\n- It gives value from 0 to infinity and assigns new numbers of they encounter new category.\n\n- It is good only for Ordnial Values - Meaning the values have priority like high, mid and low.\n\n- Not good for nominal\n\n- They trasforms panda DataFrame it to numpy array\n\n<b>2. Scikit Learn's OneHotEncoder method - Nominal:</b>\n- We have to feed the factorize() data to OneHotEncoder\n- (1 is hot) and (0 is cold)\n- Number of Categories = Number of Columns added by OneHotEncoder\n- The Column which is related to the outcome turns to 1 and others turn to 0.\n- This shouldn't be used on id column, id columns must be droped.\n- It can be used in columns like seasons, month etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert ocean_proximity to numbers\n# Use Pandas factorize()\n\n\nhousing_cat_encoded, housing_categories = housing_cat.factorize()\nhousing_cat_encoded[:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat_encoded.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))\nhousing_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(housing_cat_1hot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert to a dense array to Numpy Array\nhousing_cat_1hot.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(housing_cat_1hot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just run this cell, or copy it to your code, do not try to understand it (yet).\n# Definition of the CategoricalEncoder class, copied from PR #9151.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How to combine Attribues","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n        population_per_household = X[:, population_ix] / X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\nhousing_extra_attribs = pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\nhousing_extra_attribs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat_encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How to Create Custom Transformers\n\n1. Create a Class\n2. Implement three methods:\n    - fit()\n    - transform()\n    - fit_transform()\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer([('on',OneHotEncoder(),[1])], remainder = 'passthrough')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- It is used when the range of data has very large differences in ranges.\n\n\n- It used to transform the table with similar range of data.\n\n\n- Types:\n\n\n<b>1. Min-max scaling - Normalization</b>\n   \n   \n   - x' = {x - min(x)}/{max(x) - min(x)}\n   \n   - Normalization values are [0,1]\n   \n   - Normalization is doesnot work when the values are Audio signals, Pixel Values of Images, and Data with multiple Dimentions.\n   \n   - In normal distrituion of orginal or natural data is between 0 to 3, in that we use std deviation.\n\n<b>2. Standardization:</b>\n   \n   - x' = {x - mean(x)}/std\n   \n   - Resultant data has, mean(x) = 0 and std = 1\n   \n   - sklearn.preprocessing gives StandardScaler\n   \n   - They don't have a fixed data like normalization.\n   \n   - Neural networks cannot use StandardScaler because it required 0 and 1\n           \n           ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's build a pipeline for preprocessing the numerical attributes:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(housing_num_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Overview of  Sclearn Packages","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Transformation Pipelines - DataFrameSelector\n\n- Scikit-Learn doesn't handle DataFrames\n- How to select columns from pipeline?\n- Let's create a class DataFrame Selector:\n    - To select numerical or categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's join all these components into a big pipeline that will preprocess both the numerical and the categorical features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n        ('selector', DataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\ncat_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attribs)),\n        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline.fit_transform(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Full Pipeline - Combining num_pipeline and  cat_pipeline using sklearn.pipeline import FeatureUnion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list = \n                             [('num_pipeline',num_pipeline),\n                             ('cat_pipeline',cat_pipeline)\n                             ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared = full_pipeline.fit_transform(housing)\nprint(housing_prepared[0])\nprint(housing_prepared[1])\nprint(housing_prepared[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Explore many different model and shortlist the best ones\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Selection Bias","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. What is Overfitting?\n\n- Overfitting Selection Bias\n- Variance\n- It happens even if it captures the noises of the data\n\n<b>Solution: How to Tackle Overfitting?</b>\n\n1. Regularization: It puts constrains to the model to make it simpler. The amount of regularization to apply during learning can be controlled by a hyperparameter.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2. What is Underfitting?\n- Underfitting Bias\n\n<b>Solution: How to Tackle Underfitting?</b>\n\n1. Select a more powerful model\n2. Feed better features to train algorithm\n3. Reduce the constraints on the model (eg., reduce the regularization hyperparameter)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### How to Choose the Best Model?\n\nFind right balance between:\n   - Fiting the data perfectly.\n   - And Keeping the model simple enough to ensure that it will generalize well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's Train the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using LinearRegression from linear_model\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression() #instantiating the model\n\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try the full pipeline on a few training instances\n\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint('Predictions: ', lin_reg.predict(some_data_prepared) )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare Against Actual Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the actual values\n\nprint(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Training Linear Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the RMSE in Linear Regression Model\n\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the model is underfitting. Since the error is very high.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. Training DecisionTree Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor (random_state = 42)\n\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the RMSE in Decision Tree Model\n\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error (housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, it is the case of Overfitting. Since the error is zero.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Better Evaluation Using Cross-Validation - Similar to Leave-One-Out\n\nK-fold Cross-Validation gives:\n    - Estimate of the performance of your model and\n    - Also a measure of how precise this estimate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performs K-fold cross-validation\n# Randomly splits the training set into 10 distinct subsets called folds\n# Then it trains and evaluates the Decision Tree model 10 times By\n# Picking a different fold for evaluation every time and training on the other 9 folds\n# The result is an array containing the 10 evaluation scores\n\nfrom sklearn.model_selection import cross_val_score\n\ntree_reg = DecisionTreeRegressor(random_state=42)\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                        scoring = 'neg_mean_squared_error', cv = 10)\n\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the score of cross-validation of DecisionTreeRegressor\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now compute the same score for Linear Regression\n\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Here we found, decision tree regression have lower deviation than linear regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. Training RandomForest Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's train one more model using Random Forests\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate RMSE in Random Forest model\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display_scores(forest_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Training XGBRegressor Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation in Random Forest model\nfrom xgboost import XGBRegressor\n\nxgb =  XGBRegressor()\nfrom sklearn.model_selection import cross_val_score\n\nxforest_scores = cross_val_score(xgb, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=8)\nxforest_rmse_scores = np.sqrt(-xforest_scores)\ndisplay_scores(xforest_rmse_scores)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation in Random Forest model\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Here we found, RandomForest regression have zero deviation, which is the best model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Fine-tune the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To fine tune the model we need Hyperparameters.\n\n  \n    \n<b>What are Hyperparameters?</b>\n\n\n- A ML model is a mathematical formula with a number of parameters that need to be learned from the data.\n\n\n- The soul of ML is fitting a model to the data.\n\n\n- By Training a model with exisiting data we fit the model parameters.\n\n\n- There are another kind of parameters that cannot be directly learned form the model training process.\n\n\n- These parameters express: \n\n    - Higher-level properties of the model such as complexity or how fast it should learn\n    \n    \n- These parameters are called <b>Hyperparameters</b>\n\n\n- Hyperparameters are usually decided before the actual training begins.\n\n\n- Can be decided by - Setting different values, Training different models and Choosing the values that work best.\n\n\n- Examples: \n    - Number of leaves or depth of a tree\n    - Learning rate how fast a model should learn.\n    - Number of hidden layers in a deep neural network.\n    - Number of clusters in a k-means clustering.\n    \n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Types of Solutions to find Hyperparameters:\n\n<b>1. GridSearchCV:</b> \n\n- Evaluates all possible hyperparameters values using cross-validation. \n\n\n- We just have to tell which hyperparameters we want it to experient with and what values to try out.\n\n\n<b>2. RandomizedSearchCV:</b> \n\nIt is used when hyperparameter search space is large.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"help(forest_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. GridSearchCV:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV for best possible combination of hyperparameter values for Random Forest Regressor\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    \n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    \n    # then try 6 (2×3) combinations with bootstrap set as False\n    \n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', n_jobs = 4)\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best hyperparameter combinations\n\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the best estimator\n\ngrid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the score of each hyperparameter combination tested during the grid search\n\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. RandomizedSearchCV \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=1, cv=8, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the importance score of each attribute in GridSearchCV\n\nfeature_importances = grid_search.best_estimator_.feature_importances_\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate model on the Test Set\n\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble Methods","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Another way of fine - tuning models is to combine best perfoming models.\n\n\n- The ensemble (group) oftens perform better than the best indivisual model.\n\n\n- Eg. ensemble - Rain Forest Regressor performed better than the individual decision trees.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}