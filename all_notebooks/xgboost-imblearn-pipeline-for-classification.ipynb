{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\n  <h1 class=\"alert-heading\"> XGBoost and imbalanced-learn pipeline for imbalanced classification problem</h1>\n    By kauvin Lucas in Kaggle","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\">\n  <h4 class=\"alert-heading\">What's the purpose of this notebook?</h4>\n  <hr>\n  <p class=\"mb-0\">This book is meant to show how to solve a classification problem with imbalanced data in an elegant fashion by using Gradient Boost Decision Trees (GBDT) from the XGBoost library and imbalanced-learn pipeline for preprocessing, resampling and fitting. The dataset used here consists of data related to existing and churned credit card customers.\n      \n      In this notebook, only the relevant cells are shown in the preview state.\n   </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\">\n  <h4 class=\"alert-heading\">Business Requirements</h4>\n  <hr>\n  <p class=\"mb-0\">A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.\n   </p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Import data manipulation and vizualization libraries\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport os\n\n# Import preprocessing and composing tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\n\n# Import metrics tools\nfrom sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, classification_report, confusion_matrix\n\n# Import fitting tools\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Import data\ndf = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\", usecols=list(range(0,21)))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Data exploration","metadata":{}},{"cell_type":"markdown","source":"#### Data exploration and feature selection","metadata":{}},{"cell_type":"markdown","source":"These are the input features initially considered to be relevant for the model according to the requirements:\n- Customer's age in years (Customer_Age)\n- Customer's gender (Gender)\n- Number of the customer's dependents (Dependent_count)\n- Customer's educational qualification (Education_Level)\n- Customer's marital status (Marital_Status)\n- Customer's annual income level (Income_Category)\n- Type of credit card held by the customer (Card_Category)\n- Period of relationship with the bank (Months_on_book)\n- Total of bank products held by the customer (Total_Relationship_Count)\n- Amount of inactive in the last 12 months (Months_Inactive_12_mon)\n- Number of contact in the last 12 months (Contacts_Count_12_mon)\n- Credit limit on the credit card (Credit_Limit)\n- Total revolving balance on the credit card (Total_Revolving_Bal)\n- Average of last 12 months of the open to buy credit line (Avg_Open_To_Buy)\n- Total transaction amount in the last 12 months (Total_Trans_Amt)\n- Total transaction count in the last 12 months (Total_Trans_Ct)\n- Average card utilization ration (Avg_Utilization_Ratio)","metadata":{}},{"cell_type":"markdown","source":"We'll look for the proportion of classes in the target data:","metadata":{}},{"cell_type":"code","source":"colors = ['#39008E', '#008823']\nsns.set_palette(sns.color_palette(colors))\nsns.countplot(x=\"Attrition_Flag\", data=df)\nplt.title(\"Count of existing credit card customers versus churned credit card customers\")\nplt.ylabel(\"Count of customers\")\nplt.xlabel(\"Attrition Flag\");","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data is clearly imbalanced as shown in the graph above, as with most of the credit card customer churn datasets. We'll evaluate it after testing our model","metadata":{}},{"cell_type":"markdown","source":"Now we'll count the missing values in the data:","metadata":{}},{"cell_type":"code","source":"# Check the dataframe for any missing data\nmissing_values = []\ncolumns = df.columns.to_list()\nfor column in columns:\n    null_values = df[column].isnull().sum()\n    missing_values.append([column, null_values])\npd.DataFrame(missing_values, columns=[\"Column Name\", \"Count of missing Values\"]).set_index(\"Column Name\").rename_axis(None)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll examine the categories for each categorical attribute in the dataset","metadata":{}},{"cell_type":"code","source":"# Print the categories for each categorical attribute\npd.set_option('display.max_colwidth', 0)\ncat = []\nfor col_name in df.select_dtypes(include=['object']).columns:\n    categories_list = df[col_name].value_counts().index.to_list()\n    cat.append([col_name, categories_list])\npd.DataFrame(cat, columns=[\"Column Name\", \"Categories\"]).set_index(\"Column Name\").rename_axis(None)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some categories are marked as **Unknown** in the 'Education_Level', 'Marital_Status' and 'Income_Category' attributes. Let's see how many of these values are in the dataset:","metadata":{}},{"cell_type":"code","source":"print(\"Observations with unknown values in at least one of the attributes: {} out of {}\".format(\n    (df[(df[\"Education_Level\"] == \"Unknown\") | \n   (df[\"Marital_Status\"] == \"Unknown\") | \n   (df[\"Income_Category\"] == \"Unknown\")].shape[0]), df.shape[0]))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Treating **Unknown** categories in the observations as missing values would imply a significant data loss. As it would be impossible to input the values, we'll treat them as another category, labeling them as zeros when encoding these features","metadata":{}},{"cell_type":"markdown","source":"#### Feature selection","metadata":{}},{"cell_type":"markdown","source":"We'll query the selected features before preprocessing the data:","metadata":{}},{"cell_type":"code","source":"features = df.iloc[:,1:]\nx = features.iloc[:,1:]\ny = features.iloc[:,0]\nfeatures.head()","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: transforming and fitting","metadata":{}},{"cell_type":"markdown","source":"We'll transform, resample and fit our model in a pipeline while executing a grid search to adjust hyperparameters.\n\nBefore working with XGBoost, we must ensure that the target variable is encoded before building the pipeline as XGBoost no longer supports label encoding. Still, this doesn't represent a data leakage issue in this particular problem because the outcome variable is always expected to take two classes (existing and attrited customers)","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Transform output variable (necessary to make XGBoost Work)\ny.replace(to_replace = [\"Existing Customer\", \"Attrited Customer\"], value = [0,1], inplace = True)\n\n# Train/Test split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n\n# Defining the encoding variables\nordinal_enc_variables = [\"Education_Level\", \"Income_Category\", \"Card_Category\"]\none_hot_enc_variables = [\"Gender\",\"Marital_Status\"]\n\n# Defining ordinal categories\nedu_lvl = [\"Unknown\", \"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"]\nincome_cat = [\"Unknown\", \"Less than $40K\", \"$40K - $60K\", \"$80K - $120K\", \"$60K - $80K\", \"$120K +\"]\ncard_cat = [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"]\nattrition_cat = [\"Existing Customer\", \"Attrited Customer\"]\n\n# Calling the encoder classes\nord_enc = OrdinalEncoder(categories = [edu_lvl, income_cat, card_cat])\none_hot_enc = OneHotEncoder(handle_unknown=\"ignore\")\n\n# Make column transformer\ncolumn_transform = make_column_transformer((ord_enc, ordinal_enc_variables),\n                                          (one_hot_enc, one_hot_enc_variables),\n                                           remainder=\"passthrough\")\n\n# Make pipeline\nclassifier = XGBClassifier()\npipeline = make_pipeline(column_transform, SMOTE(), StandardScaler(), classifier)\n\n\n# Set parameters values for grid\nparam_grid = {\n    \"xgbclassifier__learning_rate\":[0.01, 0.3, 0.5],\n    \"xgbclassifier__n_estimators\": [10, 100, 200],\n    \"xgbclassifier__max_depth\": [1, 3, 5],\n    \"xgbclassifier__objective\": [\"binary:logistic\"],\n    \"xgbclassifier__use_label_encoder\": [False],\n    \"xgbclassifier__booster\": [\"gbtree\"],\n    \"xgbclassifier__eval_metric\": [\"logloss\"]}\n\n# Make custom scoring metric\nscorer = make_scorer(f1_score, pos_label=1)\n\n# Intantiate GridSearchCV\ngrid_search_cv = GridSearchCV(pipeline, param_grid, scoring = scorer)\n\n# Fit\nmodel = grid_search_cv.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: evaluation","metadata":{}},{"cell_type":"markdown","source":"We'll evaluate against our test data by using precision, recall and F1 scores:","metadata":{}},{"cell_type":"code","source":"gbdt_predictions = model.predict(x_test)\nscores = [[\"Precision score\", precision_score(gbdt_predictions, y_test, pos_label = 1)],\n[\"Recall score\", recall_score(gbdt_predictions, y_test, pos_label = 1)],\n[\"F1 score\", f1_score(gbdt_predictions, y_test, pos_label = 1)]]\npd.DataFrame(scores, columns=[\"Metric\", \"Score\"])","metadata":{"scrolled":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define plot function\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Purples):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    else:\n        pass\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, gbdt_predictions, labels=[0,1])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Existing Customer','Attrited Customer'],\n                      normalize=True, title=\"Confusion matrix (normalized)\\n\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final remarks","metadata":{}},{"cell_type":"markdown","source":"This notebook has accomplished to show a simple and elegant way to solve an imbalanced classification problem by using XGBoost library and an imbalanced-learn pipeline for transforming, resampling and fitting.\n\nScores generated here indicate that the model built here is reliable for the problem in question, with little variation between precision and recall scores. Comments related to this notebook are always appreciated.","metadata":{}}]}