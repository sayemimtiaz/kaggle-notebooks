{"cells":[{"metadata":{"trusted":true,"_uuid":"a158438b8e76d61ecd176acc01b0ec5842c85d73"},"cell_type":"markdown","source":" # Introduction\nThis notebook describes an approach to classify movies as either comedies or dramas based on plot synopsis.  As such, it attempts to solve a typical text classification problem, albeit perhaps a tricky one, since defining humor can be difficult even for humans..  Classifications and synopses are taken from Wikipedia;  supplemental training data is taken from the news category dataset, which contains headlines and short descriptions of Huffington Post articles, in addition to categories that include comedy.  I apply a simple Bag-of-Words approach, a CNN-based approach, and a hybrid CNN-LSTM approach to the problem.  I also use a few standard text-processing steps to pre-processing the data.  The simplest approach turns out to be the best; none of the more complicated approaches I tried outperformed the Bag-of-Words logistic regression model.  \n\n### Outline\n\n1. Import and define tools and functions.\n2.  Exploratory data analysis\n3.  Text processing and vectorization.\n4.  Bag-Of-Words Model\n5.  GloVE Embeddings\n6.  CNN Model\n7.  CNN-LSTM Model\n8.  Analysis and Conclusion\n\n> \"**Humor is kind of like pornography … you know it when you see it.**\" -Kevin Litman Navarro"},{"metadata":{"_uuid":"7a4f35543bbc0ad0bc3b671ecb06ef9737edab02"},"cell_type":"markdown","source":"# 1. Import Tools****"},{"metadata":{"trusted":true,"_uuid":"bf2b264fc603b226ef7b19170cc611961dc9fe34"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib\nimport json\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout, Bidirectional\nfrom keras.layers import LSTM, Dropout,Activation, Bidirectional\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem.porter import *\nimport random\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3bf9051cf7aca82f45fa4a13d3939e56e3164c3"},"cell_type":"code","source":"def removeStopWords(lowerArg):\n    i=0\n    removed=[]\n    for x in lowerArg:\n        i+=1\n        removed.append((' '.join([word for word in x.split() if word not in nltk.corpus.stopwords.words('english')])))\n        \n    return pd.Series(removed).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\nnewsDF=pd.read_json('../input/news-category-dataset/News_Category_Dataset_v2.json', lines=True,dtype='str')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a8a4579a710f22d2e18e16355cd0a0609f68534"},"cell_type":"markdown","source":"# 2. EDA for Movie Data\nQuick initial look at the dataset.  Additional analysis (like counting unique words) will occur during processing and vectorization"},{"metadata":{"trusted":true,"_uuid":"e130036c084940023138ac4f05409055f68dc2fc"},"cell_type":"code","source":"##Limit to just comedies and Genres\ngenres=['drama','comedy']\ndf=df[df['Genre'].isin(genres)]\ndf=df.reset_index()\ndf['GenreID']=df['Genre'].apply(lambda x: genres.index(x))\n\nwordCount=df['Plot'].apply(lambda x: x.count(' '))\nprint(\"Mean number of words in synopses: \",int(wordCount.mean()))\nprint(\"Standard deviation number of words in synopses: \", int(wordCount.std()))\nprint('Number of Dramas: ',df['Genre'].value_counts()[0])\nprint('Number of Comedies: ',df['Genre'].value_counts()[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13b785f9c78bb9dd64719e912118ccb8883e72ed"},"cell_type":"code","source":"matplotlib.pyplot.hist(wordCount)\nprint('Distribution of Synopsis Word Counts')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2dc07bbf7fbdf69ee10a105a2c972b4ac0ecf5f"},"cell_type":"markdown","source":"### Here's what a plot synopsis might look like.  Even to a human (at least one unfamiliar with the movie), it might not be obvious whether a synopsis describes a comedy or a drama.  An interesting side project would be to assess what human error rates are for this task."},{"metadata":{"trusted":true,"_uuid":"cd946442f4f5e656aa59418455e959aa27bee54a"},"cell_type":"code","source":"synNumber=random.randint(1,1000)\nprint(df['Title'].loc[synNumber])\nprint(df['Genre'].loc[synNumber])\nprint(df['Plot'].loc[synNumber])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b65c017ce3680e536bf2a06d370475c9c92e35"},"cell_type":"markdown","source":"1. # EDA for News Dataset"},{"metadata":{"trusted":true,"_uuid":"1fbb959d6999fc013529c9236bb91aae83fc1d50"},"cell_type":"code","source":"##Take roughly balanced sample of news dataset between comedy and non-comedy stories\nnewsDF['Comedy']=(newsDF['category']=='COMEDY')\nnews=pd.concat([newsDF[newsDF['Comedy']==False].sample(5000),newsDF[newsDF['Comedy']]],axis=0)\nprint('News Comedies: ',news['Comedy'].value_counts().values[0])\nprint('News Non-Comedies: ',news['Comedy'].value_counts().values[1])\n\nwordCountNews=newsDF['short_description'].apply(lambda x: x.count(' '))\nwordCountHeadline=newsDF['headline'].apply(lambda x: x.count(' '))\n\nprint(\"Mean number of words in synopsis: \",int(wordCountNews.mean()))\nprint(\"Standard deviation number of words in synopsis: \", int(wordCountNews.std()))\nprint()\nprint(\"Mean number of words in headline: \",int(wordCountHeadline.mean()))\nprint(\"Standard deviation number of words in headline: \", int(wordCountHeadline.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0060cd9a5956b396bff4b7eb98944a9df147354c"},"cell_type":"markdown","source":"# 3. Text processing and vectorization\nNews descriptions are quite short, so concatenate with headlines to add information"},{"metadata":{"trusted":true,"_uuid":"f82044fa6f81ca4d2106cf860a336cf49d9b894d"},"cell_type":"code","source":"news=news.reset_index()\nnews['Text']=news['short_description'].str.cat(news['headline'])\nnews['OriginalText']=news['Text']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d35ad55e0c6e6c26d2b17ddc42e2e44522015a9"},"cell_type":"markdown","source":"Switching back to the movie dataset, stem each plot synopsis.  I don't stem the news articles because I don't use the news articles for the bag-of-words model, nor do I used stemmed versions for the neural net models.  Using stemmed words for those models did not improve performance."},{"metadata":{"trusted":true,"_uuid":"9b732efb720c69ccfba9fab353de46a53eeb4730"},"cell_type":"code","source":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\ndf['StemmedPlot']=df['Plot'].str.split().apply(lambda x: ' '.join([stemmer.stem(y) for y in x]))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a17f7054fee78031f56898f9dbd402d43f07f92f"},"cell_type":"markdown","source":"Make the both datasets lower-case and remove stop words"},{"metadata":{"trusted":true,"_uuid":"390cb85980f7afe0694576e8e752fd62b3dcdcfb"},"cell_type":"code","source":"lower=news['Text'].str.lower()\nnoStops=removeStopWords(lower)\nnews['Text']=noStops\n\n#Store Original Plot for later\ndf['OriginalPlot']=df['Plot']\n\nlower=df['Plot'].str.lower()\ncleaned=removeStopWords(lower)\ndf['Plot']=cleaned","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbe8a574b6897c547dce05384ee7376aa9566eba"},"cell_type":"markdown","source":"Tokenize movie data and pad sequences to maximum length"},{"metadata":{"trusted":true,"_uuid":"e0c1be56ac22ef7aadfb003f03fb9eae8631fae1"},"cell_type":"code","source":"##Using non-stemmed\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(list(df['Plot']))\nsequences = tokenizer.texts_to_sequences(list(df['Plot']))\nmaxLen=np.max([len(sequence) for sequence in sequences])\nprint(\"Maximum Length: \",maxLen)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=maxLen)\n\n#labels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\n#print('Shape of label tensor:', labels.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"429da339ec7317403886e8384a2310614998192c"},"cell_type":"markdown","source":"Tokenize movie data and pad sequences to maximum length **from movie data**"},{"metadata":{"trusted":true,"_uuid":"b3a68dead43cb6942c72338cd2b66e7de35dc8a9"},"cell_type":"code","source":"\nnewstokenizer = Tokenizer()\nnewstokenizer.fit_on_texts(list(news['Text']))\nnewsSequences = newstokenizer.texts_to_sequences(list(news['Text']))\nnewsword_index = newstokenizer.word_index\nprint('Found %s unique tokens.' % len(newsword_index))\nnewsdata = pad_sequences(newsSequences, maxlen=maxLen)\n\n#labels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', newsdata.shape)\n#print('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d11d747b7b8052ac0a7c3f0bf1dd8b154b7de8d2"},"cell_type":"markdown","source":"Quick sanity check to make sure that the tokenizer worked properly"},{"metadata":{"trusted":true,"_uuid":"aabec182e3491f96f67aa59501937d0144975823"},"cell_type":"code","source":"##Sanity Check index is the word_index dictionary with keys reversed\nsanityCheckIndex={v: k for k, v in tokenizer.word_index.items()}\nprint(sequences[500])\nprint(' '.join([sanityCheckIndex[wordIndex] for wordIndex in sequences[500]]))\nprint(data[500][0])\nprint(data[500][-1])\nprint(' '.join([sanityCheckIndex[wordIndex] for wordIndex in data[500] if wordIndex!=0 ]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"084034c6f70d1d3f2ce8caac29db4cbb472d4483"},"cell_type":"markdown","source":"The above tokenizers will be used for the neural network models.  For the bag-of-words model, I use a TF-IDF DTM using the stemmed synopses created earlier"},{"metadata":{"trusted":true,"_uuid":"0dbfe5e8dfca3cf16ef465cd1cac821f45ede5cc"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words='english')\ntfs = tfidf.fit_transform(df['StemmedPlot'])\nprint('Shape of TF-IDF matrix: ', tfs.T.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7744287171000a0fc766343229e91d36bf99c206"},"cell_type":"markdown","source":"Split the data into train and test sets.  Additionally, create separate training sets for training data enhanced with news data and training data consisting only of movie data"},{"metadata":{"trusted":true,"_uuid":"676f00e61be7f878a183bdbe12258caf3dbc44e6"},"cell_type":"code","source":"seed=random.randint(1,1000)\nX_train, X_test, y_train, y_test = train_test_split(data, df['GenreID'], test_size=0.2, random_state=seed)\ntestIndices=y_test.index\ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)\n\ny_train_small=y_train.copy()\nX_train_small=X_train.copy()\n\ny_train_add=to_categorical(news['Comedy'])\nX_train_add=newsdata\nX_train=np.concatenate([X_train,X_train_add],axis=0)\ny_train=np.concatenate([y_train, y_train_add],axis=0)\nprint(\"X Train without news shape: \",X_train_small.shape)\nprint(\"Y train without news shape: \",y_train_small.shape)\nprint(\"X train with news shape: \",X_train.shape)\nprint(\"Y train with news shape: \",y_train.shape)\nprint(\"X test shape: \",X_test.shape)\nprint(\"Y test shape: \",y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc1a3770106b8972df9edf0333449ff36ea3ada5"},"cell_type":"code","source":"X_trainBag, X_testBag, y_trainBag, y_testBag = train_test_split(tfs, df['GenreID'], test_size=0.2, random_state=seed)\ntestIndicesBag=y_testBag.index\ny_trainBag=to_categorical(y_trainBag)\ny_testBag=to_categorical(y_testBag)\nprint(\"BoW X Train Shape: \",X_trainBag.shape)\nprint(\"BoW Y Train Shape: \",y_trainBag.shape)\nprint(\"BoW X Test Shape: \",X_testBag.shape)\nprint(\"BoW Y Test Shape: \",y_testBag.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef2d5464d743e9d022fa480631e326e30c90dadf"},"cell_type":"markdown","source":"# 4.  Bag of Words Model\nUse the TF-IDF matrix (split into training and test) from the pre-processing step to train a logistic regression binary classifier."},{"metadata":{"trusted":true,"_uuid":"aa1988b4c24d8162137138cada3e780f018eda8f"},"cell_type":"code","source":"from keras import *\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\ntf_input = Input(shape=(tfs.shape[1],), dtype='float32')\nx=Dense(len(genres),activation='sigmoid')(tf_input)\nbagOfWords = Model(tf_input, x)\nbagOfWords.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\nbagOfWords.fit(X_trainBag, y_trainBag, validation_data=(X_testBag, y_testBag),epochs=20, batch_size=128)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"946c1379ac4ca1cc6e441ecb70a1ce4f3cf0587a"},"cell_type":"markdown","source":"# 5. GloVE embeddings\nLoad the GloVE 6B 100d word embeddings in hopes that they will enhance the accuracy and training speed of the neural network models."},{"metadata":{"trusted":true,"_uuid":"95efbfbf8f6f0df726fa8493544ab26e1edbbc80"},"cell_type":"code","source":"embeddings={}\nindex=0\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as file:\n    for embeddingLine in file:\n        lineSplit=embeddingLine.split()\n        coefs = np.asarray(lineSplit[1:], dtype='float32')\n        embeddings[lineSplit[0]]=coefs\n        index+=1\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a4d100f873ac0e1398e26060597e8d4c0d32758"},"cell_type":"markdown","source":"Map the GloVE embeddings to the data"},{"metadata":{"trusted":true,"_uuid":"bced1acaea9dbe0ffb4708c0ca288fede3e59219"},"cell_type":"code","source":"embeddings_matrix=np.zeros((len(word_index)+1,len(embeddings['a'])))\nfor word,i in word_index.items():\n    if word in embeddings:\n        embeddings_matrix[i]=embeddings[word]\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19b7751768289cbd87bc07ef1bd93325b710884c"},"cell_type":"markdown","source":"Quick check to make sure that the embeddings matrix is right"},{"metadata":{"trusted":true,"_uuid":"89dd354df92463c72e8e175fcd534e769f1d2cc1"},"cell_type":"code","source":"print('Word #2: ',sanityCheckIndex[2])\nprint('Index of him : ',word_index['him'])\nprint('Embbedding in embeddings list: ',embeddings['him'][:5])\nprint('Embedding in embeddings matrix: ',embeddings_matrix[2][:5])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"779412a6f2368fe4d7ae4d7ec0c003e26aec4056"},"cell_type":"markdown","source":"Create Keras embedding layers to convert the texts to embeddings.  One uses Glove, the other doesn't.  I test models using both."},{"metadata":{"trusted":true,"_uuid":"690b752c45cba88e5309e0e8761129d4595dad2e"},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            len(embeddings['a']),\n                            weights=[embeddings_matrix],\n                            input_length=maxLen,\n                            trainable=False)\nembedding_layerNoGlove = Embedding(len(word_index) + 1,\n                            len(embeddings['a']),\n                            weights=[embeddings_matrix],\n                                   input_length=maxLen,\n                            )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b121fa82aa70b280bdf2d94601ed2db18f4cc3e7"},"cell_type":"markdown","source":"Check to make sure embedding layer does what it should be doing"},{"metadata":{"trusted":true,"_uuid":"7769ae09bf961385a08c7b76ef46f418a2807c8f"},"cell_type":"code","source":"sequence_input = Input(shape=(maxLen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nembeddingOnlyModel = Model(sequence_input, embedded_sequences)\n\nprint('Manual Embeddings Result: ',[list(embeddings[sanityCheckIndex[x]][:3]) if sanityCheckIndex[x] in embeddings else [0,0,0] for x in sequences[500] ][-5:])\n##print(sequences[500])\n##print([ sanityCheckIndex[l] for l in list(data[500]) if l>0 ])\n##print([ sanityCheckIndex[l] for l in list(sequences[500]) if l>0 ])\nprint('Model Embeddings Result: ',embeddingOnlyModel.predict(np.array(data[500]).reshape(1,maxLen))[0,-5:,:3])\n##print(embeddings_matrix[2][:5])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9fd824f2cae3c62176c2182190305b2b7847451"},"cell_type":"markdown","source":"# 6. CNN Model\nStart with a 3 layer CNN to predict the genre.  Note that here I am training the word encodings ourselves, rather than using the GloVE encodings"},{"metadata":{"trusted":true,"_uuid":"133bd08896be5e17582056b5bdf76647a0e9c650"},"cell_type":"code","source":"sequence_input = Input(shape=(maxLen,), dtype='int32')\nembedded_sequences = embedding_layerNoGlove(sequence_input)\nx=Conv1D(128, 9, activation='relu')(embedded_sequences)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx = Flatten()(x)\nx=Dense(128, activation='relu')(x)\nx=Dense(len(genres),activation='softmax')(x)\n\nnoGloveCNN = Model(sequence_input, x)\nnoGloveCNN.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\nnoGloveCNN.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ecaf68d665a6523b2c6c86246ee1b9162174665"},"cell_type":"markdown","source":"Same as above, except using GloVE embeddings.  Trains faster, but no more accurate."},{"metadata":{"trusted":true,"_uuid":"0f6b50829c35740ca57f864e768d7e54e16a409f"},"cell_type":"code","source":"from keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout, Bidirectional\nsequence_input = Input(shape=(maxLen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx=Conv1D(128, 9, activation='relu')(embedded_sequences)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\n\nx = Flatten()(x)\nx=Dense(128, activation='relu')(x)\nx=Dense(len(genres),activation='softmax')(x)\n\nmodel = Model(sequence_input, x)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\nX_train.shape\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e43f99b3b1e1ae6b212752a6dd20c042bb81254b"},"cell_type":"markdown","source":"Same as above except using only the movie data, without the additional news data"},{"metadata":{"trusted":true,"_uuid":"b588d2974d1b609e1bf8c28de06e82401d731b33","scrolled":true},"cell_type":"code","source":"\nsequence_input = Input(shape=(maxLen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx=Conv1D(128, 9, activation='relu')(embedded_sequences)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\n\nx = Flatten()(x)\nx=Dense(128, activation='relu')(x)\nx=Dense(len(genres),activation='softmax')(x)\n\nmodelSmall = Model(sequence_input, x)\nmodelSmall.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\nX_train.shape\nmodelSmall.fit(X_train_small, y_train_small, validation_data=(X_test, y_test),epochs=2, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec10d7c7eea4711c47c91ec1f4ff2021740a2e7c"},"cell_type":"markdown","source":"# 7.  LSTM-CNN model\nTry adding an LSTM layer to the end of the CNN (starting with combined data and GloVE embeddings)"},{"metadata":{"trusted":true,"_uuid":"29af26d9e05bd317b8c79fa00727bc5416153842"},"cell_type":"code","source":"\nfrom keras.layers import LSTM, Dropout,Activation, Bidirectional\nword_indices =Input(shape=(maxLen,), dtype='int32')\n# Propagate sentence_indices through your embedding layer, you get back the embeddings\nembeddingsLSTM = embedding_layer(word_indices)   \n\n# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n# Be careful, the returned output should be a batch of sequences.\nx=Conv1D(128, 9, activation='relu')(embeddingsLSTM)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\n\nX =  LSTM(128,return_sequences=False)(x)\n# Add dropout with a probability of 0.5\nX = Dropout(.65)(X)\n# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\nX = Dense(len(genres),activation='softmax')(X)\n# Add a softmax activation\nX = Activation('softmax')(X)\n    # Create Model instance which converts sentence_indices into X.\nLSTMmodel = Model(inputs = word_indices, outputs = X) \nLSTMmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nLSTMmodel.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaea2e9ceed87a4b99d6c39afef1bdf8d2d003f7"},"cell_type":"markdown","source":"Same as above, except with just the movie data"},{"metadata":{"trusted":true,"_uuid":"93195747c53c56ca4dcbf83879d70eebe45db40e"},"cell_type":"code","source":"from keras.layers import LSTM, Dropout,Activation, Bidirectional\nword_indices =Input(shape=(maxLen,), dtype='int32')\n# Propagate sentence_indices through your embedding layer, you get back the embeddings\nembeddingsLSTM = embedding_layer(word_indices)   \n\n# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n# Be careful, the returned output should be a batch of sequences.\nx=Conv1D(128, 9, activation='relu')(embeddingsLSTM)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\nx=Conv1D(128, 9, activation='relu')(x)\nx = Dropout(.4)(x)\nx=MaxPooling1D(9)(x)\n\nX =  LSTM(128,return_sequences=False)(x)\n# Add dropout with a probability of 0.5\nX = Dropout(.65)(X)\n# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\nX = Dense(len(genres),activation='softmax')(X)\n# Add a softmax activation\nX = Activation('softmax')(X)\n    # Create Model instance which converts sentence_indices into X.\nLSTMmodelSmall = Model(inputs = word_indices, outputs = X) \nLSTMmodelSmall.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nLSTMmodelSmall.fit(X_train_small, y_train_small, validation_data=(X_test, y_test),epochs=5, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2980acd9ca50b6ff00553c5836919f1c5a0fd1f7"},"cell_type":"markdown","source":"# 8 Analysis and Conclusion\nMake test set predictions to compare and evaluate the models."},{"metadata":{"trusted":true,"_uuid":"37859bd81836d673c81d473452e1153a6a3e37f4"},"cell_type":"code","source":"CNNpreds=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77687485257769bb91bb7d5da9e58b8cd0f4d05f"},"cell_type":"code","source":"LSTMpreds=LSTMmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b635a99da0425658eae8df7d68e344fe6fbb628"},"cell_type":"code","source":"noGloveCNNpreds=noGloveCNN.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b8ff4d513111cd1a9b7ce724f6a7eeb6c6432be"},"cell_type":"code","source":"bagPreds=bagOfWords.predict(X_testBag)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e214031f8512f5ad5451f5e6e728f1d200c3e63e"},"cell_type":"markdown","source":"In most train-test splits, ensembling the model predictions, weighted toward the BoW predictions, improves accuracy slightly.  But the accuracy improvement above the BoW model (the best single model) is negligible at best.  Final accuracy hovers around **77%**.  Not bad, considering that the task would probably not be straightforward even for humans."},{"metadata":{"trusted":true,"_uuid":"7bf16ccd417b2943d07bf4a9ec5d279f54522fb2"},"cell_type":"code","source":"avgPreds=np.average([bagPreds,LSTMpreds,CNNpreds],weights=[.8,.2,.2],axis=0)\navgPreds=bagPreds\n\nwithPreds=pd.concat([pd.DataFrame(avgPreds),df.loc[testIndices,['OriginalPlot','Genre','Title']].reset_index()],axis=1)\nwithPreds['Predicted Genre']=(withPreds[0]>withPreds[1]).replace(True,'drama').replace(False,'comedy')\naccuracy=(withPreds['Predicted Genre']==withPreds['Genre']).mean()\nprint('Accuracy of final ensembled model: ',accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b2f3314acf6089bfae20a32291e60f75b2ae12"},"cell_type":"markdown","source":"Confusion matrix shows that the model errors are fairly balanced."},{"metadata":{"trusted":true,"_uuid":"d5ba6ff7e439a65f1cae44735ed5212eeca3ba71"},"cell_type":"code","source":"pd.pivot_table(withPreds,columns='Predicted Genre', index='Genre', aggfunc=len)['index']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"486fc00c4409f14d7e7f1f4b22d3d4b204a5ddc2"},"cell_type":"markdown","source":"Take a look at the movies the ensemble model is most confident are comedies"},{"metadata":{"trusted":true,"_uuid":"f6e16fd7ea8ee1cce86251c11ca8e432d31e5cc7"},"cell_type":"code","source":"for x in withPreds.sort_values(by=0)[['Title','OriginalPlot',0,'Genre']].head(3).iterrows():\n    print(x[1].Title,x[1][0],x[1]['Genre'])\n    print(x[1].OriginalPlot)\n    print(' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17ca4410e5f7313211afd1262b27e75ef7151df0"},"cell_type":"markdown","source":"Movies the ensemble model is most confident are not comedies."},{"metadata":{"trusted":true,"_uuid":"60ae524dbc83c1e4167807de2e2f49f1a09b9de1"},"cell_type":"code","source":"for x in withPreds.sort_values(by=1)[['Title','OriginalPlot',0,'Genre']].head(3).iterrows():\n    print(x[1].Title,x[1][0],x[1]['Genre'])\n    print(x[1].OriginalPlot)\n    print(' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d6ffddfac5d53bb33e4ed8965c451d769f3a190"},"cell_type":"markdown","source":"Get the weights out of the Bag of Words model to see what words are most predictive of dramas and comedies"},{"metadata":{"trusted":true,"_uuid":"4a868b6727604fbda82108c8764b76cb38fbc373"},"cell_type":"code","source":"weights=bagOfWords.get_weights()[0][:,0]\nmostDramatic=weights.argsort()[-10:][::-1]\nleastDramatic=weights.argsort()[:10][::1]\nindex_to_words={v: k for k, v in tfidf.vocabulary_.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b940d49417e5313a5135651504ad0fcc08887a3"},"cell_type":"code","source":"print('Words most likely to indicate comedy: ', [index_to_words[x] for x in leastDramatic])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18e5b4317c94cd35b3014825e992da833bdf2fbf"},"cell_type":"code","source":"print('Words most likely to indicate drama: ',[index_to_words[x] for x in mostDramatic])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"231bc44f7fa6cbc0f8342dcb493f519f43b3a292"},"cell_type":"markdown","source":"# Final Notes\nAfter all that, none of the fancy machine learning techniques improved the relatively straightforward Bag of Words logistic regression.  All of the model architectures I tried were about 7% less accurate than the simple model, even when incorporating GloVE embeddings and additional training data from news articles. A reminder, perhaps, that complexity and cutting edge is not necessarily always better, especially in light of the interpretability of the Bag of Words model, highlighted by the above list of words associated with comedy and drama synopses."},{"metadata":{"_uuid":"4e27807ca215a9682cd06a80f683b1b09a987642"},"cell_type":"markdown","source":"# Predict Your Own - Just for Fun"},{"metadata":{"trusted":true,"_uuid":"9af940bcd3fa9745b94899087a14854bfacc70c3"},"cell_type":"code","source":"def bowPredict(syn):\n    noStops=removeStopWords([syn])\n    stemmed=' '.join([stemmer.stem(x) for x in noStops[0].split() ])\n    mat=tfidf.transform([stemmed])\n    preds=bagOfWords.predict(mat)\n    if preds[0][0]>preds[0][1]: return 'comedy'\n    return 'drama'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19027df7c866d6737a0ac1ea8df9743ccfef4f54"},"cell_type":"code","source":"bowPredict(\"United States Naval Aviator LT Pete ‘Maverick’ Mitchell and his Radar Intercept Officer LTJG Nick ‘Goose’ Bradshaw fly the F-14A Tomcat aboard USS Enterprise (CVN-65). During an interception with two hostile MiG-28aircraft (portrayed by a Northrop F-5), Maverick gets missile lock on one, while the other hostile aircraft locks onto Maverick's wingman, Cougar. While Maverick drives off the remaining MiG-28, Cougar is too shaken to land, and Maverick, defying orders, shepherds him back to the carrier. Cougar gives up his wings, citing his newborn child that he has never seen. Despite his dislike for Maverick's recklessness, CAG ’Stinger’ sends him and Goose to attend Topgun,[6] the Naval Fighter Weapons School at Naval Air Station Miramar.At a bar the day before Topgun starts, Maverick, assisted by Goose, unsuccessfully approaches a woman. He learns the next day that she is Charlotte ‘Charlie’ Blackwood, an astrophysicist and civilian Topgun instructor. She becomes interested in Maverick upon learning of his inverted maneuver with the MiG-28, which disproves US intelligence on the enemy aircraft's performance. During Maverick's first training sortie he defeats instructor LCDR Rick ‘Jester’ Heatherly but through reckless flying breaks two rules of engagement and is reprimanded by chief instructor CDR Mike ‘Viper’ Metcalf. Maverick also becomes a rival to top student LT Tom ‘Iceman’ Kazansky, who considers Maverick's flying ‘dangerous.’ Charlie also objects to Maverick's aggressive tactics but eventually admits that she admires his flying and omitted it from her reports to hide her feelings for him, and the two begin a romantic relationship. During a training sortie, Maverick abandons his wingman ‘Hollywood’ to chase Viper, but is defeated when Viper maneuvers Maverick into a position from which his wingman Jester can shoot down Maverick from behind, demonstrating the value of teamwork over individual prowess. Maverick and Iceman, now direct competitors for the Topgun Trophy, chase an A-4 in a later training engagement. Maverick pressures Iceman to break off his engagement so he can shoot it down, but Maverick's F-14 flies through the jet wash of Iceman's aircraft and suffers a flameout of both engines, going into an unrecoverable flat spin. Maverick and Goose eject, but Goose hits the jettisoned aircraft canopy head-first and is killed. Although the board of inquiry clears Maverick of responsibility for Goose's death, he is overcome by guilt and his flying skill diminishes. Charlie and others attempt to console him, but Maverick considers retiring. He seeks advice from Viper, who reveals that he served with Maverick's father Duke Mitchell on the USS Oriskany and was in the air battle in which Mitchell was killed. Contrary to official reports which faulted Mitchell, Viper reveals classified information that proves Mitchell died heroically, and informs Maverick that he can succeed if he can regain his self-confidence. Maverick chooses to graduate, though Iceman wins the Top Gun Trophy. During the graduation party, Viper calls in the newly graduated aviators with the orders to deploy. Iceman, Hollywood, and Maverick are ordered to immediately return to Enterprise to deal with a ‘crisis situation’, providing air support for the rescue of a stricken ship that has drifted into hostile waters. Maverick and Merlin (Cougar's former RIO) are assigned as back-up for F-14s flown by Iceman and Hollywood, despite Iceman's reservations over Maverick's state of mind. The subsequent hostile engagement with six MiGs sees Hollywood shot down; Maverick is scrambled alone due to a catapult failure and nearly retreats after encountering circumstances similar to those that caused Goose's death. Upon finally rejoining Iceman, Maverick shoots down three MiGs, and Iceman one, forcing the other two to flee. Upon their triumphant return to Enterprise, Iceman and Maverick express newfound respect for each other. Offered any assignment he chooses, Maverick decides to return to Topgun as an instructor. At a bar in Miramar, Maverick and Charlie reunite.\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}