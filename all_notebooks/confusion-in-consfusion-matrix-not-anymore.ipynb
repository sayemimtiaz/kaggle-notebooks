{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Introduction</center></h1>\n\n> If you have any confusion in confusion matrix or AUC ROC, this notebook will clear each and every doubt and you will feel more confirtable in dealing with them! \n\n> The confusion matrix is the base for calculating all the evaluation parametes like accuracy, sensitivity, specificity, precision, recall, f1-score, etc. so its understanding it very important to a data scientist. The AUC ROC is also a important evaluation matrix specially for binary classification but can be applicable to multiclass classification also.\n\n> As the threshold connects the two - confusion matrix and the ROC curve, I decided to explain them together.\n\n> I am using this dataset as it is very popular and doesn't require much preprocessing. So, let's begin!","metadata":{}},{"cell_type":"markdown","source":"<h3><center>Importing the libraries</center></h3>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(font = 'Serif', style = 'white', rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-07T05:36:44.379591Z","iopub.execute_input":"2021-07-07T05:36:44.380042Z","iopub.status.idle":"2021-07-07T05:36:45.539123Z","shell.execute_reply.started":"2021-07-07T05:36:44.379945Z","shell.execute_reply":"2021-07-07T05:36:45.538279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Reading Data and Preprocessing</center></h3>\n\n> I am not performing any EDA, Data Cleaning, Feature Engineering, etc., as the focus of this notebook is understanding of Confusion Matrix and ROC curve. If you want to see these steps, you can go through [my other notebook](https://www.kaggle.com/sonukiller99/our-drinking-water-is-safe-or-is-it-eda-ml) for the same dataset.","metadata":{}},{"cell_type":"code","source":"# Reading the data\ndf = pd.read_csv('../input/water-potability/water_potability.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:36:52.334075Z","iopub.execute_input":"2021-07-07T05:36:52.334443Z","iopub.status.idle":"2021-07-07T05:36:52.396453Z","shell.execute_reply.started":"2021-07-07T05:36:52.334412Z","shell.execute_reply":"2021-07-07T05:36:52.395427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the missing values\ndf = df.dropna().reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:36:54.728717Z","iopub.execute_input":"2021-07-07T05:36:54.729064Z","iopub.status.idle":"2021-07-07T05:36:54.753113Z","shell.execute_reply.started":"2021-07-07T05:36:54.729034Z","shell.execute_reply":"2021-07-07T05:36:54.751965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\ny = df['Potability']\nX = df.iloc[:,:-1]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:36:56.378546Z","iopub.execute_input":"2021-07-07T05:36:56.378924Z","iopub.status.idle":"2021-07-07T05:36:56.385724Z","shell.execute_reply.started":"2021-07-07T05:36:56.378887Z","shell.execute_reply":"2021-07-07T05:36:56.384688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Creating ML model and Preprocessing</center></h3>\n\n> I choose Random Forest, as it doesn't require any feature scaling or one hot encoding, also it performs pretty good. I have selected some random hyperparamets just to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"# Creating the LogisticRegression model\nmodel = RandomForestClassifier(min_samples_leaf = 20, min_samples_split = 30, max_depth = 7).fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:37:01.833466Z","iopub.execute_input":"2021-07-07T05:37:01.833839Z","iopub.status.idle":"2021-07-07T05:37:02.313924Z","shell.execute_reply.started":"2021-07-07T05:37:01.833804Z","shell.execute_reply":"2021-07-07T05:37:02.313227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><center> Confusion Matrix</center></h2>\n\n> <center><img src = \"https://skappal7.files.wordpress.com/2018/08/confusion-matrix.jpg?w=748\"></center>\n\nThere are few basic terms:\n\n- TP : Positives which we classified correctly\n- TN : Negetives which we classified correctly\n- FP : Negetives which we classified incorrectly (i.e. classified as positives)\n- FN : Positives which we classified incorrectly (i.e. classified as negetives)\n\n#### How the Confusion Matrix is plotted:\n> The data goes into the model and it predicts probabilities between 0 and 1. The defalt thresold is set at 0.5, i.e. any probability which is <0.5 is classified as 0 (negetives) and >=0.5 is classified as 1 (positives). \n\n> This is done for all the input data points whoes acutal lables are known (ground thruth). So now we have actual and predicted 1's (positives) and 0's (negetives) and the confusion matrix is contructed!\n\n#### How to interprete the Confusion Matrix:\n> The use of Confusion Matrix varies accross different fields. For eg. in medical, generally the focus is on reducing the False Negetives to minimun (ideally 0), as classifying a patient as no disease who actually has it is very dangerous. So, based on your requirement you can choose which box you want in confusion matrix.\n\n> The boxes in the Confusion Matrix can be combined to provide more information like, sensitivity, specificity, precision, recall, etc. Eg. if you want to know, how many 1's (positives) you have correctly identified then you can choose sensitivity and for 0's (negetives) specificity is used.\n> <center> <img src = \"https://miro.medium.com/max/576/1*RWwHcEAgsd-yAs7UhRLuVg.png\"> </center>\n\n> So, once you understand your need, you can choose among the evaluations parameters.","metadata":{}},{"cell_type":"code","source":"def my_confusion_matrix(thresh):\n\n    probability = model.predict_proba(X)\n    matrix = pd.DataFrame()\n\n    threshold = thresh\n    y_predict = (probability>=threshold).astype(int)[:,1]\n\n    matrix_normal = metrics.confusion_matrix(y, model.predict(X))\n    matrix = metrics.confusion_matrix(y, y_predict)\n\n    fig, ((axis1, axis2), (axis3, axis4)) = plt.subplots(2,2, figsize=(20,12), constrained_layout = True)\n\n    # Threshold = 0.5\n    axis1 = sns.heatmap(matrix_normal, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis1, annot_kws={\"fontsize\":30, 'weight':'bold'})\n    axis1.set_title(f\"Confusion Matrics | Threshold : 0.5\", fontsize=16, weight='bold', y=1.05);\n    axis1.set_xlabel('Predicted', fontsize=12, weight = 'bold')\n    axis1.set_ylabel('Actual', fontsize=12, weight = 'bold')\n    axis1.set_xticklabels([0,1], fontsize=12 )\n    axis1.set_yticklabels([0,1], fontsize=12, rotation=0)\n\n    # Threshold = given value\n    axis3 = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis3, annot_kws={\"fontsize\":30, 'weight':'bold'})\n    axis3.set_title(f\"Confusion Matrics | Threshold : {thresh}\", fontsize=16, weight='bold', y=1.05, color='green');\n    axis3.set_xlabel('Predicted', fontsize=12, weight = 'bold')\n    axis3.set_ylabel('Actual', fontsize=12, weight = 'bold')\n    axis3.set_xticklabels([0,1], fontsize=12 )\n    axis3.set_yticklabels([0,1], fontsize=12, rotation=0);\n\n    # AUC ROC plot\n    # calculate the fpr and tpr for all thresholds of the classification\n    preds = probability[:,1]\n    fpr, tpr, threshold = metrics.roc_curve(y, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    \n    axis2.set_title('ROC Curve', fontsize=16, weight='bold', y=1.05)\n    axis2.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    axis2.legend(loc = 'lower right')\n    axis2.plot([0, 1], [0, 1],'r--')\n    axis2.set_xlim([0, 1])\n    axis2.set_ylim([0, 1])\n    axis2.set_ylabel('True Positive Rate', fontsize=14, weight='bold')\n    axis2.set_xlabel('False Positive Rate', fontsize=14, weight='bold');\n    \n    # Plotting tpr and fpr\n    fpr, tpr, threshold = metrics.roc_curve(y, y_predict)\n    roc_auc = metrics.auc(fpr, tpr)\n\n    axis4.set_title(f'ROC Curve | Threshold : {thresh}', fontsize=16, weight='bold', y=1.05)\n    axis4.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    axis4.legend(loc = 'lower right')\n    axis4.plot([0, 1], [0, 1],'r--')\n    axis4.set_xlim([0, 1])\n    axis4.set_ylim([0, 1])\n    axis4.set_ylabel('True Positive Rate', fontsize=14, weight='bold')\n    axis4.set_xlabel('False Positive Rate', fontsize=14, weight='bold')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:37:05.867764Z","iopub.execute_input":"2021-07-07T05:37:05.868307Z","iopub.status.idle":"2021-07-07T05:37:05.889316Z","shell.execute_reply.started":"2021-07-07T05:37:05.868259Z","shell.execute_reply":"2021-07-07T05:37:05.888158Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Effect of Threshold on Confusion Matrix</center></h3>","metadata":{}},{"cell_type":"code","source":"interact(my_confusion_matrix, thresh=(0,1,0.05));","metadata":{"execution":{"iopub.status.busy":"2021-07-07T05:37:07.308722Z","iopub.execute_input":"2021-07-07T05:37:07.30905Z","iopub.status.idle":"2021-07-07T05:37:08.436969Z","shell.execute_reply.started":"2021-07-07T05:37:07.309022Z","shell.execute_reply":"2021-07-07T05:37:08.436282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Role of threshold in confusion matrix\n> Confusion Matrix is plotted for the default thresold of 0.5, but it can be plotted for other threshold also. As we change the threshold the predicted 1's and 0's will change and thus Confusion Matrix will also change. \n\n> Note that by changing the threshold, the probabilities predicted by model doesn't change.\n\n> The sum of TP & FN is constant also the sum of TN and FP is constant. So, by changing threshold one parameter increses while other decreses as their sum is constant.\n\n> When thresold is 0, then all the predicted probabilities will become 1 i.e. we have classified all samples with 1's, so we have correctly classified all positives while wrongly classifying all the negetives. Thus, we will have 811 TP, and 1200 FP. \n\n> You can imagine threshold as a imaginary line which moves from left to right. As we increase the threshold the imaginary line will move from left to right and thus FN will increase and TP will decrease as their sum is constant. Similarly, TN will increse and FP will decrease. When threshold is 1, then we will have 0 FP, and 0 TP.\n\n> Try moving the slider, to get feel the effect of threshold on confusion matrix.\n\n#### This is how confusion matrix works!","metadata":{}},{"cell_type":"markdown","source":"<h3><center>AUC ROC Curve</center></h3>\n\n> It is used for identifying how good our model can distinguish between 1's and 0's. First let's understand the True Positive Rate (TPR) and False Positive Rate (FPR). TPR is nothing but sensitivity, i.e. out of total positives how many we have correctly identified and FPR is out of positives that we identified how many were actual positives. This can sound same thing, but if you read it again you will get the difference.\n\n<center><img src=\"https://bit.ly/3hk9Rzr\"></center>\n\n> As the threshold increase (verticle line moves right) the TP and FP decrease so TPR adn FPR also decrease.\n\n> For different values of threshold, the FPR and TPR are calculated and the curve of FPR vs TPR is called as ROC curve. The area under this curve (AUC) tells the performance of the model. More the AOC, better the model can distinguish between postivies and negetives.\n\n- AOC = 0 : Model has classified 1's as 0's and 0's as once, i.e. wrost.\n- AOC = 0.5 : Performance is no better than a random chance, i.e. baseline\n- AOC = 1 : All 1's are identified as 1 and 0's as 0's perfectly! i.e. best\n\n> When the threshold is 0 or 1, the AUC is 0.5 and for all other values it is >=0.5, meaning the model is performing better than the baseline model as you will see from the plot. \n\n> The AUC ROC we got for the model is 0.84, it means the model can distinguish between 1's and 0's with 84% confidence.\n\n#### This is concept of AUC ROC!","metadata":{}},{"cell_type":"markdown","source":"#### I hope that your doubts have been cleared and you got some understanding of Confusion Matrix and AUC ROC.\n\n<h3><center>If you like it, don't forget to upvote!</center></h3>","metadata":{}}]}