{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n## Purpose\n\nTo help medical researchers keep up with the rapid acceleration in new literature on COVID-19, we design and build this knowledge graph based commentary generation tool. When users search for something in these papers, it can generate knowledge about what these papers explain.\n\nUnlike some traditional search engines, the tool searches for 'concept' rather than 'string'. This brings two benefits. Firstly, we can recognize the 'concept' when there are many different ways of saying the same 'concept' in different papers. Secondly, by means of knowledge graph we can find related ontologies and relations to the 'concept' rather than the location of the 'string'.\n\n## Approach\n\nSteps we take to build this tool:\n\n1. Over the provided scholarly articles, build a knowledge graph which consists of entities like disease, symptom, gene, drug, chemical and also paper\n2. Use graph embedding model to predict missing links in the graph.\n3. Build a system that can visualize concepts related to target concept and generate commentary about it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore warnings in this notebook.\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build knowledge graph\n\n## Overall approach\n\nWe combine traditional NLP based entity and relation extraction approach to extract entities and relations from large corpus of biomedical papers with modern deep learning based approach like R-GCN to predict new relations. Our approach has the following steps and more details will be given later.\n\n1. Extract entities and relations that are stated in papers using [Semantic Knowledge Representation](https://semrep.nlm.nih.gov/) which is an entity and relation extraction tool based on [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html) project. \n2. Enrich relations using existing knowledges in [UMLS Metathasaurus](https://uts.nlm.nih.gov/metathesaurus.html) library.\n3. Generate new knowledge/relations using [Relational Graphical Convoluation Network](https://arxiv.org/pdf/1703.06103.pdf).\n\n## Elements\n\n**Unified Medical Language System**\n\nThe UMLS, or Unified Medical Language System, is a set of files and software sponsored by [National Library of Medicine](https://www.nlm.nih.gov/) that brings together many health and biomedical vocabularies and standards to enable interoperability between computer systems.\n\nGetting access to UMLS and other softwares depending on UMLS(like Semantic Knowledge Representation in our work) requires a license. One can apply for the license from the [UMLS Terminology Services](https://uts.nlm.nih.gov//license.html).\n\n**UMLS Metathasaurus**\n\nWe use UMLS Metathasaurus in this work. UMLS Metathasaurus is a major knowledge source in UMLS that consists of biomedical entities, types and relations between them. Every entity in UMLS Metathasaurus has a unique CUI, several aliases of the entity, types like Gene, Disease and some existing known relations between these entities. Let us give two entities and one relation in UMLS Metathasaurus as an example\n\n*Entity*\n\n| CUI | aliases | Semantic Type | \n| --- | :--: | :--: |\n|C0034417 |quinine, Chinin etc. | Organic Chemical, Pharmacologic Substance etc. | \n|C0024535 | Malaria, Falciparum, Subtertian malaria etc | Disease or Syndrome|\n\n*Relation*\n\n| CUI2 | relation | CUI1 | \n| --- | :--: | :--: |\n|C0024535 |may_be_treated_by| C0034417| \n\nwhich stands for \"Malaria may be treated by Quinine\"\n\nIn our work, we explicitly use existing relations in UMLS Metathasaurus to enrich the relations we extract from biomedical papers using SemRep which is introduced later in this section.\n\n**UMLS Semantic Network**\n\nThe Semantic Network consists of (1) a set of broad subject categories, or Semantic Types, of concepts represented in the UMLS Metathesaurus, and (2) a set of useful and important relationships, or Semantic Relations, that exist between Semantic Types. Let's also give examples of such Semantic types and Semantic relations in the Network\n\n*Semantic Relations*\n\n| Semantic Type | relation | Semantic Type | \n| --- | :--: | :--: |\n|Pharmacologic Substance | treats | Injury or Poisoning\t| \n|Pharmacologic Substance | treats | Disease or Syndrome\t|\n\nWe did not explicitly use the Semantic Network in our work but it's an input of the relation extraction tool SemRep below. \n\n\n**Semantic Knowledge Representation (SemRep)**\n\nSemRep is a UMLS-based program that extracts three-part propositions, called semantic predications, from sentences in biomedical text along with UMLS entities that are present in the text. The extraction is done by\n\n1. Recognizing UMLS Metathasaurus concepts and their semantic types in texts using another UMLS-based program [Metamap](https://metamap.nlm.nih.gov/).\n2. Match relation stated as predicate in the text. For example, predicate such as 'treats' will be recognized.\n3. UMLS Semantic Network is employed to verify the validity of the relation. For example, entity of type 'Pharmacologic Substance' can 'treat' entity of type 'Disease or Syndrome'\n\nLet's also give one example of such extraction\n\n    Input: Quinine can treat Lung cancer\n    Partial Output: \n        entities: \n            C0034417 Quinine\n            C0242379 Malignant neoplasm of lung \n        relations:\n            C0034417|Quinine|TREATS|C0242379|Malignant neoplasm of lung \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utils for graph building\ndef get_entity_type_dict(graph):\n    ''' Get entity type'''\n    entity_type_dict = {}\n    entity_list = set([triplet[0] for triplet in graph]) | set([triplet[2] for triplet in graph])\n    for entity in entity_list:\n        entity_type = entity.split('_')[0]\n        entity_type_dict[entity] = entity_type   \n    return entity_type_dict\n\ndef get_graph_stats(graph):\n    ''' Show stats of the graph'''\n    nodes = set([edge[0] for edge in graph]) | set([edge[2] for edge in graph])\n    nodes_type = set([edge[0].split('_')[0] for edge in graph]) | set([edge[2].split('_')[0] for edge in graph])\n    predicates = set([edge[1] for edge in graph])    \n    print('Graph has {} edges, {} nodes, {} types of nodes, {} types of relations.'.\\\n          format(len(graph), len(nodes), len(nodes_type), len(predicates)))    \n\ndef deduplicate_graph(graph):\n    '''Deduplicate triplets in the graph'''\n    return [t.split('=>') for t in set(['=>'.join(t) for t in graph])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracted files\n\nThere are three data files of extracted triplets:\n\n1. Entity -> Entity relationships extracted from papers using [Semantic Knowledge Representation](https://semrep.nlm.nih.gov/) is in the file 'semrep_rela.csv'.\n2. Entity -> Entity relationships as exsiting knowledge in [UMLS Metathasaurus](https://uts.nlm.nih.gov/metathesaurus.html) library is in the file 'umls_rela.csv'.\n3. Paper -> Entity relationships extracted from papers using [Metamap](https://metamap.nlm.nih.gov/) is in the file 'semrep_entity.csv'.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Build graph from extracted triplets files\ndef add_triplets_semrep_rela(graph):\n    '''Entity-Entity relationship extracted from papers using semrep\n    Return: update graph with Entity -> Entity relationship\n    '''\n    print('\\nAdding realtions from semrep_rela...')    \n    df_semrep = pd.read_csv('/kaggle/input/covid-challenge/semrep_rela.csv', header=0, names=['s', 'r', 'o'])\n    for index, row in df_semrep.iterrows():        \n        graph.append([row['s'], '1_' + row['r'], row['o']])\n    return graph\n\ndef add_triplets_umls_rela(graph):\n    '''Add entity-entity relationship from ULMS'''\n    print('\\nAdding realtions from umls_rela...')\n    entity_type_dict = get_entity_type_dict(graph)\n    df_umls = pd.read_csv('/kaggle/input/covid-challenge/umls_rela.csv', header=0, names=['s', 'r', 'o'])    \n    for index, row in df_umls.iterrows():\n        if row['s'] in entity_type_dict or row['o'] in entity_type_dict:\n            graph.append([row['s'], '2_' + row['r'], row['o']])           \n    return graph\n\ndef add_triplets_semrep_entity(graph):\n    '''This is to add a new paper-entity relationship into the existing graph\n    Return: graph for Paper -> Entity relationship\n    '''\n    print('\\nAdding realtions from semrep_entity...')\n    entity_type_dict = get_entity_type_dict(graph)\n    df_semrep_entity = pd.read_csv('/kaggle/input/covid-challenge/semrep_entity.csv', header=0, names=['s', 'r', 'o'])\n    for index, row in df_semrep_entity.iterrows():\n        if row['o'] in entity_type_dict:\n            graph.append(['Paper_' + row['s'], '0_' + row['r'] + row['o'].split('_')[0], row['o']])\n    return graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = []\n\ngraph = add_triplets_semrep_rela(graph)\ngraph = deduplicate_graph(graph)\nget_graph_stats([[triplet[0], triplet[1].split('_', 1)[1], triplet[2]] for triplet in graph])\n\ngraph = add_triplets_umls_rela(graph)\ngraph = deduplicate_graph(graph)\nget_graph_stats([[triplet[0], triplet[1].split('_', 1)[1], triplet[2]] for triplet in graph])\n\ngraph = add_triplets_semrep_entity(graph)\ngraph = deduplicate_graph(graph)\nget_graph_stats([[triplet[0], triplet[1].split('_', 1)[1], triplet[2]] for triplet in graph])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Graph structure\n\n* Knowledge graph consists of triplets like **<subject, predicate, object>**;\n* **subject** and **object** are nodes and **predicate** is relation;\n* There are 6 types of nodes in the graph: Disease, Symptom, PharmaSub, Gene, Chemical and Paper. Each node starts with its type and ends with a global unique id, combined by '_';\n* There are 55 types of relations with direction:\n * Entity -> Entity relations extracted from papers start with '1_'.\n * Entity -> Entity relations from UMLS start with '2_'.\n * Paper -> Entity relations start with '0_'.\n \nExample as below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_graph = pd.DataFrame(graph, columns=['subj', 'pred', 'obj'])\ndf_graph.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use R-GCN (Relational Graph Convolutional Network) to predict links\n\n## Purpose\n\nLink Prediction (LP), the task of predicting missing facts among entities already a knowledge graph (recovery\nof missing facts, i.e. subject-predicate-object triples), is a promising and widely studied task aimed at addressing knowledge graph incompleteness. We also want to complete our knowledge graph with LP.  \n\nAmong the recent LP techniques, those based on knowledge graph embeddings (aims to embed the entities and relationships of a knowledge graph in low-dimensional vector spaces) have achieved very promising performances in some benchmarks.\n\n\n## Elements\n\n**R-GCN**\n\nR-GCN ([Relational Graphical Convoluation Network](https://arxiv.org/pdf/1703.06103.pdf)) is related to a recent class of neural networks operating on graphs, and is developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases.\n\nIt assumes that knowledge bases store collections of triples of the form **<subject, predicate, object>**. Consider, for example, the triple <Mikhail Baryshnikov, educated at, Vaganova Academy>, where we will refer to Baryshnikov and Vaganova Academy as entities and to educated at as a relation. Additionally, it assumes that entities are labeled with types. It is convenient to represent knowledge bases as directed labeled multigraphs with entities corresponding to nodes and triples encoded by labeled edges.\n\nThe link prediction model of R-GCN can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult.\n\n**DGL**\n\nDeep Graph Library ([DGL](https://docs.dgl.ai/index.html)) is a Python package built for easy implementation of graph neural network model family, on top of existing DL frameworks (e.g. PyTorch, MXNet, Gluon etc.). Need to install it before using.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install dgl to implement R-GCN\n!pip install dgl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nimport dgl\nfrom dgl.nn.pytorch import RelGraphConv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Consists of utils for R-GCN link prediction\nimport utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Link prediction model of R-GCN\nclass BaseRGCN(nn.Module):\n    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n                 num_hidden_layers=1, dropout=0,\n                 use_self_loop=False, use_cuda=False):\n        super(BaseRGCN, self).__init__()\n        self.num_nodes = num_nodes\n        self.h_dim = h_dim\n        self.out_dim = out_dim\n        self.num_rels = num_rels\n        self.num_bases = None if num_bases < 0 else num_bases\n        self.num_hidden_layers = num_hidden_layers\n        self.dropout = dropout\n        self.use_self_loop = use_self_loop\n        self.use_cuda = use_cuda\n\n        # create rgcn layers\n        self.build_model()\n\n    def build_model(self):\n        self.layers = nn.ModuleList()\n        # i2h\n        i2h = self.build_input_layer()\n        if i2h is not None:\n            self.layers.append(i2h)\n        # h2h\n        for idx in range(self.num_hidden_layers):\n            h2h = self.build_hidden_layer(idx)\n            self.layers.append(h2h)\n        # h2o\n        h2o = self.build_output_layer()\n        if h2o is not None:\n            self.layers.append(h2o)\n\n    def build_input_layer(self):\n        return None\n\n    def build_hidden_layer(self, idx):\n        raise NotImplementedError\n\n    def build_output_layer(self):\n        return None\n\n    def forward(self, g, h, r, norm):\n        for layer in self.layers:\n            h = layer(g, h, r, norm)\n        return h\n    \n    \nclass EmbeddingLayer(nn.Module):\n    def __init__(self, num_nodes, h_dim):\n        super(EmbeddingLayer, self).__init__()\n        self.embedding = nn.Embedding(num_nodes, h_dim)\n\n    def forward(self, g, h, r, norm):\n        return self.embedding(h.squeeze())\n    \n\nclass RGCN(BaseRGCN):\n    def build_input_layer(self):\n        return EmbeddingLayer(self.num_nodes, self.h_dim)\n\n    def build_hidden_layer(self, idx):\n        act = F.relu if idx < self.num_hidden_layers - 1 else None\n        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n                self.num_bases, activation=act, self_loop=True,\n                dropout=self.dropout)\n\n    \nclass LinkPredict(nn.Module):\n    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n        super(LinkPredict, self).__init__()\n        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n                         num_hidden_layers, dropout, use_cuda)\n        self.reg_param = reg_param\n        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n        nn.init.xavier_uniform_(self.w_relation,\n                                gain=nn.init.calculate_gain('relu'))\n\n    def calc_score(self, embedding, triplets):\n        # DistMult\n        s = embedding[triplets[:,0]]\n        r = self.w_relation[triplets[:,1]]\n        o = embedding[triplets[:,2]]\n        score = torch.sum(s * r * o, dim=1)\n        return score\n\n    def forward(self, g, h, r, norm):\n        return self.rgcn.forward(g, h, r, norm)\n\n    def regularization_loss(self, embedding):\n        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n\n    def get_loss(self, g, embed, triplets, labels):\n        # triplets is a list of data samples (positive and negative)\n        # each row in the triplets is a 3-tuple of (source, relation, destination)\n        score = self.calc_score(embed, triplets)\n        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n        reg_loss = self.regularization_loss(embed)\n        return predict_loss + self.reg_param * reg_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main body\nclass KnowledgeGraph:\n    '''Knowledge graph class.\n    Enable us store the graph, train model, load model, predict links, search the graph and get sub-graph.'''\n    def __init__(self, df_graph):\n        self.df_graph_full = df_graph.copy()\n        self.df_graph = df_graph[df_graph.apply(lambda row: row[1].split('_')[0], axis=1) != '0'].copy()\n        self.df_graph.reset_index(inplace=True, drop=True)\n        self.generate_dictionary()\n        self.total_data = self.generate_dataset()\n        self.num_nodes, self.num_rels, self.num_edges = self.get_stats()\n        \n    def generate_dictionary(self):\n        print('Generating dictionaries for all entities and relations in the graph...')\n        \n        # entity-index and relation-index\n        self.entity_dict = {}\n        self.relation_dict = {}\n        # index-entity and index-relation\n        self.inverse_entity_dict = {}\n        self.inverse_relation_dict = {}\n        # count entities and relations\n        entity_index = 0\n        relation_index = 0\n        \n        for index, row in self.df_graph.iterrows():\n            if row[0] not in self.entity_dict:\n                self.entity_dict[row[0]] = entity_index\n                self.inverse_entity_dict[entity_index] = row[0]\n                entity_index += 1\n                \n            if row[2] not in self.entity_dict:\n                self.entity_dict[row[2]] = entity_index\n                self.inverse_entity_dict[entity_index] = row[2]\n                entity_index += 1\n                \n            subj_type = row[0].split('_')[0]\n            obj_type = row[2].split('_')[0]\n            relation = row[1].split('_', 1)[1]\n            if (subj_type, relation, obj_type) not in self.relation_dict:\n                self.relation_dict[(subj_type, relation, obj_type)] = relation_index\n                self.inverse_relation_dict[relation_index] = (subj_type, relation, obj_type)\n                relation_index += 1\n        \n        print('Done.\\n')\n            \n    def get_stats(self):\n        num_nodes = len(self.entity_dict)\n        num_rels = len(self.relation_dict)\n        num_edges = len(self.df_graph)\n        print('# entities:', num_nodes)\n        print('# relations:', num_rels)\n        print('# edges:', num_edges)\n        \n        return num_nodes, num_rels, num_edges\n        \n    def generate_dataset(self):\n        print('Generating dataset for the model...')\n        # Transfer name to index in the dataset\n        triplet_list = []\n        for index, row in self.df_graph.iterrows():\n            triplet = []\n\n            subj_type = row[0].split('_')[0]\n            obj_type = row[2].split('_')[0]\n            \n            relation = row[1].split('_', 1)[1]\n\n            triplet.append(self.entity_dict[row[0]])\n            triplet.append(self.relation_dict[(subj_type, relation, obj_type)])\n            triplet.append(self.entity_dict[row[2]])\n\n            triplet_list.append(triplet)\n\n        total_data = np.asarray(triplet_list)\n        print('Done.\\n')\n        \n        return total_data\n    \n    def get_train_test_data(self, n_train, n_valid, n_test):\n        '''\n        '''\n        # Split dataset into train, valid and test\n        n_train = n_train\n        n_valid = n_valid\n        n_test = n_test\n\n        np.random.seed(777)\n        shuffle = np.random.permutation(self.num_edges)\n        train_data = self.total_data[shuffle[0:n_train]]\n        valid_data = self.total_data[shuffle[n_train:n_train + n_valid]]\n        test_data = self.total_data[shuffle[n_train + n_valid:n_train + n_valid + n_test]]\n        \n        return train_data, valid_data, test_data\n    \n    def create_model(self, n_hidden, num_bases, num_hidden_layers, dropout, use_cuda, reg_param):\n        \"\"\"\"\"\"\n        model = LinkPredict(self.num_nodes,\n                            n_hidden,\n                            self.num_rels,\n                            num_bases=num_bases,\n                            num_hidden_layers=num_hidden_layers,\n                            dropout=dropout,\n                            use_cuda=use_cuda,\n                            reg_param=reg_param)\n        return model\n    \n    def train_model(self, model_state_file,\n                    n_train=500000, n_valid=5000, n_test=5000,\n                    n_hidden=500, num_bases=100, num_hidden_layers=2, dropout=0.2, use_cuda=False, reg_param=0.01,\n                    lr=0.01, graph_batch_size=30000, graph_split_size=0.5,\n                    negative_sample=10, edge_sampler='uniform', grad_norm=1.0,\n                    evaluate_every=200, eval_batch_size=500, eval_protocol='filtered', n_epochs=800):\n        \"\"\"\"\"\"\n        # split dataset\n        train_data, valid_data, test_data = self.get_train_test_data(n_train, n_valid, n_test)\n        # validation and testing triplets\n        valid_data = torch.LongTensor(valid_data)\n        test_data = torch.LongTensor(test_data)\n\n        model = self.create_model(n_hidden, num_bases, num_hidden_layers, dropout, use_cuda, reg_param)\n        \n        # build test graph\n        new_g = dgl.DGLGraph()\n        test_graph, test_rel, test_norm = utils.build_test_graph(new_g, self.num_nodes, self.num_rels, train_data)\n        test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n        test_node_id = torch.arange(0, self.num_nodes, dtype=torch.long).view(-1, 1)\n        test_rel = torch.from_numpy(test_rel)\n        test_norm = utils.node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))\n\n        # build adj list and calculate degrees for sampling\n        adj_list, degrees = utils.get_adj_and_degrees(self.num_nodes, train_data)\n\n        # optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n        forward_time = []\n        backward_time = []\n        \n        # training loop\n        print(\"start training...\")\n\n        epoch = 0\n        best_mrr = 0\n        while True:\n            model.train()\n            epoch += 1\n\n            # perform edge neighborhood sampling to generate training graph and data\n            new_g = dgl.DGLGraph()\n            g, node_id, edge_type, node_norm, data, labels = \\\n                utils.generate_sampled_graph_and_labels(\n                    new_g, train_data, graph_batch_size, graph_split_size,\n                    self.num_rels, adj_list, degrees, negative_sample,\n                    edge_sampler)\n            print(\"Done edge sampling\")\n\n            # set node/edge feature\n            node_id = torch.from_numpy(node_id).view(-1, 1).long()\n            edge_type = torch.from_numpy(edge_type)\n            edge_norm = utils.node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n            data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n            deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n\n            t0 = time.time()\n            embed = model(g, node_id, edge_type, edge_norm)\n            loss = model.get_loss(g, embed, data, labels)\n            t1 = time.time()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm) # clip gradients\n            optimizer.step()\n            t2 = time.time()\n\n            forward_time.append(t1 - t0)\n            backward_time.append(t2 - t1)\n            print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n                  format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n\n            optimizer.zero_grad()\n\n            # validation\n            if epoch % evaluate_every == 0:\n                model.eval()\n                print(\"start eval\")\n                embed = model(test_graph, test_node_id, test_rel, test_norm)\n                mrr = utils.calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n                                     valid_data, test_data, hits=[1, 3, 10], eval_bz=eval_batch_size,\n                                     eval_p=eval_protocol)\n                # save best model\n                if mrr >= best_mrr:\n                    best_mrr = mrr\n                    torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n                               model_state_file)\n\n                if epoch >= n_epochs:\n                        break\n\n        print(\"Training done!\")\n        print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n        print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))\n    \n    def load_model(self, model_state_file,\n                   n_hidden=500, num_bases=100, num_hidden_layers=2, dropout=0.2, use_cuda=False, reg_param=0.01):\n        '''\n        '''\n        model = self.create_model(n_hidden, num_bases, num_hidden_layers, dropout, use_cuda, reg_param)\n        \n        # use best model checkpoint\n        checkpoint = torch.load(model_state_file)\n        model.eval()\n        model.load_state_dict(checkpoint['state_dict'])\n        print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n\n        # build total graph\n        new_g = dgl.DGLGraph()\n        total_graph, total_rel, total_norm = utils.build_test_graph(new_g, self.num_nodes, self.num_rels, self.total_data)\n        total_deg = total_graph.in_degrees(range(total_graph.number_of_nodes())).float().view(-1, 1)\n        total_node_id = torch.arange(0, self.num_nodes, dtype=torch.long).view(-1, 1)\n        total_rel = torch.from_numpy(total_rel)\n        total_norm = utils.node_norm_to_edge_norm(total_graph, torch.from_numpy(total_norm).view(-1, 1))\n\n        # get embed weights\n        self.total_embed = model(total_graph, total_node_id, total_rel, total_norm)\n        self.w = model.w_relation\n\n    def save_model_with_np(self):\n        ''''''\n        total_embed = graph.total_embed.detach().numpy()\n        np.save('total_embed.npy', total_embed)\n        w = graph.w.detach().numpy()\n        np.save('w.npy', w)\n        \n    def load_model_from_np(self):\n        ''''''\n        total_embed = np.load('/kaggle/input/rgcn-model/total_embed.npy')\n        self.total_embed = torch.from_numpy(total_embed)\n        w = np.load('/kaggle/input/rgcn-model/w.npy')\n        self.w = torch.from_numpy(w)\n        \n    def predict_score_with_s_r_o(self, s, r, o):\n        '''\n        '''\n        s_index = self.entity_dict[s]\n        r_index = self.relation_dict[r]\n        o_index = self.entity_dict[o]\n        \n        emb_s = self.total_embed[s_index]\n        emb_r = self.w[r_index]\n        emb_o = self.total_embed[o_index]\n\n        emb_triplet = emb_s * emb_r * emb_o\n        score = torch.sigmoid(torch.sum(emb_triplet))\n        \n        return score\n    \n    def get_filtered_index_list(self, entity_type):\n        '''entity_type: 'Paper', 'Disease', 'Gene', 'PharmaSub', 'Symptom', 'Chemical'\n        '''\n        \n        if entity_type not in ['Paper', 'Disease', 'Gene', 'PharmaSub', 'Symptom', 'Chemical']:\n            raise Exception('entity_type should not be {:s}'.format(entity_type))\n        \n        filtered_index_list = []\n        for entity, index in self.entity_dict.items():\n            if entity.split('_')[0] == entity_type:\n                filtered_index_list.append(index)\n        filtered_index_list = torch.LongTensor(filtered_index_list)\n        \n        return filtered_index_list\n    \n    def get_most_possible_subject_with_relation_and_object(self, r, o):\n        \n        subject_entity_type = r[0]\n        filtered_s = self.get_filtered_index_list(subject_entity_type)\n        \n        r = (r[0], r[1].split('_', 1)[1], r[2])\n        r_index = self.relation_dict[r]\n        o_index = self.entity_dict[o]\n        \n        # Load weights of the model\n        emb_s = self.total_embed[filtered_s]\n        emb_r = self.w[r_index]\n        emb_o = self.total_embed[o_index]\n        \n        emb_triplet = emb_s * emb_r * emb_o\n        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n        _, indices = torch.sort(scores, descending=True)\n        \n        subject_list = []\n        for i in indices:\n            # Get the entity index\n            entity_index = int(filtered_s[int(i)])\n            subject_list.append(self.inverse_entity_dict[entity_index])\n            \n        return subject_list\n    \n    def get_most_possible_object_with_subject_and_relation(self, s, r):\n        \n        s_index = self.entity_dict[s]\n        r = (r[0], r[1].split('_', 1)[1], r[2])\n        r_index = self.relation_dict[r]\n        \n        object_entity_type = r[2]\n        filtered_o = self.get_filtered_index_list(object_entity_type)\n        \n        # Load weights of the model\n        emb_s = self.total_embed[s_index]\n        emb_r = self.w[r_index]\n        emb_o = self.total_embed[filtered_o]\n        \n        emb_triplet = emb_s * emb_r * emb_o\n        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n        _, indices = torch.sort(scores, descending=True)\n        \n        object_list = []\n        for i in indices:\n            # Get the entity index\n            entity_index = int(filtered_o[int(i)])\n            object_list.append(self.inverse_entity_dict[entity_index])\n            \n        return object_list\n    \n    def get_object_with_subject(self, s):\n        ''''''\n        object_list = list(self.df_graph_full[self.df_graph_full.loc[:, 'subj'] == s]['obj'])\n        return object_list\n    \n    def get_subject_with_object(self, o):\n        ''''''\n        subject_list = list(self.df_graph_full[self.df_graph_full.loc[:, 'obj'] == o]['subj'])\n        return subject_list\n    \n    def get_object_with_subject_and_relation(self, s, r):\n        ''''''        \n        object_list = list(self.df_graph_full[(self.df_graph_full.loc[:, 'subj'] == s) &\\\n                                              (self.df_graph_full.loc[:, 'pred'] == r[1])]['obj'])\n        object_list = [x for x in object_list if x.split('_')[0] == r[2]]            \n        return object_list\n        \n    def get_subject_with_relation_and_object(self, r, o):\n        ''''''\n        subject_list = list(self.df_graph_full[(self.df_graph_full.loc[:, 'obj'] == o) &\\\n                                               (self.df_graph_full.loc[:, 'pred'] == r[1])]['subj'])\n        subject_list = [x for x in subject_list if x.split('_')[0] == r[0]]            \n        return subject_list\n    \n    def get_subgraph(self, df_subgraph=None, seed='Disease_C5203670', degree=1):            \n        df_subgraph = pd.concat([df_subgraph, self.df_graph[(self.df_graph.loc[:, 'subj'] == seed) |\\\n                                                            (self.df_graph.loc[:, 'obj'] == seed)]])\n        if degree > 1:\n            seeds = list(set(df_subgraph['subj']) | set(df_subgraph['obj']))\n            for seed in seeds[:10]:\n                df_subgraph = self.get_subgraph(df_subgraph, seed, degree=degree-1)\n        df_subgraph.reset_index(inplace=True, drop=True)\n        return df_subgraph\n    \n    def get_subgraph_full(self, seed='Disease_C5203670', degree=1):            \n        df_subgraph = self.get_subgraph(seed=seed, degree=degree)\n        seeds = list(set(df_subgraph['subj']) | set(df_subgraph['obj']))\n        df_subgraph_full = df_graph[(df_graph['subj'].isin(seeds)) & (df_graph['obj'].isin(seeds))]\n        df_subgraph_full.reset_index(inplace=True, drop=True)\n        return df_subgraph_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the graph\ngraph = KnowledgeGraph(df_graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and load R-GCN model\n\nOnly run training code once:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Train\n# graph.train_model('model_state.pth', n_train=130000)\n\n# # Load full model on full graph\n# graph.load_model('/kaggle/working/model_state.pth')\n\n# # Save the vectorized information of the knowledge graph\n# graph.save_model_with_np()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After training and saving, we get two vetorized results in output: 'total_embed.npy' and 'w.npy'. They are embedded vectors representing nodes and relations. Need to move them to '/kaggle/input/rgcn-model/' for loading:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the vectorized information of the knowledge graph\ngraph.load_model_from_np()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Commentary\n\n## Approach\n\nTo generate commentary on target concept, we apply following steps:\n\n1. Recognize the searching terms from users and match them to concepts in our knowledge graph.\n2. Serach the concept in the knowledge graph, find related concepts and visualize them.\n3. Generate commentary from results in step 2, which consists of how many papers talk about the concept, which paper is the most cited, what knowledge do we know about it, what we infer by model and etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Recognize concept\n\nTo recognize searching terms from users, we use trained NER (Named Entity Recognition) model from scispacy. The model 'en_core_sci_sm' uses full spaCy pipeline for biomedical data with a ~100k vocabulary. Then link the entity to entity code in UMLS, while it is also the entity code in our knowledge graph.\n\nInstall scispacy first:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy==0.2.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom scispacy.umls_linking import UmlsEntityLinker\n\n# Load NER model and link to UMLS\nnlp_model = spacy.load('/kaggle/input/scispacy-model/en_core_sci_sm-0.2.4/en_core_sci_sm/en_core_sci_sm-0.2.4')\nlinker = UmlsEntityLinker(resolve_abbreviations=True)\nnlp_model.add_pipe(linker)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The function to recognize users' searching terms\ndef get_entity_information(entity_text, nlp_model, linker, entity_dict):\n    '''Recognize the query string as the concept in the graph.\n    '''\n    entity_type_code = {'T047': 'Disease',\n                        'T028': 'Gene',\n                        'T121': 'PharmaSub',\n                        'T103': 'Chemical',\n                        #'T005': 'Virus',\n                        #'T001': 'Organism',\n                        #'T053': 'Behavior',\n                        'T184': 'Symptom'}\n    \n    entity_information_list = []\n    entity_list = nlp_model(entity_text).ents\n    if len(entity_list) == 0:\n        entity_list = nlp_model(entity_text + ' is').ents\n    \n    for entity in entity_list:\n        entity_flag = False\n        \n        if len(entity._.umls_ents) == 0:\n            # Not in UMLS\n            if str(entity).lower() in ['remdesivir']:\n                entity_code = 'PharmaSub' + '_' + 'C4726677'\n                entity_name = 'Remdesivir'\n                entity_definition = 'Remdesivir is an investigational nucleotide analog' +\\\n                                    'with broad-spectrum antiviral activity bothin vitroandin vivoin animal models' +\\\n                                    'against multiple emerging viral pathogens, including Ebola, Marburg, MERS and SARS.'\n                entity_flag = True\n            elif str(entity).lower() in ['covid-19', 'sars-cov-2', '2019-ncov', 'hcov-19',\n                                         'new coronavirus', 'novel coronavirus']:\n                entity_code = 'Disease' + '_' + 'C5203670'\n                entity_name = 'COVID-19'\n                entity_definition = 'A potentially severe acute respiratory infection' +\\\n                                    'caused by the novel coronavirus severe acute respiratory syndrome' +\\\n                                    'coronavirus 2 (SARS-CoV-2).'\n                entity_flag = True\n        else:\n            # In UMLS\n            for umls_entity, score in entity._.umls_ents:\n                if score == 1:\n                    cui_text = linker.umls.cui_to_entity[umls_entity]\n                    cui_id = cui_text[0]\n                    for tui_id in cui_text[3]:\n                        if tui_id in entity_type_code:\n                            # In desired entity types\n                            semantic_type = entity_type_code[tui_id]\n                            entity_code = semantic_type + '_' + cui_id\n                            entity_name = cui_text[1]\n                            entity_definition = cui_text[4]\n                            if entity_code in graph.entity_dict:\n                                entity_flag = True\n                                break\n                if entity_flag == True:\n                    break\n        \n        if entity_flag == True:\n            if (entity_code, entity_name, entity_definition) not in entity_information_list:\n                entity_information_list.append((entity_code, entity_name, entity_definition))\n        \n    return entity_information_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Search and visualize related concepts\n\nWe use pyecharts to visualize subgraph. It enables excellent interactive experience in Jupyter Notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyecharts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyecharts import options as opts\nfrom pyecharts.charts import Graph\n\nclass GraphVisualization:\n    '''Visualize subgraph'''\n    def __init__(self, subgraph, name_dict, title='COVID-19 knowledge graph', repulsion=80, labelShow=False):\n        self.subgraph = subgraph\n        self.color = {'Disease': '#FF7F50', 'Gene': '#48D1CC', 'PharmaSub': '#B3EE3A',\n                      'Chemical': '#C71585', 'Symptom': '#FF0000'}\n        self.cate = {'Disease': 0, 'Gene': 1, 'PharmaSub': 2,\n                     'Chemical': 3, 'Symptom': 4}\n        self.categories = [{'name':'Disease', 'itemStyle': {'normal': {'color': self.color['Disease']}}},\n                           {'name':'Gene', 'itemStyle': {'normal': {'color': self.color['Gene']}}},\n                           {'name':'PharmaSub', 'itemStyle': {'normal': {'color': self.color['PharmaSub']}}},\n                           {'name':'Chemical', 'itemStyle': {'normal': {'color': self.color['Chemical']}}},\n                           {'name':'Symptom', 'itemStyle': {'normal': {'color': self.color['Symptom']}}}]\n        self.visual = self.visualize_graph(name_dict, title, repulsion, labelShow)\n        \n    def get_entity_name(self, entity_code, name_dict):\n        if entity_code.split('_')[0] == 'Paper':\n            entity_name = entity_code\n        else:\n            entity_name = name_dict[entity_code]\n        return entity_name\n\n    def get_nodes_stats(self):\n        reverse_cate = dict([(v,k) for (k,v) in self.cate.items()])\n        return pd.Series([reverse_cate[node['category']] for node in self.nodes]).value_counts()\n\n    def visualize_graph(self, name_dict, title, repulsion, labelShow):\n\n        self.nodes = []\n        for entity in list(set(self.subgraph['subj']) | set(self.subgraph['obj'])):\n            self.nodes.append({'name': self.get_entity_name(entity, name_dict),\n                               'symbolSize': max(10, np.log1p(sum((self.subgraph['subj']==entity) |\\\n                                                                  (self.subgraph['obj']==entity)))*10//1),\n                               'category':self.cate[entity.split('_')[0]]})\n        self.links = []\n        for index, row in self.subgraph.iterrows():\n            self.links.append({'source': self.get_entity_name(row[0], name_dict),\n                               'target': self.get_entity_name(row[2], name_dict),\n                               'value': row[1]})\n            \n        g = (\n             Graph()\n             .add('', self.nodes, self.links, self.categories,\n                  repulsion=repulsion, label_opts=opts.LabelOpts(is_show=labelShow))\n             .set_global_opts(title_opts=opts.TitleOpts(title=title), legend_opts=opts.LegendOpts(orient='vertical',\n                                                                                                  pos_left='2%',\n                                                                                                  pos_top='40%',\n                                                                                                  legend_icon='circle'))\n             .render_notebook()\n            )\n        return g","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load these entities' names to transfer them from entity code to actual name:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_name_dict():\n    name_dict = {}\n    # Disease\n    df_disease = pd.read_csv('/kaggle/input/entity-name/disease.csv', header=None, names=['name', 'code'])\n    df_disease = df_disease.drop_duplicates(['code'], keep='first')\n    for index, row in df_disease.iterrows():\n        name_dict['Disease' + '_' + row[1]] = row[0]\n    name_dict['Disease_C5203670'] = 'COVID-19'\n    # PharmaSub\n    df_pharma = pd.read_csv('/kaggle/input/entity-name/pharma_sub.csv', header=None, names=['name', 'code'])\n    df_pharma = df_pharma.drop_duplicates(['code'], keep='first')\n    for index, row in df_pharma.iterrows():\n        name_dict['PharmaSub' + '_' + row[1]] = row[0]\n    name_dict['PharmaSub_C4726677'] = 'Remdesivir'\n    # Symptom\n    df_symptom = pd.read_csv('/kaggle/input/entity-name/symptom.csv', sep=';', header=None, names=['name', 'code'])\n    df_symptom = df_symptom.drop_duplicates(['code'], keep='first')\n    for index, row in df_symptom.iterrows():\n        name_dict['Symptom' + '_' + row[1]] = row[0]\n    # Gene    \n    df_gene = pd.read_csv('/kaggle/input/entity-name/gene.csv', header=None, names=['name', 'code'])\n    df_gene = df_gene.drop_duplicates(['code'], keep='first')\n    for index, row in df_gene.iterrows():\n        name_dict['Gene' + '_' + row[1]] = row[0]\n    # Chemical   \n    df_chemical = pd.read_csv('/kaggle/input/entity-name/chemical.csv', header=None, names=['name', 'code'])\n    df_chemical = df_chemical.drop_duplicates(['code'], keep='first')\n    for index, row in df_chemical.iterrows():\n        name_dict['Chemical' + '_' + row[1]] = row[0]\n    return name_dict\n\n# Load entity name dictionary\nname_dict = generate_name_dict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to get the most cited paper using page rank score:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_most_cited_paper_title(paper_list):\n    df_score = pd.read_csv('/kaggle/input/covid-challenge/df_paper_score.csv')\n    \n    df_part = df_score[df_score['id'].isin(paper_list)]\n    if len(df_part) > 0:\n        title = df_part.sort_values(by='score').iloc[-1, 1]\n    else:\n        title = ''\n    \n    return title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Commentary generation\n\nWe generate different kinds of commentary for different types of concepts: Symptom, Pharmacy Substance, Disease, Gene and Chemical.\n\nGenerally, for each concept we generate commentary about:\n1. How many papers talk about the concept. Do they also talk about COVID-19. Which of them is the most cited.\n2. What do we know about this concept from these public papers.\n3. What does UMLS library talk about this concept.\n4. What do we infer by R-GCN.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Utils\ndef get_commentary_paper(paper_list):\n    if len(paper_list) > 0:\n        commentary_paper = 'There are {:d} papers mentioned it in the dataset.'.format(len(paper_list))\n        title = get_most_cited_paper_title(paper_list)\n        if title != '':\n            commentary_paper += ' \"{:s}\" is the most cited.'.format(title)\n    else:\n        commentary_paper = ' No paper mentioned it in the dataset.'\n    return commentary_paper\n\ndef get_commentary_covid(paper_list):\n    all_covid_paper = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTDisease', 'Disease'), 'Disease_C5203670')\n    covid_paper_list = list(set(paper_list).intersection(set(all_covid_paper)))\n    commentary_covid = ''\n    if len(covid_paper_list) > 0:\n        commentary_covid = ' {:d} of these papers also mentioned COVID-19.'.format(len(covid_paper_list))\n    else:\n        if len(paper_list) > 0:\n            commentary_covid = ' None of these papers mentioned COVID-19.'\n    return commentary_covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seperate functions for different types of concepts:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_commentary_symptom(graph, entity_information, name_dict):\n    # Unpack the information of the entity\n    entity_code, entity_name, entity_definition = entity_information\n    commentary_definition = '\\nDefinition: ' + entity_definition + '\\n'\n    \n    # Get the list of papers mentioned this symptom\n    paper_list = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTSymptom', 'Symptom'), entity_code)\n    commentary_paper = get_commentary_paper(paper_list)\n    commentary_covid = get_commentary_covid(paper_list)\n    \n    # Information from UMLS: disease\n    disease_list = graph.get_object_with_subject_and_relation(entity_code, ('Symptom', '2_MANIFESTATION_OF', 'Disease'))\n    disease_name_list = ['\"' + name_dict[disease] + '\"' for disease in disease_list if disease in name_dict]\n    if len(disease_name_list) > 0:\n        commentary_disease = '\\n' + ', '.join(disease_name_list[0:5]) + ' also has this symptom.'\n    else:\n        commentary_disease = ''\n    \n    # Information from UMLS: pharma\n    pharma_list = graph.get_subject_with_relation_and_object(('PharmaSub', '2_TREATS', 'Symptom'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma = '\\n' + ', '.join(pharma_name_list[0:5]) + ' may treat this symptom.'\n    else:\n        commentary_pharma = ''\n        \n    # Information from papers: disease\n    disease_list = graph.get_object_with_subject_and_relation(entity_code, ('Symptom', '1_MANIFESTATION_OF', 'Disease'))\n    disease_name_list = ['\"' + name_dict[disease] + '\"' for disease in disease_list if disease in name_dict]\n    if len(disease_name_list) > 0:\n        commentary_disease_paper = '\\nFrom papers, we know: ' + ', '.join(disease_name_list[0:5]) + ' also has this symptom.'\n    else:\n        commentary_disease_paper = ''\n    \n    # Information from papers: pharma\n    pharma_list = graph.get_subject_with_relation_and_object(('PharmaSub', '1_TREATS', 'Symptom'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma_paper = '\\nFrom papers, we know: ' + ', '.join(pharma_name_list[0:5]) + ' may treat this symptom.'\n    else:\n        commentary_pharma_paper = ''\n        \n    # Predict with the model: pharma\n    pharma_list = graph.get_most_possible_subject_with_relation_and_object(('PharmaSub', '1_TREATS', 'Symptom'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma_model = '\\nGraph model predicts: ' + ', '.join(pharma_name_list[0:5]) + ' may treat this symptom.'\n    else:\n        commentary_pharma_model = ''\n        \n    commentary = 'Your query is known as \"{:s}\", symptom.'.format(entity_name) + commentary_definition +\\\n                 commentary_paper + commentary_covid +\\\n                 commentary_disease + commentary_pharma +\\\n                 commentary_disease_paper + commentary_pharma_paper +\\\n                 commentary_pharma_model\n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_commentary_pharma(graph, entity_information, name_dict):\n    # Unpack the information of the entity\n    entity_code, entity_name, entity_definition = entity_information\n    commentary_definition = '\\nDefinition: ' + entity_definition + '\\n'\n    \n    # Get the list of papers mentioned this pharmacy\n    paper_list = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTPharmaSub', 'PharmaSub'), entity_code)\n    commentary_paper = get_commentary_paper(paper_list)\n    commentary_covid = get_commentary_covid(paper_list)\n   \n    # Information from UMLS: disease\n    disease_list = graph.get_object_with_subject_and_relation(entity_code, ('PharmaSub', '2_TREATS', 'Disease'))\n    disease_name_list = ['\"' + name_dict[disease] + '\"' for disease in disease_list if disease in name_dict]\n    if len(disease_name_list) > 0:\n        commentary_disease = '\\n' + ', '.join(disease_name_list[0:5]) + ' may be treated by this pharmacy substance.'\n    else:\n        commentary_disease = ''\n    \n    # Information from papers: disease\n    disease_list = graph.get_object_with_subject_and_relation(entity_code, ('PharmaSub', '1_TREATS', 'Disease'))\n    disease_name_list = ['\"' + name_dict[disease] + '\"' for disease in disease_list if disease in name_dict]\n    if len(disease_name_list) > 0:\n        commentary_disease_paper = '\\nFrom papers, we know: ' + ', '.join(disease_name_list[0:5]) +\\\n                                   ' may be treated by this pharmacy substance.'\n    else:\n        commentary_disease_paper = ''\n        \n    # Predict with the model: disease\n    disease_list = graph.get_most_possible_object_with_subject_and_relation(entity_code, ('PharmaSub', '1_TREATS', 'Disease'))\n    disease_name_list = ['\"' + name_dict[disease] + '\"' for disease in disease_list if disease in name_dict]\n    if len(disease_name_list) > 0:\n        commentary_disease_model = '\\nGraph model predicts: ' + ', '.join(disease_name_list[0:5]) +\\\n                                   ' may be treated by this pharmacy substance.'\n    else:\n        commentary_disease_model = ''\n    \n    commentary = 'Your query is known as \"{:s}\", pharmacy substance.'.format(entity_name) + commentary_definition +\\\n                 commentary_paper + commentary_covid +\\\n                 commentary_disease + commentary_disease_paper +\\\n                 commentary_disease_model\n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_commentary_disease(graph, entity_information, name_dict):\n    # Unpack the information of the entity\n    entity_code, entity_name, entity_definition = entity_information\n    commentary_definition = '\\nDefinition: ' + entity_definition + '\\n'\n    \n    paper_list = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTDisease', 'Disease'), entity_code)\n    commentary_paper = get_commentary_paper(paper_list)\n    commentary_covid = ''\n    if entity_code != 'Disease_C5203670':\n        commentary_covid = get_commentary_covid(paper_list)\n    \n    # Information from UMLS: symptom\n    symptom_list = graph.get_subject_with_relation_and_object(('Symptom', '2_MANIFESTATION_OF', 'Disease'), entity_code)\n    symptom_name_list = ['\"' + name_dict[symptom] + '\"' for symptom in symptom_list if symptom in name_dict]\n    if len(symptom_name_list) > 0:\n        commentary_symptom = '\\n' + ', '.join(symptom_name_list[0:5]) + ' may be the symptoms of this disease.'\n    else:\n        commentary_symptom = ''\n        \n    # Information from UMLS: pharma\n    pharma_list = graph.get_subject_with_relation_and_object(('PharmaSub', '2_TREATS', 'Disease'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma = '\\n' + ', '.join(pharma_name_list[0:5]) + ' may treat this disease.'\n    else:\n        commentary_pharma = ''\n        \n    # Information from papers: symptom\n    symptom_list = graph.get_subject_with_relation_and_object(('Symptom', '1_MANIFESTATION_OF', 'Disease'), entity_code)\n    symptom_name_list = ['\"' + name_dict[symptom] + '\"' for symptom in symptom_list if symptom in name_dict]\n    if len(symptom_name_list) > 0:\n        commentary_symptom_paper = '\\nFrom papers, we know: ' + ', '.join(symptom_name_list[0:5]) +\\\n                                   ' may be the symptoms of this disease.'\n    else:\n        commentary_symptom_paper = ''\n        \n    # Information from papers: pharma \n    pharma_list = graph.get_subject_with_relation_and_object(('PharmaSub', '1_TREATS', 'Disease'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma_paper = '\\nFrom papers, we know: ' + ', '.join(pharma_name_list[0:5]) + ' may treat this disease.'\n    else:\n        commentary_pharma_paper = ''\n        \n    # Predict with the model: symptom\n    symptom_list = graph.get_most_possible_subject_with_relation_and_object(('Symptom', '1_MANIFESTATION_OF', 'Disease'),\n                                                                            entity_code)\n    symptom_name_list = ['\"' + name_dict[symptom] + '\"' for symptom in symptom_list if symptom in name_dict]\n    if len(symptom_name_list) > 0:\n        commentary_symptom_model = '\\nGraph model predicts: ' + ', '.join(symptom_name_list[0:5]) +\\\n                                   ' may be the symptoms of this disease.'\n    else:\n        commentary_symptom_model = ''\n        \n    # Predict with the model: pharma \n    pharma_list = graph.get_most_possible_subject_with_relation_and_object(('PharmaSub', '1_TREATS', 'Disease'), entity_code)\n    pharma_name_list = ['\"' + name_dict[pharma] + '\"' for pharma in pharma_list if pharma in name_dict]\n    if len(pharma_name_list) > 0:\n        commentary_pharma_model = '\\nGraph model predicts: ' + ', '.join(pharma_name_list[0:5]) + ' may treat this disease.'\n    else:\n        commentary_pharma_model = ''\n    \n    # Final commentary\n    commentary = 'Your query is known as \"{:s}\", disease.'.format(entity_name) + commentary_definition +\\\n                 commentary_paper + commentary_covid +\\\n                 commentary_symptom + commentary_pharma +\\\n                 commentary_symptom_paper + commentary_pharma_paper +\\\n                 commentary_symptom_model + commentary_pharma_model\n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_commentary_gene(graph, entity_information, name_dict):\n    # Unpack the information of the entity\n    entity_code, entity_name, entity_definition = entity_information\n    commentary_definition = '\\nDefinition: ' + entity_definition + '\\n'\n    \n    paper_list = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTGene', 'Gene'), entity_code)\n    commentary_paper = get_commentary_paper(paper_list)\n    commentary_covid = get_commentary_covid(paper_list)\n    \n    # Final commentary\n    commentary = 'Your query is known as \"{:s}\", gene.'.format(entity_name) + commentary_definition +\\\n                 commentary_paper + commentary_covid\n    \n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_commentary_chemical(graph, entity_information, name_dict):\n    # Unpack the information of the entity\n    entity_code, entity_name, entity_definition = entity_information\n    commentary_definition = '\\nDefinition: ' + entity_definition + '\\n'\n    \n    paper_list = graph.get_subject_with_relation_and_object(('Paper', '0_ISABOUTChemical', 'Chemical'), entity_code)\n    commentary_paper = get_commentary_paper(paper_list)\n    commentary_covid = get_commentary_covid(paper_list)\n    \n    # Final commentary\n    commentary = 'Your query is known as \"{:s}\", chemical.'.format(entity_name) + commentary_definition +\\\n                 commentary_paper + commentary_covid\n    \n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main function for commentary generation\ndef generate_commentary(graph, entity_information, name_dict, semantic_type):\n    if semantic_type == 'Symptom':\n        commentary = generate_commentary_symptom(graph, entity_information, name_dict)\n    elif semantic_type == 'PharmaSub':\n        commentary = generate_commentary_pharma(graph, entity_information, name_dict)\n    elif semantic_type == 'Disease':\n        commentary = generate_commentary_disease(graph, entity_information, name_dict)\n    elif semantic_type == 'Gene':\n        commentary = generate_commentary_gene(graph, entity_information, name_dict)\n    elif semantic_type == 'Chemical':\n        commentary = generate_commentary_chemical(graph, entity_information, name_dict)\n    else:\n        commentary = 'Not available now.'\n    return commentary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main search function\ndef search(entity_text, nlp_model, linker, graph, name_dict):\n    entity_information_list = get_entity_information(entity_text, nlp_model, linker, graph.entity_dict)\n    \n    if len(entity_information_list) == 0:\n        raise Exception('Not an exsiting entity in the knowledge graph.')\n    elif len(entity_information_list) > 1:\n        raise Exception('More than one entity. Please search one at a time.')\n    else:\n        entity_information = entity_information_list[0]\n        entity_code, entity_name, entity_definition = entity_information\n        semantic_type = entity_code.split('_')[0]\n        \n        # Visualize the subgraph\n        subgraph = graph.get_subgraph(seed=entity_code)\n        g = GraphVisualization(subgraph, name_dict,\n                               title='1st Degree Connection Knowledge Graph of {:s}'.format(entity_name))\n        # Generate commentary\n        commentary = generate_commentary(graph, entity_information, name_dict, semantic_type)\n        \n    return g, commentary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Real-world case analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g, commentary = search('COVID-19', nlp_model, linker, graph, name_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.visual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.get_nodes_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(commentary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g, commentary = search('Breathlessness', nlp_model, linker, graph, name_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.visual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.get_nodes_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(commentary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g, commentary = search('Remdesivir', nlp_model, linker, graph, name_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.visual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.get_nodes_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(commentary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Future Works","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we introduced our knowledge graph-based commentary generation tool for COVID-19 related concepts. Future works includes using graph database for our knowledge graph for more user friendly user interaction, developing a web service with better visualization and connecting to open medical library for automatical crawlling and updating.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}