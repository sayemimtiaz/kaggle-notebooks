{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\n\nSequences of discrete tokens can be found in many applications, namely words in a text, notes in a musical composition, pixels in an image, actions in a reinforcement learning agent, etc [[1]](https://arxiv.org/pdf/1801.00632.pdf). These sequences often show a strong correlation between consecutive or nearby tokens. The correlations on words in a sentence or characters in words express the underlying semantics and language characteristics.\nThe next token in the sequence $x_n$ can be modeled as\n\n$$p(x_n|x_{n-1}, x_{n-2},...,x_1)$$\n\nwhere $x_i$ represents the $i^{th}$ token in the sequence. In Natural Language Processing (NLP) they are defined as language models and usually, each token stands for a separate word or $n$-gram. The output generated is a probability distribution from which we can sample to generate the next token in the sequence. These models are also known as recurrent, as we can apply this generative process recurrently to create entire new sequences of tokens.\n\nOne particular type of generative model often used to tackle problems with sequences of discrete tokens is Recurrent Neural Networks (RNN). In a simpler neural network, a fixed-dimensional feature representation is transformed several times by different non-linear functions. In an RNN, these transformations are also repeated in time, which means that at every time step a new input is processed that generates a new output. They can effectively capture semantically rich representations of the input sequences [[2]](https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf). RNN showed this capacity in different settings, such as generating structured text, original images (on a per pixels basis) or even modeling user behavior on online services.\n\nOur task is to generate original text that resemble a training corpus. It is an unsupervised task, as we do not have access to any labeling or target variable. We start by creating a word embedding that maps each character to a vector with a parameterized dimension. For each character the model looks up the embedding and feeds the result to a stack of Long Short-Term Memory (LSTM) layers, a specific type of RNN. These were developed to extend the traditional RNNs capacity to model long-term dependencies and counter the vanishing gradient problem. The output of our network is a dense layer with a number of units equal to the vocabulary size. We did not define an activation function for this layer; it simply outputs one logit for each character in the vocabulary. We use these values to later sample from a categorical distribution.\n\nIn this article, we use the work of Fernando Pessoa, one of the most significant literary figures of the 20th century and one of the greatest poets in the Portuguese language. This dataset is now publicly available on Kaggle and consists of more than 4300 poems, essays and other writings [[3]](https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa).","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Preprocessing\n\nThe dataset comprises several texts written by the author under different names, heteronyms, and pseudonyms. Each one has his own style of writing, which could be interesting to learn separately and compare. Nevertheless, to efficiently train Deep Neural Networks (DNN) we need a large dataset. F. Pessoa lived part of his youth in South Africa, where he was exposed to the English language. That is why part of his work is written in English. To avoid introducing noise, we remove most of the English texts from the training dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nimport ast\nimport os\nimport json\nimport matplotlib.pyplot as plt\nfrom nltk import tokenize\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_pessoa = pd.read_csv('/kaggle/input/the-complete-literary-works-of-fernando-pessoa/f_pessoa_v2.csv')\n\ntexts = f_pessoa.copy()\n\n# Removing all pseudonyms that wrote in english.\n\ntexts = texts[~texts['author'].isin(['Alexander Search', 'David Merrick', 'Charles Robert Anon', 'I. I. Crosse'])]\n\ntexts['text'] = texts['text'].apply(lambda t: ast.literal_eval(t))\ntexts = texts.reset_index().drop('index', axis=1)\ntexts = texts['text'].tolist()\ntexts = np.concatenate(texts)\n\ntexts = np.asarray(texts)\ntexts_p = \" \".join(texts)\n\n# we will be truncating large texts soon, so this code only tries to reduce the \n# sequence size by spliting the texts that seem to be significantely larger than \n# the rest. Otherwise, we try to use the structure provided in the data itself\n\n_, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nmylen = np.vectorize(len)\n\nsns.histplot(mylen(texts), bins=50, ax=ax[0])\nax[0].set_title('Histogram of the number of characters in each \\nchunk of text BEFORE splitting sentences', fontsize=16)\n\nlarge_texts = texts[mylen(texts)>350]\nlarge_texts_p = \" \".join(large_texts)\nlarge_texts = tokenize.sent_tokenize(large_texts_p)\n\ntexts = np.concatenate((texts[~(mylen(texts)>350)], large_texts))\n\nax[1].set_title('Histogram of the number of characters in each \\nchunk of text AFTER splitting sentences', fontsize=16)\nsns.histplot(mylen(texts), bins=50, ax=ax[1]);\n\nprint(f'Length of texts dataset: {len(texts_p)} characters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After cleaning up the texts, we end up with more than 5.8M of characters. Notice that in order to avoid losing data when normalizing the text length of our sequences, we split the largest sequences by sentence. The difference in the distribution of the sequence length can be seen in the histograms above. We can preview some of the sequences.","metadata":{}},{"cell_type":"code","source":"print(texts[97:106])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More importantly, we can assess the number of unique characters, which is our vocabulary size.","metadata":{}},{"cell_type":"code","source":"vocab = sorted(set(texts_p))\nprint(f'{len(vocab)} unique characters in texts')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before training, we need to convert the strings to some numerical representation. We started by tokenizing the text with some important aspects in mind. We considered an unlimited number of tokens and created them at the character level. We did not filter any character and kept the original capitalization. We then use the tokenizer to map our texts to encoded sequences.","metadata":{}},{"cell_type":"code","source":"def create_character_tokenizer(list_of_strings):\n    tokenizer = Tokenizer(filters=None,\n                         char_level=True, \n                          split=None,\n                         lower=False)\n    tokenizer.fit_on_texts(list_of_strings)\n    return tokenizer\n\ntokenizer = create_character_tokenizer(texts)\n\ntokenizer_config = tokenizer.get_config()\n\nword_counts = json.loads(tokenizer_config['word_counts'])\nindex_word = json.loads(tokenizer_config['index_word'])\nword_index = json.loads(tokenizer_config['word_index'])\n\ndef strings_to_sequences(tokenizer, list_of_strings):\n    sentence_seq = tokenizer.texts_to_sequences(list_of_strings)\n    return sentence_seq\n\nseq_texts = strings_to_sequences(tokenizer, texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see an example of this encoding.","metadata":{}},{"cell_type":"code","source":"print('Original sequence: \\n' + texts[0] + '\\n')\nprint('Encoded sequence: ')\nprint(seq_texts[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also need to normalize the length of our sequences, for which we define a length of 300 characters. Sequences smaller than 300 are padded with zeros, while sequences bigger than 300 are truncated.","metadata":{}},{"cell_type":"code","source":"mylen = np.vectorize(len)\n\nprint(max(mylen(texts)))\nprint(np.round(np.mean(mylen(texts))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_padded_dataset(sequences):\n    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequences,\n                                                 maxlen=300,\n                                                 padding='pre',\n                                                 truncating='pre',\n                                                 value=0)\n    return padded_sequence\n\npadded_sequences = make_padded_dataset(seq_texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RNN works by receiving a sequence of characters and predicting the next character in the sequence. At training time, the model receives an input sequence and a target sequence, which is shifted by one.\n\nFor example, the expression `Diana através dos ramos` is the first verse of the first poem on our dataset. The poem is from Ricardo Reis, one of the many heteronyms of F.Pessoa. Given the input `Diana através dos ramo`, the correct prediction is `iana através dos ramos`. Notice that the prediction is the same length as the input.\n\nAnother decision we took was to build our RNN to be stateful, which means that its internal state is maintained across batches. For this to be effective, we need to make sure that each batch element follows on from the corresponding element of the preceding batch.","metadata":{}},{"cell_type":"code","source":"def create_inputs_and_targets(array_of_sequences, batch_size=32):\n    input_seq = array_of_sequences[:,:-1]\n    target_seq = array_of_sequences[:,1:]\n    \n    # Prepare the batches and ensure that is ready to be fed to a stateful RNN\n    \n    num_examples = input_seq.shape[0]\n\n    num_processed_examples = num_examples - (num_examples % batch_size)\n\n    input_seq = input_seq[:num_processed_examples]\n    target_seq = target_seq[:num_processed_examples]\n\n    steps = int(num_processed_examples / 32) \n\n    inx = np.empty((0,), dtype=np.int32)\n    for i in range(steps):\n        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, steps)))\n\n    input_seq_stateful = input_seq[inx]\n    target_seq_stateful = target_seq[inx]\n    \n    # Split data between training and validation sets\n    \n    num_train_examples = int(batch_size * ((0.8 * num_processed_examples) // batch_size))\n\n    input_train = input_seq_stateful[:num_train_examples]\n    target_train = target_seq_stateful[:num_train_examples]\n\n    input_valid = input_seq_stateful[num_train_examples:]\n    target_valid = target_seq_stateful[num_train_examples:]\n    \n    # Create datasets objects for training and validation data\n    \n    dataset_train = tf.data.Dataset.from_tensor_slices((input_train, target_train))\n    dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\n\n    dataset_valid = tf.data.Dataset.from_tensor_slices((input_valid, target_valid))\n    dataset_valid = dataset_valid.batch(batch_size, drop_remainder=True)\n    \n    return (dataset_train, dataset_valid)\n    \n\ntrain_data, valid_data = create_inputs_and_targets(padded_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Recurrent Neural Network\n\nWe started by defining an embedding layer that turns our indexes of characters into dense vectors of fixed size. It is important to note that padded values are masked in this layer, which means they are ignored. Next, we stacked 2 unidirectional stateful LSTM layers, each with 512 units. These layers have the potential to learn long-term dependencies; however, they are computationally expensive to train. In between them, we introduced a dropout layer. Finally, the last layer outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model. Notice that we get a total of about 4M parameters to train.","metadata":{}},{"cell_type":"code","source":"def get_model(vocab_size, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim = 256, mask_zero=True, batch_input_shape=(batch_size, None)),\n        tf.keras.layers.LSTM(units=512, return_sequences=True,stateful=True),\n        #tf.keras.layers.Dropout(0.2),\n        #tf.keras.layers.LSTM(units=512, return_sequences=True,stateful=True),\n        tf.keras.layers.Dense(units=vocab_size)\n    ])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\nmodel = get_model(len(tokenizer.word_index) + 1, batch_size)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of epochs is very low due to computational constraints\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath='./models/ckpt',\n                                                       save_weights_only=True,\n                                                       save_best_only=True)\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['sparse_categorical_accuracy'])\nhistory = model.fit(train_data, \n                    epochs=5, \n                    validation_data=valid_data,\n                    callbacks=[checkpoint_callback, tf.keras.callbacks.EarlyStopping(patience=2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_history(history):\n    history_dict = dict()\n    for k, v in history.history.items():\n        history_dict[k] = [float(val) for val in history.history[k]]\n    return history_dict\n\n\nhistory_dict = model_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Results\n\nThe training is quite slow even using GPU (despite reducing the training time for a factor of 15 compared to CPU) and recall that we only stacked two LSTM layers with a limited number of units. From the figure below, we can see a rapid increase of the accuracy on both the training and validation datasets and then a steady climb for several epochs. Our callback is eventually executed (when there is no increase in the validation accuracy for more than 2 epochs) to stop the training process. There was no sign of overfitting.\n\nThe resulting original text is quite interesting to analyze. Remeber that our RNN had to learn the portuguese language from scratch with a fairly small dataset. No explicitly information sucha as syntax of semantics, is at any time provided to the model other than pratical examples on writings in Portuguese. The dataset is also fairly small for the task. Nevertheless, there are interesting learnings to take notice. For example, in terms of punctuation, the quotation marks are used correctly, showing the understanding that they are required to open and close. In sentences such as _Desassossego não poderia!... Falências no meu coração..._ or _As canções... é um sono de ouvir... Ficção tanto!..._ we can almost grasp some of the rentlessness of Fernando Pessoa. On the other hand, we see that the meaning or intention is not something that an RNN can capture and we can also identify some ortographic errors.","metadata":{}},{"cell_type":"code","source":"def plot_history(history_dict):\n    \n    plt.figure(figsize=(15,5))\n    plt.subplot(121)\n    plt.plot(history_dict['sparse_categorical_accuracy'])\n    plt.plot(history_dict['val_sparse_categorical_accuracy'])\n    plt.title('Accuracy vs. epochs')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.xticks(np.arange(len(history_dict['sparse_categorical_accuracy'])))\n    ax = plt.gca()\n    ax.set_xticklabels(1 + np.arange(len(history_dict['sparse_categorical_accuracy'])))\n    plt.legend(['Training', 'Validation'], loc='lower right')\n\n    plt.subplot(122)\n    plt.plot(history_dict['loss'])\n    plt.plot(history_dict['val_loss'])\n    plt.title('Loss vs. epochs')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.xticks(np.arange(len(history_dict['sparse_categorical_accuracy'])))\n    ax = plt.gca()\n    ax.set_xticklabels(1 + np.arange(len(history_dict['sparse_categorical_accuracy'])))\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.show() \n    \nplot_history(history_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(len(tokenizer.word_index) + 1, batch_size=1)\nmodel.load_weights(tf.train.latest_checkpoint('./models/')).expect_partial()\n\ndef get_logits(model, token_sequence, initial_state1=None):\n    token_sequence = np.asarray(token_sequence)\n    if initial_state1 is not None:\n        # set states for all recurrent layers\n        model.layers[1].states = initial_state1\n    else:\n        model.layers[1].reset_states()\n    logit = model.predict(token_sequence)\n    logit = logit[:,-1,:]\n    \n    return logit\n\ndef sample_token(logits):\n    pred = tf.random.categorical(logits, num_samples=1).numpy()[0]\n    return pred[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To generate text from our model, we need to specify a seed string to get the network started. Next, we tokenize the initial string and reset the state of the network. The string is then converted to a tensor with a batch size of 1 to fed to our model. We used the prediction from the last time step to build a categorical distribution and sample from it afterward. Using the same state of our network and the previously sampled token, we can repeat the prediction step until we get the final sequence with the specified size.","metadata":{}},{"cell_type":"code","source":"init_string = 'Desassossego'\nnum_generation_steps = 500\n\ntoken_sequence = tokenizer.texts_to_sequences([init_string])\ninitial_state_1, initial_state_2 = None, None\ninput_sequence = token_sequence\n\nfor _ in range(num_generation_steps):\n    logits = get_logits(model, \n                        input_sequence, \n                        initial_state1=initial_state_1)\n    sampled_token = sample_token(logits)\n    token_sequence[0].append(sampled_token)\n    input_sequence = [[sampled_token]]\n    initial_state_1 = model.layers[1].states\n    \nprint(tokenizer.sequences_to_texts(token_sequence)[0][::2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Conclusion\n\nFor this task, the preprocessing of the data is challenging. We need to ensure that we have our input sequence encoded in a way that is suitable for the RNN to effectively capture the available semantic representation. RNNs are particularly computationally expensive to train, so we decided to keep the structure as simple as possible. The results are interesting as we were able to generate text in Portuguese without proving any structural information about the language to the model other than writings of a poet. The model learned some of the fundamental structure of the language while preserving some nuances that we can consider similar to the training corpus.\n\nThis approach can be extended by increasing the depth of the model with more recurrent layers and the number of units in each layer. Hyperparameters such as the batch size can also be tuned to increase accuracy. We tested the possibility to separate by form of writting, training one DNN with texts in prose and another with texts in poetry. The results were not satisfying, as the DNNs failed to generate text with a coherent structure. We leave it as future work.\n\n# 6. References\n\n[[1]](https://arxiv.org/pdf/1801.00632.pdf) - [De Boom et al., 2018] De Boom, C., Demeester, T., and Dhoedt, B. (2018). Character-level recur-rent neural networks in practice: comparing training and sampling schemes.Neural Computing and Applications, 31(8):4001–4017.\n\n[[2]](https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf) - [Sutskever et al., 2011] Sutskever, I., Martens, J., and Hinton, G. (2011). Generating text with recurrent neural networks. ICML’11, page 1017–1024, Madison, WI, USA. Omnipress.\n\n[[3]](https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa) - https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa","metadata":{}}]}