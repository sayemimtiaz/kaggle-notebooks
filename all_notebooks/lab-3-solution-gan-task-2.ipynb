{"cells":[{"metadata":{},"cell_type":"markdown","source":"<!-- <small>Course: Malicious AI and dark side security. Lab prepared by <a href=\"mailto:linh.vu@monash.edu\">Linh Vu</a>, Monash.</small>\n -->\n# Lab 3. Generative Adversarial Networks\n\n## Task 2: Synthesize face images with DCGAN\n\nGenerative adversarial networks are machine learning systems that can learn to mimic a given distribution of data. GANs consist of two neural networks:\n\n* Generator, which is trained to generate data by learning the distribution of the training set;\n* Discriminator, which is trained to distinguish fake data from real data.\n\nIn this tutorial, we will train a GAN to generate images of new faces after showing it pictures of many real celebrities. We will implement a DCGAN in PyTorch and train on CelebA dataset. There are 3 subtasks:\n* Implement the Discriminator\n* Implement the Generator\n* Training the models\n\nAbout the main process, it is quite silimar to task 1. However, there are some differences:\n* About dataset: CelebA dataset consist of color images\n    * The number of channels is 3\n    * We use different transformer to preprocess the image\n    * We use `torchvision.utils.make_grid` to help visualize the images\n* About the GAN:\n    * We apply a custom weights initialization on generator and discriminator\n    * The architectures of models are different\n* About training process, for each epoch:\n    * First, we feed the latent vectors to the generator and get the generated data\n    * Next, we train the discriminator with real data and generated data separatedly\n    * Then, we continue to train the generator with its output\n    * Finally, to visualize the progression of the generator, we generate samples from the same fixed latent vectors\n   "},{"metadata":{},"cell_type":"markdown","source":"### Initialization\n\nFirst, we setup environment & prepapre data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\n\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nfrom matplotlib import rcParams\nrcParams[\"savefig.jpeg_quality\"] = 50\nimport imageio\nfrom pathlib import Path\n# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up a random generator seed so that the experiment can be replicated identically on any machine\ntorch.manual_seed(111)\n\n# Create a device object that points to the CPU or GPU if available\ndevice = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# Batch size during training\nbatch_size = 128\n\n# Set image size for the transformer\nimage_size = 64\n\n# Number of channels in images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Define the transform to load images from CelebA dataset\ntransform = transforms.Compose([transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                              ])\n\n# Load CelebA dataset from Kaggle input folder\ntrain_set = torchvision.datasets.ImageFolder(\n    root=\"/kaggle/input/celeba-dataset\", transform=transform\n)\n\n# Create a data loader to shuffle the data from train_set and return data in batches for training\ntrain_loader = torch.utils.data.DataLoader(\n    train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_samples, celebA_labels = next(iter(train_loader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_samples.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom weights initialization called on generator and discriminator\ndef weights_init(m):\n    \"\"\"\n    Randomly initialize all weights to mean=0, stdev=0.2\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subtask 2.1: Implement the Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator Code\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the generator\ngenerator = Generator().to(device)\n\n# Apply the weights_init function\ngenerator.apply(weights_init)\n\n# Print the model\nprint(generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subtask 2.2: Implement the Discriminator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the Discriminator\ndiscriminator = Discriminator().to(device)\n\n# Apply the weights_init function\ndiscriminator.apply(weights_init)\n\n# Print the model\nprint(discriminator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare for training\n\nBefore training the models, we set up some parameters to use during training: learning rate, loss function, optimization algorithm, as well as define some helper classes to support visualization.\n\nSet up parameters for training process:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of training epochs\nnum_epochs = 5\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Initialize BCELoss function\nloss_function = nn.BCELoss()\n\n# Setup Adam optimizers for both generator and discriminator\noptimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define helper functions to support visualization: We want to create an animation to illustrate how the generator learn to generate more realistic data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import base64\nfrom IPython import display\n\ndef show_gif(file_path):\n    \"\"\"\n    To show gif or video in Colab, we need to load the data and encode with base64.\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        b64 = base64.b64encode(file.read()).decode('ascii')\n    return display.HTML(f'<img src=\"data:image/gif;base64,{b64}\" />')\n\n# Create batch of fixed latent vectors that we will use to visualize the progression of the generator\nfixed_latent_vectors = torch.randn((batch_size, nz, 1, 1)).to(device)\n\ndef generate_images(title=False, output_path=False, file_name=False, show=True):\n    \"\"\"\n    Generate images from a random vector using the generator.\n    Input:\n    title: title of the image showing how many epochs that the generator is trained\n    output_path: if you want to save file, define the output folder \n    show: display the plot or not. Set to False if you just want to save the image\n    Output:\n    file_path: path of the generated image file\n    \"\"\"     \n    with torch.no_grad():\n        # Generate data from fixed_latent_vectors with the generator \n        generated_samples = generator(fixed_latent_vectors)\n        # Move the data back to the CPU and create a view of data (without gradients)\n        generated_samples = generated_samples.cpu().detach()\n    # Plot the data\n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.imshow(np.transpose(vutils.make_grid(generated_samples.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n    # Save to file  \n    file_path = \"\"       \n    if output_path:\n        Path(output_path).mkdir(parents=True, exist_ok=True)\n        file_path = f\"{output_path}/{file_name}\"\n        plt.savefig(file_path)\n    # Close the plot if not show\n    if not show:\n        plt.close('all')\n    # Return path of the generated image file\n    return file_path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what the generator can do before training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_images(title='Before training')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subtask 1.3: Training the models\n\nThe GAN training process consists of a two-player minimax game in which:\n* D is adapted to minimize the discrimination error between real and generated samples;\n* G is adapted to maximize the probability of D making a mistake.\n\n**Tasks:**\n* Set the number of epochs you want to train the models\n* Create tensors of labels for data\n* Define the function `train_generator`\n* Define steps to train the models and run the training process\n\nAfter that, visualize your training process and the final result."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get total number of batches. We print the losses after training the last batch of each epoch\nnum_batches = len(train_loader)\n\n## YOUR CODE HERE ##\n# Set how many repetitions of training with the whole dataset (1 line)\nnum_epochs = 5\n\n# Because the labels remain the same for every batch, we create it here:\n# Create tensor of labels for real samples with value=1 and shape is batch_size x 1 (1 line)\nreal_samples_labels = torch.ones((batch_size, 1)).to(device=device)\n# Create tensor of labels for generated samples with value=0 and shape is batch_size x 1 (1 line)\ngenerated_samples_labels = torch.zeros((batch_size, 1)).to(device=device) \n## END CODE HERE ##\n\n# Set where to save the generated image files. We just save it temporarily\noutput_path = '/kaggle/temp/'\noutput_files = []\n\n# Save the losses to visualize\nlosses_discriminator, losses_generator = [], []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_discriminator(real_samples, generated_samples):\n    \"\"\"\n    Train the discriminator model by minimizing its error.\n    Input: \n        real_samples: tensor of images with shape: batch_size x channel x width x height\n        generated_samples: tensor of generated images with shape: batch_size x channel x width x height\n    Return:\n        loss_discriminator: for printing purpose\n    \"\"\"  \n    # Clear the gradients of the discriminator to avoid accumulating them\n    discriminator.zero_grad()\n    \n    # Train the discriminator with the real data\n    output_discriminator_real = discriminator(real_samples)\n    # Calculate the loss function for the discriminator to minimize its error\n    loss_discriminator_real = loss_function(output_discriminator_real, real_samples_labels)\n    # Calculate the gradients for the discriminator\n    loss_discriminator_real.backward()\n\n    # Train the discriminator with the generated data\n    output_discriminator_generated = discriminator(generated_samples)\n    # Calculate the loss function for the discriminator to minimize its error\n    loss_discriminator_generated = loss_function(output_discriminator_generated, generated_samples_labels)\n    # Calculate the gradients for the discriminator\n    loss_discriminator_generated.backward()\n\n    # Calculate the total loss of the discriminator to show later\n    loss_discriminator = loss_discriminator_real + loss_discriminator_generated\n    # Update the weights of the discriminator\n    optimizer_discriminator.step()\n\n    return loss_discriminator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(output_generator):\n    \"\"\"\n    Continue to train the generator model with its output by maximizing the discriminator error.\n    Input:\n        output_generator: output of the generator model when feeding the latent data\n    Return:\n        loss_generator: for printing purpose\n    \"\"\"  \n    # Clear the gradients of the generator to avoid accumulating them (1 line)\n    generator.zero_grad()\n    # Get the discriminator prediction on the generator's output (1 line)\n    output_discriminator_generated = discriminator(output_generator)\n    # Calculate the loss function for the generator to maximize the discriminator error (1 line)\n    loss_generator = loss_function(output_discriminator_generated, real_samples_labels)\n    # Calculate the gradients for the generator (1 line)\n    loss_generator.backward()\n    # Update the weights of the generator (1 line)\n    optimizer_generator.step()\n    \n    return loss_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Repeat the training process based on the number of epochs\nfor epoch in range(num_epochs):\n    # Load training data by batches\n    for batch, (real_samples, _) in enumerate(train_loader):\n        \n        ## YOUR CODE HERE ##\n        \n        ## Prepare data for training\n        # Send real samples from data loader to GPU if available (1 line)\n        real_samples = real_samples.to(device=device)   \n        # Randomize tensor of latent vectors with shape (batch_size x nz x 1 x 1) and send to GPU if available (1 line)\n        latent_vectors = torch.randn((batch_size, nz, 1, 1)).to(device)\n        # Feed the latent vectors to the generator (1 line)\n        output_generator = generator(latent_vectors)\n        # Create new tensor of generated data (without keeping track of the gradients of the generator, to use in training the discriminator) (1 line) \n        generated_samples = output_generator.detach()\n\n        ## Train the discriminator with real data and generated data and get the loss_discriminator (1 line)\n        loss_discriminator = train_discriminator(real_samples, generated_samples)\n\n        ## Continue to train the generator with its output and get the loss_generator (1 line)\n        loss_generator = train_generator(output_generator)\n\n        ## END YOUR CODE HERE ##\n        \n        losses_discriminator += [loss_discriminator]\n        losses_generator += [loss_generator]\n\n        # Print losses\n        if (batch % 500 == 0) or (batch == num_batches - 1):\n            print(f\"Epoch {epoch} - Batch {batch}. Loss D.: {loss_discriminator}. Loss G.: {loss_generator}\")\n            title = f\"After {batch} batches of {epoch} epoch(s)\"\n            file_name = f\"e{epoch:0=4d}b{batch:0=4d}.jpg\"\n            output_files += [generate_images(title, output_path=output_path, file_name=file_name, show=False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(losses_generator,label=\"G\")\nplt.plot(losses_discriminator,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make gif from list of images\nimages = [imageio.imread(file) for file in output_files]\nimageio.mimsave('results.gif', images, fps=5)\nshow_gif('results.gif')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_images(title='After training')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}