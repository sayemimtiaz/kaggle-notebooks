{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will first perform some pre-processing of data, followed by introduction to few different classifiers such as KNN, SVM, Neural network, etc. We will also explain how to tune the hyperparameters for (some of) these classifiers such that the overall performance of model is improved. In the end, as a bonus, we provide a brief intro to K-fold Cross validation which is a way of increasing the accuracy of model by including *all* of the data in the training and testing phases."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing required packages.\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, cross_val_predict\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.machine_learning.ex2 import *\nprint(\"Setup Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the folder name, so it can be added to the file path in next code kernel below (before adding the csv file name)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading dataset\nwine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking a first few lines of the dataset\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check the variables we are working with\nwine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see non-null for each column but in case we want to check how many nulls in each column\nwine.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing data: addition of new categorical column to perform ML agorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the range of values in Quality column so that bins for Good and Bad wines can be created\nwine['quality'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now the column has been changed form numerical to categorical\nwine['quality'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to encode bad as 0 and good as 1, use the sklearn preprocessing function\nlabel_quality = LabelEncoder()\nwine['quality'] = label_quality.fit_transform(wine['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the dataset again to see the quality column having binary values\nwine.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count number of good and bad quality wines\nwine['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating visualization to analyze the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#bar plot using Seaborn package\nsns.countplot(wine['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# without using the seaborn package, generating the visualization\n# wine['quality'].hist() # this treats the 0 and 1's as integers instead of categories\nwine['quality'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Beginning our predictive models with some pre-processing using Scikit functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing dataset into predictor (X) and response features (y)\nX = wine.drop('quality', axis = 1) # axis = 0 means row; axis = 1 means columns; so here we select all columns except quality\ny = wine['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test datasets (using sklearn package)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # using 20% of data for testing hence 0.2; random state is like setting a seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We scale the data using standardized scaling so that columns with higher numerical values (eg. total sulphur dioxide) are not biased compared to columns with very small numerical values (e.g. chlorides)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train) #fit_transform is gonna fit AND transform at the same time, much like label encoder\nX_test = sc.transform(X_test) # we want the same fit (i.e. values of mean and standard deviation for each column) that we used for (centering the) training data so instead of fit_transform() (which internally calls fit() followed by transform()), we just use transform()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking to see how the scaled valued look like\nX_train[1:10] #since X_train is an array now, not a dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are going to look at few models and compare them"},{"metadata":{},"cell_type":"markdown","source":"## 1. Random Forest Classifier\n#### works well for mid-sized data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators= 200) # n_estimators equals how many forests do you need. start with a higher number and bring it down slowly as smaller the model better the fit\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check some of the predicted values\npred_rfc[1:40]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check model's accuracy at prediction using the testing set we have (i.e. y_test)\nprint(classification_report(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way to print out accuracy explicity (using scikit-learn)\naccuracy_score(y_test, pred_rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, pred_rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. KNN - nearest neighbour"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 35) # to see reason for selecting 35, see Section 2.1 below\nknn.fit(X_train, y_train)\npred_knn = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the accuracy\naccuracy_score(pred_knn, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the confusion matrix\nprint(confusion_matrix(pred_knn, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the classification report\nprint(classification_report(pred_knn, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Rule of thumb for selection of number of neigbors in knn\n\n#### how to choose value of k: too small value, too much noice, too big value require larger computational resources\n\nOptions: k = \n1. sqrt(n) where n = number of data points ; \n** if it comes out to be even, add or subtract 1 to make odd (P.S. Odd values of k is preferred to avoid confusion between binary groupings) \n\nhttps://www.youtube.com/watch?v=4HKqjENq9OU : useful link to understand KNN algo at 06:49"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train))\nimport math\nprint(math.sqrt(len(X_train))) # this should be the value of k in knn algo","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will now see how to choose the best value using GridSearch CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# knn = KNeighborsClassifier()\nparameter_candidates = {\n    'n_neighbors': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n}\n\ngrid_knn = GridSearchCV(estimator = knn, param_grid = parameter_candidates, scoring = 'accuracy', cv = 10)\ngrid_knn.fit(X_train, y_train)\ngrid_knn.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn2 = KNeighborsClassifier(n_neighbors = 60)\nknn2.fit(X_train, y_train)\npred_knn2 = knn2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check accuracy after parameter tuning\naccuracy_score(pred_knn2, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. SVM classifier \n#### works better with smaller numbers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC()\nclf.fit(X_train, y_train)\npred_clf = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check model's accuracy at prediction using the testing set we have (i.e. y_test)\nprint(classification_report(y_test, pred_clf))\nprint(confusion_matrix(y_test, pred_clf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way to print out accuracy explicity (using scikit-learn)\naccuracy_score(y_test, pred_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way to print out accuracy explicity (using scikit-learn)\nclf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter Tuning: To increase the accuracy of the classifier we created.\n\nWe need to have a more systematic way of selecting the parameters which were used to create the classifier *(i.e. parameter tuning)*."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search Cross Validation : automatically find good values for the SVM classifier parameters by using tools such as grid search and cross-validation\n\n# Set the parameter candidates\nparameter_candidates = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\n\n# Create a classifier with the parameter candidates\ngrid_svc = GridSearchCV(estimator=clf, param_grid=parameter_candidates, scoring='accuracy', cv=10)\n# estimator = model we are using for hyperparameter tuning\n# param_grid = list of parameters and the range of values for each parameter of the specified estimator\n# cv: to determine the hyper-parameter value set (i.e. [C = , kernel = , gamma = ]) which provides the best accuracy level\n\n# Train the classifier on training data\ngrid_svc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best parameters for our svc model\nprint(grid_svc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's run our SVC again with the best parameters.\nsvc2 = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf') # kernel is a similarity function used to compute similarity between training data points\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svc2.score(X_test, y_test)) #OR\nprint(accuracy_score(pred_svc2, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Stochastic Gradient Descent Classifier \n#### supports different penalties for misclassification "},{"metadata":{"trusted":true},"cell_type":"code","source":"sgdc = SGDClassifier(loss = \"hinge\", penalty = \"l2\", max_iter = 500) #the concrete loss function is set via the loss parameter\nsgdc.fit(X_train, y_train)\npred_sgdc = sgdc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check accuracy\naccuracy_score(pred_sgdc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see the model parameters to be reported in journal papers\nsgdc.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see the intercepts\nsgdc.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Neural networks\n#### works well with large data / text analyses / time series"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpc = MLPClassifier(hidden_layer_sizes = (11,11,11), max_iter = 500) # hidden_layer_size is the number of nodes in each of the three layer. we chose 11 because we have 11 predictor features in our original data\nmlpc.fit(X_train, y_train)\npred_mlpc = mlpc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the accuracy of model\nprint(classification_report(y_test, pred_mlpc))\nprint(confusion_matrix(y_test, pred_mlpc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way to print out accuracy explicity (using scikit-learn)\naccuracy_score(y_test, pred_mlpc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally we want to see what happens when we feed in brand new data into the classifier and see what sort of predictions it churns out\n\n# to do that we first create a vector of values i.e. give as input one row of data  (here we are giving 2 inputs so we will get 2 prediction outputs)\nXnew = [[7.3, 0.58, 0.00, 2.0, 0.065, 15.0, 21.0, 0.9946, 3.36, 0.47, 10.0], \n        [9.3, 0.58, 0.20, 3.0, 0.065, 16.0, 22.0, 0.9946, 5.96, 0.47, 12.0]]\nXnew = sc.transform(Xnew) # very imp step because the classifier was designed using scaled data and any input to the classifier must also be scaled that too using the same 'sc' that was fitted using the original training data\nYnew = mlpc.predict(Xnew)\nYnew # Based on the predictions, both the wine inputs are supposedly poor quality wines (i.e. quality = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\npred_dtc = dtc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(pred_dtc, y_test)\n# print(accuracy_score(pred_dtc, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1 Parameter tuning for Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As of now decision tree classifier was created using default values, but now lets try to figure out the 'best' hyperparameters\n\nparameter_candidates = {\n    'criterion': ['gini', 'entropy'],\n    'min_samples_leaf' :[1,2,3,4,5],\n    'max_depth': [2,3,4,5,6,7,8,9,10]\n}\n\ngrid_dtc = GridSearchCV(estimator = dtc, param_grid= parameter_candidates, scoring= 'accuracy', cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_dtc.fit(X_train, y_train)\ngrid_dtc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recreating the dtc model but with the new hyperparameters obtained\ndtc2 = DecisionTreeClassifier(criterion= 'gini', max_depth = 2, min_samples_leaf= 1)\ndtc2.fit(X_train, y_train)\npred_dtc2 = dtc2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check accuracy of new model\naccuracy_score(pred_dtc2, y_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BONUS SECTION: 10-fold Cross validation\n\nWhat this means is that currently the models are created using a 80:20 data split (80 for training and 20 for testing). however what if all the extreme cases fall into the testing dataset or vice versa. Thus to avoid this, we create folds (or sections) in our data such that *each* fold is used at least once for testing the model. The overall aim behind doing so is increasing the accuracy of model. The new model will be less biased since it does not depend upon what portion of the data was initially selected for testing/training since we involve ALL data in the model testing/training process."},{"metadata":{},"cell_type":"markdown","source":"### Longer way to do the 10fold CV \n(in few minutes, we will introduce a one-line code for doing the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [] # to store the accuracy obtained from each 'fold'\nbest_clf = SVC(gamma= \"auto\") # we had to set the gamma parameter because we were getting a warning saying in the next version, the default value will change from gamma to auto so it is better to explicitly define one\ncv = KFold(n_splits=10, random_state=42, shuffle=False) #it is a 10-fold cv so n_splits = 10\nfor train_index, test_index in cv.split(X):\n    # printing out the indexes of the training and the testing sets in each iteration \n    # to clearly see the process of K-Fold CV where the training and testing set changes in each iteration\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n    \n    # setting the training and testing sets in each iteration,\n    # followed by generating the model using the X_train and y_train datasets and\n    # finally recording the accuracy for each model (after testing with the X_test and y_test datasets) in the 'scores' array\n    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n    best_clf.fit(X_train, y_train)\n    scores.append(best_clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We appended each score to a list called 'scores' and now, \n# we get the mean value in order to determine the overall accuracy of the model.\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alternative (and much simpler way to do K-fold Cross validation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will give you a list of r2 scores \nbest_clf_eval =  cross_val_score(best_clf, X, y, cv=10)\n\n# we will average all the scores to get a mean value from the 'new' and improved model\nbest_clf_eval.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will give you a list of predictions using the 'new' and improved model\ncross_val_predict(best_clf, X, y, cv=10) # output will be 0 or 1 depending on whether wine was good (0) or bad (1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}