{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport string, os \nimport re\nfrom tqdm import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64  # Batch size for training.\nepochs = 200  # Number of epochs to train for.\nlatent_dim = 512  # Latent dimensionality of the encoding space.\nnum_samples = 50000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading dataset\ndf = pd.read_csv('../input/chatbot-dataset-topical-chat/topical_chat.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic preprocessing\ndef process(text):\n    text = text.lower().replace('\\n', ' ').replace('-', ' ').replace(':', ' ').replace(',', '') \\\n          .replace('\"', ' ').replace(\".\", \" \").replace(\"!\", \" \").replace(\"?\", \" \").replace(\";\", \" \").replace(\":\", \" \")\n\n    text = \"\".join(v for v in text if v not in string.punctuation).lower()\n    #text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n\n    text = \" \".join(text.split())\n    #text+=\"<eos>\"\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.message = df.message.apply(process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_words_set = set()\ntarget_words_set = set()\n\nfor conversation_index in tqdm(range(df.shape[0])):\n    \n    if conversation_index == 0:\n        continue\n        \n    input_text = df.iloc[conversation_index - 1]\n    target_text = df.iloc[conversation_index]\n    \n    if input_text.conversation_id == target_text.conversation_id:\n        \n        input_text = input_text.message\n        target_text = target_text.message\n        \n        if len(input_text.split()) > 2 and \\\n            len(target_text.split()) > 0 and \\\n            len(input_text.split()) < 30 and \\\n            len(target_text.split()) < 10 and \\\n            input_text and \\\n            target_text:\n            \n            target_text = \"bos \" + target_text + \" eos\"\n                \n            input_texts.append(input_text)\n            target_texts.append(target_text)\n            \n            for word in input_text.split():\n                if word not in input_words_set:\n                    input_words_set.add(word)\n            for word in target_text.split():\n                if word not in target_words_set:\n                    target_words_set.add(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_words = sorted(list(input_words_set))\ntarget_words = sorted(list(target_words_set))\nnum_encoder_tokens = len(input_words)\nnum_decoder_tokens = len(target_words)\nmax_encoder_seq_length = max([len(txt.split()) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n\nprint(\"Number of samples:\", len(input_texts))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\ninput_token_index = dict([(word, i) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i) for i, word in enumerate(target_words)])\n\n#saving\nwith open('input_token_index.pickle', 'wb') as handle:\n    pickle.dump(input_token_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n#saving\nwith open('target_token_index.pickle', 'wb') as handle:\n    pickle.dump(target_token_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n)\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n)\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n)\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    \n    for t, word in enumerate(input_text.split()):\n        encoder_input_data[i, t] = input_token_index[word]\n    \n    for t, word in enumerate(target_text.split()):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t] = target_token_index[word]\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[word]] = 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 100\n\n# seq2seq model - https://keras.io/examples/nlp/lstm_seq2seq/\nwith strategy.scope():\n    # Define an input sequence and process it.\n    encoder_inputs = keras.Input(shape=(None,))\n    \n    encoder_embedding_output = keras.layers.Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n                                               \n    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_embedding_output)\n\n    # We discard `encoder_outputs` and only keep the states.\n    encoder_states = [state_h, state_c]\n\n    # Set up the decoder, using `encoder_states` as initial state.\n    decoder_inputs = keras.Input(shape=(None,))\n    \n    decoder_embedding = keras.layers.Embedding(num_decoder_tokens, embedding_size)\n    decoder_embedding_output = decoder_embedding(decoder_inputs)\n    \n\n    # We set up our decoder to return full output sequences,\n    # and to return internal states as well. We don't use the\n    # return states in the training model, but we will use them in inference.\n    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    # Define the model that will turn\n    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    model.compile(\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"history = model.fit(\n    [encoder_input_data, decoder_input_data],\n    decoder_target_data,\n    batch_size=batch_size,\n    epochs=20,\n    validation_split=0.1,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model\nmodel.save(\"s2s.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the encoder model \nencoder_model = keras.Model(encoder_inputs, encoder_states)\nencoder_model.summary()\n\ndecoder_state_input_h = keras.Input(shape=(None,))\ndecoder_state_input_c = keras.Input(shape=(None,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_embedding_output = decoder_embedding(decoder_inputs)\n\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\ndecoder_model = keras.Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)\n# Reverse-lookup token index to decode sequences back \nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.save(\"encoder_model.hdf5\")\ndecoder_model.save(\"decoder_model.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def respond(text):\n    input_seq = np.zeros(\n        (1, max_encoder_seq_length), dtype=\"float32\"\n    )\n    \n    for t, word in enumerate(text.split()):\n        input_seq[0, t] = input_token_index[word]\n        \n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index['bos']\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        \n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == 'eos' or\n           len(decoded_sentence) > 50):\n            stop_condition = True\n        else:\n            decoded_sentence += ' ' + sampled_char\n            \n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n        # Update states\n        states_value = [h, c]\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"respond(\"how are you\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"respond(\"good morning\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"respond(\"good bye\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for seq_index in range(20):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    decoded_sentence = respond(input_texts[seq_index])\n    print(\"-\")\n    print(\"Input sentence:\", input_texts[seq_index])\n    print(\"Decoded sentence:\", decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eos_token = target_token_index['eos']\neos_token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import log\ndef generate_beam_text(seed_text, next_words, beam_search_n, break_at_eos):\n    \n    distributions_scores_states = [[list(), 0.0, [None, None]]]\n    \n    decoder_states_value = None\n    \n    for _ in range(next_words):\n        \n        sequence_temp_candidates = list()\n        \n        for i in range(len(distributions_scores_states)): \n            \n            input_seq = np.zeros(\n                (1, max_encoder_seq_length), dtype=\"float32\"\n            )\n            \n            # Generate empty target sequence of length 1.\n            target_seq = np.zeros((1,1))\n            \n            seq, score, states_values = distributions_scores_states[i]\n            \n            if len(distributions_scores_states) == 1:\n                for t, word in enumerate(process(seed_text).split()):\n                    input_seq[0, t] = input_token_index[word]\n                \n                # Encode the input as state vectors.\n                decoder_states_value = encoder_model.predict(input_seq)\n                \n                # Populate the first character of target sequence with the start character.\n                target_seq[0, 0] = target_token_index['bos']\n                \n            else:\n                target_seq[0, 0] = seq[-1]\n                decoder_states_value = states_values\n                \n                candidate_sentence = \"\"\n                for token_index in seq:\n                    if token_index == eos_token:\n                        break\n                        \n                    word = reverse_target_char_index[token_index]\n                    candidate_sentence+=word + \" \"\n                \n                print(\"score :\", score, \" | \", candidate_sentence)\n            \n            \n            output_tokens_distribution, h, c = decoder_model.predict([target_seq] + decoder_states_value)\n            \n            # Update states\n            decoder_states_value = [h, c]\n\n            predicted_distribution = output_tokens_distribution[0][0]\n            \n            for j in range(len(predicted_distribution)):\n                if predicted_distribution[j] > 0:\n                    candidate = [seq + [j], score - log(predicted_distribution[j]), decoder_states_value]\n                    if break_at_eos and j == eos_token:\n                        continue\n                    else:\n                        sequence_temp_candidates.append(candidate)\n\n        \n        # 2. score and sort all candidates\n        ordered = sorted(sequence_temp_candidates, key=lambda tup:tup[1])\n        \n        distributions_scores_states = ordered[:beam_search_n]\n          \n        print(\"-----\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_beam_text(\"i wonder if they met how that would go from there\", 5, 5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_beam_text(\"do you like comic books\", 4, 5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_beam_text(\"thanks\", 5, 5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_beam_text(\"hi do you like to dance\", 5, 5, False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}