{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom random import shuffle\nimport glob\nfrom PIL import Image\nimport time\nimport functools\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"content1 = '../input/contentimg/shannon-kunkle-dM8INmkyDas-unsplash.jpg'\ncontent2 = '../input/contentimg/bailey-zindel-NRQV-hBF10M-unsplash.jpg'\ncontent3 = '../input/contentimg/sebastian-boring-8zD7rs8UpxU-unsplash.jpg'\n\nwave = '../input/style-images/kanagawa wave.jpg'\nnature = '../input/style-images/nature oil.jpg'\nwheat = '../input/style-images/wheat field vincent.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nstyle = Image.open(wave)\nplt.imshow(style)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nstyle = Image.open(nature)\nplt.imshow(style)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nstyle = Image.open(wheat)\nplt.imshow(style)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\ncont = Image.open(content1)\nplt.imshow(cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize = (15,10))\ncont = Image.open(content2)\nplt.imshow(cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\ncont = Image.open(content3)\nplt.imshow(cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def load_img(path_to_img):\n    max_dim = 1080\n    img = Image.open(path_to_img)\n    long = max(img.size)\n    scale = max_dim/long\n    img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n  \n    img = img_to_array(img)\n  \n  # We need to broadcast the image array such that it has a batch dimension \n    img = np.expand_dims(img, axis=0)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def preprocess_img(img_path):\n    img = load_img(img_path)\n    img = preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VGG19 is trained on images with each channel mean[103.939, 116.779, 123.68] normalized. So we'll inverse preprocess the image. Also convert BGR to RGB. Furthermore, since our optimized image may take its values anywhere between  −∞  and  ∞ , we must clip to maintain our values from within the 0-255 range.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def deprocess_img(process_img):\n    x = process_img.copy()\n    if len(x.shape) == 4:\n        x = np.squeeze(x, axis = 0)\n    assert len(x.shape) == 3   #Expected input shape [1,H,W,C] or [H,W,C]\n    if len(x.shape) !=3:\n        raise ValueError('Invalid input')\n    \n    #We will add the individual channel mean.\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    \n    x = x[..., ::-1]             #Converting BGR to RGB\n    \n    x = np.clip(x, 0, 255).astype('uint8')    #clipping pixel values\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define content and style representations","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://stackoverflow.com/questions/52230874/how-to-correctly-use-an-intermediate-layer-of-a-vgg-model","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def get_model():\n    vgg = VGG19(include_top = False, weights = 'imagenet')\n    vgg.trainable = False\n    \n    content_output = [vgg.get_layer(layer).output for layer in content_layers]\n    style_output = [vgg.get_layer(layer).output for layer in style_layers]\n    output = content_output + style_output\n    return Model(inputs = vgg.inputs, outputs = output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def content_loss(content, base_img):\n    return tf.reduce_mean(tf.square(content - base_img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def gram_matrix(input_tensor):\n    channels = int(input_tensor.shape[-1])\n    vector = tf.reshape(input_tensor, [-1, channels])\n    n = tf.shape(vector)[0]\n    gram = tf.matmul(vector, vector, transpose_a = True)\n    return gram / tf.cast(n, tf.float32)\n\ndef style_loss(style, base_img):\n    gram_style = gram_matrix(style)\n    gram_gen = gram_matrix(base_img)\n    return tf.reduce_mean(tf.square(gram_style - gram_gen))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function will simply load and preprocess both the content and style images from their path. Then it will feed them through the network to obtain the outputs of the intermediate layers. ","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def get_feature_representation(model, content_path, style_path):\n    content_img = preprocess_img(content_path)\n    style_img = preprocess_img(style_path)\n    \n    content_output = model(content_img)\n    style_output = model(style_img)\n    \n    content_features = [content_layer[0] for content_layer in content_output[:len(style_layers)]]\n    style_features = [style_layer[0] for style_layer in style_output[len(style_layers):]]\n    \n    return content_features, style_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def compute_loss(model, content_features, style_features, base_img, loss_weights):\n    \n    \"\"\"This function will compute the loss total loss.\n  \n    Arguments:\n    model: The model that will give us access to the intermediate layers\n    base_img: Our initial base image. This image is what we are updating with \n      our optimization process. We apply the gradients wrt the loss we are \n      calculating to this image.\n    style_features: Precomputed gram matrices corresponding to the \n      defined style layers of interest.\n    content_features: Precomputed outputs from defined content layers of \n      interest.\n    loss_weight: The weights of each contribution of each loss function. \n      (style weight, content weight, and total variation weight)  \n    Returns:\n    returns the total loss, content loss, style loss\n    \"\"\"\n    \n    content_weight, style_weight = loss_weights    #Also known as alpha and beta\n    \n    output = model(base_img)\n    content_base_features = output[:len(style_layers)]    #feature output of base_img w.r.t. content image\n    style_base_features = output[len(style_layers):]    #feature output of base_img w.r.t. style image\n    \n    content_score, style_score = 0, 0\n    \n    weights_per_content_layer = 1.0 / float(len(content_layers))       #getting weights from content layer \n    #content_feature is from content image and content_base_feature are from base_img or generated noise(image)\n    for content_feature, content_base_feature in zip(content_features, content_base_features):\n        content_score += weights_per_content_layer * content_loss(content_feature, content_base_feature[0])\n        \n    weights_per_style_layer = 1.0 / float(len(style_layers))     #getting equally distributed weights from individual layer\n    for style_feature, style_base_feature in zip(style_features, style_base_features):\n        style_score += weights_per_style_layer * style_loss(style_feature, style_base_feature[0])\n        \n    content_score *= content_weight\n    style_score *= style_weight\n    \n    total_loss = content_score + style_score\n    return total_loss, content_score, style_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we use tf.GradientTape to compute the gradient. It allows us to take advantage of the automatic differentiation available by tracing operations for computing the gradient later. It records the operations during the forward pass and then is able to compute the gradient of our loss function with respect to our input image for the backwards pass.  \nBase image is what we are updating with our optimization process. We apply the gradients wrt the loss we are calculating to this image.\nTo check more about GradientTape check below link:  \nhttps://stackoverflow.com/questions/53953099/what-is-the-purpose-of-the-tensorflow-gradient-tape","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def compute_grad(args):\n    with tf.GradientTape() as grad:\n        loss = compute_loss(**args)\n    \n    gradients = grad.gradient(loss[0], args['base_img'])\n    return gradients, loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def style_transfer(content_path, style_path, epochs, content_weight, style_weight):\n    model = get_model()\n    for layer in model.layers:\n        layer.trainable = False\n        \n    content_features, style_features = get_feature_representation(model, content_path, style_path)\n    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n    \n    base_img = preprocess_img(content_path)\n    base_img = tf.Variable(base_img, dtype = tf.float32)\n    \n    #optimizer = Adam(lr = 5, beta_1 = 0.99, epsilon = 1e-1)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n    best_loss, best_img = float('inf'), None    #https://stackoverflow.com/questions/34264710/what-is-the-point-of-floatinf-in-python\n    loss_weights = (content_weight, style_weight)   #alpha and beta\n    \n    args = {'model':model,'content_features':content_features,'style_features':style_features,'base_img':base_img,'loss_weights':loss_weights}\n    \n    channel_normalized_means = np.array([103.939, 116.779, 123.68])\n    min_val = -channel_normalized_means\n    max_val = 255 - channel_normalized_means\n    \n    iter_count = 1\n    \n    plt.figure(figsize=(15, 15))\n    num_rows = (epochs / 100) // 5\n    \n    start_time = time.time()\n    global_start = time.time()\n    \n    images = []\n    for i in range(epochs):\n        gradients, loss = compute_grad(args)\n        total_loss, content_score, style_score = loss\n        optimizer.apply_gradients([(gradients, base_img)])\n        clip = tf.clip_by_value(base_img, min_val, max_val)     #https://stackoverflow.com/questions/44796793/difference-between-tf-clip-by-value-and-tf-clip-by-global-norm-for-rnns-and-how/44798131\n        base_img.assign(clip)\n        end_time = time.time() \n\n        \n        if total_loss < best_loss:\n            best_loss = total_loss\n            best_img = deprocess_img(base_img.numpy())\n  \n    print('Total time: {:.4f}s'.format(time.time() - global_start))\n      \n    return best_img, best_loss             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"best, best_loss = style_transfer(content1, nature, 1000, 1e2, 2e3)\nImage.fromarray(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best, best_loss = style_transfer(content2, wheat, 1000, 1e2, 2e3)\nImage.fromarray(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best, best_loss = style_transfer(content3, wave, 1000, 1e2, 1e3)\nImage.fromarray(best)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}