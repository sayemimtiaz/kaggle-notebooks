{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nfrom itertools import chain\n# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))\n\n\n# Precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show()\n\n# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n   # plt.xlim([0.0,0.001])\n   # plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();\n    \n# Learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n                        n_jobs = 1, train_sizes = np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv = cv, n_jobs = n_jobs, train_sizes = train_sizes)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\",\n             label = \"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\",\n             label = \"Cross-validation score\")\n    plt.legend(loc = \"best\")\n    return plt\n\n# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+/- %0.5f)'%(sc, scores.mean(), scores.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n#     filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/breast-cancer-dataset/dataR2.csv', delimiter=',')\nnRow, nCol = df1.shape\ndf1.dataframeName = 'train.csv'\nprint(f'There are {nRow} rows and {nCol} columns')\nfrom sklearn.utils import shuffle\ndf1 = shuffle(df1)\ndf1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df1, 15, 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Importing Common Modules needed for project\nfrom time import time   # For calculating training time\nimport pandas as pd    # Pandas is used for handling dataFrames\nimport pickle      # Pickle is used to load .pkl dataset files\nimport matplotlib.pyplot as plt    # Pyplot is used for visualizing the information\nfrom sklearn.feature_selection import SelectKBest, f_classif    # For Selecting K-Best features of availabel ones\nfrom sklearn.model_selection import train_test_split, GridSearchCV   # train_test_split for splitting up data\n                                                                     # into train and test\n                                                                     # GridSearchCV for finding best parameters tuned\nfrom sklearn.model_selection import StratifiedShuffleSplit   # For Cross-Validation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # For evaluating different algorithms\nimport numpy as np     # Numpy is used to manipulate data operations,\n                       # Most of the sklearn libraries expects numpy ndarrays as its default arguments\n    \n### Importing Models from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Classification'].replace(1, 0,inplace=True)\ndf1['Classification'].replace(2, 1,inplace=True)\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nwarnings.filterwarnings('ignore') #ignore warning messages\n# 2 datasets\ndata=df1\nM = data[(data['Classification'] != 1)] #Malignant\nB = data[(data['Classification'] == 1)] # healthy\n#------------COUNT-----------------------\ntrace = go.Bar(x = (len(M), len(B)), y = ['malignant', 'benign'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=[ 'gold', 'lightskyblue'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of diagnosis variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#------------PERCENTAGE-------------------\ntrace = go.Pie(labels = ['benign','malignant'], values = data['Classification'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue', 'gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of diagnosis variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distribution(data_select, size_bin) :  \n    tmp1 = M[data_select]\n    tmp2 = B[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['malignant', 'benign']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distribution('Insulin', .5)\nplot_distribution('Age', .5)\nplot_distribution('BMI', .5)\nplot_distribution('HOMA', .5)\nplot_distribution('Leptin', .5)\nplot_distribution('Adiponectin', .5)\nplot_distribution('Resistin', .5)\nplot_distribution('MCP.1', .5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# correlation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation\ncorrelation = data.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   xgap = 2,\n                   ygap = 2,\n                   colorscale='Viridis',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df1.drop(['MCP.1'], axis = 1, inplace = False, errors = 'ignore')\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_2=['Glucose','Resistin','Insulin','HOMA','Classification']\nimport seaborn as sns\nsns.pairplot(df1[features_2],hue='Classification',size=3)\nplt.savefig('pairplot.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x='Classification',y='Insulin',data=df1,size=8)\nplt.savefig('violin_insulin.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Categorical scatterplots:\n* \n* stripplot() (with kind=\"strip\"; the default)\n* \n* swarmplot() (with kind=\"swarm\")\n* \n* Categorical distribution plots:\n* \n* boxplot() (with kind=\"box\")\n* \n* violinplot() (with kind=\"violin\")\n* \n* boxenplot() (with kind=\"boxen\")\n* \n* Categorical estimate plots:\n* \n* pointplot() (with kind=\"point\")\n* \n* barplot() (with kind=\"bar\")\n* \n* * countplot() (with kind=\"count\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Classification\", y=\"Glucose\", data=df1)\nplt.savefig('box_glucose.png')\nsns.boxplot(x=\"Classification\", y=\"Insulin\", data=df1)\nplt.savefig('box_insulin.png')\nsns.boxplot(x=\"Classification\", y=\"HOMA\", data=df1)\nplt.savefig('box_homa.png')\nsns.boxplot(x=\"Classification\", y=\"Resistin\", data=df1)\nplt.savefig('box_resistin.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=df2.groupby('Classification').describe().swaplevel(0, 1, 1).sort_index(1)[['mean','std']].swaplevel(0, 1, 1).sort_index(1).T\ntemp.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nboxplot = df1.boxplot(column=[x for x in df1.columns[:]],by='Classification')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Positive corelated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feat1_feat2(feat1, feat2) :  \n    trace0 = go.Scatter(\n        x = M[feat1],\n        y = M[feat2],\n        name = 'malignant',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = B[feat1],\n        y = B[feat2],\n        name = 'benign',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feat1_feat2('HOMA','Insulin')\nplot_feat1_feat2('Glucose','HOMA')\nplot_feat1_feat2('Glucose','Insulin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df1.iloc[:,-1].values\nX = df1.iloc[:,0:-1].values\n\nselector = SelectKBest( k='all').fit(X,y)\nx_new = selector.transform(X) # not needed to get the score\nscores = selector.scores_\nfor col in df1.columns:\n    print(col,end=' ')\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=[]\nfor col in df1.columns:\n    features.append(col)\nfeatures=features[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_2=['Age',\n 'BMI',\n 'Glucose',\n 'Resistin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Importing Common Modules needed for project\nfrom time import time   # For calculating training time\nimport pandas as pd    # Pandas is used for handling dataFrames\nimport pickle      # Pickle is used to load .pkl dataset files\nimport matplotlib.pyplot as plt    # Pyplot is used for visualizing the information\nfrom sklearn.feature_selection import SelectKBest, f_classif    # For Selecting K-Best features of availabel ones\nfrom sklearn.model_selection import train_test_split, GridSearchCV   # train_test_split for splitting up data\n                                                                     # into train and test\n                                                                     # GridSearchCV for finding best parameters tuned\nfrom sklearn.model_selection import StratifiedShuffleSplit   # For Cross-Validation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # For evaluating different algorithms\nimport numpy as np     # Numpy is used to manipulate data operations,\n                       # Most of the sklearn libraries expects numpy ndarrays as its default arguments\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\n### Importing Models from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn import preprocessing\n\n# x = df1[features].values #returns a numpy array\n# min_max_scaler = preprocessing.MinMaxScaler()\n# x_scaled = min_max_scaler.fit_transform(x)\n# df1[features] = pd.DataFrame(x_scaled)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df1[features] #features or features_2\ny=df1['Classification']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts() # 1-positive 0- healthy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nimport time\nstart = time.time()\n\nclf = SGDClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SGD Classifier Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, NuSVC, LinearSVC\n\nstart = time.time()\n\nclf = SVC(kernel='linear',C=1,gamma=1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\n\nend = time.time()\nlabels = np.unique(y_test)\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC LINEAR Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\nprint(confusion_matrix(y_test, prediction,labels=labels))\n\nstart = time.time()\n\nclf_svmpoly = SVC(kernel='poly')\nclf_svmpoly.fit(X_train, y_train)\nprediction = clf_svmpoly.predict(X_test)\nscores = cross_val_score(clf_svmpoly, X, y, cv=5)\n\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC POLY Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\nstart = time.time()\n\nclf_svmsigmoid = SVC(kernel='sigmoid',probability=True)\nclf_svmsigmoid.fit(X_train, y_train)\nprediction = clf_svmsigmoid.predict(X_test)\nscores = cross_val_score(clf_svmsigmoid, X, y, cv=5)\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC SIGMOID Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\n\nstart = time.time()\n\nclf_svmrbf = SVC(kernel='rbf',probability=True)\nclf_svmrbf.fit(X_train, y_train)\nprediction = clf_svmrbf.predict(X_test)\nscores = cross_val_score(clf_svmrbf, X, y, cv=5)\n\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC RBF Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_svmnu = NuSVC()\nclf_svmnu.fit(X_train, y_train)\nprediciton = clf_svmnu.predict(X_test)\nscores = cross_val_score(clf_svmnu, X, y, cv=5)\nend = time.time()\n\n\nprint(\"NuSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_svmlinear = LinearSVC()\nclf_svmlinear.fit(X_train, y_train)\nprediction = clf_svmlinear.predict(X_test)\nscores = cross_val_score(clf_svmlinear, X, y, cv=5)\n\n\nprint(\"LinearSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LINAER SVC with Gridsearch CV"},{"metadata":{},"cell_type":"markdown","source":"C=1 and gamma=1 and kernel=linear are optimal parameteres"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV \n\n# # 'C': [0.001,0.1, 1, 10, 100, 1000],  \n# # #               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n# # defining parameter range \n# param_grid = {'C': [0.001,0.1, 1, 10, 100, 1000],  \n#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n#               'kernel': ['linear']}  \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, y_train) \n\n# # print best parameter after tuning \n# print(grid.best_params_) \n\n# # print how our model looks after hyper-parameter tuning \n\n\n\n# print(grid.best_estimator_) \n\n# grid_predictions = grid.predict(X_test) \n\n# # print classification report \n# print(classification_report(y_test, grid_predictions)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.externals import joblib\n# joblib.dump(grid, 'grid_object.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom matplotlib import pyplot\n\nstart = time.time()\n\nclf_svc = SVC(kernel='linear',C=1,gamma=1,probability=True)\nclf_svc.fit(X_train, y_train)\nprediction = clf_svc.predict(X_test)\nscores = cross_val_score(clf_svc, X, y, cv=5)\n\nlr_probs = clf_svc.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\nlr_auc = roc_auc_score(y_test, lr_probs)\nprint('SVC LINEAR: ROC AUC=%.3f' % (lr_auc))\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n\npyplot.plot(lr_fpr, lr_tpr, marker='.')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n\n\nend = time.time()\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC LINEAR Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(prediction, y_test)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(prediction, y_test)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(prediction, y_test)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(prediction, y_test)\nprint('F1 score: %f' % f1)\n \n# # kappa\n# kappa = cohen_kappa_score(prediction, y_test)\n# print('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, lr_probs)\nprint('ROC AUC: %f' % auc)\nprint(classification_report(y_test, prediction))\nprint(confusion_matrix(y_test, prediction,labels=labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = DecisionTreeClassifier()\n\n# Train model and make predictions\ny_hat = clf_dt.fit(X_train, y_train).predict(X_test)\n\nprint(classification_report(y_test, y_hat))\n\nprint(confusion_matrix(y_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nstart = time.time()\n\nclf_knn = KNeighborsClassifier()\nclf_knn.fit(X_train, y_train)\nprediction = clf_knn.predict(X_test)\nscores = cross_val_score(clf_knn, X, y, cv=5)\n\n\nend = time.time()\n\n\nprint(\"KNN Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nstart = time.time()\n\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_train, y_train)\nprediction = clf_gnb.predict(X_test)\nscores = cross_val_score(clf_gnb, X, y, cv=5)\nend = time.time()\n\nprint(\"Gaussian NB Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Univariate features results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ncols=df1.columns[:-1]\nfor feature in cols:\n    print(feature,'****************')\n    X=df1[feature]\n    y=df1['Classification']\n    X=X.values.reshape(-1,1)\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.33,random_state=42)\n        \n\n    clf_rf = RandomForestClassifier()\n    clf_rf.fit(X_train1, y_train1)\n    y_pred = clf_rf.predict(X_test1)\n    y_score = clf_rf.predict_proba(X_test1)[:,1]\n    # Confusion maxtrix & metrics\n    cm = confusion_matrix(y_test1, y_pred)\n    print(cm)\n    print(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(y_pred, y_test1)))\n    print(classification_report( y_test1,y_pred))\n    fpr, tpr, t = roc_curve(y_test1, y_score)\n    #plot_roc()\n    from sklearn.metrics import roc_auc_score\n    print('auc= ',roc_auc_score(y_test1,y_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters = {'min_samples_split':np.arange(2, 80), 'max_depth': np.arange(2,10), 'criterion':['gini', 'entropy']}\n# rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n# grid = GridSearchCV(rfc, parameters,scoring='accuracy', cv=10)\n# grid.fit(X_train,y_train)\n# print('The parameters combination that would give best accuracy is : ')\n# print(grid.best_params_)\n# print('The best accuracy achieved after parameter tuning via grid search is : ', grid.best_score_)\n# import pickle\n# best_param_rf=grid.best_params_\n# # Store data (serialize)\n# with open('best_param_rf.pickle', 'wb') as handle:\n#     pickle.dump(best_param_rf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n# best_param_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KFOLD cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(clf_rf, X, y, cv=kfold, scoring=scoring)\n# print(results,results.mean(),results.std())\nprint(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# predicting with 95% Confidence interval"},{"metadata":{},"cell_type":"markdown","source":"Univariate single feature at a time"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=df1.columns[:-1]\n# values = df1.values\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_auc_score\n# configure bootstrap\nfor col in cols:\n    print(col)\n    values=df1[[col,'Classification']].values\n    n_iterations = 100\n    n_size = int(len(data) * 0.80)\n    # run bootstrap\n    stats = list()\n    score_speci=[]\n    score_precision=[]\n    score_recall=[]\n    score_f1=[]\n    score_auc=[]\n    for i in range(n_iterations):\n        # prepare train and test sets\n        train = resample(values, n_samples=n_size)\n        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n        # fit model\n        model = RandomForestClassifier()\n        model.fit(train[:,:-1], train[:,-1])\n        # evaluate model\n        predictions = model.predict(test[:,:-1])\n        score = accuracy_score(test[:,-1], predictions)\n        cm=confusion_matrix(test[:,-1], predictions)\n        tp = cm[1,1]\n        fn = cm[1,0]\n        fp = cm[0,1]\n        tn = cm[0,0]\n        y_score = model.predict_proba(test[:,:-1])[:,1]\n        score_auc.append(roc_auc_score(test[:,-1],y_score))\n        score_speci.append((tn)/(tn+fp))\n        score_precision.append(tp/(tp+fp))\n        score_recall.append(tp/(tp+fn))\n        score_f1.append(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))))\n        # print(score)\n        stats.append(score)\n    # confidence intervals\n    alpha = 0.95\n    p_lower = ((1.0-alpha)/2.0) * 100\n    p_upper = (alpha+((1.0-alpha)/2.0)) * 100\n\n    print('Accuracy %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(stats, p_lower))*100, min(1.0, np.percentile(stats, p_upper))*100))\n#     print(np.asarray(stats).mean(),np.asarray(stats).std())\n    print('Precision %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_precision, p_lower))*100, min(1.0, np.percentile(score_precision, p_upper))*100))\n#     print(np.asarray(score_precision).mean(),np.asarray(score_precision).std())\n    print('Recall %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_recall, p_lower))*100, min(1.0, np.percentile(score_recall, p_upper))*100))\n#     print(np.asarray(score_recall).mean(),np.asarray(score_recall).std())\n    print('F1 score %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_f1, p_lower))*100, min(1.0, np.percentile(score_f1, p_upper))*100))\n#     print(np.asarray(score_f1).mean(),np.asarray(score_f1).std())\n    print('Specificity %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_speci, p_lower))*100, min(1.0, np.percentile(score_speci, p_upper))*100))\n#     print(np.asarray(score_speci).mean(),np.asarray(score_speci).std())\n    print('AUC %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_auc, p_lower))*100, min(1.0, np.percentile(score_auc, p_upper))*100))\n#     print(np.asarray(score_auc).mean(),np.asarray(score_auc).std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bivariate 95% CI"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=df1.columns[:-1]\n# values = df1.values\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_auc_score\nmodels=[RandomForestClassifier(),SVC(kernel='linear',C=1,gamma=1,probability=True),LogisticRegression()]\n# configure bootstrap\nfor col in cols[1:]:\n    print(cols[0],col)\n    values=df1[[cols[0],col,'Classification']].values\n    for m in models:\n        print(m)\n        n_iterations = 100\n        n_size = int(len(data) * 0.80)\n        # run bootstrap\n        stats = list()\n        score_speci=[]\n        score_precision=[]\n        score_recall=[]\n        score_f1=[]\n        score_auc=[]\n        for i in range(n_iterations):\n            # prepare train and test sets\n            train = resample(values, n_samples=n_size)\n            test = np.array([x for x in values if x.tolist() not in train.tolist()])\n            # fit model\n            model = m\n            model.fit(train[:,:-1], train[:,-1])\n            # evaluate model\n            predictions = model.predict(test[:,:-1])\n            score = accuracy_score(test[:,-1], predictions)\n            cm=confusion_matrix(test[:,-1], predictions)\n            tp = cm[1,1]\n            fn = cm[1,0]\n            fp = cm[0,1]\n            tn = cm[0,0]\n            y_score = model.predict_proba(test[:,:-1])[:,1]\n            score_auc.append(roc_auc_score(test[:,-1],y_score))\n            score_speci.append((tn)/(tn+fp))\n            score_precision.append(tp/(tp+fp))\n            score_recall.append(tp/(tp+fn))\n            score_f1.append(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))))\n            # print(score)\n            stats.append(score)\n        # confidence intervals\n        alpha = 0.95\n        p_lower = ((1.0-alpha)/2.0) * 100\n        p_upper = (alpha+((1.0-alpha)/2.0)) * 100\n\n        print('Accuracy %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(stats, p_lower))*100, min(1.0, np.percentile(stats, p_upper))*100))\n    #     print(np.asarray(stats).mean(),np.asarray(stats).std())\n        print('Precision %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_precision, p_lower))*100, min(1.0, np.percentile(score_precision, p_upper))*100))\n    #     print(np.asarray(score_precision).mean(),np.asarray(score_precision).std())\n        print('Recall %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_recall, p_lower))*100, min(1.0, np.percentile(score_recall, p_upper))*100))\n    #     print(np.asarray(score_recall).mean(),np.asarray(score_recall).std())\n        print('F1 score %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_f1, p_lower))*100, min(1.0, np.percentile(score_f1, p_upper))*100))\n    #     print(np.asarray(score_f1).mean(),np.asarray(score_f1).std())\n        print('Specificity %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_speci, p_lower))*100, min(1.0, np.percentile(score_speci, p_upper))*100))\n    #     print(np.asarray(score_speci).mean(),np.asarray(score_speci).std())\n        print('AUC %.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, max(0.0, np.percentile(score_auc, p_lower))*100, min(1.0, np.percentile(score_auc, p_upper))*100))\n    #     print(np.asarray(score_auc).mean(),np.asarray(score_auc).std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best and worst ROC curve with k fold cross validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(n_splits=10)\nclassifier = SVC(kernel='linear',C=1,gamma=1,probability=True)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\nplt.figure(figsize=(10,10))\ni = 0\nfor train, test in cv.split(X_train, y_train):\n    probas_ = classifier.fit(X_train[train], y_train[train]).predict_proba(X_train[test])\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y_train[test], probas_[:, 1])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n         label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',\n         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n         lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                 label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.01, 1.01])\nplt.ylim([-0.01, 1.01])\nplt.xlabel('False Positive Rate',fontsize=18)\nplt.ylabel('True Positive Rate',fontsize=18)\nplt.title('Cross-Validation ROC',fontsize=18)\nplt.legend(loc=\"lower right\", prop={'size': 15})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nstart = time.time()\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nprediction = clf_rf.predict(X_test)\ny_score = clf_rf.predict_proba(X_test)[:,1]\nscores = cross_val_score(clf_rf, X, y, cv=5)\nend = time.time()\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nfrom sklearn.metrics import roc_auc_score\nprint('auc= ',roc_auc_score(y_test,y_score))\n\nstart = time.time()\n\nclf_et = ExtraTreesClassifier()\nclf_et.fit(X_train, y_train)\nprediction = clf_et.predict(X_test)\nscores = cross_val_score(clf_et, X, y, cv=5)\n\nend = time.time()\n\n\nprint(\"Extra Trees Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_dt = DecisionTreeClassifier()\nclf_dt.fit(X_train, y_train)\nprediction = clf_dt.predict(X_test)\nscores = cross_val_score(clf_dt, X, y, cv=5)\nprint(confusion_matrix( y_test,prediction))\nend = time.time()\n\nprint(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from multiscorer_py import MultiScorer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score          \nfrom sklearn.model_selection import cross_val_score\nfrom numpy import average\n\nscorer = MultiScorer({\n    'Accuracy'  : (accuracy_score , {}),\n    'Precision' : (precision_score, {'pos_label': 3, 'average':'macro'}),\n    'Recall'    : (recall_score   , {'pos_label': 3, 'average':'macro'}),\n    'AUC'       : (roc_auc_score,{}),\n    'F1 score'  : (f1_score,{})\n}) \n\nmodels = [clf_svc, clf_dt, clf_knn,clf_gnb,clf_rf,clf_et,clf_svmrbf,clf_svmsigmoid,clf_svmpoly,clf_svmnu,clf_svmlinear]\nnames = ['SVC', 'Decision Tree', 'KNN','Gaussian Naive bayes','Random forest','Extra trees','SVM RBF','SVM Sigmoid','SVM poly','SVM Nu','SVM linear']\nprint('10 Fold cross Validation results')\nfor model, name in zip(models, names):\n    print(name)\n\n    _ = cross_val_score(model, X, y,scoring=scorer, cv=10) # Added assignment of the result to `_` in order to illustrate that the return value will not be used\n    results = scorer.get_results()\n\n    for metric_name in results.keys():\n        average_score = np.average(results[metric_name])\n        print('%s : %f' % (metric_name, average_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_estimators = [100, 300, 500, 800, 1200]\n# max_depth = [5, 8, 15, 25, 30]\n# min_samples_split = [2, 5, 10, 15, 100]\n# min_samples_leaf = [1, 2, 5, 10] \n# forest = RandomForestClassifier(random_state = 1, n_estimators = 10, min_samples_split = 1)\n# hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n#               min_samples_split = min_samples_split, \n#              min_samples_leaf = min_samples_leaf)\n\n# gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n#                       n_jobs = -1)\n# bestF = gridF.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction = bestF.predict(X_test)\n# scores = cross_val_score(bestF, X, y, cv=5)\n# print(confusion_matrix( y_test,prediction))\n\n# print(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n# print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n# print(classification_report( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import img_to_array, array_to_img, load_img\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\n\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=9, activation='sigmoid'))\nmodel.add(Dense(128, activation='sigmoid'))\n# model.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(Dense(16, activation='sigmoid'))\nmodel.add(Dense(1, activation='softmax'))\nprint(model.summary())\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\nscores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\nprediction = model.predict(X_test)\n# scores = cross_val_score(model, X, y, cv=5,scoring=\"accuracy\")\n\n# print(\"Neural network Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n# print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n# print(classification_report( y_test,prediction))\n# print(confusion_matrix( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REFERENCED COMBINATION OF MODEL\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Def X and Y\ny = np.array(df1.Classification.tolist())\ndata = df1.drop('Classification', 1)\nX = np.array(data.values)\n# Normalization\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n# Train_test split\nrandom_state = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression with GridsearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nrandom_state = 42\n# Find best hyperparameters (accuracy)\nlog_clf = LogisticRegression(random_state = random_state)\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n\nCV_log_clf = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'recall', verbose = 1, n_jobs = -1)\nCV_log_clf.fit(X_train, y_train)\n\nbest_parameters = CV_log_clf.best_params_\nprint('The best parameters for using this model is', best_parameters)\n\n#Log with best hyperparameters\nCV_log_clf = LogisticRegression(C = best_parameters['C'], \n                                penalty = best_parameters['penalty'], \n                                random_state = random_state)\n\nCV_log_clf.fit(X_train, y_train)\ny_pred = CV_log_clf.predict(X_test)\ny_score = CV_log_clf.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Logistic Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression with RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n#Logistic regression with RFE\nlog_clf = LogisticRegression(C = best_parameters['C'], \n                                 penalty = best_parameters['penalty'], \n                                 random_state = random_state)\n\nselector = RFE(log_clf)\nselector = selector.fit(X_train, y_train)\n\ny_pred = selector.predict(X_test)\ny_score = selector.predict_proba(X_test)[:,1]\n\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Logistic Confusion matrix')\nplt.show()\n\nshow_metrics()\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# support and ranking RFE\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nprint(selector.support_)\nprint(selector.ranking_)\n#Learning curve Log with best hyperpara\nplot_learning_curve(CV_log_clf, 'Learning Curve For Logistic Model', X, y)\nplt.savefig('7')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Learning curve Log with RFE\nplot_learning_curve(selector, 'Learning Curve For Logistic Model with RFE', X, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross val Log \ncross_log = cross_val_metrics(CV_log_clf)\n# Cross val Log with RFE\ncross_selector = cross_val_metrics(selector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Select threshold for a recall = 100% (all malignant tumors detected)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold\nthresholds_adj = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize = (15,15))\n\nj = 1\nfor i in thresholds_adj:\n    y_score = CV_log_clf.predict_proba(X_test)[:,1] > i\n    \n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    cm = confusion_matrix(y_test, y_score)\n    \n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n\n    print('Recall w/ threshold = %s :'%i, (tp/(tp+fn)))\n    \n    class_names = [0,1]\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title='Threshold = %s'%i) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting with recall = 100%\n# Recall = 1.\ny_score = CV_log_clf.predict_proba(X_test)[:,1] > 0.3\ncm = confusion_matrix(y_test, y_score)\nclass_names = [0,1]\nshow_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 2 models we can increase the precision while keeping a recall = 100%\n\n7. Predictive model 2 : Ensemble Classifier to maximise precision and detect all malignant tumors\n7.1. Logistic Regression and GridSearch CV to optimise hyperparameters (recall)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best parameters (recall)\nlog2_clf = LogisticRegression(random_state = random_state)\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n            }\n\nCV_log2_clf = GridSearchCV(estimator = log2_clf, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1)\nCV_log2_clf.fit(X_train, y_train)\n\nbest_parameters = CV_log2_clf.best_params_\nprint('The best parameters for using this model is', best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log w best hyperparameters (recall)\nCV_log2_clf = LogisticRegression(C = best_parameters['C'], \n                                 penalty = best_parameters['penalty'], \n                                 random_state = random_state)\n\n\nCV_log2_clf.fit(X_train, y_train)\n\ny_pred = CV_log2_clf.predict(X_test)\ny_score = CV_log2_clf.decision_function(X_test)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_metrics(CV_log2_clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting classifier : log + log2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nclf_gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\nclf_gbc.score(X_test, y_test)\ny_pred = clf_gbc.predict(X_test)\ny_score = clf_gbc.predict_proba(X_test)[:,1]\n\n# Confusion maxtrix\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nprint(cm)\nshow_metrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n#Voting Classifier\nvoting_clf = VotingClassifier (\n        estimators = [('log1', clf_gnb), ('log_2', clf_rf),('log3', CV_log_clf),('log4',clf_gbc),\n                      ('svmrbf',clf_svmrbf),('knn',clf_knn),('clf_svmsigmoid',clf_svmsigmoid)],\n                     voting='soft', weights = [1.2,8,2,1,2,1.5,2])#, weights = [1.2,3,2,1,2,1.5,2]\n# voting_clf = VotingClassifier (\n#         estimators = [('log1', CV_log_clf), ('log_2', CV_log2_clf)],\n#                      voting='soft', weights = [1,2])\nvoting_clf.fit(X_train,y_train)\n\ny_pred = voting_clf.predict(X_test)\ny_score = voting_clf.predict_proba(X_test)[:,1]\n\n# Confusion maxtrix\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nprint(cm)\nshow_metrics()\nfrom sklearn.metrics import roc_auc_score\nprint('auc= ',roc_auc_score(y_test,y_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# voting_clf = VotingClassifier (\n#         estimators = [('log1', clf_gnb), ('log_2', clf_rf),('log3', CV_log_clf),('log4',clf_gbc)],\n#                      voting='soft')\n# params = {'weights':[[1.2,2.8,2,0.8],[1.1,2.9,2,0.5],[0.5,1,1.5,2,2.5,3],[0.5,1,1.5,2,2.5,3]]}\n# grid_Search = GridSearchCV(param_grid = params, estimator=voting_clf,scoring='recall')\n# grid_Search.fit(X_train,y_train)\n# print(grid_Search.best_Score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross val score voting\ncross_voting = cross_val_metrics(voting_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold\nthresholds_adj = [0.1,0.2,0.3,0.4,0.5,0.52,0.6,0.7,0.8]\n\nplt.figure(figsize = (15,15))\n\nj = 1\nfor i in thresholds_adj:\n    y_score = voting_clf.predict_proba(X_test)[:,1] > i\n    \n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    cm = confusion_matrix(y_test, y_score)\n    \n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n\n    print('Recall w/ threshold = %s :'%i, (tp/(tp+fn)),'precision = ',tn/(tn+fp))\n    \n    class_names = [0,1]\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title='Threshold = %s'%i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n# rf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\nbase_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the grid search to the data\n# grid_search.fit(X_train, y_train)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbest_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)\nprint('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_search.predict(X_test)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred.round())\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Logistic Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forestVC = RandomForestClassifier(random_state = 1,\n                                  n_estimators = 750,\n                                  max_depth = 15, \n                                  min_samples_split = 5,  min_samples_leaf = 1) \nmodelVC = forestVC.fit(X_train, y_train) \ny_pred = modelVC.predict(X_test)\n# y_score = modelVC.decision_function(X_test)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred.round())\nclass_names = [0,1]\n# plt.figure()\n# plot_confusion_matrix(cm, \n#                       classes=class_names, \n#                       title='Random forest Confusion matrix')\n# plt.savefig('6')\n# plt.show()\n\nshow_metrics()\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nmodel = XGBClassifier()\n# define grid\nweights = [1, 10, 25, 50, 75, 99, 100, 1000]\nparam_grid = dict(scale_pos_weight=weights)\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid search\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n# execute the grid search\ngrid_result = grid.fit(X, y)\n# report the best configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# report all configurations\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X_test, y_test, cv=5)\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(classification_report( y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold\nthresholds_adj = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize = (15,15))\n\nj = 1\nfor i in thresholds_adj:\n    y_score = clf.predict_proba(X_test)[:,1] > i\n    \n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    cm = confusion_matrix(y_test, y_score)\n    \n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n\n    print('Recall w/ threshold = %s :'%i, (tp/(tp+fn)),'precision = ',tn/(tn+fp))\n    \n    class_names = [0,1]\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title='Threshold = %s'%i)\ny_score = clf.predict_proba(X_test)[:,1] > 0.6\ncross_voting = cross_val_metrics(clf)\ncm = confusion_matrix(y_test, y_score)\nclass_names = [0,1]\nshow_metrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}