{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diamond Dozen or Dime in Dozen?\n\n\n## Diamond Price Regressor \n\nThis Jupyter notebook takes qualitative and quantitative features for a compiled set of diamonds to construct regressor models to predict price. \n\n## Dataset\n\nhttps://www.kaggle.com/shivam2503/diamonds\n\n## Motivation\n\nIs this diamond just a dime in dozen?$^{1}$ \n\nSuppose you are the owner of a jewellery store that offers a service to buy diamonds from customers for the purpose of resale. The incentive for you is to resell the diamond at a greater price than the purchase price, thus making profit. As you begin studying diamonds to gain an understanding of the product, you notice that seemingly identical diamonds to the untrained eye can actually differ in price drastically! With no easy way for you to learn the intricacies that lead to the exorbitant values of diamonds, you turn your attention to constructing a machine learning model that can help you predict the prices. You manage to obtain a dataset of diamond prices that were sold in your country along with corresponding relevant features. A sufficiently accurate prediction model will inform your strategy when buying and reselling diamonds to maximize profit margins. \n\n## Objective\n\nUse the diamonds dataset to construct a regression model to predict diamond prices.\n\n$^{1}$ https://www.theidioms.com/a-dime-adozen/#:~:text=something%20very%20common%20and%20not,abundant%2C%20cheap%20and%20very%20common"},{"metadata":{},"cell_type":"markdown","source":"Import standard data processing and plotting libraries."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nDATA_PATH = \"/kaggle/input/diamonds/diamonds.csv\"\ndiamonds_data = pd.read_csv(DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"diamonds_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains qualitative and quantitative features. It also looks like it is sorted in ascending order based on price. Therefore, the dataset should be shuffled when splitting into train/test sets."},{"metadata":{"trusted":false},"cell_type":"code","source":"diamonds_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The important observation here is that every feature contains 53,940 entries and therefore, there are no missing data points in the dataset. Other observations include the datatypes that comprise the dataset (ex. float64). Notably, \"object\" can represent any Python object but because the dataset was loaded using pandas' read_csv method, \"object\" must be text. We confirm this by referring to the cell block above (ex. feature \"cut\" values include \"Ideal\", Premium\", etc.).\n\nLet's drop the \"Unnamed\" column as it only provides the index to the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"diamonds_data = diamonds_data.drop(\"Unnamed: 0\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's next compute basic statistics of the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"diamonds_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's next observe how each feature affects the price of diamonds."},{"metadata":{"trusted":false},"cell_type":"code","source":"correlations = diamonds_data.corr()\ncorrelations[\"price\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like most features do correlate with the price of diamonds. The notable exception is \"depth\" which has a negligble correlation (~1%). Therefore, we will exclude \"depth\" in the input features for the regression model.\n\nLet's remove feature, \"depth\"."},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove the feature, \"depth\"\ndiamonds_data = diamonds_data.drop(\"depth\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's next plot histograms to observe the input features distributions."},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_names = diamonds_data.columns.values\n\nplt.rcParams['figure.figsize'] = [10, 8]\n\nfor name in feature_names:\n    diamonds_data[name].hist(bins=10)\n    plt.title(f\"Diamonds {name} Histogram\")\n    plt.xlabel(name)\n    plt.ylabel(\"Total Count\")\n    plt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An important obsevation is that the distribution of some quantitative features (ex. \"carat\") and the target feature (\"price\") are heavily \"right-skewed\". Let's take a closer look at the skewed \"price\" distribution by creating custom bins."},{"metadata":{"trusted":false},"cell_type":"code","source":"diamonds_data[\"price_cat\"] = pd.cut(diamonds_data[\"price\"],\n                                    bins=[0., 2500, 5000, 7500, 10000, 12500, 15000, 17500, np.inf],\n                                    labels=[1, 2, 3, 4, 5, 6, 7, 8])\n\nprint(f\"Categorized Price Strata Value Counts:\\n{diamonds_data.price_cat.value_counts()}\")\n\ndiamonds_data[\"price_cat\"].hist(bins=8)\nplt.title(\"Categorized Price Strata\", fontsize=16)\nplt.xlabel(\"Stratum\", fontsize=16)\nplt.ylabel(\"Total Count\", fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heavy \"right-skew\" is evident from the value counts printed and the histogram. When splitting the dataset, care should be given to incorporating the data points from the scarce strata (ex. bin 8 where price > 17,500) to mitigate sampling bias. \n\nLet's split the dataset into 60/20/20 train/validation/test sets using stratified splitting based on \"price_cat\"."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# extract the input features to X\nX = diamonds_data.drop(\"price\", axis=1)\n# extract the target feature to y\ny = diamonds_data[\"price\"]\n\n# perform stratified splitting based on the \"price_cat\" strata\n# random_state=42 for reproducibiility\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True,\n                                                    stratify=diamonds_data[\"price_cat\"], random_state=42)\n\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, shuffle=True,\n                                                  stratify=X_train[\"price_cat\"], random_state=42)\n\n# drop the feature, \"price_cat\" from the feature DataFrame\nfor df in (X_train, X_val, X_test):\n    df.drop(\"price_cat\", axis=1, inplace=True)\n\n# verify the splitting ratios are as expected\nprint(f\"Training set proportion: {len(X_train)/53940}\")\nprint(f\"Validation set proportion: {len(X_val)/53940}\")\nprint(f\"Test set proportion: {len(X_test)/53940}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following the generation of the train/validation/test sets, we need to prepare the train set for model training. The quantitative features should be standardized and the categorical features need to be encoded. An important attribute to keep in mind is that all the categorical features are ordinal variables, implying a ranked relationship. It follows from the dataset description:\n\n* \"cut\"\n\n\"Fair\" (Worst) ----- \"Good\" ----- \"Very Good\" ----- \"Premium\" ----- \"Ideal\" (Best)\n\n* \"color\"\n\n\"J\" (Worst) ----- \"I\" ----- \"H\" ----- \"G\" ----- \"F\" ----- \"E\" ----- \"D\" (Best)\n\n* \"clarity\"\n\n\"I1\" (Worst) ----- \"SI2\" ----- \"SI1\" ----- \"VS2\" ----- \"VS1\" ----- \"VVS2\" ----- \"VVS1\" ----- \"IF\" (Best)\n\nEncoding should capture these ranked relationships as defined."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# define 3 OrdinalEncoders to control the value mappings according to above \n# (otherwise, OrdinalEncoder encodes via alphabetical order which would not capture the ranked relationship properly)\ncut_ord_enc = OrdinalEncoder(categories=[[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]])\ncolor_ord_enc = OrdinalEncoder(categories=[[\"J\", \"I\", \"H\", \"G\", \"F\", \"E\", \"D\"]])\nclarity_ord_enc = OrdinalEncoder(categories=[[\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]])\n\n# extract the numerical input feature names\nnum_feature_names = list(X_train.drop([\"cut\", \"color\", \"clarity\"], axis=1))\n# extract each categorical input feature name separately to use the different OrdinalEncoders defined above\ncut_feature_name = [\"cut\"]\ncolor_feature_name = [\"color\"]\nclarity_feature_name = [\"clarity\"]\n\nprocessing_pipeline = ColumnTransformer([\n                      (\"num\", StandardScaler(), num_feature_names),\n                      (\"cut\", cut_ord_enc, cut_feature_name),\n                      (\"color\", color_ord_enc, color_feature_name),\n                      (\"clarity\", clarity_ord_enc, clarity_feature_name)])\n\nX_train_prepped = processing_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It was important during feature processing that standardization does not compute the mean and standard deviation of the entire dataset so as to avoid data leakage. This was avoided as we previously split the data into train/val/test sets where the validation and test sets were untouched. Consequently, standardization was performed using the mean and standard deviation computed from only the train set. When we evaluate our regression models on the validation and test sets, they must be standardized according to the train set mean and standard deviation. \n\nLet's standardize the validation and test sets now to prospectively prepare them for model evaluation."},{"metadata":{"trusted":false},"cell_type":"code","source":"# use transform and not fit_transform to maintain the mean and standard deviation of the train set\nX_val_prepped = processing_pipeline.transform(X_val)\nX_test_prepped = processing_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The validation and test sets are now standardized and encoded.\n\nFollowing numerical and categorical features standardization and encoding, respectively on the train set, we are now ready for model training. Let's train Linear Regression (LR), Linear Support Vector Machine (SVM) Regressor, Poly SVM Regressor, and Random Forest Regressor models."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\n# random_state 42 for reproducibility\nlin_reg = LinearRegression()\nlin_svr = LinearSVR(random_state=42)\npoly_svr = SVR(kernel=\"poly\")\nrf_reg = RandomForestRegressor(random_state=42)\n\n# fit all the models on the train set\nlin_reg.fit(X_train_prepped, y_train)\nlin_svr.fit(X_train_prepped, y_train)\npoly_svr.fit(X_train_prepped, y_train)\nrf_reg.fit(X_train_prepped, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's evaluate the performances of each model. Firstly, calculate the root-mean-square-error (RMSE) on the train set to gain an understanding of the train accuracy. Secondly, calculate the RMSE on the validation set to gain an understanding of the generalization error."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmodel_names = [\"Linear Regression\", \"Linear SVR\", \"Poly SVR\", \"Random Forest Regressor\"]\ntrain_rmse_list = []\nval_rmse_list = []\nval_gen_error_list = []\n\nfor model in (lin_reg, lin_svr, poly_svr, rf_reg):\n    # extract the predictions made by each model on the train set\n    y_train_pred = model.predict(X_train_prepped)\n    # extract the predictions made by each model on the validation set\n    y_val_pred = model.predict(X_val_prepped)\n    # calculate the rmse for each model on the train set\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    # calculate the rmse for each model on the validation set\n    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    # calculate the generalization error on the validation set\n    gen_error = f\"{round(((val_rmse-train_rmse)/train_rmse*100),2)}%\"\n    \n    # populate the correspoding lists to save the calculated values\n    train_rmse_list.append(train_rmse)\n    val_rmse_list.append(val_rmse)\n    val_gen_error_list.append(gen_error)\n    \n# construct a DataFrame to display the rmse values\nrmse_df = pd.DataFrame({\"Model\": model_names,\n                        \"Train RMSE\": train_rmse_list,\n                        \"Val. RMSE\": val_rmse_list,\n                        \"   Val. Error\": val_gen_error_list})\n\nprint(rmse_df.to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Key insights:\n\n1) Linear Regression and Linear SVR models generalize well on the validation set\n\n2) Poly SVR is overfitting given the relatively poor generalization error on the validation set\n\n3) Random Forest Regressor seems to perform the best on the train set but is probably heavily overfitting as the model was given total freedom in hyperparameter values --> reinforced by the poor generalization error\n\nThe next step will be to tune the hyperparameters of the models to achieve 3 objectives:\n1) Reduce overfitting --> particularly relevant to the Poly SVR and Random Forest Regressor\n\n2) Reduce the RMSE\n\n3) Determine which model performs best on the validation set\n\nNote: Standard Linear Regression using Least Squares does not have any hyperparameters. If the model were overfitting, regularization techniques such as Lasso or Ridge Regression could be used. As this is not the case, the Linear Regression model will be kept as is."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Linear SVR hyperparameters to try\n# try greater \"C\" values since the current Linear SVR has low generalization error on the validation set\nlin_svr_param_grid = [{\"C\": [5, 10, 15, 20]}]\n\n# Poly SVR hyperparameter combinations to try\n# try smaller \"C\" values since the current Poly SVR has a high generalization error on the validation set\n# avoid polynomial degrees > 3 as the Poly SVR is already overfitting (current model is using degree 3)\npoly_svr_param_grid = [{\"C\": [0.25, 0.50, 0.75, 1], \n                        \"degree\": [2, 3],\n                        \"coef0\": [0.0, 0.25, 0.50, 0.75, 1.0]}]\n\n# Random Forest hyperparameter combinations to try (there are many other hyperparameters that are not tried here)\nrf_reg_param_grid = [{\"n_estimators\": [100, 200, 300, 400, 500],\n                      \"max_features\": [2, 3, 4, 5, 6]}]\n\nlin_svr_gscv = GridSearchCV(lin_svr, param_grid=lin_svr_param_grid, cv=5)\npoly_svr_gscv = GridSearchCV(poly_svr, param_grid=poly_svr_param_grid, cv=5)\nrf_reg_gscv = GridSearchCV(rf_reg, param_grid=rf_reg_param_grid, cv=5)\n\n# re-train all 3 models with the best hyperparameters found\nlin_svr_gscv.fit(X_train_prepped, y_train)\npoly_svr_gscv.fit(X_train_prepped, y_train)\nrf_reg_gscv.fit(X_train_prepped, y_train)\n\n# display the best hyperparameters from the combinations tried\nprint(f\"Linear SVR Best Hyperparameters Found: {lin_svr_gscv.best_params_}\")\nprint(f\"Poly SVR Best Hyperparameters Found: {poly_svr_gscv.best_params_}\")\nprint(f\"Random Forest Regressor Best Hyperparameters Found: {rf_reg_gscv.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_names = [\"Linear Regression\", \"Linear SVR\", \"Poly SVR\", \"Random Forest Regressor\"]\ngscv_train_rmse_list = []\ngscv_val_rmse_list = []\ngscv_val_gen_error_list = []\n\nfor model in (lin_reg, lin_svr_gscv, poly_svr_gscv, rf_reg_gscv):\n    # extract the predictions made by each model on the train set\n    y_train_pred = model.predict(X_train_prepped)\n    # extract the predictions made by each model on the validation set\n    y_val_pred = model.predict(X_val_prepped)\n    # calculate the rmse for each model on the train set\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    # calculate the rmse for each model on the validation set\n    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    # calculate the generalization error on the validation set\n    gen_error = f\"{round(((val_rmse-train_rmse)/train_rmse*100),2)}%\"\n    \n    # populate the correspoding lists to save the calculated values\n    gscv_train_rmse_list.append(train_rmse)\n    gscv_val_rmse_list.append(val_rmse)\n    gscv_val_gen_error_list.append(gen_error)\n    \n# construct a DataFrame to display the rmse values\nrmse_df = pd.DataFrame({\"Model\": model_names,\n                        \"Prev. Train RMSE\": train_rmse_list,\n                        \"Train RMSE\": gscv_train_rmse_list,\n                        \"Prev. Val. RMSE\": val_rmse_list,\n                        \"Val. RMSE\": gscv_val_rmse_list,\n                        \"Prev. Val. Error\": val_gen_error_list,\n                        \"   Val. Error\": gscv_val_gen_error_list})\n\nprint(rmse_df.to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GridSearchCV results are summarized in the output of the above cell block. Firstly, the Linear Regression model was not modified at all as there are no hyperparameters. For the other models, the validation errors seem to have increased! While this is true, it is important to also look at the absolute RMSE differences post-GridSearchCV. Taking a deeper look at the Linear SVR, Poly SVR, and Random Forest Regressor models, both the train and validation RMSE was reduced which is a good sign that GridSearchCV has improved the models. That being said, the validation errors still suggest overfitting, especially in the case of the Poly SVR where the magnitude of the difference between the validation RMSE - train RMSE is quite large. Given these observations, it would be beneficial to try a wider range of hyperparameter values in during GridSearchCV. In particular, there are many hyperparameters that can be tuned for Random Forests. This is not done in this notebook as the relatively large dataset requires extensive training time. \n\n\nFinally, given the model performances, it is clear that the Random Forest Regressor performs the best by far. Suppose we are happy with the hyperparameter tuning and wish to evaluate the Random Forest Regressor on the test set. The first thing we need to do is compile the train and validation sets together and retrain the model. A simple way to achieve this is to regenerate the original train/test sets. The below cell block accomplishes this and reuses much of the previous code."},{"metadata":{"trusted":false},"cell_type":"code","source":"# perform stratified splitting based on the \"price_cat\" strata\n# random_state=42 for reproducibiility\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True,\n                                                    stratify=diamonds_data[\"price_cat\"], random_state=42)\n\n# Following the above step, X_train represents the train + validation set\n# Next, apply feature processing as defined by the previously contructed pipeline\nX_train_prepped = processing_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An important distinction between the above pipeline transformation we just performed is that the standardization uses the data from both the train and validation sets together. Previously, we only performed standardization on the train set and carried forward the train set mean and standard deviation to the validation set so as to avoid data leakage. We did this so that evaluating the models using the validation set, like above, provides a more reasonable and accurate performance measure. Following Random Forest Regressor hyperparameter tuning, we retrain the model and standardize based on the compiled train and validation sets data."},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_reg_final = RandomForestRegressor(n_estimators=500, max_features=5, random_state=42)\nrf_reg_final.fit(X_train_prepped, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we can evaluate on the test set, we need to also standardize the test set based on the new mean and standard deviation calculated for the aggregated train + validation sets."},{"metadata":{"trusted":false},"cell_type":"code","source":"# only use transform instead of fit_transform to use the train + val set mean and standard deviation\n# this is to avoid data leakage to the test set\nX_test_prepped = processing_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now evaluate the Random Forest Regressor on the test set."},{"metadata":{"trusted":false},"cell_type":"code","source":"# generate the predicted diamond prices on the test set\ny_test_pred = rf_reg_final.predict(X_test_prepped)\n# calculate the RMSE on the test set\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\n# display the final model hyperparameters and results\nprint(\"Random Forest Regressor Post-GSCV\")\nprint(f\"Hyperparameter max_features: {5}\")\nprint(f\"Hyperparameter n_estimators: {500}\")\nprint(f\"Test Set RMSE: {test_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test set RMSE is ~ 533.57 which is almost identical to the validation set RMSE (~ 532.72). The validation set used to obtain the validation RMSE was standardized using the training set mean and standard deviation. This minimized data leakage and we observe that the validation set becomes a good tool to get a sense of the generalization error of our model. This is reinforced by the almost identical RMSE obtained on the test set."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}