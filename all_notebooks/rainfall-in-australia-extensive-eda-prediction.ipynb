{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi,\n\nIn this project I perform an Exploratory Data Analysis (EDA) on the Rainfall Data and try to predict if it is going to rain in Australia the next day. Athough the data contains data about the locations of the measurements, my initial try is to make a general model to predict the rain without using the location.\n\n\n***Result of the project: AUC score: around 85% This could still be enhanced with further analysis***","metadata":{"id":"s73eTqqs572A"}},{"cell_type":"markdown","source":"# Schematic Project Layout:\n\nImport\n- Import Data from kaggle\n\n- Split Data into train and test (Timeseriessplit)\n\nEDA\n- EDA on the train set with Pandas profiling library\n\nThe Pipeline\n\n- Pipeline to process train (and later the test set):\n\n    - Process numerical columns (most columns)\n\n    - Process the Rainfall and Sunshine column (make feature descrete (categorical), since it has a lot of zeros and outliers)\n\n    - Power transform (log transform) features that are right skewed\n\n    - Feature engineering pipe (for example take the difference of the min and max temperature on a day for each location\n\n    - Process categorical columns (one hot encoding)\n\n\nSpecific alterations on the train data to make training easier:\n\n  - Remove outliers? Outliers are not really an issue in these data (except from Rainfall (but this is already fixed with descretion)\n\n  - Oversample the minority class using SMOTE\n\n      NOTE: Undersampling the majority class is not necessary because the ratio minority/majority is equal to about 1/3.\n\n- Drop Nan's\n\n  - In the target variable\n\n  - In columns that have more than ~40 % NaN's\n\n  - In rows that have more than 50% NaN's\n\n\n- Cross validation on different estimators\n\n  - Used metric: ROC_AUC\n\n  - Learning Curves\n\n- Pick promising estimators and perform a hyperparameter optimization with HyperOpt.\n\n- Test best estimator on test set\n\n","metadata":{"id":"-6BsKxOboATW"}},{"cell_type":"markdown","source":"# Import packages and the data","metadata":{"id":"a-FQWkQEpLMs"}},{"cell_type":"code","source":"# Installed packages\n\nimport pandas as pd\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#EDA library\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import  MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\n\n#MICE\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\n\n#PCA\n# from sklearn.decomposition import PCA # PCA is not really nessecary here\n\nimport seaborn as sns\n\n# for oversampling and undersampling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as imblearn_pipeline\n\n#see progress bar in pandas\nfrom tqdm import tqdm # display handy progess bars\ntqdm.pandas()\n","metadata":{"id":"oouOPSRJb5fD","outputId":"68332c17-287e-48f7-9566-f34cdb1fb002","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the csv file and train/test","metadata":{"id":"uZTYR3dOhQCz"}},{"cell_type":"code","source":"#read the csv\ndf = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\", parse_dates=['Date'], index_col='Date')","metadata":{"id":"xYC1NNqeexrw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#view the first rows\ndf.head()","metadata":{"id":"Pamcqpu2wZ72","outputId":"d15fa0ae-53c1-4b38-c2d2-dde2931f115b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the last rows\ndf.tail()","metadata":{"id":"maEufB4TKOIF","outputId":"3660c111-e8a7-45a4-e027-ad1da9e3f9b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the general info of the data","metadata":{"id":"LpYwE7p4L4XX"}},{"cell_type":"code","source":"# check the dataset\ndf.info()","metadata":{"id":"xejALAnv0nP1","outputId":"fa8a938b-5864-44b7-bacc-34fa8a941555","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first things I notice is that apparently the dataframe is ordered by city instead of being in a chronicle order. This would affect our data split to the train and test set, since we do a TimeSeriesSplit; We are tryin to split the data on different periods in time. With some periods representing the train set and others the test set.\n\nSecondly, the data has a lot of missing data. Fortunately, since the amount of rows is quiet big, this should not be a major problem.","metadata":{"id":"RHggLz0VKJE5"}},{"cell_type":"code","source":"#sort the df by index instead of by location\ndf = df.sort_index()\ndf.head()","metadata":{"id":"FpB_JCrEH4_Q","outputId":"0376c1e0-6418-473c-ffb0-3295a13e6125","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe has only data from Canberra in the first days of the measurements. Does this mean that Canberra might be overpresented in the data?In the EDA analysis we should check for the frequency of each location label.\n\nLet's look at the tail of the data frame.","metadata":{"id":"rhzm1lrbLM1F"}},{"cell_type":"code","source":"df.tail()","metadata":{"id":"F0-9pxBdLGaH","outputId":"79d5cbfa-4898-49d5-b66b-ed601cddec7d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now the data is chronically ordered let's split the data into train and test sets using the Timeseries split.","metadata":{"id":"8t5Bt9RSLfPn"}},{"cell_type":"markdown","source":"### Split the data into train and test","metadata":{"id":"mnicDYYaJxJk"}},{"cell_type":"code","source":"# Make a TimeseriesSplit on the copies of the variables\ntscv = TimeSeriesSplit(n_splits=5) # We use Time Series split to easily split the time series data without shuffling\nfor train_index, test_index in tscv.split(df):\n     df_train, df_test = df.iloc[train_index].copy(), df.iloc[test_index].copy() ","metadata":{"id":"R82dCUrQV6XP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"id":"iBeJuHkahTuD"}},{"cell_type":"markdown","source":"Firstly, let's do a Explorative Data Analysis (EDA) on the training data. The Pandas profiling library is an exellent and easy choice to explore the data, so let's use this one.","metadata":{"id":"0OiubwYul99a"}},{"cell_type":"code","source":"profile = ProfileReport(df_train, html={\"style\": {\"full_width\": True}}, title=\"Rain Australia\") # html attribute optional","metadata":{"id":"4BCOi_VJhM7U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile","metadata":{"id":"7FLZTILoh4A4","outputId":"97aa5eb3-b3be-4665-c840-98710182b75c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing I notice is that some columns have a very high proportion of missing data (< 35% in one column). So the first question that arises is; \"Do these columns/features matter in predicting the rain tomorrow?\" If they do, dropping them would result in a massive data loss. But on the other side, a data imputation would be ineffective because these data do definitely not have data missing at random (MAR). The columns that have a a lot of missing data are;\n\n\n    - Evaporation\n    - Sunshine\n    - Cloud9am\n    - Cloud3pm\n\nTheoretically, these features are important since you can imagine that clouds may be hang over the sky for a while. And a sunny day may be prolonged in with another sunny day. This is especially the case in a for example a land climate. The climate around a sea may be more volatile. Lastly, evaporation should (obviously) also be linked to rainfall.\n\nA second thing to mention is that the target column also contains NaN's. Since we do a fully supervised classification, the rows with NaN's in the target feature/column should also be dropped.\n\nLastly, rows with more than 50% NaN's should also be dropped, since these could not be very valueable in our analysis.\n\n\n\nLet's find out with an EDA on the data with ~40 % dropped rows, dropped NaN's in the target column and drop useless rows. \n\nNote: We drop the 40% rows by using just a few columns since the missing values are correlated to others. (according to the heatmap in the EDA). ","metadata":{"id":"m2IG1Borme0J"}},{"cell_type":"code","source":"# drop \"some\" rows\ndf_train_edit = (df_train.copy()\n                        .dropna(axis=0, subset=['RainTomorrow']) # drop the rows with no target values (useless in fully supervised training)\n                        .dropna(subset=['Cloud9am', 'Evaporation']) # drop the rows with NaN's in columns that have a lot of NaN's\n                        .dropna(axis=0, how='any', thresh=12)) # drop rows that have more than 50% NaN's in a row (~12 NaN's)   # drop the rows of some features\ndrop_profile = ProfileReport(df_train_edit, html={\"style\": {\"full_width\": True}}, title=\"Rain Australia\") # html attribute optional","metadata":{"id":"51v7XRdPf_vC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_profile","metadata":{"id":"Uftr3pgvgqr8","outputId":"73c0a5d7-d987-42b1-8d58-8bfe57c4a426","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we started with a lot of data, the histogram distributions of the features do not change that much, although we lost some locatinos and went from 49 locations to 30 locations. Since our goal is to make a general predictor for the rain the next day, this is not a huge problem. A problem might be an unbalanced (not uniform) distribution of the locations, what could cause a biased view of the weather in Australia. Let's see the histogram of the location column.","metadata":{"id":"lJinD_jSrDyh"}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.histplot(data=df_train_edit['Location'])\nplt.title('Histogram of the location labels')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"id":"lKzoZeiWstNG","outputId":"655cdec5-3a94-4f46-814a-9cbcef77ab53","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histogram is quiet uniform distributed, with a few exceptions. I think this would be a descent distribution to avoid a bias for a certain city. (Although the fact that we lost 19 cities)","metadata":{"id":"tr78tc86v0ci"}},{"cell_type":"markdown","source":"# A complete overview of the EDA\n\nPoint 1: Global exploration\n\n- Columns with high % missing values (> 40% missing values):\n      - Evaporation\n      - Sunshine ( + a lot of zeros)\n      - Cloud9am\n      - Cloud3pm\n\n**-> Action: drop a massive amount of rows, to include the features in the data analysis**\n\n\n- Columns with (very) high correlation:\n      - Min Temp - Temp 9am\n      - Max Temp - Temp 3pm\n      - Pressure9am - Pressure3pm\n\n**-> Action: Include in Columntransformer for MICE imputation, 1 of each pair is excluded afterwards** *( - an other option would be to do a PCA, but feature selection would be more dificult afterwards)*\n\n--------------------------------------------------------\n\nPoint 2: The numeric pipelines\n\nThe numeric pipeline is divided into three \"subpipes\".\n\n\n    The first subpipe\n- The first subpipe is the 'general' pipe with normally distributed features.\nThese features will be imputed using the \"multiple imputation by chained equations\" technique (MICE). This techinque will be most of the times better than just median or a mean imputation. Afterwards these features will be standardize with a MinMaxscaler to remain standardized positive values. (A Standard scaler would also create ngeative values.)\n\n  More practical info on MICE: https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/ .\n\n\n\n    The second subpipe\n- The second subpipe is for the right skewed distibutions:\n      - WindSpeed9am\n      - WindSpeed3pm\n      - WindGustSpeed\n      - Evaporation\n\n  These features will be transformed with a Powertransformer and afterwards imputed with the median and standardized with the MinMaxScaler.\n\n\n    The third subpipe\n\n- The third subpipe is to convert the features Rainfall and sunshine to variables with ordinal numbers. Because these values have a lot of zeros. Moreover, Rainfall also has a lot of outliers. The pipe will have median imputation before being converted to bins.\n\n\n    The fourth subpipe\n- The fourth subpipe will be an imputation pipe for the ordinal features. The pipe will have a 'most frequent' (mode) imputation. \n\n\n---------------------------------------------------------------------\nPoint 3: The categorical pipe\n\nThe categorical features will be encoded using a one-hot encoder\n\n------------------------------------------------------------------\nPoint 4: Feature Engineering pipe\n\nTo add additional features, the difference is calculated between the Windspeed in the morning and afternoon. More wind mostly means a change in the weather.\nFuthermore, the difference is calculated in the min and max temp on a day and difference in humidity during the day.\n\nThe pipe will consist of imputation with the median, apply a custom transformer to create the new features and lastly, to standardize these features with a MinMaxscaler.\n\n-----------------------------------------------------------------\nPoint 5: The target column\n\nThe target variable has a ratio of 1:3 in terms of the distribution of \"Yes\" and \"No\". So a convenient solution for this would be to do apply the \"Synthetic Minority Oversampling Technique\". The minority class will be oversampled to create a more balanced distribution.\n\nNOTE: The SMOTE must be performed DURING the Cross Validation. If SMOTE is performed performed before Cross Validation (only once). There will be an overfit on the synthesized data.\nHandy sources: \n\n    https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n\n\n    https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets#Test-Data-with-Logistic-Regression:\n---------------------------------------------------------------------\nA picture of the whole pipe will be displayed after the code for the pipe\n_________________________________________________________________________\n","metadata":{"id":"Yc1QwusLkFqi"}},{"cell_type":"markdown","source":"# Pipelines for the Columntransformer (Used for Train and test data)","metadata":{"id":"WLPK5su9KDUo"}},{"cell_type":"markdown","source":"## Custom transformer for feature engineering\n\nIdeas:\n\n- Difference between windspeed 9am and 3pm\n- Difference between humidity 9am and 3pm\n- Difference Min and Max temp","metadata":{"id":"maphLzN19GFJ"}},{"cell_type":"code","source":"#Custom transformer to add columns\n#This transformer calculates the difference in temp, windspeed and humidity on different time periods\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nminTemp_ix, maxTemp_ix, humidity_9_ix, humidity_3_ix, windspeed_9_ix, windspeed_3_ix = 0,1,2,3,4,5\n\nclass ColumnAdd(BaseEstimator, TransformerMixin):\n    def __init__(self): # no *args or **kargs\n        pass\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        diff_temp = np.abs(X[:,minTemp_ix] - X[:,maxTemp_ix])\n        diff_humidity = np.abs(X[:,humidity_9_ix] - X[:,humidity_3_ix])\n        diff_windspeed = np.abs(X[:,windspeed_9_ix] - X[:,windspeed_3_ix])\n        \n        return np.c_[diff_temp, diff_humidity, diff_windspeed] # transformer returns a np array","metadata":{"id":"o0u2_29gXCJE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline to transform numeric columns\n\nTo make better imputations than just the mean, the Multiple Imputation by Chained Equations is used instead of just a mean/median imputation. In order to maximize the result of a MICE, the highly correlated columns are initially included in the numeric pipeline. These columns are dropped at the final step of the pipeline to avoid multicollinearity during training. \n\nNote:\nAn alternative to dropping could be a PCA at the end. However, the interpretabillity of the features becomes harder since the features are transformed to PCA's. Just dropping the correlated features is more convenient.","metadata":{"id":"W9VOmexRVtsM"}},{"cell_type":"code","source":"# Function to use in the FunctionTransformer to drop come columns afterwards\ndef drop_cols(X):\n  \"\"\"\n  Drops the first three columns of X to remove multicollinearity (these are the highly correlated columns)\n\n  The columns were initially used to assist with the MICE imputation\n  \"\"\"\n  return X[:,3:] #drop the first three columns that are entered in the Columntransformer","metadata":{"id":"bD0hgYrSCs1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pipeline for all normally distributed features:\n\nnumeric_pipeline = Pipeline([\n                             ('Iter_imputer', IterativeImputer(initial_strategy='median')), # since there are quiete some features used in this pipe, MICE may be more effective than just the median\n                             ('scaler', MinMaxScaler()),\n                             ('drop_some_cols', FunctionTransformer(func=drop_cols)) # drops the first three columns to avoid multicollinearity\n])","metadata":{"id":"3Wd_GgAq_6LB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline for the Rainfall and Sunshine feature","metadata":{"id":"vSBqf_iwVzg0"}},{"cell_type":"code","source":"# Make the values of the rainfall feature discrete >> e.g. \"no rain\", \"little rain\", \"a lot of rain\"\nfrom sklearn.preprocessing import KBinsDiscretizer\n# another way to discretize is with a DecisionTreeDiscretiser\n\ndescrete_pipe = Pipeline([\n                          ('median_imputer',SimpleImputer(strategy='median')),\n                          ('discrete', KBinsDiscretizer(strategy='kmeans', encode='ordinal'))\n])","metadata":{"id":"8O0ZQ02mL6bu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline for right skewed distributions","metadata":{"id":"CxvYU5VW3u3U"}},{"cell_type":"code","source":"# Pipeline for all normally distributed features:\n\nnot_normal_pipe = Pipeline([\n                            ('log_trans', PowerTransformer()), #also scales the data and the Nan's remain NaN's\n                            ('median_imputer',SimpleImputer(strategy='median'))\n                            \n])\n","metadata":{"id":"GJ9k8VHj33rz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline for ordinal variables","metadata":{"id":"ScUk7-n8ctyJ"}},{"cell_type":"code","source":"# Pipeline to process ordinal features\n\nordinal_pipe = Pipeline([\n                         ('impute', SimpleImputer(strategy='most_frequent'))\n])","metadata":{"id":"-euDEp7ecrKN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline for feature engineering","metadata":{"id":"Ia64bg2uZ4pw"}},{"cell_type":"code","source":"#Feature engineering pipe (with the use ofthe custom transformer \"ColumnAdd\")\nfeature_eng_pipe = Pipeline([\n                             ('median_imputer',SimpleImputer(strategy='median')),\n                             ('add_columns', ColumnAdd()),\n                             ('scaler', MinMaxScaler())\n])","metadata":{"id":"ASAlQ0H6Z37A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline to encode Categorical Columns (one-hot encode)","metadata":{"id":"GKonqP4KV6os"}},{"cell_type":"code","source":"# pipe for all categorical columns\ncat_pipeline = Pipeline([\n                         ('one_hot', OneHotEncoder()) # all nan's get a seperate column\n])","metadata":{"id":"_wugzowWOrDm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Column Transformer","metadata":{"id":"d1C1LrXZV-5z"}},{"cell_type":"code","source":"#Remember the column names that remain in the DataFrame after dropping (to use in the Columntransformer)\nnormal_num_columns = ['Temp9am', 'Temp3pm', 'Pressure9am','MinTemp', 'MaxTemp', 'Humidity9am', 'Humidity3pm', 'Pressure9am'] # 'Temp9am', 'Temp3pm', 'Pressure9am' are dropped at the end to avoid multicollinearity\ncat_columns = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'] #Since the want to predict the general weather at first, the location column is not required\n\npreprocess_pipe = ColumnTransformer([\n                               ('numeric', numeric_pipeline, normal_num_columns), # Normally distributed columns\n                               ('descrete_pipe', descrete_pipe, ['Rainfall', 'Sunshine']), # pipe to create bins of the rainfall and sunshine features \n                               ('not_normal_pipe', not_normal_pipe, ['WindSpeed9am', 'WindSpeed3pm', 'WindGustSpeed', 'Evaporation']), # not normally distributed columns\n                               ('ordinal_pipe', ordinal_pipe, ['Cloud9am', 'Cloud3pm']),\n                               ('feature_engineer', feature_eng_pipe, ['MinTemp', 'MaxTemp', 'Humidity9am', 'Humidity3pm', 'WindSpeed9am', 'WindSpeed3pm']), # just try to see the effect\n                               ('cat_pipe', cat_pipeline, cat_columns) # categorical columns (one hot encode)\n\n])","metadata":{"id":"zNyS4QszBDqZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Graphical display of the full preprocessing pipeline","metadata":{"id":"99TeBPirCoAz"}},{"cell_type":"code","source":"from sklearn import set_config\n# from sklearn.utils import estimator_html_repr to save the display of the pipe in html format\n\n\nset_config(display='diagram') # set display to diagram instead of text\n\npreprocess_pipe","metadata":{"id":"r25XaTM_LJvL","outputId":"c1170a26-c07d-467b-fe86-1b31d1133c65","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process the training data before going into the pipeline\n# this variable was also already defined in the beginning, but just for clearity\ndf_train_edit = (df_train.copy()\n                        .dropna(axis=0, subset=['RainTomorrow']) # drop the rows with no target values (useless in fully supervised training)\n                        .dropna(subset=['Cloud9am', 'Evaporation']) # drop the rows with NaN's in columns that have a lot of NaN's\n                        .dropna(axis=0, how='any', thresh=12)) # drop rows that have more than 50% NaN's in a row (~12 NaN's)  \n\nX_train = df_train_edit.drop('RainTomorrow', axis=1)\ny_train = df_train_edit['RainTomorrow']\n\nX_processed = preprocess_pipe.fit_transform(X_train)\ncat_ordinal = LabelEncoder()\ny_processed = cat_ordinal.fit_transform(y_train)","metadata":{"id":"Wwpvnt6hCIZx","outputId":"c3db6b87-05a4-467d-fd01-46f84af873df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling the Minority class of the target data (ONLY for df_train!)","metadata":{"id":"T5F1MdQEri36"}},{"cell_type":"markdown","source":"An easy way to oversample the minority class in this example (Rain Tomorrow is True) is to pick random sample from the distribution and add them to the data. However, this would not add any extra information to the data. A better way is to use the Synthetic Minority Oversampling Technique (SMOTE).\n\n\"\" *SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b*.\"\"\n\n  â€” Page 47, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n\nBasically it produces new samples that are just slightly different. \n\nA tip from the creaters of the technique: First undersample the majority class before you oversample the minority class. So that's what we are going to do. This tip is effective because it creates more samples that are more plausible. BUT, since we already have a descent ratio (ratio = 1/3) undersampling of the majority class is not necessary.\n\nsource:\n\n    https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/","metadata":{"id":"a7G_IMUyiBvj"}},{"cell_type":"code","source":"oversample = SMOTE()\nX_train_sampled, y_train_sampled = oversample.fit_resample(X_processed, y_processed)","metadata":{"id":"j7NINHJ7r38p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_train_sampled)\nplt.show()","metadata":{"id":"EthN9Y_7_hNX","outputId":"fb2258d3-6115-457c-a4e7-7c2f784e1177","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import estimators","metadata":{"id":"r-GrKdIab16H"}},{"cell_type":"code","source":"#importing estimators\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier # Standard SGD is with SVCLinear\nfrom xgboost import XGBClassifier\n#from lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.neural_network import MLPClassifier\n\n# import CV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score","metadata":{"id":"y8lTr0KBsDHT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the models\n\nGlobal inspection of the CV scores","metadata":{"id":"nfL4vFeSpZbA"}},{"cell_type":"code","source":"# create dict of models\ndef get_models():\n  models = dict()\n  models['log'] = LogisticRegression()\n  models['knn'] = KNeighborsClassifier()\n  models['xgb'] = XGBClassifier()\n  models['sgd'] = SGDClassifier()\n  models['mlp'] = MLPClassifier()\n  return models","metadata":{"id":"ugYVFKPBurGu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, X, y):\n  cv = StratifiedKFold()\n  scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n  return scores","metadata":{"id":"bCQTGGiCw6zd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\n  # original pipe adjusted to train on the train data with SMOTE (only applicable on the train data)\n  imb_pipe = imblearn_pipeline([\n                                ('preprocess', preprocess_pipe),\n                                ('SMOTE', SMOTE()),\n                                (name, model)\n  ])\n  scores = evaluate_model(imb_pipe, X_train, y_processed)\n  results.append(scores)\n  names.append(name)\n  print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","metadata":{"id":"m4nMtDukyNdO","outputId":"68005989-a1b8-4a6d-d41c-cdb0cefa1a50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several models are around the same auc scores. To further explore these models a learning curve is plotted. In the learning curve we could observe which estimator would cause an overfit. ","metadata":{"id":"8q7nzeNnGurN"}},{"cell_type":"markdown","source":"# Plot Learning Curves of promising models","metadata":{"id":"IPs9dSWb3rLC"}},{"cell_type":"code","source":"# Adopted from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes, scoring='roc_auc',\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"id":"7yxmxFL81Szp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(20, 20))\ntitle = \"Learning Curves XGB\"\n\nxgb_pipe = imblearn_pipeline([\n                                ('preprocess', preprocess_pipe),\n                                ('SMOTE', SMOTE()),\n                                ('xgb', XGBClassifier())\n])\n\nplot_learning_curve(xgb_pipe, title, X_train, y_processed, axes=axes[:, 0], ylim=(0.8, 0.9),\n                    cv=5, n_jobs=-1)\n\n\ntitle = \"Learning Curves Logistic Regression\"\nlog_pipe = imblearn_pipeline([\n                                ('preprocess', preprocess_pipe),\n                                ('SMOTE', SMOTE()),\n                                ('log', LogisticRegression())\n])\n\nplot_learning_curve(log_pipe, title, X_train, y_processed, axes=axes[:, 1], ylim=(0.8, 0.9),\n                    cv=5, n_jobs=-1)\n\ntitle = \"Learning curves SGD\"\nsgd_pipe = imblearn_pipeline([\n                                ('preprocess', preprocess_pipe),\n                                ('SMOTE', SMOTE()),\n                                ('sgd', SGDClassifier())\n])\n\nplot_learning_curve(sgd_pipe, title, X_train, y_processed, axes=axes[:, 2], ylim=(0.8, 0.9),\n                    cv=5, n_jobs=-1)\n\n\n\nplt.show()","metadata":{"id":"xtscyBxv1o1T","outputId":"e63f644d-6381-40c7-dd1e-85ad909f6a9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, all estimators are pretty good predictors.. But which is the best? To find out, we do a search using the Hyperopt library and search for the best estimator with the best parameters. Logistic regression is left out in this grid search since it is it could be implemented in the SGD as a hyperparameter.\n\nA useful and easy library for Hyperopt in sklearn is the hpsklearn. However this library does not (yet) support a ROC_AUC metric. So that's why we use the original library. To solve this issue we could use SMOTE in the parameter tuning, but I find it more convenient to stick with the original values (without the synthesized ones)","metadata":{"id":"QLZz1Cq-OhvG"}},{"cell_type":"markdown","source":"# Hyperopt for hyperparameter optimization\n(With XGBoost and SGD)","metadata":{"id":"j0xJ3aeVfvET"}},{"cell_type":"code","source":"from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom hyperopt.pyll.stochastic import sample\nfrom functools import partial","metadata":{"id":"QyQWhyYbOWW4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Search spaces","metadata":{"id":"RfeUBrqKb69j"}},{"cell_type":"code","source":"###################################################\n##==== XGBoost hyperparameters search space ====##\n###################################################\n# parameters adopted from hpsklearn (github)\nhp_space = {\n    \n    'clf_type': hp.choice('clf_type', [\n                                       \n    {'type': 'xgb',\n     \n      'clf':{\n            'max_depth' :scope.int(hp.uniform('max_depth', 1, 11)),\n            'learning_rate':hp.loguniform('learning_rate', np.log(0.0001), np.log(0.5)) - 0.0001,\n            # 'n_estimators' : scope.int(hp.quniform('n_estimators', 100, 1000, 100)), # just do 100 since more is mostly better, but more cpu expensive\n            'n_estimators':100,\n            'scale_pos_weight':3, # target data distribution is approx. 1:3\n            'gamma': hp.loguniform('gamma', np.log(0.0001), np.log(5)) - 0.0001,\n            'min_chold_weight': scope.int(hp.loguniform('min_chold_weight', np.log(1), np.log(100))),\n            'subsample': hp.uniform('subsample', 0.5, 1),\n            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n            'colsample_bylevel':hp.uniform('colsample_bylevel', 0.5, 1),\n            'reg_alpha':hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001,\n            'reg_lambda':hp.loguniform('reg_lambda', np.log(1), np.log(4)),\n            'n_jobs':-1,\n            }},\n\n            #adopted from hpsklearn\n      {'type': 'sgd',\n          'clf':{\n             \n            'penalty': hp.pchoice('penalty', [(0.40, 'l2'),(0.35, 'l1'), (0.25, 'elasticnet')]),\n            'loss': hp.pchoice('loss',[\n            (0.25, 'hinge'),\n            (0.25, 'log'),\n            (0.25, 'modified_huber'),\n            (0.05, 'squared_hinge'),\n            (0.05, 'perceptron'),\n            (0.05, 'squared_loss'),\n            (0.05, 'huber'),\n            (0.03, 'epsilon_insensitive'),\n            (0.02, 'squared_epsilon_insensitive')]),\n\n            'alpha': hp.loguniform('alpha', np.log(1e-6), np.log(1e-1)),\n            'l1_ratio': hp.uniform('l1_ratio', 0, 1),\n            'epsilon': hp.loguniform('epsilon', np.log(1e-7), np.log(1)),\n            'eta0':hp.loguniform('eta0', np.log(1e-5), np.log(1e-1)),\n            'power_t':hp.uniform('power_t', 0, 1),\n            'class_weight':'balanced',\n            'n_jobs':-1}}\n\n    ])}\n","metadata":{"id":"I3ZBHXS1av79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample(hp_space['clf_type']['type'])","metadata":{"id":"0DX5Y1XrawOS","outputId":"52822b5b-d932-4015-d0e8-ad716d78a62a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_clf1(hps):\n    \"\"\"\n    Constructs estimator\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    \n    Returns:\n    ----------------\n    model : sklearn.Pipeline.pipeline with hyperparameters set up as per hps\n    \"\"\"\n\n\n    if hps['clf_type']['type'] == 'xgb':\n      model = Pipeline([\n                      ('preprocess', preprocess_pipe),\n                      ('xgb', XGBClassifier(**hps['clf_type']['clf']))\n      ])                                                    # ** unpacks the dictionary   (when then dictionary has subdictionarries, there must be a special function or action)\n\n    elif hps['clf_type']['type'] == 'sgd':\n      model = Pipeline([\n                      ('preprocess', preprocess_pipe),\n                      ('sgd', SGDClassifier(**hps['clf_type']['clf']))\n            ])\n    \n    return model","metadata":{"id":"fjatVvLYaw4y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_to_min1(hps, X, y, ncv=5):\n    \"\"\"\n    Target function for optimization\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    X : feature matrix\n    y : target array\n    ncv : number of folds for cross-validation\n    \n    Returns:\n    ----------------\n    : target function value (negative mean cross-val ROC-AUC score)\n    \"\"\"\n\n    estimator = f_clf1(hps)\n    cv_res = cross_val_score(estimator, X, y, cv=StratifiedKFold(ncv), \n                             scoring='roc_auc', n_jobs=-1)\n    \n    return {\n        'loss': -cv_res.mean(), # return negative value because hyperopt wants to minimize the score (while you try to maximize the postive AUC)\n        'cv_std': cv_res.std(),\n        'status': STATUS_OK}","metadata":{"id":"0I5uq3asG7Wu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hints from https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt \n\n- There is a rule of thumb that for proper search space exploration you would require around 25 runs per dimension to converge. It is entirely heuristical, but might help to estimate how many steps you would need;\n\n- Always back up your results into Trials object, and possibly pickle it to hard drive. It is very frustrating to lose several hours of optimization due to a power cut or OS freezing;\n\n- Check carefully sign of your target function. Shouldn't a minus be there?\n\n- Don't use full capacity of your model for hyperparameter tuning. This means: your don't need 1000 trees in your XGBoost, nor do you need all 8 million samples of your training set. It is very unlikely that optimization on a reduced set/capacity would yield significantly different results, while computation time would be much lower;\n\n- MOST IMPORTANT: Never substitute proper feature engineering with extensive hyperparameter optimization. Former is much more important - untuned model with great features easily outperforms heavily optimized model without intelligent design. Do some EDA, try things, find out what works, what might work and what is garbage, and then, before you go to sleep - launch optimization for the night. Not the other way around.\n","metadata":{"id":"N0jvFlHsiVzn"}},{"cell_type":"code","source":"trials_clf = Trials()\nbest_clf1 = fmin(partial(f_to_min1, X=X_train, y=y_processed), hp_space, algo=tpe.suggest, max_evals=10, trials=trials_clf, verbose=1) # the loss does not improve that much after 10 evals","metadata":{"id":"Ef_g2frbG73a","outputId":"ad774711-fca9-4650-b6de-bc3e502c404d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss does not improve that much after 10 evals","metadata":{"id":"5j0xJcLLpPor"}},{"cell_type":"code","source":"space_eval(hp_space, best_clf1)","metadata":{"id":"KxzcmcaEsbzx","outputId":"3bd764f1-44c5-4f4c-a5a0-47ebde7b229f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the XGB it iss","metadata":{"id":"UsqA3oClFL6P"}},{"cell_type":"code","source":"best_clf1","metadata":{"id":"7mWUWJquRJ4K","outputId":"8f53f7aa-d55f-4fc4-fa05-95fd67f2c82b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_optimized = f_clf1(space_eval(hp_space, best_clf1)).fit(X_train, y_processed)","metadata":{"id":"iznyck3KHyIC","outputId":"b64960cd-ff22-45cd-d31d-fdb9183dc653","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate on the test set\nfrom sklearn.metrics import roc_auc_score\n\ndf_test = (df_test.dropna(axis=0, subset=['RainTomorrow'])\n                        .dropna(axis=0, how='any', thresh=12))\n\nX_test = df_test.drop('RainTomorrow', axis=1)\ny_test = df_test['RainTomorrow']\n\ny_test_processed = cat_ordinal.transform(y_test)\n\ny_pred = clf_optimized.predict_proba(X_test)[:,1]\n\nroc_auc_score(y_test_processed, y_pred)\n\n# print(test_score)","metadata":{"id":"V-mkf0amFAUt","outputId":"61f5058b-4953-47cb-8a19-425b74bc4926","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suggestions:\n\nTo further improve the model the following things could be performed:\n\n- Feature selection\n\n- More feature engineering\n\n- Enhance the amount of estimators\n\n- Ensemble some estimators\n\n- Create a Neural network in for example keras\n\n\nThanks for reading!","metadata":{"id":"2FpaIPSDYUBK"}}]}