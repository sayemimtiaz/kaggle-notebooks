{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Classification\nIn this section we will examine BitCoin data and see if we can predict a buy or sell. The data comes from a set of Coinbase trades from December of 2014 to January of 2018 and is available from Kaggle. We will also examine a set of data that describes wheat seeds of various geometries and their attributes. _See the references section of this chapter for links._\n\n\n### Logistic Regression\nLogistic Regression is a common algoriothm (and amongst the simplest used for classification tasks). To build a classifier, the algorithm attempts to find the line that best splits the data into the target classes.\n\nThis generally happens by:\n\n1. Picking a parameter value at random and placing a random line through the distribution.\n2. Measure how well the line separates the two classes (statistical deviance is used for the metric).\n* Guess the new values of the parameters and measure the separation.\n* Repeat until there are no better guesses. Gradient descent is typically used for the optimization.\n\n\n### Install Dependencies\nSome of the visualization steps in the lab require that GraphViz be installed in the machine where the command is run. This can be installed with:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%bash\n# apt install graphviz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Dependencies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import ensemble, model_selection, preprocessing, tree\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.classifier import ClassificationReport, ConfusionMatrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Resampling data from minute interval to day\nbit_df = pd.read_csv('../input/coinbase/coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv',\n  low_memory=False, error_bad_lines=True)\nbit_df['Timestamp'] = bit_df.Timestamp.astype('int', errors='ignore')\n\n# Convert unix time to datetime\nbit_df['date'] = pd.to_datetime(bit_df.Timestamp, unit='s', errors='coerce')\n\n# Reset index\nbit_df = bit_df.set_index('date')\n\n# Rename columns so easier to code\nbit_df = bit_df.rename(columns={'Open':'open', 'High': 'hi', 'Low': 'lo',\n                       'Close': 'close', 'Volume_(BTC)': 'vol_btc',\n                       'Volume_(Currency)': 'vol_cur',\n                       'Weighted_Price': 'wp', 'Timestamp': 'ts'})\n\n# Coerce to numeric types\nbit_df['hi'] = pd.to_numeric(bit_df.hi, errors='coerce')\nbit_df['lo'] = pd.to_numeric(bit_df.lo, errors='coerce')\nbit_df['close'] = pd.to_numeric(bit_df.close, errors='coerce')\nbit_df['open'] = pd.to_numeric(bit_df.open, errors='coerce')\nbit_df['ts'] = pd.to_numeric(bit_df.ts, errors='coerce')\n\n# Resample and only use recent samples that aren't missing\nbit_df = bit_df.resample('d').agg({'open': 'first', 'hi': 'mean',\n    'lo': 'mean', 'close': 'last', 'vol_btc': 'sum',\n    'vol_cur': 'sum', 'wp': 'mean', 'ts': 'min'}).iloc[-1000:]\nbit_df['buy'] = (bit_df.close.shift(-1) > bit_df.close).astype(int)\n\n# drop last row as it is not complete\nbit_df = bit_df.iloc[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the data\nbit_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the data types\nbit_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a description of the data\nbit_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Load Data\nExercises associated with this example look at predicting the whether a mushroom is poisonous.\n\n* Load the mushroom data\n\n\n### Decision Tree\nDecision tree models construct a set of rules based on the desired outcome\n\n* The process of training classifier is to get X and y and call ``.fit``.\n* To predict values of y (y hat), call ``.predict(X)``\n* To get the accuracy call ``.score(X, y)``","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Partition data in order to train the model\nignore = {'buy'}\ncols = [c for c in bit_df.columns if c not in ignore]\nX = bit_df[cols]\ny = bit_df.buy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model instance, train to create classifier\n# Random state is used to seed the initial state of the model\ndt_model = tree.DecisionTreeClassifier(random_state=42)\ndt_model.fit(X, y)\n\n# Score against the earlier buy decision\ndt_model.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note that this goes to a Unix path\ntree.export_graphviz(dt_model, out_file='/kaggle/working/tree1.dot',\n                     feature_names=X.columns, class_names=['Sell', 'Buy'],\n                    filled=True\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%bash\n# Export a visualization of what the algorithm thinks is impotant\n# This doesn't run on Windows. Also requires that you have graphviz installed (not a Python module)\n\n# %%bash\n# dot -Tpng -o/kaggle/working/tree1.png /tmp/tree1.dot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show parameters which were used to create the model\ndt_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a list of the most important parameters used in the creation of the model\nprint(sorted(zip(X.columns, dt_model.feature_importances_), key=lambda x:x[1], reverse=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Predict Which Mushrooms Are Poisonous\nExercises associated with this example look at predicting the whether a mushroom is poisonous.\n\n* Create a decision tree to model whether a mushroom is poisonous.\n* Determine the most important features.\n\n\n### Try and Generalize the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Partition out buy column (the data's buy column will be used to judge the model accuracy)\nignore = {'buy'}\ncols = [c for c in bit_df.columns if c not in ignore]\nX = bit_df[cols]\ny = bit_df.buy\nX_train, X_test, y_train, y_test = model_selection.\\\n    train_test_split(X, y, test_size=.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Truncate the depth to which the classifier is allowed to grow\ndt2 = tree.DecisionTreeClassifier(random_state=42, max_depth=3)\ndt2.fit(X_train, y_train)\ndt2.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export the decision tree to a visualization\ntree.export_graphviz(dt2, out_file='/tmp/tree2.dot',\n                     feature_names=X.columns, class_names=['Sell', 'Buy'],\n                    filled=True\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%bash\n# %%bash\n# dot -Tpng -o/data/analytics/img/tree2.png /tmp/tree2.dot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Create a Decision Tree using Segmented Data\nExercises associated with this example look at predicting the whether a mushroom is poisonous.\n\n* Create a testing and training set.\n* Check if the model generalizes to the testing set.\n* Visualize the tree.\n\n\n### Feature Engineering\nOnly using historical price data results in a poor model. We need to be a little more intelligent about what we are basing our decisions on.\n\n* What might a predictive model based purely on price be a poor predictor?\n* How might we derive additional insight from the data?\n\nFeature engineering is the practice of using a transformation of raw input data to create new features that can be used in an ML model. It can be used to add \"additional insight\" (usually derived from a procedure provided by a domain expert) that can help the machine model find more accurate predictions. Examples:\n\n* dividing a stock price by its earnings in order to get a ratio of how much an equity costs to how much money it makes\n* counting the occurrence of a particular word across a text document\n* joining data across tables (for example data describing cardiac events with neurological events) to get a better feel for a patient's overall health\n* applying signal-processing tools to an image and summarizing the output, for example transform functions to an EKG signal or a histogram to a medical image\n\nWhy use feature engineering?\n\n1. Transform original data relative to the target\n2. Bring in external data sources\n3. Use unstructured data sources\n4. Create features which are more easily interpreted\n\nAs the relative predictive accuracy of the model is assessed, it can be updated over time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Introduce Financial Measurements\n\ndef rsi(df, num_periods=14):\n    \"\"\" Relative strength index: technical measure of whether a stock\n      is strong or week based on closing prices of a recent trading pool.\n    \"\"\"\n    prev = df.close.shift(1)\n    change = (df.close - prev) / prev\n    change = change.rolling(window=num_periods).mean().fillna(0)\n    up, down = change.copy(), change.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n    up2 = up.rolling(center=False, window=num_periods).mean()\n    down2 = down.rolling(center=False, window=num_periods).mean()\n    rs = (up2 / down2).fillna(0)\n    res = (100 - 100/(1 + rs))\n\n    return res.replace([np.inf], 0)\n\n\ndef stoc(df, num_periods=14):\n    \"\"\" Stochastic Oscillator: a \"momentum indicator\" intended to predict\n      whether a stock is on an upswing or downswing.\n    \"\"\"\n    cur = df.close\n    low = df.close.rolling(center=False, window=num_periods).min()\n    high = df.close.rolling(center=False, window=num_periods).max()\n    return (100 * (cur - low)/(high - low)).fillna(0)\n\n\ndef williams(df, num_periods=14):\n    \"\"\" Buy/sell indicator.\n        Williams %R ranges from -100 to 0. When its value is above -20,\n        it indicates a sell signal and when its value is below -80, it indicates a buy signal.\n    \"\"\"\n    cur = df.close#.iloc[-1]\n    low = df.close.rolling(center=False, window=num_periods).min() #shift(-num_periods) .iloc[-num_periods:].min()\n    high = df.close.rolling(center=False, window=num_periods).max() #df.close.iloc[-num_periods:].max()\n    return (-100 * (high - cur) / (high - low)).fillna(-50)\n\n\ndef proc(df, num_periods=14):\n    \"\"\" It measures the most recent change in price with respect to the price in n days ago.\n        https://www.investopedia.com/terms/p/pricerateofchange.asp\n    \"\"\"\n    cur = df.close\n    prev = df.close.shift(-num_periods)\n    return ((cur - prev)/(prev*100)).fillna(0)\n\n\ndef obv(df, vol='vol_btc'):\n    \"\"\" On balance volume - Use volume flow to predict changes\n    if close up add vol, if down subtract\n    \"\"\"\n    # -1 if down 1 if up\n    close_up_or_down = (bit_df.close.diff().le(0) * 2 - 1)\n    obv = (close_up_or_down * bit_df[vol]).cumsum()\n\n    return obv.fillna(0)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code a new column with the metrics\nfor func in [rsi,\n             stoc, williams, proc, obv]:\n    bit_df[func.__name__] = func(bit_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exclude buy (used, to assess the accuracy of the model), generate outcome variable\nignore = {'buy'}\ncols2 = [c for c in bit_df.columns if c not in ignore]\nX = bit_df[cols2]\ny = bit_df.buy\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = model_selection.\\\n    train_test_split(X, y, test_size=.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a decision tree classifier. Train and score.\ndt3 = tree.DecisionTreeClassifier(random_state=42, max_depth=7)\ndt3.fit(X_train, y_train)\ndt3.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the important columns\nprint(sorted(zip(X.columns, dt3.feature_importances_), key=lambda x:x[1], reverse=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an alternative algorithm model and generate the overall\nrf1 = ensemble.RandomForestClassifier(random_state=3)#, max_depth=7)\nrf1.fit(X_train, y_train)\nrf1.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Feature Engineering\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Does the classification score improve if a feature engineered column is included?\n\n\n### ROC Curve\nMany machine learning predictions involve a degree of uncertainty and classification algorithms output not only the zero-one predictions, but the full probabilities. These probabilities can be summarized as a probabilistic classifiers (also called probability vectors or class probabilities). When evaluating a test data set, there is generally a number from 0 to 1 which describes the probability of a particular target. Generally the machine learning algorithm picks a threshold which is used to assign a particular prediction.\n\nThe probabilities can be visualized as an ROC curve to determine if there are \"accuracy tradeoffs\" for a specific dataset. By convention, you plot the false positive rate on the x-axis and the true-positive rate on the y-axis. A perfectly predictive model is a right angle with no false positives and no missed detections.\n\nThe area under the ROC curve is also used as an evaluation metric. The larger the area, the better the classification performance. Using both the visualization and the area provides a powerful way to gauge accuracy versus mis-classification tradeoffs.\n\n* If classifying for a disease, it is better to classify some healthy patients as sick rather than miss truly healthy pateints. _Though this comes with a cost as well._","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc, confusion_matrix, roc_curve\n\ndef fig_with_title(ax, title, figkwargs):\n    ''' Helper curve for plotting a figure\n    '''\n    if figkwargs is None:\n        figkwargs = {}\n    if not ax:\n        fig = plt.figure(**figkwargs)\n        ax = plt.subplot(111)\n    else:\n        fig = plt.gcf()\n    if title:\n        ax.set_title(title)\n    return fig, ax\n\n\ndef plot_roc_curve_binary(clf, X, y, label='ROC Curve (area={area:.3})',\n                          title=\"ROC Curve\", pos_label=None, sample_weight=None,\n                          ax=None, figkwargs=None):\n    ax = ax or plt.subplot(111)\n    ax.set_xlim([-.1, 1])\n    ax.set_ylim([0, 1.1])\n    y_score = clf.predict_proba(X)\n    if y_score.shape[1] != 2 and not pos_label:\n        warnings.warn(\"Shape is not binary {} and no pos_label\".format(y_score.shape))\n        return\n    try:\n        fpr, tpr, thresholds = roc_curve(y, y_score[:,1], pos_label=pos_label,\n                                     sample_weight=sample_weight)\n    except ValueError as e:\n        if 'is not binary' in str(e):\n            warnings.warn(\"Check if y is numeric\")\n            raise\n\n    roc_auc = auc(fpr, tpr)\n    fig, ax = fig_with_title(ax, title, figkwargs)\n\n    ax.plot(fpr, tpr, label=label.format(area=roc_auc))\n    ax.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Guessing')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend(loc=\"lower right\")\n\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve_multilabels(clf, X, y, labels, label_nums, label='ROC Curve {label} (area={area:.3})',\n                          title=\"ROC Curve\", sample_weight=None,\n                               ax=None, figkwargs=None, add_avg=True):\n    ''' ROC curvefor multiplabel data\n    '''\n    y_bin = preprocessing.label_binarize(y, label_nums)\n    y_score = clf.predict_proba(X)\n    fprs = {}\n    tprs = {}\n    roc_aucs = {}\n    for i, l in enumerate(labels):\n        try:\n            fprs[i], tprs[i], _ = roc_curve(y_bin[:,i], y_score[:,i],\n                                          sample_weight=sample_weight)\n            roc_aucs[i] = auc(fprs[i], tprs[i])\n        except ValueError as e:\n            if 'is not binary' in str(e):\n                warnings.warn(\"Check if y is numeric\")\n                raise\n    fig, ax = fig_with_title(ax, title, figkwargs)\n    for i, l in enumerate(labels):\n        x = fprs[i]\n        y = tprs[i]\n        text=label.format(area=roc_aucs[i], label=l)\n        ax.plot(x, y, label=text)\n    if add_avg:\n        f, t, _ = roc_curve(y_bin.ravel(), y_score.ravel())\n        r = auc(f, t)\n        text=label.format(area=r, label='Average')\n        ax.plot(f, t, label=text, color='k', linewidth=2)\n    ax.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Guessing')\n    ax.set_xlim([-.1, 1])\n    ax.set_ylim([0, 1.1])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend(loc=\"lower right\")\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve_binary(rf1, X_test, y_test, figkwargs={'figsize':(14,10)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yellowbrick version\nfig, ax = plt.subplots(figsize=(10, 10))\nroc_viz = ROCAUC(rf1)\nroc_viz.score(X_test, y_test)\nroc_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: ROC Curve\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Inspect the ROC curve for the seed classifier.\n\n\n### Confusion Matrix\nA Confusion Matrix is another way to evaluate performance. You can see where false positives (lower left) and false negatives (upper right) are. A confusion matrix is a two-by-two diagram where each element shows the class-wise accuracy or confusion between the negative and positive classes.\n\n![Confusion matrixes provide ways to evaluate model performance. They provide a way to see where false positives (lower left) and falst negatives (upper right) appear.](images/lab-analytics/classification/lab-analytics03-classification_57_1.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_confusion_matrix(clf, X, y, labels, random_state=42, annotate=True,\n                          cmap=plt.cm.Blues,\n                          title=\"Confusion Matrix\", ax=None, figkwargs=None):\n    fig, ax = fig_with_title(ax, title, figkwargs)\n    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n    y_pred = clf.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    fig.colorbar(im)\n    ax.set_xticks(range(len(labels)))\n    ax.set_xticklabels(labels, rotation=45)\n    ax.set_yticks(range(len(labels)))\n    ax.set_yticklabels(labels)\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n    if annotate:\n        for x in range(len(labels)):\n            for y in range(len(labels)):\n                plt.annotate(str(cm[x][y]),\n                             xy=(y,x),\n                             ha='center',va='center',color='red', fontsize=25, fontstyle='oblique')\n\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(rf1, X_test, y_test, ['sell', 'buy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notice that the training set performs much better!\nplot_confusion_matrix(rf1, X_train, y_train, ['sell', 'buy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Yellowbrick - Using percent\nmapping = {0:'sell', 1:'buy'}\nfig, ax = plt.subplots(figsize=(10, 10))\ncm_viz = ConfusionMatrix(rf1, classes=['sell', 'buy'], label_encoder=mapping)\ncm_viz.score(X_test, y_test)\ncm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Yellowbrick - Using count\nfig, ax = plt.subplots(figsize=(10, 10))\ncm_viz = ConfusionMatrix(rf1, classes=['sell', 'buy'], label_encoder=mapping)\ncm_viz.score(X_test, y_test)\n# cm_viz.score(X_test, y_test, percent=False)\n\ncm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Confusion Matrix\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Plot a confusion matrix for the seed model\n\n\n### Classification Report\n* Precision - Correct positive over all positive - True positives / (false + true positives) - How many selected items are relevant?\n* Recall - Correct positive over positive that should have been returned - True positives / (true postives + false negatives) - How many relevant items are selected?\n* F1 - Harmonic mean of above","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cr_viz = ClassificationReport(rf1, classes=['buy', 'sell'])\ncr_viz.score(X_test, y_test)\ncr_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Create a classification report\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Create a classification report.\n\n\n### Calibration Curve\nFrom http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html and https://jmetzen.github.io/2015-04-14/calibration.html.\n\nWhen performing classification, we want not only to predict the class label but also obtain a probability of the respective label. This gies a degree of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction.\n\nA well calibrated binary classifier should be able to pick among samples that approximates 80% (0.8). Some of the implementations in `sklearn` struggle, however. The `sklearn.calibration` module adds additional support for manging calibration in a uniform fashion. It also helps to assess the calibratioin of a specific model.\n\n_In a calibration curve, a perfectly calibrated curve will be a straight line. Logistic regression returns a well calibrated curve by default as it directly optimizes log-loss._","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score)\n\ndef plot_calibration_curve(est, name, fig_index,                      \n    X_train, X_test, y_train, y_test):\n    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1., solver='lbfgs')\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    for clf, name in [(lr, 'Logistic'),\n                      (est, name),\n                      (isotonic, name + ' + Isotonic'),\n                      (sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        if hasattr(clf, \"predict_proba\"):\n            prob_pos = clf.predict_proba(X_test)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_test)\n            prob_pos = \\\n                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n\n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n        print(\"%s:\" % name)\n        print(\"\\tBrier: %1.3f\" % (clf_score))\n        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n        print(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred))\n        print(\"\\tScore: %1.3f\\n\" % clf.score(X_test, y_test))\n\n        fraction_of_positives, mean_predicted_value = \\\n            calibration_curve(y_test, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n                 label=\"%s (%1.3f)\" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype=\"step\", lw=2)\n\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title('Calibration plots  (reliability curve)')\n\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    ax2.legend(loc=\"upper center\", ncol=2)\n\n    plt.tight_layout()\n\n\nplot_calibration_curve(rf1, 'Random Forest', 1,\n    X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Optimizing Models\nModels have *hyperparameters* that we can tune. These allow for different variations of the model to be explored for which is the most accurate.\n\nGrid search cross validation will hold out some of the data for testing purposes, so we can pass in the full X and y into it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf4 = ensemble.RandomForestClassifier()\nparams = {'max_features': [.4, 'auto'],\n         'n_estimators': [15, 200, 500],\n         'min_samples_leaf': [1, .1],\n         'random_state':[42]}\ncv = model_selection.GridSearchCV(rf4, params).fit(X, y)\nprint(cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf5 = ensemble.RandomForestClassifier(**cv.best_params_)\nrf5.fit(X_train, y_train)\nrf5.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf6 = ensemble.RandomForestClassifier(random_state=41)\nrf6.fit(X_train, y_train)\nrf6.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Optimize Model\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Optimize the classifier.\n\n\n### Learning Curves: Do we have enough data?\nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n\nAn important question that often needs to be addressed in machine learning is \"Do we have enough data?\" Learning curves can be helpful in assessing the answer. Every estimator has advantages and drawback with three general sources of error: bias, variance, and noise:\n\n* **bias**: average error between different training sets\n* **variance**: how sensitive a model is to different data sets\n* **noise**: property of the data that can be used to describe how much samples may deviate from the underlying relationship. Some distributions adhere very closely to predicted values while others deviate wildly.\n\nA highly biased model will describe the training data well, but offers poor predictions on testing data even if it is from the same sample or distribution. A highly variable model will describe training and testing data well (if the data is from the same sample/distribution), but offers poor predictions on new data from a different sample/distribution.\n\nIt is common for different of estimators to describe data differently. For example a simple model may provide a poor fit because it is too simple (and for that reason, highly biased). Or it is possible that a complex model may fit the training data too well, and is not able to make good predictions on new data (high variance).\n\nWhen training and assessing models, the goal is to make both [bias and variance as low as possible](https://en.wikipedia.org/wiki/Bias-variance_dilemma).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),\n                       fig_opts=None):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum y values plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    fig_opts = fig_opts or {}\n    plt.figure(**fig_opts)\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = model_selection.learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nplot_learning_curve(rf6, 'Random Forest', X, y, fig_opts={'figsize':(14,10)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef get_data(filename, resample='d', size=1000):\n    bit_df = pd.read_csv(filename)\n    # Convert unix time to datetime\n    bit_df['date'] = pd.to_datetime(bit_df.Timestamp, unit='s')\n    # Reset index\n    bit_df = bit_df.set_index('date')\n    # Rename columns so easier to code\n    bit_df = bit_df.rename(columns={'Open':'open', 'High': 'hi', 'Low': 'lo',\n                           'Close': 'close', 'Volume_(BTC)': 'vol_btc',\n                           'Volume_(Currency)': 'vol_cur',\n                           'Weighted_Price': 'wp', 'Timestamp': 'ts'})\n    # Resample and only use recent samples that aren't missing\n    bit_df = bit_df.resample(resample).agg({'open': 'first', 'hi': 'mean',\n        'lo': 'mean', 'close': 'last', 'vol_btc': 'sum',\n        'vol_cur': 'sum', 'wp': 'mean', 'ts': 'min'})\n\n    # drop if open is missing - ADDED!\n    bit_df = bit_df[~bit_df.open.isnull()]\n\n    if size:\n        bit_df = bit_df.iloc[-size:]\n    bit_df['buy'] = (bit_df.close.shift(-1) > bit_df.close).astype(int)\n    # drop last row as it is not complete\n    bit_df = bit_df.iloc[:-1]\n    return bit_df\n\nhour_df = get_data('../data/coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv',\n                   resample='h', size=None)\nprint(hour_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_train(df):\n    for func in [rsi,\n             stoc, williams, proc, obv]:\n        df[func.__name__] = func(df)\n\n    ignore = {'buy'}\n    cols2 = [c for c in df.columns if c not in ignore]\n    X = df[cols2]\n    X = X.fillna(0)\n    y = df.buy\n    X_train, X_test, y_train, y_test = model_selection.\\\n        train_test_split(X, y, test_size=.3, random_state=42)\n    return X_train, X_test, y_train, y_test\n\nhX_train, hX_test, hy_train, hy_test = get_test_train(hour_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hX_train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curve(ensemble.RandomForestClassifier(),\n                    'Random Forest', hX_train, hy_train, fig_opts={'figsize':(14,10)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Learning Curves\nExercises associated with this example look at creating a classifier from the (wheat) seed dataset.\n\n* Run a learning curve against the seed data. How much data do we need to train on?","execution_count":null},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}