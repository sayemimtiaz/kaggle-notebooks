{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt \nimport seaborn as sns\nfrom keras.backend import clear_session\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.metrics import AUC\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve, classification_report\nfrom tensorflow.random import set_seed\nfrom tensorflow import get_logger\n\n\nget_logger().setLevel('ERROR')\n\n# We use these random seeds to ensure reproductibility\nnp.random.seed(1)\nset_seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The challenge is to classify the health of a fetus as Normal, Suspect or Pathological based on cardiotocogram exam data. The dataset is composed of 1 csv file based on the research work of Ayres-de-Campos et al. [Ayres de Campos et al. (2000) SisPorto 2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal Med 5:311-318]. Let's do some exploratory data analysis to start with."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')\n\ndf.info()\n\ndf.head(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is composed of 2126 observations and all the features (there are 21 features and one outcome) are numerical. It appears that no value is missing. Let's see if some data are irrelevant, if there are outliers to tackle with, or if some standardization is needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ndf.hist(bins=20, figsize=(20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these histograms, we can notice several points to address to improve a priori the predictive power of a developed machine learning model.\n- The variance of the features differs a lot between variables, therefore standardization can be needed\n- The feature \"histogram_tendency\" has -1, 0, 1 values but we do not know what they correspond to (are they categories?). Without extra information (there is nothing about it in the documentation), it is probably safer not to consider this feature\nA last idea to consider : \n- Regarding the outcome we want to predict : we observe that there are way more healthy foetus rather than suspect or pathological ones. It could be good to stratify the data while splitting to train the model on a dataset with a more balanced distribution of 'healthy', 'suspect' and 'pathological' foetuses. Because of this inbalance, we will avoid to use 'accuracy' as a metrics. \n\nLet's try and compare two approaches :\n1) Developing a neural network without performing any feature engineering.\n2) A comparison of different ensemble techniques after performing some feature selection. \n\nOn this notebook is shown the first approach. Let's go ! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's remove the \"histogram_tendency\", standardize the data and split it between a train and test set\ndf = df.drop(['histogram_tendency'], axis=1)\n\nX, y = df.drop(['fetal_health'],axis=1), df['fetal_health']\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nencoder = OneHotEncoder()\ny = encoder.fit_transform(y.values.reshape(-1,1)).toarray()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state = 42)\n\ndef compile_model():\n    clear_session()\n\n    model = Sequential()\n\n    model.add(Dense(20, input_shape=(20,), activation='relu'))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[AUC(multi_label=True)])\n    return model\n\n\nloss, val_loss = [[],[]]\n\nmodel = compile_model()\nour_model = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                      epochs=100, verbose=0)\n\nloss.append(our_model.history['loss'])\nval_loss.append(our_model.history['val_loss'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get a first sense of how the model can overfit if we do not pay too much attention."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5,5))\n\nax.plot(loss[0], color='k', label='Train set')\nax.plot(val_loss[0], color='r', label='Test set')\nax.set_xlabel('Number of epochs')\nax.set_ylabel('Loss')\nax.set_xlim(0,30)\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the number of epochs increases from 1 to ~5, the loss function decreases on both the training and the test set. However, when the number of epochs is > 5, the loss function keeps decreasing on the training set but increases on the test set, which is a sign that the model is **overfitting**. We will keep the number of epochs equal to 4!\n\nLet's see how our model performs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = compile_model()\nour_model = model.fit(X_train, y_train, epochs=4, verbose=0)\ny_pred0 = model.predict(X_test)\n\ndef print_f1score(model, X_train, y_train, y_test, y_pred):\n    \n    training_score = f1_score(np.argmax(y_train,axis=1), \n                              np.argmax(model.predict(X_train),axis=1), average='micro')\n\n    test_score = f1_score(np.argmax(y_test,axis=1), \n                          np.argmax(y_pred,axis=1), average='micro')\n\n    print('f1-score on the training set: %s'%training_score)\n    print('f1-score on the test set: %s'%test_score)\n\nprint_f1score(model, X_train, y_train, y_test, y_pred0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check out these nice tricks of Dennis T to plot confusion matrix\n# [https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea]\n\ndef print_confusion_matrix(model, X_train, y_train, y_test, y_pred):\n    train_confusion = confusion_matrix(np.argmax(y_train,axis=1), np.argmax(model.predict(X_train),axis=1))\n    test_confusion = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n\n    fig, ax = plt.subplots(1,2,figsize=(12,5))\n\n    sns.heatmap(train_confusion/np.sum(train_confusion), ax=ax[0], annot=True, fmt='.2%', cmap='Reds')\n    ax[0].set_xlabel('Predicted labels')\n    ax[0].set_ylabel('Actual labels')\n    ax[0].set_title('Confusion matrix (train set)')\n\n    sns.heatmap(test_confusion/np.sum(test_confusion), ax=ax[1], annot=True, fmt='.2%', cmap='Reds')\n    ax[1].set_title('Confusion matrix (test set)')\n    ax[1].set_xlabel('Predicted labels')\n    ax[1].set_ylabel('Actual labels')\n\nprint_confusion_matrix(model, X_train, y_train, y_test, y_pred0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suspect correctly labeled\na = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred0, axis=1))\ntmp1 = np.round(100*a[1,1]/np.sum(a[1,:]),1)\n\n# Pathological correctly labeled\ntmp2 = np.round(100*a[2,2]/np.sum(a[2,:]),1)\n\nprint('The confusion matrix show us that, on unseen data:')\nprint(tmp1,'% of the Suspect foetus are correctly labeled')\nprint(tmp2,'% of the Pathological foetus are correctly labeled')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am sure that we can do better ! One solution to reduce the proportion of false negatives and false positives is to perform a hyperparameter tuning. In a neural network model like that, the number of degrees of freedom is quite large. It is possible to modify the architecture of the network (number of layers, number of neurons per layer etc.), the activation functions, the optimizer etc. For the sake of simplicity, I will not tweak the architecture of the neural network here. Let's find better activation functions and optimizer!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_new_model(act, opt):\n    clear_session()\n    model = Sequential()\n    model.add(Dense(20, input_shape=(20,), activation=act))\n    model.add(Dense(20, activation=act))\n    model.add(Dense(20, activation=act))\n    model.add(Dense(20, activation=act))\n    model.add(Dense(20, activation=act))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(optimizer=opt, loss='categorical_crossentropy')\n    return model\n\nnew_model = KerasClassifier(build_fn=my_new_model, verbose=0)\n\nparameters = dict(opt = ['adam', 'sgd', 'adamax'], \n                  act=['relu', 'softmax', 'tanh', 'selu'],  \n                  batch_size=[32, 64, 128, 256, 512])\n\nrandom_search = RandomizedSearchCV(new_model, param_distributions=parameters, \n                                   n_iter=30, scoring='roc_auc', random_state=123)\n\nres = random_search.fit(X_train, y_train)\n\nprint(res.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see how we perform with the prescribed optimizer and activation function! \n\noptimized_model = my_new_model('relu', 'adam')\n\noptimized_model.fit(X_train, y_train, epochs=14, batch_size=32, verbose=0)\n\ny_pred = optimized_model.predict(X_test)\n\nprint_f1score(optimized_model, X_train, y_train, y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_confusion_matrix(optimized_model, X_train, y_train, y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suspect correctly labeled\na = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\ntmp1 = np.round(100*a[1,1]/np.sum(a[1,:]),1)\n\n# Pathological correctly labeled\ntmp2 = np.round(100*a[2,2]/np.sum(a[2,:]),1)\n\nprint('Thanks to the hyperparameter tuning, on unseen data, we have now:')\nprint(tmp1,'% of the Suspect foetus are correctly labeled')\nprint(tmp2,'% of the Pathological foetus are correctly labeled')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is definitely better than what we have achieved before. Further, the confusion matrix derived from the model predictions on the training set and the test set are rather similar. It is a good hint that the model has not overfitted.\nLet's take a look at the precision-recall curve for both models (before and after hyperparameter tuning)."},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = dict()\nrecall = dict()\nprecision_opt = dict()\nrecall_opt = dict()\n\nfor i in range(3):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:,i],y_pred0[:,i])\n    precision_opt[i], recall_opt[i], _ = precision_recall_curve(y_test[:,i],y_pred[:,i])\n\nfig, ax = plt.subplots()\n\ncolors = ['k','r','b']\nlabels = ['Healthy','Suspect','Pathological']\n\nfor i in range(3):\n    ax.plot(recall[i],precision[i],color=colors[i],label=labels[i],linestyle='--')\n    ax.plot(recall_opt[i],precision_opt[i],color=colors[i],label=labels[i],linestyle='-')\n\nax.set_xlabel('Recall TP/(FN+TP)')\nax.set_ylabel('Precision TP/(FP+TP)')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**:\nThe f1-score and the area under the precision-recall curve has increased with the hyperparameter tuning. The advantage of this technique is its easiness as it does not require much effort on feature engineering and/or selection. It can definitely be improved by tuning other aspects of the model, for instance, its architecture (adding a batch normalization, a dropout layer etc...). Further, an effort should be made to validate the model performance (by analysing -for instance- the model performance on different initial random seeds).\n\n**COMING SOON** I'll publish soon another approach with feature selection and different ensemble techniques. The idea is to compare a voting classification technique, a random forest classifier and an extreme gradient boosting algorithm. Preliminary results show a better predictive strength of these models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}