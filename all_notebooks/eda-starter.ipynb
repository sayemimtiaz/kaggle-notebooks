{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nplt.style.use('seaborn-talk')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/system-identification-of-an-electric-motor/Dataset_Electric_Motor.csv')\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution\n\nAccording to [this introductory paper](https://arxiv.org/pdf/2003.07273.pdf), *id_k1* and *iq_k1* are to be treated as target features.\n\nAt the same time, they depend on elementary vectors label-encoded into integers.\n\nLet's analyze how they are distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2,sharex=True, sharey=True)\nfor c, ax in zip(['n_k', 'n_1k'], axes.flatten()):\n    sns.countplot(x=c, data=dataset, palette=\"ch:.25\", ax=ax)\nunique_elem_vecs = dataset['n_k'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Element vector with k = 1 appears significantly more often than the remaining element vector types."},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs = dataset.assign(pairs=lambda r: r.n_k.astype(str)+'->'+r.n_1k.astype(str))['pairs']\npairs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Transition between elementary vectors count')\npairs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_data = dataset.iloc[::1000, :]\nanalyzed_cols = [c for c in dataset if c != 'n_k']\nfig, axes = plt.subplots(nrows=unique_elem_vecs, ncols=len(analyzed_cols), sharex='col', figsize=(20, 20))\n\nfor k, df in reduced_data.groupby('n_k'):\n    for i, c in enumerate(analyzed_cols):\n        sns.distplot(df[c], ax=axes[k-1, i])\n        if i == 0:\n            axes[k-1, i].set_ylabel(f'n_k = {k}')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It becomes evident that certain transitions in the elementary vectors are more common than others.\n\nMoreover, depending on the current elementary vector, distribution of currents and rotor angle *epsilon_k* is either unimodal or bimodal distributed.\n\nMore subtle, we recognize a semi-sphere shape of the 2d histogram between the currents (remember, *d* and *q* currents are to be plotted perpendicular to each other).\n\nIt might be auspicious, to add another feature denoting the current vector norm *id^2 + iq^2*.\n\nOn another note, epsilon is the rotor angle, which by design has a value discontinuity in the extreme points over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_data['epsilon_k'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously, the value range is clipped to *[-$pi$, $pi$]*.\n\nAs many ML methods do not respond well to discontinuities in the input space with no corresponding effect on the target space, we replace epsilon by its sine and cosine.\n\n# Feature Engineering\n\nWe add sine and cosine of the rotor angle and the current vector norm."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.assign(sin_eps_k=lambda df: np.sin(df.epsilon_k), \n                         cos_eps_k=lambda df: np.cos(df.epsilon_k),\n                         i_norm=lambda df: np.sqrt(df.id_k**2 + df.iq_k**2)).drop('epsilon_k', axis=1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = reduced_data.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(250, 15, s=75, l=40,n=9, center=\"dark\", as_cmap=True)\n\nplt.figure(figsize=(14,14))\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe strong linear correlation between consecutive current measurements in *d/q* coordinates each.\n\nAll other pair-wise comparisons are relatively uncorrelated."},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\n\nWe kick off regression with a linear model, as the correlation matrix suggests expedient estimation performance just from actual currents.\nSince elementary vectors are to be treated as categorical, we one-hot encode them before training.\n\nMoreover, in order to fit in RAM, we subsample the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset.iloc[::100, :]\\\n            .assign(**{**{f'n_k_{i}': lambda x: (x.n_k == i).astype(int) for i in range(1, 8)},\n                       **{f'n_1k_{i}': lambda x: (x.n_1k == i).astype(int) for i in range(1, 8)}})\\\n            .drop(['n_k', 'n_1k'], axis=1)\n\ntarget_cols = ['id_k1', 'iq_k1']\ninput_cols = [c for c in df if c not in target_cols]\ncv = KFold(shuffle=True, random_state=2020)\n\nss_y = StandardScaler().fit(df[target_cols])\ndf = pd.DataFrame(StandardScaler().fit_transform(df),\n                     columns=df.columns)  # actually methodically unsound, but data is large enough","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX, y = df[input_cols].values, df[target_cols].values\n\nscores = []\nfor train_idx, test_idx in cv.split(X, y):\n    ols = LinearRegression().fit(X[train_idx], y[train_idx])\n    pred = ols.predict(X[test_idx])\n    pred = ss_y.inverse_transform(pred)\n    gtruth = ss_y.inverse_transform(y[test_idx])\n    scores.append(mean_squared_error(pred, gtruth))\nscores = np.asarray(scores)\nprint('MSE:')\nprint(f'Scores Mean: {scores.mean():.4f} A² +- {2*scores.std():.4f} A²\\nScores Min: {scores.min():.4f} A², Scores Max: {scores.max():.4f} A²')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a rather weak estimation.\nCan you beat this score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}