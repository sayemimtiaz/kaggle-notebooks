{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abstract:\n\nIn this Notebook, LSTM is applied to correlate/ predict whether the pump may be shut down (broken) for a period of time based on the input signal of the sensors.\n\nThe input parameters of the model used are the sensor parameters (in the database called sensors from 00 to 51). The output of the model is a single parameter: the pump operating state corresponding to 0 is the shutdown state, 1 is the normal operation state and 0.5 is recovering.\n\nThis Notebook will analyze the correlation between the input parameters (sensors) and determine which parameters are the most important, deciding on the output of the model, thereby building the simplest model, which requires less input parameters but the most accurate prediction results.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata =  pd.read_csv(\"../input/pump-sensor-data/sensor.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1. Data cleaning","metadata":{}},{"cell_type":"markdown","source":"<h4> The author of the data set has reported that the system had 7 system failures over one year, which caused serious problems. Thus, the problem consists on predicting when will the next failure occur is very important. <h4>\n    (www.kaggle.com/nphantawee/pump-sensor-data) ","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> The data frame shows that, there are 55 columns with 220320 recordings. Moreover, the measurements have different scales, as following.<h4>","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Let remove first all NaN columns and all columns have zero standard values.<h4>","metadata":{}},{"cell_type":"code","source":"data.drop(['Unnamed: 0', 'timestamp','sensor_00','sensor_15','sensor_50','sensor_51'],axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to note that, because sensors are not specifically noted what operating parameters, for example, pressure, temperature, flow, vibration, ... they measure. However, concretely and quickly making a judgment about which sensor will be the decisive parameter to the operating status and operation of the pump is very important. \n\nIn actual operation, pump systems are often equipped with more than one sensor for a single operating parameter, such as pressure, flow rate or temperature, for a variety of reasons such as safety, operation or system reliability or automation equipment confidence. This can be the cause of the overlap in the measured signals of some sensors as analyzed following.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As can be seen there are a pattern being captured by the sensors, example:\n\n+ (1,2,3), \n+ (4,5,6,7,8,9),\n+ (10,11,12), \n+ (14,16,17,18), \n+ (19,20,21,22,23,24), \n+ (25,26,28,29,30,31,32,33), \n+ (34,35), \n+ (38,39,40,41,42,43,45,46,47). \n    \nIn turn, there are signals that are very noisy and seem to follow no trend in particular.","metadata":{}},{"cell_type":"markdown","source":"On the basis of that analysis, determining which sensor signal influence the operating state of the pump is important for modeling. To optimize the model, we proceed to select the input parameters of the model according to the following hypotheses.","metadata":{}},{"cell_type":"code","source":"data['machine_status'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The database has 7 BROKEN states, which are then RECOVERED and returned to a NORMAL operating state. For the sack of simplicity, we can assume that 25% of the data could be used to train the model (covering 2 BROKEN states), the remaining 75% of the data is used to test the predictability of the model based on input parameters (covers 5 BROKEN points).\n\nFor graphical illustration purpose, we assume the BROKEN state transitions have a value of 0, the RECOVERING state and NORMAL operation value 0.5 and 1, respectively and converted it into a new column named: \"Operation\".","metadata":{}},{"cell_type":"code","source":"import numpy as np\nconditions = [(data['machine_status'] =='NORMAL'), (data['machine_status'] =='BROKEN'), (data['machine_status'] =='RECOVERING')]\nchoices = [1, 0, 0.5]\ndata['Operation'] = np.select(conditions, choices, default=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to check if there is some obvious patterns that could be landmarked in a certain period, we have added the \"Operation\" code in the illustrations. That could helps us to define a good dataset to fitthe model.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2. Assumptions and LSTM model","metadata":{}},{"cell_type":"markdown","source":"As analyzed above, many measurements follow the same trend. To this end, one starts by keeping only the features of interest and drop the rest. Then, one performes feature normalization to bring all values into the range [0,1]. Starting by dropping unused features, one can proceed as follows.","metadata":{}},{"cell_type":"markdown","source":"# Set 0: \nsensors numbers 4, 6, 7, 8, 9 will be included in the dataset.","metadata":{}},{"cell_type":"code","source":"df0 = pd.DataFrame(data, columns=['Operation','sensor_04', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set 1: \nsensors 1, 4, 10, 14, 19, 25, 34, 38","metadata":{}},{"cell_type":"code","source":"df1 = pd.DataFrame(data, columns=['Operation','sensor_01', 'sensor_04', 'sensor_10', 'sensor_14', 'sensor_19', 'sensor_25'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set 2: \nsensors 2, 5, 11, 16, 20, 26, 39","metadata":{}},{"cell_type":"code","source":"df2 = pd.DataFrame(data, columns = ['Operation','sensor_02', 'sensor_05', 'sensor_11', 'sensor_16', 'sensor_20', 'sensor_26'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set 3: \nsensors 3, 6, 12, 17, 21, 28, 40","metadata":{}},{"cell_type":"code","source":"df3 = pd.DataFrame(data, columns = ['Operation','sensor_03', 'sensor_06', 'sensor_12', 'sensor_17', 'sensor_21', 'sensor_28'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0.plot(subplots =True, sharex = True, figsize = (20,20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that this time data series correlate a lot with the failure of the machine and can be a good indicator of the failure of the system, we will check it for another dataset. For now, the only concern is manipulation and prediction to test the robustness of classical methods.","metadata":{}},{"cell_type":"code","source":"df = df0\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3. Traing the model and implement the prediction","metadata":{}},{"cell_type":"markdown","source":"# Training set:\n\nWe choose 50,000 data points with 2 broken points to train the model, \n\n# Testing set:\n\nthe remaining 170,000 points with 5 broken states will be used to test the predictivity of the model.","metadata":{}},{"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ne = np.round(sum(np.abs(inv_y[:]-inv_yhat[:]))/(sum(inv_y[:])*len(inv_y[:]))*100,2)\naa=[x for x in range(160000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:160000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:160000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df2\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.plot(subplots =True, sharex = True, figsize = (20,20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=150, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\naa=[x for x in range(170000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:170000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:170000], 'r', label=\"prediction with the model\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4. Conclusions","metadata":{}},{"cell_type":"markdown","source":"In this Notebook, we focus on analyzing the raw data and looking for a logical approach. The construction of maintenance prediction plan for pump system operation is not only simple on the basis of net data but also needs to detail specific operating data, such as pressure, temperature, flow, vibration, ... of the pump system. These specific data will be very useful to analyze and apply industrial equipment operating knowledge to data analysis to build maintenance predictive models.\n\nThis notebook also analyzes and shows sensor signals that have similar characteristics or reflect similar operational parameters of the pump system. On that basis, it is necessary to select a suitable, simple, but relevant series of data that accurately reflects the nature of the operating characteristics of the pump system. Choosing the right data not only reduces the computational costs but also allows to build a predictive-model with high accuracy and reliability.\n\nSince I am a beginner, many mistakes can occur in terms of the methodology, figures or model ideas as well as during the construction of this Notebook. I would be happy to receive all of your comments and feedback.\n\nThank you very much.","metadata":{}}]}