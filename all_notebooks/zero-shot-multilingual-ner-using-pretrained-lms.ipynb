{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Zero-Shot Multilingual Transfer for Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LTRLIO1MAFw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers\n!pip install protobuf\n!pip install deep_translator\n!pip install pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport warnings\n\nimport numpy\nimport pandas\nimport torch\nfrom numpy import VisibleDeprecationWarning\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.exceptions import UndefinedMetricWarning\nfrom sklearn.metrics import classification_report\nfrom torch import nn\nfrom torch import from_numpy\nfrom torch.nn.functional import softmax\nfrom transformers import XLNetTokenizer, T5Tokenizer, GPT2Tokenizer\n\n# Add these to silence the warnings\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\nwarnings.filterwarnings(\"ignore\", category=VisibleDeprecationWarning)\n\n\ndef get_label2id_vocab(data):\n    label2idx = {'PAD': 0, 'UNK': 1}\n    idx = len(label2idx)\n    for line in data:\n        for l in line:\n            if l not in label2idx:\n                label2idx[l] = idx\n                idx += 1\n    return label2idx\n\n\ndef get_label_ids(data, labels2ids):\n    labels_ids = []\n    for line in data:\n        label_ids = []\n        for l in line:\n            label_ids.append(labels2ids.get(l, labels2ids.get('UNK')))\n        labels_ids.append(label_ids)\n    return labels_ids\n\n\nclass Classifier(nn.Module):\n    def __init__(self, embedding_dim, num_labels, dropout):\n        super(Classifier, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_labels = num_labels\n\n        self.dropout = nn.Dropout(dropout)\n        self.ff = nn.Linear(self.embedding_dim, num_labels)\n\n    def forward(self, inputs):\n        tensor = self.dropout(inputs)\n        tensor = self.ff(tensor)\n        tensor = tensor.view(-1, self.num_labels)\n        probs = softmax(tensor, dim=-1)\n        return tensor, probs\n\n\nclass Batcher(object):\n    def __init__(self, data_x, data_y, batch_size,\n                 emb_dim, pad_id, pad_id_labels, seed=0):\n        self.data_x = data_x\n        self.data_y = data_y\n        self.batch_size = batch_size\n        self.num_samples = data_x.shape[0]\n        self.emb_dim = emb_dim\n        self.indices = numpy.arange(self.num_samples)\n        self.ptr = 0\n        self.rnd = numpy.random.RandomState(seed)\n        self.pad_id = pad_id\n        self.pad_id_labels = pad_id_labels\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.ptr >= self.num_samples:\n            self.ptr = 0\n            self.rnd.shuffle(self.indices)\n            raise StopIteration\n        else:\n            batch_x = self.data_x[self.ptr: self.ptr + self.batch_size]\n            batch_y = self.data_y[self.ptr: self.ptr + self.batch_size]\n            self.ptr += self.batch_size\n\n            lengths = [len(x) for x in batch_x]\n            max_length_batch = max(lengths)\n\n            paddings = torch.full((max_length_batch * self.emb_dim,),\n                                  fill_value=self.pad_id,\n                                  dtype=torch.float32,\n                                  )\n            padded_labels = self.pad_id_labels * numpy.ones((len(batch_y), max_length_batch))\n            all_emb = list()\n            for idx, embeddings in enumerate(batch_x):\n                curr_len = embeddings.shape[0]\n                padded_labels[idx][:curr_len] = batch_y[idx]\n\n                all_emb += [emb for emb in embeddings]\n                num_pads = max_length_batch - curr_len\n\n                if num_pads > 0:\n                    t = paddings[\n                        :num_pads * self.emb_dim\n                    ]\n                    all_emb.append(t)\n            batch_x_final = torch.cat(all_emb).view(\n                [len(batch_x), max_length_batch, self.emb_dim]\n            )\n\n            batch_y_final = torch.LongTensor(padded_labels)\n\n            return batch_x_final, batch_y_final\n\n\ndata_df = pandas.read_csv('../input/multilingual-ner-data-english/train.train.csv', sep='\\t')\nprint(data_df.head())\nsentences = data_df.text.tolist()[:5000]\nlabels = data_df.labels.tolist()[:5000]\nsentences_filtered = []\nlabels_filtered = []\nfor text, label in zip(sentences, labels):\n    if len(text.split())< 80 and len(text.split()) == len(label.split()):\n        sentences_filtered.append(text)\n        labels_filtered.append(label)\n    else:\n        sys.stderr.write(f'Ignored example: {text} {label}\\n')\n\n\nlabels_filtered = [l.split() for l in labels_filtered]\ntrain_sentences = sentences_filtered\ntrain_labels = labels_filtered\n\ndata_df = pandas.read_csv('../input/multilingual-wikipedia-ner/test.dev.csv', sep='\\t')\ntest_sentences = data_df.text.tolist()[:1000]\ntest_labels = data_df.labels.tolist()[:1000]\ntest_labels = [l.split() for l in test_labels]\n\n\nlabels2idx = get_label2id_vocab(train_labels)\nidx2labels = {labels2idx[key]: key for key in labels2idx}\n\ntrain_labels = get_label_ids(train_labels, labels2idx)\ntest_labels = get_label_ids(test_labels, labels2idx)\n\n# Models:\n# SBERT: https://www.sbert.net/docs/pretrained_models.html\n# Huggingface: https://huggingface.co/models\n#\n# DATA\n# Download link: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n\nencoder = SentenceTransformer('quora-distilbert-multilingual')\n# encoder = SentenceTransformer('distilbert-multilingual-nli-stsb-quora-ranking')\n\ntrain_embeddings = encoder.encode(train_sentences,\n                                  output_value='token_embeddings',\n                                  show_progress_bar=True,\n                                  convert_to_tensor=False,\n                                  device='cpu',\n                                  )\n\ntest_embeddings = encoder.encode(test_sentences,\n                                 output_value='token_embeddings',\n                                 show_progress_bar=True,\n                                 convert_to_tensor=False,\n                                 device='cpu',\n                                 )\n\ntokenizer = encoder.tokenizer\n\n# Most models have an initial BOS/CLS token, except for XLNet, T5 and GPT2\nbegin_offset = 1\nif type(tokenizer) == XLNetTokenizer:\n    begin_offset = 0\nif type(tokenizer) == T5Tokenizer:\n    begin_offset = 0\nif type(tokenizer) == GPT2Tokenizer:\n    begin_offset = 0\n\n\ndef subword2word_embeddings(subword_embeddings, text):\n    sentence_embeddings = []\n    pooling = 'mean'\n    for embedding, sentence in zip(subword_embeddings, text):\n        start_sub_token = begin_offset\n        token_embeddings = []\n        for word in sentence.split():\n            sub_tokens = tokenizer.tokenize(word)\n            num_sub_tokens = len(sub_tokens)\n            all_embeddings = embedding[start_sub_token:start_sub_token + num_sub_tokens]\n\n            if not isinstance(all_embeddings, torch.Tensor):\n                # Convert numpy embeddings to tensor\n                all_embeddings = from_numpy(all_embeddings)\n\n            if pooling == 'mean':\n                final_embeddings = torch.mean(all_embeddings, dim=0)\n            elif pooling == 'last':\n                final_embeddings = all_embeddings[-1]\n            else:\n                final_embeddings = all_embeddings[0]\n\n            token_embeddings.append(final_embeddings)\n            start_sub_token += num_sub_tokens\n\n        sentence_embeddings.append(torch.stack(token_embeddings))\n\n    return sentence_embeddings\n\n\ntrain_embeddings = subword2word_embeddings(train_embeddings, sentences_filtered)\ntest_embeddings = subword2word_embeddings(test_embeddings, test_sentences)\n\n# This will create a array of pointers to variable length tensors\n# np_array = numpy.asarray(sentence_embeddings, dtype=object)\n\nn_samples = len(train_embeddings)\n_, emb_dim = train_embeddings[0].size()\n\npad_id = tokenizer.pad_token_id\npad_id_labels = labels2idx['PAD']\n\n# dtype=object is important\nbatcher = Batcher(numpy.asarray(train_embeddings, dtype=object),\n                  numpy.asarray(train_labels, dtype=object), 32, emb_dim,\n                  pad_id=pad_id, pad_id_labels=pad_id_labels)\n\nmodel = Classifier(embedding_dim=emb_dim, num_labels=len(labels2idx), dropout=0.01)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nfor e in range(30):\n    total_loss = 0\n    for batch in batcher:\n        batch_x, batch_y = batch\n        optimizer.zero_grad()\n        model_output, _ = model(batch_x)\n        loss = loss_fn(model_output, batch_y.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'Epoch:{e}, Loss: {total_loss/n_samples}')\n\nfinal_predictions = []\nfinal_labels = []\nfor emb, label in zip(test_embeddings, test_labels):\n    _, probs = model(emb)\n    predict_labels = torch.argmax(probs, dim=-1)\n    final_predictions.extend(predict_labels.tolist())\n    final_labels.extend(label)\n\n# Recommend for reading\n# https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\nprint(classification_report(final_predictions, final_labels))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}