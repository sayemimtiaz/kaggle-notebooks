{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta, date\nfrom statistics import mean \nimport copy\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dropout, Dense, Activation\nfrom sklearn.preprocessing import StandardScaler\nsns.set(rc={'figure.figsize':(11,8)})\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', 20)\npd.set_option('display.width', 1000)\nplt.rcParams[\"axes.grid\"] = False\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n\n\npaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname,filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General path variables which we'll use throughout the code\nlotteryPath = paths[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Dataset\n\nFirst, Let's load our main lottery dataframe and visualize the content\n*  Draw Number: Self explanatory\n*  Jackpot: Jackpot available for the winner\n*  Date: Date of Draw\n*  Ball 1, 2, 3, 4, 5, 6 & Extra Ball: Drawn balls with their respective order"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\nlottery = pd.read_csv(lotteryPath, encoding='latin-1')\nlottery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Overall Distribution\n\nTo better understand what's happening, let's start with the basics.\nWe'll try to answer the following question: \n*  What distribution does the lottery follow?"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_balls = {}\nfor i in range(1,7):\n    ball_ser = lottery['Ball ' +str(i)].value_counts()\n    for key in ball_ser.keys():\n        all_balls[key] = all_balls.get(key,0) + ball_ser[key]\n        \nball_ser = lottery['Extra Ball'].value_counts()\nfor key in ball_ser.keys():\n    all_balls[key] = all_balls.get(key,0) + ball_ser[key]\n\nall_balls = pd.Series(all_balls) \n\nplt.title('Distribution of all balls')\nplt.xticks(rotation=0)\nsns.barplot(x=all_balls.keys(), y=all_balls.values, palette=\"OrRd\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the result we got describes a uniform distribution.\nTechnically, that's what every lottery should look like, since it must assure equal probability of having any of the elements."},{"metadata":{},"cell_type":"markdown","source":"# Distribution of each ball\n\nSo now that we got the overall distribution of all of the balls, let's try to examine the underlying distribution of each ball.\nMaybe, we'll have something fishy down there"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the distributions of each ball\nf, axes = plt.subplots(7, 1)\nf.tight_layout() \nfor i in range(1,7):\n    ball_dist = lottery['Ball ' +str(i)].value_counts().sort_index()\n    axes[i-1].set_title('Distribution of ball '+str(i))\n    plt.xticks(rotation=90)\n    sns.barplot(x=ball_dist.keys(), y=ball_dist.values, ax=axes[i-1], palette=\"PuBuGn_d\")\n\nball_dist = lottery['Extra Ball'].value_counts().sort_index()\naxes[6].set_title('Distribution of extra ball')\nplt.xticks(rotation=90)\nsns.barplot(x=ball_dist.keys(), y=ball_dist.values, ax=axes[6], palette=\"PuBuGn_d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wait, what? Shouldn't that be uniform?\n\nShort answer, yes it should!  \nHowever, more insights about the data showed that LLDJ (La libanaise des jeux) ordered the numbers before posting them on their website.  \nThat being said, the distribution of the balls is irrelevant in this case, we only look back to the main distribution of the whole system.  \nIn simple terms, the order of the balls does not affect the result.  \nThe visualization here only showcases the distribution for the ordered data, which is not what's happening really."},{"metadata":{},"cell_type":"markdown","source":"# More insights in the correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    #filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    del df['Draw Number']\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr('pearson')\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1,cmap = \"BuGn\")\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=0)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n   \n    plt.title(f'Correlation Matrix', fontsize=15)\n    plt.show()\nplotCorrelationMatrix(lottery, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis\n\n## WE ARE TAKING IN CONSIDERATION THAT THE BALLS ARE ORDERED\n\nA little elaboration on the number distributions would explain what really is happening in the correlation matrix.  \nSo, let's state our main points that we'll rely on to build on in further analysis:\n* Each Individual ball has a normal distribution holding possible numbers that it can take.\n* The distributions of the 6 random balls (not the extra ball) have the same standard deviation and have means -more or less- equally distributed along numbers 1 -> 42  \n\nSince the distributions are shifted following a consecutive order, we are able to correlate Balls sequences with each other.  \nThe first obvious observation is that every ball correlates maximum with itself, since we're correlating the same distribution with itself.  \nTo elaborate on the latter, the next consecutive ball (Ex. Ball 4 then Ball 5) would generate also a substantial correlation. *But why is that?*   \nWell, since consecutive normal distributions hold regions of intersections (of possible numbers to take) then correlating the same numbers with each other will increase the relationship. You may look back to the ball distribution figure above to get a better feel of this.  \nSo, in simple terms, yes consecutive balls have correlations between them due to the intersection of their distributions. (That explains the perfect correlation of the ball with itself).  \nGoing on, the further the balls are, the more distant their distributions, the less the intersection, the less their correlation.\n\n### That leads us to the Extra Ball\n\nWe already know that Ball 1 -> Ball 6 have normal distributions and the extra ball follows a uniform distribution.  \nIn the same line of thought, the extra ball has intersections with all of the distributions. But due to its uniformity, the occurence of each ball is deminished to provide equal probability **and** that means that typically the correlation to each of the distributions should be the same, while also proving a close to zero correlation."},{"metadata":{},"cell_type":"markdown","source":"# Now what?\n\nMaybe more details will give insight about a possible flaw.  \n> \"The devil lies in the details\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def getDate(strDate):\n    return datetime.strptime(strDate, '%Y-%m-%d').date()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Occurences per Month\n\nLet's start looking into occurences per month.  \nThat means we need to visualize the occurence of each ball on a monthly basis."},{"metadata":{},"cell_type":"markdown","source":"## Step 1 \n\n### Define the underlying data structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This list will hold all of the monthly draws\nallMonthsData = []\n\n# The data that will be held will simply have the date of the draw along with the {\"ball drawn\": occurence}\nclass MonthData:\n    def __init__(self,date,ballsDict):\n        self.date = date\n        self.ballsDict = ballsDict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2\n\n### Helper functions\n\n* **generateStatsForDraws(draws, drawDate)**  \nThis function takes care of counting the drawn balls and storing them appropriately in the allMonthsData list.  \n\n* **plotBallsInMonths(index)**  \nPlot the occurences of balls for the index of the month given.\n\n#### N.B: Months are already sorted from the main dataframe, that means that index 0 is the first draw."},{"metadata":{"trusted":true},"cell_type":"code","source":"# What we know about these draws is that they'll be part of the same month\ndef generateStatsForDraws(draws, drawDate):\n    if(draws.empty == False):\n        currentBalls = {}\n        del draws['Date']\n        del draws['Draw Number']\n        del draws['Jackpot']\n        balls_list = draws.values.T.tolist()\n        balls_flat_list = [item for sublist in balls_list for item in sublist]\n        for i in range(1,43):\n            currentBalls[i] = balls_flat_list.count(i)\n        data = MonthData(drawDate, currentBalls)\n        allMonthsData.append(data)\n\n\n        \ndef plotBallsInMonths(index):\n    all_balls = pd.Series(allMonthsData[index].ballsDict) \n    plt.title('Balls in ' + str(allMonthsData[index].date.month) + \"-\" + str(allMonthsData[index].date.year))\n    plt.xticks(rotation=0)\n    sns.barplot(x=all_balls.keys(), y=all_balls.values, palette=\"GnBu_d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3\n\n### Organize draws by dates \n\n* **ball_month**  \nThe dataframe holds all draws for a given month (used within the for loop).  \n\n#### N.B: Months are already sorted from the main dataframe, that means that index 0 is the first draw."},{"metadata":{"trusted":true},"cell_type":"code","source":"ball_month = pd.DataFrame()\n\ninitDate = getDate(lottery['Date'][0])\ncurrentMonth = initDate.month\ncurrentYear = initDate.year\n\ndef getOccurencesPerMonth():\n    global ball_month\n    global currentMonth\n    global currentYear\n    for index, draw in lottery.iterrows():\n        drawDate = getDate(draw['Date'])\n        if(drawDate.month == currentMonth  and drawDate.year == currentYear): \n            ball_month = ball_month.append(draw)\n        else:\n            generateStatsForDraws(ball_month,drawDate)\n            ball_month = pd.DataFrame()\n            currentMonth = (currentMonth % 12) + 1 \n            if(currentYear != drawDate.year):\n                currentYear = drawDate.year\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4\n\n### Call the main method"},{"metadata":{"trusted":true},"cell_type":"code","source":"getOccurencesPerMonth()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5\n\n### Visualize balls for each month\n\nI know, I know, I could have optimized my code better.  \nNevertheless, for the time being, index 0 is the first month, 1 is the second, etc.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBallsInMonths(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nWell, nothing additional really.  \nJust more visualizations of the data.  \nThe results keep showing that the distribution is more or less uniform monthly (what did you expect? the main distribution already shows that)\n"},{"metadata":{},"cell_type":"markdown","source":"# Occurences per Year\n\nThis one is very similar to the latter, I wont explain the code.  \nMainly duplicated with slight changes.  \nOnly change here is the visualization, you'll get a better feel of what the data means"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I want to know on how many interval is each ball appearing\n# For ex: #23 appeared X times this year\n\nball_dataset = pd.DataFrame(columns = ['Year', 'Ball Number', 'Occurences'])\nball_dataset[\"Year\"] = pd.to_numeric(ball_dataset[\"Year\"])\nball_dataset[\"Ball Number\"] = pd.to_numeric(ball_dataset[\"Ball Number\"])\nball_dataset[\"Occurences\"] = pd.to_numeric(ball_dataset[\"Occurences\"])\n\n\n\n# What we know about these draws is that they'll be part of the same month\n# So we can have a data frame of that month where we count the numbers\ndef generateYearStatsForDraws(draws, drawDate):\n    global ball_dataset\n    if(draws.empty == False):\n        currentBalls = {}\n        #print(draws)\n        del draws['Date']\n        #print(draws)\n        balls_list = draws.values.T.tolist()\n        balls_flat_list = [item for sublist in balls_list for item in sublist]\n        #print(balls_flat_list)\n        for i in range(1,43):\n            currentBalls['Year'] = int(drawDate.year)\n            currentBalls['Ball Number'] = int(i)\n            currentBalls['Occurences'] = int(balls_flat_list.count(i))\n            ball_at_year = pd.Series(currentBalls)\n            currentBalls = {}\n            ball_dataset = ball_dataset.append(ball_at_year, ignore_index = True)\n        #print(\"yalla\")\n        \n        \nball_month = pd.DataFrame()\n\ninitDate = getDate(lottery['Date'][0])\ncurrentMonth = initDate.month\ncurrentYear = initDate.year\n\nfor index, draw in lottery.iterrows():\n    del draw['Draw Number']\n    del draw['Jackpot']\n    drawDate = getDate(draw['Date'])\n    if(drawDate.month == currentMonth  and drawDate.year == currentYear): \n        #print(draw['Date'])\n        ball_month = ball_month.append(draw)\n    else:\n        #break\n        #print(\"New Month\")\n        currentMonth = (currentMonth % 12) + 1 \n        if(currentYear != drawDate.year):\n            #print(\"New Year\")\n            #print(ball_month)\n            generateYearStatsForDraws(ball_month, drawDate)\n            ball_month = pd.DataFrame()\n            currentYear = drawDate.year\n\n\nprint(ball_dataset)    \n    \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmap of occurences\n\nX axis represents the year, Y axis is the ball number and the value encapsulated represents the occurance of that number within this year.\n\n## Some observations\n\n1. 2003 was the first actual lottery year, so it typically doesn't hold data for all months.\n\n2. 2006 held a war *(won't go into that)*, so lottery draws deminished\n\n3. Everything else shows up pretty uniform, no real discrepencies in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"balls = ball_dataset.pivot(\"Ball Number\", \"Year\", \"Occurences\")\nf, ax = plt.subplots(figsize=(18, 18))\nplt.title(\"Occurence of Each Ball per Year\")\nsns.heatmap(balls, annot=True, fmt=\"d\", linewidths=0.0, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's make things interesting...\n\nSo let's start by examinating some of the combinations which lead to higher probabilities.  \nThe most famous one is the ~**_even odd_**~ distribution.  \nLet's see how it works, I will first develop the code and we'll then explain it.\n\n### For the sake of simplicity, we'll ignore the extra number since we only care about the jackpot \n\n#### We'll evalute the following cases\n\n* 0 even 6 odd\n* 1 even 5 odd\n* 2 even 4 odd\n* 3 even 3 odd\n* 4 even 2 odd\n* 5 even 1 odd\n* 6 even 0 odd"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of even and odd numbers in a given list\ndef getNumberOfEvenAndOdd(numbers):\n    countEven = 0\n    countOdd = 0\n    for number in numbers:\n        if number % 2 == 0:\n            countEven += 1\n        else:\n            countOdd += 1\n    return str(countEven) + \" Even, \" +str(countOdd) +\" Odd\"\n\ndef visualizeEvenOddCombination():\n    even_odd = {}\n    numbers = []\n    for index, draw in lottery.iterrows():\n            numbers.append(int(draw['Ball 1']))\n            numbers.append(int(draw['Ball 2']))\n            numbers.append(int(draw['Ball 3']))\n            numbers.append(int(draw['Ball 4']))\n            numbers.append(int(draw['Ball 5']))\n            numbers.append(int(draw['Ball 6']))\n            even_odd[getNumberOfEvenAndOdd(numbers)] = even_odd.get(getNumberOfEvenAndOdd(numbers), 0) + 1 \n            numbers = []\n    even_odd_ser = pd.Series(even_odd)\n    print(even_odd_ser)\n    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n    labels = even_odd.keys()\n    sizes = list(even_odd.values())\n    sizes = [x / float(len(lottery)) * 100 for x in sizes]\n    explode = (0.1, 0, 0, 0, 0,0,0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n    fig1, ax1 = plt.subplots(figsize=(15, 6))\n    colors = [\"#416fc4\", \"#48cfdf\", \"#48c4d6\", \"#41d6d3\", \"#5decd2\", \"#87fde8\",\"#b4f6eb\"]\n    ax1.pie(sizes, explode=explode, labels=labels,colors = colors, shadow=True)\n    plt.title(\"Even Odd Probabilities\")\n    plt.legend( loc = 'lower right', labels=['%s, %1.2f %%' % (l, s) for l, s in zip(labels, sizes)])\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    \n\nvisualizeEvenOddCombination()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\nBriefly, around 33% of jackpots were formed from 3 even and 3 odd numbers.  \nYou can elaborate on the rest.  \nDoes that mean if you pick 3 even 3 odds you have higher chances to win?  \n**Definitely!**  "},{"metadata":{},"cell_type":"markdown","source":"# High - Low\n\nWe'll evaluate another aspect of combination.  \nSince the lotto is formed out of 42 numbers, we'll classify two classes of numbers\n1. High numbers >= 22\n2. Low numbers < 22\n\nYou'll remark that the codebase is very similar to the latter"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of even and odd numbers in a given list\ndef getNumberOfHighAndLow(numbers):\n    countHigh = 0\n    countLow = 0\n    for number in numbers:\n        if number >= 22:\n            countHigh += 1\n        else:\n            countLow += 1\n    return str(countHigh) + \" High, \" +str(countLow) +\" Low\"\n\ndef visualizeHighLowCombination():\n    high_low = {}\n    numbers = []\n    for index, draw in lottery.iterrows():\n            numbers.append(int(draw['Ball 1']))\n            numbers.append(int(draw['Ball 2']))\n            numbers.append(int(draw['Ball 3']))\n            numbers.append(int(draw['Ball 4']))\n            numbers.append(int(draw['Ball 5']))\n            numbers.append(int(draw['Ball 6']))\n            high_low[getNumberOfHighAndLow(numbers)] = high_low.get(getNumberOfHighAndLow(numbers), 0) + 1 \n            numbers = []\n    high_low_ser = pd.Series(high_low)\n    print(high_low_ser)\n    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n    labels = high_low.keys()\n    sizes = list(high_low.values())\n    sizes = [x / float(len(lottery)) * 100 for x in sizes]\n    explode = (0.0, 0.1, 0, 0, 0,0,0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n    fig1, ax1 = plt.subplots(figsize=(15, 6))\n    colors = [\"#006F3F\", \"#006F4F\", \"#006F53\", \"#006753\", \"#007e65\", \"#0a9d80\",\"#81c6b8\"]\n    ax1.pie(sizes, explode=explode, labels=labels,colors = colors, shadow=True)\n    plt.title(\"High Low Probabilities\")\n    plt.legend( loc = 'lower right', labels=['%s, %1.2f %%' % (l, s) for l, s in zip(labels, sizes)])\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    \n\nvisualizeHighLowCombination()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\nAlso, around 33% of jackpots were formed from 3 high and 3 low numbers.  \nThis is the exact same explanation as above, with just tweaked conditions!  \n**Use the statistics in your favor!**"},{"metadata":{},"cell_type":"markdown","source":"# Summing values\n\nLet's evaluate our last aspect of combination.  \nSince the lotto is formed out of 42 numbers.  \nLet's take the gaussian distributions into our advantage.  \n  \n  \nSteps to follow:\n1. Calculate the mean of distribution for each ball\n2. Find the sum of all means\n3. Evaluate the % of jackpots where the balls' sum falls around the mean (+ or - 29)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMean(ball_dist):\n    mean = 0\n    allOcc = 0\n    for value in list(ball_dist.keys()):\n        for occ in list(ball_dist.values):\n            mean += value * occ\n            allOcc += occ\n    mean = mean/allOcc\n    return mean\n\nsumOfMeans = 0\nfor i in range(1,7):\n    ball_dist = lottery['Ball ' +str(i)].value_counts().sort_index()\n    sumOfMeans += getMean(ball_dist)\nsumOfMeans = int(sumOfMeans)\nprint(sumOfMeans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Same line of thought applies to this method, you just divide the targets into two main parts: In Range & Out Of Range** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of even and odd numbers in a given list\ndef isNumbersInRange(numbers):\n    sumOfNumbers = 0 \n    for number in numbers:\n        sumOfNumbers += number\n    if (sumOfNumbers >= sumOfMeans - 29 and sumOfNumbers <= sumOfMeans + 29):\n        return \"In Range\"\n    else:\n        return \"Out of Range\"    \n\n\ndef visualizeInRangeCombination():\n    in_range = {}\n    numbers = []\n    for index, draw in lottery.iterrows():\n            numbers.append(int(draw['Ball 1']))\n            numbers.append(int(draw['Ball 2']))\n            numbers.append(int(draw['Ball 3']))\n            numbers.append(int(draw['Ball 4']))\n            numbers.append(int(draw['Ball 5']))\n            numbers.append(int(draw['Ball 6']))\n            in_range[isNumbersInRange(numbers)] = in_range.get(isNumbersInRange(numbers), 0) + 1 \n            numbers = []\n    in_range_ser = pd.Series(in_range)\n    print(in_range_ser)\n    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n    labels = in_range.keys()\n    sizes = list(in_range.values())\n    sizes = [x / float(len(lottery)) * 100 for x in sizes]\n    fig1, ax1 = plt.subplots(figsize=(15, 6))\n    colors = [\"#E4665C\", \"#F9B189\"]\n    ax1.pie(sizes, labels=labels, shadow=True, colors=colors)\n    plt.title(\"Jackpot Sum between \" + str(sumOfMeans - 29) + \" and \" + str(int(sumOfMeans + 29)))\n    plt.legend( loc = 'lower right', labels=['%s, %1.2f %%' % (l, s) for l, s in zip(labels, sizes)])\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    \n\nvisualizeInRangeCombination()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\nThe results here can be quite deceiving.  \nAround 70% of probability is substantial. So in simple words, if you choose to take 6 number whose sum will be between 100 and 158. That does not mean that you'll hit the jackpot 70% of the times. What that means is that 70% out of the jackpot is made out of 6 numbers whose sum falls between 100 and 158.  \nDoes that increases your chances, if you choose to use it?  \n**Definitely!**"},{"metadata":{},"cell_type":"markdown","source":"# If we group the three methods together, will we have a higher probability?\n\n\nMore specifically, if we choose the best combination of each of the latter methods, would we maximize our chances to win?  \nIn this case taking:\n* 3 Even 3 Odd\n* 3 High 3 Low\n* Sum of the ball numbers is between 100 and 158\n\n\n**For naming convention we'll define numbers that comform with the conditions as Elite Number**\n\n#### Well, let's not talk about it.  See below!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll assume that \"isEliteNumbers(numbers)\" function will determine that the numbers comform with the conditions specified above.\n\n# I know, I can optimize my code!\ndef isEliteNumbers(numbers):\n    for number in numbers:\n        if (isNumbersInRange(numbers) == \"In Range\" and getNumberOfEvenAndOdd(numbers) == \"3 Even, 3 Odd\" and getNumberOfHighAndLow(numbers) == \"3 High, 3 Low\"):\n            return \"Elite Numbers\"\n        else:\n            return \"Other Numbers\"    \n\n    \ndef visualizeEliteCombination():\n    numbers_type = {}\n    numbers = []\n    for index, draw in lottery.iterrows():\n            numbers.append(int(draw['Ball 1']))\n            numbers.append(int(draw['Ball 2']))\n            numbers.append(int(draw['Ball 3']))\n            numbers.append(int(draw['Ball 4']))\n            numbers.append(int(draw['Ball 5']))\n            numbers.append(int(draw['Ball 6']))\n            numbers_type[isEliteNumbers(numbers)] = numbers_type.get(isEliteNumbers(numbers), 0) + 1 \n            numbers = []\n    numbers_type_ser = pd.Series(numbers_type)\n    print(numbers_type_ser)\n    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n    labels = numbers_type.keys()\n    sizes = list(numbers_type.values())\n    sizes = [x / float(len(lottery)) * 100 for x in sizes]\n    fig1, ax1 = plt.subplots(figsize=(15, 6))\n    colors = [\"#424F60\", \"#003041\"]\n    ax1.pie(sizes, labels=labels, shadow=True, colors=colors)\n    plt.title(\"Elite Number probabilities\")\n    plt.legend( loc = 'lower right', labels=['%s, %1.2f %%' % (l, s) for l, s in zip(labels, sizes)])\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    \n\nvisualizeEliteCombination()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DO NOT PICK ALL THREE METHODS TOGETHER!\n\nIf you've taken a probability course, you should have seen it coming.\nCascading probabilities for dependent events will only worsen the result.\nSimple proof:\n> A: Getting 3 Even 3 Odd  \nB: Getting 3 High 3 Low  \nC: Getting 6 numbers whith the sum between 100 and 158\n  \n> P(A ∩ B ∩ C) = P(A) x P(B|A) x P(C|A ∩ B) ==> The more events you take into consideration the lesser your probabilty\n  \nI wont calculate the exact probability I'll keep it to you nerds out there (It's 10% duh!)"},{"metadata":{},"cell_type":"markdown","source":"# Enough Statistics, Let's talk **MACHINE LEARNING**\n\nSo we know that this is a regression problem, so there are a lot of ways to forecast using a multivariate system.  \nModel types which can be used in this context are:\n* Linear Regression\n* Vanilla RNN\n* GRU (close similarity with the LSTM)\n* CNN (Yes, google it)\n* CNN + LSTM\n* ConvLSTM\n* Even U-net can be used here , add LSTM(s) before your dense et voila.\n* Some statistical methods also include ARIMA and TBats (you can google both)\n\nSeveral architectures can be used such as Encoder - Decoder, AutoEncoders, Transformers etc..\n\nThere are a lot of models, would love to see them coming into life.  \n\nIn this kernel i'll use mainly 2 types:\n* LSTM\n* CNN + LSTM\n\n## LSTM (Long short term memory)\n\n### What is LSTM?\n\nI will assume that you already know how neural networks work.\nLSTM is a recurrent NN which was mainly designed to solve the vanishing/exploding gradient problem while backpropagating.\nIts architecure differs from a regular RNN. Instead of having a simple perceptron at the root, we'll have an LSTM cell.\n\n### LSTM cell\n\nBriefly, think of an LSTM cell as its own neural network.\nIt mainly contains four gates:\n* Input: Gets the input\n* Output: releases the output of the cell\n* Forget: In a broad manner, chooses what to forget and what to remember. It has a sigmoid activation function, so it learns how much to remember/forget.\n* Modulation: Adds non linearity to the input of the cell. Mainly used for faster learning convergence.\n\n### Why LSTM?\n\nBefore explaining why LSTM, I will define the goals that I'm trying to reach and explain accordingly.  \n*I first chose what I wanted to do, then decided what model to use*  \n  \n  \n1. Since lottery sequences are dependent events, are we able to find somekind of pattern that will explain the sequence?\n2. Do different lottery draws affect the result of the upcoming draw? (Will keep this one for another kernel)\n\nHaving set up our main concerns, we know that RNNs are well performing in temporal (time based) data applications.  \nWell, you have your answer.  \n  \n      \n        \n        \n### For more information on LSTM, visit the [publication][1].\n[1]: https://www.bioinf.jku.at/publications/older/2604.pdf"},{"metadata":{},"cell_type":"markdown","source":"## Step 1\n\nGather relevant data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lottery_ml = copy.deepcopy(lottery)\ndel lottery_ml['Draw Number']\ndel lottery_ml['Jackpot']\ndel lottery_ml['Date']\nlottery_ml.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2\n\nNormalize data for faster computations and also since we know that distributions are normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing and downscaling the data such that mean per column is 0\nscaler = StandardScaler().fit(lottery_ml.values)\ntransformed_dataset = scaler.transform(lottery_ml.values)\nlottery_ml_normalized = pd.DataFrame(data=transformed_dataset, index=lottery_ml.index)\nlottery_ml_normalized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3\n\nFrom our 1800 rows, we'll divide simple training and validation sets.  \nTraining/Validation= 80/20  \nWe'll also base our training on 50 previous games for our next prediction.  \nThe number of features we'll be passing = number of balls to be drawn within a game (7)  \nFinally, we'll prepare both our train and label arrays to train our network on. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_to_retain_for_test = 350\nnumber_of_rows= lottery_ml.values.shape[0] - rows_to_retain_for_test\ngames_window_size = 50 #amount of past games we need to take in consideration for training (It's also the number of draws)\nnumber_of_features = lottery_ml.values.shape[1] #balls count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of rows = number of games to train (samples)\n#Number of columns = Number of previous games (timestep)\n#Number of depths = Number of features (in this case on 7 balls) (features)\ntrain = np.empty([number_of_rows-games_window_size, games_window_size, number_of_features], dtype=float)\nlabel = np.empty([number_of_rows-games_window_size, number_of_features], dtype=float)\n\n\n# train represents the training data\n# label represents the expected result\n\n#train[2][0][5] holds the value of ball 6 (or feature number 6) in previous game 0 (that means the current game) which is actually game number 1.\n#train[200][4][3] holds the value of ball 4 (or feature number 4) in previous game 4 (that means 4 games before the current draw) where the current draw is 199.\n\n#Think about it as a row based visualization\n\n#For each row ==> Technically we can say that every row is a training batch\n\nfor i in range(0, number_of_rows-games_window_size):\n    train[i]=lottery_ml_normalized.iloc[i:i+games_window_size, 0: number_of_features]\n    # the label will have the result of the next draw.\n    label[i]=lottery_ml_normalized.iloc[i+games_window_size: i+games_window_size + 1, 0: number_of_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model architecture\n\nSo the model which we used was a very simple model with 2 cascading LSTM networks, each with 200 units and dropout of 0.2."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(200,      \n            input_shape=(games_window_size, number_of_features),\n            return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(200,           \n               return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(number_of_features))\nmodel.compile(loss='mse', optimizer='rmsprop', metrics=[\"accuracy\"])\n\n# train model normally\nhistory =  model.fit(train, label, epochs= 500, validation_split=0.25, batch_size = 128, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overfit\nThe model is obviously over-fit and was not able to train on the data."},{"metadata":{},"cell_type":"markdown","source":"# Step 5\n\nThe best way to evaluate the accuracy in my opinion is through a plot.\nHere, the X axis represents the draw number, while the Y axis repesents the ball number.\nI will show plots for Ball 1, 4 and the extra ball so that it doesn't get messy.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df_0 = pd.DataFrame(columns = ['Validation Draw','Ball','Value'])\nvalidation_df_0[\"Validation Draw\"] = pd.to_numeric(validation_df_0[\"Validation Draw\"])\nvalidation_df_0[\"Value\"] = pd.to_numeric(validation_df_0[\"Value\"])\n\nvalidation_df_1 = pd.DataFrame(columns = ['Validation Draw','Ball','Value'])\nvalidation_df_1[\"Validation Draw\"] = pd.to_numeric(validation_df_1[\"Validation Draw\"])\nvalidation_df_1[\"Value\"] = pd.to_numeric(validation_df_1[\"Value\"])\n\nvalidation_df_3 = pd.DataFrame(columns = ['Validation Draw','Ball','Value'])\nvalidation_df_3[\"Validation Draw\"] = pd.to_numeric(validation_df_3[\"Validation Draw\"])\nvalidation_df_3[\"Value\"] = pd.to_numeric(validation_df_3[\"Value\"])\n\nvalidation_df_6 = pd.DataFrame(columns = ['Validation Draw','Ball','Value'])\nvalidation_df_6[\"Validation Draw\"] = pd.to_numeric(validation_df_6[\"Validation Draw\"])\nvalidation_df_6[\"Value\"] = pd.to_numeric(validation_df_6[\"Value\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(lottery_ml.values.shape[0]-rows_to_retain_for_test, lottery_ml.values.shape[0] - games_window_size ):\n    to_predict = lottery_ml.iloc[i:i+games_window_size].values.tolist()\n    scaled_to_predict = scaler.transform(to_predict)\n    scaled_predicted_output_1 = model.predict(np.array([scaled_to_predict]))\n    predicted_draw = scaler.inverse_transform(scaled_predicted_output_1).astype(int)[0]\n    actual_draw = lottery_ml.iloc[i+games_window_size].values.tolist()\n    validation_df_0 = validation_df_0.append({'Validation Draw': int(i), 'Ball': \"Actual\", 'Value': actual_draw[0]}, ignore_index=True)\n    validation_df_0 = validation_df_0.append({'Validation Draw': int(i), 'Ball': \"Predicted\", 'Value': predicted_draw[0] }, ignore_index=True)\n    validation_df_1 = validation_df_1.append({'Validation Draw': int(i), 'Ball': \"Actual\", 'Value': actual_draw[1]}, ignore_index=True)\n    validation_df_1 = validation_df_1.append({'Validation Draw': int(i), 'Ball': \"Predicted\", 'Value': predicted_draw[1] }, ignore_index=True)\n    validation_df_3 = validation_df_3.append({'Validation Draw': int(i), 'Ball': \"Actual\", 'Value': actual_draw[3]}, ignore_index=True)\n    validation_df_3 = validation_df_3.append({'Validation Draw': int(i), 'Ball': \"Predicted\", 'Value': predicted_draw[3] }, ignore_index=True)\n    validation_df_6 = validation_df_6.append({'Validation Draw': int(i), 'Ball': \"Actual\", 'Value': actual_draw[6]}, ignore_index=True)\n    validation_df_6 = validation_df_6.append({'Validation Draw': int(i), 'Ball': \"Predicted\", 'Value': predicted_draw[6] }, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24, 8))\nplt.title(\"Ball 1: Actual vs Predicted\")\nax = sns.lineplot(x=\"Validation Draw\", y=\"Value\", hue=\"Ball\",style=\"Ball\",data=validation_df_0,markers=True, dashes=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24, 8))\nplt.title(\"Ball 4: Actual vs Predicted\")\nax = sns.lineplot(x=\"Validation Draw\", y=\"Value\", hue=\"Ball\",style=\"Ball\",data=validation_df_3,markers=True, dashes=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24, 8))\nplt.title(\"Extra Ball: Actual vs Predicted\")\nax = sns.lineplot(x=\"Validation Draw\", y=\"Value\", hue=\"Ball\",style=\"Ball\",data=validation_df_6,markers=True, dashes=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Visualization\nWe'll also take a look at the training, to visualize the overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrained_df = pd.DataFrame(columns = ['Validation Draw','Ball','Value'])\ntrained_df[\"Validation Draw\"] = pd.to_numeric(validation_df_0[\"Validation Draw\"])\ntrained_df[\"Value\"] = pd.to_numeric(trained_df[\"Value\"])\n\nfor i in range(0, int((lottery_ml.values.shape[0]-rows_to_retain_for_test)/2)):\n    to_predict = lottery_ml.iloc[i:i+games_window_size].values.tolist()\n    scaled_to_predict = scaler.transform(to_predict)\n    scaled_predicted_output_1 = model.predict(np.array([scaled_to_predict]))\n    predicted_draw = scaler.inverse_transform(scaled_predicted_output_1).astype(int)[0]\n    actual_draw = lottery_ml.iloc[i+games_window_size].values.tolist()\n    trained_df = trained_df.append({'Validation Draw': int(i), 'Ball': \"Actual\", 'Value': actual_draw[0]}, ignore_index=True)\n    trained_df = trained_df.append({'Validation Draw': int(i), 'Ball': \"Predicted\", 'Value': predicted_draw[0] }, ignore_index=True)\n    \nfig, ax = plt.subplots(figsize=(24, 8))\nplt.title(\"Ball 1: Plot of prediction over trained data\")\nax = sns.lineplot(x=\"Validation Draw\", y=\"Value\", hue=\"Ball\",style=\"Ball\",data=trained_df,markers=False, dashes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis on Method 1\n\nSo we already see that model is overfit. However, one can make the statement that the trend can have some similarities on different draws.\nIn all cases, that's not really conclusive and there is not particular pattern.\n\n## Next step\nSo, in order to solve the latter we might need to infer new features. To do that, we will use a CNN + LSTM architecture.  \nMore technically, we're shifting our domain from only temporal to spatio - temporal.  \nWe're not relying anymore on time based draws only. Instead, we're trying to infer some spatial features about the data that might affect the next batch of balls being drawn.  \n"},{"metadata":{},"cell_type":"markdown","source":"# Method 2\n\nThis method makes use of a CNN + LSTM architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nfrom numpy import split\nfrom numpy import array\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers import ConvLSTM2D, Conv1D, MaxPooling1D\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the dataset\n\nSo phase 1 is to prepare the dataset.  \nThe inputs of the model have to be reshaped.  \nThe train and test datasets are divided similarly to the previous method. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_datasets(lottery_ml_normalized):\n    train = np.empty([number_of_rows,games_window_size,number_of_features], dtype=int)\n    label = np.empty([number_of_rows,number_of_features], dtype=int)\n    test = np.empty([rows_to_retain_for_test,games_window_size,number_of_features], dtype=int)\n    \n    for i in range(0, number_of_rows):\n        train[i]=lottery_ml_normalized.iloc[i:i+games_window_size, 0: number_of_features]\n        label[i]=lottery_ml_normalized.iloc[i+games_window_size: i+games_window_size + 1, 0: number_of_features]\n\n    for i in range(number_of_rows, number_of_rows + rows_to_retain_for_test - games_window_size):\n        test[i-number_of_rows]=lottery_ml_normalized.iloc[i:i+games_window_size, 0: number_of_features]\n\n    # current shape is [samples, timesteps, features]\n    # reshape into [samples, subsequences, timesteps, features]\n\n    # subsequences means the number of rows every single data represents.\n    \n    # In our case it is 1 subsequence,then the whole draw as treated as a single a row (that means the 6 balls)\n    train = train.reshape([number_of_rows, 1, games_window_size, number_of_features])\n    test = test.reshape([rows_to_retain_for_test, 1, games_window_size, number_of_features])\n    \n    return train, label, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building & Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(train, label, n_epochs, val_ratio):\n    \n    model = Sequential()\n    # input (batch, steps, channels)\n    model.add(TimeDistributed(Conv1D(filters=3, kernel_size=16, activation='relu'), input_shape=(None, games_window_size, number_of_features)))\n    model.add(TimeDistributed(Conv1D(filters=3, kernel_size=32, activation='relu')))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(512, activation='relu',return_sequences=True))\n    model.add(Dropout(0.5))\n    model.add(LSTM(512, activation='relu'))\n    model.add(Dropout(0.3))\n    #model.add(Flatten())\n    #model.add(LSTM(10, activation='relu'))\n    model.add(Dense(7))\n\n    model.compile(optimizer='adam', loss='mse', metrics=[\"accuracy\"])\n\n    history = model.fit(train, label,validation_split=val_ratio, epochs=n_epochs, verbose=0)\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, label, test = prepare_datasets(lottery_ml_normalized)\nhistory = build_model(train, label, 2000, 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Overfit\n\nThe model is also overfit, no real significant changes to our predictions.\nIt seems that even spatial data have no correlation with the resulting draw."},{"metadata":{},"cell_type":"markdown","source":"# Testing on new data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As for the testing, we'll plot a confusion matrix\ntest_pred = pd.DataFrame(columns = [\"Predicted\", \"Actual\", \"Occurences\"])\n\ntest_pred[\"Predicted\"] = pd.to_numeric(test_pred[\"Predicted\"])\ntest_pred[\"Actual\"] = pd.to_numeric(test_pred[\"Actual\"])\ntest_pred[\"Occurences\"] = pd.to_numeric(test_pred[\"Occurences\"])\n\n\noccurences = {}\n#for test_in in test:\nfor i in range(0,test.shape[0]):\n        prediction_normalized = model.predict(test[i], verbose = 0)\n        pred = scaler.inverse_transform(prediction_normalized).astype(int)[0]\n        actual = lottery_ml.iloc[i+rows_to_retain_for_test]\n        for nb in range(0, 7):\n            occurences[str(pred[nb]) + \"\" + str(actual[nb])]  = occurences.get(str(pred[nb]) + \"\" + str(actual[nb]), 0) + 1 \n            if(occurences[str(pred[nb]) + \"\" + str(actual[nb])] == 1):\n                test_pred = test_pred.append({'Predicted': pred[nb], 'Actual': actual[nb], 'Occurences': occurences[str(pred[nb]) + \"\" + str(actual[nb])] }, ignore_index=True)\n            else:\n                test_pred.loc[(test_pred[\"Predicted\"] == pred[nb]) & (test_pred[\"Actual\"] == actual[nb]), \"Occurences\"] = occurences.get(str(pred[nb]) + \"\" + str(actual[nb]), 0) + 1\n                #print(ok)\n        #print(pred)\n        #print(actual)\n        #break\nprint(test_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_hm = test_pred.pivot(\"Actual\", \"Predicted\", \"Occurences\")\nf, ax = plt.subplots(figsize=(15, 15))\nplt.title(\"Occurences of balls with their predicted result\")\nsns.heatmap(test_pred_hm, annot=True,linewidths=0.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\nFirst thing you can see that the plot is pretty scattered.  \nIn simple terms, for each actual ball drawn of the heat map, we count the number of occurences of what the model predicted for it.  \nTheoretically if the model was accurate Ball #32 should only match with ball #32  \nHowever, through the testing dataset, 32 was matched once with -13 (which does not exist), once with -1, 3 times with 11, 3 times with 32, etc..  \nThat means that our model performs really poorly also on the testing dataset"},{"metadata":{},"cell_type":"markdown","source":"# Final Method that we'll try (Most Promising)\n\nLet's take in consideration the results that we have found throughout our statistics.  \nWith all the models that we have worked on so far, none of the inputs that we used took in consideration extra features.  In other terms, our input for the learning agent was only the ball sequence.  \nIn this method, we'll try to mix up some stuff.  \n\n## Trial 1\nSince we know that the probability of picking a sequence from the winning draw where the sum of the balls is between 100 - 158 is 70%, then we'll try to integrate this feature in to our model in the hope of increasing the chances of picking a right sequence.\n\n## Trial 2\nWe'll evaluate the feature of having 3 odds and 3 evens in our predictions.\n\n\n**N.B:** We will not take other features into consideration for the reason provided in the section of **DO NOT PICK ALL THREE METHODS TOGETHER!**  \n  \n  \nFor the sake of simplicity I'll use the same model architecture as the previous method "},{"metadata":{},"cell_type":"markdown","source":"## Preparing the dataset\n\nThe most important part here is to add the new feature into consideration.  \n\n### Adding the missing feature\nOur input data looks like this:  \nBall 1 --> Extra Ball.\nThe dilemna in this case is that we are not able to add an extra feature quantitavely because it doesn't make sense.  \nFor instance, adding a column which holds the SUM of the numbers would typically do nothing since we're training our model to have a prediction of the next draw.  \nSo knowing that, two consecutive draws might have different or same sums, leading the network to learn nothing.  \n\nInstead, and for the sake of simplicity we will **only train on the data we want to predict**. (Yes, there exists a better combination of what I am showing)  \nThat means that our training set will be only formed by the predictions we want it to make.  \nIn a way, we're only feeding our neural network with the pattern we want it to learn"},{"metadata":{},"cell_type":"markdown","source":"## Trial 1\n\nWe will be only training on draws where the sum of the first 6 numbers is between 100 - 158"},{"metadata":{"trusted":true},"cell_type":"code","source":"lottery_ml_sum = copy.deepcopy(lottery_ml)\nlottery_ml_sum['SUM'] = 0\nlottery_ml_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter the training set \nKeep only the rows we want"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sumBalls(balls):\n    sumCurrent = 0\n    for ball in balls:\n        sumCurrent += ball\n    return sumCurrent\n\nindexsToDrop = []\ncurrentBalls = []\nfor index, draw in lottery_ml_sum.iterrows():\n    currentBalls.append(draw['Ball 1'])\n    currentBalls.append(draw['Ball 2'])\n    currentBalls.append(draw['Ball 3'])\n    currentBalls.append(draw['Ball 4'])\n    currentBalls.append(draw['Ball 5'])\n    currentBalls.append(draw['Ball 6'])\n    sumCurrentBalls = sumBalls(currentBalls)\n    if( sumCurrentBalls >= 100 and  sumCurrentBalls <= 158):\n        draw['SUM'] = 100\n    else:\n        draw['SUM'] = 30\n        indexsToDrop.append(index)  \n    currentBalls = []\n\nlottery_ml_sum = lottery_ml_sum.drop(indexsToDrop)\n\ndel lottery_ml_sum['SUM']\nlottery_ml_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_to_retain_for_test = 350\nnumber_of_rows= lottery_ml_sum.values.shape[0] - rows_to_retain_for_test\ngames_window_size = 50 #amount of past games we need to take in consideration for training (It's also the number of draws)\nnumber_of_features = lottery_ml_sum.values.shape[1] #balls count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing and downscaling the data such that mean per column is 0\nscaler = StandardScaler().fit(lottery_ml_sum.values)\ntransformed_dataset = scaler.transform(lottery_ml_sum.values)\nlottery_ml_normalized = pd.DataFrame(data=transformed_dataset, index=lottery_ml_sum.index)\nnumber_of_features = lottery_ml_sum.values.shape[1] #balls count\nlottery_ml_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, label, test = prepare_datasets(lottery_ml_normalized)\nhistory = build_model(train, label, 500, 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis\n\nThe results didn't really improve on the previous findings, that means that the trained combination wasn't able to crack the lottery. **BUT** it was representative of a big portion of how the output could look like (I will elaborate on this more in the analysis of the following trial)"},{"metadata":{},"cell_type":"markdown","source":"## Trial 2\n\nWe will be only training on draws where we have 3 even balls and 3 odd balls out of the first 6 balls.  \nSame steps as Trial 1 with minor tweeks."},{"metadata":{"trusted":true},"cell_type":"code","source":"lottery_ml_even = copy.deepcopy(lottery_ml)\nlottery_ml_even['EVEN'] = 0\nlottery_ml_even","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evenBalls(balls):\n    numEven = 0\n    for ball in balls:\n        if(ball % 2 == 0):\n            numEven += 1\n    return numEven\n\nindexsToDrop = []\ncurrentBalls = []\nfor index, draw in lottery_ml_even.iterrows():\n    currentBalls.append(draw['Ball 1'])\n    currentBalls.append(draw['Ball 2'])\n    currentBalls.append(draw['Ball 3'])\n    currentBalls.append(draw['Ball 4'])\n    currentBalls.append(draw['Ball 5'])\n    currentBalls.append(draw['Ball 6'])\n    numEven = evenBalls(currentBalls)\n    if( numEven == 3):\n        draw['EVEN'] = numEven\n    else:\n        draw['EVEN'] = numEven\n        indexsToDrop.append(index)  \n    currentBalls = []\n\nlottery_ml_even = lottery_ml_even.drop(indexsToDrop)\n\ndel lottery_ml_even['EVEN']\nlottery_ml_even","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_to_retain_for_test = 350\nnumber_of_rows= lottery_ml_even.values.shape[0] - rows_to_retain_for_test\ngames_window_size = 50 #amount of past games we need to take in consideration for training (It's also the number of draws)\nnumber_of_features = lottery_ml_even.values.shape[1] #balls count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing and downscaling the data such that mean per column is 0\nscaler = StandardScaler().fit(lottery_ml_even.values)\ntransformed_dataset = scaler.transform(lottery_ml_even.values)\nlottery_ml_normalized = pd.DataFrame(data=transformed_dataset, index=lottery_ml_even.index)\nnumber_of_features = lottery_ml_even.values.shape[1] #balls count\nlottery_ml_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, label, test = prepare_datasets(lottery_ml_normalized)\nhistory = build_model(train, label, 500, 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis\n\nIn this case, the results worsened a lot.  \nA lot of hypothesis could be introduced here, but we can for sure elaborate that this combination is not the optimal one to always base our assumptions on.\nThat means that following the previous trial, it is better to follow the latter combination rather to follow this one.\nIn the same taste, you would also predict that using HIGH - LOW combination would lead to the same result, but I will keep this one for you to implement."},{"metadata":{},"cell_type":"markdown","source":"# Closing Comments\n\nI wont take long, this post was already filled with conclusive information.  \nDefinitely the lottery is not rigged (at least that's what this data shows).  \nI hope I have provided you with good insights about Lotteries, time series (at least an intro) and machine learning in general.  \nPersonally, I don't play the lotto and I plan on keeping it that way.\n> *That's all folks!*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}