{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hurricans and Typhoons Classification.\n\n## Objectives:\n1. Clean the data.\n2. Statistical Analysis of data.\n - *Find Top ten hurricanes by frequency.*\n - *Find frequency of hurricanes by month.*\n - *Find frequency of hurricanes by year.*\n - *Find frequency of hurricanes by category.*\n3. Classification into Hurricanes or Typhoons using Logistic Regression, Decision Tree, Random Forrest , Naive Bayes and SVM.\n - *Perform Feature selection using Random Forest.* \n - *Compare the prediction by Decision Tree Model performance using all the features and top five features.* \n - *Find the prediction accuracy of Random Forest Model model using the top five features.* \n - *Compare the prediction by Naive Bayes Model performance using all the features and top five features.* \n - *Find the prediction accuracy of SVM model using the top five features.* \n - *Show which model has performed the best.* "},{"metadata":{},"cell_type":"markdown","source":"## Clean the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore Warnings.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Status can be the following types:\n# TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n# TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n# HU – Tropical cyclone of hurricane intensity (> 64 knots)\n# EX – Extratropical cyclone (of any intensity)\n# SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n# SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n# LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n# WV – Tropical Wave (of any intensity)\n# DB – Disturbance (of any intensity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Import the libraries.\n\n# Import pandas.\nimport pandas as pd\n# Import numpy.\nimport numpy as np\n# Import matplotlib.\nimport matplotlib.pyplot as plt\n# Import seaborn.\nimport seaborn as sns\n# Import regular expression.\nimport re\n# import datetime.\nimport datetime as dt\n# Import the data.\ndf = pd.read_csv('/kaggle/input/hurricane-database/pacific.csv')\n# Convert date column as datetime.\ndf['Date'] = pd.to_datetime(df['Date'] , format= '%Y%m%d')\n\n# I want to create columns Latitude Hemisphere and Longitude Hemisphere with code 0 = N , 1 = S & 0 = E , 1 = W.\ndef hemisphere(coord):\n        hem = re.findall(r'[NSWE]' , coord)[0]\n        if hem == 'N' or hem == 'E':\n            return 0\n        else:\n            return 1\n\n# Creating the column Latitude_Hemisphere.    \ndf['Latitude_Hemisphere'] = df['Latitude'].apply(hemisphere)\ndf['Longitude_Hemisphere'] = df['Longitude'].apply(hemisphere)\ndf['Latitude_Hemisphere'] = df['Latitude_Hemisphere'].astype('category')\ndf['Longitude_Hemisphere'] = df['Longitude_Hemisphere'].astype('category')\n\n# Convert the latitude and longitude Column to numeric type.\ndf['Latitude'] =  df['Latitude'].apply(lambda x: re.match('[0-9]{1,3}.[0-9]{0,1}' , x)[0])\ndf['Longitude'] =   df['Longitude'].apply(lambda x: re.match('[0-9]{1,3}.[0-9]{0,1}' , x)[0])\n\n# The missing values are given by -999. So , we need to fill them appropriately.\n\n# Show the count of missing values and fill them with mean.\nfor column in df.columns:\n    missing_cnt = df[column][df[column] == -999].count()\n    print('Missing Values in column {col} = '.format(col = column) , missing_cnt )\n    if missing_cnt!= 0:\n#         print('in ' , column)\n        mean = round(df[column][df[column] != -999 ].mean())\n#         print(\"mean\",mean)\n        index = df.loc[df[column] == -999 , column].index\n#         print(\"index\" , index )\n        df.loc[df[column] == -999 , column] = mean\n#         print(df.loc[index , column])\n        \n# Restructure the dataframe for visibility and remove columns ID and Event.        \ndf =  df[['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Latitude_Hemisphere' , \n       'Longitude', 'Longitude_Hemisphere' ,'Maximum Wind', 'Minimum Pressure', 'Low Wind NE',\n       'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE',\n       'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW',\n       'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW']]\n\n# Change all time to format HHMM.\ndf['Time'] = df['Time'].astype('object')\ndef hhmm(time):\n    time = str(time)\n    digits = re.findall(r'\\d', time)\n    t = ''\n    if len(digits) == 1:\n        t ='0{i}00'.format(i =time)\n    elif len(digits) == 2:\n        t = '{i}00'.format(i =time)\n    elif len(digits) == 3:\n        t = '0{i}'.format(i =time)\n    else:\n        t = time\n    return t\n# Apply the function.\ndf['Time'] = df['Time'].apply(hhmm)\n\n# Convert the column into Datetime.\ndf['Time'] = pd.to_datetime(df['Time'] , format='%H%M').dt.time\n\n\n# Convert the status column to categorical.\ndf['Status'] = df['Status'].astype('category')\n\ndata = df.drop(columns = ['ID' , 'Event'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statististical Analysis of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the data.\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top ten cyclones which occured the maximum number of times."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the top ten cyclones which have occured the maximum number of times.\nlst = [x.strip() for x in data.groupby('Name').count().sort_values(by = 'Date' , ascending = False).index[:10]]\nval = data.groupby('Name').count().sort_values(by = 'Date' , ascending = False)[:10]['Date'].values\nfont = {'family' : 'monospace',\n        'weight' : 'bold',\n        'size'   : 22}\nplt.rc('font', **font)\nfig , ax = plt.subplots()\nfig.set_size_inches(12,12)\nax.pie(  labels = lst , x = val , autopct='%.1f%%' , explode = [0.1 for x in range(10)])\nplt.title(' Top Ten Hurricanes by Frequency.' , fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency of Hurricanes by Month."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Month'] = data['Date'].apply(lambda x: x.month)\ndata['Year'] = data['Date'].apply(lambda x: x.year)\nmnt = ['Jan' , 'Feb' , 'Mar' , 'Apr' , 'May' , 'June' , 'July' , 'Aug' , 'Sep','Oct' , 'Nov' , 'Dec']\ntemp = data.groupby('Month').count()\ntemp.loc[4] = 0\ntemp = temp.sort_values(by = 'Month' , ascending = False)\nfont = {'family' : 'monospace',\n        'weight' : 'bold',\n        'size'   : 22}\nplt.rc('font', **font)\nplt.figure(figsize = (10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x = temp.index , y = 'Date' , data=temp , palette = 'RdBu' )\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11] , mnt , rotation = 90)\nplt.ylabel('Frequency')\nplt.title('Frequency of Cyclones by Month.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Year Wise Frequency of Hurricanes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Year-Wise Frequency of Hurricanes.\ntemp = data.groupby('Year').count().sort_values(by = 'Month' , ascending = False)\nplt.figure(figsize= (12,12))\nsns.lineplot(x = temp.index , y = 'Month' , data = temp , label = 'Frequency')\nplt.ylabel('Frequency')\nplt.title('Year Wise Frequency of Hurricanes.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Probability Distribution Function of Frequency.\ntemp = data.groupby('Year').count().sort_values(by = 'Date' , ascending = False)\nplt.figure(figsize=(15,15))\nsns.distplot(temp['Date'].values , norm_hist = True , axlabel = 'Probability Distribution of Frequency of Cyclones.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency of Cyclones by category."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Frequency of Cyclones by Category\n# TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n# TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n# HU – Tropical cyclone of hurricane intensity (> 64 knots)\n# EX – Extratropical cyclone (of any intensity)\n# SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n# SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n# LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n# WV – Tropical Wave (of any intensity)\n# DB – Disturbance (of any intensity)\ntemp = data.groupby('Status').count().sort_values(by = 'Date' , ascending = False)\nfig , ax = plt.subplots()\nfig.set_size_inches(12,12)\nsns.barplot(y = list(temp.index) , x = 'Date' , data = temp, palette= 'pastel' )\nplt.xlabel('Frequency')\nplt.ylabel('Catehory')\nplt.title('Category wise Frequency Distribution of Cyclones.')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Classification model. "},{"metadata":{},"cell_type":"markdown","source":"## 1. Decision Tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Decision Tree Classifier.\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import train-test split.\nfrom sklearn.model_selection import train_test_split\n\n# Import accuracy Score.\nfrom sklearn.metrics import accuracy_score\n\n#Import Recall Score.\nfrom sklearn.metrics import recall_score \n\n#Import Precision Score.\nfrom sklearn.metrics import precision_score \n\n# Form the model.\ndt = DecisionTreeClassifier(min_samples_leaf=50 , criterion='entropy')\n\n\n# Set the dependent and independent variables.\nx_train = data[['Latitude', 'Latitude_Hemisphere',\n       'Longitude', 'Longitude_Hemisphere', 'Maximum Wind', 'Minimum Pressure',\n       'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW',\n       'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW',\n       'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW',\n       'High Wind NW' , 'Month' , 'Year']]\ny_train = data['Status']\n\n\n# Perform the Kfold validation.\n\n# Import the KFold library.\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10 , shuffle= True , random_state=42 )\n\ndt_scores = []\ndt_recall_scores = []\ndt_precision_scores = []\nfor tr , ts in kf.split(x_train):\n    xtr = x_train.loc[tr]\n    ytr = y_train.loc[tr]\n    xts = x_train.loc[ts]\n    yts = y_train.loc[ts]\n    dt.fit(xtr , ytr)\n    y_pred = dt.predict(xts) \n    dt_scores.append(accuracy_score(yts, y_pred)) \n    dt_recall_scores.append(recall_score(yts , y_pred , average = 'weighted'))\n    dt_precision_scores.append(precision_score(yts , y_pred , average = 'weighted'))\n# dt.fit(x_train, y_train)\n# y_pred = dt.predict(x_test)\n# accuracy_score(y_test, y_pred)\ndt_scr = {'accuracy' : np.mean(dt_scores) , 'recall': np.mean(dt_recall_scores) , 'precision' :  np.mean(dt_precision_scores) }\nprint('Accuracy score for Decision Tree is :' , dt_scr['accuracy'])\nprint('Recall score for Decision Tree is :' , dt_scr['recall'])\nprint('Precision score for Decision Tree is :' , dt_scr['precision'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# First I want to determine the important features.\nrf = RandomForestClassifier(oob_score=True , n_estimators=1000)\nrf.fit(x_train , y_train)\nfeatures = pd.Series(rf.feature_importances_ , index= x_train.columns).sort_values(ascending=False)\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a decision tree for top ten most important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top ten most important features.\nfeatures.index[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the dependent and independent variables.\nx_trainf = data[features.index[:5]]\ny_train = data['Status']\n\n# Perform the Kfold validation.\n\n# Import the KFold library.\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10 , shuffle= True , random_state=42 )\n\ndt_scores = []\ndt_recall_scores = []\ndt_precision_scores = []\nfor tr , ts in kf.split(x_trainf):\n    xtr = x_trainf.loc[tr]\n    ytr = y_train.loc[tr]\n    xts = x_trainf.loc[ts]\n    yts = y_train.loc[ts]\n    dt.fit(xtr , ytr)\n    y_pred = dt.predict(xts) \n    dt_scores.append(accuracy_score(yts, y_pred)) \n    dt_recall_scores.append(recall_score(yts , y_pred , average = 'weighted'))\n    dt_precision_scores.append(precision_score(yts , y_pred , average = 'weighted'))\n# dt.fit(x_train, y_train)\n# y_pred = dt.predict(x_test)\n# accuracy_score(y_test, y_pred)\ndt_scr5 = {'accuracy' : np.mean(dt_scores) , 'recall': np.mean(dt_recall_scores) , 'precision' :  np.mean(dt_precision_scores) }\nprint('Accuracy score for Decision Tree is :' , dt_scr['accuracy'])\nprint('Recall score for Decision Tree is :' , dt_scr['recall'])\nprint('Precision score for Decision Tree is :' , dt_scr['precision'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As we can see the Top five features('Maximum Wind', 'Minimum Pressure', 'Latitude', 'Year', 'Longitude') give the same accuracy as when we get choosing all the features."},{"metadata":{},"cell_type":"markdown","source":"## 2 . Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here instead of cross validation we will be using oob score as a measure of accuracy.\n# I will hyper tuning the parameter: No of Trees.\n\ntrees  = [10, 20 , 50, 100,200,500,1000,1200]\nmaxn_five = {}\nmaxn = {}\nfor i in trees:\n    rf = RandomForestClassifier(n_estimators=i , oob_score=True)\n    rf.fit(x_trainf , y_train)\n    print('Obb Score for {x} trees: and taking top five features '.format(x = i) , rf.oob_score_)\n    maxn_five[i] = rf.oob_score_\n    rf.fit(x_trainf , y_train)\n    print('Obb Score for {x} trees: and taking all the features '.format(x = i) , rf.oob_score_)\n    maxn[i] = rf.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into training and testing.\nx_trains , x_tests , y_trains, y_tests  = train_test_split(x_trainf, y_train, test_size=0.33, random_state=42)\n# Set n to the feature of maximum oob score.\nn = 0\nfor i in maxn_five:\n    if max(maxn_five.values()) == maxn_five[i]:\n        n= i\n# Set n_estimators to n.\nrf = RandomForestClassifier(oob_score=True , n_estimators=n)\nrf.fit(x_trains , y_trains)\ny_pred_rf = rf.predict(x_tests[features.index[:5]])\nscores_rf = {'accuracy': accuracy_score(y_tests , y_pred_rf) ,'recall' : recall_score(y_tests , y_pred_rf , average='weighted') ,'precision' : precision_score(y_tests , y_pred_rf , average='weighted') }\nprint('Scores for Random Forest with n = ' , n , ' and using features ',  features.index[:5] , ' are : ')\nprint('Accuracy: ' , scores_rf['accuracy'])\nprint('Recall: ' , scores_rf['recall'])\nprint('Precision: ' , scores_rf['precision'])\n\n# n_All = 0\n# for i in maxn:\n#     if max(maxn.values()) == maxn[i]:\n#         n_All= i\n# # Set n_estimators to n.\n# rf = RandomForestClassifier(oob_score=True , n_estimators=n_All)\n# rf.fit(x_train , y_train)\n# y_pred_rf_all = rf.predict(x_test)\n# scores_rf_all = {'accuracy': accuracy_score(y_test , y_pred_rf) ,'recall' : recall_score(y_test , y_pred_rf , average='weighted') ,'precision' : precision_score(y_test , y_pred_rf , average='weighted') }\n# print('Scores for Random Forest with n = ' , n_All , ' and using all features ' , ' are : ')\n# print('Accuracy: ' , scores_rf_all['accuracy'])\n# print('Recall: ' , scores_rf_all['recall'])\n# print('Precision: ' , scores_rf_all['precision'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Naive Bayes Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb5 = GaussianNB()\nacc_s = [] \nrcl_s = [] \nps_scr = []\nacc_s_5 = [] \nrcl_s_5 = [] \nps_scr_5 = []\nfor tr, ts in kf.split(x_train):\n    xtr = x_train.loc[tr]\n    ytr = y_train.loc[tr]\n    xts = x_train.loc[ts]\n    yts = y_train.loc[ts]\n    xtr5 = x_trainf.loc[tr]\n    xts5 = x_trainf.loc[ts]\n\n    \n    # Accuracy , Precision and recall with all features.\n   \n    nb.fit(xtr , ytr)\n    y_nb_pred = nb.predict(xts)\n    acc_s.append(accuracy_score(yts , y_nb_pred))\n    rcl_s.append(recall_score(yts , y_nb_pred , average = 'weighted'))\n    ps_scr.append(precision_score(yts , y_nb_pred , average = 'weighted'))\n    \n#     Accuracy , Precision and recall with top five features.\n    nb5.fit(xtr5 , ytr)\n    y_nb5_pred = nb5.predict(xts5)\n    acc_s_5.append(accuracy_score(yts , y_nb5_pred))\n    rcl_s_5.append(recall_score(yts , y_nb5_pred , average = 'weighted'))\n    ps_scr_5.append(precision_score(yts , y_nb5_pred , average = 'weighted'))\n    \nnb_scores = {'accuracy':np.mean(acc_s) , 'recall':np.mean(rcl_s) , 'precision':np.mean(ps_scr)}\nnb5_scores = {'accuracy':np.mean(acc_s_5) , 'recall':np.mean(rcl_s_5) , 'precision':np.mean(ps_scr_5)}\nprint('Naive Bayes results for top five features for Accuracy ' , nb5_scores['accuracy'] , 'Recall: ' , nb5_scores['recall'], 'and Precision: ' , nb5_scores['precision'] )\nprint('Naive Bayes results for all features for Accuracy ' , nb_scores['accuracy'] , 'Recall: ' , nb_scores['recall'], 'and Precision: ' , nb_scores['precision'] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the overall score with top five features is significantly greater than the overall score with all the features. Hence , we can see that feature selection is very important for Naive Bayes."},{"metadata":{},"cell_type":"markdown","source":"## 4. Support Vector Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import SVM.\nfrom sklearn import svm\nmdl5 = svm.SVC()\nacc_s_5 = [] \nrcl_s_5 = [] \nps_scr_5 = []\n\n# Split the data into train and test.\nxtr5, xts5 , ytr , yts = train_test_split(x_trainf , y_train , test_size = 0.25 , random_state = 42)\n\n# Train the model.\nmdl5.fit(xtr5 , ytr)\ny_mdl5_pred = nb5.predict(xts5)\nacc_s_5.append(accuracy_score(yts , y_mdl5_pred))\nrcl_s_5.append(recall_score(yts , y_mdl5_pred , average = 'weighted'))\nps_scr_5.append(precision_score(yts , y_mdl5_pred , average = 'weighted'))\n\n# for tr, ts in kf.split(x_train):\n#     ytr = y_train.loc[tr]\n#     yts = y_train.loc[ts]\n#     xtr5 = x_trainf.loc[tr]\n#     xts5 = x_trainf.loc[ts]\n\n# #   Accuracy , Precision and recall with top five features.\n#     mdl5.fit(xtr5 , ytr)\n#     y_mdl5_pred = nb5.predict(xts5)\n#     acc_s_5.append(accuracy_score(yts , y_mdl5_pred))\n#     rcl_s_5.append(recall_score(yts , y_mdl5_pred , average = 'weighted'))\n#     ps_scr_5.append(precision_score(yts , y_mdl5_pred , average = 'weighted'))\n    \nsvm_scores = {'accuracy':np.mean(acc_s_5) , 'recall':np.mean(rcl_s_5) , 'precision':np.mean(ps_scr_5)}\nprint('SVM results for top five features for Accuracy ' , svm_scores['accuracy'] , 'Recall: ' , svm_scores['recall'], 'and Precision: ' , svm_scores['precision'] )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the overall score with top five features is significantly greater than the overall score with all the features. Hence , we can see that feature selection is very important for SVM."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Comparing the algorithms.\nres = {'DecisionTree':dt_scr5['accuracy'] , 'RandomForest': scores_rf['accuracy'] , 'GaussianNB': nb5_scores['accuracy'] , 'SVM':svm_scores['accuracy']}\nmax_res = max(res.values())\nmax_index = ''\nfor i in res:\n    if res[i] == max_res:\n        max_index = i\nprint('The most effictive algorithm is :' , max_index , 'with accuracy: ' , res[max_index])        \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}