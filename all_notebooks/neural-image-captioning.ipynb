{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction üëãüèª"},{"metadata":{},"cell_type":"markdown","source":"This notebook aims to reproduce the paper [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555v2.pdf) by [Vinyals](vinyals@google.com) et al. \n\nGenerating a description of an image is called **image captioning** , but it's not that simple. A description must capture not only the objects contained in an image, but it also must express how these objects relate to each other as well as their attributes and the activities they are involved in. Moreover, the above semantic knowledge has to be expressed in a natural language like English, which means that a language model is needed in addition to visual understanding.\n\nThe authors propose a single joint model that takes an image $I$ as input, and is trained to maximize the likelihood $p(S|I)$ of producing a target sequence of words $S = {S_1, S_2, . . .}$ where each word $S_t$ comes from a given dictionary, that describes the image adequately. They replace the \"*encoder*\" in a vanilla RNN with a CNN to transform the image into a fixed length vector representations which are then fed as input to the RNN decoder that generates sentences."},{"metadata":{},"cell_type":"markdown","source":"The following hidden cell contains basic imports, random seeds, tokenizer instantiation and weightsandbiases login.\n\n## Packages\n\n* [torch](https://pytorch.org/docs/stable/torch.html): The deep learning framework we'll use in this kernel\n* [pandas](https://pandas.pydata.org/): To pre-process input data which is later converted into a PyTorch `Dataset` instance\n* [transformers](https://github.com/huggingface/transformers): We'll use a pre-trained `BertTokenizer` for tokenising our captions. We could have also used `BPE`. \n* [torchvision](https://pytorch.org/docs/stable/torchvision/index.html): We'll use pre-trained `resnet50` from torchvision.models and torchvision.transforms\n* [sklearn](https://scikit-learn.org/stable/): For splitting our raw dataset into train, valid and test splits.\n* [PIL](https://pillow.readthedocs.io/en/stable/): For handling images with PyTorch"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%capture\n!pip install --upgrade wandb\n\n## Importing Packages\n\nimport os\nimport torch\nimport random\nimport warnings\nimport numpy as np\nimport transformers\nimport pandas as pd \nfrom PIL import Image\nimport torch.nn as nn\nwarnings.filterwarnings(\"ignore\")\nimport torchvision.transforms as T\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom typing import Callable, Optional\n\n## Logging into Weights and Biases\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nimport wandb\nwandb.login(key=api_key);\n\nwandb.init(project=\"show-and-tell\", entity=\"collaborativeml\")\n\n## For Reproducibility\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)\n\n## Tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n\n## Device Configuration \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Basic File Paths\ndata_dir = '../input/flickr-image-dataset/flickr30k_images'\nimage_dir = f'{data_dir}/flickr30k_images'\ncsv_file = f'{data_dir}/results.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üßπ Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"## ‚úçÔ∏è Some Hardcoding"},{"metadata":{},"cell_type":"markdown","source":"As pointed out by [@aritrag](https://www.kaggle.com/aritrag), The entry at index 19999 was messed up. Therefore, we'll hardcode the value at that particular instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(csv_file, delimiter='|')\ndf[' comment_number'][19999] = ' 4'\ndf[' comment'][19999] = ' A dog runs across the grass .'\ndf['image_name'] = image_dir+'/'+df['image_name']\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## üèõ Restructuring Data"},{"metadata":{},"cell_type":"markdown","source":"In the following code block we create the following structure:\n\n| image_name | comment_0 | comment_1 | comment_2 | comment_3 | comment_4 |\n|------------|-----------|-----------|-----------|-----------|-----------|\n|            |           |           |           |           |           |"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_name = {\n    'image_name':df[df[' comment_number'] == df[' comment_number'][0]]['image_name'].values,\n}\ncomments = {\n    'comment_0':df[df[' comment_number'] == df[' comment_number'][0]][' comment'].values,\n    'comment_1':df[df[' comment_number'] == df[' comment_number'][1]][' comment'].values,\n    'comment_2':df[df[' comment_number'] == df[' comment_number'][2]][' comment'].values,\n    'comment_3':df[df[' comment_number'] == df[' comment_number'][3]][' comment'].values,\n    'comment_4':df[df[' comment_number'] == df[' comment_number'][4]][' comment'].values,\n}\n\nimage_name_df = pd.DataFrame.from_dict(image_name)\ncomments_df = pd.DataFrame.from_dict(comments)\n\ndf = pd.concat([image_name_df,comments_df], axis=1)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ‚úÇÔ∏è Splitting into Train, Valid and Split"},{"metadata":{},"cell_type":"markdown","source":"Split the data into train, validation and test splits using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from [`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). \n\nWe use `0.2` for train and test split and `0.25` for train and validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Obtain Train and Test Split \ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\n## Reset Indexes \ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\n## Obtain Train and Validation Split \ntrain, val = train_test_split(train, test_size=0.25, random_state=42)\n\n## Reset Indexes \ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)\n\n## Let's see how many entries we have\nprint(train.shape)\nprint(val.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üóÑ Dataset"},{"metadata":{},"cell_type":"markdown","source":"The following code cell aims to convert the Flickr dataset into a torch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) object. \n\nAll `Dataset` objects in pytorch represent a map from keys to data samples. We create a subclass which overwrites the `__getitem__()` and `__len__()` method. We also provide a option to perform augmentations on the image using `torchvision.transforms`\n\n---\n\nEach element of our dataset returns:\n\n* Image (single image)\n* Captions (list of 5 tokenized captions)\n\n\n---"},{"metadata":{},"cell_type":"markdown","source":"Here, we create the `FlickrDataset` class. \n\nWe inherit from the [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class which is a abstract class. While creating a subclass of `Dataset` one must overwrite two methods, `__getitem__()` and `__len__()` for it to work well with the [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n\nWe take as input our dataframe and a `bool` transforms. Feel free to edit the transforms and experiment. !! \n\nIn the `__getitem__()` method, we use `df.<column_name>.values[]` to get the image_id and then use `PIL` to open the image in `RGB` format. If the `transforms` bool is set to `True`, we apply the transforms.  We then extract the comments for our specified instance and then create a empty nested list. We then iterate over the comments and encode them into encodings using the `BertTokenizer`. Finally we convert the captions into a `torch.Tensor` and return a tuple of the form `(image, captions)`.\n\nIn the first version of this kernel, I made a empty nested loop which I iterated over while encoding using the `.encode()` function but that resulted in a output shape of `[5,500]` without batching. After going over [this](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/) blogpost I figured out how to encode the entire list, resulting in the desired shape of `[<batch_size>,100]`. Although for this tutorial we'll just use the first caption for each entry."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FlickrDataset(Dataset):\n    \n    def __init__(self, df, \n                 transforms: Optional[Callable] = None) -> None:\n        self.df = df\n        self.transforms = T.Compose([\n            T.ToTensor(),\n            T.Normalize(mean = [0.5], std = [0.5]),\n            T.Resize((256,256)),\n        ])\n        \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        \n        image_id = self.df.image_name.values[idx]\n        image = Image.open(image_id).convert('RGB')\n            \n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        comments = self.df[self.df.image_name == image_id].values.tolist()[0][1:][0] # Last zero is to obtain the first caption ONLY\n        encoded_inputs = tokenizer(comments,\n                                   return_token_type_ids = False, \n                                   return_attention_mask = False, \n                                   max_length = 100, \n                                   padding = \"max_length\",\n                                  return_tensors = \"pt\")\n        \n        sample = {\"image\":image.to(device),\"captions\": encoded_inputs[\"input_ids\"].flatten().to(device)}\n        \n        return sample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, our dataset has a odd number of instances, we can't have perfect splits into batches. Thus, we have to use the `drop_last` parameter inorder to avoid any errors while training. [This](https://discuss.pytorch.org/t/runtimeerror-expected-hidden-0-size-2-20-256-got-2-50-256/38288/10) discuss post has a nice introduction to this problem.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n\ntrain_dataset = FlickrDataset(train, transforms = True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, drop_last=True)\n\nval_dataset = FlickrDataset(val, transforms = True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size,drop_last=True)\n\ntest_dataset = FlickrDataset(test, transforms = True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size,drop_last=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üõ† The Model (NIC)"},{"metadata":{},"cell_type":"markdown","source":"## üëÅ CNN Encoder (Show)"},{"metadata":{},"cell_type":"markdown","source":"We'll use a `resnet50` backbone to serve as the encoder part of our model. We create a custom `CNN` class which inherits from the `nn.Module` class. We introduce a parameter `embed_size` and we'll add a fully-connected layer in the end specifying output dimensions = embed_size."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    \n    def __init__(self, embed_size):\n        super(CNN, self).__init__()\n        model = models.resnet50(pretrained=True)\n        for param in model.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(model.children())[:-1]\n        self.model = nn.Sequential(*modules)\n        self.embed = nn.Linear(model.fc.in_features, embed_size)\n        \n    def forward(self, image):\n        features = self.model(image)\n        features = features.view(features.size(0), -1)\n        features = self.embed(features)\n                \n        return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## üìö RNN Decoder"},{"metadata":{},"cell_type":"markdown","source":"We create a custom `RNN` class which inherits from the `nn.Module` class. \n\n* During the forward pass, we'll first create the intial hidden and cell states by creating a tuple of `autograd` variables. The hidden states are initialised to zeros array of shape `(1, <batch_size>, <hidden_size>)` and the cell state is the output from the last hidden layer of the CNN. [This](https://discuss.pytorch.org/t/tuple-object-has-no-attribute-size-in-lstm-but-not-in-rnn/90307) post was helpful in figuring out how to make this kind of system work\n\n* Then we use the `nn.Embedding` layer to convert the real captions into a simple lookup table that stores our embeddings.\n\n* We then pass the generated embeddings into our `nn.LSTM` layer using our previously initialized hidden states.\n\n* Lastly, we pass the output from our LSTM into a fully connected layer with output_dimensions = vocab_size,  and return this output"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, embedding_dim,vocab_size):\n        super(RNN, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        \n        self.embedding = nn.Embedding(num_embeddings = vocab_size,embedding_dim = embedding_dim)\n        \n        self.lstm = nn.LSTM(input_size=input_size,\n                            hidden_size=hidden_size,\n                            batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def init_hidden(self, features):\n        \n        return (torch.autograd.Variable(torch.zeros(1,32,512).to(device)), \n                torch.autograd.Variable(features.unsqueeze(0)).to(device))\n        \n    def forward(self, features, captions):\n        \n        state = self.init_hidden(features)\n        \n        embed = self.embedding(captions)\n                    \n        lstm_out, state = self.lstm(embed, state)\n                        \n        outputs = self.fc(lstm_out)\n        outputs = outputs.view(-1, self.vocab_size)\n        \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ‚û°Ô∏è Example Forward Pass"},{"metadata":{},"cell_type":"markdown","source":"Here, we extract a example batch from our `train_dataloader` and view the Shape Transformation of our images and captions."},{"metadata":{"trusted":true},"cell_type":"code","source":"example_batch = next(iter(train_dataloader))\n\nimage, captions = example_batch[\"image\"], example_batch[\"captions\"]\n\nencoder = CNN(embed_size = 512).to(device)\ndecoder = RNN(input_size = 512, hidden_size = 512, embedding_dim=512, vocab_size = 28881).to(device)\n\nfeatures = encoder(image)\nembed = decoder(features, captions)\n\nprint(\"Image Transformation: \", image.shape, \" --> \", features.shape)\nprint(\"Captions Transformation: \", captions.shape, \" --> \", embed.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üìñ Some Theory"},{"metadata":{},"cell_type":"markdown","source":"Our goal with this method is to maximize the probability of the correct description given an input image. \n\n$$\n\\theta^{*} = arg\\max_{\\theta} \\sum_{(I , S)} log \\, p(S | I ; \\theta)\n$$\n\nHere, \n\n* $\\theta$ -> Parameters of our model\n* $I$ -> Image\n* $S$ -> Sentence\n\nThe CNN just serves as a encoder which downsamples our image into a fixed-length vector representation. \n\nFor the RNN, \n\n$$\nx_{-1} = CNN(I)\n$$\n\n$$\nx_t = W_eS_t, \\, t \\in \\{ 0 ... N - 1 \\}\n$$\n\n$$\np_{t+1} = LSTM(x_t), \\, t \\in \\{ 0 ... N - 1\\}\n$$\n\n## Loss Function\n\nThe paper uses the negative log-likelihood of the correct word at each step: \n\n$$\nL (I,S) = - \\sum_{t=1}^{N} log p_t(S_t)\n$$\n\nThis loss function is minimized w.r.t all the parameters of our RNN decoder and the last fully connected layer of the CNN encoder. We use the `Adam` optimizer with a arbitrarily set learning_rate of `0.001`"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n\nvocab_size = 90000\nsteps_per_epoch = 19069 // 32\n\nencoder = CNN(embed_size = 512).to(device)\ndecoder = RNN(input_size = 512, hidden_size = 512, embedding_dim=512, vocab_size = vocab_size).to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\nparams = list(decoder.parameters()) + list(encoder.embed.parameters())\n\noptimizer = torch.optim.Adam(params, lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üèãÔ∏è Training"},{"metadata":{},"cell_type":"markdown","source":"We'll train the model**s** for 10 epochs, in the next update we'll perform hyperparameter optimization using [wandb sweeps](https://docs.wandb.ai/sweeps). I'll print the last metrics only in order to avoid a huge output window, In the next update I'll include links to the wandb dashboard used for this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(10):\n\n    for idx, sample in enumerate(train_dataloader):\n        \n        if idx > steps_per_epoch:\n            break\n        \n        image, captions = torch.tensor(sample['image']).to(device), torch.tensor(sample['captions']).to(device)\n        \n        # zero the parameter gradients\n        decoder.zero_grad()\n        encoder.zero_grad()\n        \n        # Forward pass\n        features = encoder(image)\n        outputs = decoder(features, captions)\n        \n        # Compute the Loss\n        loss = criterion(outputs.view(-1, vocab_size), \n                         captions.view(-1))\n        \n        # Backward pass.\n        loss.backward()\n        \n        # Update the parameters in the optimizer.\n        optimizer.step()\n            \n        # Get training statistics.\n        stats = 'Epoch [%d], Loss: %.4f' % (epoch, loss.item())\n        wandb.log({\"Loss\": loss.item()})\n        print('\\r' + stats, end=\"\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}