{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Women's Clothing Ecommerce Review: Sentiment Analysis\n\nThis project will implement a bidirectional LSTM model to predict the sentiment of the ecommerce clothing reviews using two dependent variables:\n\n1. Rating in scale of 1 to 5. 1 is worst and 5 is best. 1 and 2 is considered negative. 4 and 5 is considered      positive. \n\n2. Recommendation [0 for not recommended and 1 for recommended] "},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom IPython.display import display\n\n\nfrom pylab import *\nfrom wordcloud import WordCloud, STOPWORDS \nimport collections\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.initializers import Constant\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as mt\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading and cleaning the data\nreview=pd.read_csv('../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\n\nprint ('Raw data input:')\ndisplay(review.head())  \n#print (review.head().to_html())\n\nreview=review[['Title','Review Text','Rating','Recommended IND']]\n\n#converting Rating and Recommendation to categorical variables\n\nreview['Rating'] = review['Rating'].astype('category')\nreview['Recommended IND'] = review['Recommended IND'].astype('category')\nprint ('Checking dataframe data type:',review.dtypes)\n\nprint (\"Saving subset of the dataset by dropping few features:\")\ndisplay(review.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of clothing id, age of the reviewer, title of the review, review text, rating in scale of 1 to 5, recommended or not recommended, feedback to the review, and three columns which covers details about the dress. Since, the focus of this analysis is on review and prediction of sentiment analysis, a subset of the dataset including review title, review text, rating, and recommendation was considered for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary of data\n\nprint ('Summary of the dataset')\nreview.info()\n\nreview.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are clearly some missing data in the Title and Review Text columns. The NaNs will be removed later and the final dataset will not include the missing rows.\n\n### Exploratory Data Analysis\n\n#### Questions Answered:\n\n1. Distribution of Rating\n2. Distribution of Recommendation\n3. Number of missing titles and number of missing reviews\n4. Word cloud for positive review or recommended\n5. Word cloud for negative review or not recommended\n6. Word length for positve and negative recommendations\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of rating\nprint ('Distribution of review text by Rating and Recommendation:')\n\nprint (review.groupby(['Recommended IND','Rating'])['Review Text'].count())\n\nprint ('\\n')\nprint ('Number of positive and negative recommendations')\nprint (review.groupby(['Recommended IND'])['Recommended IND'].count())\n\nprint ('\\n')\nprint ('Count of different ratings')\nprint (review.groupby(['Rating'])['Rating'].count())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above distribution and the below plot indicates that most of the reviews were positive and the reviewers recommended the product. The training and test data set needs to take into account that bad reviews and non-recommendations are few in number. Another interesting fact to know is the small number of reviews with high rating but non-recommendations and similarly reviews with poor rating and postive recommendations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#barchart showing the distribution of rating and recommendations\nf, ax = plt.subplots()\nf.set_size_inches(20,10)\n\n#plt.plot(x,y,'b--',label=)\n\n\nsns.countplot(x=\"Recommended IND\", hue='Rating', data=review)\n\nplt.xlabel(\"Recommendations\",fontsize='large')\nplt.ylabel('Count', fontsize='large')\n#plt.xlim(50,80)\n#plt.ylim(58,62)\nplt.rcParams.update({'font.size':12})\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\nplt.title('Distribution of rating and recommendations' )\nax.legend(loc='best',fontsize='large')\n#plt.savefig('Distribution of rating and recommendations' +  '.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a number of reviews with missing titles. There are also few without a text review but a numerical rating with a recommendation (positive or negative). Such reviews were not considered in the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of missing titles and reviews\n\nmiss_title = review.describe().loc['count','Rating']-review.describe().loc['count','Title']\nmiss_review = review.describe().loc['count','Rating']-review.describe().loc['count','Review Text']\n\nprint ('No. of missing titles:',miss_title)\nprint ('No. of missing reviews:',miss_review)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next section analyzes the distribution of word length in the positive and negative recommendations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset of the dataset\n\nrecc = review[review['Recommended IND']==1]  #reviews that were recommendation\nnon_recc=review[review['Recommended IND']==0] #reviews that were not recommendation\n\nsurp_recc=review[((review['Recommended IND']==1) & (review['Rating']==1)) | ((review['Recommended IND']==1) & (review['Rating']==2))] \n#reviews that were recommended but with poor rating\n\nsurp_non_recc=review[((review['Recommended IND']==0) & (review['Rating']==4)) | ((review['Recommended IND']==0) & (review['Rating']==5))]\n#reviews that were not recommended but had high rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#length of words\n\ndef review_length(string):\n   \n    '''\n    Measures the length of the review for each entry\n    \n    Arguments:\n    string -- Input string for each review\n    \n    Returns:\n    word_length -- Number of words in each review\n    '''\n\n    word_length = len(str(string).split())\n    return word_length\n\n\nrecc.loc[:,'Length'] = recc.loc[:,'Review Text'].apply(review_length)\nnon_recc.loc[:,'Length'] = non_recc.loc[:,'Review Text'].apply(review_length)\n\n\nprint ('Average length of review in Recommended reviews:',round(recc.loc[:,'Length'].mean(),2))\nprint ('Average length of review in Non-Recommended reviews:',round(non_recc.loc[:,'Length'].mean(),2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of words in positve and negative recommneded reviews are similar. Negative recommendation reviews are slightly longer. The distribution is also quite similar. Since ratings and recommendations are closely related, this analysis was not conducted on the ratings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of word lengths of positve and negative recommendations\n\nf, ax = plt.subplots()\nf.set_size_inches(20,10)\n\n#plt.plot(x,y,'b--',label=)\nsns.distplot( recc[\"Length\"] , color=\"skyblue\", label=\"Recommended\")\nsns.distplot( non_recc[\"Length\"] , color=\"red\", label=\"Non-Recommended\")\n\nplt.xlabel(\"Word Length\",fontsize='large')\nplt.rcParams.update({'font.size':12})\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\nplt.title('Distribution of word length ' )\nax.legend(loc='best',fontsize='large')\n#plt.savefig('Distribution of rating and recommendations' +  '.png', dpi=300, bbox_inches='tight')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Cloud\n\nA word cloud analysis is undertaken to understand frequently used words in positive and negative recommendations."},{"metadata":{},"cell_type":"markdown","source":"The word cloud for positive reviews is represented below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#developing word cloud for Recommended reviews\n\npos_comment_words = '' \n#adding specific words related to clothing review as stop words\ncustom_words=set(['dress','fit','size','color', 'will','look','wear','fabric','colors','much','ordered','-','it.','got','top','small','really','one','material','shirt','way','even'])\nstopwords = set(STOPWORDS)\nstopwords=stopwords.union(custom_words)\n  \nfor rev in recc['Review Text']: \n    \n    tokens = str(rev).split() \n\n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    pos_comment_words += \" \".join(tokens)+\" \"\n  \n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords,\n                collocations = False,\n                min_font_size = 10).generate(pos_comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() \n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word cloud for negative recommendations is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#word cloud for non recommended reviews\n\ncomment_words = '' \n#adding specific words related to clothing review as stop words\ncustom_words=set(['dress','fit','size','color', 'will','look','wear','fabric','colors','much','ordered','-','it.','got','top','small','really','one','material','shirt','way','even','looks','looked'])\nstopwords = set(STOPWORDS)\nstopwords=stopwords.union(custom_words) \n  \nfor rev in non_recc['Review Text']: \n    \n    tokens = str(rev).split() \n\n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords,\n                collocations = False,\n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word clouds indicate a lot of similarity as well as some differences between the recommended and non-recommended reviews as could be expected as the review is about similar clothing items. The positive reviews wordcloud shows words like love, perfect, beautiful, flattering, soft, great, cute, comfortable, pretty, fits, etc. On the other hand the negative reviews indicate frequent usage of words like love, unfortunately, short, sadly, unflattering, pretty, cute, tight, back, return, etc. Some positive words are also frequently used in negative reviews, often times with negation like not.\n\n\nThe numerical distribution of words is analyzed next. The top 20 words that occur more in positive and negative reviews are shown below"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing the distribution of words\n\n#counting number of words in negative recommendations\nneg=comment_words.split()\nneg_counter=collections.Counter(neg)\n#counting number of words in positive recommendations\npos=pos_comment_words.split()\npos_counter=collections.Counter(pos)\n\n#removing stopwords from the dictionaries\nfor word in stopwords:\n    try:\n        neg_counter.pop(word)\n    except:\n        pass\n    try:\n        pos_counter.pop(word)\n    except:\n        pass\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging the positive and negative word dataframes\nneg_df=pd.DataFrame(list(neg_counter.items()),columns = ['Words','Neg_Count']) \npos_df=pd.DataFrame(list(pos_counter.items()),columns = ['Words','Pos_Count']) \n\ncombined_df=pd.merge(pos_df,neg_df,on='Words',how='outer')\ncombined_df = combined_df.fillna(0)\ncombined_df['Difference']=combined_df['Pos_Count']-combined_df['Neg_Count']\n\ncombined_df = combined_df.sort_values(by=['Difference'], ascending=True)\nstrong_neg =combined_df.head(20)\n\nstrong_neg['Difference']=abs(strong_neg['Difference'])\nstrong_pos=combined_df.tail(20)\n\n#barchart showing the distribution of more frequently observed words in negative reviews\nf, ax = plt.subplots()\nf.set_size_inches(20,10)\n\n#plt.plot(x,y,'b--',label=)\n\nsns.barplot(x=\"Words\", y='Difference', data=strong_neg)\n\nplt.xlabel(\"Higher Count of Words in Negative Review\",fontsize='large')\nplt.ylabel('Count', fontsize='large')\nplt.xticks(rotation=90)\nplt.rcParams.update({'font.size':12})\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\nplt.title('Count of words occuring more in Negative Reviews' )\nax.legend(loc='best',fontsize='large')\n#plt.savefig('Distribution of rating and recommendations' +  '.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n#barchart showing the distribution of more frequently observed words in positive reviews\nf, ax = plt.subplots()\nf.set_size_inches(20,10)\n\n#plt.plot(x,y,'b--',label=)\n\nsns.barplot(x=\"Words\", y='Difference', data=strong_pos)\n\nplt.xlabel(\"Higher Count of Words in Positive Review\",fontsize='large')\nplt.ylabel('Count', fontsize='large')\nplt.xticks(rotation=90)\nplt.rcParams.update({'font.size':12})\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\nplt.title('Count of words occuring more in Positive Reviews' )\nax.legend(loc='best',fontsize='large')\n#plt.savefig('Distribution of rating and recommendations' +  '.png', dpi=300, bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis of the words indicate that the negative reviews show a higher count of the following words:dissapointed, cheap, unflattering, returning, poor, unfortunately, awful, strange, poorly, shame, sadly. These words generally have a negative connotation. Similarly, the positive reviews exhibited higher count of the following words: well, looks, soft, nice, perfect, fits, great, love, etc. Although the positive review also included several words which were neutral."},{"metadata":{},"cell_type":"markdown","source":"# Bidirectional LSTM for Recommendation Prediction\n\nSentiment analysis was carried out using Bidirectional LSTM on the recommendation. Several architecture of the neural network was considered including Unidirectional LSTM and two layers LSTM. The performance difference was minimal.\n\nThe first step involved cleaning the text and identifying the maximum sequence length after tokenizing to enable padding. This padded vector will be used as input."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word Preprocessing\n#creating subset of the dataframe\n\nrecc_lstm=review[['Review Text','Rating','Recommended IND']]\nrecc_lstm=recc_lstm.dropna()  #dropping missing text cases in reviews\n\n#converting the panda series to numpy array\nX=recc_lstm['Review Text']\nX=np.array(X)\nY=recc_lstm['Recommended IND']\nY=np.array(Y)\n\n#tokenizing the strings\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(X)\nsequencer=tokenizer.texts_to_sequences(X)\n\n#finding maximum length of a review\n\nmaxLen=0\nfor string in sequencer:\n    temp=len(string)\n    if temp>maxLen:\n        maxLen=temp\n\nprint ('Maximum sequence length:',maxLen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe Embeddings\n\nPre-trained GloVe embeddings were used to create the embedding matrix. The embedding vectors are 50 dimensional and the vocabulary is about 400k words. The word_to_index and index_to_word dictionary was created from the review text"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the word to index and index to word vectors\nword_to_index=tokenizer.word_index #dictionary that maps words in the reviews to indices\nindex_to_word=tokenizer.index_word #dictionary that maps indices back to words\n\n#loading the GloVe embeddings\nembeddings_dict = {} #dictionary of words and their correspondng GloVe vector representation\nindices=0\n\nwith open(\"../input/glove6b50dtxt/glove.6B.50d.txt\", 'r', encoding ='utf8') as f:\n    for line in f:\n        words = line.split()\n        word = words[0]\n        vector = np.asarray(words[1:], \"float32\")\n        embeddings_dict[word] = vector\n        indices+=1\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#preparing embedding matrix\nvocab_size=len(word_to_index)+1 #to account for out of vocabulary words\nembedding_dim=50 #number of dimensions chosen in the GloVe representation\npresent=0\nabsent=0\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_to_index.items():\n    #embedding_vector = embeddings_dict[word]\n    embedding_vector=  embeddings_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        present+=1\n    else:\n        absent+=1\n\nprint (\"No. of words in the matrix\",present)\nprint (\"No.of missing words\",absent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite a number of words in the reviews are missing in the GloVe representation. A review of the missing words indicate multiple words with wrong spelling, numerical numbers related to weight or height, and few words like didn't (did not), couldn't, etc. The GloVe representation does not handle such words well. The performance of the model did not improve significantly when we trained the embedding layer. In the interest of saving time, we will  use the GloVe embeddings instead of training our own embedding matrix. \n\nThe review text is next padded to have equal length as the max length. The sequences are then divided into training and test data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing the text to create indices\n\ndef sentences_to_indices(sequencer,maxLen):\n    \n    '''\n    Converts the tokenized sequencer to 2D matrix for each text entry.\n    Each row in the matrix is one review. Each column indice indicate one word in the text.\n    \n    Arguments:\n    sequencer -- List of list comprisong of text reviews converted to indices.\n    maxLen -- Maximum length of the nested list consisting of text converted to indices in the sequencer list\n    \n    Returns:\n    X_indices -- 2D matrix where each row corresponds to each review. Each column indice correspond to a word in the review.\n    '''\n    \n    X_indices=np.zeros((len(sequencer),maxLen))\n    for i in range(len(sequencer)):\n        j=0\n        for n in sequencer[i]:\n             X_indices[i,j]= n\n             j+=1\n    return X_indices\n\nX_indices=sentences_to_indices(sequencer,maxLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dividing into training and test data set\nnp.random.seed(2)\nX_tr,X_test,Y_tr,Y_test=train_test_split(X_indices,Y,test_size=0.1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the Model\n\nA bidirectional LSTM model was built with 10 units. It is many-to-one architecture in the secoond layer. The LSTM output was fed into a dense layer to 10 units and then Sigmoid activation was used on the next dense layer to predict the recommendation. Recommendation is a binary classification problem. Therefore, the loss function was binary_crossentropy and the Adam optimizer was used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the Bidirectional LSTM model\nearly_stopping=EarlyStopping(monitor='val_loss',patience=5)\nmodel_save=ModelCheckpoint('top_model.hdf5',save_best_only=True)\nmodel=Sequential()\nmodel.add(Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    input_length=maxLen,\n    embeddings_initializer=Constant(embedding_matrix),\n    trainable=False,\n))\nmodel.add(Bidirectional(LSTM(units = 10, return_sequences= True)))\n#model.add(Dropout(rate=0.5))\nmodel.add(Bidirectional(LSTM(units = 10, return_sequences= False)))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compiling and fitting the model\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel_his=model.fit(X_tr, Y_tr, epochs = 400, batch_size = 32, validation_split=0.1, shuffle=True,verbose=True,callbacks=[early_stopping,model_save])\n\n\nplt.figure()\nplt.plot(model_his.history['accuracy'])\nplt.plot(model_his.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'])\nplt.show()\n\nplt.figure()\nplt.plot(model_his.history['loss'])\nplt.plot(model_his.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n\nThe model was fitted on training data with training accuracy of 90% and validation accuracy of 89%. The training was stopped to avoid overfitting as the validation loss was increasing. The model when tested on test data showed an accuracy of 88%. The majority class accuracy is 81%. The class prediction is done with a threshold of 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating the model on the test data set\n\nprint ('Loss=',model.evaluate(X_test,Y_test)[0])\nprint ('Accuracy=',model.evaluate(X_test,Y_test)[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the class\npred=model.predict_proba(X_test)\npred_class=np.array([0 if i<0.5 else 1 for i in pred]) #using threshold of 0.5\npred_class=pred_class.reshape(pred_class.shape[0],1)\nY_test=Y_test.reshape((Y_test.shape[0],1))\ndiff_new=Y_test-pred_class\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve\nY_test=Y_test.reshape((Y_test.shape[0],1))\nfpr_lstm,tpr_lstm,_ = roc_curve(Y_test,pred)\nroc_auc_lstm = auc(fpr_lstm,tpr_lstm)\n\nf, ax = plt.subplots()\nf.set_size_inches(10,5)\nplt.plot(fpr_lstm, tpr_lstm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lstm))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nax.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\nax.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Bidirectional LSTM')\nplt.legend(loc='lower right', fontsize=13)\n\n#Confusion Matrix\n\nlstm_cm=np.around(confusion_matrix(pred_class,Y_test),0)\n\n\nplt.figure(figsize=(10,7))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.title(\"Bidirectional LSTM\")\nsns.heatmap(lstm_cm, annot = True,fmt='0g',cmap=\"Blues\",cbar=False)\n\n#Classification Metrics\n\n\nprint(\"Bidirectional LSTM\")\nprint(mt.classification_report(Y_test, pred_class))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model does a fairly good job in predicting the positive recommendation with a precision and recall of approximately 0.92-0.95. The positive recommendation is the majority class. On the other hand, the model only predicts the negative recommendations with precision of 0.74 and recall of 0.63. We will next look at few of the wrong predictions to analyze the limitations of the model."},{"metadata":{},"cell_type":"markdown","source":"# Analysis of Wrong Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating false positive and negative count\nprint ('False Negative Count:',count_nonzero(diff_new==1))\nprint ('False Positive Count:',count_nonzero(diff_new==-1))\nprint ('Correct Prediction Count:',count_nonzero(diff_new==0))\nprint ('\\n')   \n\n#Identifying wrong predictions and storing the index in two lists\n\nfalse_pos_index=[]\nfalse_neg_index=[]\n\nfor i in range(len(diff_new)):\n    if diff_new[i][0]>0:\n        false_neg_index.append(i)\n    if diff_new[i][0]<0: \n        false_pos_index.append(i)\n\n#Randomly shuffling the index of positive and negative reviews\nnp.random.seed(7)\nnp.random.shuffle(false_pos_index)        \nnp.random.shuffle(false_neg_index)   \n        \n\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ten Randomly Selected False Positive Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The index were shuffled in the previous cell. We choose the first 10 reviews.\nprint ('\\n')  \nfor i in range(10):\n     index=false_pos_index[i]\n     print ('\\n')\n     wrong_pred_text=[]  #stores the wrong predicted review text\n     for w in X_test[index]:\n            if w==0.0:\n                 pass\n            else:\n                 wrong_pred_text.append(index_to_word[w])  #converting index to word\n     print (str(index) + ':')\n     print (' '.join(wrong_pred_text))  #joining the word\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ten Randomly Selected False Negative Recommendations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The index were shuffled in the previous cell. We choose the first 10 reviews.\nprint ('\\n')  \nfor i in range(10):\n     index=false_neg_index[i]\n     print ('\\n')\n     wrong_pred_text=[]  #stores the wrong predicted review text\n     for w in X_test[index]:\n            if w==0.0:\n                 pass\n            else:\n                 wrong_pred_text.append(index_to_word[w])  #converting index to word\n     print (str(index) + ':')\n     print (' '.join(wrong_pred_text))  #joining the word\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Notes:\n\n Recommendation is a binary choice - positive or negative. Often in clothing review, the reviewer might like a dress but returned it due to size or fit issues. On the other hand, there are many reviews where the reviewer appreciates certain features like color or fabric but dislikes the cut or some design. This kind of review points to a neutral outlook and it was difficult to determine if the recommendation is positive or negative. Such reviews typically consisted of both positive words and negative words. The analysis will be repeated with Ratings to checks if it performs better."},{"metadata":{},"cell_type":"markdown","source":"# Bidirectional LSTM for Rating Prediction"},{"metadata":{},"cell_type":"markdown","source":"The ratings are in a scale of 1 to 5. For the purpose of this sentiment classification, a New Rating variable is created with ratings 4 and 5 classified as 1 (Positive)and rating 1 and 2 are classified as 0 (Negative). Rating 3 is excluded from the model. A two layer Bidirectional LSTM model with only ten nodes in each layer is created to reduce overfitting. Increasing the number of nodes did not improve the accuracy of the validation set. \n\nAs the sample size changed from the Recommendation model, the word to index dictionary and the embedding matrix has to be calculated again."},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing the review text and one-hot encoding of Ratings\n\ndef reclassifying (var):\n    \n    '''\n    Reclassifies the ratings to two categories: 1 (Positive and >3) and 0 (Negative and <3)\n    \n    Arguments:\n    var -- Acts as dummy input for the original rating\n\n    Returns:\n    New Rating with O and 1\n    '''\n    \n    if var>3:\n        return 1  \n    if var<3:\n        return 0\n    \nnew_recc_lstm=recc_lstm[recc_lstm['Rating']!=3]\nnew_recc_lstm['New Rating']=new_recc_lstm['Rating'].apply(reclassifying)\n\nnew_recc_lstm['New Rating'] = new_recc_lstm['New Rating'].astype('category')\n#recc_lstm['New Rating']=recc_lstm['New Rating'].cat.codes\n#Y_rating = to_categorical(recc_lstm['New Rating'])\nY_rating=np.array(new_recc_lstm['New Rating'])\n\n#converting the panda series to numpy array. Recalculating X as rating 3 were excluded.\nX_rating=new_recc_lstm['Review Text']\nX_rating=np.array(X_rating)\n\n#tokenizing the strings\nrat_tokenizer=Tokenizer()\nrat_tokenizer.fit_on_texts(X_rating)\nsequencer_rating=rat_tokenizer.texts_to_sequences(X_rating)\n\nX_rating_indices=sentences_to_indices(sequencer_rating,maxLen)\n\nr_maxLen=0\nfor string in sequencer_rating:\n    temp=len(string)\n    if temp>r_maxLen:\n        r_maxLen=temp\n\nprint ('Maximum sequence length:',r_maxLen)\n\nX_rating_indices=sentences_to_indices(sequencer_rating,r_maxLen)\n\n#dividing into training and test data set\nnp.random.seed(199)\nX_rating_train,X_rating_test,Y_rating_train,Y_rating_test=train_test_split(X_rating_indices,Y_rating,test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recreating the word to index vector and the embedding matrix\n\nrat_word_to_index=rat_tokenizer.word_index #dictionary that maps words in the reviews to indices\nrat_index_to_word=rat_tokenizer.index_word #dictionary that maps indices back to words\n\n#preparing embedding matrix\nrat_vocab_size=len(rat_word_to_index)+1 #to account for out of vocabulary words\nrat_embedding_dim=50 #number of dimensions chosen in the GloVe representation\npresent=0\nabsent=0\nrat_embedding_matrix = np.zeros((rat_vocab_size, rat_embedding_dim))\nfor word, i in rat_word_to_index.items():\n    rat_embedding_vector=  embeddings_dict.get(word)\n    if rat_embedding_vector is not None:\n        rat_embedding_matrix[i] = rat_embedding_vector\n        present+=1\n    else:\n        absent+=1\n\nprint (\"No. of words in the matrix\",present)\nprint (\"No.of missing words in the matrix\",absent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the Bidirectional LSTM model\nrat_early_stopping=EarlyStopping(monitor='val_loss',patience=10)\nrat_model_save=ModelCheckpoint('rating_model.hdf5',save_best_only=True)\nrat_model=Sequential()\nrat_model.add(Embedding(\n    input_dim=rat_vocab_size,\n    output_dim=rat_embedding_dim,\n    input_length=r_maxLen,\n    #embeddings_initializer=Constant(rat_embedding_matrix),\n    weights=[rat_embedding_matrix],\n    trainable=False,\n))\nrat_model.add(Bidirectional(LSTM(units = 10, return_sequences= True)))\n#rat_model.add(Dropout(rate=0.5))\nrat_model.add(Bidirectional(LSTM(units = 10, return_sequences= False)))\n#rat_model.add(Dropout(rate=0.5))\nrat_model.add(Dense(10,activation='relu'))\nrat_model.add(Dense(5,activation='relu'))\nrat_model.add(Dense(1,activation='sigmoid'))\n\nrat_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compiling and saving the model\nrat_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nrat_model_his=rat_model.fit(X_rating_train,Y_rating_train, epochs = 1000, batch_size = 128, validation_split=0.03, shuffle=True,verbose=True,callbacks=[rat_early_stopping,rat_model_save])\n\n\nplt.figure()\nplt.plot(rat_model_his.history['accuracy'])\nplt.plot(rat_model_his.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'])\nplt.show()\n\nplt.figure()\nplt.plot(rat_model_his.history['loss'])\nplt.plot(rat_model_his.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating the model on the test data set\n\nprint ('Loss=',rat_model.evaluate(X_rating_test,Y_rating_test)[0])\nprint ('Accuracy=',rat_model.evaluate(X_rating_test,Y_rating_test)[1])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the class\nrat_pred=rat_model.predict_proba(X_rating_test)\nrat_pred_class=np.array([0 if i<0.8 else 1 for i in rat_pred]) #using threshold of 0.8\nrat_pred_class=rat_pred_class.reshape(rat_pred_class.shape[0],1)\nY_rating_test=Y_rating_test.reshape((Y_rating_test.shape[0],1))\ndiff_new_rating=Y_rating_test-rat_pred_class\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve\nfpr_rating_lstm,tpr_rating_lstm,_ = roc_curve(Y_rating_test,rat_pred)\nroc_auc_lstm_rating = auc(fpr_rating_lstm,tpr_rating_lstm)\n\nf, ax = plt.subplots()\nf.set_size_inches(10,5)\nplt.plot(fpr_rating_lstm, tpr_rating_lstm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lstm_rating))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nax.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\nax.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Bidirectional LSTM')\nplt.legend(loc='lower right', fontsize=13)\n\n#Confusion Matrix\n\nrating_lstm_cm=np.around(confusion_matrix(rat_pred_class,Y_rating_test),0)\n\n\nplt.figure(figsize=(10,7))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.title(\"Bidirectional LSTM\")\nsns.heatmap(rating_lstm_cm, annot = True,fmt='0g',cmap=\"Blues\",cbar=False)\n\n#Classification Metrics\n\n\nprint(\"Bidirectional LSTM for Rating\")\nprint(mt.classification_report(Y_rating_test, rat_pred_class))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performs quite well with about 93% accuracy. Since, identifying negative reviews was considered more important, the threhsold probability was increased to 0.8. The recall and precision for positive reviews is very high. The majority class prediction would have resulted in an accuracy of 89%. The recall and precision for the negative reviews was low (0.56-0.79). If identifying negative reviews is more important, it might be useful to increase the threshold."},{"metadata":{},"cell_type":"markdown","source":"# Analysis of Wrong Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating false positive and negative count\nprint ('False Negative Count:',count_nonzero(diff_new_rating==1))\nprint ('False Positive Count:',count_nonzero(diff_new_rating==-1))\nprint ('Correct Prediction Count:',count_nonzero(diff_new_rating==0))\nprint ('\\n')   \n\n#Identifying wrong predictions and storing the index in two lists\n\nrating_false_pos_index=[]\nrating_false_neg_index=[]\n\nfor i in range(len(diff_new_rating)):\n    if diff_new_rating[i][0]>0:\n        rating_false_neg_index.append(i)\n    if diff_new_rating[i][0]<0: \n        rating_false_pos_index.append(i)\n\n#Randomly shuffling the index of positive and negative reviews\nnp.random.seed(8)\nnp.random.shuffle(rating_false_pos_index)        \nnp.random.shuffle(rating_false_neg_index) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## False Positive Cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The index were shuffled in the previous cell. We choose the first 10 reviews.\nprint ('\\n')  \nfor i in range(10):\n     rating_index=rating_false_pos_index[i]\n     print ('\\n')\n     rating_wrong_pred_text=[]  #stores the wrong predicted review text\n     for w in X_rating_test[rating_index]:\n            if w==0.0:\n                 pass\n            else:\n                 rating_wrong_pred_text.append(rat_index_to_word[w])  #converting index to word\n     print (str(rating_index) + ':')\n     print (' '.join(rating_wrong_pred_text))  #joining the word","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## False Negative Cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The index were shuffled in the previous cell. We choose the first 10 reviews.\nprint ('\\n')  \nfor i in range(10):\n     rating_index=rating_false_neg_index[i]\n     print ('\\n')\n     rating_wrong_pred_text=[]  #stores the wrong predicted review text\n     for w in X_rating_test[rating_index]:\n            if w==0.0:\n                 pass\n            else:\n                 rating_wrong_pred_text.append(rat_index_to_word[w])  #converting index to word\n     print (str(rating_index) + ':')\n     print (' '.join(rating_wrong_pred_text))  #joining the word\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions"},{"metadata":{},"cell_type":"markdown","source":"Sentiment analysis was repeated using Rating. The neutral rating of 3 was not considered. Only positive (4 and 5) and negative (1 and 2) ratings were considered. Probably, this resulted in a higher accuracy of 93%. However, using recommendation as the criterion, sentiment analysis demonstrated an accuracy of 88%. The model performed better for the positive cases. Review of the wrong predictions indicates most of these reviews included both positive and negative words where the reviewer liked certain aspects of the dress and did not like other aspects. This made it harder for the model to make the right prediction.\n\nThe GloVe embeddings were based on the Wikipedia dataset. Since, reviews are written in a more casual form, it might be useful to rerun the model with the Twitter dataset GloVe embeddings."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}