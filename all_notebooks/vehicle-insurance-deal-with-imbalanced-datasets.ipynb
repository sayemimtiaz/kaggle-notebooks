{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Before we Begin\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards this subject and will look for more efficient ways so that our models are able to detect more accurately both interested and non-interested customers.\n\n# Introduction\nIn this kernel we will use various predictive models to see how accurate they are in detecting whether a customer we target will be interested for cross-selling or not. As described in the dataset, the features are not scaled. We will find interesting insights now. Let's start!"},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n#classifies\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n#other libraries\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/health-insurance-cross-sell-prediction/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#categorical features\ncategories = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\ndf1 = pd.get_dummies(df,columns = categories, drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#columns\ndf1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dependent Variable"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#check out the output variable\nprint('Not Interested', round(df1['Response'].value_counts()[0]/len(df1) * 100, 2), '% of the dataset')\nprint('Interested', round(df1['Response'].value_counts()[1]/len(df1) * 100, 2), '% of the dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Response', data=df1, palette=colors)\nplt.title('Class Distributions \\n (0: Not Interested || 1: Interested)', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df1.drop('id', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df1.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = df1.drop('Response', axis = 1)\ny = df1['Response']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling - Robust Scaler"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\nscaled_values = rob_scaler.fit_transform(X)\n\nX = pd.DataFrame(scaled_values, columns = X.columns)\ndf1 = pd.concat([X,y], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the Data (Original DataFrame)**\n\nBefore proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Splitting the dataset\nprint('Not Interested', round(df1['Response'].value_counts()[0]/len(df1) * 100, 2), '% of the dataset')\nprint('Interested', round(df1['Response'].value_counts()[1]/len(df1) * 100, 2), '% of the dataset')\n\nX = df1.drop('Response', axis = 1)\ny = df1['Response']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle = False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train: \", train_index, \"Test: \", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n\n#turning them into arrays\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n#see if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_count_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_count_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 50)\n\nprint('Label Distributions: \\n')\nprint(train_count_label / len(original_ytrain))\nprint(test_count_label / len(original_ytest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Under-Sampling\n\nIn this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Random Under-sampling\ndf1 = df1.sample(frac = 1)\n\n#amount of Interested response values : 46710\ninterest_df = df1.loc[df1['Response'] == 1]\nnot_interest_df = df1.loc[df1['Response'] == 0][:46710]\n\nnormal_distributed_df = pd.concat([interest_df, not_interest_df])\n#shuffle \nnew_df = normal_distributed_df.sample(frac = 1, random_state = 420)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Equally Distributing and Coorelating\n\nNow that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Equally distributed now\nprint(\"Distribution:\\n\")\nprint(new_df['Response'].value_counts()/len(new_df))\n\nsns.countplot('Response', data = new_df, palette=colors)\nplt.title(\"Equally Distributed Responses\", fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrices\n\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n**Summary**\n\n* Positive Correlation: Age, Vehicle_Damage_Yes\n* Negative Correlation: Previously_Insured, Policy_Sales_Channel, Vehicle_Age_< 1 Years"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Correlation Matrix\ncorrelation = new_df.corr()\nplt.figure(figsize = (20,13))\nsns.heatmap(correlation, cmap = 'coolwarm_r', annot_kws={'size : 20'})\nplt.title(\"Correlation matrix after doing undesampling\", fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boxplots (Positive & Negative Variables)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#For negative correlation\nf, axes = plt.subplots(ncols=3, figsize = (20,4))\n\n#negative correlation with our Response.\nsns.boxplot(x = 'Response', y = 'Previously_Insured', data = new_df, palette=colors, ax = axes[0])\naxes[0].set_title(\"Previously Insured Vs Class Negative correlation\")\n\nsns.boxplot(x = 'Response', y = 'Policy_Sales_Channel', data = new_df, palette=colors, ax = axes[1])\naxes[1].set_title(\"Policy Sales Channel Vs Class Negative correlation\")\n\nsns.boxplot(x = 'Response', y = 'Vehicle_Age_< 1 Year', data = new_df, palette=colors, ax = axes[2])\naxes[2].set_title(\"Vehicle_Age_< 1 Years Vs Class Negative correlation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#For positive correlation\nf, axes = plt.subplots(ncols=2, figsize = (20,4))\n\n#positive correlation with our Response.\n#sns.boxplot(x = 'Response', y = 'Vehicle_Age_> 2 Years', data = new_df, palette=colors, ax = axes[0])\n#axes[0].set_title(\"Vehicle_Age_> 2 Years Vs Class Positive correlation\")\n\n#sns.boxplot(x = 'Response', y = 'Region_Code_28.0', data = new_df, palette=colors, ax = axes[1])\n#axes[1].set_title(\"Region Code 28.0 Vs Class Positive correlation\")\n\nsns.boxplot(x = 'Response', y = 'Age', data = new_df, palette=colors, ax = axes[0])\naxes[0].set_title(\"Age Vs Class Positive correlation\")\n\nsns.boxplot(x = 'Response', y = 'Vehicle_Damage_Yes', data = new_df, palette=colors, ax = axes[1])\naxes[1].set_title(\"Vehicle Damage Yes Vs Class Positive correlation\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection\n\nOur main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.\n\n**Interquartile Range Method**\n* IQR : We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.\n* Boxplots : Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme).\n\n**Outlier Removal Tradeoff**\nWe have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.\n\n**The Tradeoff:** The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#Removing Outliers From Age\nAge_interested = new_df['Age'].loc[new_df['Response'] == 1].values\nq25, q75 = np.percentile(Age_interested, 25), np.percentile(Age_interested, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nAge_Iqr = q75 - q25\nprint('iqr: {}'.format(Age_Iqr))\n\nAge_cut_off = Age_Iqr * 1.5\nAge_lower, Age_Upper = q25 - Age_cut_off, q75 + Age_cut_off\nprint('Cut_off: {}'.format(Age_cut_off))\nprint('Age_lower: {}'.format(Age_lower))\nprint('Age_Upper: {}'.format(Age_Upper))\n\noutliers = [x for x in Age_interested if x < Age_lower or x > Age_Upper]\nprint('Feature Age Outliers for Interested cases: {}'.format(len(outliers)))\nprint('Age Outliers: {}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['Age'] > Age_Upper) | (new_df['Age'] < Age_lower)].index)\nprint('--' * 25)\n\n\n#Vehicle_Damage_Yes Outliers\nVehicle_Damage_interested = new_df['Vehicle_Damage_Yes'].loc[new_df['Response'] == 1].values\nq25, q75 = np.percentile(Vehicle_Damage_interested, 25), np.percentile(Vehicle_Damage_interested, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nVehicle_Damage_Iqr = q75 - q25\nprint('iqr: {}'.format(Vehicle_Damage_Iqr))\n\nVehicle_Damage_cut_off = Vehicle_Damage_Iqr * 1.5\nVehicle_Damage_lower, Vehicle_Damage_Upper = q25 - Vehicle_Damage_cut_off, q75 + Vehicle_Damage_cut_off\nprint('Vehicle_Damage_cut_off: {}'.format(Vehicle_Damage_cut_off))\nprint('Vehicle_Damage_lower: {}'.format(Vehicle_Damage_lower))\nprint('Vehicle_Damage_Upper: {}'.format(Vehicle_Damage_Upper))\n\noutliers = [x for x in Vehicle_Damage_interested if x < Vehicle_Damage_lower or x > Vehicle_Damage_Upper]\nprint('Feature Damage Outliers for Interested cases: {}'.format(len(outliers)))\nprint('Vehicle Damage Outliers: {}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['Vehicle_Damage_Yes'] > Vehicle_Damage_Upper) | (new_df['Vehicle_Damage_Yes'] < Vehicle_Damage_lower)].index)\nprint('--' * 25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality Reduction and Clustering\n\n**Understanding T-SNE**\nIn order to understand this algorithm you have to understand the following terms:\n* Eucledian Distance\n* Conditional Probability\n* Normal and T-Distribution Plots\n\n**Summary**\n* t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset.\n* Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Dimensional Reduction & Clustering\n#T-SNE Algorithm and PCA\n\nX = new_df.drop('Response', axis = 1)\ny = new_df['Response']\n\n#T-sne\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=420).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took: {:.2} seconds\".format(t1 - t0))\n\n#PCA\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=420).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took: {:.2} seconds\".format(t1-t0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize = (24,6))\n\nf.suptitle('Clusters using Dimensionality Reduction', fontsize = 14)\nblue_patch = mpatches.Patch(color = \"#0A0AFF\", label = \"Not Interested\")\nred_patch = mpatches.Patch(color = \"#AF0000\", label = \"Interested\")\n\n#t-sne scatterplot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1],\n            c = (y == 0), cmap = \"coolwarm\",\n            label = \"Not Interested\", linewidths = 2)\n\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1],\n            c = (y == 1), cmap = \"coolwarm\",\n            label = \"Interested\", linewidths = 2)\nax1.set_title('t-SNE', fontsize = 14)\nax1.grid(True)\nax1.legend(handles = [blue_patch, red_patch])\n\n#PCA Scatterplot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1],\n            c = (y == 0), cmap = \"coolwarm\",\n            label = \"Interested\", linewidths = 2)\n\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1],\n            c = (y == 1), cmap = \"coolwarm\",\n            label = \"Interested\", linewidths = 2)\nax2.set_title('PCA', fontsize = 14)\nax2.grid(True)\nax2.legend(handles = [blue_patch, red_patch])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifiers (UnderSampling):\nIn this section we will train four types of classifiers and decide which classifier will be more effective in detecting Interested transactions. Before we have to split our data into training and testing sets and separate the features from the labels.\n\n**Summary**\n* **Logistic Regression** classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression)\n* **GridSearchCV** is used to determine the paremeters that gives the best predictive score for the classifiers.\n* Logistic Regression has the best Receiving Operating Characteristic score (ROC), meaning that LogisticRegression pretty accurately separates interested and non-interested transactions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Classifiers\nX = new_df.drop('Response', axis = 1)\ny = new_df['Response']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n#Let's implement Classifiers\nclassifiers = {\"LogisticRegression\": LogisticRegression(),\n               \"KNearest\": KNeighborsClassifier(),\n               \"Decision Tree Classifier\": DecisionTreeClassifier(),\n               \"Random Forest Classifier\": RandomForestClassifier()\n              }\n\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of \", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#We will choose 2 best models and apply hyper parameters.\n#logistic, Random forest\n\nfrom sklearn.model_selection import GridSearchCV\n\n#logistic\nlog_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log = GridSearchCV(LogisticRegression(), log_params)\ngrid_log.fit(X_train, y_train)\n\n#logistic best estimators\nlog_reg = grid_log.best_estimator_\n\nrandom_params = {'n_estimators': [100, 200, 500], 'criterion': ['gini', 'entropy'],\n                 'max_depth': [None, 10, 15, 20]}\n\ngrid_random = GridSearchCV(RandomForestClassifier(), random_params)\ngrid_random.fit(X_train, y_train)\n\n#Random Forest best estimators\nrandom_reg = grid_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overfitting Case"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(random_reg, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will undersample during cross validating**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"undersample_X = new_df.drop('Response', axis=1)\nundersample_y = new_df['Response']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values\n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Random Forest Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC Curve"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nrandom_pred = cross_val_predict(random_reg, X_train, y_train, cv=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint(\"Logistic Regression: \", roc_auc_score(y_train, log_reg_pred))\nprint(\"Random Forest: \", roc_auc_score(y_train, random_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, random_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 2 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred))) \n    plt.plot(tree_fpr, tree_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, random_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, tree_fpr, tree_tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/health-insurance-cross-sell-prediction/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df.drop('id', axis = 1, inplace = True)\n\ncategories = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\ntest_df = pd.get_dummies(test_df,columns = categories, drop_first=True)\n\nscaled_test_values = rob_scaler.fit_transform(test_df)\n\ntest_df = pd.DataFrame(scaled_test_values, columns = test_df.columns)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predictions = log_reg.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df['Predictions'] = predictions\ntest_df.to_csv(\"submission.csv\")\nprint(\"Submission file created\")\ntest_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The End"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}