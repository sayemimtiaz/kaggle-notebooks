{"cells":[{"metadata":{},"cell_type":"markdown","source":"![title](https://raw.githubusercontent.com/emdemor/drug-classification/master/source/title.png)\n<a id=\"toc\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h2>Table of Contents</h2>\n        </center>\n\n   \n<ol>\n    <li><a href=\"#first\" style=\"color: #7a0723;\">Introduction</a></li>\n    <li><a href=\"#second\" style=\"color: #7a0723;\">Initializing Script</a></li>\n    <li><a href=\"#third\" style=\"color: #7a0723;\">Exploratory Analysis</a></li>\n    <li><a href=\"#fourth\" style=\"color: #7a0723;\">Pre-Processing</a></li>\n    <li><a href=\"#fifth\" style=\"color: #7a0723;\">Machine Learning: Classification Algorithms</a></li>\n\n<!--     <li><a href=\"#seventh\"></a></li> -->\n<!--     <li><a href=\"#eighth\">Teste t pareado para duas médias</a></li> -->\n<!--     <li><a href=\"#ninth\">Teste F para duas variâncias</a></li> -->\n<!--     <li><a href=\"#tenth\">Teste t para duas médias</a></li> -->\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"init\"></a>\n\n# Initializing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Useful Functions & Classes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<!-- <a id=\"first\"></a> -->\n\n<a id=\"first\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n  \n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>1. Introduction</h1>\n        </center>\n\n   \n<ol>\n<!--     <li><a href=\"#ea-init\">Initializing</a></li> -->\n\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To construct a system capable to make good predictions, it is necessary to understand how the features influence the desired variable. To make this, I will split my work in two stages:\n\n1. Exploring: An exploratory analysis, to understand the structure of data, missing values, correlations and etc, to select the best set of features to be used at the analysis.\n\n2. Modeling: In this step, I try to find the best and simplest model for describing the data. I do this by testing the influence of features on target values. Since that the definition of a best model is something complicated and subjective, I will choose the model which best efficiency in the follow approach:\n\n    > A. For each Classification Algorithm, I will build a shuffled K-Folded Cross Validation model with `n_splits`= 10\n    \n    > B. After, I'll optimize hyperparameters with a GridSearchCV. The configuration retrieving the higher cv_score will be called 'best_model' for that algorithm\n\n    > C. Split dataset into test (33%) and training (67%)\n\n    > D. Use training dataset to fit the inner parameters of best_model\n\n    > E. Use the trained model to predict the drug variable of test dataset\n\n    > F. Use predicted an original total to evaluate the accuracy_score\n\n    > G. After this, analyzing cv_score and accuracy_score, I'll choose the best estimator for the problem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"second\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>2. Initializing Script</h1>\n        </center>\n\n   \n   \n<ol>\n    <li><a href=\"#is-dep\" style=\"color: #7a0723;\">Packages and Modules</a></li>\n    <li><a href=\"#is-set\" style=\"color: #7a0723;\">Settings</a></li>\n    <li><a href=\"#is-func\" style=\"color: #7a0723;\">Useful Functions</a></li>\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"is-dep\"></a>\n\n<h2>2.1 Packages and Modules <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas and numpy for data manipulation\nimport warnings\nimport pandas as pd\nimport numpy as np\n\n# Matplotlib and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scipy for statistics\nfrom scipy import stats\n\n# os to manipulate files\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score \nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder, LabelEncoder\n\nfrom sklearn.model_selection import cross_validate,KFold,GridSearchCV\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# from sklearn.metrics import mean_absolute_error,r2_score\n# from sklearn import linear_model\n# from sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"is-set\"></a>\n\n<h2>2.2 Settings <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# Setting seaborn style\nsns.set_style(\"darkgrid\")\nsns.set_palette(sns.color_palette(\n    [\"#7a0723\",\"#ff5b21\",\"#f7cf60\", \"#ff94c0\",\"#323133\"]\n))\n\n# Setting Pandas float format\npd.options.display.float_format = '{:,.1f}'.format\n\nSEED = 42   #The Answer to the Ultimate Question of Life, The Universe, and Everything\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"is-func\"></a>\n\n<h2>2.3 Useful Functions <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_categories(encoder,num_list,title):\n    n_list = np.array(num_list).reshape(-1,)\n    res = [[encoder.inverse_transform([[elem]]).reshape(-1,)[0],elem] for elem in n_list]\n    \n    print('\\nEncoding for '+title)\n    for elem in res:\n        print(' '+str(elem[0])+'\\t->\\t'+str(elem[1]))\n        \n        \ndef cross_validate_interval(results):\n    mean = results['test_score'].mean()\n    std = results['test_score'].std()\n    interval = [mean-2*std,mean+2*std]\n    print('A acurácia média é {:.3}%'.format(100*media))\n    print('A acurácia está entre {:.3}% e {:.3}%'.format(*(100*np.array(intervalo))))\n    \ndef print_score(scores):\n    mean= scores.mean() * 100\n    std = scores.std() * 100\n    print(\"Accuracy médio %.2f\" % mean)\n    print(\"Intervalo [%.2f, %.2f]\" % (mean - std, mean + std))\n    \ndef hyperparameter_mean(results,hyperpar):\n    str_ = 'param_'+hyperpar\n    return results[[str_,'mean_fit_time','mean_score_time','mean_test_score']].groupby(str_).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"class-class\"></a>\n### Class to Classifiers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier:\n    '''\n    Description\n    -----------------\n    \n    Class to approach classification algorithm\n    \n    \n    Example\n    -----------------\n        classifier = Classifier(\n                 algorithm = ChooseTheAlgorith,\n                 hyperparameters_range = {\n                    'hyperparameter_1': [1,2,3],\n                    'hyperparameter_2': [4,5,6],\n                    'hyperparameter_3': [7,8,9]\n                 }\n             )\n\n        # Looking for best model\n        classifier.grid_search_fit(X,y,n_splits=10)\n        #dt.grid_search_results.head(3)\n\n        # Prediction Form 1\n        par = classifier.best_model_params\n        dt.fit(X_trn,y_trn,params = par)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 2\n        classifier.fit(X_trn,y_trn,params = 'best_model')\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 3\n        classifier.fit(X_trn,y_trn,min_samples_split = 5,max_depth=4)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n    '''\n    def __init__(self,algorithm, hyperparameters_range={},random_state=42):\n        \n        self.algorithm = algorithm\n        self.hyperparameters_range = hyperparameters_range\n        self.random_state = random_state\n        self.grid_search_cv = None\n        self.grid_search_results = None\n        self.hyperparameters = self.__get_hyperparameters()\n        self.best_model = None\n        self.best_model_params = None\n        self.fitted_model = None\n        \n    def grid_search_fit(self,X,y,verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        \n        self.grid_search_cv = GridSearchCV(\n            self.algorithm(),\n            self.hyperparameters_range,\n            cv = KFold(n_splits = n_splits, shuffle=shuffle, random_state=self.random_state),\n            scoring=scoring,\n            verbose=verbose\n        )\n        \n        self.grid_search_cv.fit(X, y)\n        \n        col = list(map(lambda par: 'param_'+str(par),self.hyperparameters))+[\n                'mean_fit_time',\n                'mean_test_score',\n                'std_test_score',\n                'params'\n              ]\n        \n        results = pd.DataFrame(self.grid_search_cv.cv_results_)\n        \n        self.grid_search_results = results[col].sort_values(\n                    ['mean_test_score','mean_fit_time'],\n                    ascending=[False,True]\n                ).reset_index(drop=True)\n        \n        self.best_model = self.grid_search_cv.best_estimator_\n        \n        self.best_model_params = self.best_model.get_params()\n    \n    def best_model_cv_score(self,X,y,parameter='test_score',verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        if self.best_model != None:\n            cv_results = cross_validate(\n                self.best_model,\n                X = X,\n                y = y,\n                cv=KFold(n_splits = 10,shuffle=True,random_state=self.random_state)\n            )\n            return {\n                parameter+'_mean': cv_results[parameter].mean(),\n                parameter+'_std': cv_results[parameter].std()\n            }\n        \n    def fit(self,X,y,params=None,**kwargs):\n        model = None\n        if len(kwargs) == 0 and params == 'best_model' and self.best_model != None:\n            model = self.best_model\n            \n        elif type(params) == dict and len(params) > 0:\n            model = self.algorithm(**params)\n            \n        elif len(kwargs) >= 0 and params==None:\n            model = self.algorithm(**kwargs)\n            \n        else:\n            print('[Error]')\n            \n        if model != None:\n            model.fit(X,y)\n            \n        self.fitted_model = model\n            \n    def predict(self,X):\n        if self.fitted_model != None:\n            return self.fitted_model.predict(X)\n        else:\n            print('[Error]')\n            return np.array([])\n            \n    def predict_score(self,X_tst,y_tst,score=accuracy_score):\n        if self.fitted_model != None:\n            y_pred = self.predict(X_tst)\n            return score(y_tst, y_pred)\n        else:\n            print('[Error]')\n            return np.array([])\n        \n    def hyperparameter_info(self,hyperpar):\n        str_ = 'param_'+hyperpar\n        return self.grid_search_results[\n                [str_,'mean_fit_time','mean_test_score']\n            ].groupby(str_).agg(['mean','std'])\n        \n    def __get_hyperparameters(self):\n        return [hp for hp in self.hyperparameters_range]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"third\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>3. Exploratory Analysis</h1>\n        </center>\n\n   \n<ol>\n    <li><a href=\"#ea-data\" style=\"color: #7a0723;\">The Dataset</a></li>\n    <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description</a></li>\n    <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features</a></li>\n    <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features</a></li>\n\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ea-data\"></a>\n<h2>3.1 The Dataset <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">↻</a></h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/drug-classification/drug200.csv')#.drop('Unnamed: 0',axis=1)\n\ndf1.columns = ['age','gender','pressure','cholesterol','na_k_ratio','drug']\ndf1['drug'] = df1['drug'].str.replace('drug','Drug')\ndf1.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ea-feat-descr\"></a>\n<h2>3.2 Feature Description  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">↻</a></h2>\n\n`age`(*RN*): the age of the person\n\n`sex`(*NC*): the gender of the person\n\n`pressure`(*OC*): the blood pressure\n\n`cholesterol`(*OC*): cholesterol levels in blood test\n\n`na_k_ratio`(*RN*): urinary Na/K ratio\n\nPS.:\n* (*RN*) : real numeric variable\n* (*NC*) : nominal categorical variable\n* (*OC*) : ordinal categorical variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ea-feat-cat\"></a>\n  \n<h2>3.3 Categorical Features <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">↻</a></h2>","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(12, 12))\n#sns.despine(left=True)\nsns.countplot(x=df1['drug'],data=df1, ax=axes[0,0])\nsns.countplot(x=df1['drug'],hue='cholesterol',data=df1, ax=axes[0,1])\nsns.countplot(x=df1['drug'],hue='gender',data=df1, ax=axes[1,0])\nsns.countplot(x=df1['drug'],hue='pressure',data=df1, ax=axes[1,1])\n\naxes[0,0].set(xlabel='', ylabel='Number of Occurrences',title='Drug Occurrences')\naxes[0,1].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Cholesterol')\naxes[1,0].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Gender')\naxes[1,1].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Blood Pressure')\n\naxes[0,1].legend(loc=1,title='Cholesterol')\naxes[1,0].legend(loc=1,title='Gender')\naxes[1,1].legend(loc=1,title='Blood Pressure')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**\n\n* **DrugY** is the most observed\n\n* **DrugC** is only observed in people with high cholesterol and low blood pressure\n\n* **DrugX** is observed more in people with normal cholesterol than with high cholesterol and normal bloos pressure\n\n* **DrugA** is only observed on people with high blood pressure\n\n* **DrugB** is only observed on people with high blood pressure","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ea-feat-num\"></a>\n<h2>3.4 Numerical Features <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax1 = sns.catplot(x='drug', y='age', data = df1)\nax2 = sns.catplot(x='drug', y='na_k_ratio', data = df1)\n\nax1.set(xlabel='', ylabel='Age',title=\"People's age\")\nax2.set(xlabel='', ylabel='Na/K Ratio',title=\"Urinary Na/K ratio\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(df1, hue=\"drug\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**\n1. **DrugY**: Mainly for `Na_to_K $\\gtrsim 10$ \n2. Except for **DrugY**, all drugs exhibits `Na_to_K` $\\lesssim 10$ \n3. **DrugA** not observed in people over 55 years old (conf: ~95%)\n3. **DrugB** mainly for people over 48 years old (conf: ~95%)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"fourth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>4. Pre-Processing</h1>\n        </center>\n\n   \n<ol>\n    <li><a href=\"#pp-feat-target\" style=\"color: #7a0723;\">Defing Feature and Target Variables</a></li>\n    <li><a href=\"#pp-enc\" style=\"color: #7a0723;\">Encoding Categorical Features</a></li>\n    <li><a href=\"#pp-train-test\" style=\"color: #7a0723;\">Splitting dataset into train and test</a></li>\n    <li><a href=\"#pp-scl\" style=\"color: #7a0723;\">Scaling Features</a></li>\n<!--     <ol>\n        <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description</a></li>\n        <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features</a></li>\n        <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features</a></li>\n    </ol> -->\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pp-feat-target\"></a>\n\n<h2>4.1 Defining Features and Target Variables <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting target from features\ny_column = 'drug'\nX_columns = list(df1.drop(columns=y_column).columns)\n\nX = df1[X_columns]\ny = df1[y_column]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pp-enc\"></a>\n\n<h2>4.2 Encoding Categorical Features<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">↻</a></h2>\n  \n\nFor the case of nominal categorical variables, I will use the `OneHotEnconder` technique. However, as the unique nominal categorical feature  has only two alternatives, the encoding process gives rise to one column (avoiding the dummy variable trap). Because of this, we choose to use the `LabelEncoder`. In the case of ordinal categorical variables, the best approach will be `OrdinalEncoder`.\n\nThe target exhibits 5 different values. It needs to be encoded in a single column such that the `OneHotEncoder` method is not the most indicated one. Since that there is no concept of order in target values, th best encoding technique is `LabelEnconder`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Ord_Encoder = OrdinalEncoder(categories=[['LOW', 'NORMAL', 'HIGH']])\nGender_Encoder = LabelEncoder()\nDrugs_Encoder = LabelEncoder()\n\npressure_list = [[elem] for elem in X['pressure'].to_list()]\nX['pressure'] = Ord_Encoder.fit_transform(pressure_list).astype(int)\n\ncholesterol_list = [[elem] for elem in X['cholesterol'].to_list()]\nX['cholesterol'] = Ord_Encoder.fit_transform(cholesterol_list).astype(int)\n\nX['gender'] = Gender_Encoder.fit_transform(X[['gender']]).astype(int)\n\ny = Drugs_Encoder.fit_transform(y)\n\ndf_enc = pd.concat([X,pd.DataFrame(y,columns=['drug'])],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_categories(Ord_Encoder,X['pressure'].unique(),'pressure')\nprint_categories(Ord_Encoder,X['cholesterol'].unique(),'cholesterol')\nprint_categories(Gender_Encoder,X['gender'].unique(),'gender')\nprint_categories(Drugs_Encoder,np.unique(y),'drugs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_enc.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pp-train-test\"></a>\n\n<h2>4.3 Splitting dataset into train and test<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">↻</a></h2>\n  \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trn, X_tst, y_trn, y_tst = train_test_split(\n    X, y, test_size = 0.33,stratify = y,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pp-scl\"></a>\n\n<h2>4.4 Scaling Features<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">↻</a></h2>\n  \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler_trn = StandardScaler()\nX_scl_trn = scaler_trn.fit_transform(X_trn)\n\nscaler_tst = StandardScaler()\nX_scl_tst = scaler_trn.fit_transform(X_tst)\n\nscaler = StandardScaler()\nX_scl = scaler_trn.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scl = pd.concat([\n            pd.DataFrame(X_scl,columns=X.columns),\n            pd.DataFrame(y,columns=['drug'])\n        ],axis=1)\n\nX_trn_scl = pd.concat([\n            pd.DataFrame(X_scl_trn,columns=X.columns),\n            pd.DataFrame(y_trn,columns=['drug'])\n        ],axis=1)\n\nX_tst_scl = pd.concat([\n            pd.DataFrame(X_scl_tst,columns=X.columns),\n            pd.DataFrame(y_tst,columns=['drug'])\n        ],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"fifth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>5. Machine Learning: Classification Algorithms</h1>\n        </center>\n\n   \n<ol>\n    <li><a href=\"#ml-dummy\" style=\"color: #7a0723;\">Dummy Classifier</a></li>\n    <li><a href=\"#ml-log\" style=\"color: #7a0723;\">Logistic Regression</a></li>\n    <li><a href=\"#ml-svc\" style=\"color: #7a0723;\">Suport Vector Classifier</a></li>\n    <li><a href=\"#ml-tree\" style=\"color: #7a0723;\">Decision Tree Classifier</a></li>\n    <li><a href=\"#ml-gnb\" style=\"color: #7a0723;\">Gaussian Naive Bayes</a></li>\n    <li><a href=\"#ml-knn\" style=\"color: #7a0723;\">K-Nearest Neighbors Classifier</a></li>\n    <li><a href=\"#ml-rf\" style=\"color: #7a0723;\">Random Forest Classifier</a></li>\n    <li><a href=\"#ml-comp\" style=\"color: #7a0723;\">Comparison of Model Scores</a></li>\n    \n    \n\n<!--     <li><a href=\"#ea-data\" style=\"color: #7a0723;\">The Dataset</a></li> -->\n<!--     <ol>\n        <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description</a></li>\n        <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features</a></li>\n        <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features</a></li>\n    </ol> -->\n</ol>\n\n\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section, I will repeat the same approach for different classification algorithms. The approach is detailed for the first case (Logistic Regression) and, for the others, I will use the class implemented in the <a href=\"#class-class\" style=\"color: #7a0723;\">Class to Classifiers</a> section.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:,.4f}'.format\n\nModel_Scores = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-dummy\"></a>\n\n<h2>5.1 Dummy Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I want to define a dummy classifier to compare the efficiency of our classifcation approach with a random classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 40\nmodel_dummy = DummyClassifier(strategy='stratified',random_state = SEED)\n\ndummy_results = cross_validate(\n    model_dummy,\n    X = X,\n    y = y,\n    cv=KFold(n_splits = 200,shuffle=True,random_state = SEED)\n)\n\ndummy_score = dummy_results['test_score'].mean()\n\nmodel_dummy = DummyClassifier(strategy='stratified',random_state = SEED)\n\nmodel_dummy.fit(X_trn,y_trn)\ny_pred = model_dummy.predict(X_tst)\ndummy_acc_score = accuracy_score(y_tst,y_pred)\n\n\nprint('Dummy Accuracy Score: %.2f' % (dummy_acc_score))\nprint('Dummy CV Score: %.1f%%' % (100*dummy_score))\n\n\nModel_Scores['dummy'] = {\n    'model' : model_dummy,\n    'best_params' : model_dummy.get_params(),\n    'test_accuracy_score' : dummy_acc_score,\n    'cv_score' : dummy_results['test_score'].mean(),\n    'cv_score_std' : dummy_results['test_score'].std()\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-log\"></a>\n\n<h2>5.2 Logistic Regression <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a logistic regression is:\n```\nLogisticRegression(\n    penalty='l2', *, dual=False, tol=0.0001, C=1.0, \n    fit_intercept=True, intercept_scaling=1, \n    class_weight=None, random_state=None, solver='lbfgs',\n    max_iter=100, multi_class='auto', verbose=0, \n    warm_start=False, n_jobs=None, l1_ratio=None)\n```\n\nHere, I'll looking for the best configuration of hyperparameters `C` and `solver`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3> A. Logistic Regression: Grid-Search with Cross Validation<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">↻</a></h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    From previous tests, it was possible to conclude that sag and saga solvers\n    retrieve the worst results. I'll not consider this in GridSearchCV.\n\"\"\"\nSEED = 42\n\nhyperparametric_space = {\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n    'C' : [5,7,10,20,30,50,100]\n}\n\ngrid_search_cv = GridSearchCV(\n    LogisticRegression(random_state=SEED),\n    hyperparametric_space,\n    cv = KFold(n_splits = 10, shuffle=True,random_state=SEED),\n    scoring='accuracy',\n    verbose=0\n)\n\ngrid_search_cv.fit(X, y)\nresults = pd.DataFrame(grid_search_cv.cv_results_)\n\npd.options.display.float_format = '{:,.5f}'.format\n\ncol = ['param_C', 'param_solver','mean_fit_time', 'mean_test_score', 'std_test_score']\n\nresults[col].sort_values(\n    ['mean_test_score','mean_fit_time'],\n    ascending=[False,True]\n).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> B. Logistic Regression: Best Model<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">↻</a></h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the results of previous section, we can see that `newton-cg` presents the best test score $ \\left( 98\\% \\pm 3\\% \\right)$. But, if we compare with the best parametric configuration for `liblinear`  $ \\left( 96\\% \\pm 4\\% \\right)$, we can see that the difference is not too big when considering standard deviation. However, there is a big advantage in use `liblinear`: the fit time is much smaller than `newton-cg`  ($0.006 << 0.16 $). I will adopt `newton-cg` with the best hyperparametric configuration as my best Logistic Regression Model but is good to keep in mind that if it were a model to work in real life,  `liblinear` is the best choice because of its fit time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Cros-Validate Score**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nbest_solver = 'newton-cg'\n\nres_liblinear = results[results['param_solver']==best_solver].sort_values(\n    ['mean_test_score','mean_fit_time'], ascending=[False,True] )\n\nlogReg_model_params = res_liblinear['params'].iloc[0]\n\nlogReg_model = LogisticRegression(**logReg_model_params,random_state=SEED)\n\nlogReg_cv_results = cross_validate(\n    logReg_model,\n    X = X,\n    y = y,\n    cv=KFold(n_splits = 10,shuffle=True,random_state=SEED)\n)\n\nlogReg_cv_score = logReg_cv_results['test_score'].mean()\nlogReg_cv_std = logReg_cv_results['test_score'].std()\n\nprint('Logistic Regression Classifier CV Score: %.2f%% ± %.2f%%' % (100*logReg_cv_score,100*logReg_cv_std))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test Data Score**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nlogReg = LogisticRegression(**logReg_model_params,random_state=SEED)\n\nlogReg.fit(X_trn,y_trn)\n\ny_pred = logReg.predict(X_tst)\n\nlogReg_score = accuracy_score(y_tst, y_pred)\nlogReg_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_Scores['logistic_regression'] = {\n    'model' : logReg,\n    'best_params' : logReg.get_params(),\n    'test_accuracy_score' : logReg_score,\n    'cv_score' : logReg_cv_score,\n    'cv_score_std' : logReg_cv_std\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> C. Using Classifier Class<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">↻</a></h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, I will do the same work using the <a href=\"#class-class\" style=\"color: #7a0723;\">classifier class</a>.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating the Classifier class\nlog = Classifier(\n         algorithm = LogisticRegression,\n         hyperparameters_range = {\n            'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n            'C' : [5,7,10,20,30,50,100]\n        }\n     )\n\nlog.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',log.best_model)\n\nsc_dict = log.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\nlog.fit(X_trn,y_trn,params = 'best_model')\npsc = log.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % psc)\n\nModel_Scores['logistic_regression'] = {\n    'model' : log.best_model,\n    'best_params' : log.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\nlog.grid_search_results.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-svc\"></a>\n\n<h2>5.3 Support Vector Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a Support Vector Classifier is:\n```\nSVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = Classifier(\n         algorithm = SVC,\n         hyperparameters_range = {\n                'kernel' : ['linear', 'poly','rbf','sigmoid'],\n                'C' : [0.1,0.5,1,3,7,10]\n            }\n     )\n\nsv.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',sv.best_model)\n\nsc_dict = sv.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\nsv.fit(X_trn,y_trn,params = 'best_model')\npsc = sv.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\nModel_Scores['svc'] = {\n    'model' : sv.best_model,\n    'best_params' : sv.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\nsv.grid_search_results.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-tree\"></a>\n\n<h2>5.4 Decision Tree Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a Support Vector Classifier is:\n```\nDecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n```","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dt = Classifier(\n         algorithm = DecisionTreeClassifier,\n         hyperparameters_range = {\n            'min_samples_split': [2,5,10],\n            'max_depth': [2,5,10],\n            'min_samples_leaf': [1,5,10]\n         }\n     )\n\ndt.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',dt.best_model)\n\nsc_dict = dt.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\ndt.fit(X_trn,y_trn,params = 'best_model')\npsc = dt.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\nModel_Scores['decision_tree'] = {\n    'model' : dt.best_model,\n    'best_params' : dt.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\ndt.grid_search_results.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-gnb\"></a>\n\n<h2>5.5 Gaussian Naive Bayes  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a Gaussian Naive Bayes is:\n```\nGaussianNB(*, priors=None, var_smoothing=1e-09)\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = Classifier(\n         algorithm = GaussianNB,\n         hyperparameters_range = {\n            'var_smoothing': [1e-09,1e-08,1e-07,1e-06,1e-05,4e-05,1e-04],\n         }\n     )\n\ngnb.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',gnb.best_model)\n\nsc_dict = gnb.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\ngnb.fit(X_trn,y_trn,params = 'best_model')\nprint('\\nAccuracy Score: %.2f ' % (gnb.predict_score(X_tst,y_tst)))\n\n\npd.options.display.float_format = '{:,.8f}'.format\n\nModel_Scores['gaussian_nb'] = {\n    'model' : gnb.best_model,\n    'best_params' : gnb.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\ngnb.grid_search_results.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-knn\"></a>\n\n<h2>5.6 K-Nearest Neighbors Classifier  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a  k-nearest neighbors  is:\n```\nKNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = Classifier(\n         algorithm = KNeighborsClassifier,\n         hyperparameters_range = {\n            'n_neighbors': [2,5,10,20],\n             'weights' : ['uniform', 'distance'],\n             'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n             'p' : [2,3,4,5]\n         }\n     )\n\nknn.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',knn.best_model)\n\nsc_dict = knn.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\nknn.fit(X_trn,y_trn,params = 'best_model')\npsc = knn.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\nModel_Scores['knn_classifier'] = {\n    'model' : knn.best_model,\n    'best_params' : knn.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\n\nknn.grid_search_results.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-rf\"></a>\n\n<h2>5.7 Random Forest Classifier  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>\n  \n  The complete call of a Random Forest Classifier is:\n```\nRandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = Classifier(\n         algorithm = RandomForestClassifier,\n         hyperparameters_range = {\n            'n_estimators': [5,10,15,20,25,30,50,100],\n             'random_state': [42]\n         }\n     )\n\nrf.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',rf.best_model)\n\nsc_dict = rf.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% ± %.2f%%' % (sc_list[0],sc_list[1]))\n\nrf.fit(X_trn,y_trn,params = 'best_model')\npsc = rf.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\nModel_Scores['random_forest'] = {\n    'model' : rf.best_model,\n    'best_params' : rf.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\n\nrf.grid_search_results.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ml-comp\"></a>\n\n<h2>5.8 Comparison of Models  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">↻</a></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([\n    [\n        key,\n        Model_Scores[key]['test_accuracy_score'],\n        Model_Scores[key]['cv_score'],\n        Model_Scores[key]['cv_score_std']\n    ]\n    \n    for key in Model_Scores\n],columns=['model','accuracy_score','cv_score','cv_score_std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sixth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC ↻</a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>6. Conclusion</h1>\n        </center>\n\n   \n</div>\nThe use of the `GridSearchCV` for hyperperparamers optimization proved to be very efficient since most of the algorithms are returning excellent results for predicting the target variable. Here, the model selection criteria points to the Support Vector Classifier since it exhibits the higher `cv_score`.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}