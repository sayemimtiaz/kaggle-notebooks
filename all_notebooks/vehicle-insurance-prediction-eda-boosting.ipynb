{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1> Vehicle Insurance Prediction ðŸš˜ ðŸš– </h1>\n    <h2> Ensemble Boosting Models </h2>\n<img src=\"https://doctorins.com/wp-content/uploads/2017/12/auto-insurance-banner.jpg\" width=\"1000\" >\n</center>\n\n<br><br>\n\n\n<h3>Navigate to<h3>\n    \n* [Problem Description](#section-zero)\n* [EDA and Visualization](#section-one)\n* [Feature Engineering](#section-two)\n* [Model Selection](#section-three)\n* [Hyperparameter Tuning](#section-four)\n* [Model Training](#section-five)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Model Selection and Metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, reset_parameter\nfrom sklearn.metrics import f1_score, recall_score, accuracy_score, roc_auc_score, precision_score, auc, roc_curve\n\n# Hyperparamter Tuning\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train =pd.read_csv('../input/health-insurance-cross-sell-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ntrain.head(500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# EDA and Visualization\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nplt.style.use('fivethirtyeight')\n\nplt.subplot(221)\ncolors = ['#1849CA', 'crimson']\nplt.title('Insurance Clients based on Gender',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\ntrain['Gender'].value_counts().plot(kind='pie', figsize=(8, 8), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(222)\ncolors = ['lightblue', 'crimson', 'pink', '#1849CA']\nexplode = [0, 0.075, 0, 0.075]\nplt.title('Health Insuranced Clients',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\nhealth = train[['Gender','Previously_Insured']].values.tolist()\nhealth = pd.DataFrame([h[0] + ' Insured' if h[1] == 1 else h[0] for h in health ],columns=['Gen_Ins'])\nhealth['Gen_Ins'].value_counts().plot(kind='pie', explode=explode, figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(223)\ncolors = ['#1849CA', 'crimson']\nplt.title('Vehicle damage based on Gender',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\ntrain['Vehicle_Damage'].value_counts().plot(kind='pie',figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\nplt.subplot(224)\ncolors = ['#1849CA', 'pink', 'lightblue', 'crimson']\nexplode = [0.075, 0, 0, 0.075]\nplt.title('Vehicle Insuranced Clients',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\nhealth = train[['Gender','Vehicle_Damage']].values.tolist()\nhealth = pd.DataFrame([h[0] + ' Insured' if h[1] == 'Yes' else h[0] for h in health ],columns=['Veh_Ins'])\nhealth['Veh_Ins'].value_counts().plot(kind='pie', explode=explode, figsize=(16, 16), rot=1, colors=colors, autopct = '%.2f%%')\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age is definitely affecting the Response as we can observe in below plots"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplot(311)\nplt.title('Age Distribution',fontsize=15)\nmen = train[train['Gender']=='Male']\nwomen = train[train['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.subplot(312)\nplt.title('Health Insured Clients Distribution',fontsize=15)\nhealth = train[['Gender','Previously_Insured','Age']]\nhealth = health[health['Previously_Insured'] == 1]\nmen = health[health['Gender']=='Male']\nwomen = health[health['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.subplot(313)\nplt.title('Vehicle Insured Clients Distribution',fontsize=15)\nhealth = train[['Gender','Response','Age']]\nhealth = health[health['Response'] == 1]\nmen = health[health['Gender']=='Male']\nwomen = health[health['Gender']=='Female']\na = sns.kdeplot(men['Age'], shade='True', legend='True', label='Male')\nb = sns.kdeplot(women['Age'], shade='True', legend='True', label='Female')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vehicle Damage is clearly related to Response, clients whose vehicle never damaged didn't opt for Vehicle Insurance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax = sns.catplot(data=train, x='Vehicle_Age', hue='Response', col='Vehicle_Damage', kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Annual Premium and Vintage effect is very less on Vehicle Insurance Response"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplot(211)\nplt.title('Effect of Annual Premium on Response',fontsize=15)\nax = sns.violinplot(data=train[train['Annual_Premium']<100000], y=\"Annual_Premium\", x=\"Response\")\nplt.subplot(212)\nplt.title('Effect of Vintage on Response',fontsize=15)\nbx = sns.violinplot(data=train, y=\"Vintage\", x=\"Response\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Feature Engineering\n\n* Need to encode categorical data to integers\n* We will Drop of Vintage and Annual Premium as we observe their impact on Response is not encouraging\n* Need to scale all parameters which makes easy for algorith to reach minima"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Gender_Code'] = pd.CategoricalIndex(train['Gender']).codes\ntrain['Vehicle_Age_code'] = pd.CategoricalIndex(train['Vehicle_Age']).codes\ntrain['Vehicle_Damage_code'] = pd.CategoricalIndex(train['Vehicle_Damage']).codes ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model_train = train[['Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n                   'Policy_Sales_Channel', 'Gender_Code',\n                   'Vehicle_Age_code', 'Vehicle_Damage_code']]\n\nscaler = StandardScaler()\n\nfor param in ['Age',\n              'Driving_License',\n              'Region_Code',\n              'Previously_Insured',\n              'Policy_Sales_Channel',\n              'Gender_Code',\n              'Vehicle_Age_code',\n              'Vehicle_Damage_code']:\n    model_train[param] = scaler.fit_transform(model_train[param].values.reshape(-1, 1))\n    \nX_train, X_test, y_train, y_test = train_test_split(model_train, train['Response'], test_size = 0.2, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Model Selection \n\n\n    \n* Problem can be identified as Binary Classification (wheather customer opts for vehicle insurance or not)\n* Dataset has more than 300k records\n* cannot go with SVM Classifier as it takes more time to train as dataset increase\n* Idea is to start selection of models as:\n\n    1. Decision Tree\n    2. Random Forest\n    3. SGD\n    4. Gradient Boost\n    5. XG Boost\n    6. Cat Boost\n    7. LGBM\n    \n\n    \n    \n    \n## Boosting Models\n\n### Boosting is one of the techniques that uses the concept of ensemble learning. A boosting algorithm combines multiple simple models"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = {\n    \"Decision Tree\": DecisionTreeClassifier(), \n    \"SGD\" : SGDClassifier(), \n    \"Random Forest\" : RandomForestClassifier(), \n    \"Gradient Boosting\" : GradientBoostingClassifier(),\n    \"XGBoost\" : XGBClassifier(),\n    \"CatBoost\" : CatBoostClassifier(),\n    \"LGBM\" : LGBMClassifier()\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"scores = []\nprob_score = {}\nfor mod in model:\n    classifier = model[mod]\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    try:\n        score = classifier.predict_proba(X_test)[:,1]\n        roc = roc_auc_score(y_test, score, average='weighted')\n        prob_score[mod] = score\n    except:\n        roc = 0\n    scores.append([\n        mod,\n        accuracy_score(y_test, pred),\n        f1_score(y_test, pred, average='weighted'),\n        precision_score(y_test, pred, average='weighted'),\n        recall_score(y_test, pred, average='weighted'),\n        roc\n    ])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def highlight_max(s):\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\n\nscores_df  = pd.DataFrame(scores)\nindex_model = {count: s for count, s in enumerate(scores_df[0])}\ncol = {count+1: s for count, s in enumerate(['Accuracy','F1 Score','Precision','Recall','ROC AUC'])}\nscores_df = scores_df.drop(0, axis=1)\nscores_df = scores_df.rename(columns=col, index=index_model)\nscores_df.style.apply(highlight_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.title('ROC Curves of Classifiers')\nplt.xlabel('Precision')\nplt.ylabel('Recall')\n\nfor key in prob_score:\n    fpr, tpr, _ = roc_curve(y_test, prob_score[key])\n    plt.plot(fpr, tpr, label=key)\n\nplt.plot((0,1), ls='dashed',color='black')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Hyperparameter Tuning\n\n### Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained. \n\n\n> ### We select **LGBM** Model for hypertuning because it has highest ROC Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n\n\n\nclf = LGBMClassifier(max_depth=-1, random_state=15, silent=True, metric='None', n_jobs=4, n_estimators=5000)\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=100,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=15,\n    verbose=True)\n\n\n# Uncomment to perform Randomsearch\n# gs.fit(X_train, y_train, **fit_params)\n# print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n\nRandomsearch_params = {'colsample_bytree': 0.6261473679815167, 'min_child_samples': 237, 'min_child_weight': 0.001, 'num_leaves': 28, 'reg_alpha': 10, 'reg_lambda': 10, 'subsample': 0.7567691135431514} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Model Training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\n#set optimal parameters\nclf_sw = LGBMClassifier(**clf.get_params())\nclf_sw.set_params(**Randomsearch_params)\nclf_sw.fit(X_train,y_train, **fit_params, callbacks=[reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n## The ROC AUC score using LGBM is around 0.86\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}