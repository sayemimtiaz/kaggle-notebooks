{"cells":[{"metadata":{"_uuid":"ce168384a18ac6a0c14b4702b2009dd8a1b59522"},"cell_type":"markdown","source":"# An Analysis of UCI Mushroom Data\n\n\n## Disclaimer\n\nUnless you want to end up like the protagonist from _Into the Wild_, i.e. dead, I wouldn't recommend using the results from this data analysis to actually hunt mushrooms. This data set only represent 22 species and most likely can't be generalized. A quick google search reveals that there are at least 14,000 known species of mushrooms, 10,000 in North America alone. Remember, this only an exercise, not a definitive primer on how to identify edible mushrooms. \n\n\n## The Data\n\nThe mushroom dataset was donated to the University of California Irvine Machine Learning Repository in 1987. It consists of 8124 hypothetical samples which correspond to 23 species of gilled mushrooms in the Agaricus and Lepiota genera. The samples are constructed from The Audubon Society Field Guide to North American Mushrooms (1981). The samples are labeled as either 'e' or 'p' for edible or poisonous. Mushrooms which were of dubious or unknown edibility were incorporated as poisonous. The dataset includes 22 predictor variables which correspond to information regarding physical, visual and olfactory features of each specimen as well as habitat and growth pattern information.\n\n## Data Preprocessing:\n\nInitially the data is represented categorically however <code>sklearn</code> is incompatible with categorical data, therefore the data is first converted into dummy variables. We do this using the <code>pandas.get_dummies</code> function. <code>get_dummies</code> provides each category of data with its own column and replaces strings with 0s and 1s. <code>get_dummies</code> expands the number of independent variable columns from 22 to 117."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import modules\nimport pandas as pd\nimport numpy as np\n\n#read csv to pandas dataframe\nmushrooms = pd.read_csv(\"../input/mushrooms.csv\")\n\n#create dummy variables\nmushrooms = pd.get_dummies(mushrooms)\n\n#subset data into dependent and independent variables x,y\nLABELS = ['class_e', 'class_p']\nFEATURES = [a  for a in mushrooms.columns if a not in LABELS ]\ny = mushrooms[LABELS[0]]\nx= mushrooms[FEATURES]\n\nmushrooms.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab6e89752e7d1a237a8a5273b88552289a1c994"},"cell_type":"markdown","source":"## Visual EDA\n\n\n### Visualizing clusters with T-SNE\n\nVisual EDA (exploratory data analysis) is the first step in our data science pipeline. However, our data has too many dimensions to visualize directly. Luckily, smart people have figured out methods for representing higher dimensional data in lower dimensional spaces which can be visualized. One such method is [t-sne](https://www.youtube.com/watch?v=NEaUSP4YerM) or t-Distributed Stochastic Neighbor Embedding, a state of the art visualization method which is especially adept at preserving clustering. The plot below was constructed using <code>sklearn.manifold.TSNE</code> by setting the <code>learning_rate</code> parameter to 100. The t-sne plot separates the sample into clusters with color representing distinct classes, i.e. poisonous or edible. Here we observe a good separation between yellow and purple clusters. Therefore, we expect a machine learning algorithm to distinguish between edible and poisonous mushrooms with good accuracy.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import modules\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n#create model\nmodel = TSNE(learning_rate = 100)\n\n#fit model\ntransformed = model.fit_transform(x.values)\nxs = transformed[:,0]\nys = transformed[:,1]\ndf_trans = pd.DataFrame({'xs':xs, 'ys':ys})\n\n#create plots\nplt.scatter(df_trans.loc[y==0]['xs'], df_trans.loc[y ==0]['ys'], c= 'tab:green')\nplt.scatter(df_trans.loc[y ==1]['xs'], df_trans.loc[y ==1]['ys'], c= 'tab:blue')\nplt.legend(loc ='lower left', labels = ['p', 'e'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a638c582c054aaa0d9fd43c382e27f633a203cd"},"cell_type":"markdown","source":"### Exploring important variables with Lasso\n\n[Lasso regression](https://www.youtube.com/watch?v=NGf0voTMlcs) is a type of linear regression which is used to prevent overfitting of data by penalizing high linear coefficients. One interesting property of Lasso compared to other similar techniques is that it tends to drop coefficients of unimportant variables to exactly zero, which gives it utility as a means to identify the most important variables in high dimensional problems. The plots below were constructed using <code>sklearn.linear_model.Lasso</code> with respective alphas of .1 and .001. Alpha or lambda is a parameter which sets the degree of the aforementioned penalty. The higher the alpha, the less sensitive Lasso will be to the data, the more variables coefficients will tend towards zero. With an alpha of .1, the plot reveals a single distinct spike in a field of zeroes corresponding to the property odor_n or odor of 'none'. At an alpha of .001, other variables emerge including odor_l (odor = 'anise') and odor_a (odor = 'almond'), which count towards edibility, as well as spore-print-color_r (color = 'green') and stalk-color-below-ring_y (color = yellow), which count against edibility. However, because these other variables did not appear at the higher alpha they are also less important overall. Odor_n is clearly the most important variable in our dataset. For instance, if we ignored every other variable and sorted the data into edible and poison categories based on whether the mushrooms exhibited an odor alone, we would be right 88.7% of the time with a false positive rate of 3.4%, which is pretty remarkable. Of course, a false positive rate of 3.4% is unacceptable in the scenario so we still need to consider other factors if we don't want to be poisoned eventually. \n"},{"metadata":{"trusted":true,"_uuid":"2550e6361ab202fd786de7e904d3ff60a485f823"},"cell_type":"code","source":"#import modules\nfrom sklearn.linear_model import Lasso\n\n#create model, set alpha\nlasso = Lasso(alpha = 0.1)\n\n#fit model, access coef_ attribute: lasso_coef\nlasso_coef = lasso.fit(x,y).coef_\n\n#create plot\n_=plt.plot(range(len(x.columns)), lasso_coef)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eff07985a76541b83a2951938aa1a29732bacdc3"},"cell_type":"code","source":"#import modules\nfrom sklearn.linear_model import Lasso\n\n#create model, set alpha\nlasso = Lasso(alpha = 0.001)\n\n#fit model, access coef_ attribute: lasso_coef\nlasso_coef = lasso.fit(x,y).coef_\n\n#create plot\n_=plt.plot(range(len(x.columns)), lasso_coef)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd684e142b97dd209dad7d3a6c69b625cc4e2952"},"cell_type":"markdown","source":"## Supervised Learning: KNN and Decision Trees\n\n\n### Training a KNN model\n\n[K-nearest neighbors](https://www.youtube.com/watch?v=HVXime0nQeI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=24&t=0s) (knn) is an algorithm used in classification problems, which predicts test data based on a tally of its k-nearest neighbors in the training data. Here we use sklearn's <code>GridSearchCV</code> to tune a K-nearest neighbors model. First we split the data into test data (30%) and training data (70%)  and define a parameter grid for possible values of k, in this case 1-9. Gridsearch by default uses three fold [cross-validation](https://www.youtube.com/watch?v=fSytzGwwBVw&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=3&t=0s) on each of the values in the parameter grid and saves the model with the best parameters, in this case k=1. Finally we test the model on the training data.  As you can see from the output below our knn model performs perfectly on the test data. Here precision and recall are outputted instead of accuracy. A precision of 1 means that the model did not output any false positives, i.e. predicted edible when in fact poisonous. A recall of 1 means that the model also did not output any false negatives, i.e. predicted poisonous when in fact edible."},{"metadata":{"trusted":true,"_uuid":"29eef0a3e0933c267dea1c1ad38fc07036ce88da"},"cell_type":"code","source":"#import modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n#create parameter grid for values of k\nparameters = {'knn__n_neighbors': np.arange(1,10)}\n\n#instatiate pipeline with KNNClassifier: pl\npl = Pipeline([('knn', KNeighborsClassifier())])\n\n#split data into test and training data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n\n#instatiate GridsearchCV:cv\ncv = GridSearchCV(pl, param_grid= parameters, cv = 3)\n\n#fit model to training data\ncv.fit(X_train, y_train)\n\n#predict test data: y_pred\ny_pred = cv.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"571258e26444fad2938d1c9ba49306a785e5baef"},"cell_type":"code","source":"#print performance metrics\nprint (cv.best_params_)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"539d3354849ce955e5b33c8999584519bd5b1d32"},"cell_type":"markdown","source":"### Decision Tree\n\nA [Decision Tree](https://www.youtube.com/watch?v=7VeUPuFGJHk) is a model which uses a series of specially selected questions to iteratively optimize the classifications of data. When data contains mixed classes, it is called 'impure'. The learning algorithm selects questions at each node which minimize a measure of impurity,  compared to previous nodes. The decision tree was constructed using sklearn's <code>DecisionTreeClassifier</code> and the graphic representation was made using <code>graphviz</code>.  The first node evaluates whether the sample has the property odor = n, if it does it is classifies the sample as edible if not it classifies the sample as poisonous. The reason the algorithm picked odor_n for the root node is because evaluating this property maximized the separation between poisonous and edible mushrooms as measured by by the Gini impurity metric. For instance, at the root node the training data had a [2951,2735] division between edible and poisonous mushrooms respectively, after the first split the first edible internal node now contains 2141 edible samples and only 81 poisonous samples with a Gini of .063 down from .499. Similarly the first poisonous internal node now contains 537 edible samples and 2654 poisonous samples with a Gini of 0.28. According to representation below, the model is able to perfectly split the training data into edible and poisonous categories as noted by ginis of 0 at the end leaf nodes. Further, the model generalized perfectly to unseen test data split prior to training. \n\nArmed with this trusty decision tree, we can hunt mushrooms without fear of ingesting poison (not in real life!). When in doubt we need only consult the decision tree to determine if a mushroom is edible or poisonous. For instance, we will know that a mushroom with an odor of 'none', a broad gill size and which doesn't have a green spore print, a scaly stalk surface below the ring, a grooved cap surface, or a conical cap shape, will always be edible. This rule is a good one to know since it encompasses 76% of the edible mushrooms in our dataset. Aggregated from 8124 samples in 118 dimension and condensed into one simple chart that can easily be stuffed in a pocket or memorized. Pretty cool!"},{"metadata":{"trusted":true,"_uuid":"d06e9826d20ab3185ce7f71b19a890735133dc11"},"cell_type":"code","source":"#import modules\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.tree import export_graphviz\n\n\n#instantiate DecisionTreeClassifier:tree\ntree = DecisionTreeClassifier()\n\n#split data into test and training data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n\n#fit data\ntree.fit(X_train, y_train)\n\n#predict test data:y_pred\ny_pred=tree.predict(X_test)\n\n#print performance metrics\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56357fb03af27dff95ae80cea2eb412bd78eab94"},"cell_type":"code","source":"import graphviz\n\n#export a graphic representation of tree to file\ndot_data =export_graphviz(tree, out_file = None, feature_names =x.columns, class_names = ['edible', 'poisonous'])\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a74a442193ce57536b390ddf95c8771a9c7fcff"},"cell_type":"markdown","source":"\n## Unsupervised Learning: How many species of mushrooms are in our data set?\n\n\n### Exploring the question\n\nHow many species of mushrooms are in our dataset? We already know the answer to this question. The description of the dataset states that the hypothetical sample corresponds to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. However, description aside, could we arrive at this conclusion independently based solely on the structure of the data? Further, can we restore species labels to the data? One strategy is to count the different clusters in our t-sne plot, assuming that different species are represented by different clusters. If we do this we get 23 clusters. This method however is subjective. It is unclear whether some smaller closely spaced clusters should be counted together, or whether some larger clusters should be broken up. Subjectivity, aside, counting the clusters on the t-sne is an important first step. By the time we are done with t-sne we have a good ballpark estimate as to what the answer will be. The rest is just confirming in what we already kind of know. One downside with the t-sne approach is that it doesn't restore species labels to the samples themselves. Therefore we proceed to cluster analysis. \n\n\n### Cluster Analysis: K-means and hierarchical clustering\n\nWith cluster analysis there are many approaches we could take. Here we try three using [k-means](https://www.youtube.com/watch?v=4b5d3muPQmA) and [hierarchical clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo), picking the one that performs best according to a chosen performance metric. Assuming that species are homogenous with respect to edibility, we define the best clustering technique as the one that optimizes homogeneity while minimizing the total number of clusters. In other words, we assume that a given species will only contain members which are all 'edible' or all 'poisonous', but not both. A homogeneity score is calculated using sklearn's <code>homogeneity_completeness_v_measure</code>. The function evaluates homogeneity of cluster labels given a ground truth, in this case edibility labels. It returns a homogeneity of 1 if the cluster labels perfectly separate the edibility labels. The function also calculates two other metrics which are less relevant in this situation given that we expect multiple species to exhibit both edibility and toxicity. For each model type we loop through parameters which alter the number of clusters in the model. For each iteration of the loop we append the number of clusters to list xp, and the homogeneity score to list yp. Once the loop is complete xp and yp are plotted using <code>matplotlib.pyplot</code>.\n\nThe two clustering techniques used here are k-means and hierarchical clustering. K-means was implemented using <code>sklearn.cluster.KMeans</code>. K-means is a top down approach. The model receives the number of desired clusters and attempts to optimize the clusters by minimizing the sum of squared distances of samples to closest cluster centers, called inertia. Because the k-means iterative optimization process is highly dependent on randomly selected starting points for cluster centers, the algorithm typically is repeated several times to find the best clustering. By default in sklearn <code>n_init</code> will run the algorithm ten times with different centroid seeds and return the model with the lowest inertia.\n\nHierarchical clustering is a bottom up approach. The algorithm iteratively merges samples and clusters based on which samples are clusters are most similar. How ‘similar’ is defined is a matter of choice. Here we try two approaches: i) clustering based on an average pairwise distance between points in different clusters and ii) based on the centroid distance between two clusters. In this case we set the number of clusters indirectly by specifying a distance threshold which prevents clusters with similarity measures above the specified threshold from merging. Hierarchical clustering was implemented using <code>scipy.cluster.hierarchy.fcluster</code> and <code>scipy.cluster.hierarchy.linkage</code>. First we instantiate a linkage matrix with our data and define the desired linkage method. By default <code>linkage</code> defines distance as the euclidean distance. Then we pass the linkage matrix and our desired threshold criteria to <code>fcluster</code> which returns an array of cluster labels corresponding to our data. With <code>fcluster</code> instead of directly looping through the number of desired clusters as we did with k-means we instead loop through different distance threshold criteria, which indirectly vary the number of clusters. By iteratively decreasing the distance threshold we effectively increase the number of clusters. Again after each iteration we append the number of clusters to xp and the homogeneity score to yp and plot the result.\n\nAs can be seen from the plots, the different clustering methods provide similar ranges for the number of species. K-means performs the worst, since it never achieves 100% homogeneity instead leveling off at about 97%. That said, the results of k-means indicate that the number of species contained in our dataset is around 25 which is not too far off from the truth. Hierarchical clustering using  ‘average’ and ‘centroid’ criteria do slightly better. Using a linkage method of ‘average’ hits 100% of homogeneity at 24, while ‘centroid’ hits 100% homogeneity at 23. Based on our previously stated criteria of maximizing homogeneity while minimizing total clusters, the centroid method is the winner. Further, it predicts 23 clusters which is commensurate with the number of species we know to be true. So even if we didn’t know the answer to this particular question, based on cluster analysis we would still have a good reason to believe that the dataset contained 23 species based on a cluster analysis of the data. Even better, we now have best guess species labels for each of the samples in our dataset. Pretty cool. \n\n"},{"metadata":{"trusted":true,"_uuid":"143f7d893cf76b896379ccbfe5c44184467b66c9"},"cell_type":"code","source":"#import modules\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\n\n#instantaiate lists\nxp=[]\nyp=[]\n\n# fit KMeans model for n_clusters 2:30\nfor i in range(2,30):\n    model = KMeans(n_clusters = i, random_state = 42)\n    model.fit(x.values)\n    labels = model.predict(x)\n    scores =metrics.homogeneity_completeness_v_measure(y, labels)\n    xp.append(i)\n    yp.append(scores[0])\nplt.plot(xp,yp)\nplt.title('Kmeans Clustering')\nplt.xlabel(s = 'Number of Clusters')\nplt.ylabel(s = 'Homogeneity Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035451bc5fd308dc3a973c23b59c44b623cfe696"},"cell_type":"code","source":"#import modules\nfrom scipy.cluster.hierarchy import linkage,  fcluster\n\n#instantiate linkage with 'average'\nmerging = linkage(x, method = 'average')\n\n#create empty lists\nxp=[]\nyp=[]\n\n#modify distance threshold of clustering and append cluster number and homogeneity scores to xp, yp\nfor i in range(30):\n    labels = fcluster(merging,6-i*.1, criterion= 'distance')\n    scores =metrics.homogeneity_completeness_v_measure(y, labels)\n    xp.append(len(np.unique(labels)))\n    yp.append(scores[0])\n\n#plot number of cluster vs homogeneity score\nplt.plot(xp,yp)\n\nplt.title('Hierarchical Clustering - Average')\nplt.xlabel(s = 'Number of Clusters')\nplt.ylabel(s = 'Homogeneity Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da2c720eacaf589655ec65a7e9ba36dbb4fd14b2"},"cell_type":"code","source":"#instantiate linkage with 'centroid'\nmerging = linkage(x, method = 'centroid')\n\n#create empty lists\nxp=[]\nyp=[]\n\n#modify distance threshold of clustering and append cluster number and homogeneity scores to xp, yp\nfor i in range(40):\n    labels = fcluster(merging,6-i*.1, criterion= 'distance')\n    scores =metrics.homogeneity_completeness_v_measure(y, labels)\n    xp.append(len(np.unique(labels)))\n    yp.append(scores[0])\n\n#plot number of cluster vs homogeneity score\nplt.plot(xp,yp)\n\nplt.title('Hierarchical Clustering - Centroid')\nplt.xlabel(s = 'Number of Clusters')\nplt.ylabel(s = 'Homogeneity Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15cf319941c5cf15df92bd3a2935e54e0adbb08b"},"cell_type":"markdown","source":"###  T-sne revisited\n\nWe can use t-sne to visualize how well our chosen clustering performs on the data. As before, we plot the transformed data with the points color coded according to their edibility. Additionally we annotate the plot with the species labels commensurate with the cluster centers. The plot below confirms that the calculated cluster centers for each species label visually correspond with the apparent clusters on the t-sne plot. The t-sne plot also allows us to assess the edibility and the size of the different species. For instance, the largest edible species is 16 followed by 13 and 21, whereas the largest poisonous species are 9 and 5. In fact species 16, 13 and 21 comprise 71% of all the edible samples in the dataset while  9 and 5 comprise 77% of all poisonous mushrooms in the dataset. Therefore learning to identify these species would be a good return on investment for our hypothetical mushroom hunters.\n"},{"metadata":{"trusted":true,"_uuid":"699b8259dd892ed13d86a396312a8f15a5078875"},"cell_type":"code","source":"#create species labels using optimal clustering parameters\nspecies = fcluster(merging,2.3, criterion= 'distance')\n\n#transform mushroom data using-tsne\nmodel = TSNE(learning_rate = 100)\ntransformed = model.fit_transform(x.values)\nxs = transformed[:,0]\nys = transformed[:,1]\ndf_trans = pd.DataFrame({'xs':xs, 'ys':ys})\n\n#determine centroid locations of t-sne clusters\ncpx =[]\ncpy=[]\n\nfor i in range(1,24):\n    xi = df_trans.loc[(species ==i)]['xs'].mean()\n    yi = df_trans.loc[(species ==i)]['ys'].mean()\n    cpx.append(xi)\n    cpy.append(yi)\n\n#plot edible and poisonous samples with different colors\nplt.scatter(df_trans.loc[y==0]['xs'], df_trans.loc[y ==0]['ys'], c= 'tab:green' )\nplt.scatter(df_trans.loc[y ==1]['xs'], df_trans.loc[y ==1]['ys'], c= 'tab:blue')\n\n#annotate clusters centroids for each species\nfor i in range(1,24):\n    plt.annotate(s = str(i), xy = (cpx[i-1], cpy[i-1]), xytext = (-4,-4), textcoords = 'offset points')\n\n#insert legend with labels 'p', 'e'\nplt.legend(loc ='lower left', labels = ['p', 'e'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da694941c102914c1237d6821d2a43baf8ac7d82"},"cell_type":"markdown","source":"## Conclusion and next steps\n\nI think this is a good place to stop for now. We've learned a lot about our dataset. We started by using t-sne to visualize the dataset in a way that otherwise would have been impossible, we used lasso regression to determine that the most important predictor of edibility in our dataset is lack of smell, we trained knn and decision tree models which generalized to unseen test data and we used cluster analysis to restore species labels, which were not included in the original dataset. Now that we have species labels we can extend our inquiries further. We can determine the distinctive characteristics of each species and create data sheets and decision trees to assist in identification of species individually. We can create a taxonomy, based on similarity to determine which species are most related. We can investigate which characteristics are most likely to vary between species and which one are most likely the vary within species. Perhaps I will revisit some of these inquiries in the future.\n\nAlso if you enjoyed this analysis, check out my blog at [https://www.notnormalblog.com](https://www.notnormalblog.com). "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}