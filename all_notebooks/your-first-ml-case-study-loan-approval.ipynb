{"cells":[{"metadata":{"id":"ymmGx-Mj-J3S"},"cell_type":"markdown","source":"# **Loan-Prediction Case Study**\n"},{"metadata":{},"cell_type":"markdown","source":"This a beginner-friendly case study on [loan-prediction dataset](https://www.kaggle.com/ninzaami/loan-predication)\n\n## Major topics discussed:\n1. Data Preprocesing\n2. Data Visualization\n3. Pipelines\n4. GridSearch\n5. Build Custom Estimators\n\nBase Framework borrowed from : [Sagidur's Kernel](https://www.kaggle.com/sazid28/home-loan-prediction), consider visiting this also.\n"},{"metadata":{"id":"gdQGVA8OH1hj"},"cell_type":"markdown","source":"# **Importing necessary packages and data**"},{"metadata":{"id":"oxjm9Wfg-H9G"},"cell_type":"markdown","source":"Importing neccessary Packages"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Packages used\n!pip install numpy pandas scikit-learn seaborn yellowbrick category_encoders xgboost","execution_count":null,"outputs":[]},{"metadata":{"id":"NWto3Hxt8h5S","trusted":true},"cell_type":"code","source":"import warnings                                                     # To ignore any warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np                                                  # To perform numerical operations on vectors/arrays.\nimport pandas as pd                                                 # To store and manupulate dataset\nimport matplotlib.pyplot as plt                                     # To visualize data\nimport seaborn as sns                                               # To visualize data\nfrom yellowbrick.target import FeatureCorrelation                   # To plot correlation bar chart\nfrom sklearn.model_selection import train_test_split                # To split data to train and test set\nfrom sklearn.preprocessing import  OrdinalEncoder                   # To encode data\nfrom category_encoders import TargetEncoder                         # To encode data\nfrom sklearn.impute import SimpleImputer                            # To impute data\nfrom sklearn.pipeline import Pipeline                               # To build preprocessing and prediction pipeline\nfrom sklearn.compose import ColumnTransformer                       # To apply different tranformation on different columns parallely\nfrom sklearn.base import BaseEstimator, TransformerMixin            # For creating custom estimator\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score                          # Evalution metric that we will be using\nfrom sklearn.model_selection import GridSearchCV, cross_val_score   # To select a descent model\n\nfrom string import ascii_uppercase                                  # To get alphabetical labels\n\nnp.random.seed(42)                                                  # To have same random values on each run.\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"QYm1lvUY-7G7"},"cell_type":"markdown","source":"**Reading the train data**"},{"metadata":{"id":"OhPQmjgtAWef","outputId":"a4d9d7a7-cdc6-4c67-fc41-78a2c03da37f","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv')\ndata.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"8GselBChAPV6"},"cell_type":"markdown","source":"**Reading the test data**"},{"metadata":{"id":"GLW-GqtGArYA","trusted":true},"cell_type":"code","source":"X = data.drop(['Loan_ID','Loan_Status'], axis=1)\ny = data['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"id":"KyQqI8Knd8Fh","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"id":"n08cetcdHxjr","outputId":"ff8d18af-a3d3-4e99-a4d5-a9975bca0452","trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ggq7TM0_A3ma"},"cell_type":"markdown","source":"# **Understanding the Data**\n\nIn this section, we will look at the structure of the train and test datasets. Firstly, we will check the features present in our data and then we will look at their data types.\n"},{"metadata":{"id":"thJq1nyEAyzN","outputId":"536f580a-8844-4142-f347-2188ec2fa261","trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"X4B_14bODCC5"},"cell_type":"markdown","source":"We have 12 independent variables and 1 target variable, i.e. Loan_Status in the train dataset."},{"metadata":{"id":"ZwHfVRY2HGxj"},"cell_type":"markdown","source":"We will predict the Loan_Status using the model built using the train data."},{"metadata":{"id":"FPG_511mHOh5"},"cell_type":"markdown","source":"### Given below is the description for each variable.\n\n##### **Variable\tDescription** \n\n* **Loan_ID**:\tUnique Loan ID\n* **Gender**:\tMale/ Female\n* **Married**:\tApplicant married (Y/N)\n* **Dependents**:\tNumber of dependents\n* **Education**:\tApplicant Education (Graduate/Under Graduate)\n* **Self_Employed**:\tSelf employed (Y/N)\n* **ApplicantIncome**:\tApplicant income\n* **CoapplicantIncome**:\tCoapplicant income\n* **LoanAmount**:\tLoan amount in thousands\n* **Loan_Amount_Term**:\tTerm of loan in months\n* **Credit_History**:\tCredit history meets guidelines\n* **Property_Area**:\tUrban/ Semi Urban/ Rural\n* **Loan_Status**:\tLoan approved (Y/N)\n"},{"metadata":{"id":"sD56V-87G58B","outputId":"edb376cb-0dbb-4b47-daab-a0687d01a9f3","trusted":true},"cell_type":"code","source":"# Print data types for each variable\nX_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"ggtOV7LNH4h3","trusted":true},"cell_type":"code","source":"categorical_features = ['Gender', 'Married', 'Dependents', 'Credit_History',\n                        'Self_Employed' , 'Property_Area', 'Education']\nnumerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n                      'Loan_Amount_Term'] \n\n# Eye-rolling over DataFrame sample you could obseverve tha 'Credit_History'\n# is actually a categorical value.","execution_count":null,"outputs":[]},{"metadata":{"id":"Kl-GXAy8H5xA"},"cell_type":"markdown","source":"Let’s look at the shape of the dataset."},{"metadata":{"id":"CQPJUv8X6bwK"},"cell_type":"markdown","source":"### Checking whether there exists null values"},{"metadata":{"id":"J3NiTrYY6RKB","outputId":"7b4e4832-6368-4198-9ab1-2a298e6e3fb8","trusted":true},"cell_type":"code","source":"X_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"YOR3hsHEDUsL"},"cell_type":"markdown","source":"So, we have null values in our dataset and will have to impute them. We'll do that based upond decision made during **Univariate Analysis**."},{"metadata":{"id":"hebTkUwRIaeH"},"cell_type":"markdown","source":"## **Univariate Analysis:**\n\nIn this section, we will do univariate analysis. It is the simplest form of analyzing data where we examine each variable individually.\n\nFor categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable. \n\nFor numerical features, probability density plots can be used to look at the distribution of the variable."},{"metadata":{"id":"lj5KM5ONIrP2"},"cell_type":"markdown","source":"### Visualizing Target Variable\n\nWe will first look at the target variable, i.e., **Loan_Status**. \n\nAs it is a categorical variable, let us look at its frequency table, percentage distribution and bar plot.\n\nFrequency table of a variable will give us the count of each category in that variable.\n"},{"metadata":{"id":"FP7S6gbuI-3y","outputId":"c50b2eed-ed88-48d6-c3c7-dcf62cff46a0","trusted":true},"cell_type":"code","source":"sns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"zCQqCU_8ELmU"},"cell_type":"markdown","source":"*So our datasets is **unbalanced** and have a Yes:No ratio as 2:1 (approx)*"},{"metadata":{"id":"gqv3HvBcJUj8"},"cell_type":"markdown","source":"### Visualizing categorical features\n"},{"metadata":{"id":"1hED6mqVK7rV","outputId":"e508c596-6c25-47bb-9b0f-c4584053e4eb","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 10))\n\nfor i,x in enumerate(categorical_features):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(X_train[x])\n\nplt.suptitle(\"Categorical Features\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sFNDi7WiJ5l9"},"cell_type":"markdown","source":"\nIt can be inferred from the above bar plots that:\n* Majority loan applicants in the dataset are male.\n* Around 65% of the applicants in the dataset are married.\n* Ratio of aplicants with dependents and without is roughly 7:5\n* Around 80% of the applicants hold a graduate degree.\n* Only about 15% applicants in the dataset are self employed.\n* Among applicants distribution of people living in urban, semi-urban and rural is pretty much similar.\n* Around 85% applicants have repaid their debts.\n"},{"metadata":{"id":"8TcJ00GzO_4U"},"cell_type":"markdown","source":"### Visualizing numerical features"},{"metadata":{"id":"LUAaFbcVPD8N","outputId":"1c419970-fbc8-4ee0-a15a-e6b1744e43ad","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 10))\n\nfor i,x in enumerate(numerical_features):\n    plt.subplot(2, 2, i+1)\n    sns.distplot(X_train[x], kde_kws={'bw':3})\n\nplt.suptitle(\"Numerical Features\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"PUpqAvfKKqrM"},"cell_type":"markdown","source":"It can be seen that in the distributions of **'ApplicantIncome', 'CoapplicantIncome' and 'LoanAmount'** graph is normal distribution skewed towards right. And in case of **Loan_Amount_Term** we see a high concentration around 350 months.\n\nAs algorithms works better if the data is normally distributed, we will either cap this or cut the data into bins(i.e; making numerical to categorical)"},{"metadata":{"id":"ZUb_j5JTe-CA"},"cell_type":"markdown","source":"Lets recall some of the hypotheses that we generated earlier:\n\n•\tApplicants with high income should have more chances of loan approval.\n\n•\tApplicants who have repaid their previous debts should have higher chances of loan approval.\n\n•\tLoan approval should also depend on the loan amount. If the loan amount is less, chances of loan approval should be high.\n\n•\tLesser the amount to be paid monthly to repay the loan, higher the chances of loan approval.\n\nLets try to test the above mentioned hypotheses using bivariate analysis\n\nAfter looking at every variable individually in univariate analysis, we will now explore them again with respect to the target variable.\n"},{"metadata":{"id":"EE_RzW1pTHqq"},"cell_type":"markdown","source":"## **Bivariate analysis with respect to output(Loan_Status)**"},{"metadata":{"id":"cdD1RQ-UTT8W"},"cell_type":"markdown","source":"### Categorical Features"},{"metadata":{"id":"NQ6FSSHyL5bx","outputId":"9beda9aa-5ffd-4148-d956-4f41fc0dbb6f","trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=4)\nfig.set_figheight(12)\nfig.set_figwidth(20)\n\ni=0\n\nfor j,x in enumerate(categorical_features):\n    if j>3:\n        i=1; j-=4\n    _crs_tab = pd.crosstab(X_train[x].dropna(), y_train)\n    _crs_tab.div(_crs_tab.sum(1).astype(float), axis=0).plot(kind=\"bar\",\n                                                             stacked=True,\n                                                             ax=axes[i,j])\n\nplt.suptitle(\"Categorical Features\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"id":"qio1SItuhA7O"},"cell_type":"markdown","source":"* It seems people with credit history as 1 are more likely to get their loans approved.\n* For all other features approval distribution amongst various classes is more or less the same. "},{"metadata":{"id":"26WDd6BEhJfw"},"cell_type":"markdown","source":"### Numerical Features"},{"metadata":{"id":"majksn4kN-TP"},"cell_type":"markdown","source":"For the ease of visualization and further in preprocessing to handle outliear we will cut the continous varianble into ordinal bins."},{"metadata":{"id":"LZkXuDo4PAPz","outputId":"f709ec03-e887-4fc5-e501-54f789d84495","trusted":true},"cell_type":"code","source":"print(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{"id":"8Dm-xCWTgK20","trusted":true},"cell_type":"code","source":"#list of binned numerical feature vectors \nbinned_nums = []\n\n## ApplicantIncome ##\nAIbins = [0, 1000, 2000, 3000, 4000, 6000, 8000, 15000, 20000, 40000, 81000]\nbinned_nums.append(pd.cut(X_train['ApplicantIncome'], bins=AIbins,\n                          labels=list(ascii_uppercase[0:10])))\n\n## CoapplicantIncome ##\nCIbins = [0, 500, 1000, 2000, 3000, 4000, 5000, 8000, 12000, 20000, 50000]\nbinned_nums.append(pd.cut(X_train['CoapplicantIncome'], bins=CIbins,\n                          labels=list(ascii_uppercase[0:10])))\n\n## LoanAmount ##\nLAbins = [0, 50, 100, 200, 500, 1000]\nLAlabels = ['very_small_sum', 'small_sum', 'moderate_sum',\n            'large_sum', 'very_large_sum']\nbinned_nums.append(pd.cut(X_train['LoanAmount'], bins=LAbins, labels=LAlabels))\n\n\n## Loan_Amount_Term ##\nLATbins = [0, 60, 360, 600]\nLATlabels = ['short_term', 'moderate_term', 'long_term']\nbinned_nums.append(pd.cut(X_train['Loan_Amount_Term'], bins=LATbins,\n                          labels=LATlabels))","execution_count":null,"outputs":[]},{"metadata":{"id":"DGAuZSihHDvW","outputId":"89dd3ff8-899e-4e1b-8a89-b41ad39e52d4","trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_figheight(10)\nfig.set_figwidth(20)\n\ni=0\n\nfor j,x in enumerate(binned_nums):\n    \n    if j>1:\n        i=1; j-=2\n    _crs_tab = pd.crosstab(x.dropna(), y_train)\n    _crs_tab.div(_crs_tab.sum(1).astype(float), axis=0).plot(kind=\"bar\",\n                                                             stacked=True,\n                                                             ax=axes[i,j])\n\nplt.suptitle(\"Numerical Features (Binned)\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"id":"zfMKIk8vJ4JG"},"cell_type":"markdown","source":"#### From the graphs we can conclude:\n\nWe don't see any clear relation between Applicant income and loan approval but on ther hand we can clearly see a relation coapplicant income and loan amount which is having a lower income co-applicant reaises odds of geeting loan approved. And surety of approval when Co-Applicant Income is very low.\n> This is strange. Let's get value counts of **'Coapplicant_Income'**"},{"metadata":{"id":"hveZKYtDKW1V","outputId":"83df56c4-ad27-4430-b098-d2f6de8ea167","trusted":true},"cell_type":"code","source":"X_train['CoapplicantIncome'].value_counts(sort=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"kRdIE7UKKp-0"},"cell_type":"markdown","source":">The reason we were observing 100% loan approval at very low coapplicant income was because a significant number of applicant do not have a coapplicant at all.\n\n>That makes sense. What we can do is that, we could make a new feature **'Overall_Income'** which would be addition of **'Applicant Income'** and **'Coapplicant Income'**. "},{"metadata":{"id":"5FN-2D1XaGdH","outputId":"921349d2-ecbc-4fea-b674-a3827953d6b3","trusted":true},"cell_type":"code","source":"Overall_Income = X_train['CoapplicantIncome'] + X_train['ApplicantIncome']\n\nOIbins = [0, 2000, 4000, 8000, 16000, 30000, 60000, 100000]\nOI_trans = pd.cut(Overall_Income, bins=OIbins,\n                          labels=list(ascii_uppercase[0:7]))\n_crs_tab = pd.crosstab(OI_trans.dropna(), y_train)\n_crs_tab.div(_crs_tab.sum(1).astype(float), axis=0).plot(kind=\"bar\",\n                                                         stacked=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vKmeRxUlPht5","outputId":"aa0ebadb-74ec-4029-bcf0-2cd8474d25aa","trusted":true},"cell_type":"code","source":"corr_matrix = X_train.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"id":"P4C72oYKQHu9","outputId":"d55bc31d-3956-4c0c-b640-802171b0345a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.heatmap(corr_matrix, annot=True, vmin=-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZSbNTwE3QnTU","outputId":"89103bf9-0475-4367-94af-88c7022773d5","trusted":true},"cell_type":"code","source":"_temp_X_train = X_train[numerical_features].dropna()\n_temp_y_train = y_train[_temp_X_train.index]\n\nvis_pearson = FeatureCorrelation(labels=numerical_features,\n                                 method='mutual_info-classification', sort=True)\nvis_pearson.fit(_temp_X_train, _temp_y_train)\nvis_pearson.poof()","execution_count":null,"outputs":[]},{"metadata":{"id":"pa7XZ8ZgTKW7"},"cell_type":"markdown","source":"\n\n**Mutual information** is a measure between two (possibly multi-dimensional) random variables and , that quantifies the amount of information obtained about one random variable, through the other random variable.\n\n>mutual_info-classification method returns 0 if no correlation and non-zero otherwise.\n\nSo, we can remove feature **LoanAmount** and **ApplicantIncome**. Similar inference could also be from the divided bar blot that we earlier plotted."},{"metadata":{"id":"JVZTH7jlPHKB"},"cell_type":"markdown","source":"# **Pre-Processing (Handling null-values and outliers)**"},{"metadata":{"id":"FILhUAT7PVfu"},"cell_type":"markdown","source":"Based upon the insights we got form univariate and bi-variate anlysis we are going process oour data.\n\nActions/Steps to be performed:\n1. Impute Categorical values based on **most_frequent** (mode) value. Using *SimpleImputer* estimator.\n2. Impute Numercal values based on **median**.  Using *SimpleImputer* estimator.\n3. Cut numerical values to bins. *We will create a **custom estimator** for this step*.\n4. Apply *target_encoder* on nominal features.\n5. Apply *OrdinalEncoder* on ordinal features."},{"metadata":{"id":"qt6tx9epnhRo","trusted":true},"cell_type":"code","source":"# TO-DO\nclass Nums2Cats(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A custom class used to pre-process loan dataset.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Effectively nothing performed in fitting\n    transform(X)\n        Transforms the given dataframe in to dataframe such that numerical\n        features:\n            - 'ApplicantIncome' and 'LoanAmount' are dropped\n            - 'CoapplicantIncome' and 'Loan_Amount_Term' are converted into\n               ordinal categorical values.\n    \"\"\"\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X):\n        \"\"\"\n        This method transforms 'CoapplicantIncome' and 'Loan_Amount_Term' are\n        converts into ordinal categorical values.\n        And drops 'ApplicantIncome' and 'LoanAmount'.\n\n        Parameters\n        ----------\n        X : pandas.DataFrame\n            The dataframe on which processing is needed to be performed.\n\n        Returns\n        -------\n        pandas.DataFrame\n            a processed DataFrame.\n        \"\"\"\n        \n        # These steps are exactly the same as the one we did above in\n        # bivariate visualization with respect to outcome(y_train). \n        # The only difference is that here we use \n        # numpy.digitize() instead of pandas.cut()\n        # Please consider going through the documentation of numpy.digitize()\n             \n        AI_ix, CI_ix, LA_ix, LAT_ix = 0, 1, 2, 3\n\n        ## CoapplicantIncome ##\n        CIbins = [500, 1000, 2000, 3000, 4000, 5000, 8000, 12000, 20000]\n        CI_cat = np.digitize( (X[:, CI_ix]), bins=CIbins)    \n\n\n        ## Loan_Amount_Term ##\n        LATbins = [60, 360]\n        LAT_cat = np.digitize(X[:, LAT_ix], bins=LATbins)\n\n        return np.c_[CI_cat, LAT_cat]","execution_count":null,"outputs":[]},{"metadata":{"id":"mSp_s9OLzRFh","trusted":true},"cell_type":"code","source":"mode_imp = SimpleImputer(strategy='most_frequent')\nmedian_imp = SimpleImputer(strategy='median')\nnum2cats = Nums2Cats()\n\ncat_pipe = Pipeline(steps=[('mode_imp', mode_imp)])\nnum_pipe = Pipeline(steps=[('median_imp', median_imp), ('num2cats', num2cats)])\ncol_trans_lv1 = ColumnTransformer([('cat_pipe', cat_pipe, categorical_features),\n                                   ('num_pipe', num_pipe, numerical_features)])","execution_count":null,"outputs":[]},{"metadata":{"id":"oj0-wnShH1lC","outputId":"de733b47-ffcc-41c4-9118-83803f1917bc","trusted":true},"cell_type":"code","source":"numerical_features","execution_count":null,"outputs":[]},{"metadata":{"id":"V82MGZB7H1lH","outputId":"fb932c45-d994-40fe-c570-f1e4b7352c13","trusted":true},"cell_type":"code","source":"# Testing lvl 1 transformation\ntest_lvl1 = col_trans_lv1.fit_transform(X_train)  #TO-DO beautify\nprint(test_lvl1[:5])","execution_count":null,"outputs":[]},{"metadata":{"id":"rgwwdouRLoO5","outputId":"076c56e9-60af-46c1-aebc-7ef23c14d4cb","trusted":true},"cell_type":"code","source":"\"\"\"To replicate transformations from Num2Cats, for visualizing the tranformation\nbeing made to the numpy array\"\"\"\n\nnumerical_features_updated = ['CoapplicantIncome', 'Loan_Amount_Term']\n\nprint(':: Indices of features after column-transform-lvl1 ::')\ndict(enumerate(categorical_features+numerical_features_updated))","execution_count":null,"outputs":[]},{"metadata":{"id":"pjxn7Cn1KpUW"},"cell_type":"markdown","source":"Now that we have imputed numerical and categorical fetures and converted numerical features into ordinal categorical features.\n\nLet's now **target_encode** nominal features and **OrdinalEncode** ordinal features "},{"metadata":{"id":"kY3M7SQaLeh5"},"cell_type":"markdown","source":"After **col_trans_lvl1** transformation we will get a numpy.ndarray.\n\n>From our list of feature ordinal feature will be  ***'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Education', 'Dependents'*** out of this except for ***'Education', 'Dependents'*** are already ordinally encoded (thanks to numpy.digitizer).\n\n> And we have nominal features, ***'Gender', 'Married', 'Credit_History', 'Self_Employed', 'Property_Area'***. We will target_encode these features."},{"metadata":{"id":"L-sVC6loLP9H","trusted":true},"cell_type":"code","source":"ordinal_feat_ix = [2, 6]             # 'Dependents', 'Education'\n\nnominal_feat_ix = [0, 1, 3, 4, 5]    # 'Gender', 'Married', 'Credit_History',\n                                     # 'Self_Employed', 'Property_Area'","execution_count":null,"outputs":[]},{"metadata":{"id":"m1Y0s3_EzSTq","trusted":true},"cell_type":"code","source":"targ_enc = TargetEncoder()\nord_enc = OrdinalEncoder()\n\ncol_trans_lv2 = ColumnTransformer([('targ_enc', TargetEncoder(), nominal_feat_ix),\n                                   ('ord_enc', OrdinalEncoder(), ordinal_feat_ix)],\n                                  remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"id":"v6JpD6b_IIZX","trusted":true},"cell_type":"code","source":"pre_process_pipe = Pipeline([('col_trans_lvl1', col_trans_lv1),\n                             ('col_trans_lvl2', col_trans_lv2)])","execution_count":null,"outputs":[]},{"metadata":{"id":"9-JRZLP5wRLs"},"cell_type":"markdown","source":"# **Model Building and Predictions**\nApproach : \n1. We will firt build a base model(logistic regression) and get a base accuracy from the model.\n2. Then we will build several complex models and judge them basing the base model.\n3. We will pick the most promising model and then fine tune its hyper-parameter."},{"metadata":{"id":"bJHFYEORH1lg"},"cell_type":"markdown","source":"### Base Model (Logistic Regression)"},{"metadata":{"id":"-3KEtcdIH1lg","trusted":true},"cell_type":"code","source":"y_train = y_train.replace(['Y', 'N'], [1, 0])   # replacing Y=1 and N=0\n\nX_train_processed = pre_process_pipe.fit_transform(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"8HLBuFgQyP2t","outputId":"6c1c15b0-f650-46a9-98d5-d72eeb4eabc3","trusted":true},"cell_type":"code","source":"base_model = LogisticRegression()\nbase_model.fit(X_train_processed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"GjSzfHWCyvV7"},"cell_type":"markdown","source":"Let’s predict the Loan_Status for validation set and calculate its accuracy."},{"metadata":{"id":"eE9Xeh1YH1lp","outputId":"954ca244-5504-4130-ab92-b0d8916ad2a2","trusted":true},"cell_type":"code","source":"train_prediction = base_model.predict(X_train_processed)\n\nbase_accuracy_train = round(accuracy_score(y_train, train_prediction)*100, 2)\nbase_accuracy_cv = round(np.mean(cross_val_score(base_model, X_train_processed,\n                                                 y_train, scoring='accuracy',\n                                                 cv=10, n_jobs=-1))*100, 2)\nprint(':: Base Accuracy :: \\nTrain :: {0} \\nCV :: {1}'.format(base_accuracy_train,\n                                                              base_accuracy_cv))","execution_count":null,"outputs":[]},{"metadata":{"id":"5A53Ht_uH1lu"},"cell_type":"markdown","source":"**We will take these as our base accuracy now, that is our next model must give atleast this much accuracy.**"},{"metadata":{"id":"j2WD4KbpH1lu"},"cell_type":"markdown","source":"### Model Selection"},{"metadata":{"id":"y7hpj5k_y_0X"},"cell_type":"markdown","source":"We will build the following models using our processed dataset and check how it perform with base model.\n\n* Logistic Regression\n* Decision Tree\n* Random Forest\n* XGBoost\n* Multi-layer Perceptron\n* KNN\n* Naive-Bayes Classifier\n* SVM"},{"metadata":{"id":"oiNvKvVCH1lx","trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state=42)\nrfc = RandomForestClassifier(random_state=42, n_jobs=-1)\nmlpc = MLPClassifier(random_state=42)\nknn = KNeighborsClassifier(n_jobs=-1)\ngnb = GaussianNB()\nsvc = SVC(random_state=42)\nxgb = XGBClassifier(n_jobs=-1)\n\nclassifiers = [dtc, rfc, mlpc, knn, gnb, svc, xgb]\ntrain_accuracies = dict()\ncv_accuracies = dict()","execution_count":null,"outputs":[]},{"metadata":{"id":"iteKzb2EH1l0","trusted":true},"cell_type":"code","source":"for clf in classifiers:\n    clf.fit(X_train_processed, y_train)\n    y_train_pred = clf.predict(X_train_processed)\n    train_accuracies[type(clf).__name__] = round(accuracy_score(y_train_pred,\n                                                                y_train)*100, 2)\n    cv_accuracies[type(clf).__name__] = round(np.mean(cross_val_score(clf,\n                                        X_train_processed, y_train,\n                                        scoring='accuracy',\n                                        cv=20, n_jobs=-1))*100, 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"51KZlqqwH1l3","outputId":"e0f8f952-832b-4fe7-9dfa-d524d40fa37a","trusted":true},"cell_type":"code","source":"pd.DataFrame([train_accuracies, cv_accuracies], index=['Train', 'CV'])","execution_count":null,"outputs":[]},{"metadata":{"id":"E1x1VZBuH1l7"},"cell_type":"markdown","source":"### Model tuning (GaussainNB)"},{"metadata":{"id":"k3T1pSPTH1l8"},"cell_type":"markdown","source":"From the above DataFrame we can see that **Gaussian Naive-Bayes** gives the best performance out of all models on default parameters.\nAlso a 11.42 % accuracy from the base model.\n\n> Decision Tree and XGBClassifier are having a better accuracy on training set but quite low on CV set, which indicates models are overfittin training data."},{"metadata":{"id":"DbRJDTUF6fCQ"},"cell_type":"markdown","source":"We will use GridSearchCV for hyper parameter tuning"},{"metadata":{"id":"AJHhteGC6WZk","outputId":"0c891637-e1a0-4f82-91b0-a2b136fdddfd","trusted":true},"cell_type":"code","source":"# GaussianNB have very few hyperparameter to tune.\nparams = { 'var_smoothing': [1e-30, 1e-20, 1e-10, 1] }\n\ngrdsrch_clf = GridSearchCV(gnb, params, cv=20)\ngrdsrch_clf.fit(X_train_processed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"wdcoknVa67sc","outputId":"fa1e8e86-61c5-48af-82cc-730dbe57b3a7","trusted":true},"cell_type":"code","source":"print(\"Best Score CV :: {0} \\n\"\n      \"Best Estimator :: {1}\".format(grdsrch_clf.best_score_,\n                                     grdsrch_clf.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"id":"bjMyvKgh8CGv"},"cell_type":"markdown","source":"# **Prediction**\nAs, now we have our best estimator. Lets evaluate our model on test set."},{"metadata":{"id":"AWbw5vzP9mWQ","outputId":"600fc0ff-ed88-4c1c-ed58-e05d4742bf16","trusted":true},"cell_type":"code","source":"# First we have to pre-process the data\nX_test_processed = pre_process_pipe.transform(X_test)\n\n# As we trained on outcomes as [0,1] output values will be also in [0,1]\ny_test = y_test.replace(['Y', 'N'], [1, 0])\n\ny_pred_test = grdsrch_clf.predict(X_test_processed)\ntest_accuracy = round(accuracy_score(y_test, y_pred_test), 2)\nprint('Accuracy of our model :: ', test_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"iddzWc6M9nHL"},"cell_type":"markdown","source":"Thats a descent accuracy score from our model."},{"metadata":{},"cell_type":"markdown","source":"# Addional Notes\nIf you don't completely understand the topics discussed or you feel like having a bit more details, it is completely OK.\n\nHere are a few quick reads worth going through after the case study.\n1. [Seaborn Visualiztions](https://medium.com/swlh/introduction-to-data-visualization-with-seaborn-6232b70e9b30)\n2. [Choosing Metrics](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)\n3. [Sklearn-Pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n4. [Custom Estimators](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)\n5. [KNN](https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/)\n6. [Decision Trees](https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134)\n7. [Naive-Bayes](https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/)\n8. [SVM](https://medium.com/data-science-simplified/svm-support-vector-machine-beginners-ml-series-part-1-the-theory-part-7c618ef8ff9b)\n9. [XGBoost](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7)\n10. [MLP](https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f)\n\nI hope this helps you."},{"metadata":{},"cell_type":"markdown","source":"# Thank You"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}