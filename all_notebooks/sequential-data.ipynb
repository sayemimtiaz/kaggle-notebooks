{"cells":[{"metadata":{},"cell_type":"markdown","source":"<details>\n<summary>ðŸ““ &nbsp; Modeling Sequential Data Using Recurrent Neural Networks\n</summary><br/>Github Links<br/>    \n<a href=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb\">Part 1</a><br/>\n<a href=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part2.ipynb\">Part 2</a><br/>\n</details>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#11ff66','#6611ff','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dhtml('Code Modules, Setting, & Functions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np,pylab as pl\nimport tensorflow as tf\nimport tensorflow.keras.layers as tkl\nimport tensorflow_datasets as tfds\nfrom collections import Counter\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing \\\nimport sequence as tksequence","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_weights_shape(layer):\n    w_xh,w_oo,b_h=layer.weights\n    print('w_xh shape: %s'%w_xh.shape)\n    print('w_oo shape: %s'%w_oo.shape)\n    print('b_h shape: %s'%b_h.shape) \ndef compare_calc(layer,x):\n    n,inputs=x.shape[0],x.shape[1]\n    w_xh,w_oo,b_h=layer.weights\n    out_calc=[]\n    output=layer(\n        tf.reshape(x,shape=(1,n,inputs)))\n    pl.figure(figsize=(10,5))\n    for t in range(n):\n        xt=tf.reshape(x_seq[t],(1,inputs))\n        print('time step {} =>'.format(t))\n        print(5*' '+'input'+13*' '+': '+\n              str(xt.numpy()))\n        ht=tf.matmul(xt,w_xh)+b_h \n        print(5*' '+'hidden'+11*' '+': '+   \n              str(ht.numpy()))   \n        if t>0:\n            prev_o=out_calc[t-1]\n        else:\n            prev_o=tf.zeros(shape=(ht.shape))        \n        ot=ht+tf.matmul(prev_o,w_oo)\n        ot=tf.math.tanh(ot)\n        out_calc.append(ot)\n        print(5*' '+'calculated output: '+\n              str(ot.numpy()))\n        print(5*' '+'SimpleRNN output:   '.format(t)+\n              str(output[0][t].numpy())+'\\n')\n        pl.plot(output[0][t].numpy(),'-o',\n                label='time step %d'%t)\n    pl.grid(); pl.legend(); pl.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def encode(text_tensor,label):\n    text=text_tensor.numpy()\n    encoded_text=encoder.encode(text)\n    return encoded_text,label\ndef encode_fmap(text,label):\n    return tf.py_function(encode,inp=[text,label], \n                          Tout=(tf.int64,tf.int64))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dhtml('Data Exploration')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# 25,000 movies reviews from IMDB, \n# labeled by sentiment (positive/negative)\nnum_words=10000; max_length=1000\nembedding_vector_length=32\n(x_train,y_train),(x_test,y_test)=\\\nimdb.load_data(path=\"imdb_full.pkl\",\n               num_words=num_words,\n               skip_top=0,seed=113,\n               maxlen=max_length,\n               start_char=1,oov_char=2,\n               index_from=3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)\nx=np.vstack([x_train.reshape(-1,1),\n             x_test.reshape(-1,1)])\ny=np.vstack([y_train.reshape(-1,1),\n             y_test.reshape(-1,1)])\nx=x.reshape(-1); y=y.reshape(-1)\nprint(x.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"word_to_id=imdb.get_word_index()\nword_to_id=\\\n{k:(v+3) for k,v in word_to_id.items()}\nsw=[\"<PAD>\",\"<START>\",\"<UNK>\",\"<UNUSED>\"]\nfor i in range(4): word_to_id[sw[i]]=i\nid_to_word=\\\n {value:key for key,value in word_to_id.items()}\ndef get_string(x,i):\n    return ' '.join(id_to_word[id] for id in x[i] if id>3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features=[get_string(x,i) \n          for i in range(x.shape[0])]\nfeatures=np.array(features)\ntargets=y\ndhtml(features[0],c2,f2,fs2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dhtml('Data Building')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds=tf.data.Dataset.\\\nfrom_tensor_slices((features,targets))\nfor ex in ds.take(3):\n    tf.print(ex[0].numpy()[:60],ex[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(123)\nds=ds.shuffle(50000,reshuffle_each_iteration=False)\nds_test=ds.take(10000)\nds_train_valid=ds.skip(10000)\nds_valid=ds_train_valid.take(10000)\nds_train=ds_train_valid.skip(10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=tfds.features.text.Tokenizer()\ntoken_counts=Counter()\nfor example in ds_train:\n    tokens=tokenizer.tokenize(example[0].numpy())\n    token_counts.update(tokens)\nprint('vocabulary size:',len(token_counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder=tfds.features.text\\\n.TokenTextEncoder(token_counts)\nexample_str='hi this is an example of sentences'\nencoder.encode(example_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(123)\ntrain=ds_train.map(encode_fmap)\nvalid=ds_valid.map(encode_fmap)\ntest=ds_test.map(encode_fmap)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"tf.random.set_seed(1)\nfor example in train.shuffle(1000).take(3):\n    print('sequence length:',example[0].shape)\n    print(example[0].numpy())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"ds_example=train.take(20)\nprint('individual sizes:')\nfor example in ds_example:\n    print(example[0].shape)\nds_batched_example=ds_example\\\n.padded_batch(4,padded_shapes=([-1],[]))\nprint('batch dimensions:')\nfor batch in ds_batched_example:\n    print(batch[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train.padded_batch(\n    32,padded_shapes=([-1],[]))\nvalid_data=valid.padded_batch(\n    32,padded_shapes=([-1],[]))\ntest_data=test.padded_batch(\n    32,padded_shapes=([-1],[]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for example in train_data.take(1):\n    print(example[0].numpy()[0],'\\n',\n          example[1].numpy())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dhtml('Embedding, RNN, LSTM, & GRU Layers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=tf.keras.Sequential(\n    name='embedding_structure')\nmodel.add(tkl.Embedding(\n    input_dim=256,output_dim=5,\n    input_length=32,name='embedding_1'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rnn_layer(inputs,units):\n    rnn_layer=tkl.SimpleRNN(\n        units=units,use_bias=True,\n        return_sequences=True)\n    rnn_layer.build(\n        input_shape=(None,None,inputs))\n    return rnn_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m=5; inputs=7; units=4\ntf.random.set_seed(123)\nrnn_layer74=rnn_layer(inputs,units)    \nget_weights_shape(rnn_layer74)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"x_seq=tf.convert_to_tensor(\n    [[1.*(i+1)]*inputs for i in range(m)],\n    dtype=tf.float32)\ncompare_calc(rnn_layer74,x_seq)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=tf.keras.Sequential(\n    name='simple_rnn_structure')\nmodel.add(tkl.Embedding(1000,32))\nmodel.add(\n    tkl.SimpleRNN(32,return_sequences=True))\nmodel.add(tkl.SimpleRNN(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=tf.keras.Sequential(\n    name='lstm_structure')\nmodel.add(tkl.Embedding(10000,32))\nmodel.add(\n    tkl.LSTM(32,return_sequences=True))\nmodel.add(tkl.LSTM(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=tf.keras.Sequential(\n    name='gru_structure')\nmodel.add(tkl.Embedding(10000,32))\nmodel.add(\n    tkl.GRU(32,return_sequences=True))\nmodel.add(tkl.GRU(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dhtml('Predicting Sentiments')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dhtml(' '.join(\n    [list(token_counts)[i]\n     for i in range(32)]),c2,f2,fs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim=32\nvocabulary_size=len(token_counts)+2\nmodel=tf.keras.Sequential(\n    name='bi_lstm_model')\nmodel.add(tkl.Embedding(\n    input_dim=vocabulary_size,\n    output_dim=embedding_dim,\n    name='embedding_layer'))\nmodel.add(tkl.Bidirectional(\n    tkl.LSTM(64,name='lstm_layer'),\n    name='bidirect_lstm_layer'))\nmodel.add(tkl.Dense(64,activation='relu',\n                    name='dense_64'))\nmodel.add(tkl.Dense(1,activation='sigmoid',\n                    name='out'))\noptimizer=tf.keras.optimizers.Adam(1e-3)\nloss_fun=tf.keras.losses\\\n.BinaryCrossentropy(from_logits=False)\nmodel.compile(\n    optimizer=optimizer,loss=loss_fun,\n    metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"history=model.fit(\n    train_data,epochs=5,\n    validation_data=valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_data)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dhtml('Functions in Construction Process')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def preprocess_datasets(\n    ds_train,ds_valid,ds_test,\n    max_seq_len=None,batch_size=32):\n    tokenizer=tfds.features.text.Tokenizer()\n    token_counts=Counter()\n    for ds in [ds_train,ds_valid,ds_test]:\n        for example in ds:\n            tokens=tokenizer.tokenize(\n                example[0].numpy())\n            if max_seq_len is not None:\n                tokens=tokens[-max_seq_len:]\n            token_counts.update(tokens)\n    print('vocabulary size: ',len(token_counts))\n    encoder=tfds.features.text\\\n    .TokenTextEncoder(token_counts)\n    def encode(text_tensor,label):\n        text=text_tensor.numpy()\n        encoded_text=encoder.encode(text)\n        if max_seq_len is not None:\n            encoded_text=encoded_text[-max_seq_len:]\n        return encoded_text,label\n    def encode_fmap(text,label):\n        return tf.py_function(\n            encode,inp=[text,label], \n            Tout=(tf.int64,tf.int64))\n    train=ds_train.map(encode_fmap)\n    valid=ds_valid.map(encode_fmap)\n    test=ds_test.map(encode_fmap)\n    train_data=train.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    valid_data=valid.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    test_data=test.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    return (train_data,valid_data,test_data,\n            len(token_counts))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def build_rnn_model(\n    embedding_dim,vocabulary_size,\n    recurrent_type='SimpleRNN',\n    n_rnn_units=64,n_rnn_layers=1,\n    bidirect=True):\n    tf.random.set_seed(123)\n    model=tf.keras.Sequential()\n    model.add(tkl.Embedding(\n        input_dim=vocabulary_size,\n        output_dim=embedding_dim,\n        name='embedding_layer'))\n    for i in range(n_rnn_layers):\n        return_sequences=(i<n_rnn_layers-1)    \n        if recurrent_type=='SimpleRNN':\n            recurrent_layer=tkl.SimpleRNN(\n                units=n_rnn_units, \n                return_sequences=return_sequences,\n                name='simple_rnn_layer{}'.format(i))\n        elif recurrent_type=='LSTM':\n            recurrent_layer=tkl.LSTM(\n                units=n_rnn_units, \n                return_sequences=return_sequences,\n                name='lstm_layer{}'.format(i))\n        elif recurrent_type=='GRU':\n            recurrent_layer=tkl.GRU(\n                units=n_rnn_units, \n                return_seq=return_sequences,\n                name='gru_layer{}'.format(i))     \n        if bidirect:\n            recurrent_layer=tkl.Bidirectional(\n                recurrent_layer,\n                name='bidirect_'+recurrent_layer.name)       \n        model.add(recurrent_layer)\n    model.add(tkl.Dense(64,activation='relu'))\n    model.add(tkl.Dense(1,activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=32\nmax_seq_len=100\ntrain_data,valid_data,test_data,n=\\\npreprocess_datasets(\n    ds_train,ds_valid,ds_test,\n    max_seq_len=max_seq_len,\n    batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"vocabulary_size=n+2\nembedding_dim=32\nrnn_model=build_rnn_model(\n    embedding_dim,vocabulary_size,\n    recurrent_type='SimpleRNN', \n    n_rnn_units=64,n_rnn_layers=3,\n    bidirect=True)\nrnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"optimizer=tf.keras.optimizers.Adam(1e-3)\nloss_fun=tf.keras.losses\\\n.BinaryCrossentropy(from_logits=False)\nrnn_model.compile(\n    optimizer=optimizer,loss=loss_fun,\n    metrics=['accuracy'])\nhistory=rnn_model.fit(\n    train_data,epochs=10,\n    validation_data=valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_model.evaluate(test_data)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}