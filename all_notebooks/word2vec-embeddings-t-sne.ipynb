{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Word2Vec Embeddings & t-SNE\n\n## Description\n\nThis project aims to extract the text from the <a href=\"https://www.kaggle.com/hsankesara/medium-articles\">Medium Articles</a>, create a <a href=\"https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/models/word2vec/index.html\">Gensim's Word2Vec Model</a> and analyse the relationships between words in the high dimensional corpus using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\">Sklearn's TSNE Module</a>.\n\nLink to the Project: https://github.com/VETURISRIRAM/Word2Vec_t-SNE"},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec & t-SNE Introduction\n\nThe deep learning models designed for Natural Language Processing domain require the text data in numeric form (though some of them nowadays have their own internal word embedding processes and just requires you to pass the text as it is). But the overall idea is to trainform the textual data into numbers that the models understand.\n\n`Word2Vec` is a way to represent similar words in similar numeric form. This ensures that a high dimensional data could be transformed into a much lower dimension. This way similar words are near each other (in the same neighborhood) as their word vectors are also similar. In this project, Word2Vec model is used for <a href=\"https://pypi.org/project/gensim/\">Gensim</a> library.\n\n`t-Distributed Stochastic Neighbor Embedding (t-SNE)` is a dimentionality reduction technique just like PCA or TruncatedSVD but with a slightly different approach. Imagine there are two distributions, one which measures the pairwise similarities between the actual input points and another which measures the pairwise similarities between low dimensional input points in the word embeddings. But the approach is computationally expensive.\n\nIn this project, this technique is used to visualize the high dimensional word vectors to identify similar words in a clustered form."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport spacy\nimport string\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialize stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"# nltk.download()\nSTOPWORDS = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basically, this script reads the data and gets specifically the text present in the `text` column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data.\nprint(\"Started: \", datetime.datetime.now())\ndf = pd.read_csv(\"/kaggle/input/medium-articles/articles.csv\")\nprint(\"Data Read: \", datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, minor preprocessing is done like lowercasing, removing words like `[something]` and the words which contain numbers in them, and removes all the special charatcters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n    Function to clean the text by performing:\n    1) Lowercase Operation.\n    2) Removing words with square brackets.\n    3) Removing punctuations.\n    4) Removing stopwords.\n    :param text: Raw Text.\n    :return text: Clean Text.\n    \"\"\"\n\n    text = text.lower()\n    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n    if len(text) > 3:\n        text = \" \".join([t for t in text.split() if t not in STOPWORDS])\n\n        return text\n    else:\n\n        return \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean the text.\ndf[\"text\"] = df[\"text\"].apply(lambda text_value: clean_text(text_value))\nprint(\"Cleaned Text: \", datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words are then lemmatized to their root form and corpus of documents is created."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_text_tokens(text, nlp):\n    \"\"\"\n    Funciton to tokenize a sentence, lemmatize the tokens and return lemmatized sentence back.\n    :param text: Raw Text.\n    :param nlp: Spacy Object with \"en\" corpus loaded.\n    :return lemmatized_text: Lemmatized sentence.\n    \"\"\"\n\n    tokens = nlp(text)\n    lemmatized_tokens = list()\n    for token in tokens:\n        lemmatized_token = token.lemma_\n        lemmatized_tokens.append(lemmatized_token)\n    lemmatized_text = \" \".join(lemmatized_tokens)\n\n    return lemmatized_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatize the tokens.\nnlp = spacy.load(\"en\")\ndf[\"lemmatized_text\"] = df[\"text\"].apply(lambda text_value: lemmatize_text_tokens(text_value, nlp))\nprint(\"Lemmatized the Documents' Tokens: \", datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the documents into tokens.\ndoc_sentences = [text.split() for text in list(df[\"lemmatized_text\"])]\nprint(\"Split Token of Documents: \", datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word embeddings are created using `Word2Vec` model. I used all but one cores of my system in doing this. While training the Word2Vec model, you can play around with the hyperparametrs like min_count, window, size, workers, etc. These parameters basically define the number of occurrences of the word, the distance between the actual word and the predictions, the number of cores to use and the size of the feature vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=200,\n                         window=5,\n                         size=100,\n                         workers=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Building Vocabulary: \", datetime.datetime.now())\nw2v_model.build_vocab(doc_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Word2Vec Model: \", datetime.datetime.now())\nw2v_model.train(doc_sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\nprint(\"Training Done: \", datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following are the most similar words related the word `computer`."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.init_sims(replace=True)\nmost_similar_words = w2v_model.wv.most_similar(positive=['computer'])\nfor similar_word in most_similar_words:\n    print(similar_word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`t-SNE` is used to plot the models vocabulary (limited) so associations between words could be analyzed."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plt_tsne(word2vec_model):\n    \"\"\"\n    Function to plot the words using t-SNE from the models vocabulary and the probability associations.\n    :param word2vec_model: Word2Vec Model.\n    \"\"\"\n\n    labels = list()\n    words = list()\n\n    for word in word2vec_model.wv.vocab:\n        words.append(word2vec_model[word])\n        labels.append(word)\n\n    tsne_model = TSNE(perplexity=25, n_components=2, init=\"pca\", n_iter=2000, random_state=0)\n    new_values = tsne_model.fit_transform(words)\n\n    x, y = list(), list()\n\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n\n    plt.figure(figsize=(18, 18))\n\n    for i in range(len(x)):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords=\"offset points\",\n                     ha=\"right\",\n                     va=\"bottom\")\n    plt.savefig(\"./tsne_plot_word2vec.png\")\n    plt.show(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt_tsne(w2v_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interesting Observations\n\nIf you look at the image (`tsne_plot_word2vec.png`) created after the program execution, you would notice clusters of words with semantic similarities.\n\n<img src=\"./tsne_plot_word2vec.png\">\n\nExample, there is cluster with words - `\"neural\"`, `\"network\"`, and `\"architecture\"`, another cluster with words - `\"machine\"`, `\"deep\"`, `\"learning\"`, and `\"learn\"`.\n\nIt is a great way to identify semantic similarities between words in the corpus and looking at the results, it actually makes sense from a human perspective."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}