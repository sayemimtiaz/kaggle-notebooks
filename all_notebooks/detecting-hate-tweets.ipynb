{"cells":[{"metadata":{"_uuid":"e78c15c05b99a04ec5b4468299c8308041255bd0"},"cell_type":"markdown","source":"## Help Twitter Combat Hate Speech Using NLP and Machine Learning\n\n## Anantha Babu \nUsing NLP and ML, make a model to identify hate speech (racist or sexist tweets) in Twitter."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndata = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"How are tweets spread among these 2 classes?"},{"metadata":{"trusted":true,"_uuid":"62127fadccac984b3e5e5dcaad931f176f40a480"},"cell_type":"code","source":"print(\"Hatred labeled: {}\\nNon-hatred labeled: {}\".format(\n    (data.label == 1).sum(),\n    (data.label == 0).sum()\n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ca037dffdd54922b4e06ab0616ca0235c1c5e00"},"cell_type":"markdown","source":"Classes are pretty much skewed, it's better to use F1 score as evaluation metric."},{"metadata":{"_uuid":"92839e8dbabe53008a1c73f8c708fd7e675bbe03"},"cell_type":"markdown","source":"## Extracting features\n\n#### Hashtags and mentions\nWe'll extract hashtags for each tweet as an extra column to explore them later.   \nFor user mentions, all of the usernames have been replaced with `'user'` so we can't get any data from it, we'll just remove mentions and keep the number of mentions in each tweet as an extra features for that tweet.  "},{"metadata":{"trusted":true,"_uuid":"6e31e542112433b66ca725da19b647da4c71585f"},"cell_type":"code","source":"hashtags = data['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata.loc[:, 'hashtags'] = hashtags['hashtag']\ndata['hashtags'].fillna('', inplace=True)\n\ndata.loc[:, 'mentions'] = data['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata.tweet = data.tweet.str.replace('@[a-zA-Z0-9_]+', '')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed53500474e96d939cdc5fc6c10a91b5a2dfc198"},"cell_type":"markdown","source":"#### Removing anything but the words\nNow we'll remove anything but the words (punctuations, numbers, etc). Note that this time we'll replace them with a blank space since it might be a `_` or `-` or a punctuation with no space from the next word and we don't want the words to join together.  "},{"metadata":{"trusted":true,"_uuid":"0c62abb7ca3a8a806221ed3cae8a3d9e69faf0aa"},"cell_type":"code","source":"data.tweet = data.tweet.str.replace('[^a-zA-Z]', ' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"996dfe01552929c9fba1db37f66f822dda269984"},"cell_type":"markdown","source":"#### Lemmatization\nWe lemmatize tweets' words as we have the sentences and we can tag part of speeches, and will stem hashtags.  "},{"metadata":{"trusted":true,"_uuid":"d2786200ecbb879dc9fd3e34d8766bc098cf6ad4"},"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag, FreqDist, word_tokenize\n\nstemmer = SnowballStemmer('english')\nlemmer = WordNetLemmatizer()\n\npart = {\n    'N' : 'n',\n    'V' : 'v',\n    'J' : 'a',\n    'S' : 's',\n    'R' : 'r'\n}\n\ndef convert_tag(penn_tag):\n    if penn_tag in part.keys():\n        return part[penn_tag]\n    else:\n        return 'n'\n\n\ndef tag_and_lem(element):\n    sent = pos_tag(word_tokenize(element))\n    return ' '.join([lemmer.lemmatize(sent[k][0], convert_tag(sent[k][1][0]))\n                    for k in range(len(sent))])\n    \n\ndata.loc[:, 'tweet'] = data['tweet'].apply(lambda x: tag_and_lem(x))\ndata.loc[:, 'hashtags'] = data['hashtags'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b4a8749bc8f8d6624e2c41a710df454aff0f611"},"cell_type":"markdown","source":"#### Frequent words\nNow let's see what words and hashtags are the most frequenst in hate tweets and in total."},{"metadata":{"trusted":true,"_uuid":"b8a9afebbffcfb27e6267ef5227ee6032250db48"},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = STOPWORDS.add('amp')\n\nall_words = ' '.join(data.tweet.values)\nhatred_words = ' '.join(data[data.label == 1].tweet.values)\n\nplt.figure(figsize=(16, 8))\n\ncloud1 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(all_words)\nplt.subplot(121)\nplt.imshow(cloud1, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('All tweets', size=20)\n\ncloud2 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(hatred_words)\nplt.subplot(122)\nplt.imshow(cloud2, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Hatred tweets', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea2d98b94c65d8694ed867c45289a9be6adc6cee"},"cell_type":"markdown","source":"#### Hashtags\nNow let's see which hashtags are used the most in hatred tweets and in total."},{"metadata":{"trusted":true,"_uuid":"a40bfb2f3306029a02ab2bd66ede4cfc78a11b36"},"cell_type":"code","source":"all_hashtags = FreqDist(list(' '.join(data.hashtags.values).split())).most_common(10)\nhatred_hashtags = FreqDist(list(' '.join(data[data.label==1].hashtags.values).split())).most_common(10)\nplt.figure(figsize=(14, 6))\nax = plt.subplot(121)\npd.DataFrame(all_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\nplt.xlabel('# occurrences')\nplt.title('Hashtags in all tweets', size=13)\nax = plt.subplot(122)\npd.DataFrame(hatred_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\nplt.xlabel('# occurrences')\nplt.ylabel('')\nplt.title('Hashtags in hatred tweets', size=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bded2c905bb43ad971c7758a40ebac58f45a5df7"},"cell_type":"markdown","source":"#### Mentions\nLet's see how many mentions are there in total and if they can be of any use"},{"metadata":{"trusted":true,"_uuid":"35a5534cf0cd55ea9d1a2f9b3cf6cd2bf883850d"},"cell_type":"code","source":"print(\"Number of mentions: {}\\nNumber of tweets having a mention: {}\\nCorrelation with label: {}\".format(\n    data.mentions.sum(),\n    len(data[data.mentions > 0]),\n    np.corrcoef(data.mentions, data.label)[0][1]\n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b025142fbec15c1b20a20b7d4f23efd491b697bf"},"cell_type":"markdown","source":"There is no meaningful relation between number of mentions and it looks like there is not a correlation either. So we'll remove number of mentions and won't use it as a feature.  "},{"metadata":{"trusted":true,"_uuid":"b0b1d3f2d665e1224cf5f863846bd4b50aede99d"},"cell_type":"code","source":"data.drop('mentions', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a5d7e7cf2868b0f311f4a95f8439427d2e08a5f"},"cell_type":"markdown","source":"#### Tfidf vectorizing\nNow we use the frequency of each word in tweets as our features"},{"metadata":{"trusted":true,"_uuid":"ae7203862ad3b6df4ab284c408344c5a7b131915"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\nvectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=10)\nfeatures = vectorizer.fit_transform(data.tweet)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82037d5250db29273352848200420258d08fbc27"},"cell_type":"markdown","source":"## Classifying\nWe'll build a SVC and a LogsiticRegression model for classifying our tweets."},{"metadata":{"trusted":true,"_uuid":"67b4be23a8922ca538c2d5d112ee28222d64a791"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(features, data.label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd5b362a478fb22fb210d7e49d13dc3b92ff92b0"},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"e510254d2bca194b3337cdf44fee12488b86a31e"},"cell_type":"code","source":"params = {'penalty': ['l1', 'l2'], 'C': [3, 10, 30, 100, 300]}\nlrmodel = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=150), param_grid=params, scoring='f1', cv=5, n_jobs=-1)\nlrmodel.fit(X_train, y_train)\nprint(\"Best parameters found were {} with F1 score of {:.2f}\".format(\n    lrmodel.best_params_,\n    lrmodel.best_score_\n))\nprobas = lrmodel.predict_proba(X_test)\nthresholds = np.arange(0.1, 0.9, 0.1)\nscores = [f1_score(y_test, (probas[:, 1] >= x).astype(int)) for x in thresholds]\nplt.plot(thresholds, scores, 'o-')\nplt.title(\"F1 score for different thresholds\")\nplt.ylabel(\"Score\")\nplt.xlabel(\"Threshold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5558cbefcd6a4669c9075af747106f40dec0a3fd"},"cell_type":"markdown","source":"Best threshold found for our model is 0.4 so we will label any tweet with probability higher than or equal to 0.4 as hate tweet."},{"metadata":{"_uuid":"0fec9711d4bd0735351c959d22b6bee713ff4bcf"},"cell_type":"markdown","source":"#### SVC"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0c1f38beac5ac8a1602664210a37ccb085b5776b"},"cell_type":"code","source":"params = {'C': [1000, 3000, 9000, 15000]}\nsvc = GridSearchCV(SVC(kernel='rbf', gamma='auto'), param_grid=params, scoring='f1', cv=3, n_jobs=-1)\nsvc.fit(X_train, y_train)\nprint(\"Best parameters found were {} with F1 score of {:.2f}\".format(\n    svc.best_params_,\n    svc.best_score_\n))\npredictions = svc.predict(X_test)\nprint(\"\\nF1 test score for SVC: {:.2f}\".format(f1_score(y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e24de80a704fe48188f6206888a49337faa59a28"},"cell_type":"markdown","source":"## Conclusion\nWe saw some of the most common words and hashtags in general and in racist/sexist tweets, extracted features by counting word tokens and Tfidf weighting them. We used unigrams, bigrams and trigrams as tokens.   \nFinally, we built a logistic regression model and a support vector classifier to classify future tweets in these 2 classes.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}