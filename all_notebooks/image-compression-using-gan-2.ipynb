{"cells":[{"metadata":{"_uuid":"e265b52c-584d-4f80-92ab-f9bfec0f3734","_cell_guid":"08963816-b283-4302-af42-1357b7e25052","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04dc0a56-cb90-4c69-b973-0c79214dc59e","_cell_guid":"c4ab2f4f-a173-4553-a22f-f35a2d1db962","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport torchvision.utils as vutils\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nimport cv2 as cv\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.image as mpimg\n\nimport torchvision.transforms.functional as TF","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f80c88-1643-4205-8688-7916305d26f2","_cell_guid":"15441219-cac0-409e-bbc6-a90c647c9665","trusted":true},"cell_type":"code","source":"# Code -- https://github.com/alexandru-dinu/cae\n# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n\n\n\n\nimg_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\nimg_list = os.listdir(img_dir)\nprint(len(img_list))\nvalid_ratio = 0.8","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cf61d41-c8f2-4a15-89cc-3be91e942e62","_cell_guid":"352ebe3e-f5e0-4fce-b5da-71a230783edd","trusted":true},"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self,is_train=True):\n        self.is_train = is_train\n        self.transform = transforms.Compose([transforms.ToTensor(),])\n        self.train_index = int(valid_ratio * len(img_list))\n        self.crop = transforms.CenterCrop((218,178))\n    def __len__(self):\n        if self.is_train:\n            return self.train_index\n        else:\n            return len(img_list) - self.train_index -1\n    def __getitem__(self, index):\n        if not self.is_train:\n            index = self.train_index + index\n#         print(\"hey  \"*4 + str(index))\n        img = mpimg.imread(img_dir+img_list[index])\n        img = self.crop(TF.to_pil_image(img))\n        img = self.transform(img)\n        img = (img-0.5) /0.5\n#         img = (img - 255.0) / 255.0\n        return img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcdd031a-9938-40c6-a3cc-7417733e858d","_cell_guid":"9732df15-3bc9-46ec-be0c-304661836ac2","trusted":true},"cell_type":"code","source":"batch_size=20\ndataset = ImageData()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75e6a343-b614-481e-8a12-3eca4ff0c7b0","_cell_guid":"8d126d61-eebd-4959-adc4-d0cbfda2278c","trusted":true},"cell_type":"code","source":"a = next(iter(dataloader))\nprint(a[0].shape)\nimg = a[15]\nimg = img *0.5 + 0.5\nplt.imshow(img.permute(1,2,0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0511b538-0033-492e-a269-4b1b64d320ef","_cell_guid":"8e6a177b-dc74-401d-8ffa-5befd521651e","trusted":true},"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5953492-9a59-43bd-9a2e-5f69e5227946","_cell_guid":"a3e261cf-79be-4cf6-bfd4-208be76cbd12","trusted":true},"cell_type":"code","source":"IMG_WIDTH = 178\nIMG_HEIGHT = 218\nlatent_size = 200\n\nnum_channels_in_encoder = 8","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fd1ec79-1cc6-4b96-b451-f74d29969670","_cell_guid":"0ef5c761-0d0f-490e-a4f8-b770c3eab3bd","trusted":true},"cell_type":"code","source":"# Encoder Model\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        # ENCODER\n\n        # 64x64x64\n        self.e_conv_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=(2, 2)),nn.LeakyReLU()\n        )\n\n        # 128x32x32\n        self.e_conv_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(5, 5), stride=(2, 2)),\n            nn.LeakyReLU()\n        )\n        \n        # 128x32x32\n        self.e_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 32x32x32\n        self.e_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=num_channels_in_encoder, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n            nn.Tanh()\n        )\n    def forward(self, x):\n        ec1 = self.e_conv_1(x)\n        ec2 = self.e_conv_2(ec1)\n        eblock1 = self.e_block_1(ec2) + ec2\n        eblock2 = self.e_block_2(eblock1) + eblock1\n        eblock3 = self.e_block_3(eblock2) + eblock2\n        ec3 = self.e_conv_3(eblock3)  # in [-1, 1] from tanh activation\n        return ec3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"839c8341-9f75-4a31-8b7e-818269e482b5","_cell_guid":"5bff9e09-cee6-428a-8368-95da81e9f10a","trusted":true},"cell_type":"code","source":"device","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fd9bdb1-b0f0-4679-aa6e-afd357e009b4","_cell_guid":"758c449a-49db-480c-a5ea-8a443a082ace","trusted":true},"cell_type":"code","source":"netE = Encoder().to(device)\nnetE.apply(weights_init)\ninp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100)\ninp = inp.view((-1,3,IMG_HEIGHT,IMG_WIDTH))\noutput = netE(inp.to(device))\nprint(output.shape)\nprint('The Compression Ratio is :  ' + str((output.shape[1]*output.shape[2]*output.shape[3])/(IMG_WIDTH*IMG_HEIGHT*3)*100) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ddb97c8-6514-4cab-817b-8222e972596e","_cell_guid":"1c175dca-7190-4bb2-a510-1d86de4e902c","trusted":true},"cell_type":"code","source":"# Generator / Decoder Model\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # DECODER\n#         self.latent_fc1 = nn.Sequential(\n#             nn.Linear(latent_size,1000),\n#             nn.Sigmoid(),\n#         )\n#         self.latent_fc2 = nn.Sequential(\n#             nn.Linear(1000,54*44),\n#             nn.Sigmoid(),\n#         )\n        # 128x64x64\n        self.d_up_conv_1 = nn.Sequential(\n        nn.Conv2d(in_channels=num_channels_in_encoder, out_channels=64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 128x64x64\n        self.d_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 256x128x128\n        self.d_up_conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=32, out_channels=256, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 3x128x128\n        self.d_up_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ReflectionPad2d((3, 3, 3, 3)),\n            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n            nn.Tanh()\n        )\n\n        \n        \n    def forward(self, x):\n        uc1 = self.d_up_conv_1(x)\n        dblock1 = self.d_block_1(uc1) + uc1\n        dblock2 = self.d_block_2(dblock1) + dblock1\n        dblock3 = self.d_block_3(dblock2) + dblock2\n        uc2 = self.d_up_conv_2(dblock3)\n        dec = self.d_up_conv_3(uc2)\n        return dec","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e38c861-d53b-4b65-91f1-d473f7c803ef","_cell_guid":"7592eb2f-d136-42d1-8143-c5f0e0321a75","trusted":true},"cell_type":"code","source":"netG = Generator().to(device)\nnetG.apply(weights_init)\ninp = torch.randn(100*num_channels_in_encoder*54*44).view((-1,num_channels_in_encoder,54,44)).to(device)\noutput = netG(inp)\nprint(output.shape)\n#218 * 178","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29afd95d-1df5-47b9-91ee-4a8eb2b57157","_cell_guid":"7e6a9272-4465-466c-9faa-52d00de04919","trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35f29815-00b3-4b85-97d4-dded7d211be3","_cell_guid":"9a9bf161-84f8-4601-ad54-e7073cb87824","trusted":true},"cell_type":"code","source":"# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.latent_layer1 = nn.Sequential(\n            nn.ConvTranspose2d(num_channels_in_encoder, 12, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer2 = nn.Sequential(\n            nn.ConvTranspose2d(12, 16, (3,3), stride=1, padding=2, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer3 = nn.Sequential(\n            nn.ConvTranspose2d(16, 24, (3,3), stride=2, padding=2, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer4 = nn.Sequential(\n            nn.ConvTranspose2d(24, 36, (5,5), stride=2, padding=0, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer5 = nn.Sequential(\n            nn.ConvTranspose2d(36, 3, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Tanh(),\n        )\n\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=6, out_channels=64, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5,stride = 2,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3,stride = 2,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3,stride = 1,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Tanh(),\n        )\n        \n        \n        self.fc1 = nn.Sequential(\n            nn.Linear(8*54*44,2000),\n            nn.Sigmoid(),\n        )\n        \n        self.fc2 = nn.Sequential(\n            nn.Linear(2000,100),\n            nn.Sigmoid(),\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(100,1),\n            nn.Sigmoid(),\n        )\n        \n        \n    def forward(self, x):\n        y = x['encoded'].to(device)\n        y = self.latent_layer1(y)\n        y = self.latent_layer2(y)\n        y = self.latent_layer3(y)\n        y = self.latent_layer4(y)\n        y = self.latent_layer5(y)\n#         print(y.shape)\n        x = x['img'].to(device)\n#         print(x.shape)\n        x = torch.cat((x,y),1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n#         print(x.shape)\n        x= x.reshape((x.shape[0],-1))\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9a8d23-2753-458c-830c-da685ed808d8","_cell_guid":"1b9b92fc-5ae8-46e0-9eda-d7fc9c297686","trusted":true},"cell_type":"code","source":"netD = Discriminator().to(device)\nnetD.apply(weights_init)\ninp_x = {}\ninp_x['img']=torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100).view((-1,3,IMG_HEIGHT,IMG_WIDTH))\ninp_x['encoded'] = torch.randn(100*num_channels_in_encoder*54*44).view((-1,num_channels_in_encoder,54,44))\noutput = netD(inp_x)\noutput.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac5efd94-cebc-4876-8fc9-094c00aa9a79","_cell_guid":"e5aa4e37-9710-48fd-8a3c-adbf3e9fd5d3","trusted":true},"cell_type":"code","source":"lr = 0.0002\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\nmsecriterion = nn.MSELoss()\nl1criterion = nn.L1Loss()\n# Establish convention for real and fake labels during training\nreal_label = 1\nfake_label = 0\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerE = optim.Adam(netE.parameters(), lr=lr, betas=(0.5, 0.999))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset = ImageData(is_train=False)\nnum_images_to_show = 1\nvalid_dataloader = DataLoader(valid_dataset, batch_size=num_images_to_show, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150d8266-c420-4f2b-b65b-a851c8700fe1","_cell_guid":"50c6fd8b-493d-40d8-bdae-436381ab07ca","trusted":true},"cell_type":"code","source":"# Training Loop\n\n# Lists to keep track of progress\nG_losses = []\nD_losses = []\nE_losses = []\niters = 0\nnum_epochs = 9\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, (images) in enumerate(dataloader, 0):\n        netG.train()\n        netD.train()\n        netE.train()\n        \n        netD.zero_grad()\n        \n        images = images.to(device)\n        fake_images = netG(netE(images))\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        \n        ## Create a fake pair batch --\n\n        inp_x = {}\n        inp_x['img']=images\n        inp_x['encoded'] = netE(images)\n        \n#         label = torch.full((images.size(0),), real_label, device=device)\n        label = torch.FloatTensor(np.random.uniform(low=0.855, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x).view(-1)\n        errD_real = criterion(output, label)\n        errD_real.backward(retain_graph=True)\n        D_x = output.mean().item()\n        \n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        label = torch.FloatTensor(np.random.uniform(low=0.005, high=0.155, size=(images.size(0)))).to(device)\n#         label.fill_(fake_label)\n        output = netD(inp_x_fake).view(-1)\n        errD_fake = criterion(output, label)\n        errD_fake.backward(retain_graph=True)\n        D_G_z1 = output.mean().item()\n        \n        errD = errD_real + errD_fake\n        \n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n#         label.fill_(real_label)\n        output = netD(inp_x_fake).view(-1)\n        \n        errG = criterion(output, label) + 4*l1criterion(images,fake_images)\n        errG.backward(retain_graph=True)\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        \n        netE.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x_fake).view(-1)\n\n        errE = criterion(output, label) + 4*l1criterion(images,fake_images)\n        errE.backward(retain_graph=True)\n        E_G_z2 = output.mean().item()\n        optimizerE.step()\n        \n        #################################_______STATS________###########################################\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLoss_E: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(),errE.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        E_losses.append(errE.item())\n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n#         if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n#             netG.eval()\n#             with torch.no_grad():\n#                 fake = netG(fixed_noise).detach().cpu()\n#                 fake[:] = fake[:]*0.5 + 0.5\n#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n        del images\n        del inp_x_fake\n        del inp_x\n        del label\n        del output\n        torch.cuda.empty_cache()\n        iters += 1\n        \n        \n        \n        if i%500 ==0:\n            netE.eval()\n            netG.eval()\n            encoded_img = netE(valid_batch)\n            reconstructed_img = netG(encoded_img)\n            f, axarr = plt.subplots(num_images_to_show,2)\n            for i in range(num_images_to_show):\n                validimg = (valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n                rec_img = (reconstructed_img[i].cpu().detach().permute(1, 2, 0) *0.5 ) + 0.5\n                axarr[0].imshow(validimg)\n                axarr[1].imshow(rec_img)\n                f.set_figheight(20)\n                f.set_figwidth(20)\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3fc7d82-31b4-4985-9d55-62816769136c","_cell_guid":"60bc2609-e9fd-4705-a335-e4e17f92db9b","trusted":true},"cell_type":"code","source":"print(\"Evaluating the model ...\")\nnetE.eval()\nnetG.eval()\ntot_img_size = IMG_WIDTH * IMG_HEIGHT * 3\n# print(\"Size reduction is : \"+ str(float(encode_size/tot_img_size)*100.0)+\" percent\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71fb24d1-4bc4-497f-bac2-9c7272cf7247","_cell_guid":"5242517f-745a-4a6d-ba39-0ec5c3c73781","trusted":true},"cell_type":"code","source":"valid_dataset = ImageData(is_train=False)\nbatch_size=20\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"455c713c-47dc-49a1-b127-db82978fea5c","_cell_guid":"8b16da2d-dd02-4cd7-a846-858e10b1d498","trusted":true},"cell_type":"code","source":"print(valid_batch.shape)\nencoded_img = netE(valid_batch)\nprint(encoded_img.shape)\nreconstructed_img = netG(encoded_img)\nprint(reconstructed_img.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13d5c95e-e08d-4a01-a5be-905804e6df0c","_cell_guid":"3350ab34-4f26-47a1-a12a-c20dae0ba8a6","trusted":true},"cell_type":"code","source":"num_images_to_show = 5\nf, axarr = plt.subplots(num_images_to_show,2)\nfor i in range(num_images_to_show):\n    validimg = (valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n    rec_img = (reconstructed_img[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n    axarr[i,0].imshow(validimg)\n    axarr[i,1].imshow(rec_img)\n    f.set_figheight(20)\n    f.set_figwidth(20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(netE.state_dict(), \"netE\"+str(num_channels_in_encoder)+\".model\")\ntorch.save(netG.state_dict(), \"netG\"+str(num_channels_in_encoder)+\".model\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}