{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"iris = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\nprint(iris.head())\nprint(iris.info())\niris.drop('Id',axis=1,inplace=True)\nprint(iris.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nThe above graph shows **relationship between the SEPAL length and width**. \n\n\nNow we will check **relationship between the PETAL length and width**.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfig = iris[iris.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=iris)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,4)) \nsns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# **Splitting The Data into Training And Testing Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features\ntrain_y=train.Species# output of our training data\n\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features\ntest_y =test.Species   #output value of test data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Support Vector Machine (SVM)**\nSupport Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives.The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n\nThe advantages of support vector machines are:\n\n        Effective in high dimensional spaces.\n\n        Still effective in cases where number of dimensions is greater than the number of samples.\n\n        Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n        Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n        If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n\n        SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n\n**References:**\n1. https://scikit-learn.org/stable/modules/svm.html\n1. https://www.sciencedirect.com/topics/neuroscience/support-vector-machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC() #select the algorithm\nmodel.fit(train_X,train_y) # we train the algorithm with the training data and the training output\nprediction=model.predict(test_X) #now we pass the testing data to the trained algorithm\nprint('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_y))#now we check the accuracy of the algorithm. \n#we pass the predicted output by the model and the actual output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# **K-Nearest Neighbours**\nThe K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc. \nThe intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=KNeighborsClassifier(n_neighbors=15) #this examines 3 neighbours for putting the new data into a class\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 26\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\n#The next step is to split our dataset into its attributes and labels. To do so, use the following code:\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt1\nimport pandas as pd\nfrom sklearn import neighbors, datasets\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n\n# Assign colum names to the dataset\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n\n# Read dataset to pandas dataframe\niris = datasets.load_iris()\n\n#getting label names i.e the three flower species\ny_names = iris.target_names\n\n\n#The next step is to split our dataset into its attributes and labels. To do so, use the following code:\nX = iris.data[:, :2] #array of the data\ny = iris.target #array of labels (i.e answers) of each data entry\n\n\n'''To avoid over-fitting, we will divide our dataset into training and test splits, which gives us a better idea as to how our algorithm performed during the testing phase. This way our algorithm is tested on un-seen data, as it would be in a production application.\nTo create training and test splits, execute the following script: The above script splits the dataset into 80% train data and 20% test data. This means that out of total 150 records, the training set will contain 120 records and the test set contains 30 of those records.'''\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n'''Before making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated. '''\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n'''It is extremely straight forward to train the KNN algorithm and make predictions with it, especially when using Scikit-Learn.'''\n'''The first step is to import the KNeighborsClassifier class from the sklearn.neighbors library. In the second line, this class is initialized with one parameter, i.e. n_neigbours. This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm.'''\n\nclassifier = KNeighborsClassifier(n_neighbors=12)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\n'''For evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The confusion_matrix and classification_report methods of the sklearn.metrics can be used to calculate these metrics. Take a look at the following script:'''\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n'''In the training and prediction section we said that there is no way to know beforehand which value of K that yields the best results in the first go. We randomly chose 5 as the K value and it just happen to result in 100% accuracy. One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.'''\nerror = []\n# Calculating error for K values between 1 and 40\nfor i in range(1, 100):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))\n    print('The accuracy of {i} is {result}'.format(i=i, result=metrics.accuracy_score(pred_i,y_test)))\n    \n\n    \n'''From the output we can see that the mean error is zero when the value of the K is between 5 and 18. '''\nplt.rcParams.update({'font.size': 16})\nplt.figure(figsize=(50,25))\nplt.plot(range(1, 100), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=20)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.xticks(np.arange(0, len(error)+1, 1))\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n#loading the iris dataset\niris = load_iris()\n\nx = iris.data #array of the data\ny = iris.target #array of labels (i.e answers) of each data entry\n\n#getting label names i.e the three flower species\ny_names = iris.target_names\n\n#taking random indices to split the dataset into train and test\ntest_ids = np.random.permutation(len(x))\n\n#splitting data and labels into train and test\n#keeping last 10 entries for testing, rest for training\n\nx_train = x[test_ids[:-10]]\nx_test = x[test_ids[-10:]]\n\ny_train = y[test_ids[:-10]]\ny_test = y[test_ids[-10:]]\n\n#classifying using decision tree\nclf = tree.DecisionTreeClassifier()\n\n#training (fitting) the classifier with the training set\nclf.fit(x_train, y_train)\n\n#predictions on the test dataset\npred = clf.predict(x_test)\n\nprint(pred) #predicted labels i.e flower species\nprint(y_test) #actual labels\nprint((accuracy_score(pred, y_test))*100) #prediction accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Deep learning with Keras**\n*https://codelabs.developers.google.com/codelabs/tensorflow-lab1-helloworld#2\n\nKeras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n\n\n#Next, create the simplest possible neural network. It has one layer, that layer has one neuron, and the input shape to it is only one value.\nmodel = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\n\n\n#You can see that the relationship between those is that Y=3X+1, so where X is -1, Y is -2.\nxs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\nys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)\n\n'''The process of training the neural network, where it learns the relationship between the X‘s and Y‘s, is in the model.fit call. \nThat's where it will go through the loop before making a guess, measuring how good or bad it is (the loss), or using the optimizer to make another guess.'''\nmodel.fit(xs, ys, epochs=500)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the model is not calculating, but predicting. In this example the relation is known, \n#but in real life situations the relations are unknown or partially known. \n#Some relations are not discovered if not after, and some relations are be extremly hard to detect. \n#Depending on the number of features and layers debugging deep learning models could be a daunting task.\nprint(model.predict([10.0]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Saving models**\n\nhttps://medium.com/fintechexplained/how-to-save-trained-machine-learning-models-649c3ad1c018\n"},{"metadata":{},"cell_type":"markdown","source":"# **pickle**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport pickle\nmodel = LogisticRegression()\nmodel.fit(xtrain, ytrain)\n\n# save the model to disk\npickle.dump(model, open(model_file_path, 'wb'))\n\n#load the model\nmodel = pickle.load(open(model_file_path, 'rb'))result_val = model.score(xval, yval)\nresult_test = model.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing our trained model into another program**"},{"metadata":{"trusted":true},"cell_type":"code","source":"    import pickle\n    # Import our model\n    with open('area_model.pickle', \"rb\") as file:\n        regression_model = pickle.load(file)\n    # Ask the user to enter an area and calculate \n    # its price using the imported model\n    input_area = int(input(\"Enter area: \"))\n    proped_price = regression_model.predict([[input_area]])\n    print (\"Proped price:\", round(proped_price[0], 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **JobLib**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nmodel = LogisticRegression()\nmodel.fit(xtrain, ytrain)\n\n# save the model to disk\njoblib.dump(model, model_file_path)\n\n#deserialize\nmodel = joblib.load(model_file_path)result_val = model.score(xval, yval)\nresult_test = model.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Deployment ML-Flask APPS**:\n* https://blog.cambridgespark.com/deploying-a-machine-learning-model-to-the-web-725688b851c7\n* https://www.analyticsvidhya.com/blog/2020/04/how-to-deploy-machine-learning-model-flask/\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}