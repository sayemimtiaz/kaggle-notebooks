{"cells":[{"metadata":{},"cell_type":"markdown","source":"Original Credit: Brandon Eychaner @hbeychaner\n\n\n\n# Pre-processing Pipeline\n## COVID-19 Open Research Dataset Challenge (CORD-19)\n\n### Task Details\nWhat is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n\nThe first question we need to ask is what we mean by transmission, incubation, and environmental stability -- or, rather, what should a computer understand when we ask this? We can go about encoding this information in several ways: 1) keywords for analysis in some kind of TF-IDF format, probably including a list of synonyms that we would need to develop by hand, 2) high-dimensional vectors vis a vis word2vec or GloVe, or 3) using heavy, but state-of-the-art transformer models for vectorization. \n\nKeywords probably aren't going to give us the robust results we're looking for, because typical pre-processing methods remove all sorts of punctuation and numbers, but these are really important in biomedical texts! We could skip the pre-processing except for removing stop words, but we'd still need to address the fact that keywords have synonyms, and we'd need to hand-write these. But there may be an easier way to get better results without all the hassle. \n\nI propose method 2: spaCy is a popular NLP package that's blazingly fast and has (mostly) everything we need to process the text. It'll break sentences apart, lemmatize, and even provide vectors for us. Spacy vectors are somewhat simplistic because the vector of several tokens is just the average of the vectors of each token individually -- so we may not get state of the art results. But we'll get them fast, and we'll know if we need to change something up!"},{"metadata":{"trusted":false},"cell_type":"code","source":"#!pip install spac scispacy spacy_langdetect https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_core_sci_lg-0.2.3.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import spacy \nimport scispacy\nimport pandas as pd \nimport os\nimport numpy as np\n#import scispacy\nimport json\nfrom tqdm.notebook import tqdm\nfrom scipy.spatial import distance\nimport ipywidgets as widgets\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom spacy_langdetect import LanguageDetector\n# UMLS linking will find concepts in the text, and link them to UMLS. \nfrom scispacy.umls_linking import UmlsEntityLinker\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time for NLP!\n\nLet's load our language model. Based on the type of text we'll be dealing with, we want something that's been pretrained on biomedical texts, as the vocabulary and statistical distribution of words is much different from, say, the news or Wikipedia articles. Luckily, there's already pre-trained models for spacy, so let's load the largest one we can! "},{"metadata":{"trusted":false},"cell_type":"code","source":"#nlp = spacy.load(\"en_core_sci_lg\")\nnlp = spacy.load(\"en_core_sci_lg\", disable=[\"tagger\"])\n# If you're on kaggle, load the model with the following, if you run into an error:\n#nlp = spacy.load(\"/opt/conda/lib/python3.6/site-packages/en_core_sci_lg/en_core_sci_lg-0.2.3/\", disable=[\"tagger\"])\n\n# We also need to detect language, or else we'll be parsing non-english text \n# as if it were English. \nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n\n# Add the abbreviation pipe to the spacy pipeline. Only need to run this once.\nabbreviation_pipe = AbbreviationDetector(nlp)\nnlp.add_pipe(abbreviation_pipe)\n\n# Our linker will look up named entities/concepts in the UMLS graph and normalize\n# the data for us. \nlinker = UmlsEntityLinker(resolve_abbreviations=True)\nnlp.add_pipe(linker)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding a vector for COVID-19\n\nOne last thing. COVID-19 is a new word, and doesn't exist in the vocabulary for our spaCy model. We'll need to add it manually; let's try setting it to equal the average vector of words that should represent what COVID-19 refers to, and see if that works. I'm not an expert so I just took definitions from Wikipedia and the etiology section of https://onlinelibrary.wiley.com/doi/full/10.1002/jmv.25740. There's a much better way of doing this (fine-tuning the model on our corpus) but I have no idea how to do this in spaCy..."},{"metadata":{"trusted":false},"cell_type":"code","source":"from spacy.vocab import Vocab\nnew_vector = nlp(\n               \"\"\"Single‐stranded RNA virus, belongs to subgenus \n                   Sarbecovirus of the genus Betacoronavirus.5 Particles \n                   contain spike and envelope, virions are spherical, oval, or pleomorphic \n                   with diameters of approximately 60 to 140 nm.\n                   Also known as severe acute respiratory syndrome coronavirus 2, \n                   previously known by the provisional name 2019 novel coronavirus \n                   (2019-nCoV), is a positive-sense single-stranded RNA virus. It is \n                   contagious in humans and is the cause of the ongoing pandemic of \n                   coronavirus disease 2019 that has been designated a \n                   Public Health Emergency of International Concern\"\"\").vector\n\nvector_data = {\"COVID-19\": new_vector,\n               \"2019-nCoV\": new_vector,\n               \"SARS-CoV-2\": new_vector}\n\nvocab = Vocab()\nfor word, vector in vector_data.items():\n    nlp.vocab.set_vector(word, vector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity Check\nAlright, let's check if this work. "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\n    nlp(\"COVID-19\").similarity(nlp(\"novel coronavirus\")), \"\\n\",\n    nlp(\"SARS-CoV-2\").similarity(nlp(\"severe acute respiratory syndrome\")), \"\\n\",\n    nlp(\"COVID-19\").similarity(nlp(\"sickness caused by a new virus\")))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I guess we'll find out if that's good enough for our purposes! Let's save it so other people can use it!"},{"metadata":{"trusted":false},"cell_type":"code","source":"#nlp.to_disk('/home/acorn/Documents/covid-19-en_lg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the texts is particularly long, so we need to increase the max_length attribute of nlp to more then 1.25mil. The alternative would be cutting the length of the article or dropping it entirely (I believe there's some sort of anomaly with this particular article), but we'll keep it for now. "},{"metadata":{"trusted":false},"cell_type":"code","source":"nlp.max_length=2000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we want to replace all abbreviations with their long forms. This is important for semantic indexing because the model has probably seen words like \"Multiple sclerosis\" but may have seen the abbreviation \"MS\" in different contexts. That means their vector representations are different, and we don't want that! \n\nSo here we'll add the abbreviation expansion module to our scispaCy pipeline. "},{"metadata":{"trusted":false},"cell_type":"code","source":"doc = nlp(\"Attention deficit disorcer (ADD) is treated using various medications. However, ADD is not...\")\n\nprint(\"Abbreviation\", \"\\t\", \"Definition\")\nfor abrv in doc._.abbreviations[0:10]:\n\tprint(f\"{abrv} \\t ({abrv.start_char}, {abrv.end_char}) {abrv._.long_form}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice we get some weird results towards the end if you print **all** of them (lots of a's being converted to at's, but we can ignore that for now. If we need to remove stop words later, we can. "},{"metadata":{},"cell_type":"markdown","source":"### Making the Vector DataFrames\nAppending to a dataframe increases time to copy data linearly because df.append copies the entire object. The following will take an article's text, break it into sentences, and vectorize each sentence (using scispacy's pre-trained word2vec model). Finally, the list of dicts is loaded as a DataFrame and saved.\n\nSo here's the real meat of our pre-processing. This is really heavy because it processes line-by-line and then generates a lot of metadata (entities, vectors). We can break it into pieces later depending on the task we want to use this information for, but querying lines is a lot more useful that querying whole documents when you want to know about something specific like seroconversion, spike proteins, or something else. Once you identify lines of interest, you can generate more data about the actual document, since each line will be indexed with document, start and end character, entities, vectors, and language. \n\n#### Lemmatized Text\n\nJust in case we need it, let's do some text cleaning and include that in a different column. Lemmatization normalizes data so that when you're creating word clouds or simplified TF-IDF, the number of dimesions you're dealing with are significantly reduced. It's also nice to remove words that don't contribute much meaning, but do note that removing stop-words will make neural models less accurate depending on the task you're using them for.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def df_cleaner(df):\n    df.fillna(\"Empty\", inplace=True) # If we leave floats (NaN), spaCy will break.\n    for i in df.index:\n        for j in range(len(df.columns)):\n            if \" q q\" in df.iloc[i,j]:\n                df.iloc[i,j] = df.iloc[i,j].replace(\" q q\",\"\") # Some articles are filled with \" q q q q q q q q q\"\n\n# Convenience method for lemmatizing text. This will remove punctuation that isn't part of\n# a word. \ndef lemmatize_my_text(doc):\n    lemma_column = []\n    for i in df.index:\n        if df.iloc[i][\"language\"] == \"en\":\n            doc = nlp(str(df.iloc[i][column]), disable=[\"ner\",\"linker\", \"language_detector\"])\n            lemmatized_doc = \" \".join([token.lemma_ for token in doc])\n            lemma_column.append(lemmatized_doc)\n        else: \n            lemma_column.append(\"Non-English\")\n    return lemma_column\n\n#Unnabreviate text. This should be done BEFORE lemmatiztion and vectorization. \ndef unnabreviate_my_text(doc):\n    if len(doc._.abbreviations) > 0 and doc._.language[\"language\"] == \"en\":\n        doc._.abbreviations.sort()\n        join_list = []\n        start = 0\n        for abbrev in doc._.abbreviations:\n            join_list.append(str(doc.text[start:abbrev.start_char]))\n            if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n                join_list.append(str(abbrev._.long_form))\n            else:\n                join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n            start = abbrev.end_char\n        # Reassign fixed body text to article in df.\n        new_text = \"\".join(join_list)\n        # We have new text. Re-nlp the doc for futher processing!\n        doc = nlp(new_text)\n        return(doc)\n    \ndef pipeline(df, column, dataType, filename):\n    create = pd.DataFrame(columns={\"_id\",\"language\",\"section\",\"sentence\",\"startChar\",\"endChar\",\"entities\",\"lemma\",\"w2vVector\"})\n    create.to_csv(filename + \"_text_processed\" + \".csv\", index=False)\n    \n    docs = nlp.pipe(df[column].astype(str))\n    i = -1\n    for doc in tqdm(docs):\n        languages = []\n        start_chars = []\n        end_chars = []\n        entities = []\n        sentences = []\n        vectors = []\n        _ids = []\n        columns = []\n        lemmas = []\n        i = i + 1\n        \n        if doc._.language[\"language\"] == \"en\" and len(doc.text) > 5:\n            for sent in doc.sents:\n                languages.append(doc._.language[\"language\"])\n                sentences.append(sent.text)\n                vectors.append(sent.vector)\n                start_chars.append(sent.start_char)\n                end_chars.append(sent.end_char)\n                doc_ents = []\n                for ent in sent.ents: \n                    if len(ent._.umls_ents) > 0:\n                        poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n                        doc_ents.append(poss)\n                entities.append(doc_ents)\n                _ids.append(df.iloc[i,0])\n                if dataType == \"tables\":\n                    columns.append(df.iloc[i][\"figure\"])\n                elif dataType == \"text\":\n                    columns.append(column)\n                lemmatized_doc = \" \".join([token.lemma_ for token in doc])\n                lemmas.append(lemmatized_doc)\n        else: \n            start_chars.append(0)\n            end_chars.append(len(doc.text))\n            entities.append(\"Non-English\")\n            sentences.append(doc.text)\n            vectors.append(np.zeros(200))\n            _ids.append(df.iloc[i,0])\n            languages.append(doc._.language[\"language\"])\n            if dataType == \"tables\":\n                columns.append(df.iloc[i][\"figure\"])\n            elif dataType == \"text\":\n                columns.append(column)\n            lemmas.append(\"Non-English\")\n            \n        rows = pd.DataFrame(data={\"_id\": _ids, \"language\": languages, \"section\": columns, \"sentence\": sentences, \n            \"startChar\": start_chars, \"endChar\": end_chars, \"entities\": entities, \"lemma\": lemmas, \"w2vVector\":vectors})\n        rows.to_csv(filename, mode='a', header=False, index=False)\n        del rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"files = [f for f in os.listdir(\"./unnabreviated_parts/\") if f.startswith(\"unna\") and not f.endswith(\"csv\")]\nfor f in tqdm(files):\n    f = \"./unnabreviated_parts/\" + f\n    df = pd.read_csv(f)\n    pipeline(df=df, column=\"text\", dataType=\"text\", filename=\"tables_unnabrev_lemma\")\n    os.remove(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_list = []\ndf = pd.concat([i for i in [pd.read_csv(f) for f in files]])\ntimestamp = time.strftime(\"%Y%m%d\")\ndf.to_csv(f\"covid_TitleAbstract_processed-{timestamp}.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}