{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Deep topic model for COVID-19\n\n<img src=\"https://img-blog.csdnimg.cn/20200416180916778.png\" width = \"600\" height = \"500\" alt=\"tree\" align=center />\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https://pages.semanticscholar.org/coronavirus-research) and there is a research challenge on [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n\n\n\nThe goal of this project is to discover interesting hierarchical topic information from a large number of covid-19 research corpus. In details, we use a [deep topic model](https://arxiv.org/abs/1511.02199) to analyze the semantic structure of articles. The first-layer topics denote the combination of words, while the higher-level topics denote the combination of the lower-level topics, Therefore, these topics describe the global word co-occurrence tatistics and hierarchical semantics from detailed to coarse.\n\n\nAn example of hierarchical topics learned from cord-19 corpus is shown below. We can conclude that covid-19 has the following four characteristics: group, lung infection, animal-related,and infectious.\n\n\nAll results are saved in [dropbox](https://www.dropbox.com/home/GBN_Covid19/plot_images_tree)"},{"metadata":{},"cell_type":"markdown","source":"## install from github\nFull source code for this project is on [GitHub](https://github.com/wds2014/topic_model_for_Covid) and be installed into this notebook as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!echo -e \"StrictHostKeyChecking no\\n\" >> ~/.ssh/config\n!git clone https://github.com/wds2014/topic_model_for_Covid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd topic_model_for_Covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## install scibert for word representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers\n!wget -O scibert_uncased.tar https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n!tar -xvf scibert_uncased.tar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## have a look at the data form\ncite from [maksimeren](https://www.kaggle.com/maksimeren/covid-19-literature-clustering)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport tqdm\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.info()\ndel meta_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## load the CORD-19 data\nThe raw CORD-19 data is stored across a metadata.csv file and json files with the full text, \nwe just use the abstract and full-text frames in our topic model. \nThe following code returns a txt file, where each line contains the summary and full text words of the article, corresponding to a json file. we remove articles which contain less then 20 words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from get_doc import *\nbiorxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\ncomm_dir = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'\ncustom_dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/'\nnoncomm_dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/'\njson_files = [biorxiv_dir, comm_dir, custom_dir, noncomm_dir]\njson_files_names= ['biorxiv_dir', 'comm_dir', 'custom_dir', 'noncomm_dir']\ndoc_info = []\nfor each_files, each_files_name in zip(json_files, json_files_names):\n    filenames = os.listdir(each_files)\n    print(\"Number of articles retrieved from {} : {}\".format(each_files_name, len(filenames)))\n    all_files = []\n\n    for filename in filenames:\n        filename = each_files + filename\n        file = json.load(open(filename, 'rb'))\n        all_files.append(file)\n\n    cleaned_files = []\n    doc_num = 0\n    with open('{}.txt'.format(each_files_name),'w') as f:\n        for file in all_files:\n            doc = format_body(file['abstract']) + format_body(file['abstract'])\n            \n            if len(doc) > 20:\n                doc_info.append((file['paper_id'], file['metadata']['title'], file['metadata']['authors']))\n                f.write(doc)\n                f.write('\\n')\n                doc_num +=1\n    print('doc_num : ', doc_num)\nprint('doneeeee')\ndel all_files\ndel doc\ndel file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## clean the txt data\n\nthis project extracts the bag-of-word feature of each article."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport numpy as np\nimport scipy.sparse as sp\nfrom tokenizer import Tokenizer\n \n\npath1 = 'biorxiv_dir.txt'\npath2 = 'comm_dir.txt'\npath3 = 'custom_dir.txt'\npath4 = 'noncomm_dir.txt'\ncorpus = []\n\nwith open(path1) as f:\n    lines = f.readlines()\nfor line in lines:\n    corpus.append(line.strip())\n\nwith open(path2) as f:\n    lines = f.readlines()\nfor line in lines:\n    corpus.append(line.strip())\nwith open(path3) as f:\n    lines = f.readlines()\nfor line in lines:\n    corpus.append(line.strip())\nwith open(path4) as f:\n    lines = f.readlines()\nfor line in lines:\n    corpus.append(line.strip())\nprint('total doc :',len(corpus))\nvectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=10000,tokenizer=Tokenizer.tokenize)\n\nX = vectorizer.fit_transform(corpus)\nvoc = vectorizer.vocabulary_\nvectorizer = CountVectorizer(vocabulary=voc, tokenizer=Tokenizer.tokenize)\nX = vectorizer.fit_transform(corpus)\nvoc = vectorizer.get_feature_names()\n\nsp.save_npz('cord19_10000_full.npz', X)\nnp.save('voc_10000_full.npy',voc)\nwith open('voc_10000_full.txt','w') as f:\n    for word in voc:\n        f.write(word)\n        f.write('\\n')\nprint('doneeeeee')\ndel corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using wordcloud to visualize the global information of the corpus。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndoc_data = X.toarray()\ndoc_fre = np.sum(doc_data,0)\nword_fre = {}\nfor idx, word in enumerate(voc):\n    word_fre[word] = doc_fre[idx]\nwc = WordCloud(max_words=1000)\nwc.generate_from_frequencies(word_fre)\n\n# show\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()\ndel word_fre\ndel doc_fre\ndel X\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## build the topic model([PGBN](https://arxiv.org/abs/1511.02199))\nDue to CPU memory limitations, you can load pre-trained parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PGBN import PGBN\nfrom scipy import sparse as sp\nimport pickle\nimport numpy as np\npre_trained = True\nif not pre_trained:\n    pgbn = PGBN(doc_data.T,K=[400,200,64,32],voc=voc)\n    pgbn.train('./output',iteration =2000)\n    Phi = pgbn.Phi\n    Theta = pgbn.Theta\nelse:\n    voc = np.load('/kaggle/input/pre-trained/data/output/voc_10000.npy')\n    doc_data = sp.load_npz('/kaggle/input/pre-trained/data/output/cord19_10000.npz')\n    doc_data = doc_data.toarray()\n    pgbn = PGBN(doc_data.T,K=[400,200,64,32],voc=voc)\n    with open('/kaggle/input/pre-trained/data/output/Phi.pick','rb') as f:\n        Phi = pickle.load(f)\n    with open('/kaggle/input/pre-trained/data/output/Theta.pick','rb') as f:\n       Theta = pickle.load(f) \n#pgbn.Phi_vis(Phi)\n#graph = pgbn.show_tree(Phi)\n#graph.render('output/tree')\ndel doc_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT\nwe use pre-trained bert as a encoder to extract word embedding. in detail, when a query (here is the task or the question) comes, we match it with the trained topic in the semantic space， to this end, the topics most relevant to the task will be found based on the scores obtained. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport heapq\nmodel_version = 'scibert_scivocab_uncased'\ndo_lower_case = True\nmodel = BertModel.from_pretrained(model_version)\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef embed_text(text, model):\n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n    return last_hidden_states.detach().numpy()[0,1:-1,:]\ndef topic_match_v1(q, k, model, q_weight=None, greedy=False):\n    q_embed = embed_text(q, model) ## n_word*768\n    if q_weight:\n        q_embed = q_embed * q_weight\n        q_embed = q_embed.sum(0, keepdims=True)\n    else:\n        q_embed = q_embed.mean(0, keepdims=True)\n    score = []\n    for each_k in k:\n        k_embed = embed_text(each_k, model).mean(0, keepdims=True)  ## 1*768\n        score.append(cosine_similarity(q_embed, k_embed)[0][0])\n        if greedy:\n            k_embed = embed_text(each_k, model)  ##  n_word * 768\n            word_score = cosine_similarity(q_embed, k_embed)  ## 1*n_word\n            score.append(word_score[0][np.argmax(word_score[0])])\n    max_num_index_list = map(score.index, heapq.nlargest(5, score))\n    return list(max_num_index_list)\n\ndef topic_sents(path):\n    k = []\n    with open(path) as f:\n        lines = f.readlines()\n    for line in lines:\n        k.append(line.strip())\n    return k\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_word = topic_sents('/kaggle/input/pre-trained/data/output/phi3.txt')\ntop_id = topic_match_v1('medical care', topic_word, model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## show topics related to Medical Care\nwe build four-layer topic model for this task, and show the hierarchical topics most related to Medical Care below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = pgbn.show_tree(Phi,topic_id=top_id[0],threshold=0.05)\ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## get articles about this task\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_doc_id(layer, top_id=[0,1], top_k = 100):\n    target_Theta=Theta[layer].T\n    \n    topic_list=[]\n    for i in top_id:\n        data = np.argsort(target_Theta[:,i])[-top_k:]\n        data=data[::-1]\n        topic_list.extend(data)\n    \n    return topic_list\nwith open('/kaggle/input/pre-trained/doc_info.pkl','rb')as f:\n    doc_info = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport pandas as pd\ndoc_id = get_doc_id(3, top_id=top_id)\n\ndata=[]\nfor i in doc_id:\n    #data.append((doc_info[i][0],doc_info[i][1],doc_info[i][2]['first']+ doc_info[i][2]['last']))\n    names=\"\"\n    for j in range(len(doc_info[i][2])):\n        names+=doc_info[i][2][j]['first']+ doc_info[i][2][j]['last']+\" \"\n        \n    data.append((doc_info[i][0],doc_info[i][1],names))\n\ncsvfile = open('info.csv', 'w')\nwriter = csv.writer(csvfile)\nwriter.writerow(['Paper ID', 'Title', 'Authors'])\nwriter.writerows(data)\ncsvfile.close()\nmeta_df = pd.read_csv('info.csv')\nmeta_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pros and cons\nThis project is based on the deep topic model, which can discover the hierarchical semantic concepts in the covid-19 corpus, for example the medical care. And, given a topic of interest, you can find the articles most relevant to the topic based on the topic proportion.\n### Pros\n1. Topic model can learn concepts in the corpus, help us overall, quickly understand what these articles is discussing. Further, the deep model can explore the hidden hierarchical relationships between topics. For example, we focus on medical care and use it as the root node. We can get sub-topics about it, such as nursing, Extracorporeal membrane oxygenation (ECMO), interventions and so on.\n2. Besed on the topic proportion of each article, We can collect the articles that are most relevant to the topic of interest。\n3. this preject use pre-trained scibert as encoder, which achieved state-of-art in Natural Language Processing (NLP). Scibert emcode both questions and topics into the same semantic space, which is useful for downstream task.\n\n### Cons\n1. As seen above, topics are composed of keywords, often requiring domain experts to discover more interesting phenomena.\n2. There are some duplicate topics in the result"},{"metadata":{},"cell_type":"markdown","source":"## Answer the questions\n\n### given the questions as query, return the hierarchical topic structure and papers related to the question.\n\n* [Resources to support skilled nursing facilities and long term care facilities.](https://www.kaggle.com/danawan/covid-19-q?scriptVersionId=32117016)\n* [Mobilization of surge medical staff to address shortages in overwhelmed communities](https://www.kaggle.com/goyeah/covid-19-q-2)\n* [Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies](https://www.kaggle.com/goyeah/covid-19-q-3)\n* [Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients](https://www.kaggle.com/goyeah/covid-19-q-4)\n* [Outcomes data for COVID-19 after mechanical ventilation adjusted for age.](https://www.kaggle.com/goyeah/covid-19-q-5)\n* [Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.](https://www.kaggle.com/goyeah/covid-19-q-6)\n* [Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.](https://www.kaggle.com/goyeah/covid-19-q-7)\n* [Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.](https://www.kaggle.com/goyeah/covid-19-q-8)\n* [Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.](https://www.kaggle.com/goyeah/covid-19-q-9)\n* [Guidance on the simple things people can do at home to take care of sick people and manage disease.](https://www.kaggle.com/goyeah/covid-19-q-10)\n* [Oral medications that might potentially work.](https://www.kaggle.com/goyeah/covid-19-q-11)\n* [Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.](https://www.kaggle.com/goyeah/covid-19-q-12)\n* [Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.](https://www.kaggle.com/goyeah/covid-19-q-13)\n* [Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials](https://www.kaggle.com/goyeah/covid-19-q-14)\n* [Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials](https://www.kaggle.com/goyeah/covid-19-q-15)\n* [Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)](https://www.kaggle.com/goyeah/covid-19-q-16)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}