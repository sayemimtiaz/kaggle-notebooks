{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a little notebook for anyone who wants to practice his/her Python skills trying to tackle a real world problem.\nA very simple machine learning model is at the bottom, for the most patient of you. So let's begin!"},{"metadata":{},"cell_type":"markdown","source":"Step 1: The various necessary imports:"},{"metadata":{"_uuid":"102d3255-501a-4202-ad9d-ae752350464d","_cell_guid":"063864f3-8809-4f23-8998-ff88e622c059","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport time\nimport datetime as dt\nfrom datetime import date\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2: Importing the data! Then: dropping unnecessary column and having a quick look."},{"metadata":{"trusted":true},"cell_type":"code","source":"covdata_filepath = '/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv'\ncovdata = pd.read_csv(covdata_filepath)\n#covdata.head()\ncovdata['Last Update'] = covdata['Last Update'].apply(pd.to_datetime)\ncovdata.drop(['SNo'],axis=1,inplace=True)\ncovdata.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another quick look: list of countries affected by the virus"},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = covdata['Country/Region'].unique().tolist()\nprint(countries)\nprint(\"\\nTotal countries affected by virus: \",len(countries))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3: Beginning to look closely at the data. We start from China, for logical reasons. Quick look..."},{"metadata":{"trusted":true},"cell_type":"code","source":"covdata_china = covdata.loc[covdata['Country/Region'] == 'Mainland China']\ncovdata_china.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... then dive in! Summing up all the cases of the various provinces, and comparing with the Hubei province, the biggest player for China."},{"metadata":{"trusted":true},"cell_type":"code","source":"Mainland_China_provinces = covdata_china['Province/State'].unique().tolist()\nprint(Mainland_China_provinces)\ncases = 0\nfor province in Mainland_China_provinces:\n    province = covdata_china.loc[covdata_china['Province/State'] == province]\n    cases += province['Confirmed'].iloc[[-1][0]]\nprint('Mainland China total cases number is : ', int(cases))\n\nhubei = covdata_china.loc[covdata_china['Province/State'] == 'Hubei']\n#hubei.tail()\nhubei_cases = hubei['Confirmed'].iloc[[-1][0]]\nprint('Hubei confirmed cases: ', int(hubei_cases))\n\nprint('Mainland China cases w/ Hubei: ', int(cases - hubei_cases))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Closer look at Hubei province. We create a pandas dataframe, to handle the data in a nicer way. Then we plot confirmed cases, deaths & recovered to get the full picture. Nice (but also very sad) logistical curves!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hubei = pd.DataFrame(hubei)\ndates = df_hubei.ObservationDate\nx = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df_hubei.Confirmed, '.y')\nax.plot(x, df_hubei.Deaths, '.r')\nax.plot(x, df_hubei.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now jump the Pacific and look at the USA data. Very sad! Same procedure as before. NY the most affected.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"covdata_US = covdata.loc[covdata['Country/Region'] == 'US']\nUS_states = covdata_US['Province/State'].unique().tolist()\n#print(US_states)\ncases = 0\ncases_max = cases\nfor state in US_states:\n    state = covdata_US.loc[covdata_US['Province/State'] == state]\n    cases_state = state['Confirmed'].iloc[[-1][0]]\n    cases += cases_state\n    if cases_state > cases_max:\n        cases_max = cases_state\n        state_with_max_number = state['Province/State'].iloc[-1]\n        #print('in {} there are {} cases.'.format(state['Province/State'].iloc[-1], cases_max))\nprint('US total cases number is: ', int(cases))\nprint('The US state/province with the max number of cases is: {}, having {} cases'.format(state_with_max_number, int(cases_max)))\nNY = covdata_US.loc[covdata_US['Province/State'] == 'New York']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Closer look at NY state. We plot confirmed cases, deaths & recovered to get the full picture. Unfortunately we still have exponential increases :("},{"metadata":{"trusted":true},"cell_type":"code","source":"df_NY = pd.DataFrame(NY)\ndates = df_NY.ObservationDate\nx = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df_NY.Confirmed, '.y')\nax.plot(x, df_NY.Deaths, '.r')\nax.plot(x, df_NY.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I must divert your attention to Italy, my beloved home Country. We simplify taking the most meaningful data, starting from 15/02/2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"covdata_new = covdata[covdata['Last Update'] > pd.Timestamp(date(2020,2,15))]\ncovdata_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a pandas dataframe for Italy..."},{"metadata":{"_uuid":"89702945-3495-47fc-90bd-4a44a5226e39","_cell_guid":"16f8d980-3d6c-4774-9384-83045df502b5","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(covdata_new)\ndf = df.loc[df['Country/Region'] == 'Italy']\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..and plotting, same as before! Still a very steep increase, but nowhere near an exponential (Gott sei Dank!). I think we are near the change of curvature of the function, or just past it."},{"metadata":{"_uuid":"4a910edd-0c99-49b3-bfd1-514efaa5a0c4","_cell_guid":"2559bf23-0f75-47e0-a99a-9dec56da4566","trusted":true},"cell_type":"code","source":"dates = df.ObservationDate\nx = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df.Confirmed, '.y')\nax.plot(x, df.Deaths, '.r')\nax.plot(x, df.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting with a logarithmic x axis, to get a more mathematical view. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_yscale('log')\nax.plot(x, df.Confirmed, '.y')\nax.plot(x, df.Deaths, '.r')\nax.plot(x, df.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But, as the virus cannot care less of the dates, we need something more meaningful on the graph. For an exponential phenomenon, the natural scale is log - log. Most interesting are the new cases, so we want to plot them on the y-axis. But first we need to calculate them. We do this very simply:"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_Confirmed = [0]\nnew_Deaths = [0]\nnew_Recovered = [0]\nfor i in range(1, (df.Confirmed).size):\n    new_Confirmed.append(df.Confirmed.tolist()[i]-df.Confirmed.tolist()[i-1])\n    new_Deaths.append(df.Deaths.tolist()[i]-df.Deaths.tolist()[i-1])\n    new_Recovered.append(df.Recovered.tolist()[i]-df.Recovered.tolist()[i-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we can plot everything. Now we have more information to look at."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y', label = 'italian confirmed')\nax.plot(df.Deaths, new_Deaths,  '.r', label = 'italian dead')\nax.plot(df.Recovered, new_Recovered,  '.g', label = 'italian recovered')\n\nax.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But this still look messy. So we fit the data points, in order to have more insight on the growths. We do this in a very ugly way: don't do this at home. I'm going to fix this mess one day. The important part is to discard the infinities and nans. Saving our fit parameters a & b.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log10(new_Confirmed) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df.Confirmed.tolist()[i]))\n    i +=1    \nx_confirmed = np.asarray(cleanx); y = np.asarray(cleany)\na_confirmed, b_confirmed = np.polyfit(x_confirmed, y, 1)\n\ny = np.log10(new_Deaths) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df.Deaths.tolist()[i]))\n    i +=1\nx_deaths = np.asarray(cleanx); y = np.asarray(cleany)\na_deaths, b_deaths = np.polyfit(x_deaths, y, 1)\n\ny = np.log10(new_Recovered) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf:\n        if np.isnan(j) != True:\n            if df.Recovered.tolist()[i] != 0:\n                cleany.append(j)        \n                cleanx.append(np.log10(df.Recovered.tolist()[i]))\n    i +=1\nx_recovered = np.asarray(cleanx); y = np.asarray(cleany)\na_recovered, b_recovered = np.polyfit(x_recovered, y, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...and plotting everything. Sadly the deaths curve still grows more rapidly than the recovered one."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y')\nax.plot(10**(x_confirmed), 10**(a_confirmed*x_confirmed + b_confirmed), 'y', label = 'italian confirmed')\nax.plot(df.Deaths, new_Deaths,  '.r')\nax.plot(10**(x_deaths), 10**(a_deaths*x_deaths + b_deaths), 'r', label = 'italian dead')\nax.plot(df.Recovered, new_Recovered,  '.g')\nax.plot(10**(x_recovered), 10**(a_recovered*x_recovered + b_recovered), 'g', label = 'italian recovered')\n\nax.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To gain a little more insight, we do the same thing for Hubei and NY. "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_cases_hubei = [0]; new_cases_NY = [0]\nfor i in range(1, (df_hubei.Confirmed).size):\n    new_cases_hubei.append(df_hubei.Confirmed.tolist()[i]-df_hubei.Confirmed.tolist()[i-1])\nfor i in range(1, (df_NY.Confirmed).size):\n    new_cases_NY.append(df_NY.Confirmed.tolist()[i]-df_NY.Confirmed.tolist()[i-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log10(new_cases_NY) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df_NY.Confirmed.tolist()[i]))\n    i +=1    \nx_confirmed_NY = np.asarray(cleanx); y = np.asarray(cleany)\na_confirmed_NY, b_confirmed_NY = np.polyfit(x_confirmed_NY, y, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting everything together: it is quite apparent that the NY curve has a greater growth rate than the italian one. NY is unfortunately going to outpace the italian cases in the very near future, so Cuomo wake up! Hubei has the descending trend that everybody in the world looks forward to (trusting official data). We hope that it will soon come for everyone else, the sooner the better."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y')\nax.plot(10**(x_confirmed), 10**(a_confirmed*x_confirmed + b_confirmed), 'y', label = 'italian confirmed')\n\nax.plot(df_hubei.Confirmed, new_cases_hubei, '.k', label = 'Hubei_cases')\nax.plot(df_NY.Confirmed, new_cases_NY, '.b')\nax.plot(10**(x_confirmed_NY), 10**(a_confirmed_NY*x_confirmed_NY + b_confirmed_NY), 'b', label = 'NY confirmed')\n\nax.legend(loc = 'best')\nplt.gcf().autofmt_xdate()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 4 (final step): the machine learnig part. Just a simple LSTM (stacked means with more than 2 LSTM layers). Importing the models from keras..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...definig a function, used to split the data in sequences (with input and output part of the pattern)..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequence)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the sequence\n\t\tif end_ix > len(sequence)-1:\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... defining input sequence, choosing a number of time steps, splitting into samples, reshaping..."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_seq = np.array(new_Confirmed)\nn_steps = 5\nX, y = split_sequence(raw_seq, n_steps)\n\n# summarize the data\n#for i in range(len(X)):\n\t#print(X[i], y[i])\n    \n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...defining the model..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel1 = Sequential()\nmodel1.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel1.add(Dropout(0.1))\nmodel1.add(LSTM(50, activation='relu', return_sequences=True))\nmodel1.add(Dropout(0.1))\nmodel1.add(LSTM(50, activation='relu'))\nmodel1.add(Dense(1))\n\nstart = time.time()\nmodel1.compile(optimizer='adam', loss='mse')          \nprint('compilation time : ', time.time() - start)\nprint('\\n')\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...fitting the model! Not so joyfully :(\nAccuracy is demonstrated by testing the model on the last established data. It can vary a little bit if you run the code multiple times."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X, y, epochs=300, verbose=0)\nx_input = np.array(new_Confirmed)[-n_steps:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model1.predict(x_input, verbose=0)\ncases_forecast = int(round(yhat[0][0]))\nprint('cases forecast for today: {}'.format(cases_forecast))\n\naccuracy = 100*(1-np.abs(cases_forecast - new_Confirmed[-1])/new_Confirmed[-1])\nprint('forecast accuracy: {:.2f} %'.format(accuracy))\n\nprint('and for tomorrow?')\nx_input2 = np.array(new_Confirmed)[(-n_steps+1):]\nx_input2 = np.concatenate((x_input2, yhat), axis=None)\nx_input2 = x_input2.reshape((1, n_steps, n_features))\nyhat2 = model1.predict(x_input2, verbose=0)\nprint('tomorrow there might be {:.0f} cases'.format(int(yhat2[0][0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doing the same for the deaths prediction. If you want, you can change n_steps, add other LSTM layers or Dense layers as you please. The more you play around, the more you'll learn. \nFor this forecast, I used the simplest LSTM possible. As you can see, the results are still very grim. The accuracy is generally worse than with the previous model."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_seq = np.array(new_Deaths)\nn_steps = 4\nX, y = split_sequence(raw_seq, n_steps)\n\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n\n# define model\nmodel2 = Sequential()\nmodel2.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel2.add(LSTM(50, activation='relu', return_sequences=True))\nmodel2.add(LSTM(50, activation='relu'))\nmodel2.add(Dense(1))\n\nstart = time.time()\nmodel2.compile(optimizer='adam', loss='mse')          \nprint('compilation time : ', time.time() - start)\nprint('\\n')\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(X, y, epochs=300, verbose=0)\n\nx_input = np.array(new_Deaths)[-n_steps:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model2.predict(x_input, verbose=0)\n\ndeaths_forecast = int(round(yhat[0][0]))\nprint('deaths forecast for today: {}'.format(deaths_forecast))\n\naccuracy = 100*(1-np.abs(deaths_forecast - new_Deaths[-1])/new_Deaths[-1])\nprint('forecast accuracy: {:.2f} %'.format(accuracy))\n\nprint('and for tomorrow?')\nx_input2 = np.array(new_Deaths)[(-n_steps+1):]\nx_input2 = np.concatenate((x_input2, yhat), axis=None)\nx_input2 = x_input2.reshape((1, n_steps, n_features))\nyhat2 = model2.predict(x_input2, verbose=0)\nprint('tomorrow, in total, there might be {:.0f} deaths'.format(int(yhat2[0][0])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}