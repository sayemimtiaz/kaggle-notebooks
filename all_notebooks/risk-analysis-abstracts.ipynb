{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Our Team\n\nThis submission is the result of a collaboration between the following:\n\n* Catherine Doyle ([Catherine Doyle](https://www.kaggle.com/cddata))\n* John Doyle ([JohnDoyle](https://www.kaggle.com/johndoyle))\n* Keith Finnerty ([KeithFinnerty](https://www.kaggle.com/ketihf))\n* Piyush Rumao ([PIYUSH RUMAO](https://www.kaggle.com/piyushrumao))\n\nWe are all members of the Data Insights team in Deutsche Bank. Sitting within the Chief Data Office, the Data Insights team is the global Centre of Excellence for data science and data analytics within Deutsche Bank. Mostly based in Dublin, Ireland with some team members sitting in London and other locations, the Data Insights team comprises approximately forty people and includes data scientists, data analysts, data engineers, data architects, data visualization experts, and programme managers. We have expertise in areas such as advanced analytics, artificial intelligence, machine learning, natural language processing, visualization, dashboards, and software development.  We engage with teams across all areas and functions of the Bank, partnering with them to design and deliver analytics solutions that leverage the large amount of available data in order to create value for the Bank and our clients. Engagements vary between time-boxed Proofs of Concept and longer-term projects, and are conducted using the Scaled Agile Framework (SAFe).  As a Centre of Excellence, we also work to uplift data science and analytics across the Bank, for example by fostering Communities of Practice on different topics and rolling out governance support and resources around best practices for model development and analytics delivery.\n\nWe decided to collaborate on this Kaggle challenge and pool our knowledge and skills with the aim of making a positive impact in the on-going struggle against COVID-19."},{"metadata":{},"cell_type":"markdown","source":"# CORD: Risk Factor "},{"metadata":{},"cell_type":"markdown","source":"The following analysis is supporting material for the work carried out in an [accompanying notebook](https://www.kaggle.com/cddata/documenting-sub-tasks-dictionaries/) on the risk factors associated with contracting COVID-19 and is working towards contributing to the question [**What do we know about COVID-19 risk factors?**](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=558) proposed under the [COVID-19 Open Research Dataset Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)"},{"metadata":{},"cell_type":"markdown","source":"## Approach\n\nInitial ETL is carried out in our notebook [**Load and Process Abstracts**](https://www.kaggle.com/johndoyle/fork-of-load-and-process-data-abstracts) where for processing speed the abstract and reference text bodies are analysed for entity pairs. This can be expanded later to process the full text which will generate a more in depth analysis.\n\n\n\nThis analysis will build on this work and aims to enrich and extract related material from the CORD corpus in the following process:\n1. Load abstract entity enriched document objects.\n2. Create a directed graph of entities and linkages from the extracted information. \n3. Enrich refined dictionary of terms using a pre-trained Word-to-Vec model to resolve similar entities to the target dictionary.\n4. Merge nodes using the enriched dictionary of terms to improve publication linkage.\n5. Extract and analyse relevant paths between the target risk and the virus returning a list of publications which correspond to the edges of the graph.\n\nIn the analysis step, background research is introduced to produce lists of related keywords to analyse that corpus under 4 main topics related to COVID-19 risks task. \n"},{"metadata":{},"cell_type":"markdown","source":"# Goal \n\nA set of linked publications relating to the target topic, in this case the risk associated with contracting COVID-19."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import networkx as nx\n\n\nimport sys, os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport json\nimport pickle\nimport gc\nimport difflib\nimport gensim\n\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\n\n\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_lg\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = []\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/load-and-process-data-abstracts/'):\n        filenames = [names for names in filenames if '.pickle' in names]\n        if filenames != []:\n            files.append({'dirpath':dirname, 'filenames':filenames})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# recreate the schema from \"json_schema.txt\"\nclass author():\n    \"\"\"\n    This class object is derived from the json schema associated with the CORD\n    publications under the Author class.\n    \n    input_dict: This is the Author object from the publication class\n    \"\"\"\n    \n    def __init__(self, input_dict=None):\n        \n        self.first = \"\"\n        self.middle = []\n        self.last = \"\"\n        self.suffix = \"\"\n        self.affiliation = {}\n        self.email = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"first\" in key:\n                    self.first = input_dict[key]\n                if \"middle\" in key:\n                    self.middle = input_dict[key]\n                if \"last\" in key:\n                    self.last = input_dict[key]\n                if \"suffix\" in key:\n                    self.suffix = input_dict[key]\n                if \"affiliation\" in key:\n                    self.affiliation = input_dict[key]\n                if \"email\" in key:\n                    self.email = input_dict[key]    \n    \n    def print_items(self):\n        \n        print(\"first: \" + str(self.first) +  \n              \", middle: \" + str(self.middle) + \n              \", last: \" + str(self.last) + \n              \", suffix: \" + str(self.suffix) +\n              \", email: \" + str(self.email) + \n              \", affiliation: \" + json.dumps(self.affiliation, indent=4, sort_keys=True)\n             )\n\n\nclass inline_ref_span():\n    \"\"\"\n    This class object is derived from the json schema associated with the CORD\n    publications under the reference location.\n    \n    input_dict: this is the reference location object from the document class.\n    \"\"\"\n    \n    def __init__(self, input_dict=None):\n        \n        self.start = 0\n        self.end = 0\n        self.text = \"\"\n        self.ref_id = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"start\" in key:\n                    self.start = input_dict[key]\n                if \"end\" in key:\n                    self.end = input_dict[key]\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"ref_id\" in key:\n                    self.ref_id = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"Text: \" + str(self.text) + \", Start: \" + \n              str(self.start) + \", End: \" + str(self.end) + \n              \", Ref_id: \" + str(self.ref_id))\n\n    def step_index(self, n):\n        \n        self.start += n\n        self.end += n\n        \n        \nclass text_block():\n    \"\"\"\n    This class object is derived from the json schema associated with the CORD\n    publications and is associated with all relevent text blocks.\n    \n    input_dict: this is the text block object [abstract, text_body, ...] from the document class.\n    \"\"\"\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.eq_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"eq_spans\" in key:\n                    self.eq_spans = [inline_ref_span(e) for e in input_dict[key]]\n                if \"section\" in key:\n                    self.section = input_dict[key]\n        \n    def clean(self, swap_dict=None):\n            \n        self.text = clean(self.text, swap_dict)\n    \n    def print_items(self):\n        \n        print(\"\\ntext: \" + str(self.text))\n        print(\"\\nsection: \" + str(self.section))\n        print(\"\\ncite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"\\nref_spans: \")\n        [r.print_items() for r in self.ref_spans]\n        print(\"\\neq_spans: \")\n        [e.print_items() for e in self.eq_spans]\n\n\ndef combine_text_block(text_block_list):\n    \n    if text_block_list:\n        \n        combined_block = text_block_list[0]\n        block_length = len(combined_block.text)\n        \n        for i in range(1,len(text_block_list)):\n            combined_block.text += \" \" + text_block_list[i].text\n            block_length += 1\n            \n            # update spans start & stop index\n            [ref.step_index(block_length) for ref in text_block_list[i].cite_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].ref_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].eq_spans]\n            \n            # combine spans\n            combined_block.cite_spans += text_block_list[i].cite_spans\n            combined_block.ref_spans += text_block_list[i].ref_spans\n            combined_block.eq_spans += text_block_list[i].eq_spans           \n            combined_block.section += \", \" + str(text_block_list[i].section)           \n            \n            block_length += len(text_block_list[i].text)\n                       \n        return [combined_block]\n    else:\n        return [text_block()]\n      \n\nclass bib_item():\n    \"\"\"\n    This class object is derived from the json schema associated with the CORD\n    publications and is associated with all relevent bibliography objects.\n    \n    input_dict: this is associated with the bibliography objects in the the document class.\n    \"\"\"\n    \n    def __init__(self, input_dict=None):\n        \n        self.ref_id: \"\"\n        self.title: \"\"\n        self.authors = []\n        self.year = 0\n        self.venue = \"\"\n        self.volume = \"\"\n        self.issn = \"\"\n        self.pages = \"\"\n        self.other_ids = {}\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"ref_id\" in key:\n                    self.ref_id = input_dict[key]\n                if \"title\" in key:\n                    self.title = input_dict[key]\n                if \"authors\" in key:\n                    self.authors = [author(a) for a in input_dict[key]]\n                if \"year\" in key:\n                    self.year = input_dict[key]\n                if \"venue\" in key:\n                    self.venue = input_dict[key]\n                if \"volume\" in key:\n                    self.volume = input_dict[key]\n                if \"issn\" in key:\n                    self.issn = input_dict[key]\n                if \"pages\" in key:\n                    self.pages = input_dict[key]\n                if \"other_ids\" in key:\n                    self.other_ids = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"\\nBib Item:\")\n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"title:\" + str(self.title))\n        print(\"Authors:\")\n        [a.print_items() for a in self.authors]\n        print(\"year: \" + str(self.year))\n        print(\"venue:\" + str(self.venue))\n        print(\"issn:\" + str(self.issn))\n        print(\"pages:\" + str(self.pages))\n        print(\"other_ids:\" + json.dumps(self.other_ids, indent=4, sort_keys=True))\n        \n        \nclass ref_entries():\n    \n    def __init__(self, ref_id=None, input_dict=None):\n        \n        self.ref_id = \"\"\n        self.text = \"\"\n        self.latex = None\n        self.type = \"\"\n        \n        if ref_id:\n            self.ref_id = ref_id\n            \n            if input_dict:\n                for key in input_dict.keys():\n                    if \"text\" in key:\n                        self.text = input_dict[key]\n                    if \"latex\" in key:\n                        self.latex = input_dict[key]\n                    if \"type\" in key:\n                        self.type = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"text:\" + str(self.text))\n        print(\"latex: \" + str(self.latex))\n        print(\"type:\" + str(self.type))\n        \n                    \nclass back_matter():\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"section\" in key:\n                    self.section = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"text: \" + str(self.text))\n        print(\"cite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"ref_spans: \")\n        [r.print_items() for r in self.ref_spans]        \n        print(\"section:\" + str(self.section))\n\n        \n# The following Class Definition is a useful helper object to store various \n# different covid-19 data types.\nclass document():\n    \"\"\"\n    The following class object is based on the schema for each publication with \n    appropiate sub classes for more complex data types. \n    \n    This object aims to make the analysis of mixed data type quicker and more intutitive.\n    \"\"\"\n    def __init__(self, file_path=None):\n        \n        self.doc_filename = \"\"\n        self.doc_language = {}\n        self.paper_id = \"\"\n        self.title = \"\"\n        self.authors = []\n        self.abstract = []\n        self.text = []\n        self.bib = []\n        self.ref_entries = []\n        self.back_matter = []\n        self.tripples = {}\n        self.key_phrases = {}\n        self.entities = {}\n        \n        # load content from file on obj creation\n        self.load_file(file_path)\n     \n    def _load_paper_id(self, data):\n        \n        if \"paper_id\" in data.keys():\n            self.paper_id = data['paper_id']\n    \n    def _load_title(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"title\" in data['metadata'].keys():\n                self.title = data['metadata'][\"title\"]\n    \n    def _load_authors(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"authors\" in data['metadata'].keys():\n                self.authors = [author(a) for a in data['metadata'][\"authors\"]]\n                \n    def _load_abstract(self, data):\n        \n        if \"abstract\" in data.keys():\n            self.abstract = [text_block(a) for a in data[\"abstract\"]]\n    \n    def _load_body_text(self, data):\n        \n        if \"body_text\" in data.keys():\n            self.text = [text_block(t) for t in data[\"body_text\"]]\n    \n    def _load_bib(self, data):\n        \n        if \"bib_entries\" in data.keys():\n            self.bib = [bib_item(b) for b in data[\"bib_entries\"].values()]\n    \n    def _load_ref_entries(self, data):\n        \n        if \"ref_entries\" in data.keys():\n            self.ref_entries = [ref_entries(r, data[\"ref_entries\"][r]) for r in data[\"ref_entries\"].keys()]\n            \n    def _load_back_matter(self, data):\n        \n        if \"back_matter\" in data.keys():\n            self.back_matter = [back_matter(b) for b in data[\"back_matter\"]]\n        \n    def load_file(self, file_path):\n        \n        if file_path:\n            \n            with open(file_path) as file:\n                data = json.load(file)\n                \n                # call inbuilt data loading functions\n                self.doc_filename = file_path\n                self._load_paper_id(data)\n                self._load_title(data)\n                self._load_authors(data)\n                self._load_abstract(data)\n                self._load_body_text(data)\n                self._load_bib(data)\n                self._load_ref_entries(data)\n                self._load_back_matter(data)\n    \n    def combine_data(self):\n        \n        self.data = {'doc_filename': self.doc_filename,\n                     'doc_language': self.doc_language,\n                     'paper_id': self.paper_id,\n                     'title': self.title,\n                     'authors':self.authors,\n                     'abstract': self.abstract,\n                     'text': self.text,\n                     'bib_entries':self.bib,\n                     'ref_entries': self.ref_entries,\n                     'back_matter': self.back_matter,\n                     'tripples': self.tripples,\n                     'key_phrases': self.key_phrases,\n                     'entities': self.entities}\n\n    def extract_data(self):\n        \n        self.doc_filename = self.data['doc_filename']\n        self.doc_language = self.data['doc_language']\n        self.paper_id = self.data['paper_id']\n        self.title = self.data['title']\n        self.authors = self.data['authors']\n        self.abstract = self.data['abstract']\n        self.text = self.data['text']        \n        self.bib = self.data['bib_entries']\n        self.ref_entries = self.data['ref_entries']\n        self.back_matter = self.data['back_matter']\n        self.tripples = self.data['tripples']\n        self.key_phrases = self.data['key_phrases']\n        self.entities = self.data['entities']\n\n    def save(self, dir):\n        \n        self.combine_data()\n\n        if not os.path.exists(os.path.dirname(dir)):\n            try:\n                os.makedirs(os.path.dirname(dir))\n            except OSError as exc:  # Guard against race condition\n                if exc.errno != errno.EEXIST:\n                    raise\n\n        with open(dir, 'w') as json_file:\n            json_file.write(json.dumps(self.data))\n\n    def load_saved_data(self, dir):\n        \n        with open(dir) as json_file:\n            self.data = json.load(json_file)\n        self.extract_data()\n    \n    def print_items(self):\n         \n        print(\"---- Document Content ----\") \n        print(\"doc_filename: \" + str(self.doc_filename))\n        print(\"doc_language: \" + str(self.doc_language))\n        print(\"paper_id: \" + str(self.paper_id))\n        print(\"title: \" + str(self.title))\n        print(\"\\nAuthors: \")\n        [a.print_items() for a in self.authors]\n        print(\"\\nAbstract: \")\n        [a.print_items() for a in self.abstract]\n        print(\"\\nText: \")\n        [t.print_items() for t in self.text]\n        print(\"\\nBib_entries: \")\n        [b.print_items() for b in self.bib]\n        print(\"\\nRef_entries: \")\n        [r.print_items() for r in self.ref_entries]\n        print(\"\\nBack_matter: \")\n        [b.print_items() for b in self.back_matter]\n        \n        print(\"\\nTripples: \")\n        print(json.dumps(self.tripples, indent=4, sort_keys=True))\n        print(\"\\nKey Phrases: \")\n        print(json.dumps(self.key_phrases, indent=4, sort_keys=True))        \n        print(\"\\nEntities: \")\n        print(json.dumps(self.entities, indent=4, sort_keys=True))\n\n    def clean_text(self, swap_dict=None):\n        \n        # clean all blocks of text\n        [t.clean(swap_dict) for t in self.text]\n    \n    def clean_abstract(self, swap_dict=None):\n        \n        [t.clean(swap_dict) for t in self.abstract]\n    \n    def combine_text(self):\n        \n        # this function takes all text blocks within document.text and combines them into a single text_block object\n        self.text = combine_text_block(self.text)\n    \n    def combine_abstract(self):\n        \n        self.abstract = combine_text_block(self.abstract)   \n        \n    def set_abstract_tripples(self):\n                \n        abstract_tripples = {}\n        for i in range(0, len(self.abstract)):\n            #for every block in the abstract, extract entity tripples\n            self.abstract[i].clean()                       \n            pairs, entities = get_entity_pairs(self.abstract[i].text)\n            \n            #if any tripples found\n            if pairs.shape[0]>0:\n                abstract_tripples[\"abstract_\" + str(i)] = pairs.to_json()\n                       \n        self.tripples.update(abstract_tripples)\n        \n    def set_text_tripples(self):\n        \n        text_tripples = {}\n        for i in range(0, len(self.text)):\n            \n            self.text[i].clean()                       \n            pairs, entities = get_entity_pairs(self.text[i].text)\n            if pairs.shape[0]>0:\n                text_tripples[\"text_\" + str(i)] = pairs.to_json()\n                       \n        self.tripples.update(text_tripples)\n        \n    def set_ref_tripples(self):\n        \n        ref_tripples = {}\n        for r in self.ref_entries:\n            pairs, entities = get_entity_pairs(r.text)\n            if pairs.shape[0]>0:\n                ref_tripples[\"ref_\" + r.ref_id] = pairs.to_json()\n        \n        self.tripples.update(ref_tripples)\n        \n    def set_doc_language(self):\n        # set the doc language based on the analysis of the first block within the abstract\n        self.doc_language = get_text_language(self.text[0].text)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Entities\n\nLoading entity information into a pandas dataframe to aid in the creation of a network graph for analysis later. \nThis step builds on previous work carried out in our notebook [**Load and Process Abstracts**](https://www.kaggle.com/johndoyle/fork-of-load-and-process-data-abstracts).\n\nSteps:\n1. To conserve memory a corpus look up table is used and the indices will replace the title's in data structures during analysis and will be retrieved later for presentation.\n2. Triples are extracted from the document objects extracted from our previous work.\n3. The entity pairs are extracted for analysis and loaded into a dataframe with the document metadata that will be used during analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1\n\nwith open('/kaggle/input/publication-link-analysis/corpus_documents_lookup.json') as f:\n    pub_dict = json.load(f)\n    \ndef get_corpus_labels(corpus_dir, index):\n    \"\"\"\n    A helper function that will be used to return publication titles from indices\n    \n    corpus_dir: File associated with the corpus index\n    index:  A list of indices to be returned.\n    \"\"\"\n\n    with open(corpus_dir) as file:\n                corpus = json.load(file)\n    return {value:key for key, value in corpus.items() if value in index}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2\nentity_pairs_list = {}\nfor file in files:\n    directory = file['dirpath']\n    for filenames in file['filenames']:\n        \n        file = open(directory +'/'+filenames,'rb')\n        list_of_pubs = pickle.load(file)\n        for pub in list_of_pubs:\n            if pub is not None:\n                if pub.tripples != {}:\n                    subjects = []\n                    relations = []\n                    objects = []\n                    entity_pairs_list[pub_dict[pub.title]] = {'tripples':pub.tripples, 'file_path':str(pub.doc_filename)}\n\n        del list_of_pubs\ndel pub_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3\nentity_pair_df_list = []\nfor k, v in entity_pairs_list.items():\n    df_list = []\n    for k_, v_ in v['tripples'].items():\n        df_list.append(pd.read_json(v_)[['subject', 'relation', 'object']])\n    df_ = pd.concat(df_list, ignore_index=True)\n    df_['file_path'] = v['file_path']\n    df_['publication'] = k\n    entity_pair_df_list.append(df_)\n    \ndel entity_pairs_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter empty strings and numeric value only entity enteries as they do not provide any understandable information.\n\nThis filtering can be expanded to remove any additional unwanted entities before the graph is generated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"entity_pairs_df = pd.concat(entity_pair_df_list, ignore_index=True)\nentity_pairs_df = entity_pairs_df[entity_pairs_df.subject != '']\nentity_pairs_df = entity_pairs_df[entity_pairs_df.object != '']\nentity_pairs_df = entity_pairs_df[entity_pairs_df.object.str.isnumeric() != True]\nentity_pairs_df = entity_pairs_df[entity_pairs_df.subject.str.isnumeric() != True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Entity Pair Graph\nA network graph gives us a way to link structured and un-structured data for visualisation, exploration and analysis. Using concepts developed in graph theory, network graphs are popular methods of analysis and have been used in many fields such as mathematics, computer science, physics, chemistry, biology, sociology, and linguistics  to investigate a variety of systems including communication networks, social networks, biological nervous systems, and neural networks. [Pachayappan and Venkatesakumar (2018)](https://doi.org/10.4236/tel.2018.85067) apply a social network analysis based on graph theory to investigate influence among literature publications, and also provide a thorough overview of network graphs and their applications.\n\nThe nodes or vertices in a graph represent entities, while the links or edges between nodes represent the interconnections between entities. A simple visual introduction to several different types and layouts of network graphs is available [here](https://www.data-to-viz.com/graph/network.html).\n\nIn this analysis we leverage the NetworkX Python package to generate a network graph from a pandas dataframe. The entities are the nodes of the graph and the relationships are the edges which are enriched with publication meta data (title, path and page_rank).\n\nNetworkx: Aric A. Hagberg, Daniel A. Schult and Pieter J. Swart, “Exploring network structure, dynamics, and function using NetworkX”, in Proceedings of the 7th Python in Science Conference (SciPy2008), Gäel Varoquaux, Travis Vaught, and Jarrod Millman (Eds), (Pasadena, CA USA), pp. 11–15, Aug 2008"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_kg(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',edge_attr = ['relation','publication', 'file_path'],\n            create_using=nx.MultiDiGraph())\n    return k_graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G = create_kg(entity_pairs_df)\nprint(nx.info(G))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sub_graph(G, nodes, font = 12):\n    \"\"\"\n    visualisation of the entire network graph is often un-helpful.\n    This function creates a sub-graph from selected nodes and \n    plots the nodes, relationship and relative importance.\n\n    \n    G: networkx graph object.\n    nodes: a list of nodes to create a sub-graph object.\n    font: a variable to modify the size of the text and nodes in the graph. default at 12 for large sub-graphs.\n    \"\"\"\n    \n    G_sub = G.subgraph(nodes)\n    node_deg = nx.degree(G_sub)\n    layout = nx.spring_layout(G_sub, k=0.25, iterations=20)\n    plt.figure(num=None, figsize=(120, 90), dpi=80)\n    nx.draw_networkx(\n        G_sub,\n        node_size=[int(deg[1]) * 500*(font/12) for deg in node_deg],\n        arrowsize=40,\n        linewidths=2.5,\n        pos=layout,\n        edge_color='red',\n        edgecolors='black',\n        node_color='white',\n        font_size = font, \n        )\n\n    subject = []\n    obj = []\n    relation = []\n    tasks = []\n    for element in list(G_sub.edges()):\n            subject.append(element[0])\n            obj.append(element[1])\n            relation.append(G_sub.get_edge_data(element[0],element[1])[0]['relation'])\n    labels = dict(zip(list(zip(subject, obj)),relation))\n    nx.draw_networkx_edge_labels(G_sub, pos=layout, edge_labels=labels,\n                                     font_color='black', font_size=font)\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following sub-graph is a selection of top network nodes ranked by the degree of centrality within the network.\n[Degree of centrality](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality) is calculated as the fraction of nodes that a single node is connected to. "},{"metadata":{"trusted":true},"cell_type":"code","source":"node_rank = nx.degree_centrality(G)\nnode_rank_sorted = {k: v for k, v in sorted(node_rank.items(), key=lambda item: item[1],reverse=True)}\ntop_nodes = [k for k in node_rank_sorted.keys()][1:1000]\nplot_sub_graph(G, top_nodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Graph Connections\n\nThe following function are used to evaluate the paths between two nodes and return appropriate metadata for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_simple_paths(G, source, target, cutoff):\n    \"\"\"\n    Return all simple paths between source and target and enrich \n    the path information with the edge attributes\n    \n    Input\n    G: networkx graph object\n    source: node in G\n    target: node in G\n    cutoff: maximum path depth \n    \n    output\n    publications: list of publications attributed to each edge.\n    file_paths: list of file paths associated with the publications.\n    paths: list paths which are ordered list of nodes in each path.\n    \n    \"\"\"\n    publications = []\n    file_paths = []\n    paths = [p for p in nx.all_simple_paths(G,source=source,target=target, cutoff = cutoff)]\n    i = 0\n    for path in paths:\n        s = \"\"+ path[0]\n        for i in range(len(path)-1):\n            relation = []\n            for k,v in G.get_edge_data(path[i], path[i+1]).items():\n                publications.append(v['publication'])\n                file_paths.append(v['file_path'])\n                relation.append(v['relation'])\n            relation = \"/\".join(relation)\n            s = s + \"  --->  \" + relation + \"  --->  \" + path[i+1]\n        if i< 2:\n            print(s)\n        i = i+1\n        if i < 100:\n            break\n        \n    return publications, file_paths, paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_shortest_paths(G, source, target):\n    \n    \"\"\"\n    Return shortest paths between source and target and enrich \n    the path information with the edge attributes\n    \n    Input\n    G: networkx graph object\n    source: node in G\n    target: node in G\n    \n    output\n    publications: list of publications attributed to each edge.\n    file_paths: list of file paths associated with the publications.\n    \n    \"\"\"\n    \n    publications = []\n    file_paths = []\n    path = nx.shortest_path(G,source=source,target=target)\n    s = \"\"+ path[0]\n    for i in range(len(path)-1):\n        relation = []\n        for k,v in G.get_edge_data(path[i], path[i+1]).items():\n            publications.append(v['publication'])\n            file_paths.append(v['file_path'])\n\n            relation.append(v['relation'])\n        relation = \"/\".join(relation)\n        s = s + \"  --->  \" + relation + \"  --->  \" + path[i+1]\n    print(s)\n        \n    return publications, file_paths","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resolve Nodes Using Pubmed word2vec Model"},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec - Neural Word Embeddings:\n\nThe vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: “Writing about music is like dancing about architecture.” Word2vec “vectorizes” about words, and by doing so it makes natural language computer-readable – we can start to perform powerful mathematical operations on words to detect their similarities. \n\nUsing a word to predict a target context the model learns word vectors of the training corpus. When applied to a corpus that is specific to a single discipline, the word embedding learns the context in which specific terms are related. In this work we propose using a word vector trained on medical texts to improve the accuracy of processing, resolution and analysis of entities extracted from the CORD corpus.\n\nThe below word vector has been pre-trained on PubMed data and is sourced from:\n - Distributional Semantics Resources for Biomedical Text Processing. Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio Salakoski and Sophia Ananiadou. LBM 2013 [{1}](http://bio.nlplab.org/)  [{2}](http://bio.nlplab.org/pdf/pyysalo13literature.pdf).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pubmed_word_vector():\n    ! wget http://evexdb.org/pmresources/vec-space-models/PubMed-w2v.bin\n        \ntry:\n    model = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/w2vmodel/PubMed-w2v.bin\", binary=True)\nexcept:\n    get_pubmed_word_vector()\n    model = gensim.models.KeyedVectors.load_word2vec_format(\"PubMed-w2v.bin\", binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resolve_topic_list(G, topic_list, cuttoff = 0.85):\n    \"\"\"\n    Resolving a list of topics against the nodes in the network graph G \n    using the similarity metric provided by a word2vec model.\n    \n    Input\n    G: netwrokx graph object\n    topic_list: list of topics which will be compared against nodes in the network.\n    cuttoff: similarity metric threshold for returned nodes to be resolved.\n    \n    Output\n    resolution_dictionary:  \n  \n    \"\"\"\n\n    resolution_dictionary = {}\n    for topic in topic_list:\n        node_select={}\n        for node in G.nodes():\n            try:\n                node_select[str(node)] = model.n_similarity(topic.split(' '), str(node).split(' '))\n            except:\n                pass\n\n        resolution_dictionary[topic] = {k: v for k, v in sorted(node_select.items(), key=lambda item: item[1], reverse=True) if v > cuttoff}\n    return resolution_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Disease = ['covid-19', 'coronavirus','severe acute respiratory syndrome']\nresolution_dictionary_disease = resolve_topic_list(G, Disease, cuttoff = 0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = widgets.Dropdown(\n    options=list(resolution_dictionary_disease.keys()),\n    description='Task:',\n    value = 'covid-19'\n)\n\ndef plot_weightings(D):\n    plt.figure(figsize = [10,len(D)+1])\n    plt.barh(range(len(D)), list(D.values()), align='center')\n    plt.yticks(range(len(D)), list(D.keys()))\n    plt.show()\n\n\ndef on_change(change):\n    value = change['new']  \n    D = resolution_dictionary_disease[value]\n    clear_output()\n    display(w)\n    plot_weightings(D)\n        \n\nw.observe(on_change,'value')\ndisplay(w)\nplot_weightings(resolution_dictionary_disease[w.value])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge graph nodes\nThe resolved entities can be used to merge nodes in the graph, this will result in increased connectivity. Increased connectivity enables analysis of similar entities and how they link to our target term. \n\nThis example we will use the resolved Disease terms to improve linkage to covid-19 and SARS nodes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_resolved_nodes(G, resolution_dictionary):\n    \"\"\"\n    Useful for a small number of terms the nodes in the dictionary \n    are resolved to the root key for analysis.\n    Using the networkx API can be computationally expensive.\n    \"\"\"\n    G_ = G.copy(as_view=False)\n    for k, v in resolution_dictionary.items():\n        for key in v.keys():\n            try:\n                G_ = nx.contracted_nodes(G_,k, key)\n            except:\n                pass\n    return G_\n\ndef replace_entities_in_df(resolution_dictionary):\n    \"\"\"\n    An alternative to the merge_resolved_nodes for larger dictionaries. \n    We build a dictionary to leverage the string replace API in pandas. \n    \"\"\"\n    # prepair replace dictionary\n    replace_dict = {}\n    for k, v in resolution_dictionary.items():\n        for key in v.keys():\n            replace_dict[key] = k\n    return replace_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_dict = replace_entities_in_df(resolution_dictionary_disease)\nentity_pairs_disease_resolved_df = entity_pairs_df.replace(to_replace = replace_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list of common alaises for covid-19\ncovid_19_names = [\"2019 novel coronavirus disease\", \"2019 novel coronavirus infection\", \n                  \"2019-nCoV disease\", \"2019-nCoV infection\", \"COVID-19 pandemic\",\n                  \"COVID-19 virus disease\", \"COVID-19 virus infection\", \"COVID19\",\n                  \"SARS-CoV-2 infection\", \"coronavirus disease 2019\", \"coronavirus disease-19\", \"coronavirus\"]\n\ncovid_19_names = [name.lower() for name in covid_19_names]\n\nreplace_dict = {}\nfor name in covid_19_names:\n    replace_dict[name] = 'covid-19'\n    \nentity_pairs_disease_resolved_df = entity_pairs_disease_resolved_df.replace(to_replace = replace_dict)\n\nG_ = create_kg(entity_pairs_disease_resolved_df)\nprint(nx.info(G_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resolve_entities_graph(G, Risks,entity_pairs_df):\n    resolution_dictionary_risk = resolve_topic_list(G, Risks, cuttoff = 0.825)\n    replace_dict = replace_entities_in_df(resolution_dictionary_risk)\n    entity_pairs_resolved_df = entity_pairs_df.replace(to_replace = replace_dict)\n    return create_kg(entity_pairs_resolved_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis\n## Background Research\n\nThere are several different lenses through which to view the Risk Factors related to COVID-19. We are concentrating on the below sub-tasks:\n\n**1. Risk Factors Relating to Contracting COVID-19**\n\nSome demographics and sections of society are at a higher risk of contracting COVID-19. Risk factors include:\n\n* Occupation.\n\n  - The World Health Organization (WHO) notes that \"[p]eople most at risk of acquiring the disease are those who are in contact with or care for patients with COVID-19.\" [1] Front-line personnel and those working in jobs that cannot be performed remotely and that require attendance at a workplace and interaction with others are at increased risk of contracting COVID-19. These occupations include healthcare workers, support staff in hospitals and healthcare facilities, law enforcement and military, essential retail, warehouse and distribution staff.\n  \n  - There has been an especially high impact on healthcare workers. The European Centre for Disease Prevention and Control (ECDC) estimates that between 9% and 26% of the total number of diagnosed COVID-19 cases in EU and EEA countries are healthcare workers. [2]  In addition, the WHO notes that healthcare workers face hazards such as pathogen exposure, psychological distress, long working hours, fatigue, occupational burnout, stigma, and physical and psychological violence. [3-4] \n         \n      \n* Living in certain types of residential facility.\n\n  - Residents of Long Term Care Facilities (LTCF) such as nursing homes have been identified as \"vulnerable populations who are at a higher risk for adverse outcome and for infection due to living in close proximity to others\" by the WHO. [5] Some of the hazards presented by LTCF include large populations in close confinement, leading to difficulties in implementing physical distancing and isolation measures, as well as shortages in Personal Protective Equipment (PPE) and insufficient training of staff members in infection control measures and use of PPE. In some situations healthcare workers may move between multiple LTCF, posing a risk of cross-contamination.  A high number of clusters of infections have been reported in LTCF across Europe and the United States. [6-9]   \n  \n   \n* Travel to regions with high levels of community transmission. [10]\n\n\n* Close contact with an infected person. [10]\n\n\n* Belonging to a vulnerable section of society, where there may be reduced ability to practice public health measures such as hand-washing and self-isolating. The ECDC notes that, with respect to communicating the risks around COVID-19, \"[v]ulnerable individuals including the elderly, those with underlying health conditions, disabled people, people with mental health problems, homeless people, and undocumented migrants will require extra support...\" [11]\n    \n\n\n**List of Risk Factors Relating to Contracting COVID-19 - Occupational Risk factors - Healthcare Workers**\n\n['pathogen exposure', 'droplet', 'fomites', 'airborne', 'respiratory hygiene', 'PPE', 'personal protective equipment', 'aerosol', 'particulate respirator', 'secretions', 'fluids', 'intubation', 'ventilation', 'IPC', 'infection prevention control', 'gloves', 'mask', 'face shield', 'goggles', 'gowns', 'supply chain', 'disruption']\n\nReferences: [7, 22, 59, 60]"},{"metadata":{"trusted":true},"cell_type":"code","source":"Risk_Occupation = ['pathogen exposure', 'droplet', 'fomites', 'airborne', 'respiratory hygiene',\n 'PPE', 'personal protective equipment', 'aerosol', 'particulate respirator',\n 'secretions', 'fluids', 'intubation', 'ventilation', 'IPC', 'infection prevention control',\n 'gloves', 'mask', 'face shield', 'goggles', 'gowns', 'supply chain', 'disruption']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_risk = resolve_entities_graph(G, Risk_Occupation, entity_pairs_disease_resolved_df)\nprint(nx.info(G_risk))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get all simple paths and display a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"Virus = 'covid-19'\nnode_risk_dict = {}\nfor risk in Risk_Occupation:\n    print(Virus + ' - has relationship with - ' + risk)\n    print('\\n\\n')\n    publications, file_paths, nodes = get_all_simple_paths(G_risk, source = Virus, target = risk, cutoff=4)\n    \n    publications = get_corpus_labels('/kaggle/input/publication-link-analysis/corpus_documents_lookup.json',publications).values()\n\n    print('\\n From the following publications: \\n')\n    print('\\n'.join(publications))\n    print('\\n -----------------------------------------------------------------------------------')\n    node_risk_dict[risk] = {'nodes': [node for node_list in nodes for node in node_list], \n                            'publications': dict(zip(publications, file_paths))}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Sub-Graph Associated with Risk "},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw1 = widgets.Dropdown(\n    options=list(node_risk_dict.keys()),\n    description='Task:',\n)\n\ndef display_entities(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"ent\")\n    display(HTML(html))\n    \ndef on_change(change):\n    if change['type'] == 'change' and change['name'] == 'value':\n        value = change['new']\n        dictionary = node_risk_dict[value]\n        clear_output()\n        display(w1)\n        plot_sub_graph(G_, dictionary['nodes'], font = 40)\n        try:\n            w2.options = dictionary['publications'].keys()\n        except:\n            pass\n\n\n\nw1.observe(on_change)       \n\ndisplay(w1)\n\ndictionary = node_risk_dict[w1.value]\nplot_sub_graph(G_, dictionary['nodes'], font = 40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Named Entity Recognition for Text of Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw2 = widgets.Dropdown(\n    options=list(node_risk_dict[w1.value]['publications'].keys()),\n    description='Task:',\n)\n\ndef display_entities(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"ent\",minify=True)\n    display(HTML(html))\n    \ndef on_change_print(change):\n    if change['type'] == 'change' and change['name'] == 'value':\n        value = change['new']\n        doc = document(node_risk_dict[w1.value]['publications'][value])\n        doc.combine_text()\n        clear_output()\n        display(w2)\n        display_entities(doc.text[0].text)\n        \nw2.observe(on_change_print)\n\ndisplay(w2)\ndoc = document(node_risk_dict[w1.value]['publications'][w2.value])\ndoc.combine_text()\ndisplay_entities(doc.text[0].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output findings for later analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/Risk_Factors_Relating_to_Contracting_results.json', 'w')as f:\n    json.dump(node_risk_dict,f)\n    \nrefined_results = {}\nagg_pub = []\nagg_risk = []\nfor k, v in node_risk_dict.items():\n    refined_results[k] = list(set(v['publications']))\n    agg_pub.append(list(v['publications']))\n    for pub in v['publications']:\n        agg_risk.append(k)\nagg_pub = [pub for pubs in agg_pub for pub in pubs]\n\n\nwith open('/kaggle/working/Risk_Factors_Relating_to_Contracting_results_refined.json', 'w')as f:\n    json.dump(refined_results,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Top Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_top_texts(agg_pub, agg_risk):\n    pd.options.display.max_rows = 25\n    df = pd.DataFrame({'Publications':agg_pub, 'Risks':agg_risk})\n    df['Occurrence'] = 1\n    df = df.groupby('Publications').agg({'Occurrence':['sum'], 'Risks':[lambda x: set(x)]})\n    df.columns = df.columns.droplevel(1)\n    return df.sort_values('Occurrence', ascending=False)\n\ndf_contracting = display_top_texts(agg_pub,agg_risk)\ndf_contracting.to_csv('/kaggle/working/Risk_Factors_Relating_to_Contracting.csv')\n\ndf_contracting\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Risk Factors Relating to the Likelihood of Experiencing a Severe Illness Once Infected**\n\nThe WHO advises that 80% of COVID-19 patients will experience a mild illness, approximately 14% will experience a severe illness, and 5% will be critically ill. [12]  Those patients experiencing severe and critical illness will require hospitalisation and in some cases specialised medical care such as admission to Intensive Care Units (ICU) and access to ventilators and other equipment.\n\nOnce infected, what are the risk factors associated with a more severe illness?\n\n    \n* Smoking - Risk factors include increased hand-to-mouth contact, as well as increased likelihood of pre-existing lung conditions or reduced lung capacity. [13]\n* Pre-existing or underlying medical conditions (co-morbidities) such as high blood pressure, cardiovascular disease, chronic respiratory disease, cancer, diabetes, chronic kidney disease, liver disease, as well as conditions causing patients to be immunocompromised. [5, 10, 14-16]\n* Age - The WHO note that COVID-19 \"causes higher mortality in people aged ≥60 years\". [5] Similarly, the Centers for Disease Control and Prevention (CDC) advise that \"8 out of 10 deaths reported in the U.S. have been in adults 65 years old and older\" [17], while in China, \"the case fatality rate was highest among older persons...\" [16]\n* Size of 'infective dose' - The amount of viral particles received initially may be correlated to the progression of the infection, as \"...the outcome of infection...can sometimes be determined by how much virus actually got into your body and started the infection off...\" [18]\n\n\n**List of Risk Factors Relating to the Likelihood of Experiencing a Severe Illness Once Infected**\n\n['smoking', 'reduced lung capacity', 'hand to mouth contact', 'lung disease', 'oxygen', 'chronic', 'cancer', 'high blood pressure', 'diabetes', 'cardiovascular', 'chronic respiratory disease', 'heart disease', 'liver disease', 'kidney disease', 'immunocompromised', 'comorbidities', 'infective dose', 'infectious dose', 'viral load']\n\nReferences: [5, 10, 13-16, 18]"},{"metadata":{"trusted":true},"cell_type":"code","source":"Risk_Likelyhood = ['smoking', 'reduced lung capacity', 'hand to mouth contact', 'lung disease',\n                   'oxygen', 'chronic', 'cancer', 'high blood pressure', 'diabetes', 'cardiovascular',\n                   'heart', 'chronic respiratory disease', 'heart disease', 'liver disease', 'kidney disease',\n                   'immunocompromised', 'comorbidities', 'infective dose', 'infectious dose', 'viral load']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nG_risk = resolve_entities_graph(G, Risk_Likelyhood, entity_pairs_disease_resolved_df)\nprint(nx.info(G_risk))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get all simple paths and display a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"Virus = 'covid-19'\nnode_risk_dict = {}\nfor risk in Risk_Likelyhood:\n    print(Virus + ' - has relationship with - ' + risk)\n    print('\\n\\n')\n    publications, file_paths, nodes = get_all_simple_paths(G_risk, source = Virus, target = risk, cutoff=4)\n    \n    publications = get_corpus_labels('/kaggle/input/publication-link-analysis/corpus_documents_lookup.json',publications).values()\n\n    print('\\n From the following publications: \\n')\n    print('\\n'.join(publications))\n    print('\\n -----------------------------------------------------------------------------------')\n    node_risk_dict[risk] = {'nodes': [node for node_list in nodes for node in node_list], \n                            'publications': dict(zip(publications, file_paths))}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Sub-Graph Associated with Risk "},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw1 = widgets.Dropdown(\n    options=list(node_risk_dict.keys()),\n    description='Task:',\n)\n\nw1.observe(on_change)       \n\ndisplay(w1)\n\ndictionary = node_risk_dict[w1.value]\nplot_sub_graph(G_, dictionary['nodes'], font = 40)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Named Entity Recognition for Text of Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw2 = widgets.Dropdown(\n    options=list(node_risk_dict[w1.value]['publications'].keys()),\n    description='Task:',\n)\n        \nw2.observe(on_change_print)\n\ndisplay(w2)\ndoc = document(node_risk_dict[w1.value]['publications'][w2.value])\ndoc.combine_text()\ndisplay_entities(doc.text[0].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output findings for later analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/Likelihood_of_Experiencing_a_Severe_Illness_results.json', 'w')as f:\n    json.dump(node_risk_dict,f)\n    \nrefined_results = {}\nagg_pub = []\nagg_risk = []\nfor k, v in node_risk_dict.items():\n    refined_results[k] = list(set(v['publications']))\n    agg_pub.append(list(v['publications']))\n    for pub in v['publications']:\n        agg_risk.append(k)\nagg_pub = [pub for pubs in agg_pub for pub in pubs]\n    \nwith open('/kaggle/working/Likelihood_of_Experiencing_a_Severe_Illness_results_refined.json', 'w')as f:\n    json.dump(refined_results,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Top Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_contracting = display_top_texts(agg_pub,agg_risk)\ndf_contracting.to_csv('/kaggle/working/Likelihood_of_Experiencing_a_Severe_Illness.csv')\n\ndf_contracting\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Risk Factors Relating to Community Transmission**\n\nThere are a number of aspects that determine the rate of community transmission.\n\n* Incubation periods can be up to 14 days, with the average 5-6 days. [19] During this time, pre-symptomatic people can transmit the virus. [19-20] In addition, asymptomatic transmission can occur from people who do not experience symptoms. [19; 21]\n\n\n* The public health measures that can be imposed with the aim of \"limit[ing] the impact of the pandemic on healthcare systems and vulnerable population groups by delaying the epidemic peak and decreasing the magnitude of the peak\". [11]\n   \n   * There are a number of public health measures which can be rolled out. Contact tracing involves the systematic identification of \"all social, familial/household, work, health care, and any other contacts\" [22] with a view to limiting pre-symptomatic or asymptomatic spread. Widespread testing is vital to understand the levels of infection, with the WHO identifying \"[l]imited testing capacity in many countries globally\" as a risk factor for the spread of COVID-19. [23] Frequent and thorough hand-washing and social or physical distancing when in public are recommended by the WHO and ECDC. [24; 11]\n   \n   * Restrictions on mass gatherings [25] and the closure of schools, universities, creches, workplaces, shops, restaurants and bars can be implemented [26], as well as restrictions on citizens leaving their homes. [27]\n   \n   * The effectiveness of the public health measures will depend on when they are introduced, and the levels of public compliance with the measures and restrictions. The ECDC notes that \"[m]onitoring systems should be put in place to observe public perceptions, opinions and compliance with individual measures.\" [11] A related topic is how compliance is enforced. There have been a range of different approaches, from a relatively low number of restrictions in South Korea coupled with widespread testing [28-29], to country-wide quarantine in Italy [30], to relatively late introduction of restrictions in the UK which enabled mass gatherings such as the Cheltenham Festival to go ahead [31], to a relatively low testing rate in the US [32], to restrictions being enforced through fines and arrests in France [33-34], to mandatory quarantine for travelers in China [35].  Other considerations are the manner in which restrictions are communicated, with reports of mass travel in Italy arising from planned restrictions being leaked by media organisations. [36-37] The proliferation of misinformation and disinformation on social media can also affect public compliance with health measures, with the ECDC noting that \"[p]rocedures for identifying and rapidly addressing misinformation, disinformation and rumours, especially on social media platforms, should be established.\" [11]\n\n**List of Risk Factors Relating to Community Transmission**\n\n['surfaces', 'contamination', 'pathogens', 'isolation', 'quarantine', 'physical distancing', 'social distancing', 'recognition', 'identification', 'source control', 'fever', 'cough', 'shortness of breath', 'contacts', 'human-to-human transmission', 'triage', 'respiratory hygiene', 'coughing', 'sneezing', 'elbow', 'hand hygiene', 'awareness', 'mitigation', 'delay', 'peak', 'testing', 'contact tracing', 'surveillance', 'early detection', 'risk communication', 'infection control', 'impact on healthcare system', 'preparedness', 'public awareness', 'compliance', 'misinformation', 'disinformation', 'social media', 'surge', 'vulnerable', 'asymptomatic', 'pre-symptomatic', 'droplet', 'mass gatherings', 'restrictions', 'quarantine', 'public health', 'incubation']\n\n\nReferences:  [5, 11, 19, 22, 60]"},{"metadata":{"trusted":true},"cell_type":"code","source":"Risk_Community = ['surfaces', 'contamination', 'pathogens', 'isolation', 'quarantine',\n                  'physical distancing', 'social distancing', 'recognition', 'identification',\n                  'source control', 'fever', 'cough', 'shortness of breath', 'contacts',\n                  'human-to-human transmission', 'triage', 'respiratory hygiene', 'coughing',\n                  'sneezing', 'elbow', 'hand hygiene', 'awareness', 'mitigation', 'delay', 'peak',\n                  'testing', 'contact tracing', 'surveillance', 'early detection', 'risk communication',\n                  'infection control', 'impact on healthcare system', 'preparedness', 'public awareness',\n                  'compliance', 'misinformation', 'disinformation', 'social media', 'surge', 'vulnerable', \n                  'asymptomatic', 'pre-symptomatic', 'droplet', 'mass gatherings', 'restrictions', 'quarantine', \n                  'public health', 'incubation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_risk = resolve_entities_graph(G, Risk_Community, entity_pairs_disease_resolved_df)\nprint(nx.info(G_risk))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get all simple paths and display a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"Virus = 'covid-19'\nnode_risk_dict = {}\nfor risk in Risk_Community:\n    print(Virus + ' - has relationship with - ' + risk)\n    print('\\n\\n')\n    publications, file_paths, nodes = get_all_simple_paths(G_risk, source = Virus, target = risk, cutoff=4)\n    \n    publications = get_corpus_labels('/kaggle/input/publication-link-analysis/corpus_documents_lookup.json',publications).values()\n\n    print('\\n From the following publications: \\n')\n    print('\\n'.join(publications))\n    print('\\n -----------------------------------------------------------------------------------')\n    node_risk_dict[risk] = {'nodes': [node for node_list in nodes for node in node_list], \n                            'publications': dict(zip(publications, file_paths))}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Sub-Graph Associated with Risk "},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw1 = widgets.Dropdown(\n    options=list(node_risk_dict.keys()),\n    description='Task:',\n)\n\nw1.observe(on_change)       \n\ndisplay(w1)\n\ndictionary = node_risk_dict[w1.value]\nplot_sub_graph(G_, dictionary['nodes'], font = 40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Named Entity Recognition for Text of Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw2 = widgets.Dropdown(\n    options=list(node_risk_dict[w1.value]['publications'].keys()),\n    description='Task:',\n)\n        \nw2.observe(on_change_print)\n\ndisplay(w2)\ndoc = document(node_risk_dict[w1.value]['publications'][w2.value])\ndoc.combine_text()\ndisplay_entities(doc.text[0].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output findings for later analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/Risk_Factors_Relating_to_Community_Transmission_results.json', 'w')as f:\n    json.dump(node_risk_dict,f)\n    \nrefined_results = {}\nagg_pub = []\nagg_risk = []\nfor k, v in node_risk_dict.items():\n    refined_results[k] = list(set(v['publications']))\n    agg_pub.append(list(v['publications']))\n    for pub in v['publications']:\n        agg_risk.append(k)\nagg_pub = [pub for pubs in agg_pub for pub in pubs]\n    \nwith open('/kaggle/working/Risk_Factors_Relating_to_Community_Transmission_results_refined.json', 'w')as f:\n    json.dump(refined_results,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Top Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_contracting = display_top_texts(agg_pub,agg_risk)\ndf_contracting.to_csv('/kaggle/working/Risk_Factors_Relating_to_Community_Transmission.csv')\n\ndf_contracting\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.  Risk Factors Relating to Adverse Socio-economic Impacts from the Virus**\n\nAdverse socio-economic impacts include loss of income through loss of employment, reduction in hours, etc; loss of access to educational opportunities, e.g. closure of schools and universities, and uncertainty around exams; inability or reluctance to access medical care for non-COVID-19 issues.\n\nRisk factors are present at both macro and micro levels, i.e. the risk of a particular country experiencing adverse socio-economic impacts, and the risk of a particular citizen experiencing adverse socio-economic impacts.\n\nRisk factors include:\n\n* The severity and duration of the lockdown - The world is now in recession, with the International Monetary Fund (IMF) noting that the \"economic damage is mounting across all countries\". [38] The IMF expect this recession to be worse than the global financial crisis [38] and possibly \"the worst economic fallout since the Great Depression\". [39]\n\n\n* Public compliance with lockdown measures over an extended period of time - There are concerns that 'isolation fatigue' may lead to increased flouting of restrictions. [40] In addition, there have been examples of growing social unrest in regions which have experienced severe restrictions for several weeks [41], as well as concerns that measures such as hotlines for informing on neighbours breaking restrictions may lead to social division. [42]\n\n\n* The exit strategy from lockdown - The lifting of restrictions must be balanced against the risk of a possible second wave of infections. [43-45] There is a possibility of a cycle of lockdowns and easing of restrictions being implemented over several months. [46]\n\n\n* Sector of employment - Sectors particularly impacted include aviation [47], tourism [48], hospitality [49], non-essential retail [49], manufacturing [49], and food production and agriculture [50].\n\n\n* Type of employment - Temporary, part-time, seasonal and casual workers, and those on lower pay, minimum wage, or hourly contracts will be severely impacted.  The self-employed and those working in small to medium size enterprises are also at risk of adverse impacts. In the US, the initial jobless claims for the last two weeks in March exceeded 10 million. [51]\n\n\n* Age - School and university students will be impacted by closures, loss of educational opportunities, and mental health issues relating to uncertainty around exams and future plans. Over 91% of the global student population are affected, with vulnerable and disadvantaged communities most severely impacted. [52] United Nations Educational, Scientific and Cultural Organization (UNESCO) advise that school closures may lead to \"increased drop-out rates which will disproportionately affect adolescent girls, further entrench gender gaps in education and lead to increased risk of sexual exploitation, early pregnancy and early and forced marriage.\" [53]\n\n\n* Ethnicity - There is evidence from a number of countries including the US [54] and the UK [55] that ethnic minorities are disproportionately affected by COVID-19. Proposed reasons include unequal access to healthcare and a greater proportion of ethnic minorities working in essential industries where remote working is not feasible. [54]\n\n\n* Risk of preventable non-COVID-19 deaths - Hospitals globally are reporting significantly lower than normal admissions for non-COVID-19 conditions, leading to fears that people are ignoring warning signs as they are afraid of contracting COVID-19 while in hospital. [56-57]  Similar drops in emergency room admissions were seen in Canada during the SARS outbreak of 2002 [56], while in West Africa during the Ebola outbreak of 2014-2016, \"more people died from lack of health-care access for non-Ebola needs than Ebola itself\". [56]\n\n\n* Impact of long-term isolation on mental health - People living under long-term restrictions may experience mental health issues. In addition, the uncertainty coupled with the constant stream of news about the progression of the pandemic can lead to worry and anxiety. [58]\n\n**List of Risk Factors Relating to Adverse Socio-Economic Impact**\n\n['economy', 'unemployment', 'job loss', 'redundancy', 'layoffs', 'jobless claim', 'unemployment benefit', 'sector', 'aviation', 'tourism', 'hospitality', 'retail', 'agriculture', 'manufacturing', 'restrictions', 'lockdown', 'shutdown', 'exit strategy', 'social welfare', 'school closure', 'ethnic minority', 'disproportionate', 'secondary deaths', 'social unrest', 'isolation fatigue', 'recession', 'depression', 'mental health', 'anxiety', 'worry', 'stress']\n\n\nReferences:  [38, 41, 47-52, 54-55, 58]"},{"metadata":{"trusted":true},"cell_type":"code","source":"Risk_Socio_Economic = ['economy', 'unemployment', 'job loss', 'redundancy', 'layoffs',\n 'jobless claim', 'unemployment benefit', 'sector', 'aviation',\n 'tourism', 'hospitality', 'retail', 'agriculture', 'manufacturing',\n 'restrictions', 'lockdown', 'shutdown', 'exit strategy', 'social welfare',\n 'school closure', 'ethnic minority', 'disproportionate', 'secondary deaths',\n 'social unrest', 'isolation fatigue', 'recession', 'depression', 'mental health',\n 'anxiety', 'worry', 'stress']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_risk = resolve_entities_graph(G, Risk_Socio_Economic, entity_pairs_disease_resolved_df)\nprint(nx.info(G_risk))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get all simple paths and display a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"Virus = 'covid-19'\nnode_risk_dict = {}\nfor risk in Risk_Socio_Economic:\n    print(Virus + ' - has relationship with - ' + risk)\n    print('\\n\\n')\n    publications, file_paths, nodes = get_all_simple_paths(G_risk, source = Virus, target = risk, cutoff=4)\n    \n    publications = get_corpus_labels('/kaggle/input/publication-link-analysis/corpus_documents_lookup.json',publications).values()\n\n    print('\\n From the following publications: \\n')\n    print('\\n'.join(publications))\n    print('\\n -----------------------------------------------------------------------------------')\n    node_risk_dict[risk] = {'nodes': [node for node_list in nodes for node in node_list], \n                            'publications': dict(zip(publications, file_paths))}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Sub-Graph Associated with Risk "},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw1 = widgets.Dropdown(\n    options=list(node_risk_dict.keys()),\n    description='Task:',\n)\n\n\n\nw1.observe(on_change)       \n\ndisplay(w1)\n\ndictionary = node_risk_dict[w1.value]\nplot_sub_graph(G_, dictionary['nodes'], font = 40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Named Entity Recognition for Text of Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = widgets.Output(layout={'border': '1px solid black'})\n\n\nw2 = widgets.Dropdown(\n    options=list(node_risk_dict[w1.value]['publications'].keys()),\n    description='Task:',\n)\n        \nw2.observe(on_change_print)\n\ndisplay(w2)\ndoc = document(node_risk_dict[w1.value]['publications'][w2.value])\ndoc.combine_text()\ndisplay_entities(doc.text[0].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output findings for later analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/Risk_Factors_Relating_to_Socio_Economic_results.json', 'w') as f:\n    json.dump(node_risk_dict,f)\n    \nrefined_results = {}\nagg_pub = []\nagg_risk = []\nfor k, v in node_risk_dict.items():\n    refined_results[k] = list(set(v['publications']))\n    agg_pub.append(list(v['publications']))\n    for pub in v['publications']:\n        agg_risk.append(k)\nagg_pub = [pub for pubs in agg_pub for pub in pubs]\n\nwith open('/kaggle/working/Risk_Factors_Relating_to_Socio_Economic_results_refined.json', 'w') as f:\n    json.dump(refined_results,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Top Publications"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_contracting = display_top_texts(agg_pub,agg_risk)\ndf_contracting.to_csv('/kaggle/working/Risk_Factors_Relating_to_Socio_Economic.csv')\n\ndf_contracting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nThe linkages and connections between the publications in the corpus have been displayed using a network graph, where the nodes of the graph represent publications and the edges of the graph represent links between entities. Entity resolution has been applied to the graph to enhance connectivity. A Word-to-Vec model pre-trained on a PubMed corpus was used to resolve the entities.\n\nSeveral different topics relating to risk factors around COVID-19 were identified. A review of documents and resources from the World Health Organisation, European Centre for Disease Prevention and Control, the Centers for Disease Control and Prevention, other reputable organisations and news outlets was carried out in order to research each of the topics and construct a list of risk factors.\n\nThese lists of risk factors were then used to assess the linkage between each of the terms and 'covid-19' in the network graph. The shorter the path connecting a term with 'covid-19', the stronger the association.\n\nFor each of the risk factors, the list of related publications has been determined, along with the linked nodes in the network graph, and these results have been output as a json file. The publications have also been ranked by their frequency of occurrence across the different risk factors, i.e. how many risk factors a publication is linked to. This identifies the most relevant publications across a range of risk factors."},{"metadata":{},"cell_type":"markdown","source":"# Next Steps\n\nThe work presented in this notebook is the analysis and results of a body of work that includes:\n1. [Background Research](https://www.kaggle.com/cddata/documenting-sub-tasks-dictionaries)\n2. [Data Loading and Processing](https://www.kaggle.com/johndoyle/load-and-process-data-abstracts)\n3. [Publications Analysis](https://www.kaggle.com/johndoyle/publication-link-analysis)\n\nEach of these works can be continued and the approaches expanded, some tasks that would be targeted first:\n1. Custom entity tagging using manually created or automated medical ontology\n2. Graph enrichment and linkage improvement using the text body of the publications.\n3. Enrichment of risks using custom [word embedding](https://www.kaggle.com/piyushrumao/word-embedding-analysis-on-covid-19-dataset), [document embedding](https://www.kaggle.com/piyushrumao/doc2vec-analysis-on-covid-19-datas) and  [topic analysis](https://www.kaggle.com/piyushrumao/topic-modelling-analysis-on-covid-19-dataset) which were explored during this project.\n4. Target content extraction from publications to further reduce the burden of knowledge extraction.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# References\n\n\n[1] https://apps.who.int/iris/bitstream/handle/10665/331496/WHO-2019-nCov-HCW_risk_assessment-2020.2-eng.pdf\n\n[2] https://www.ecdc.europa.eu/sites/default/files/documents/covid-19-rapid-risk-assessment-coronavirus-disease-2019-eighth-update-8-april-2020.pdf\n\n[3] https://iris.wpro.who.int/bitstream/handle/10665.1/14482/COVID-19-022020.pdf\n\n[4] https://www.who.int/publications-detail/coronavirus-disease-(covid-19)-outbreak-rights-roles-and-responsibilities-of-health-workers-including-key-considerations-for-occupational-safety-and-health\n\n[5] https://apps.who.int/iris/bitstream/handle/10665/331508/WHO-2019-nCoV-IPC_long_term_care-2020.1-eng.pdf\n\n[6] https://www.theguardian.com/world/2020/apr/02/coronavirus-outbreaks-us-nursing-homes-lockdowns\n\n[7] https://apnews.com/6b9b54cb3626906277232b8e2bdcf69a\n\n[8] https://www.euronews.com/2020/03/24/coronavirus-elderly-found-dead-and-abandoned-in-spanish-nursing-homes\n\n[9] https://www.theguardian.com/world/2020/apr/09/covid-19-hundreds-of-uk-care-home-deaths-not-added-to-official-toll\n\n[10] https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200407-sitrep-78-covid-19.pdf?sfvrsn=bc43e1b_2\n\n[11] https://www.ecdc.europa.eu/sites/default/files/documents/RRA-sixth-update-Outbreak-of-novel-coronavirus-disease-2019-COVID-19.pdf \n\n[12] https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200301-sitrep-41-covid-19.pdf?sfvrsn=6768306d_2    \n\n[13] https://www.who.int/news-room/q-a-detail/q-a-on-smoking-and-covid-19\n\n[14] https://www.who.int/news-room/q-a-detail/q-a-coronaviruses\n\n[15] https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/groups-at-higher-risk.html\n\n[16] https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-guidance-management-patients.html\n\n[17] https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/older-adults.html\n\n[18] https://www.sciencemediacentre.org/expert-reaction-to-questions-about-covid-19-and-viral-load/\n\n[19] https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200402-sitrep-73-covid-19.pdf?sfvrsn=5ae25bc7_2\n\n[20] https://www.ecdc.europa.eu/sites/default/files/documents/covid-19-guidance-discharge-and-ending-isolation-first%20update.pdf\n\n[21] https://www.weforum.org/agenda/2020/03/people-with-mild-or-no-symptoms-could-be-spreading-covid-19/\n\n[22] https://www.who.int/publications-detail/considerations-in-the-investigation-of-cases-and-clusters-of-covid-19\n\n[23] https://www.who.int/publications-detail/strategic-preparedness-and-response-plan-for-the-new-coronavirus\n\n[24] https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public\n\n[25] https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/points-of-entry-and-mass-gatherings\n\n[26] https://www.ecdc.europa.eu/sites/default/files/documents/covid-19-social-distancing-measuresg-guide-second-update.pdf\n\n[27] https://www.ecdc.europa.eu/en/publications-data/video-covid-19-stay-home-importance-social-distancing\n\n[28] https://www.weforum.org/agenda/2020/03/south-korea-covid-19-containment-testing/\n\n[29] https://www.sciencemag.org/news/2020/03/coronavirus-cases-have-dropped-sharply-south-korea-whats-secret-its-success\n\n[30] https://www.bbc.com/news/world-europe-51810673\n\n[31] https://www.irishtimes.com/sport/racing/cheltenham-faces-criticism-after-racegoers-suffer-covid-19-symptoms-1.4219458\n\n[32] https://www.worldometers.info/coronavirus/covid-19-testing/\n\n[33] https://www.france24.com/en/20200318-france-coronavirus-lockdown-violation-attestation-epidemic-christophe-castaner-public-health\n\n[34] https://www.connexionfrance.com/French-news/France-arrests-people-for-flouting-confinement-rules-as-Macron-calls-for-confinement-rules-to-be-taken-more-seriously\n\n[35] https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30421-9/fulltext\n\n[36] https://www.cnbc.com/2020/03/09/italys-quarantine-provokes-panic-italian-stocks-plunge.html\n\n[37] https://www.theguardian.com/world/2020/mar/08/leaked-coronavirus-plan-to-quarantine-16m-sparks-chaos-in-italy\n\n[38] https://blogs.imf.org/2020/04/06/an-early-view-of-the-economic-impact-of-the-pandemic-in-5-charts/\n\n[39] https://economictimes.indiatimes.com/news/economy/indicators/covid-19-imf-anticipates-sharply-negative-economic-growth-fallout-since-the-great-depression/articleshow/75067158.cms\n\n[40] https://www.theguardian.com/world/2020/apr/04/uks-covid-19-lockdown-could-crumble-as-frustration-grows-police-warn\n\n[41] https://www.theguardian.com/world/2020/mar/29/italy-sets-aside-400m-for-food-vouchers-as-social-unrest-mounts\n\n[42] https://www.theguardian.com/uk-news/2020/apr/09/uk-police-tool-report-covid-19-rule-breakers-risks-fuelling-social-division\n\n[43] https://www.irishtimes.com/news/world/us/us-immunologist-warns-against-hasty-return-to-business-as-usual-1.4227390\n\n[44] https://edition.cnn.com/2020/04/10/asia/china-korea-singapore-coronavirus-second-wave-intl-hnk/index.html\n\n[45] https://www.controlrisks.com/covid-19/covid-19-no-sign-of-a-second-wave-in-asia\n\n[46] https://www.businessinsider.com/countries-may-need-more-lockdowns-coronavirus-2020-3?r=US&IR=T\n\n[47] https://www.ilo.org/sector/Resources/publications/WCMS_741466/lang--en/index.htm\n\n[48] https://www.ilo.org/sector/Resources/publications/WCMS_741468/lang--en/index.htm\n\n[49] https://news.un.org/en/story/2020/04/1061322\n\n[50] http://www.fao.org/2019-ncov/q-and-a/impact-on-food-and-agriculture/en/\n\n[51] https://www.telesurenglish.net/news/US-Jobless-Claims-Surge-Rises-to-10-Million-Due-to-COVID-19-20200402-0013.html\n\n[52] https://en.unesco.org/covid19/educationresponse\n\n[53] https://en.unesco.org/news/covid-19-school-closures-around-world-will-hit-girls-hardest\n\n[54] https://www.theguardian.com/world/2020/apr/08/its-a-racial-justice-issue-black-americans-are-dying-in-greater-numbers-from-covid-19\n\n[55] https://www.icnarc.org/Our-Audit/Audits/Cmp/Reports\n\n[56] https://www.cbc.ca/news/health/covid-19-emergency-departments-canada-1.5510778\n\n[57] https://www.thejournal.ie/tony-holohan-hospital-symptoms-5065098-Apr2020/\n\n[58] https://www.mayoclinic.org/diseases-conditions/coronavirus/in-depth/mental-health-covid-19/art-20482731\n\n[59] https://www.who.int/publications-detail/infection-prevention-and-control-during-health-care-when-novel-coronavirus-(ncov)-infection-is-suspected-20200125\n\n[60] https://apps.who.int/iris/bitstream/handle/10665/331498/WHO-2019-nCoV-IPCPPE_use-2020.2-eng.pdf"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}