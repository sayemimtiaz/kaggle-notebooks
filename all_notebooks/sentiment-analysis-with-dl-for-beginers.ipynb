{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"iiFEZASOL2rm","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"92yoxmtbL2r3"},"cell_type":"markdown","source":"## 1. Loading Dataframes"},{"metadata":{"id":"zRfavkqUNOwM","outputId":"ffed936e-8abe-4cd7-d8be-c596212160b9","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='ISO-8859-1')\ntest = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"id":"jNb2Z1q-L2r7"},"cell_type":"markdown","source":"## 2. Explotary Data Analysis"},{"metadata":{"id":"vfnqm_npL2r8"},"cell_type":"markdown","source":"### checking out dfs"},{"metadata":{"id":"tPWJBA3NL2r9","outputId":"8e6492f7-6332-48aa-b0a2-04e131d0ef89","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"i_eJH3xVL2sA","outputId":"a4ba533b-a523-4ebd-d2c3-fcb55f6bb22b","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"F1dxk10pL2sB","outputId":"752d9b1f-31ef-477d-833d-c3cb8f337105","trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"Q4kn4lyGL2sC","outputId":"e29fc70f-58e7-4226-f2e3-97b026ee4723","trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"51Csw9lzL2sD","outputId":"30dbe612-fd59-430d-d4ba-13cb90483e71","trusted":true},"cell_type":"code","source":"test.info()  # same dtypes with train df","execution_count":null,"outputs":[]},{"metadata":{"id":"psx460CJL2sF","outputId":"436ba278-2f1b-47fd-a85a-325f080d3206","trusted":true},"cell_type":"code","source":"test.isnull().sum()  # there are null values again in location column","execution_count":null,"outputs":[]},{"metadata":{"id":"ZqumhvqrL2sF"},"cell_type":"markdown","source":"### tweet locations"},{"metadata":{"id":"95OO-B01L2sG","outputId":"b33a01f2-ac88-404d-bbd0-8c51552e58eb","trusted":true},"cell_type":"code","source":"train.Location.value_counts(dropna = False)[:20]","execution_count":null,"outputs":[]},{"metadata":{"id":"JyZ-R2l8L2sH","trusted":true},"cell_type":"code","source":"train.Location = train.Location.str.split(\",\").str[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"TH5I1nKOL2sH","outputId":"c3f35110-644b-4b8b-ea84-b44fcdec5d9e","trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,4)})\n\nplt.figure(figsize=(12, 6))\nsns.barplot(train[\"Location\"].value_counts().values[:10],\n            train[\"Location\"].value_counts().index[:10]);\nplt.title(\"Top 10 Countries with maximum Covid-19 tweets\", fontsize=14)\nplt.xlabel(\"Number of tweets\", fontsize=14)\nplt.ylabel(\"Country Name\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZAbt6B7QL2sI"},"cell_type":"markdown","source":"### tweet sentiment values"},{"metadata":{"id":"liHM4Z_OL2sJ","outputId":"9872dfe7-3cba-4f3b-8ecd-27bd0d3ea05d","trusted":true},"cell_type":"code","source":"train['Sentiment'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"id":"XpqV5LdzL2sJ","outputId":"8c700cdf-62c5-4b63-aa12-fccf94b1bbe8","trusted":true},"cell_type":"code","source":"sns.countplot(x = \"Sentiment\", data = train)","execution_count":null,"outputs":[]},{"metadata":{"id":"-1YL-eZ4L2sK"},"cell_type":"markdown","source":"### regrouping train and test dfs"},{"metadata":{"id":"rt9p79LKL2sK","trusted":true},"cell_type":"code","source":"encoding = {'Extremely Negative': 0,\n            'Negative': 0,\n            'Neutral': 1,\n            'Positive':2,\n            'Extremely Positive': 2\n           }\n\nlabels = ['Negative', 'Neutral', 'Positive']\n           \ntrain[\"Sentiment\"].replace(encoding, inplace=True)\ntest[\"Sentiment\"].replace(encoding, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"_sAcrtRQL2sL","outputId":"d48735a8-1ebc-4881-e250-448f5344305c","trusted":true},"cell_type":"code","source":"sns.countplot(x = \"Sentiment\", data = train)","execution_count":null,"outputs":[]},{"metadata":{"id":"WB1Yxd7tL2sM","outputId":"032ac438-d897-4480-d1bd-21976a78e602","trusted":true},"cell_type":"code","source":"sns.countplot(x = \"Sentiment\", data = test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Z9F9Pc3IL2sM"},"cell_type":"markdown","source":"### analysis of locations with sentiment"},{"metadata":{"id":"hWdPPfJ-L2sN","trusted":true},"cell_type":"code","source":"loc_with_sentiment = train.iloc[:, [2,5]]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Vrn9vvxkL2sN","outputId":"7b588960-70f5-435b-8c59-06e1e171871f","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nax = sns.countplot(x = \"Location\", hue = \"Sentiment\", data = loc_with_sentiment, \n              order = train.Location.value_counts()[:10].index, orient = \"h\", palette = \"Paired\") \n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.0f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   size=15,\n                   xytext = (0, -12), \n                   textcoords = 'offset points')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"z7QzH0ZVlKAO"},"cell_type":"markdown","source":"## hashtags"},{"metadata":{"id":"Cgyz8kCalPK1","trusted":true},"cell_type":"code","source":"import regex as re\n\ndef extract_hash_tags(s):\n    hashes = re.findall(r\"#(\\w+)\", s)\n    return \" \".join(hashes)\n    \ntrain['hashtags'] = train['OriginalTweet'].apply(lambda x : extract_hash_tags(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"3lyqs8bOlh8_","outputId":"13e2180d-5717-4e62-8e73-fa62d338bf44","trusted":true},"cell_type":"code","source":"from collections import Counter\n\nallHashTags = list(train[(train['hashtags'] != None) & (train['hashtags'] != \"\")]['hashtags'])\nallHashTags = [tag.lower() for tag in allHashTags]\nhash_df = dict(Counter(allHashTags))\ntop_hash_df = pd.DataFrame(list(hash_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:15]\ntop_hash_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"9Hp0869GlzUl","outputId":"e6b369da-11c9-47f8-88c5-b595d54c6aef","trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(x=top_hash_df['word'],y=top_hash_df['count'],\n       orientation='v',\n       color=top_hash_df['word'],\n       text=top_hash_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eh0V9GVhmfsT"},"cell_type":"markdown","source":"## mentions"},{"metadata":{"id":"U4UWWKb8mim_","trusted":true},"cell_type":"code","source":"def get_mentions(s):\n    mentions = re.findall(\"(?<![@\\w])@(\\w{1,25})\", s)\n    return \" \".join(mentions)\ntrain['mentions'] = train['OriginalTweet'].apply(lambda x : get_mentions(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"fdUV5WqMms4W","outputId":"21c2b7fb-f779-412d-88fa-d53bd08abb8f","trusted":true},"cell_type":"code","source":"allMentions = list(train[(train['mentions'] != None) & (train['mentions'] != \"\")]['mentions'])\nallMentions = [tag.lower() for tag in allMentions]\nmentions_df = dict(Counter(allMentions))\ntop_mentions_df = pd.DataFrame(list(mentions_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:15]\ntop_mentions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"bxGRHa4Gm1RX","outputId":"ae5328e8-ca36-43a4-9c2b-b4a5d0080698","trusted":true},"cell_type":"code","source":"fig = px.bar(x=top_mentions_df['word'],y=top_mentions_df['count'],\n       orientation='v',\n       color=top_mentions_df['word'],\n       text=top_mentions_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #mentions in Covid19 Tweets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"vvQ0LhYBnPrl","trusted":true},"cell_type":"code","source":"train.drop([\"hashtags\", \"mentions\"], axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"0mm4qnkRL2sO"},"cell_type":"markdown","source":"### tweet times"},{"metadata":{"id":"g7NTMf4FL2sO","outputId":"91e85724-1a69-43dd-b03e-68981f7940d8","trusted":true},"cell_type":"code","source":"train[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"TweetAt\"].apply(lambda x : x.dayofweek).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"0DzBKFt8L2sP","outputId":"7e7357bd-4826-4ecf-810a-9176bf163b0a","trusted":true},"cell_type":"code","source":"train[\"TweetAt\"].apply(lambda x : x.dayofweek).value_counts().plot.barh()\nplt.title(\"maximun tweets during 2020\")","execution_count":null,"outputs":[]},{"metadata":{"id":"N_8Hnuv4L2sQ","outputId":"0a54fcd7-ad31-4ba9-eccd-2bfa53a7318f","trusted":true},"cell_type":"code","source":"train[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"day\"] = train[\"TweetAt\"].apply(lambda x : x.dayofweek)\ndmap = {0: 'Mon', 1: 'Tue', 2:'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\ntrain[\"day\"] = train[\"day\"].map(dmap)\nplt.title(\"Day with maximun tweets\")\nsns.countplot(train[\"day\"])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"c-IJ-SnZL2sQ","trusted":true},"cell_type":"code","source":"train.drop(\"day\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"7laLvaK9L2sR"},"cell_type":"markdown","source":"### making cheack points and dropping duplicated rows"},{"metadata":{"id":"cdZTZ3yVL2sR","trusted":true},"cell_type":"code","source":"train.drop_duplicates(inplace = True)\ntest.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y1KAnVxQL2sS","trusted":true},"cell_type":"code","source":"train_df = train.copy()\ntest_df = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"xGL-Arr2L2sS"},"cell_type":"markdown","source":"## 3. Text Mining"},{"metadata":{"id":"jVIgeGc_L2sS","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re","execution_count":null,"outputs":[]},{"metadata":{"id":"mykcmOGaL2sT","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import TweetTokenizer","execution_count":null,"outputs":[]},{"metadata":{"id":"paQ5A305Q6gn","outputId":"4f39ee48-731f-461e-89ec-c8e48c08759f","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"YJR8_ig3L2sT","outputId":"1d82984e-3bb1-4c15-d83a-2cacbdbc8b15","trusted":true},"cell_type":"code","source":"sentences = train['OriginalTweet'][:5]\n\nfor i in sentences[3:4]:\n    print(\"Original:\\n\")\n    print(i)\n    print('\\nTensorflow Tokenizer\\n:')\n    a = Tokenizer()\n    a.fit_on_texts([i])\n    print(a.word_index)\n    print(\"\\nTweet Tokenizer:\\n\")\n    print(TweetTokenizer().tokenize(i))\n    print('\\nNLTK word_tokenizer:\\n')\n    print(word_tokenize(i))","execution_count":null,"outputs":[]},{"metadata":{"id":"HW3awozgL2sU"},"cell_type":"markdown","source":"#### As you can see these all yield different results and you have to see which works best for your use case. \n#### For now we will use NLTK Tweet-Tokenizer."},{"metadata":{"id":"oMB5VzLSL2sU","trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\nlem = WordNetLemmatizer()\n\ndef cleaning(data):\n    #1. Remove urls \n    tweet_without_url = re.sub(r'http\\S+', ' ', data)\n    \n    #2. Remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+',' ', tweet_without_url)\n    \n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n    return \" \".join(text_cleaned)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nB_hY_noL2sW","trusted":true},"cell_type":"code","source":"train_df['OriginalTweet'] = train_df['OriginalTweet'].apply(lambda x: cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"MBWPGbOEL2sW","outputId":"1a61f8f5-c30e-49ea-9d93-ef7bcc3f5af4","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)  \ntrain_df.iloc[:, [4,5]].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SyW049aFL2sX","trusted":true},"cell_type":"code","source":"test_df['OriginalTweet'] = test_df['OriginalTweet'].apply(lambda x: cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"r8IvV2g5L2sX","outputId":"db874932-2892-4fdc-a8f9-aecc649b688b","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)  \ntest_df.iloc[:, [4,5]].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6OCFa8Rpnkze"},"cell_type":"markdown","source":"## uni grams for train tweets"},{"metadata":{"id":"YXP6aVGFnxZl","trusted":true},"cell_type":"code","source":"HQ_words = ' '.join([i for i in train_df['OriginalTweet']]).split() \nunigram_HQ = pd.Series(nltk.ngrams(HQ_words, 1)).value_counts()[:15]\nunigram_HQ = pd.DataFrame(unigram_HQ)\nunigram_HQ['idx'] = unigram_HQ.index\nunigram_HQ['idx'] = unigram_HQ.apply(lambda x: '('+x['idx'][0]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ry722hSgoJ3U","outputId":"cca02b1d-c19f-4d8f-ea4e-b33c152837cc","trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly.offline as pyoff\n\nplot_data = [\n    go.Bar(\n        x=unigram_HQ['idx'],\n        y=unigram_HQ[0],\n        marker = dict(\n            color = 'Blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 uni-grams from Covid-19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Uni-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"sR1nQmuErcro"},"cell_type":"markdown","source":"## Bi-grams for Tweets"},{"metadata":{"id":"-YQy0k5YrkiV","trusted":true},"cell_type":"code","source":"bigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 2)).value_counts())[:15]\nbigram_HQ = pd.DataFrame(bigram_HQ)\nbigram_HQ['idx'] = bigram_HQ.index\nbigram_HQ['idx'] = bigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"F-Fy_S0gr7VK","outputId":"74dda273-4589-48a9-8af6-86424b173ba4","trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=bigram_HQ['idx'],\n        y=bigram_HQ[0],\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 bi-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"CBuI1GlpsEia"},"cell_type":"markdown","source":"## Tri-grams for Tweets"},{"metadata":{"id":"Fq8fH6dpsJZK","trusted":true},"cell_type":"code","source":"trigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 3)).value_counts())[:20]\ntrigram_HQ = pd.DataFrame(trigram_HQ)\ntrigram_HQ['idx'] = trigram_HQ.index\ntrigram_HQ['idx'] = trigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', '+x['idx'][2]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"nfn5rZIisJAo","outputId":"85e7e0d6-20c8-4f32-8193-fa2a2d9234f8","trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=trigram_HQ['idx'],\n        y=trigram_HQ[0],\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 15 Tri-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"eKoucP4OL2sY"},"cell_type":"markdown","source":"## 4. WordCloud - Repetition of WordsÂ¶"},{"metadata":{"id":"LiVWPv3RL2sZ","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"id":"uUjm7vNHL2sZ","outputId":"fcbd4815-9bb4-4b0f-9eba-14e96d000c73","trusted":true},"cell_type":"code","source":"Positive = ' '.join([tweet for tweet in train_df['OriginalTweet'][train_df['Sentiment'] == 0]])\n\nwordcloud = WordCloud(background_color = \"white\", width = 800, height = 500,\n                      random_state = 21, max_font_size = 110).generate(Positive)\nplt.figure(figsize = (10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9vAvpdxkL2sa","outputId":"9e7a83fc-9a0e-4425-8843-003bd6cb5992","trusted":true},"cell_type":"code","source":"Negative = ' '.join([tweet for tweet in train_df['OriginalTweet'][train_df['Sentiment'] == 1]])\n\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(Negative)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Qrq1MNFoL2sa"},"cell_type":"markdown","source":"## 5. Sentiment Analysis Models \n\n\n"},{"metadata":{"id":"h4uoF7ALL2sd","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"id":"bSr9Eb5nL2sd","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df[\"OriginalTweet\"])  # fitting tokenizer on training_datase\n\nX = tokenizer.texts_to_sequences(train_df[\"OriginalTweet\"])  # getting text sequences from training dataframe\ny = train_df[\"Sentiment\"]\n\nvocab_size = len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"id":"gvEgfdZl_X-b","outputId":"779e91b4-23d3-46a0-ade7-f0777d3cf330","trusted":true},"cell_type":"code","source":"vocab_size","execution_count":null,"outputs":[]},{"metadata":{"id":"0PbE060nL2se","outputId":"1ef45155-d5d8-40c5-87aa-1a65fe1dceab","trusted":true},"cell_type":"code","source":"print(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\n----------Example----------\\n\")\nprint(\"Sentence:\\n{}\".format(train_df[\"OriginalTweet\"][6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[6]))\n\nX = pad_sequences(X, padding='post')  # adding padding of zeros to obtain uniform length for all sequences\nprint(\"\\nAfter padding :\\n{}\".format(X[6]))","execution_count":null,"outputs":[]},{"metadata":{"id":"LWgKvVnV_lGl","outputId":"184ef71b-2e9a-4df9-c158-0ef915418fe9","trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"tqbTBcDAL2se"},"cell_type":"markdown","source":"### a. Modeling with LSTM"},{"metadata":{"id":"y8sLiFnWI4_p","outputId":"382024d5-63ae-48d9-e20c-6015e0dcd24a","trusted":true},"cell_type":"code","source":"# hyper parameters\nEPOCHS = 3\nBATCH_SIZE = 32 \nembedding_dim = 16\nunits = 256\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))  # This version performs the same function as Dropout, however, it drops entire 1D feature maps instead of individual elements.\nmodel.add(LSTM(units, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(3,activation = 'softmax'))  # we have 3 categories so we have to use softmax \nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"id":"uc7ftVAkJagI","outputId":"c4ae9bd8-a537-40b1-9903-931ef4b9cb05","trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(train['Sentiment']).values  \n# categorical cross entropy requires get_dummies cause of it only accepts [0]s and [1]s\n\nprint(X.shape,Y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_qiCby-CJudm","outputId":"210e9948-b2eb-48d3-9a5f-11bec7411558","trusted":true},"cell_type":"code","source":"model.fit(X, Y, epochs = 5, validation_split = 0.12, batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"Sbtp3T4jYJZD"},"cell_type":"markdown","source":"#### results of LSTM model"},{"metadata":{"id":"ipUZwPz0O9d-","outputId":"7eea9f95-30c9-44b4-d4f7-2b4951137d2b","trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"hK3odR8amJbE"},"cell_type":"markdown","source":"## b. Modeling with Bidirectional LSTM"},{"metadata":{"id":"cbpOAs-qpyGf","trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\n\n\nmodel_blstm = tf.keras.Sequential([\n    Layers.Embedding(vocab_size, embedding_dim, input_length = X.shape[1]),\n    Layers.Bidirectional(Layers.LSTM(units, return_sequences = True)),  # recurrent layer with lstm\n    Layers.GlobalMaxPool1D(),  # Downsamples the input representation by taking the maximum value over the target\n    Layers.Dropout(0.2),\n    Layers.Dense(64, activation = \"relu\"),\n    Layers.Dropout(0.2),\n    Layers.Dense(3, activation = 'softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{"id":"u8lcln4zL2sf","trusted":true},"cell_type":"code","source":"model_blstm.compile(loss = SparseCategoricalCrossentropy(from_logits = True),  # Computes the crossentropy loss between the labels and predictions.\n              optimizer = 'adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"rRtiLNOHL2si","outputId":"6ac63407-37b6-410e-e40b-e501f0ae9c38","trusted":true},"cell_type":"code","source":"model_blstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"2-ieLO_1L2si","outputId":"875a192c-3c2c-4695-d08d-0f4ebdfd75fd","trusted":true},"cell_type":"code","source":"model_blstm.fit(X, y, epochs = EPOCHS, validation_split = 0.12, batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"eImPD0nfL2sj"},"cell_type":"markdown","source":"#### Results of Bi-LSTM model"},{"metadata":{"id":"3NZu5DMsL2sj","outputId":"7a0d9488-5075-46a6-f7c6-8882ed11a254","trusted":true},"cell_type":"code","source":"model_blstm_loss = pd.DataFrame(model_blstm.history.history)\nmodel_blstm_loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"uJ7cA_YjMibQ"},"cell_type":"markdown","source":"## c. Modeling with CNN"},{"metadata":{"id":"DP20JhM3MtT9","outputId":"9cb43479-bad2-4fe9-c2d2-1092a91c28af","trusted":true},"cell_type":"code","source":"model_cnn = Sequential()\nmodel_cnn.add(Embedding(vocab_size,embedding_dim,input_length=X.shape[1]))\n\nmodel_cnn.add(Conv1D(64, kernel_size=3, padding='same', activation='relu', strides=1))\n# This layer creates a convolution kernel that is convolved with the layer input over a single spatial \n# (or temporal) dimension to produce a tensor of outputs.\n\nmodel_cnn.add(GlobalMaxPooling1D()) \n# Downsamples the input representation by taking the maximum value over the dimension.\n\nmodel_cnn.add(Dense(128, activation='relu'))\nmodel_cnn.add(Dropout(0.2))\n\nmodel_cnn.add(Dense(3,activation='softmax'))\n\nmodel_cnn.compile(loss='categorical_crossentropy',optimizer= 'adam',metrics=['accuracy'])\n\nmodel_cnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"UXk4DhHwN4a8","outputId":"bd4e8d35-5204-40bc-fca5-77e3f3c96372","trusted":true},"cell_type":"code","source":"model_cnn.fit(X, Y, validation_split = 0.12,epochs=2, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"ddDiSZCBcasy","outputId":"587b7e84-1605-498b-867b-3f2f4dcaa301","trusted":true},"cell_type":"code","source":"model_cnn_loss = pd.DataFrame(model_cnn.history.history)\nmodel_cnn_loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"NtRhGOEhL2sk"},"cell_type":"markdown","source":"## 6. Evaluation"},{"metadata":{"id":"HKisgSvaL2sk"},"cell_type":"markdown","source":"### Preprocessing test data "},{"metadata":{"id":"nXTx2IjFL2sl","trusted":true},"cell_type":"code","source":"X_test = test['OriginalTweet'].copy()\ny_test = test['Sentiment'].copy()\n\nX_test = X_test.apply(cleaning)\n\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_test = pad_sequences(X_test, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"id":"BVck2aEEL2sm"},"cell_type":"markdown","source":"### Making Predictions with Bi-LSTM"},{"metadata":{"id":"7gjtMEzBL2sm","outputId":"347bb002-f2fe-4479-be1c-3d163cf673e8","trusted":true},"cell_type":"code","source":"pred = model_cnn.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"3yDQFWYRL2sn"},"cell_type":"markdown","source":"### Model Results"},{"metadata":{"id":"sZo79Hb1L2so","outputId":"60d571b2-e3b7-4bac-a84e-ebc5048df762","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"8nqDtSpyL2so","outputId":"77603826-03e9-4831-c608-d09a51cb5d33","trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test, pred)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in labels],\n    columns = [i for i in labels]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ThQdj8-cL2sp","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}