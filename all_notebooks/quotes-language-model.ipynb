{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\nThis Notebook Contains mainly two things.\n\n1 WordCloud<br>\n2 Language Model\n\nDataset : Quotes-500k <br>\nAlso, Generated text(Quotes) from trained Language Model. Though, most of the time it doesn't make any sense but it is sounds like quotes. It also has learnt punctuation marks like comma,< eos >.\n\n<img src=\"https://i.imgur.com/VkOp3Qw.png\" />"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/quotes-500k/quotes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word-cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = df['quote']\ntext = dff.str.cat(sep=' ')\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"one\")\n\nwc = WordCloud(max_font_size=40, max_words=200,stopwords=stopwords, contour_width=3, contour_color='steelblue')\n\nwordcloud = wc.generate(text)\nplt.figure(figsize=(12, 9))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Language Model"},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# source of code https://github.com/pytorch/examples.git\n\nimport os\nfrom io import open\nimport torch\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        \"\"\"Tokenizes a text file.\"\"\"\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['<eos>']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append(torch.tensor(ids).type(torch.int64))\n            ids = torch.cat(idss)\n\n        return ids\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RNNModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if rnn_type in ['LSTM', 'GRU']:\n            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n        else:\n            try:\n                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n            except KeyError:\n                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output)\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        if self.rnn_type == 'LSTM':\n            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n        else:\n            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 2020\ncuda = True\ndata_path = '../input/quotes-500k/'\nbatch_size = 20\neval_batch_size = 10\nbptt = 35\nmodel_name = 'LSTM'\nemsize = 200\nnhid = 200\nnlayers = 2\ndropout = 0.2\nlr = 20\nclip = 0.25\nepochs = 8\ntied = True\nsave = 'model.pth'\nlog_interval = 2000\nonnx_export = ''\nnhead = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.onnx\n\n\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\n###############################################################################\n# Load data\n###############################################################################\n\ncorpus = Corpus(data_path)\n\n\nprint(\"Data loaded \")\n\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\n\ntrain_data = batchify(corpus.train, batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\n\nmodel = RNNModel(model_name, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\n# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\n\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n\n\ndef evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n    if True:\n        hidden = model.init_hidden(eval_batch_size)\n        \n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            output, hidden = model(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    if True:\n        hidden = model.init_hidden(batch_size)\n        \n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        model.zero_grad()\n        \n        hidden = repackage_hidden(hidden)\n        output, hidden = model(data, hidden)\n        loss = criterion(output.view(-1, ntokens), targets)\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        for p in model.parameters():\n            p.data.add_(-lr, p.grad.data)\n\n        total_loss += loss.item()\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n                    'loss {:5.2f} | ppl {:8.2f}'.format(\n                epoch, batch, len(train_data) // bptt, lr,\n                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\n\ndef export_onnx(path, batch_size, seq_len):\n    print('The model is also exported in ONNX format at {}'.\n          format(os.path.realpath(onnx_export)))\n    model.eval()\n    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n    hidden = model.init_hidden(batch_size)\n    torch.onnx.export(model, (dummy_input, hidden), path)\n\n\n# Loop over epochs.\nlr = lr\nbest_val_loss = None\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            torch.save(model, save)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 4.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n\n# Load the best saved model.\n\nmodel = torch.load(save)\n    # after load the rnn params are not a continuous chunk of memory\n    # this makes them a continuous chunk, and will speed up forward pass\n    # Currently, only rnn model supports flatten_parameters function.\nmodel.rnn.flatten_parameters()\n\n# Run on test data.\ntest_loss = evaluate(test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)\n\nif len(onnx_export) > 0:\n    # Export the model in ONNX format.\n    export_onnx(onnx_export, batch_size=1, seq_len=bptt)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '/kaggle/input/quotes-500k/'\ncheckpoint = 'model.pth'\noutf = 'generated.txt'\nwords = 1000\nseed = 2020\ncuda = True\ntemperature = 0.7   #temperature - higher will increase diversity\nlog_interval = 100\ngen_text = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\nif temperature < 1e-3:\n    parser.error(\"--temperature has to be greater or equal 1e-3\")\n\n\nmodel = torch.load(checkpoint).to(device)\nmodel.eval()\n\ncorpus = Corpus(data_path)\nntokens = len(corpus.dictionary)\nprint(\"data loaded\")\n#is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n\nhidden = model.init_hidden(1)\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n\n\nwith open(outf, 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(words):\n            \n            output, hidden = model(input, hidden)\n            word_weights = output.squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            input.fill_(word_idx)\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n            gen_text+=(word + ('\\n' if i % 20 == 19 else ' '))\n            if i % log_interval == 0:\n                print('| Generated {}/{} words'.format(i, words))\nprint(gen_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Generated Text (on local machine)\neven though the human life has stopped in the many and the ways of living its original and the one\nwho is utterly aware of the ability to create a new future. <br><br> When I lost my childhood I could\nwrite your grief for a while, and one day she might be a girl, for she could know she would\nbe still alone. <br><br> The pen and the cold of our minds are the only creatures that are born in\nour hearts. They are different and the possibilities are in the dark and the same. <br><br> When there is one\nof the deepest things in life that sometimes that is the only one that is the only thing you can\never find is that it reminds you, even when you make the same decision. <br><br> I have always been in\nthis way: I know the slightest things I have to share in the world, and let myself know my own\ntrue love and a little way to be told to myself, and I guess this is a fairy tale in\nwhich one should not know where the rest of it will be. <br><br> What it was like to be about\nthe worst of the people that could be written in the best acts of a fairy tale a world at\nthe same time. In the end, there are no more than the same things and the worst part in the\nworld. <br><br> You make a mistake and never allow your work to make a difference between your ability to be\nan inspired person who needs a physical and commitment to be an old person who will yield to the rest\nof your life. <br><br> You are not the One who answers you to the past. <br><br> Sometimes I wonder what\nthe world feels in the world and I have been here in a new period with such a thing which\nis an active and special one, and that I have been as much to do with a little sense of\nthem and see their own desires that are literally what I can do with the people who have patience and\nthen provide the home that the child has been in their years to the extent about the past and the\nfuture. <br><br> If you want to convert the future, but not to change any event is a little. You cannot\ngo to the bathroom at the top of the mountain, you will never be able to go to the future.\n<br><br> I need to write to the future, but it is my name. <br><br> We are not the best one\nover the future. When we see that we are bound to one that we choose to make a future to\nthat fact, we have only a more effective and precious combination of our problems and the worst of our life\nfrom its own hands. <br><br> When a person is a writer you see a lot to a person and a\nperson of men to do the best and most beautiful things of the world. <br><br> Love, and not a mirror,\nfor the sake of human nature and the belief in the first time and what is going on. <br><br> The\nfirst time you look at your mind and become aware of things, you can only write real things. <br><br> It\nwas such one that which could be the most potent factor in the Christian world was the theatre of the\nlaws of the present and the universe that the dangers of the gods are going to look out at their\nlives the way it became the difference between the ages and the new and the other things and the drama\nand the method. And then the evidence had been taken away from the laws and the progressive systems of the\ngeneral of the young. <br><br> The man has been in a state of enjoyment and a very peculiar and early\nanimal that is constantly known, and he is a child and who do not have to pay for the wish\nto sustain it. <br><br> There is a greater reason for the fear of the past than it takes to be\na writer, and never lies in the present and aligning to the future. <br><br> The future is as much as\na lie in the past and that we are born to as a place where our present and the future.\nWe are not in the future, but in the past we bring into the future and the future. <br><br> It\nis a wise man's mind. <br><br> When you have the power to stop , you reach and a great future\nthat is to be a reality. <br><br> Love is not a man, it is a kind of love. <br><br> The\ngreatest love is a state of action and a blessing for their own glory. <br><br> The best thing to do\nis think about saying that only one can understand their feelings inside you and have found you in the light\nof your life like a roaring and scratching past, and that you are in the direction of your own life.\nYou can only be able to see them with them. <br><br> Isn't it a natural thing to do with that\nperson and a person who thinks there's one thing and another can have a right to do the same thing.\n<br><br> Once I have seen what I think about that I spend my life on the other side of the\nwindow and I know it as a hidden one. <br><br> If you are good, you will forget the way you\nare in the choices of your work and time to understand what you are doing to your future. <br><br> When\nhe has been in the middle of a world of awareness he has been in a hurry to tell the\ndifference in his family. <br><br> A person's heart is extraordinary and a man must not be in the name of"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}