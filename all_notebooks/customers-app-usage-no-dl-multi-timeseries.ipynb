{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h2 style = \"font-size:50px; font-family:Garamond ; font-weight : normal; background-color:#FA4659; color :  #F0FFF3; text-align: center; border-radius: 5px 5px; padding: 5px\"> Customers And App Usage: Multiple TimeSeries with No DL</h2> \n<br>\n<div class = 'image'> <img style=\"float:center; width:80%;border:10px solid #FA4659;\" align=center src = https://cdn.searchenginejournal.com/wp-content/uploads/2020/07/3f358ca2-3ebe-4783-931c-f58b110b2254-5f1ce28e38466-1520x800.jpeg> \n</div>\n<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks....Do checkout if you find my work Helpful....Happy Reading :)</strong></p>\n<ol>\n<li><a href =\"https://www.kaggle.com/bhuvanchennoju/netflix-is-awesome-why-see-here\" >Netflix is awesome.why? see here</a></li>\n<li> <a href =\"https://www.kaggle.com/bhuvanchennoju/data-stroytelling-auc-focus-on-strokes\" >Datastorytelling with auc focus on strokes</a></li>\n<li> <a href = \"https://www.kaggle.com/bhuvanchennoju/s-s-experimets-fun-with-r2-99-21\">Diamonds and Dollars: Experiments and fun r2(99.2)</a></li>\n<li> <a href = \"https://www.kaggle.com/bhuvanchennoju/ancient-roots-of-agriculture-a-data-overview\">Ancient roots of Agriculture: a data overview</a></li>\n<li> <a href = \"https://www.kaggle.com/bhuvanchennoju/hey-siri-is-it-a-or-f1-0-992\">Hey siri: cat or dog classificaiton</a></li>\n<li> <a href = \"https://www.kaggle.com/bhuvanchennoju/women-and-cancer-analysis-and-detection\">women and cancer: analysis and detection</a></li>\n    \n</ol>","metadata":{}},{"cell_type":"markdown","source":"<a id = '0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px; padding: 5px;text-align:center; font-weight: bold\" >Table of Contents</h2> \n\n* [1. Introduction](#1.0)\n    * [1.1 Libraries and Initial Data with Stats](#1.1)\n \n* [2. Data Visualizations and Insights](#2.0)\n    * [2.1 Data Visualizations of Primary Features](#2.1)\n    * [2.2 Data Visualizations with AutoViz](#2.2)\n\n* [3. Modeling and Predictions](#3.0)\n    * [3.1 Approch 1: Failed 1 ](#3.1)\n    * [3.2 Approch 2: Failed 2](#3.2)\n    * [3.3 Approch 3: Going In A Right Direction ](#3.3)\n    * [3.3.1 Approch 3:  Lag0 addition ](#3.3.1)\n    * [3.3.2 Approch 3: Lag1 addition](#3.3.2)\n    * [3.4 Feature Engineering - Rolling Windows, Lag Addition,and Datetimes ](#3.4)\n    * [3.5 Baseline Model with Feature engineered data ](#3.5)\n    * [3.6 Feature Selection with Permuation Feature Importance](#3.6)\n    * [3.7 Final Baseline Model with Selected Features ](#3.7)\n    * [3.8 Hyperparameter Optimization with Optuna](#3.8)\n    * [3.9 Error Analysis](#3.9)\n\n\n* [4. Summary](#4)\n\n* [5. References](#5)\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id = '1.0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >1. Introduction</h2>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What are we trying to solve with this dataset? - Problem Statement</strong></p>\n\nWe have a lot of customer on our InvestorAI platform. Whenever they login in our app and view anything, we get pings from their mobile phone indicating that they are using the app.\n<br>\nYou have been provided with 3 weeks of training data and 1 week of test data.\n<br>\n<br>\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What this data about?</strong></p>\n\nThe dataset has three files. The data in the first two films can be used for training the model\nand the third file contains test data.\n\n* customers.csv: This file contains customers profile data\n* pings.csv: This file contains: This file contains the customer pings\n* test.csv: This file has the test data\n\nThe fields present in these files are described below.\n\n**customers.csv:**\n\n* id: Unique customer id\n* gender: Gender of the customer\n* age: Customer’s age\n* numberofkids: The number of kids the customer has\n\n**pings.csv:**\n\n* id: The id of the customer whose ping has been received. It is guaranteed\n  that the details of this customer will be available in the customers.csv file.\n* timestamp: The Unix epoch timestamp when ping was received by the\n  system.\n\n**test.csv:**\n\n* id: The id of the customer whose ping has been received. It is guaranteed\n  that the details of this customer will be available in the cutomers.csv file.\n* date: The date in YYYY-MM-DD format for which the online hours needs\n  to be predicted.\n* onlinehours: The number of online hours Given id and date, you should predict onlinehours.\n\n<br>\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Objective</strong></p>\n\nWe want to predict how many hours the customer will be online / using our app on a given day. So the test data contains customer id, and date (during the test data period).\nThe test data also contains the actual online hours, which is what our model should predict.\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Evaluation Metric:</strong></p>\n\n\nWe will be looking at Root Mean Squared Error or RMSE for short (lower the better) to see how good your model is.","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#11CBD7 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Initial Insights:</strong></p>\n\n1. Looks like we are handling a raw data as custumer database and the activity are seperated out in the data itself. So few assumpitions and data cleaning could be needed ?\n2. Clearly given in the problem statement that we are going to handle a time related data for each customer.  This could be a multivariante time series problem with multiple timeseries. So modeling stategy and model building need to  be done to capture patterns, ( lets assume)\n3. Error metric is root mean squred error so its sensitive to small changes, can I optimize my model for better metric, root mean squared log error which is more reponsive to very fine changes?","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:35px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Lets Dive Into The Data....</strong></p>","metadata":{}},{"cell_type":"markdown","source":"<a id = '1.1'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > Libraries,Data and Palette</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>1. Libraries Importing</strong></p>","metadata":{}},{"cell_type":"code","source":"#importing modules\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport time\nt = time.time()\n\nprint('Importing startred...')\n\n# base libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nfrom scipy import stats\nfrom random import randint\nfrom datetime import datetime\n\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib \n%matplotlib inline\nimport seaborn as sns\nimport missingno as msno\n\n\n# preprocessing libraries\n\n\nfrom sklearn.model_selection import (TimeSeriesSplit,\n                                     GridSearchCV,\n                                     RandomizedSearchCV,\n                                     train_test_split, \n                                     KFold, \n                                     StratifiedKFold,\n                                    cross_val_score)\n\nfrom sklearn.preprocessing import (LabelEncoder,\n                                   StandardScaler, \n                                   MinMaxScaler, \n                                   OrdinalEncoder)\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n# metrics\nfrom sklearn.metrics import (mean_squared_error, \n                             r2_score, \n                             mean_absolute_error)\nfrom sklearn.metrics import make_scorer\n\n\n# modeling algos\nfrom sklearn.linear_model import (LogisticRegression,\n                                  Lasso, \n                                  ridge_regression,\n                                  LinearRegression)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (AdaBoostRegressor, \n                              RandomForestRegressor,\n                              VotingRegressor, \n                              GradientBoostingRegressor)\nfrom xgboost import XGBRegressor\nfrom lightgbm import (LGBMRegressor,\n                      early_stopping)\n\nfrom sklearn.base import clone ## sklearn base models for stacked ensemble model\n\n\n#Interpretiability of the model\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n\n## misea\nfrom sklearn.pipeline import make_pipeline\n#!pip install rich\n#import rich     # terminal color fonts\n#from rich import print as color_print\n#from rich.console import Console\n#from rich.theme import Theme\n\n# terminal color palette\n#custom_theme = Theme({\n#    \"info\" : \"italic bold cyan\",\n#    \"warning\": \"bold blue\",\n#    \"danger\": \"bold blue\"\n#})\n\n#console = Console(theme=None,color_system = 'standard',)\n\n\nprint('Done, All the required modules are imported. Time elapsed: {}sec'.format(time.time()-t))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:59:05.048492Z","iopub.execute_input":"2021-07-02T08:59:05.049034Z","iopub.status.idle":"2021-07-02T08:59:15.792001Z","shell.execute_reply.started":"2021-07-02T08:59:05.048924Z","shell.execute_reply":"2021-07-02T08:59:15.790975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>2. Color Palette</strong></p>","metadata":{}},{"cell_type":"code","source":"colors= ['#11CBD7' ,'#C6F1E7' ,'#F0FFF3' ,'#FA4659']\n\nsns.palplot(colors,size = 3)\n\nplt.gcf().set_size_inches(10,2)\nplt.gcf().set_dpi(100)\n\nplt.text(-0.75,-0.77, 'Customers And App Usage: Color Palette',{'fontfamily':'serif', 'size':18, 'weight':'bold'})\nplt.text(-0.75,-0.66, 'Lets try to stick to these colors throughout presentation.',{'fontfamily':'serif', 'size':12},alpha = 0.9)\nfor idx,values in enumerate(colors):\n    plt.text(idx-0.25,0, colors[idx],{'fontfamily':'serif', 'size':16, 'weight':'bold','color':'black'}, alpha =1)\nplt.gcf().set_facecolor('#f5f6f6')\nplt.box(None)\nplt.axis('off')\nplt.text(3.5,0.65,'© Made by bhuvanchennoju/Kaggle',{'fontfamily':'serif', 'size':10,  'color':'black'})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:19:47.546319Z","iopub.execute_input":"2021-07-02T09:19:47.546671Z","iopub.status.idle":"2021-07-02T09:19:47.652282Z","shell.execute_reply.started":"2021-07-02T09:19:47.546641Z","shell.execute_reply":"2021-07-02T09:19:47.65159Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>3. Data Loading and Initila insights</strong></p>","metadata":{}},{"cell_type":"code","source":"# loading data\ncustomer_df = pd.read_csv('../input/mobile-usage-time-prediction/customers.csv', delimiter = ',', encoding = 'utf-8')\npings_df = pd.read_csv('../input/mobile-usage-time-prediction/pings.csv', delimiter = ',', encoding = 'utf-8')\ntest_df = pd.read_csv('../input/mobile-usage-time-prediction/test.csv', delimiter = ',', encoding = 'utf-8')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:10:54.770119Z","iopub.execute_input":"2021-07-02T09:10:54.770482Z","iopub.status.idle":"2021-07-02T09:11:12.459658Z","shell.execute_reply.started":"2021-07-02T09:10:54.770451Z","shell.execute_reply":"2021-07-02T09:11:12.458794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Sneak peak into the data....</strong></p>","metadata":{}},{"cell_type":"code","source":"print('*'*10 + ' Data Summary and Inital peaks ' + '*'*10 )\n\nprint('\\n'+'*'*15 + 'Shapes of Data ' + '*'*15+'\\n' )\n\nprint('Shape of the Customer database: {}'.format(customer_df.shape))\nprint('Shape of the Pings dataset: {}'.format(pings_df.shape))\nprint('Shape of the Test dataset: {}'.format(test_df.shape))\n\nprint('\\n'+'*'*50 + '\\n')\n\nprint(' Head of Customer database '+ '\\n')\nprint(customer_df.head())\n\nprint('\\n' +'*'*50+ '\\n')\n\nprint('Head of Ping dataset'+ '\\n')\nprint(pings_df.head())\n\nprint('\\n' +'*'*50 + '\\n')\n\nprint('Head of Test dataset'+ '\\n')\nprint(test_df.head())\n\nprint('\\n' +'*'*50 + '\\n')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:12:20.943005Z","iopub.execute_input":"2021-07-02T09:12:20.943724Z","iopub.status.idle":"2021-07-02T09:12:20.980253Z","shell.execute_reply.started":"2021-07-02T09:12:20.943679Z","shell.execute_reply":"2021-07-02T09:12:20.979324Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Initial Insights about Data</strong></p>\n1. As given in problem statement we have three csv files, customer, pings, test datasets. All in common is uniques id, Its customer id.\n2. Customer database gives all the unique things about customer. This include children, gender, age, of the customer.\n3. Coming to Ping dataset, data of timestamp is little difficult to compehend as its in unix epoch encodoing, as it is number of seconds from 1970s to specific time, need conversion to make something out of it.\n4. Test data giving some hints, how I need to convert my training dataset? May be to datatime object.\n\n\nNow questions are how to convert ping data timestamp form epoch timestamp to pandas timestamp? and how to calculate the online hours like test dataset? and finaly need to merge customer dataset to ping dataset and test dataset..\n\n\nfirst thing first... lets convert ping data timestep to datatime timestamp and extract date, and other basic things later. \n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Number of Online Hours Calculation Assumption: </strong></p>\n\n\nHere is the thing, I can calculate online hours directly by taking the one step slide diffrence of timestamps for each date for each customer (by grouping them together). But logically this could be a flawed approch. Reason behind this is no idea when session of custemer end as there is no capping limit given in problem statement. \n\nSo, assuming if the slide time difference is less than 2minutes (i.e, 120Sec)\nuser active online time is same, if its more than that lets cap it to 2mins.\n\n\nlets implement this idea to the ping dataset... ","metadata":{}},{"cell_type":"code","source":"### value sorting with respect to id and timestamp\npings_df = pings_df.sort_values(by = ['id','timestamp']).reset_index(drop=True)\n\n# creating a copy to preserve actual ping data\ntemp_ping_df = pings_df.copy()\n\n#pre-processing data\ntemp_ping_df.drop_duplicates(inplace = True)\ntemp_ping_df['timestamp_decode'] = temp_ping_df['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n\n\n## timestamp and datagrouping\ntemp_ping_df['date'] = temp_ping_df['timestamp_decode'].dt.date\ntemp_ping_df['online_hours'] = (temp_ping_df.groupby(by=['id','date'])['timestamp'].diff())/(60*60)\ntemp_ping_df['online_hours']  =  temp_ping_df['online_hours'].apply(lambda x: x if x< (2/60) else (2/60))\n\ntemp_ping_df.fillna(0,inplace = True)\n\n#### creating our training data\ntrain_df= (temp_ping_df.groupby(by = ['id','date'])['online_hours'].sum()).reset_index()\ntrain_df['online_hours'] = round(train_df['online_hours'],1)\n\n\n## print statements\nprint('\\n' +'*'*50+ '\\n')\n\nprint('Head of the processed train data'+ '\\n')\nprint(train_df.head())\n\nprint('\\n' +'*'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:26:59.991861Z","iopub.execute_input":"2021-07-02T09:26:59.992261Z","iopub.status.idle":"2021-07-02T09:30:31.425376Z","shell.execute_reply.started":"2021-07-02T09:26:59.992228Z","shell.execute_reply":"2021-07-02T09:30:31.424344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now its looks good!! I got my train data like test data given! and now I can concatinate both train and test data together for feature extractions with less effort...\n\nIdea is to sort test data with customer id and date, and store the value of online hours seperately while dropping online hours from train dataset, There by concatinate both test and train datasets.","metadata":{}},{"cell_type":"markdown","source":"Okay everything ready, now lets see the min, max dates for train and test datasets\nThen move with concatination of both dataframes...","metadata":{}},{"cell_type":"code","source":"print('\\n' +'*'*50 + '\\n')\nprint('Minimum date and Maximum date for train dataset: {},{}'.format(train_df.date.min(), train_df.date.max()))\nprint('Minimum date and Maximum date for test dataset: {},{}'.format(test_df.date.min(), test_df.date.max()))\nprint('\\n' +'*'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:30:59.444391Z","iopub.execute_input":"2021-07-02T09:30:59.444722Z","iopub.status.idle":"2021-07-02T09:30:59.468335Z","shell.execute_reply.started":"2021-07-02T09:30:59.444692Z","shell.execute_reply":"2021-07-02T09:30:59.467306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '2.0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 2. Data Visualization and Insights</h2>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<a id = '2.1'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 2.1 Data visualization of Primary Features</h2>","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (15,6))\nfig.set_facecolor('#f5f6f6')\n\ngs = fig.add_gridspec(10,10)\n\nax0 = fig.add_subplot(gs[:,0:5])\nax1 = fig.add_subplot(gs[:,5:], sharey = ax0)\n\n\naxes = [ax0,ax1]\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right', 'top','bottom']:\n        ax.spines[loc].set_visible(False)\n\nax0.tick_params(axis='y', which='major', \n                    labelsize=12, labelright =False,left =False,right=False,labelleft=True,rotation = 0)\n\n\n\n### plots \nsns.distplot(x = train_df['online_hours'],\n             hist_kws = {'color':colors[0], 'linewidth': 2,'edgecolor':colors[-1], 'alpha':0.9 },\n             kde = True,kde_kws = {'color':'black', \"linewidth\": 1.5}, ax=ax0)\nsns.distplot(x = test_df['online_hours'],\n             hist_kws = {'color':colors[0], 'linewidth': 2,'edgecolor':colors[-1], 'alpha':0.9},\n             kde = True,kde_kws = {'color':'black', \"linewidth\": 1.5}, ax=ax1)\nax1.axes.get_yaxis().set_visible(False)\nax0.set_ylabel('Count',**{'font':'serif','size':16,'weight':'bold'})\nax0.set_xlabel('Online Hours',**{'font':'serif','size':16,'weight':'bold'})\nax1.set_xlabel('Online Hours',**{'font':'serif','size':16,'weight':'bold'})\n\n\n\n\n#### final writigs and titles\n\n\nfig.text(0.05,1,'Customers and App Usage: Train,Test Data Online Hours Distribution ' ,**{'font':'serif', 'size':22,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.05,0.92,'''It is clear that our customers fell into somekind of  definite categories such as rare users, modereate users, high users. \nLets put speculations a side and try to add no user data to train dataframe, which seems missing''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.70,0,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':12, 'weight':'bold',},alpha = 0.7)\n\n\n### subplot titles\nax0.text(9,.4, 'Train Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\nax1.text(9,.4, 'Test Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:03:21.030961Z","iopub.execute_input":"2021-07-02T10:03:21.031537Z","iopub.status.idle":"2021-07-02T10:03:22.265641Z","shell.execute_reply.started":"2021-07-02T10:03:21.031492Z","shell.execute_reply":"2021-07-02T10:03:22.264691Z"},"_kg_hide-output":false,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (13,5))\nfig.set_facecolor('#f5f6f6')\n\ngs = fig.add_gridspec(20,20)\n\nax0 = fig.add_subplot(gs[:,0:9])\nax1 = fig.add_subplot(gs[:,11:], sharey = ax0)\n\naxes = [ax0,ax1]\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right', 'top','bottom']:\n        ax.spines[loc].set_visible(False)\n\nax0.tick_params(axis='y', which='major', \n                    labelsize=12, labelright =False,left =False,right=False,labelleft=True,rotation = 0)\n   \n    \n### plots \n\ntrain_date_order = train_df.date.sort_values().unique().tolist()\ntest_date_order = test_df.date.sort_values().unique().tolist()\n\nsns.countplot(x = train_df['date'], ax=ax0,order = train_date_order,\n              color = colors[0], edgecolor = colors[-1], linewidth =2,alpha = 0.8)\nsns.countplot(x = test_df['date'],ax=ax1,order = test_date_order,\n             color = colors[0], edgecolor  = colors[-1], linewidth =2,alpha = 0.8)\n\nax1.axes.get_yaxis().set_visible(False)\nax0.set_ylabel('Count',**{'font':'serif','size':16,'weight':'bold'})\nax0.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\nax1.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\n\nax0.set_xticklabels(ax0.axes.get_xticklabels(), rotation = 90,**{'font':'serif','size':12})\nax1.set_xticklabels(ax1.axes.get_xticklabels(), rotation = 90,**{'font':'serif','size':12})\n\n#### final writigs and titles\n\n\nfig.text(0.05,1.065,'Customers and App Usage: Train,Test Data Date Distribution ' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.05,0.98,'''From Training data it is clear that we have few zero online hour usage users for few dates.\nAnd one more thing is training data for 22nd was given which need to remove from our training data.''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,-0.125,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\n\n\n### subplot titles\nax0.text(15,2650,'Train Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\nax1.text(5,2650,'Test Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:08:02.395266Z","iopub.execute_input":"2021-07-02T10:08:02.395646Z","iopub.status.idle":"2021-07-02T10:08:02.949065Z","shell.execute_reply.started":"2021-07-02T10:08:02.395613Z","shell.execute_reply":"2021-07-02T10:08:02.947806Z"},"_kg_hide-output":false,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (13,4))\nfig.set_facecolor('#f5f6f6')\n\ngs = fig.add_gridspec(20,20)\n\nax0 = fig.add_subplot(gs[:,0:9])\nax1 = fig.add_subplot(gs[:,11:], sharey = None)\n\naxes = [ax0,ax1]\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right', 'top','bottom']:\n        ax.spines[loc].set_visible(False)\n\nax0.tick_params(axis='y', which='major', \n                    labelsize=12, labelright =False,left =False,right=False,labelleft=True,rotation = 0)\n   \n    \n### plots \n\ntrain_date_order = train_df.id.sort_values().unique().tolist()\ntest_date_order = test_df.id.sort_values().unique().tolist()\n\nsns.kdeplot(x = train_df.id.value_counts().values,ax = ax0,color = colors[1], fill = True,alpha = 0.9)\nsns.kdeplot(x = train_df.id.value_counts().values,ax = ax0,color = colors[-1], shade = False)\nsns.kdeplot(x = test_df.id.value_counts().values,ax = ax1,color = colors[1], fill = True,alpha = 0.9)\nsns.kdeplot(x = test_df.id.value_counts().values,ax = ax1,color = colors[-1], shade = False)\n\nax0.axes.get_yaxis().set_visible(False)\nax1.axes.get_yaxis().set_visible(False)\nax0.set_ylabel('Count',**{'font':'serif','size':16,'weight':'bold'})\nax0.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\nax1.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\n\n\n#### final writigs and titles\n\n\nfig.text(0.05,1.085,'Customers And App Usage: Train,Test Data Id Distribution ' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.05,0.98,'''From the distribution plot it is clear that, Our customer ids are not uniformly in training data.\nTest data is very uniform, so need to filter out few observations from training data.''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,-0.125,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\n\n\n### subplot titles\nax0.text(7.5,0.125,'Train Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\nax1.text(12,5.95,'Test Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:15:09.087283Z","iopub.execute_input":"2021-07-02T10:15:09.08763Z","iopub.status.idle":"2021-07-02T10:15:09.458936Z","shell.execute_reply.started":"2021-07-02T10:15:09.087598Z","shell.execute_reply":"2021-07-02T10:15:09.457924Z"},"_kg_hide-output":false,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Insights for Primary Data Plots:</strong></p>\n\n\n1. Clearly some values of onlinehours zero need to be added to the training data. From the plot is is clear that data is not balanced for few dates, lets add zeros for online hours in those particular dates.\n2. Date in training set have 22nd date values, which we need to predict, so lets fileter our data for excluding that dates data.\n3. Distribution data of Id, implies the same that, training data has to balance with zeros, when online hours is not present for a particular date.\n\n\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Lets clean training data as per our observations...</strong></p>\n","metadata":{}},{"cell_type":"code","source":"## lets correct the training dataset...\ntrain_df['date'] = train_df['date'].astype(str)\ntrain_df =  train_df[train_df['date'] < '2017-06-22'] ## datecorrection\n\ntrain_date_df = pd.DataFrame({'date':pd.date_range(start='2017-06-01', end='2017-06-21')}) ## data time series for correcting training data date\ntrain_date_df['date'] = (train_date_df['date'].dt.date).astype(str)\n\n\n## creating a grouped id, data dict \nids_group = train_df.groupby(by ='id')['date'].unique()\n\n##### creating a zero usage user dataframe to concatinate with training dataset\n\nid_list = []\ndate_list = []\nonline_hours = []\nfor index, ids, dates in zip( range(len(ids_group)), ids_group.keys(), ids_group.values):\n    \n    date_ = dates.tolist()\n    \n    for date in train_date_df['date'].tolist():\n        if date_ == train_date_df['date'].tolist():\n            break\n        else:\n            if date not in date_:\n                #tempdf[['id','date','online_hours'] ]= ids,date,0\n                id_list.append(ids)\n                date_list.append(date)\n                online_hours.append(0)\n                date_.append(date)\n\nzero_df = pd.DataFrame({'id':id_list,'date':date_list, 'online_hours':online_hours})\n\ntrain_df_backup = train_df.copy() # lets store away train_df for any futrue purpose\n\ntrain_df = pd.concat([train_df,zero_df],sort = False)\n\ntrain_df = train_df.sort_values(by = ['id','date'])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:30:57.453094Z","iopub.execute_input":"2021-07-02T10:30:57.453484Z","iopub.status.idle":"2021-07-02T10:30:58.071776Z","shell.execute_reply.started":"2021-07-02T10:30:57.453451Z","shell.execute_reply":"2021-07-02T10:30:58.070741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\n\nprint('\\n'+'*'*50 + '\\n')\n\n\nprint('\\n'+'*'*10 + 'Sanity Check for Training Data' + '*'*10 +'\\n')\n\nprint('Any duplicate values in the training data'+ '\\n')\nprint(train_df.duplicated().any())\n\nprint('\\n' +'*'*50+ '\\n')\n\nprint('Any Null values in the training data'+ '\\n')\nprint(train_df.isnull().any())\n\nprint('\\n' +'*'*50 + '\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:31:04.860683Z","iopub.execute_input":"2021-07-02T10:31:04.861029Z","iopub.status.idle":"2021-07-02T10:31:04.889401Z","shell.execute_reply.started":"2021-07-02T10:31:04.860997Z","shell.execute_reply":"2021-07-02T10:31:04.888487Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (13,4))\nfig.set_facecolor('#f5f6f6')\n\ngs = fig.add_gridspec(20,20)\n\nax0 = fig.add_subplot(gs[:,0:9])\nax1 = fig.add_subplot(gs[:,11:])\n\n\naxes = [ax0,ax1]\nfor ax in axes:\n    ax.set_facecolor('#f5f6f6')\n    for loc in ['left','right', 'top','bottom']:\n        ax.spines[loc].set_visible(False)\n\nax0.tick_params(axis='y', which='major', \n                    labelsize=12, labelright =False,left =False,right=False,labelleft=True,rotation = 0)\n   \n    \n### plots \n\ntrain_date_order = train_df.date.sort_values().unique().tolist()\ntest_date_order = test_df.date.sort_values().unique().tolist()\n\nsns.distplot(x = train_df['online_hours'],\n             hist_kws = {'color':colors[0], 'linewidth': 2,'edgecolor':colors[-1], 'alpha':0.9 },\n             kde = True,kde_kws = {'color':'black', \"linewidth\": 1.5}, ax=ax0)\n\nsns.countplot(x = train_df['date'],ax=ax1,\n             color = colors[0], edgecolor  = colors[-1], linewidth =2)\n\n\nax0.set_ylabel('Count',**{'font':'serif','size':16,'weight':'bold'})\nax0.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\nax1.set_xlabel('',**{'font':'serif','size':16,'weight':'bold'})\nax1.set_ylabel('Count',**{'font':'serif','size':16,'weight':'bold'})\n\n\n#ax0.set_xticklabels(ax0.axes.get_xticklabels(), rotation = 90,**{'font':'serif','size':12})\nax1.set_xticklabels(ax1.axes.get_xticklabels(), rotation = 90,**{'font':'serif','size':12})\n#ax2.set_xticklabels(ax2.axes.get_xticklabels(), rotation = 90,**{'font':'serif','size':12})\n#### final writigs and titles\n\n\nfig.text(0.05,1.25,'Customers And App Usage: Corrected Trainind Date Distribution' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.05,1.08,'''Now Training dataset is clearen to good extent. Date wise distribution \nis perfectly unifrom for all the users and online hours now have the no-using  \nusers online hours (zero values). All set for futrure work.''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,-0.250,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\n\n\n### subplot titles\n#ax0.text(15,2650,'Train Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\n#ax1.text(5,2650,'Test Data',{'font':'serif', 'size':14, 'weight':'bold',},alpha = 0.85)\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:33:13.760389Z","iopub.execute_input":"2021-07-02T10:33:13.760811Z","iopub.status.idle":"2021-07-02T10:33:15.053733Z","shell.execute_reply.started":"2021-07-02T10:33:13.760776Z","shell.execute_reply":"2021-07-02T10:33:15.052916Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Lets concatinate the training and dataset now , by removing the values of online training from the test data and lets keep them in a list...</strong></p>\n","metadata":{}},{"cell_type":"code","source":"\n# preparing test dataset for concatination with train data\ntest_df = test_df.sort_values(by = ['id','date'])\ntest_df['date'] =  pd.to_datetime(test_df['date']).dt.date\ntest_hours = test_df['online_hours']\ntest_dropped_df  = test_df.drop(columns = ['online_hours'])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:34:03.283989Z","iopub.execute_input":"2021-07-02T10:34:03.284598Z","iopub.status.idle":"2021-07-02T10:34:03.319762Z","shell.execute_reply.started":"2021-07-02T10:34:03.284544Z","shell.execute_reply":"2021-07-02T10:34:03.318824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint('\\n' + '\\n')\nprint('\\n' +'*'*10+ ' Head of Test DataFrame with dropped online_hours' +'*'*10+'\\n')\n\nprint(test_dropped_df.head())\n\n    \ndf = pd.concat([train_df, test_dropped_df], sort=False)\n\nprint('\\n' + 'Shape of The Concatinated DateFrame: {}'.format(df.shape) +'\\n')\n\nprint('\\n' +'*'*10+ ' Head of The Concatinated DataFrame ' +'*'*10+'\\n')\nprint(df.head())\n\nprint('\\n' +'*'*10+ ' Tail of The Concatinated DataFrame ' +'*'*10+'\\n')\nprint(df.tail())\n\nprint('\\n' +'*'*5+ ' After filling NaN values with Zeros in Concatinated DataFrame ' +'*'*5+'\\n')\n#df.fillna(0,inplace = True)\nprint(df.tail())\nprint('\\n' +'*'*50+ '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:34:15.096463Z","iopub.execute_input":"2021-07-02T10:34:15.096833Z","iopub.status.idle":"2021-07-02T10:34:15.124821Z","shell.execute_reply.started":"2021-07-02T10:34:15.096801Z","shell.execute_reply":"2021-07-02T10:34:15.123759Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '2.2'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" >2.2 Data visualization of Features with AutoViz - Simple, Yet Powerful Library</h2>","metadata":{}},{"cell_type":"code","source":"# installing autoviz for simple insights\n!pip install autoviz #insatlling autoviz\n!pip install xlrd\nfrom autoviz.AutoViz_Class import AutoViz_Class","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-02T10:36:42.623835Z","iopub.execute_input":"2021-07-02T10:36:42.624274Z","iopub.status.idle":"2021-07-02T10:36:59.557634Z","shell.execute_reply.started":"2021-07-02T10:36:42.624234Z","shell.execute_reply":"2021-07-02T10:36:59.556422Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 1. Customer Data Visualization with AutoViz</h2>","metadata":{}},{"cell_type":"code","source":"autoviz = AutoViz_Class().AutoViz('../input/mobile-usage-time-prediction/customers.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:38:18.463296Z","iopub.execute_input":"2021-07-02T10:38:18.463654Z","iopub.status.idle":"2021-07-02T10:38:22.362842Z","shell.execute_reply.started":"2021-07-02T10:38:18.46362Z","shell.execute_reply":"2021-07-02T10:38:22.361755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice!!! I got to know about all the basic information about my customers.","metadata":{}},{"cell_type":"markdown","source":"<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 2. Training Data Visualization with AutoViz</h2>","metadata":{}},{"cell_type":"code","source":"train_autoviz = AutoViz_Class()\n\nfilename = \"\"\nsep = \",\"\ndft = train_autoviz.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"\",\n    dfte= train_df,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=30,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:39:17.416118Z","iopub.execute_input":"2021-07-02T10:39:17.416496Z","iopub.status.idle":"2021-07-02T10:39:20.093028Z","shell.execute_reply.started":"2021-07-02T10:39:17.416465Z","shell.execute_reply":"2021-07-02T10:39:20.091907Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 3. Testing Data Visualization with AutoViz</h2>","metadata":{}},{"cell_type":"code","source":"train_autoviz = AutoViz_Class()\n\nfilename = \"\"\nsep = \",\"\ndft = train_autoviz.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"\",\n    dfte= test_df,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=30,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:40:22.457914Z","iopub.execute_input":"2021-07-02T10:40:22.458314Z","iopub.status.idle":"2021-07-02T10:40:25.710121Z","shell.execute_reply.started":"2021-07-02T10:40:22.458279Z","shell.execute_reply":"2021-07-02T10:40:25.708988Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 3. Modeling and Predictions</h2>\n<br>","metadata":{}},{"cell_type":"markdown","source":"I have done datacleaning and visualiztion till now, moving on to the modeling...\n\n<br>\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Modeling Strategy:</strong></p>\nThis is a quite tricky problem to solve, data seems to be very huge when handling the raw training data. Even ran into Memory Error, as data points are 5 core in size, and Thanks to kaggle 16GB ram server, I was able to do the preprocessing of data. Now data size reduced drastically with calculation of online hours for training data. \n\n<br>\n<br>\nAs there are <strong>multiple timeseries</strong> for each cusomter,initially I was struck with questions.\n\n<ol><li> What model Should I use? given data is timeseries data but 2500 timeseries</li>\n    <li> Should I train models with simple ARIMA or LSTM models for each of the timeseries and concat results or should I use a complex model to catpure all the details.</li>\n    <li> Training data is very very less to train a deep learning models or satistical models as only 21 days data is available for training.</li>\n</ol>\n\n<br>\nI have done experiments with data prior to forming my strategy for model building.... few of my attempt are failed to reach good result and insights are as follows:\n<ol>\n    <li>I have attempted to solve this with arima modeling initially, and due to limited data for individual timeseries data, it was unable to capture any kind of patterns.</li>\n    <li>later moved to simple LSTM model and same issue was faced with that as they are data hungrey models</li>\n    <li>My timeseries approch is not good for this problem as very limited data for timeseries. Later realized, one way to solve this model is problem is by formulating a single regression model, by taking the advantage of the similarities in the customers.</li>\n    <li> While attempting, this basic regression approch few blenders were commited, such as using startifiedkfold, and train_test_split method for spliting data. Resluts are very very bad, as I messed up with time data. Later realized my mistakes I corrected them... </li>\n    <li>For this basic model a baseline score was set by assuming the predictions for t(n) date are t(n-1) ( i.e,today predictions are yesterday values). And with this model sensitivity for rmse was measured and it gave good results so, went on..</li>\n    <li> With lag implementation this simple regression models are made to mimimic the Time series models, and this gave very effective results...</li>\n    \n   <li> Upon understanding this drop in error, done feature engineering and worked for final solution...</li>\n\n</ol>\n\n ","metadata":{}},{"cell_type":"markdown","source":"<a id = '3.1'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 1. Approch 1: Learning From Mistakes - Failed Attempt 1</h2>","metadata":{}},{"cell_type":"code","source":"### basic feature engineering and modeling withignored time dependency\n\ntemp_df = pd.merge(left = customer_df, right = train_df, on = 'id', how = 'outer')\n\ntemp_df.dropna(inplace = True)\ntemp_df['gender'] = temp_df['gender'].replace({'MALE':1, 'FEMALE':0})\n\ntemp_df['date'] = pd.to_datetime(temp_df['date'])\n\n## data and time related basic features\ntemp_df['day_name'] = temp_df['date'].dt.day_name()\ntemp_df['day'] = temp_df['date'].dt.day\ntemp_df['month'] = temp_df['date'].dt.month\ntemp_df['month_name'] = temp_df['date'].dt.month_name()\ntemp_df['year'] = temp_df['date'].dt.year\n\n\n\nweek_names = {'Sunday':0,'Monday':'1','Tuesday':2,'Wednesday':3, 'Thursday':4,'Friday':5,'Saturday':6}\nmonth_names = {'January':0, 'February':1,'March':2,'April':3,'May':4,'June':5,'July':6,\n                'August':7, 'September':8,'October':9,'November':10,'December':11}\n\ntemp_df['day_name'] = temp_df['day_name'].map(week_names)\ntemp_df['month_name'] = temp_df['month_name'].map(month_names)\n\n\n### solving model like a typical regression problem without any considerations and feature engineering\nX = temp_df.drop(columns = ['date','id','online_hours'])\ny = temp_df['online_hours']\n\n\n# train, testing data spliting\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 2021) #shuffle and randomized selection\n\n# scaling\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n\n\n## appending all the regressors\n\nregressors = []\n\nregressors.append(LinearRegression())\nregressors.append(Lasso(alpha = 0.0005,max_iter = 1000))\nregressors.append(RandomForestRegressor(n_estimators = 1000,max_depth=15))\nregressors.append(AdaBoostRegressor())\nregressors.append(GradientBoostingRegressor())\nregressors.append(XGBRegressor(n_estimators = 1000,importance_type = 'gain'))\nregressors.append(LGBMRegressor(n_estimators = 1000, objective ='regression',\n                                importance_type = 'gain'))\n\n\n#### fitting and modeling \n\nimport time\n\nprint('Working on base models and fitting begains.....')\n\nkfolds = KFold(n_splits = 5, shuffle = False)\n\nbaseline_models= clone(regressors)\nalgo = ['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']\n\ncv_results = list()\nfor idx, reg in enumerate(baseline_models):\n    t = time.time()\n    print('Fitting of {} Model'.format(algo[idx]))\n    print('Parameters of the model are: {}'. format(reg.get_params()))\n    \n    cv_results.append(cross_val_score(estimator=reg,X =X_train,y= y_train, cv = kfolds, n_jobs = -1, scoring= 'r2'))\n\n    print('time elapsed is : {} sec'.format(round((time.time() - t),2)))\n    print('\\n\\n\\n*****************************\\n\\n\\n')\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \nbaseline_cv_res = pd.DataFrame({'CrossValMeans':cv_means,\"CrossValerrors\":\n                      cv_std,\"Algorithm\":['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']})","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:55:04.297125Z","iopub.execute_input":"2021-07-02T10:55:04.297507Z","iopub.status.idle":"2021-07-02T11:11:05.33317Z","shell.execute_reply.started":"2021-07-02T10:55:04.297474Z","shell.execute_reply":"2021-07-02T11:11:05.331901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,5), dpi = 100)\nfig.patch.set_facecolor('#f5f6f6')\nax.axes.set_facecolor('#f5f6f6')\n\nax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n\nax.barh(y = baseline_cv_res['Algorithm'], width = baseline_cv_res['CrossValMeans'] , height = 0.5, color = colors[1], edgecolor = colors[0], linewidth =2)\nax.set_yticklabels(baseline_cv_res['Algorithm'], {'font':'serif','size':16,'weight':'bold'}, )\n\n#ax.errorbar(x = , b, xerr=c, fmt=\"o\", color=\"r\")\n\nfor pa in ax.patches: \n        ax.text((pa.get_width())+0.01, pa.get_y()+0.05,'{}'.format(round(pa.get_width(),3)), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n        \nax.axes.get_xaxis().set_visible(False)\n\nfor loc in ['left', 'right', 'top','bottom']:\n    ax.spines[loc].set_visible(False)\n    \n\nfig.text(-0.15,1.1,'Customers And App Usage: Failed Attempt R2 Score Comparision' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(-0.15,0.99,'''This Graph shows the crossvalidation mean r2 score for various models with stratifiedkfold, crossvalidation.\nWe can clearly see out model is utterly failed to make any predictions from the given data...''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,0,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-02T11:12:26.1779Z","iopub.execute_input":"2021-07-02T11:12:26.178358Z","iopub.status.idle":"2021-07-02T11:12:26.403652Z","shell.execute_reply.started":"2021-07-02T11:12:26.178319Z","shell.execute_reply":"2021-07-02T11:12:26.402628Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.2'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 2. Approch 2: Failed Attempt2</h2>","metadata":{}},{"cell_type":"markdown","source":"This approch is Same as Approch 1 with few changes with No Time Related info Cross feeding.\nIn the above method few blunders are commited with ignoring the time dependencey... I have realized that and Implemented same thing again with same strategy by spliting train and test data with date selection and no change in values... \n\n1. Removed Stratifiedkfold impelmentation\n2. splitted train and test data with date\n3. Removed Crossvalidation impelmentaion\n\nlets see how things will go...\n","metadata":{}},{"cell_type":"code","source":"# train and test data spliting\ntrain = temp_df[temp_df['day'] <= 18]\nval = temp_df[temp_df['day'] > 18]\n    \nX_train,X_test = train.drop(columns = ['online_hours','date']),val.drop(columns = ['online_hours','date'])\ny_train,y_test = train['online_hours'].values, val['online_hours'].values\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n## appending all the regressors\n\nregressors = []\n\nregressors.append(LinearRegression())\nregressors.append(Lasso(alpha = 0.0005,max_iter = 1000))\nregressors.append(RandomForestRegressor(n_estimators = 1000,max_depth=15))\nregressors.append(AdaBoostRegressor())\nregressors.append(GradientBoostingRegressor())\nregressors.append(XGBRegressor(n_estimators = 1000,importance_type = 'gain'))\nregressors.append(LGBMRegressor(n_estimators = 1000, objective ='regression',\n                                importance_type = 'gain'))\n\n\nimport time\n\nprint('Working on base models and fitting begains.....')\n\n\n\nbaseline_models= clone(regressors)\nalgo = ['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']\nr2_score_list = []\nfor idx, reg in enumerate(baseline_models):\n    t = time.time()\n    print('Fitting of {} Model'.format(algo[idx]))\n    print('Parameters of the model are: {}'. format(reg.get_params()))\n    \n    model = reg\n    model.fit(X_train,y_train)\n    preds = model.predict(X_test)\n    r2 = r2_score(y_test,preds)\n\n    print('time elapsed is : {} sec'.format(round((time.time() - t),2)))\n    print('\\n\\n\\n*****************************\\n\\n\\n')\n    r2_score_list.append(r2)\n\nbaseline = pd.DataFrame({'r2_score':r2_score_list,\n                      \"Algorithm\":['LinearRegression','Lasso', 'RandomForestRegressor' ,'AdaBoostRegressor',\n                                          'GradientBoostingRegressor', 'XGBRegressor','LGBMRegressor']})","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:12:54.503633Z","iopub.execute_input":"2021-07-02T11:12:54.503968Z","iopub.status.idle":"2021-07-02T11:14:21.40723Z","shell.execute_reply.started":"2021-07-02T11:12:54.503939Z","shell.execute_reply":"2021-07-02T11:14:21.406053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,5), dpi = 100)\nfig.patch.set_facecolor('#f5f6f6')\nax.axes.set_facecolor('#f5f6f6')\n\nax.tick_params(axis='both', \n                   labelsize = 12, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.001)\n\nax.barh(y = baseline['Algorithm'], width = baseline['r2_score'] , height = 0.5, color = colors[1], edgecolor = colors[0], linewidth =2)\nax.set_yticklabels(baseline['Algorithm'], {'font':'serif','size':16,'weight':'bold'}, )\n\n#ax.errorbar(x = , b, xerr=c, fmt=\"o\", color=\"r\")\n\nfor pa in ax.patches: \n        ax.text((pa.get_width())+0.01, pa.get_y()+0.05,'{}'.format(round(pa.get_width(),3)), **{'font':'serif', 'size':12, 'weight':'bold'}, alpha = 1)\n        \nax.axes.get_xaxis().set_visible(False)\n\nfor loc in ['left', 'right', 'top','bottom']:\n    ax.spines[loc].set_visible(False)\n    \n\nfig.text(-0.15,1.08,'Customers And App Usage: Approch 2 R2 Score Comparision' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(-0.15,0.99,'''Something is interesting happened here, though its a failed model, there is slight improvement in the r2\nscore, still this is not a good method, as r2 score supposed to be near 1 for better model. lets try another approch..''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,0,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-02T11:15:40.340628Z","iopub.execute_input":"2021-07-02T11:15:40.341151Z","iopub.status.idle":"2021-07-02T11:15:40.572057Z","shell.execute_reply.started":"2021-07-02T11:15:40.341118Z","shell.execute_reply":"2021-07-02T11:15:40.570995Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.3'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 3. Approch 3: Going In Right Direction</h2>","metadata":{}},{"cell_type":"markdown","source":"Clearly, with the both above approches I understood that I need a better model with a relatively good baseline score. As I see in approch 2 something happened but due to lack of baseline score, I am not in a position to know the direction of shift with new approch.  \n\nWith this kind of timeseries data, having a lag of past date values surely helps. But Before that I need a basline score to compare with.\n\n\nFrom my kaggle experience, I have learned that one can safely assume for todays true values as predictions of tomorrow.\n\ni.e, prediction of t(n) th day as t(n-1) day true values... <strong>Assumption made for a baseline score</strong>\n<br>\n<br>\n\n<strong>For Baseline Score Only Primary features are considering </strong> based on the score I will improve upon this model..\n<br>\n<br>\n\n1. <strong> last_date_hours:</strong> This is simply the number of hours that a customer had in the previous date (t(n-1))\n2. <strong> last_date_hours:</strong> This is the difference between the number of online hours in the previous date and the date before it (t(n-1) - t(n-2))\n","metadata":{}},{"cell_type":"code","source":"## preparing the training data\ntrain_df_copy = train_df.copy()\n\ntrain_df_copy['date'] = pd.to_datetime(train_df_copy['date'])\n\ntrain_df_copy['day']= train_df_copy['date'].dt.day\ntrain_df_copy['date']= train_df_copy['date'].dt.date\n\ntrain_df_copy['last_day_onlinehours'] = train_df_copy.groupby(['id'])['online_hours'].shift(1) # one row shift to get online_hours\ntrain_df_copy['last_day_diff'] = train_df_copy.groupby(['id'])['last_day_onlinehours'].diff() # last day and previous day online hours\n#train_df_copy.fillna(0,inplace = True)\n\n\n\nprint('\\n' +'*'*5+ ' Lag added Training Data Copy Head ' +'*'*5+'\\n')\n\nprint(train_df_copy.head())\nprint('\\n' +'*'*50+ '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:23:05.360552Z","iopub.execute_input":"2021-07-02T11:23:05.360966Z","iopub.status.idle":"2021-07-02T11:23:05.967723Z","shell.execute_reply.started":"2021-07-02T11:23:05.360933Z","shell.execute_reply":"2021-07-02T11:23:05.966184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Lets write helper function for rmse metric:</strong></p>","metadata":{}},{"cell_type":"code","source":"## root mean squre error\ndef rmse(ytrue, ypred):\n    return np.sqrt(mean_squared_error(ytrue,ypred))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:23:24.322681Z","iopub.execute_input":"2021-07-02T11:23:24.323058Z","iopub.status.idle":"2021-07-02T11:23:24.327671Z","shell.execute_reply.started":"2021-07-02T11:23:24.323008Z","shell.execute_reply":"2021-07-02T11:23:24.326558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have read in kaggle discussion, about this idea of validating on fly mode, which seems better to validate model when data is very less. \n<strong>Idea is that train model upto (n-m) datapoints and validate on last m datapoints, and there by take the mean of metric as a evaluation metric. </strong> Lets implement this idea here to find the <strong>Baseline Score.</strong>\n<br>\n<br>\nas we have total 21 days of training data, I choose to apply this sliding window validataion from 15th day datapoints. \n<br>\nI am going to assume yesterdays online hours per user as predictions for today and calculate the root mean square error, and with sliding window validation, I am going to move from 15th day plus one for each cylce to validate all the day after.","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Baseline Score:</strong></p>","metadata":{}},{"cell_type":"code","source":"baseline_errors = []\nfor day in range(15,21):\n    train = train_df_copy[train_df_copy['day'] < day]\n    valid = train_df_copy[train_df_copy['day'] == day]\n    \n    preds = valid['last_day_onlinehours'].values\n    \n    score = rmse(valid['online_hours'].values,preds)\n    print('Trained upto Day %d and RMSE for next %d th day Predictions is %.5f' % (day, 21- day,score))\n    baseline_errors.append(score)\n\nprint('\\n' +'*'*20  +'Final Mean RMSE Score'+'*'*20 + '\\n')\nprint('                  Mean RMSE Score:    %.5f' % np.mean(baseline_errors))\nprint('\\n' +'*'*61 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:23:33.948398Z","iopub.execute_input":"2021-07-02T11:23:33.949001Z","iopub.status.idle":"2021-07-02T11:23:33.985436Z","shell.execute_reply.started":"2021-07-02T11:23:33.94895Z","shell.execute_reply":"2021-07-02T11:23:33.98434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<strong>Baseline Score: 3.448883</strong> which need to be crossed by our model..\n\nNow I am going to use the online hours lag...\n\n<strong>Lag is a past days and beyond values which we feed into model to predict current timestep, This is similar to last_day_onlinehours and last_day_diff</strong>\n\nAs I already have lag1 data, lets fit this on <strong>LGBMRegressor</strong> as this showed some good response in Approch 2 compared to Approch 1.\n\n<strong>I will follow the same method an see whether my RMSE Value reduces or not</strong>\n\n\nI am checking model response to my strategy so, just feeding data as it is without any feature engineering and scaling... if things work out well I will build upon this approch...","metadata":{}},{"cell_type":"markdown","source":"<a id = '3.3.1'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 3.1 Approch3 Attempt 1: Fitting lag0 with Sliding Window Predictions</h2>","metadata":{}},{"cell_type":"code","source":"lag0_errors = []\nfor day in range(15,21):\n    \n    train = train_df_copy[train_df_copy['day'] < day]\n    valid = train_df_copy[train_df_copy['day'] == day]\n    \n    xtrain,xtest = train.drop(columns = ['online_hours','date']),valid.drop(columns = ['online_hours','date'])\n    ytrain,ytest = train['online_hours'].values, valid['online_hours'].values\n    \n    model = LGBMRegressor(n_estimators = 1000)\n    model.fit(xtrain,ytrain)\n    preds = model.predict(xtest)\n    \n    score = rmse(valid['online_hours'].values,preds)\n    print('Trained upto Day %d and RMSE for %d day Prediction is %.5f' % (day, day+1,score))\n    lag0_errors.append(score)\n\nprint('\\n' +'*'*20  +'Final Mean RMSE Score'+'*'*20 + '\\n')\nprint('                  Mean RMSE Score:    %.5f' % np.mean(lag0_errors))\nprint('\\n' +'*'*61 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:25:19.010519Z","iopub.execute_input":"2021-07-02T11:25:19.010944Z","iopub.status.idle":"2021-07-02T11:25:27.8574Z","shell.execute_reply.started":"2021-07-02T11:25:19.010904Z","shell.execute_reply":"2021-07-02T11:25:27.856302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Strategy is work, my rmse value is reduced by 10% and that to with out any feature engineering and scaling.... \n<br>\nlets add few more lags and see the effect...\n\n\n<strong>Adding lag2:<strong>similar to first lag, for lag one we need to shift online_hours column to 2 rows down, and take the row difference to get lag2","metadata":{}},{"cell_type":"markdown","source":"<a id = '3.3.2'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" >3.2 Approch3 Attempt 2: Fitting lag1 with Sliding Window Predictions</h2>","metadata":{}},{"cell_type":"code","source":"train_df_copy['last-1_day_onlinehours'] = train_df_copy.groupby(by = ['id'])['online_hours'].shift(2)\ntrain_df_copy['last-1_day_hours_diff'] = train_df_copy.groupby(by = ['id'])['last-1_day_onlinehours'].diff()\n#train_df_copy = train_df_copy.fillna(0)\n\n\nprint('\\n' +'*'*5+ ' Lag1 added Training Data Copy Head ' +'*'*5+'\\n')\n\nprint(train_df_copy.head())\nprint('\\n' +'*'*50+ '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:26:39.036112Z","iopub.execute_input":"2021-07-02T11:26:39.03673Z","iopub.status.idle":"2021-07-02T11:26:39.628538Z","shell.execute_reply.started":"2021-07-02T11:26:39.036685Z","shell.execute_reply":"2021-07-02T11:26:39.627632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets repeat the method and see how rmse is going...","metadata":{}},{"cell_type":"code","source":"lag1_errors = []\nfor day in range(15,21):\n    \n    train = train_df_copy[train_df_copy['day'] < day]\n    valid = train_df_copy[train_df_copy['day'] == day]\n    \n    xtrain,xtest = train.drop(columns = ['online_hours','date']),valid.drop(columns = ['online_hours','date'])\n    ytrain,ytest = train['online_hours'].values, valid['online_hours'].values\n    \n    model = LGBMRegressor(n_estimators = 1000)\n    model.fit(xtrain,ytrain)\n    preds = model.predict(xtest)\n    \n    score = rmse(valid['online_hours'].values,preds)\n    print('Trained upto Day %d and RMSE for %d day Prediction is %.5f' % (day, day+1,score))\n    lag1_errors.append(score)\n\nprint('\\n' +'*'*20  +'Final Mean RMSE Score'+'*'*20 + '\\n')\nprint('                  Mean RMSE Score:    %.5f' % np.mean(lag1_errors))\nprint('\\n' +'*'*61 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:20:31.749775Z","iopub.execute_input":"2021-05-20T09:20:31.750117Z","iopub.status.idle":"2021-05-20T09:20:42.640837Z","shell.execute_reply.started":"2021-05-20T09:20:31.750088Z","shell.execute_reply":"2021-05-20T09:20:42.640079Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <strong>Decided To go with Approch 3...<strong>\n<a id = '3.4'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 4. Feature Engineering - Rolling window means, Lags, and Datetime features</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Helper functions for datetime features, rolling window, and lag features</strong></p>","metadata":{}},{"cell_type":"code","source":"def datetime_features(data):\n    data['date'] = pd.to_datetime(data['date'])\n    data['month'] = data.date.dt.month\n    data['day_of_month'] = data.date.dt.day\n    data['day_of_year'] = data.date.dt.dayofyear\n    data['week_of_year'] = data.date.dt.weekofyear\n    data['day_of_week'] = data.date.dt.dayofweek + 1\n    data['year'] = data.date.dt.year\n    data[\"is_wknd\"] = data.date.dt.weekday // 4\n    data[\"quarter\"] = data.date.dt.quarter\n    data['is_month_start'] = data.date.dt.is_month_start.astype(int)\n    data['is_month_end'] = data.date.dt.is_month_end.astype(int)\n    data['is_quarter_start'] = data.date.dt.is_quarter_start.astype(int)\n    data['is_quarter_end'] = data.date.dt.is_quarter_end.astype(int)\n    data['is_year_start'] = data.date.dt.is_year_start.astype(int)\n    data['is_year_end'] = data.date.dt.is_year_end.astype(int)\n    \n    week_names = {'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3, 'Thursday':4,'Friday':5,'Saturday':6}\n    month_names = {'January':0, 'February':1,'March':2,'April':3,'May':4,'June':5,'July':6,\n                'August':7, 'September':8,'October':9,'November':10,'December':11}\n    \n    return data\n\n\ndef rolling_window_mean(data):\n    # 7 day rolling window mean\n    for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,61,17,18,19,20,21]:\n        data[\"onlinehours_roll_mean_\"+str(i)] = data.groupby([\"id\"])['online_hours'].rolling(i).mean().shift(1).values\n    #data.fillna(0,inplace = True)\n    return data\n\ndef lag_features(data,lags = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,61,17,18,19,20,21]):\n    data.sort_values(by=['id','date'], axis=0, inplace=True)\n    data = data.copy()\n    for lag in lags:\n        data['last_day_onlinehours'+str(lag)] = data.groupby('id')['online_hours'].transform(lambda x: x.shift(lag))\n        data['last_day_hours_diff' +str(lag)] = data.groupby('id')['last_day_onlinehours'+str(lag)].transform(lambda x: x.diff())\n        #dataframe.fillna(0,inplace = True)\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:29:23.491217Z","iopub.execute_input":"2021-07-02T11:29:23.491818Z","iopub.status.idle":"2021-07-02T11:29:23.509902Z","shell.execute_reply.started":"2021-07-02T11:29:23.49178Z","shell.execute_reply":"2021-07-02T11:29:23.508671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_traindata(df):\n    data = df.copy()\n    data = datetime_features(data)\n    data = rolling_window_mean(data)\n    data = lag_features(data)\n    return data\n\ndf_copy  = preprocessing_traindata(df)\n\ntrain_final = df_copy[df_copy['day_of_month'] < 22]\ntest_final = df_copy[df_copy['day_of_month'] >= 22]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:29:39.279698Z","iopub.execute_input":"2021-07-02T11:29:39.280125Z","iopub.status.idle":"2021-07-02T11:30:10.496639Z","shell.execute_reply.started":"2021-07-02T11:29:39.280085Z","shell.execute_reply":"2021-07-02T11:30:10.49575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n'+'*'*15 + 'Shapes of Final Data ' + '*'*15+'\\n' )\n\nprint('Shape of the Train Data: {}'.format(train_final.shape))\nprint('Shape of the Test Data: {}'.format(test_final.shape))\n\n\nprint('\\n'+'*'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:32:23.772291Z","iopub.execute_input":"2021-07-02T11:32:23.772705Z","iopub.status.idle":"2021-07-02T11:32:23.779749Z","shell.execute_reply.started":"2021-07-02T11:32:23.772669Z","shell.execute_reply":"2021-07-02T11:32:23.77864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.5'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 5. Baseline Model From Approch 3 - After feature engineering</h2>\n\n# <strong>Still Actual Test in on hold... .<strong>","metadata":{}},{"cell_type":"code","source":"errors = []\nfor day in range(15,21):\n    \n    train = train_final[train_final['day_of_month'] < day]\n    valid = train_final[train_final['day_of_month'] == day]\n    \n    xtrain,xtest = train.drop(columns = ['online_hours','date']),valid.drop(columns = ['online_hours','date'])\n    ytrain,ytest = train['online_hours'].values, valid['online_hours'].values\n    \n    model = LGBMRegressor(n_estimators = 1000)\n    model.fit(xtrain,ytrain)\n    preds = model.predict(xtest)\n    \n    score = rmse(valid['online_hours'].values,preds)\n    print('Trained upto Day %d and RMSE for %d day Prediction is %.5f' % (day, day+1,score))\n    errors.append(score)\n\nprint('\\n' +'*'*20  +'Final Mean RMSE Score'+'*'*20 + '\\n')\nprint('                  Mean RMSE Score:    %.5f' % np.mean(errors))\nprint('\\n' +'*'*61 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:34:28.031208Z","iopub.execute_input":"2021-07-02T11:34:28.031628Z","iopub.status.idle":"2021-07-02T11:35:08.281791Z","shell.execute_reply.started":"2021-07-02T11:34:28.031593Z","shell.execute_reply":"2021-07-02T11:35:08.280599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <strong>Now testing model on actual Test Data....<strong>","metadata":{}},{"cell_type":"code","source":"\ntrain = df_copy[df_copy['day_of_month'] < 22]\ntest = df_copy[df_copy['day_of_month'] >= 22]\n\n\nxtrain,xtest = train.drop(columns = ['id','date','online_hours']),test.drop(columns = ['id','date','online_hours'])\nytrain,ytest = train['online_hours'].values, test['online_hours'].values\n    \nmodel = LGBMRegressor(n_estimators = 1000)\nmodel.fit(xtrain,ytrain)\npreds = model.predict(xtest)\n    \nscore = rmse(test_df.online_hours.tolist(),preds)\n\nprint('\\n' +'*'*20  +'Baseline Model RMSE Score'+'*'*20 + '\\n')\nprint('                 RMSE Score:    %.5f' % score)\nprint('\\n' +'*'*61 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:36:34.544958Z","iopub.execute_input":"2021-07-02T11:36:34.545424Z","iopub.status.idle":"2021-07-02T11:36:44.752557Z","shell.execute_reply.started":"2021-07-02T11:36:34.545388Z","shell.execute_reply":"2021-07-02T11:36:44.751391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.6'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 6. Feature Selection with PermutationImportance - eli5 implementation</h2>","metadata":{}},{"cell_type":"code","source":"train = train_final[train_final['day_of_month'] < 16]\nvalid = train_final[train_final['day_of_month'] >=16]\n    \nxtrain,xvalid = train.drop(columns = ['online_hours','date']),valid.drop(columns = ['online_hours','date'])\nytrain,yvalid = train['online_hours'].values, valid['online_hours'].values\n\nmodel = XGBRegressor().fit(xtrain,ytrain)\n\nperm = PermutationImportance(model,scoring='r2' ).fit(xvalid,yvalid)\neli5_feature_importance = (pd.DataFrame({'Features':xtrain.columns.tolist(),'Importance':perm.feature_importances_})\n                           .sort_values(by = 'Importance'))\n\nprint('Feature impact weights and importance for the given model:')\neli5.show_weights(perm, feature_names = xtrain.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:39:26.323527Z","iopub.execute_input":"2021-07-02T11:39:26.324029Z","iopub.status.idle":"2021-07-02T11:39:42.346608Z","shell.execute_reply.started":"2021-07-02T11:39:26.32399Z","shell.execute_reply":"2021-07-02T11:39:42.345176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_imp(data, ax = None):\n    if ax == None:\n        fig,ax = plt.subplots(figsize = (10,15), dpi = 100)\n        fig.patch.set_facecolor('#f5f6f6')\n        ax.set_facecolor('#f5f6f6')\n        \n        ax.tick_params(axis='both', \n                   labelsize = 10, which = 'major',\n                   direction = 'out',pad = 2,\n                   length = 0.0001)\n    \n    #data['shades'] = data['Features'].apply(lambda x: '#FB5B68' if x == 'Random' else colors[1])\n    \n    ax.barh(y = data.Features, width = data.Importance,\n            height = 0.5, color = colors[1],  edgecolor = colors[0], linewidth = 1.5)\n    \n    for loc in ['left','right','bottom','top']:\n        ax.spines[loc].set_visible(False)\n    ax.axes.get_xaxis().set_visible(False)\n    #ax.axes.get_yaxis().set_visible(False)\n    \n    ax.set_yticklabels(data.Features, {'font':'serif','size':10,'weight':'bold'})\n\n\nplot_imp(eli5_feature_importance)\nplt.gcf().text(-0.05,0.89,'Customers And App Usage: Feature Importance with Premutated Features',{'font': 'serif', 'size':18,'weight':'bold',})\n\nplt.gcf().text(0.7,0.1,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':8, 'weight':'bold'},alpha = 0.8)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-02T11:40:25.091878Z","iopub.execute_input":"2021-07-02T11:40:25.092691Z","iopub.status.idle":"2021-07-02T11:40:26.76107Z","shell.execute_reply.started":"2021-07-02T11:40:25.092636Z","shell.execute_reply":"2021-07-02T11:40:26.760318Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## feature selection based on permutation importance\nfeature_import = (eli5_feature_importance.sort_values(by = 'Importance', ascending = False)\n                  .reset_index(drop = True).Features.tolist()[0:40]) ","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:40:54.880803Z","iopub.execute_input":"2021-07-02T11:40:54.881403Z","iopub.status.idle":"2021-07-02T11:40:54.886856Z","shell.execute_reply.started":"2021-07-02T11:40:54.881366Z","shell.execute_reply":"2021-07-02T11:40:54.88607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '3.7'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 7. Final Baseline Model with Feature Selection</h2>","metadata":{}},{"cell_type":"code","source":"\ntrain = df_copy[df_copy['day_of_month'] < 22]\ntest = df_copy[df_copy['day_of_month'] >= 22]\n\n\nxtrain,xtest = train.drop(columns = ['id','date','online_hours']),test.drop(columns = ['id','date','online_hours'])\nxtrain = xtrain[feature_import]\nxtest = xtest[feature_import]\nytrain,ytest = train['online_hours'].values, test['online_hours'].values\n    \nmodel = LGBMRegressor(n_estimators = 1000)\nmodel.fit(xtrain,ytrain)\npreds = model.predict(xtest)\n    \nscore = rmse(test_df.online_hours.tolist(),preds)\n\n    \nprint('\\n' +'*'*20  +'Baseline Model RMSE Score with feature selection'+'*'*20 + '\\n')\nprint('                 RMSE Score:    %.5f' % score)\nprint('\\n' +'*'*81 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:41:38.832631Z","iopub.execute_input":"2021-07-02T11:41:38.833256Z","iopub.status.idle":"2021-07-02T11:41:45.746758Z","shell.execute_reply.started":"2021-07-02T11:41:38.833218Z","shell.execute_reply":"2021-07-02T11:41:45.745963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a slight improvement here... with feature selection (selected top 40 out of 80 features)","metadata":{}},{"cell_type":"markdown","source":"<a id = '3.8'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 8. Hyperparameter tunning with optuna</h2>","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:50:34.838002Z","iopub.execute_input":"2021-05-20T09:50:34.838364Z","iopub.status.idle":"2021-05-20T09:50:34.842349Z","shell.execute_reply.started":"2021-05-20T09:50:34.838334Z","shell.execute_reply":"2021-05-20T09:50:34.841613Z"}}},{"cell_type":"code","source":"### this takes hell lot of time so changed grid and number of trial to min\n## feel free to fork and play wiht parameters\n\nimport optuna\n\ndef objective(trial):\n    \n    xtrain,xvalid = train.drop(columns = ['online_hours','date']),valid.drop(columns = ['online_hours','date'])\n    xtrain,xvalid = xtrain[feature_import], xvalid[feature_import]\n    \n    ytrain,yvalid = train['online_hours'].values, valid['online_hours'].values  \n    \n    param = {\n        'objective': 'regression',\n        'metric': 'root_mean_squared_error',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 1.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 1.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 15),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0, 0.1 ),\n        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n        #'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n        #'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n        #'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        #'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n       # 'device':\"gpu\",\n       # 'gpu_use_dp':True\n        \n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(xtrain,ytrain,eval_set=[(xvalid,yvalid)],early_stopping_rounds=10,verbose=False)\n    \n    preds = model.predict(xvalid)\n    \n    rmse_error = rmse(yvalid, preds)\n    \n    return rmse_error\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:51:50.899299Z","iopub.execute_input":"2021-07-02T11:51:50.899792Z","iopub.status.idle":"2021-07-02T11:52:13.340486Z","shell.execute_reply.started":"2021-07-02T11:51:50.899748Z","shell.execute_reply":"2021-07-02T11:52:13.33944Z"},"_kg_hide-input":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Optimization History - RMSE and Trail</strong></p>","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:54:25.215446Z","iopub.execute_input":"2021-07-02T11:54:25.215989Z","iopub.status.idle":"2021-07-02T11:54:25.406951Z","shell.execute_reply.started":"2021-07-02T11:54:25.215934Z","shell.execute_reply":"2021-07-02T11:54:25.404994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Ploting Parameters Importances for Tunned Modle</strong></p>","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:54:57.960609Z","iopub.execute_input":"2021-07-02T11:54:57.961073Z","iopub.status.idle":"2021-07-02T11:54:58.37713Z","shell.execute_reply.started":"2021-07-02T11:54:57.961023Z","shell.execute_reply":"2021-07-02T11:54:58.376006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Final Tunned Model and Parameters</strong></p>","metadata":{}},{"cell_type":"code","source":"best_params = study.best_trial.params\nprint(best_params)\nopt_model = LGBMRegressor(**best_params,objective='regression', \n                          metric='root_mean_squared_error')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:55:38.413682Z","iopub.execute_input":"2021-07-02T11:55:38.41421Z","iopub.status.idle":"2021-07-02T11:55:38.420891Z","shell.execute_reply.started":"2021-07-02T11:55:38.414145Z","shell.execute_reply":"2021-07-02T11:55:38.419888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain = df_copy[df_copy['day_of_month'] < 22]\ntest = df_copy[df_copy['day_of_month'] >= 22]\n\n\nxtrain,xtest = train.drop(columns = ['id','date','online_hours']),test.drop(columns = ['id','date','online_hours'])\nxtrain = xtrain[feature_import]\nxtest = xtest[feature_import]\nytrain,ytest = train['online_hours'].values, test['online_hours'].values\n    \nopt_model.fit(xtrain,ytrain)\npreds = opt_model.predict(xtest)\n    \nscore = rmse(test_df.online_hours.tolist(),preds)\n\n    \n\nprint('\\n' +'*'*20  +' Optimized Model RMSE Score with feature selection'+'*'*20 + '\\n')\nprint('                 RMSE Score:    %.5f' % score)\nprint('\\n' +'*'*81 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:55:45.462564Z","iopub.execute_input":"2021-07-02T11:55:45.462978Z","iopub.status.idle":"2021-07-02T11:55:48.930911Z","shell.execute_reply.started":"2021-07-02T11:55:45.462941Z","shell.execute_reply":"2021-07-02T11:55:48.929838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Change = (np.array(baseline_errors).mean() - score)/np.array(baseline_errors).mean() *100\n\nprint('\\n' +'*'*20  +' Change in RMSE value from base score '+'*'*20 + '\\n')\nprint('                 Percentage Change in RMSE from Baseline:    %.5f' % Change+'%')\nprint('\\n' +'*'*81 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:56:03.655467Z","iopub.execute_input":"2021-07-02T11:56:03.655953Z","iopub.status.idle":"2021-07-02T11:56:03.663758Z","shell.execute_reply.started":"2021-07-02T11:56:03.655905Z","shell.execute_reply":"2021-07-02T11:56:03.662531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So its look good we got the percentage change in RMSE value from our baseline to optimized model is around 41% thats a big jump... Lets see the final error analysis... how model do? and which data is correctly predicted by model?","metadata":{}},{"cell_type":"markdown","source":"<a id = '3.9'></a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 3px 3px;padding:4px;text-align:left; font-weight: bold\" > 9. Final Error Analyis</h2>","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:54:15.029985Z","iopub.execute_input":"2021-05-20T11:54:15.030574Z","iopub.status.idle":"2021-05-20T11:54:15.037348Z","shell.execute_reply.started":"2021-05-20T11:54:15.030524Z","shell.execute_reply":"2021-05-20T11:54:15.036585Z"}}},{"cell_type":"code","source":"error = pd.DataFrame({\n    \"date\":test.date,\n    \"id\":test.id,\n    \"actual_online_hours\":test_df.online_hours.tolist(),\n    \"pred\":preds\n}).reset_index(drop = True)\n\nerror[\"error\"] = np.abs(error.actual_online_hours-error.pred)\n\nerror.sort_values(\"error\").head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:56:51.532473Z","iopub.execute_input":"2021-07-02T11:56:51.532948Z","iopub.status.idle":"2021-07-02T11:56:51.571868Z","shell.execute_reply.started":"2021-07-02T11:56:51.532899Z","shell.execute_reply":"2021-07-02T11:56:51.570735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,8))\nfig.patch.set_facecolor('#f5f6f6')\nax.set_facecolor('#f5f6f6')\nax.grid('off')\nax.scatter(x = error.actual_online_hours,\n           y = error.pred, s = error.error*20, c = colors[0], edgecolors = 'black', linewidth = 1 )\n\nax.tick_params(axis='both', which='major', \n                    labelsize=12, rotation = 0)\n   \nax.set_ylabel('Predicted Online Hours',**{'font':'serif','size':16,'weight':'bold'})\nax.set_xlabel('Actual Online Hours',**{'font':'serif','size':16,'weight':'bold'})\n\n\n\nfig.text(0.05,0.955,'Customers And App Usage: Error Analysis' ,**{'font':'serif', 'size':20,'weight':'bold',}, alpha = 0.9)\n\nfig.text(0.05,0.9,'''It seems model under predicted high online hours and over \npredicted lower online hours''',**{'font':'serif', 'size':14,}, alpha = 0.8)\n\nfig.text(0.75,0,'© Made by bhuvanchennoju/Kaggle',{'font':'serif', 'size':10, 'weight':'bold',},alpha = 0.7)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:59:01.904888Z","iopub.execute_input":"2021-07-02T11:59:01.905456Z","iopub.status.idle":"2021-07-02T11:59:02.43153Z","shell.execute_reply.started":"2021-07-02T11:59:01.905419Z","shell.execute_reply":"2021-07-02T11:59:02.430375Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '4.0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 4. Summary</h2>\n<br></h2>","metadata":{}},{"cell_type":"markdown","source":"Everything turn out to be excellent, the baseline value to final optimized model is 41%, while model underly predicting zero online hours. Still this approch does a great job. We haven't used any LSTMs or Statistical models for predicting timeseries prediction. Actually this predictions can be done with complex stack of LSTMs as well but highly computationally expensive. \n\n\n<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What happend so far?</strong></p>\n\n<ol>\n    <li>Made few initial assumptions on how to handle mutliple timeseries data, and strategies about modeling with DL</li>\n    <li>Performed the Data analysis and balanced the data, with manual work and Autoviz library</li>\n    <li>Explored options and implemented Sliding window validtion by mimicing the timeseries analaysis with lag information about past days</li>\n    <li>Explored why conventional model performed very poorly </li>\n    <li>Optimized model with optuna and Did Error Analysis</li>\n</ol>\n\n\n<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Well, This a lengthy notebook and Best of my work as far I know, I hope this is helpful..... Thank you so much for reading all the way here...!!</strong></p>\n","metadata":{}},{"cell_type":"markdown","source":"<a id = '5.0'></a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 5. References</h2>\n<br>\n\n<ol>\n<li><a href =\"https://www.kaggle.com/ekrembayar/store-item-demand-forecasting-with-lgbm\" >Store item demand forecasting with lgbm</a></li>\n<li> <a href =\"https://www.kaggle.com/hamzaghanmi/lgbm-hyperparameter-tuning-using-optuna/\" >Hyperparameter optimization with optuna</a></li>\n<li> <a href = \"https://www.kaggle.com/nicapotato/keras-timeseries-multi-step-multi-output\">Keras timeseries multistep multi output</a></li>\n<li> <a href = \"https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts\">timeseris basic exploring with traditional ts</a></li>\n<li><a href =\"https://www.mariofilho.com/how-to-predict-multiple-time-series-with-scikit-learn-with-sales-forecasting-example/\" >Predict multiple time series with sklearn</a></li>\n<li> <a href =\"https://www.kaggle.com/ktakuma/easy-eda-with-single-line-by-autoviz/\" >Autoviz visualization with single line</a></li>\n<li> <a href = \"https://www.kaggle.com/bhuvanchennoju/s-s-experimets-fun-with-r2-99-21\">Diamonds and Dollors: experiments fun</a></li>\n   \n\n    ","metadata":{}}]}