{"cells":[{"metadata":{"_uuid":"3e523b2a8d3d1edc74ef120f9dcac537e9de9dd8"},"cell_type":"markdown","source":"Code based on Git repo: https://github.com/srp98/Movie-Recommender-using-RBM"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"rating_data = pd.read_csv('../input/rating.csv')\nanime_data = pd.read_csv('../input/anime.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b74533a07b7860d8ac6d86e6c5c336b528d7b04d"},"cell_type":"code","source":"anime_list = anime_data.anime_id.unique()\nrated_anime_list = rating_data.anime_id.unique()\nuser_list = rating_data.user_id.unique()\n                 \nprint('anime_count: {}, rated_anime_count: {}, user_count: {}'.format(len(anime_list), len(rated_anime_list), len(user_list)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94a190520c1bca47cfd0052582c464e296d8033"},"cell_type":"code","source":"rating_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdd9f23c7784eae4136c1bda717d2e2037364f58"},"cell_type":"code","source":"anime_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe2d37de9cfa37375213a994f0b1c6f875a0ede0"},"cell_type":"code","source":"# drop watched but not rated scores\n# TODO: replace with avg score \nrating_data.drop(rating_data[rating_data.rating == -1].index, inplace=True)\nrating_data.reset_index(drop=True, inplace=True)\n\n# Histogram of ratings\nplt.hist(rating_data.rating)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02d4a57836ce3b809523168169fead26f7f8f9a7"},"cell_type":"code","source":"print('Min ID: {}, Max ID: {}, count: {}'.format(anime_data.anime_id.min(), anime_data.anime_id.max(), anime_data.anime_id.count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e93f78343196c2e91a8e98fb878de57b055f73"},"cell_type":"code","source":"# We won't be able to index items through their ID since we would get memory indexing errors. \n# To amend we can create a column that shows the spot in our list that particular movie is in:\n\nanime_data['anime_index'] = anime_data.index\nanime_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"328dd964709bfb58f8005fc4b949871e9c60422d"},"cell_type":"code","source":"data_combined = pd.merge(rating_data[['user_id', 'anime_id', 'rating']], anime_data[['anime_id', 'anime_index']], on='anime_id')\ndata_combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acc90540f0019e894bf36f43ba3c1f14f0f371ce"},"cell_type":"code","source":"data_grouped = data_combined.groupby('user_id')\ndata_grouped.first().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3da925ff0b1585bf458b642f5e4ebcfa627973"},"cell_type":"code","source":"\"\"\"\nFormatting the data into input for the RBM. \nStore the normalized users ratings into a list of lists called trX.\n\"\"\"\n\nmax_rating = rating_data.rating.max()\n\n# Amount of users used for training\namountOfUsedUsers = 1000\n\n# Creating the training list\ntrX = []\n\n# For each user in the group\nfor userID, curUser in data_grouped:\n\n    # Create a temp that stores every movie's rating\n    temp = [0]*len(anime_list)\n\n    # For each anime in curUser's list\n    for num, anime in curUser.iterrows():\n\n        # Divide the rating by max and store it\n        temp[anime['anime_index']] = anime.rating/max_rating\n\n    # Add the list of ratings into the training list\n    trX.append(temp)\n\n    # Check to see if we finished adding in the amount of users for training\n    if amountOfUsedUsers == 0:\n        break\n    amountOfUsedUsers -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cba50c6e3b9adf19d1221a8028cc1fe1da4d023e"},"cell_type":"code","source":"# Check that user_id 0 have rated anime with index 1709\ntrX[0][1709]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef4d1f1935093bf30233d381fc774566d7ae2c2"},"cell_type":"code","source":"# Setting the models Parameters\nhiddenUnits = 50\nvisibleUnits = len(anime_list)\nvb = tf.placeholder(tf.float32, [visibleUnits])  # Number of unique animes\nhb = tf.placeholder(tf.float32, [hiddenUnits])  # Number of features were going to learn\nW = tf.placeholder(tf.float32, [visibleUnits, hiddenUnits])  # Weight Matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e51246be955c57d5bdff8282f73792303479f8cb"},"cell_type":"code","source":"# Phase 1: Input Processing\nv0 = tf.placeholder(\"float\", [None, visibleUnits])\n_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)  # Visible layer activation\nh0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))  # Gibb's Sampling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a32bc175cf71e38563c4f8888f36823a7df4965a"},"cell_type":"code","source":"# Phase 2: Reconstruction\n_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)  # Hidden layer activation\nv1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))\nh1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3970a384119909d0a2a72bfe74098b85d26f9aca"},"cell_type":"code","source":"\"\"\" Set RBM Training Parameters \"\"\"\n\n# Learning rate\nalpha = 1.0\n\n# Create the gradients\nw_pos_grad = tf.matmul(tf.transpose(v0), h0)\nw_neg_grad = tf.matmul(tf.transpose(v1), h1)\n\n# Calculate the Contrastive Divergence to maximize\nCD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])\n\n# Create methods to update the weights and biases\nupdate_w = W + alpha * CD\nupdate_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)\nupdate_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)\n\n# Set the error function, here we use Mean Absolute Error Function\nerr = v0 - v1\nerr_sum = tf.reduce_mean(err*err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d2695b4530203117f6a46f3f993f944e6c4e268"},"cell_type":"code","source":"\"\"\" Initialize our Variables with Zeroes using Numpy Library \"\"\"\n\n# Current weight\ncur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n\n# Current visible unit biases\ncur_vb = np.zeros([visibleUnits], np.float32)\n\n# Current hidden unit biases\ncur_hb = np.zeros([hiddenUnits], np.float32)\n\n# Previous weight\nprv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n\n# Previous visible unit biases\nprv_vb = np.zeros([visibleUnits], np.float32)\n\n# Previous hidden unit biases\nprv_hb = np.zeros([hiddenUnits], np.float32)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf252e42f0876d88dae66884e77581edd143846b"},"cell_type":"code","source":"# Train RBM with 15 Epochs, with Each Epoch using 10 batches with size 100, After training print out the error by epoch\nepochs = 15\nbatchsize = 100\nerrors = []\nfor i in range(epochs):\n    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n        batch = trX[start:end]\n        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_hb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        prv_w = cur_w\n        prv_vb = cur_vb\n        prv_hb = cur_hb\n    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))\n    print(errors[-1])\nplt.plot(errors)\nplt.ylabel('Error')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4c827065bed57b5447c90c3d1293aea3d7f468b"},"cell_type":"code","source":"\"\"\"\nRecommendation System :-\n- We can now predict anime that an arbitrarily selected user might like. \n- This can be accomplished by feeding in the user's watched preferences into the RBM and then reconstructing the \n  input. \n- The values that the RBM gives us will attempt to estimate the user's preferences for items that he hasn't watched \n  based on the preferences of the users that the RBM was trained on.\n\"\"\"\n\n# Select the example User\ninputUser = [trX[50]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"500d5327de77ac948582c494538062bf38d85a39"},"cell_type":"code","source":"# Feeding in the User and Reconstructing the input\nhh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nfeed = sess.run(hh0, feed_dict={v0: inputUser, W: prv_w, hb: prv_hb})\nrecommendation = sess.run(vv1, feed_dict={hh0: feed, W: prv_w, vb: prv_vb})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22f601f1301a0125e7e6377bb2f3c1d4b6bc8543"},"cell_type":"code","source":"# List the 20 most recommended items for our mock user by sorting it by their scores given by our model.\nrecos_for_user = anime_data\nrecos_for_user[\"Recommendation Score\"] = recommendation[0]\nrecos_for_user.sort_values([\"Recommendation Score\"], ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fd46433475dbb9c63c02575674447625efe89f1"},"cell_type":"code","source":"\"\"\" Recommend User what movies he has not watched yet \"\"\"\n\n# Find the mock user's UserID from the data\ndata_combined.iloc[50]  # Result you get is UserID 191","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ee59ecdf72d8c492edbee9d2786feac45461d70"},"cell_type":"code","source":"# Find all items the mock user has watched before\nrated_anime_for_user = data_combined[data_combined['user_id'] == 191]\nrated_anime_for_user.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82726a15a5655a0398bf1632432f2aa93a7ca6bf"},"cell_type":"code","source":"\"\"\" Merge all items that our mock users has watched with predicted scores based on his historical data: \"\"\"\n\n# Merging scored items with rated items by ID\nmerge_watched_recommended = recos_for_user.merge(rated_anime_for_user, on='anime_id', how='outer')\nmerge_watched_recommended.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27339f678780fcbc95019c2ac848a48f808ed376"},"cell_type":"code","source":"# Dropping unnecessary columns\nmerge_watched_recommended = merge_watched_recommended.drop('anime_index_y', axis=1).drop('user_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"442174d2ea39411144bbfa46b736c8a8516cb492"},"cell_type":"code","source":"# Sort and take a look at first 20 rows\nmerge_watched_recommended.sort_values(['Recommendation Score'], ascending=False).head(20)\n\n# \"\"\" There are some item the user has not watched and has high score based on our model. So, we can recommend them. \"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}