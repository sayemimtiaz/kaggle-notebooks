{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Build a classifier to predict whether a person is suffering from covid, cold, flu or allergy?**","metadata":{}},{"cell_type":"markdown","source":"**import the required packages and libraries**\n\n\n1.   Pandas is a Python library. Pandas is used to analyze data.\n2.   sklearn is a machine learning library for Python. It has classes and functions with respect to various algorithms like support vector machine, logistic regression, random forests, etc.\n\n  a. Label Encoder is for converting the values in the given column into numeric form \n\n  b.  StandardScaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\n  c. sklearn.metrics module implements functions assessing prediction error for specific purposes.\n\n  d.  train_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data\n  \n5.   Keras is a Python library for neural networks\n\n6. joblib provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently. \n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport keras\nimport joblib\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Data Preprocessing**\n1. read the dataset file\n2. do a little preprocessing to convert alphanumerical values in certain columns to numerical/ordinal values\n3. split the dataset into test and training datasets\n4. perform feature scaling","metadata":{}},{"cell_type":"code","source":"def readDataMulti():\n#add the csv file to the current directory before proceeding\n# read the CSV file\n  global scaler\n  global dictionary\n  global colnames\n   #path to the file in the current directory\n  features = pd.read_csv('/kaggle/input/covid-flu-cold-symptoms/large_data.csv')                            #reading the file\n  features = features.rename(columns={'TYPE' : 'class'})  #renaming the result column\n  colnames=features.columns\n  # Label Encoding refers to converting the labels into numeric form so as to convert it \n  # into the machine-readable form. \n  # Machine learning algorithms can then decide in a better way on how those labels must be operated\n  label_encoder = LabelEncoder()                               \n  features['class']= label_encoder.fit_transform(features['class']) #performing label encoding in the given column\n  labels = features.pop('class')  #removing the class column from the features table\n  keys = label_encoder.classes_  \n  values = label_encoder.transform(label_encoder.classes_)\n  dictionary = dict(zip(keys, values)) #storing the converted column entries as (key,value) pairs\n  print(dictionary)\n  X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20,random_state=5)  #splitting the dataset into train and test set\n  scaler = StandardScaler()\n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train) \n  X_test = scaler.transform(X_test)\n  return X_train,y_train, X_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a neural network classifier","metadata":{}},{"cell_type":"markdown","source":"Machine learning algorithms that use neural networks generally do not need to be programmed with specific rules that define what to expect from the input. The neural net learning algorithm instead learns from processing many labeled examples that are supplied during training and using this answer key to learn what characteristics of the input are needed to construct the correct output. Once a sufficient number of examples have been processed, the neural network can begin to process new, unseen inputs and successfully return accurate results.   \n\nHere we build a neural network classifier with 2 layers having 32 neurons each.\n1. Use tanh activation function for hidden layers\n2. Use softmax activation function for output layer","metadata":{}},{"cell_type":"code","source":"# Train and evaluate\ndef train_and_evaluate(X_train, Y_train, X_test, Y_test):\n    global accuracyNN\n    m=X_train.shape[0]\n    n=X_train.shape[1]\n    classes=4\n    # Create layers\n    inputs = keras.layers.Input(shape=(n,), dtype='float32', name='input_layer') # Input (2 dimensions)\n    outputs = keras.layers.Dense(32, activation='tanh', name='hidden_layer1')(inputs) # Hidden layer\n    outputs = keras.layers.Dense(32, activation='tanh', name='hidden_layer2')(outputs) # Hidden layer\n    outputs = keras.layers.Dense(classes, activation='softmax', name='output_layer')(outputs) # Output layer \n    # Create a model from input layer and output layers\n    model = keras.models.Model(inputs=inputs, outputs=outputs, name='neural_network')\n    # Convert labels to categorical: categorical_crossentropy expects targets to be binary matrices (1s and 0s) of shape (samples, classes)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    Y_binary = keras.utils.to_categorical(Y_train, num_classes=classes, dtype='int')\n    # Train the model on the train set (output debug information)\n    model.fit(X_train, Y_binary, epochs=100, verbose=1)\n    # Save the model (Make sure that the folder exists)\n    model.save('nn.h5')\n    # Evaluate on training data\n    print('\\n-- Training data --')\n    predictions = model.predict(X_train)\n    #now make the prob. of the class which has the highest prob. as 1 and the prob. of other classes as 0\n    for i in range(m):\n      max=predictions[i].max()\n      predictions[i][predictions[i] != max]=0\n      predictions[i][predictions[i] == max]=1\n    accuracy = sklearn.metrics.accuracy_score(Y_binary, predictions)\n    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n    print('Classification Report:')\n    print(sklearn.metrics.classification_report(Y_binary, predictions))\n    print('')\n    # Evaluate on test data\n    print('\\n---- Test data ----')\n    predictions = model.predict(X_test)\n    Y_test=np.asarray(Y_test).astype('int32')\n    Y_binary_test = keras.utils.to_categorical(Y_test, num_classes=classes, dtype='int')\n    m1=Y_test.shape[0]\n    for i in range(m1):\n      max=predictions[i].max()\n      predictions[i][predictions[i] != max]=0\n      predictions[i][predictions[i] == max]=1\n    accuracyNN = sklearn.metrics.accuracy_score(Y_binary_test, predictions)\n    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n    print('Classification Report:')\n    print(sklearn.metrics.classification_report(Y_binary_test,predictions))\n\n# The main entry point for this module\ndef main():\n    # Load data set (includes header values)\n    X_train_label,Y_train_label,X_test,Y_test=readDataMulti()\n    # Train and evaluate\n    train_and_evaluate(X_train_label, Y_train_label, X_test, Y_test)\n# Tell python to run main method\nif __name__ == \"__main__\": \n  main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Random Forest classifier","metadata":{}},{"cell_type":"markdown","source":"The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate_RF(X_train, y_train, X_test, y_test):\n  global accuracyRF\n  RFclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42) # 10 decision trees used in this classifier\n  RFclassifier.fit(X_train, y_train)  #training  \n  filename = 'rf_model.sav'\n  joblib.dump(RFclassifier, filename)  #save the model\n  y_pred = RFclassifier.predict(X_test) #predict on test set\n  accuracyRF = sklearn.metrics.accuracy_score(y_test, y_pred)\n  print(sklearn.metrics.confusion_matrix(y_test,y_pred))\n  print(sklearn.metrics.classification_report(y_test,y_pred))\n\ndef main():\n    X_train,Y_train,X_test,Y_test=readDataMulti()\n    train_and_evaluate_RF(X_train, Y_train, X_test, Y_test)\n# Tell python to run main method\nif __name__ == \"__main__\": \n  main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building an SVM classifier","metadata":{}},{"cell_type":"markdown","source":"Support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.  An SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\ndef train_and_evaluate_SVM(X_train, y_train, X_test, y_test):\n  global accuracySVM\n  svclassifier = SVC(kernel='rbf')\n  svclassifier.fit(X_train, y_train)\n  filename = 'svm_model.sav'\n  joblib.dump(svclassifier, filename) #save the model\n  y_pred = svclassifier.predict(X_test)  #predict on test set\n  accuracySVM = sklearn.metrics.accuracy_score(y_test, y_pred)\n  print(sklearn.metrics.confusion_matrix(y_test,y_pred))\n  print(sklearn.metrics.classification_report(y_test,y_pred))\n\ndef main():\n    X_train,Y_train,X_test,Y_test=readDataMulti()\n    train_and_evaluate_SVM(X_train, Y_train, X_test, Y_test)\n# Tell python to run main method\nif __name__ == \"__main__\": \n  main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing on a single independent test instance**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# It can be used to reconstruct the model identically.\nn=colnames.shape[0]-1\ntest_input=[]\nfor i in range(n):\n    print(\"Do you have \", colnames[i], \"?? Enter 1 for Yes and 0 for No-\")\n    inp=input()\n    test_input.append(inp)\ntest_input=np.asarray(test_input)\ntest_input = test_input.reshape(1, -1)  #reshaping because right now shape of array is (n,) which has to be converted to (1,n)\ntest_input = scaler.transform(test_input)\nkey_list = list(dictionary.keys()) #make a list of keys \nval_list = list(dictionary.values()) #make a list of values \n\n# load the Neural Network model from disk\nnn_model = keras.models.load_model(\"nn.h5\")\n\nprediction1=nn_model(test_input) \nprediction1=np.asarray(prediction1)\nmax_index_col = np.argmax(prediction1, axis=1) #find the max value in the output vector\nprint(\"NN says you have\", key_list[val_list.index(max_index_col)]) #printing the key value wrt the output given by NN\n\n# load the Log. Reg. model from disk\nrf_model = joblib.load('rf_model.sav')\nprediction2=rf_model.predict(test_input)\nprint(\"RF says you have\", key_list[val_list.index(prediction2)]) #printing the key value wrt the output given by NN\n# load the SVM model from disk\nsvm_model = joblib.load('svm_model.sav')\nprediction3=svm_model.predict(test_input)\nprint(\"SVM says you have\", key_list[val_list.index(prediction3)]) #printing the key value wrt the output given by NN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Plotting the performance of all the 3 classifiers on our dataset**","metadata":{}},{"cell_type":"code","source":"y = np.array([accuracyNN,accuracySVM,accuracyRF]) \nx = ['Neural Network','SVM','Random Forest']\nplt.bar(x,y)\nplt.title('Performance comparison')\nplt.xlabel('Classifiers')\nplt.ylabel('accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}