{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Interactive search tool for notebooks\nThis notebook builds a simple, general purpose, search tool based on word embeddings. \nThe embeddings are trained, and document / paragraph vectors calculated in a separate notebook. This notebook focuses on the interactive search tool using widgets."},{"metadata":{},"cell_type":"markdown","source":"## Load word vector model and pre-calculated paragraph vectors "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install Facebooks similarity search library\n!pip install faiss-cpu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport re\nfrom unidecode import unidecode\nimport pickle\nimport gensim \nfrom IPython.display import display, HTML\nfrom faiss import read_index\nfrom ipywidgets import widgets, interact, Layout, Dropdown, Label, HBox, VBox, interactive_output\nfrom ipywidgets import HTML as widgetHTML\nimport seaborn as sns\ncm = sns.light_palette(\"green\", as_cmap=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/CORD-19-research-challenge'\nPRE_PROCESSED_PATH = '/kaggle/input/cord-19-interactive-word2vec-paragraph-search'\nEMBEDDING_DIMS = 300\n\nall_data = pd.read_pickle(os.path.join(PRE_PROCESSED_PATH, 'CORD_19_all_papers.pkl'))\nvocab = pickle.load(open(os.path.join(PRE_PROCESSED_PATH, 'covid_vocab_frequencies.pkl'), 'rb'))\nmodel = gensim.models.Word2Vec.load(os.path.join(PRE_PROCESSED_PATH, 'covid_w2v'))\n# paragraph_vectors = pickle.load(open(os.path.join(PRE_PROCESSED_PATH, 'all_para_vectors.pkl'), 'rb'))\nindex_cosine = read_index(os.path.join(PRE_PROCESSED_PATH, 'index_cosine.faiss'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Various helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regex used for cleaning and tokenisation\nspace = re.compile('\\s+')\nreference = re.compile(r'[\\(\\[]\\d+(, ?\\d+)?[\\)\\]]')\nlinks = re.compile(r'https?://\\S+')\nsentence  = re.compile(r'(\\S{3,})[.!?]\\s')\nhyphen_1 = re.compile(r'([A-Z0-9])\\-(.)')\nhyphen_2 = re.compile(r'(.)\\-([A-Z0-9])')\n\n\nPAT_ALPHABETIC = re.compile(r'(((?![\\d])\\w)+)', re.UNICODE) # from gensim - removes digits - keeps only other alpha numeric and tokenises on everything else\nPAT_ALL = re.compile(r'((\\d+(,\\d+)*(\\.\\d+)?)+|([\\w_])+)', re.UNICODE) # Includes digits - tokenises on space and non alphanumeric characters (except _)\n\ndef clean_text(text):\n    text = text.replace('\\t', ' ').replace('\\n', ' ')\n    text = sentence.sub(r'\\1 _SENT_ ', text)\n    text = text.replace('doi:', ' http://a.website.com/')\n    text = unidecode(text) # converts any non-unicode characters to a unicode equivalent\n    text = hyphen_1.sub(r'\\1\\2', text)\n    text = hyphen_2.sub(r'\\1\\2', text)\n    text = links.sub(' ', text)\n    text = reference.sub(' ', text)\n    text = space.sub(' ', text)\n\n    return text.strip()\n\ndef fetch_tokens(text, reg_pattern):\n    for match in reg_pattern.finditer(text):\n        yield match.group()\n\ndef tokenise(text, remove_stop=False, lowercase=False, include_digits=True):\n    text = clean_text(text)\n    \n    if lowercase:\n        text = text.lower()\n    \n    if include_digits:\n        tokens = list(fetch_tokens(text, reg_pattern=PAT_ALL))\n    else:\n        tokens = list(fetch_tokens(text, reg_pattern=PAT_ALPHABETIC))\n            \n    if remove_stop:\n        return ' '.join([x for x in tokens if x.lower() not in stopWords])\n    else:\n        return ' '.join(tokens)\n\ndef fetch_vector(words):\n    tokens = tokenise(words).split()\n\n    if len(tokens) > 0:\n        myvectors = np.zeros((1, EMBEDDING_DIMS), dtype=np.float32)\n        myvectors[0, :] = calc_vector(tokens)\n\n        # normalize\n        myvectors = myvectors / np.linalg.norm(myvectors, axis=1, keepdims=True)\n        return myvectors\n\ndef calc_vector(tokens):\n    vec = np.zeros((EMBEDDING_DIMS,), dtype=np.float32)\n    for word in tokens:\n        found=False\n\n        if word not in model.wv.vocab:\n            if word.title() in model.wv.vocab:\n                word = word.title()\n                found = True\n            elif word.lower() in model.wv.vocab:\n                word = word.lower()\n                found = True\n            elif word.upper() in model.wv.vocab:\n                word = word.upper()\n                found = True\n        else:\n            found = True\n\n        if found:\n            vec += model.wv.get_vector(word).astype(np.float32) / np.log(5 + vocab[word])\n\n#         normalise\n    vec = vec / np.linalg.norm(vec, axis=0, keepdims=True)\n    return vec\n\ndef fetch_similar(positive=[], negative=[], k=10):\n    label = ';'.join(positive)\n    if negative:\n        label += ' - ' + ';'.join(negative)\n    return pd.DataFrame([(x, y, vocab[x]) for x,y in model.wv.most_similar(positive, negative, topn=k)], columns=[label, 'relevance', 'frequency'])\n\ndef view_similar(positive='Beijing,Paris', negative='China', top_k=10):\n    if negative=='':\n        negative = []\n    else:\n        negative = negative.split(',')\n    return fetch_similar(positive=positive.split(','),\n                        negative=negative,\n                        k=top_k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore word similarities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# common cold is to sore throat, blocked or runny nose as COVID19 is to ?\ninteract(view_similar, positive='COVID19,SARSCoV2,NCP,runny,nose,sore,throat,sneezing,rhinorrhea', negative='common,cold');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Search the CORD-19 document set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the main search function\ndef search_db(words, k=10, covid_only=True, date_range=[1957,2020], paper_select='All'):\n    vec = fetch_vector(words)\n    cm = sns.light_palette(\"green\", as_cmap=True)\n\n    if paper_select=='All':\n        D, I = index_cosine.search(vec, k+500)\n        tmp = all_data.loc[I[0], ['title', 'para_context', 'para_text', 'publish_year', 'doi', 'id']].copy()\n        tmp['Relevance'] = np.round(D[0], 2)\n\n        if covid_only:\n            # We will inlcude documents that have a blank title\n            tmp = tmp.loc[tmp.title.fillna('covid').str.lower().str.contains('covid|sars\\-cov|novel coronavirus')]\n\n        if date_range[0]>1957 or date_range[1]<2020:\n            # Also include documents missing the published date\n            tmp = tmp.loc[(tmp.publish_year=='0') | ((tmp.publish_year>=str(date_range[0])) & (tmp.publish_year<=str(date_range[1])))]\n        if len(tmp) > 0:\n            tmp = tmp.loc[:, ['Relevance', 'para_context', 'title', 'publish_year', 'doi', 'id']].iloc[:k].copy()\n            paper.options = [('All', 'All')] + [(y, x) for x,y in tmp[['title', 'id']].groupby('id').first().itertuples()]\n            tmp['title'] = tmp.apply(lambda x: f'<a href=\"http://doi.org/{x.doi}\" target=\"_blank\">{x.title}</a>', axis=1)\n            tmp['para_context'] = tmp['para_context'].apply(lambda x: x.replace('_STARTREF_', '<p><i>').replace('_ENDREF_', '</i></p>'))\n            \n            tmp = tmp.reset_index(drop=True)[['Relevance', 'para_context', 'title', 'publish_year']]\\\n                            .rename(columns={'para_context': 'Body text', 'title': 'Paper title', 'publish_year': 'Published'})\n            display(tmp.style.background_gradient(cmap=cm))\n        else:\n            print('No results found')\n    else:\n        D, I = index_cosine.search(vec, index_cosine.ntotal)\n#         print(paper_select)\n        tmp = all_data.loc[all_data.id==paper_select, ['para_num', 'title', 'section', 'para_text', 'publish_year', 'doi', 'id']].copy()\n        display(HBox([Label('Link to paper:', layout=Layout(width='20%')),\n                      widgetHTML(f'<a href=\"http://doi.org/{tmp.doi.iloc[0]}\" target=\"_blank\">{tmp.title.iloc[0]}</a>')]))\n        tmp['Relevance'] = 0\n        # fetch relevance scores for paragraphs in this document\n        scores = [(x,y) for x,y in zip(I[0], np.round(D[0], 2)) if x in tmp.index]\n        tmp.loc[[x[0] for x in scores], 'Relevance'] = [x[1] for x in scores]\n        tmp['para_text'] = tmp['para_text'].apply(lambda x: '<p>' + x.replace('_STARTREF_', '<p><i>').replace('_ENDREF_', '</i></p>') + '</p>')\n        tmp = tmp[['Relevance', 'section', 'para_text', 'para_num']].rename(columns={'para_text': 'Body text'}).set_index('para_num')\n        display(tmp.style.background_gradient(cmap=cm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are the interactive widgets for the search tool\npaper = Dropdown(options=[('All', 'All')], \n                layout=Layout(width='80%'))\n\nword_widget = widgets.Text(\n    value='COVID19 symptoms',\n    placeholder='?',\n    disabled=False,\n    layout=Layout(width='80%')\n)\n\ndate_range_widget = widgets.IntRangeSlider(\n        value=[1957, 2020],\n        min=1957,\n        max=2020,\n        step=1,\n        description='Publish date:',\n        disabled=False,\n        continuous_update=False,\n)\n\nnum_results = widgets.IntSlider(value=10,\n                                min=5,\n                                max=50,\n                                step=1,\n                                description='# Results',\n                                continuous_update=False,\n                                orientation='horizontal')\n\ncovid_filter = widgets.Checkbox(value=True,\n                               description='Show only COVID articles')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This creates the layout of the widgets\nsearch_tool = VBox([\n    HBox([Label('Search phrase:', layout=Layout(width='20%')), word_widget]),\n    HBox([Label('Filters:', layout=Layout(width='20%')), num_results, date_range_widget, covid_filter]),\n    HBox([Label('Select paper to view full text:', layout=Layout(width='20%')), paper])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next we link the widgets to the search_db method\noutput = interactive_output(search_db, {'words':word_widget, 'date_range':date_range_widget, 'paper_select':paper, 'k':num_results, 'covid_only':covid_filter})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure we can see very long paragraphs when we display a pandas dataframe\npd.options.display.max_colwidth = 3000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the search tool\ndisplay(search_tool, output);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}