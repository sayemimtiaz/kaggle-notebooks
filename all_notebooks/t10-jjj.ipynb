{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial Recognition (RUN ALL)","metadata":{"id":"ksnsDbd_sJpj"}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer","metadata":{"id":"qdXPQe18sKty","outputId":"a24f9aab-c91c-47ae-9432-0e453cb8a783","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"../input/facialrecognitiont10/\")  \n%pwd\nfrom triplet_loss import TripletLossLayer\nfrom lfw_preprocessor import LfwDataGenerator","metadata":{"id":"qdXPQe18sKty","outputId":"a24f9aab-c91c-47ae-9432-0e453cb8a783","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"models/face_recognition\")\nfrom align import AlignDlib\nfrom model import create_model\nos.chdir(\"../..\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\nmodel_sm = create_model()\n\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\nfacial_rec_model = Model([in_a, in_p, in_n], triplet_loss_layer)\nfacial_rec_model.load_weights(\"epoch097_loss0.176.hdf5\")\nfacial_rec_model.summary()\n\nfacial_rec_base_model = facial_rec_model.layers[3]","metadata":{"id":"qdXPQe18sKty","outputId":"a24f9aab-c91c-47ae-9432-0e453cb8a783","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_base_embeddings():\n    embeddings = np.empty((49, 128))\n    for i in range(49):\n        print(\"now processing: face_recognition_dataset/\" + str(i) + \".png\" )\n        img = load_image('face_recognition_dataset/' + str(i) + \".png\")\n        img = align_image(img)\n        img = img.astype('float32')\n        img = img / 255.0\n        img = np.expand_dims(img, axis=0)\n        embeddings[i] = facial_rec_base_model.predict(img)\n        \n    return embeddings\n\nimport csv\nfrom numpy import asarray\ndef name_mapping():\n    id_names = []\n    with open('face_recognition_dataset/person_id_name_mapping.csv') as id_name_map_csv:\n        csv_dict_reader = csv.DictReader(id_name_map_csv)\n        for row in csv_dict_reader:\n            id_names.append({\n                \"id\": int(row[\"person_id\"]),\n                \"name\": row[\"person_name\"]})\n    return id_names\n\ndef preprocess(img, box):\n    face = img.crop((int(box[0]), int(box[1]), int(box[2]), int(box[3])))\n    face = asarray(face)\n    face = face[...,::-1]\n    face = align_image(face)\n    face = face.astype('float32')\n    face = face / 255.0\n    face = np.expand_dims(face, axis=0)\n    return face\n\ndef distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef infer(face):\n\n    face_embed = facial_rec_base_model.predict(face)\n    minDistance = distance(face_embed, embeddings[0])\n    minIndex = 0\n\n\n    for i in range(1, 49):\n        if(distance(face_embed, embeddings[i]) < minDistance):\n            minDistance = distance(face_embed, embeddings[i]) \n            minIndex = i\n    return next(item for item in names if item[\"id\"] == minIndex)[\"name\"]\n\ndef align_image(img):\n    alignment = AlignDlib('models/landmarks.dat')\n    bb = alignment.getLargestFaceBoundingBox(img)\n    if bb is None:\n        return cv2.resize(img, (96,96))\n    else:\n        return alignment.align(96, \n                               img, \n                               bb,\n                               landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]","metadata":{"id":"OgN-iCrI2lQe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DETR Model","metadata":{"id":"oS9WlRsDuKcj"}},{"cell_type":"code","source":"os.chdir('detr')","metadata":{"id":"_5OhwugOtQRI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport random\nfrom pathlib   import Path\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport PIL.Image\nimport util.misc as utils\nos.chdir('..')\nimport detr.models\nfrom detr.models import build_model\n!pip install pycocotools\nos.chdir('detr')\n%pwd \n%ls \nimport pycocotools\nfrom main import get_args_parser","metadata":{"id":"epyFE6uKuQIH","outputId":"32c5e2c9-65ac-44d9-87e4-ce74b462b528","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\n#This now loads the newly trained face detection weights\nargs.resume = 'checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\nargs.distributed = False\nprint(args)","metadata":{"id":"m_FEiWiEuQ-6","outputId":"af21effc-a127-44c3-ae75-a45f476e6df0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face_detection_model, criterion, postprocessors = build_model(args)\ndevice = torch.device(args.device)\nface_detection_model.to(device)","metadata":{"id":"zHlahe62uST-","outputId":"2205ad89-d3a1-4400-ada5-3dfc6ac0472e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('..')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = Path(args.output_dir)\nif args.resume:\n    if args.resume.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            args.resume, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n    face_detection_model.load_state_dict(checkpoint[\"model\"], strict=True)","metadata":{"id":"7EQWVilOuT-p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSES = ['none', 'person']\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n","metadata":{"id":"lgmIyRmjuVfk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect(im, model, transform):\n    img = transform(im).unsqueeze(0)\n    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each size'\n\n    outputs = face_detection_model(img)\n\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled","metadata":{"id":"fv7i1Gt0uc77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n        text =f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor = 'yellow', alpha=0.5))\n\n    plt.axis('off')\n    plt.show()","metadata":{"id":"d5CtLMf9ufJK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Driver (RUN ALL)\n","metadata":{"id":"y0aj2YNxk-ms"}},{"cell_type":"code","source":"def plot_predictions(pil_img, prob, boxes, classes, matched_people):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c, mp in zip(prob, boxes.tolist(), COLORS * 100, matched_people):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n        text = mp\n        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor = 'yellow', alpha=0.5))\n\n    plt.axis('off')\n    plt.show()","metadata":{"id":"MgnbaSIDX_wn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_names(boxes, img_letter):\n  #Below are 2 arrays recording the corresponding id for each bounding box and \n  #their distance\n    prediction_names = [None] * len(boxes)\n    prediction_distances = [None] * len(boxes)\n\n    for i in range(len(base_embeddings)):\n        face_embed = base_embeddings[i]\n        distances = []\n        for j in range(len(pp_face_embeds)):\n            distances.append(distance(face_embed, pp_face_embeds[j]))\n        while min(distances) != 999:\n            dist = min(distances)\n            closestIndex = distances.index(dist)\n            if prediction_distances[closestIndex] == None or prediction_distances[closestIndex][0] > dist:\n                if prediction_distances[closestIndex] != None:\n                    del submission_dict[img_letter + \"_\" + str(prediction_distances[closestIndex][1])]\n                prediction_distances[closestIndex] = (dist, i)\n                prediction_names[closestIndex] = next(item for item in names if item[\"id\"] == i)[\"name\"]\n                submission_dict[img_letter + \"_\" + str(i)] = boxes[closestIndex].detach().numpy()\n                break\n            distances[closestIndex] = 999\n    return prediction_names","metadata":{"id":"eEoLbqsO_0n3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = name_mapping()\n#line below will take a while, that's normal\nbase_embeddings = set_base_embeddings()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url1 = '../c/2021-spring-coml-face-recognition-competition/a.jpg'\nurl2 = '../c/2021-spring-coml-face-recognition-competition/b.jpg'\nurl3 = '../c/2021-spring-coml-face-recognition-competition/c.jpg'\nurl4 = '../c/2021-spring-coml-face-recognition-competition/d.jpg'\nplot_classes = [\"person\"]\n\nsubmission_dict = {}\nthe_image = PIL.Image.open(url1)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\npredictions = predict_names(boxes, \"a\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\nthe_image = PIL.Image.open(url2)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"b\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\n\nthe_image = PIL.Image.open(url3)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"c\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\n\nthe_image = PIL.Image.open(url4)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"d\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)","metadata":{"id":"bOkVeKdEk58l","outputId":"c54bdd43-a2cb-4bbc-faa1-b824d5487aed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nos.chdir(\"../../working\")\nwith open('../input/c/2021-spring-coml-face-recognition-competition/kaggle_sample_submission.csv') as sample_submission_csv:\n    with open(\"submission.csv\", \"w\", newline = \"\") as submission_csv:\n        sample_csv_reader = csv.DictReader(sample_submission_csv)\n        open('submission.csv', 'w').close()\n        submission_csv_writer = csv.DictWriter(submission_csv, sample_csv_reader.fieldnames)\n        submission_csv_writer.writeheader()\n        for row in sample_csv_reader:\n            if row[\"id\"] in submission_dict:\n                submission_csv_writer.writerow({\"id\": row[\"id\"], \n                                                \"xmin\": submission_dict[row[\"id\"]][0], \n                                                \"xmax\": submission_dict[row[\"id\"]][1], \n                                                \"ymin\": submission_dict[row[\"id\"]][2], \n                                                \"ymax\": submission_dict[row[\"id\"]][3]})\n            else:\n                submission_csv_writer.writerow({\"id\": row[\"id\"], \n                                                \"xmin\": 0, \n                                                \"xmax\": 0, \n                                                \"ymin\": 0, \n                                                \"ymax\": 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}