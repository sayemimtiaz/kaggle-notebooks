{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport string\nimport nltk\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport time\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/news-aggregator-dataset/uci-news-aggregator.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Independent Features- [ID,TITLE,URL,PUBLISHER,STORY,HOSTNAME,TIMESTAMP]\n\n# Dependent Feature- 'CATEGORY'\t"},{"metadata":{},"cell_type":"markdown","source":"# Checking for Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature ',end=' ')\nif(any(df.isnull().any())):\n    print('Missing Data\\n')\n    print(df.isnull().sum())\nelse:\n    print('NO missing data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Data Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PUBLISHER'] = df['PUBLISHER'].fillna(df['PUBLISHER'].mode()[0]) # Mode- 'Reuters'\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for Duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data Size {}'.format(df.shape))\nif(any(df.duplicated())==True):\n    print('Duplicate rows found')\n    print('Number of duplicate rows= ',df[df.duplicated()].shape[0])\n    df.drop_duplicates(inplace=True,keep='first')\n    df.reset_index(inplace=True,drop=True)\n    print('Dropping duplicates\\n')\n    print(df.shape)\nelse:\n    print('NO duplicate data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of 'CATEGORY' (Dependent Variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# (b = business, t = science and technology, e = entertainment, m = health)\n\ndef label_to_name(label):\n    if(label=='e'):\n        return 'entertainment'\n    elif(label=='b'):\n        return 'business'\n    elif(label=='t'):\n        return 'science and technology'\n    else:\n        return 'health'\n    \ndf['CATEGORY'] = df['CATEGORY'].apply(label_to_name)\nprint('Distribution of labels in %\\n')\nprint(df['CATEGORY'].value_counts()/df.shape[0]*100)\n\nsns.set(font_scale=1.2)\nplt.figure(figsize=(12,6))\nsns.countplot(df['CATEGORY']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Since, we have to classify news into respective categories based on their headlines we will drop other features for building the classifier."},{"metadata":{},"cell_type":"markdown","source":"# PREPROCESSING\n\n### - Dropping features [ID,URL,PUBLISHER,STORY,HOSTNAME,TIMESTAMP]\n### - Lowercasing text in 'TITLE' column\n### - Removing Punctuation\n### - Removing StopWords\n### - Lemmatizing (since Stemming may create non-existent/incorrect words)\n### - Removing emojis if any (None were found)\n### - Removing emoticons (None were found)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['ID','URL','PUBLISHER','STORY','HOSTNAME','TIMESTAMP'],inplace=True)\n\n\n# lowercasing\ndf['lower'] = df['TITLE'].str.lower()\n\n\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\ndf[\"punc_removed\"] = df[\"lower\"].apply(lambda text: remove_punctuation(text))\n\n\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf[\"stopwords_removed\"] = df[\"punc_removed\"].apply(lambda text: remove_stopwords(text))\n\n\ndf.head()\n\n\n# lemmatizer = WordNetLemmatizer()\n# def lemmatize_words(text):\n#     return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n# df[\"lemmatized_without_stopwords\"] = df[\"punc_removed\"].apply(lambda text: lemmatize_words(text))\n\n# df[\"lemmatized_stopwords\"] = df[\"stopwords_removed\"].apply(lambda text: lemmatize_words(text))\n\n\n\n\n# NO EMOJI IN DATA\n# print(all(df['lemmatized'] == df['removed_emoji'])) # TRUE\n# def remove_emoji(string):\n#     emoji_pattern = re.compile(\"[\"\n#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                            u\"\\U00002702-\\U000027B0\"\n#                            u\"\\U000024C2-\\U0001F251\"\n#                            \"]+\", flags=re.UNICODE)\n#     return emoji_pattern.sub(r'', string)\n# df[\"removed_emoji\"] = df[\"lemmatized\"].apply(lambda text: remove_emoji(text))\n\n\n\n\n# NO EMOTICONS IN DATA\n# print(all(df['lemmatized'] == df['removed_emoticons'])) # TRUE\n# def remove_emoticons(text):\n#     emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n#     return emoticon_pattern.sub(r'', text)\n\n# df[\"removed_emoticons\"] = df[\"lemmatized\"].apply(lambda text: remove_emoticons(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WORDCLOUD"},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = ' '\nstopwords = set(STOPWORDS) \n  \n\nfor val in df.stopwords_removed[0:10000]:  \n    tokens = val.split()     \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n  \n  \nwordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf['CATEGORY']=le.fit_transform(df['CATEGORY'])\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results using CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert data into vectors\nvectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df['stopwords_removed'])\ny = df['CATEGORY']\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42,stratify=df.CATEGORY)\nprint('Training Data ',x_train.shape,y_train.shape)\nprint('Test Data     ',x_test.shape,y_test.shape)\n\n\nresults = pd.DataFrame(columns=['Model','Accuracy','F1-score'])\n\nmodels_name = ['Logistic Regression','Decision Tree','Multinomial NaiveBayes']\n\nmodel_list = [LogisticRegression(), DecisionTreeClassifier(),MultinomialNB()]\n\nfor idx,model in enumerate(model_list):\n    clf = model.fit(x_train, y_train)\n    predictions = model.predict(x_test)\n    results.loc[idx] = [models_name[idx],accuracy_score(y_test, predictions),f1_score(y_test, predictions, average = 'weighted')]\n\nresults.sort_values(by='Accuracy',inplace=True,ascending=False)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results using TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['stopwords_removed'].values)\ny = df['CATEGORY']\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42,stratify=df.CATEGORY)\nprint('Training Data ',x_train.shape,y_train.shape)\nprint('Test Data     ',x_test.shape,y_test.shape)\n\n\nresults = pd.DataFrame(columns=['Model','Accuracy','F1-score'])\n\nmodels_name = ['Logistic Regression','Decision Tree','Multinomial NaiveBayes']\n\nmodel_list = [LogisticRegression(), DecisionTreeClassifier(),MultinomialNB()]\n\nfor idx,model in enumerate(model_list):\n    clf = model.fit(x_train, y_train)\n    predictions = model.predict(x_test)\n    results.loc[idx] = [models_name[idx],accuracy_score(y_test, predictions),f1_score(y_test, predictions, average = 'weighted')]\n\nresults.sort_values(by='Accuracy',inplace=True,ascending=False)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using LSTM for training Deep Learning model"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = to_categorical(df['CATEGORY'], num_classes=4)\n\nn_most_common_words = 10000\nmax_len = 130\ntokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df[\"lower\"].values)\nsequences = tokenizer.texts_to_sequences(df[\"lower\"].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = pad_sequences(sequences, maxlen=max_len)\n\nX_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.2, random_state=42,stratify=df.CATEGORY)\n\nepochs = 10\nemb_dim = 150\nbatch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n\nmodel = Sequential()\nmodel.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.15, recurrent_dropout=0.15))\n\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())\n\nfilepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test,y_test),callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = plt.figure(figsize=(12,5))\nplt.plot(history.history['loss'],'r',linewidth=3.0)\nplt.plot(history.history['val_loss'],'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n#fig1.savefig('loss.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2=plt.figure(figsize=(12,5))\nplt.plot(history.history['acc'],'r',linewidth=3.0)\nplt.plot(history.history['val_acc'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)\n#fig2.savefig('accuracy.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('** Results for LSTM Model **\\n')\npredictions = model.predict_classes(X_test)\nprint(\"Accuracy score: \", accuracy_score(y_test.argmax(1), predictions)) # to convert OHE vector back to label\nprint(\"F1 score: \", f1_score(y_test.argmax(1), predictions, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION- Logistic Regression has the best Accuracy & F1-score."},{"metadata":{},"cell_type":"markdown","source":"# Further improvements-Â¶\n**1. Extensive Hyper-parameter tuning**\n\n**2. Use Pre-trained models (Eg- BERT)**"},{"metadata":{},"cell_type":"markdown","source":"# Thank You"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}