{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndf = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv').loc[:,['math score','reading score','writing score']]\ndf.columns = ['Math', 'Reading', 'Writing']\n\n# plot the original data:\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nxs = df['Math']\nys = df['Reading']\nzs = df['Writing']\ncolor_control = (df['Math'] + df['Reading'] + df['Writing']) / 300\nax.scatter(xs, ys, zs, marker='o', c=color_control, cmap='inferno', edgecolors='black')\nax.set_xlabel('Math')\nax.set_ylabel('Reading')\nax.set_zlabel('Writing')\nax.set_title('scores in math, reading and writing')\nplt.tight_layout()\nplt.savefig('3d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can already see that an almost linear relation between the three variables exists, because they more or less form a line in 3D space!"},{"metadata":{},"cell_type":"markdown","source":"# goal: reduce the 3 'score' columns to 2 principal components\nFundamental idea of dimensionality reduction via PCA:\n\nGiven a matrix X containing N many observations in its rows and M many numerical features in its columns, the eigenvalues of the corresponding (unified) eigenvectors determine, how much the data is spread along the corresponding eigenvector-axis. We can use these so called 'principal axes' to obtain M_pca < M many principal axes with the M_pca biggest eigenvalues instead of the original features.\n\nFirstly, we have to standardize the columns by subtracting the mean (to obtain zero means) and, if necessary, dividing by the standard deviation, in order to bring the columns on the same scale with var(col) = 1 for all columns. This works, because the std has the same unit as the values within the column. Thus, dividing by the std provides plane numbers.\n\nAfterwards we have to calculate the covariance matrix using covmat(X) = c * X.T @ X.\nWhere c is any constant factor.\n\nWe can obtain the (already sorted in descending order) eigenvalues and their corresponding eigenvectors of the covariance matrix of X via eigendecomposition.\n\nLastly, we can easily choose which principal components we want to keep.\n\n\nDownside of PCA: PCA merges the original features to fewer principal components and thus is less interpretable."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = df\nnum_pc = 2\n\n# step 1: standardize the data and obtain X:\n# per columns: subtract mean along the column\ndf_pca = df_pca.apply(lambda col: col - col.mean(), axis=0)\n# per column: divide by standard deviation along the column \n# (for practice; even though we already have scores between 0 and 100)\ndf_pca = df_pca.apply(lambda col: col / col.std(), axis=0)\nX = df_pca.values\n# step 2: obtain the covariance matrix of X (up to a constant factor) via X.T@X\ncovmat = X.T @ X\n# step 3_ obtain the eigenvalues via eigendecomposition of the covariance matrix\ne_val, e_vec = np.linalg.eig(covmat)\n\n# obtain the reduced data by X @ e_vec*, where e_vec*:= the matrix of eigenvectors containing only the\n# eigenvectors corresponding to the N largest eigenvalues:\npc = X @ e_vec[:, :num_pc]\n\n# amount of variability which is covered by each pc:\nvar_pc = np.round(e_val / np.sum(e_val), 3)\n\n\n\n# plot the resulting principal components:\nwith plt.style.context('dark_background'):\n    fig = plt.figure(figsize=(10,10))\n    ax1 = fig.add_subplot(211)\n    ax2 = fig.add_subplot(212)\n    df_plot = pd.DataFrame(pc, columns=['PC_1', 'PC_2'])\n    df_plot.plot(kind='scatter', x='PC_1', y='PC_2', alpha=0.5, c='cyan', ax=ax1, edgecolor='black')\n    df_plot.plot.hexbin(x='PC_1', y='PC_2', gridsize=(50, 8), cmap=\"plasma\", \n                        edgecolors='black', mincnt=1, ax=ax2)\n\n    ax1.set_xlabel(f'PC 1: describes {var_pc[0]}/1 of the variability')\n    ax1.set_ylabel(f'PC 2: {var_pc[1]}/1 of the variability')\n    ax1.set_title('Scatterplot of PC1 and PC2')\n    ax1.axis('equal')\n\n    ax2.axis('equal')\n    ax2.set_title('Heat Map of PC1 and PC2') \n\n    fig.tight_layout()\n    \n    plt.savefig('pca')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually, one would decide how many principal components one wants to keep by the values displayed in the Scree plot, i.e. a plot displaying the amount of variability explained by each principal axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_scree = plt.figure()\nax_scree = plt.axes()\npc_num = np.arange(1, len(var_pc)+1)\nx_ticks = np.char.add(['PC ']*3, pc_num.astype('str'))\nax_scree.plot(x_ticks, var_pc*100, \n              '-s', color='black',\n              markersize=10, linewidth=3,\n              markerfacecolor='white',\n              markeredgecolor='black',\n              markeredgewidth=3)\n\nax_scree.set_xlabel('Principal Component')\nax_scree.set_ylabel('Covered Variability in %')\nax_scree.set_title('Scree Plot of the PCA')\nax_scree.set_yticks(np.arange(0, 110, 10))\n\nax_scree.text('PC 1', var_pc[0]*100 + 4, f'{round(var_pc[0]*100, 1)}%')\nax_scree.text('PC 2', var_pc[1]*100 + 4, f'{round(var_pc[1]*100, 1)}%')\nax_scree.text('PC 3', var_pc[2]*100 + 4, f'{round(var_pc[2]*100, 1)}%')\n\nplt.savefig('Screeplot.png')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It turns out that it definitely makes sense to apply PCA to reduce the dimensions, since even reducing it to just one dimension covers more than 90% of the overall variability of the data. It would even make sense to discard all dimensions but one."},{"metadata":{},"cell_type":"markdown","source":"# what else does this result tell us?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: since we standardized each column, each variance equals 1 and thus covmat[0][0] is the scaling factor. \n# of the approx covariance matrix\ncovmat_real = covmat/covmat[0][0]\nnumerical_cols = df.columns[-3:] \n\n\nheatmap = sns.heatmap(covmat_real,\n                      square=True,\n                      cbar=True,\n                      annot=True,\n                      xticklabels=numerical_cols,\n                      yticklabels=numerical_cols)\n                      \nplt.title('Covariance Matrix = Correlation Matrix (in this case)')\nplt.savefig('covarianceAkaCorrelation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the covariances of all 3 features are positive, we can determine, that high values of one feature correlate with high values of the other features and vice versa. Note that the (pearson) correlation coefficients can be calculated by dividing the covariance of two variables by the product of their standard deviations. Since we already standardized the columns, these covariances equal the correlation coefficients.\n\nOne can already see this relationship in Figure 1. This property allows us to reduce to two dimensions (or even one dimension) without losing too much information. \n\nAccording to this dataset, students are very  likely to obtain similar scores in all 3 disciplines."},{"metadata":{},"cell_type":"markdown","source":"Thank you for reading my short notebook about PCA and the relatin between covariance and correlation!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}