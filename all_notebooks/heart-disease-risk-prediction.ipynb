{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns #plots\nimport matplotlib.pyplot as plt #more plots\nimport sklearn as sk #machine learning\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **About this kernel  **  \nBefore I start, I want to establish my goals for this dataset. My end goal is to predict the risk of patient developing a cardio vascular disease in the next ten years. I will start by modifying the dataset if needed to remove null data or make categorical data numerical. Next, I will make visualizations to understand the data pool I am using and the prevalence of each feature. Then, I'm going to being using models to test the importance of each feature (feature engineering). When I find the optimal set of features, I will use them in many machine learning and maybe some deep learning models and compare their effectiveness when predicting on the test data."},{"metadata":{},"cell_type":"markdown","source":"**Dataset Description**  \nWorld Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant/risk factors of heart disease as well as predict the overall risk using logistic regression.\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. **Basic Data Analysis**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#loading the data in\ndata = pd.read_csv(\"/kaggle/input/logistic-regression-heart-disease-prediction/framingham_heart_disease.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before anything is done to the data, we must take a look at the features and values included."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are what all the features are described as:\n* male: 0 = Female; 1 = Male\n* age: Age at exam time.\n* education: 1 = Some High School; 2 = High School or GED; 3 = Some College or Vocational School; 4 = college\n* currentSmoker: 0 = nonsmoker; 1 = smoker\n* cigsPerDay: number of cigarettes smoked per day (estimated average)\n* BPMeds: 0 = Not on Blood Pressure medications; 1 = Is on Blood Pressure medications\n* prevalentStroke\n* prevalentHyp\n* diabetes: 0 = No; 1 = Yes\n* totChol: mg/dL\n* sysBP: mmHg\n* diaBP: mmHg\n* BMI: Body Mass Index calculated as: Weight (kg) / Height(meter-squared)\n* heartRate: Beats/Min (Ventricular)\n* glucose: mg/dL\n* TenYearCHD: If the patient has had congestive heart failure in the last ten years. This will be what my models will be predicting."},{"metadata":{},"cell_type":"markdown","source":"There are 16 features in total. I predict that the gender, total cholesterol, TenYearCHD and age will be the most useful in predicting if a person has/will have congestive heart failure."},{"metadata":{"trusted":true},"cell_type":"code","source":"# listing how many null values are in the dataset\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Analyzing Features using Visualizations"},{"metadata":{},"cell_type":"markdown","source":"To begin, I plotted histograms of all the feature to visualize the distribution of data. I'm comparing features using a heatmap."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating histograms to visualize all the data\nfig = plt.figure(figsize = (40,40))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nax = fig.gca()\ndata.hist(ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a heatmap to find the correlation between each feature\nsns.set(font_scale=3)\ndef plot_corr( df ):\n    corr = df.corr()\n    _, ax=plt.subplots( figsize=(50,25) )\n    cmap = sns.diverging_palette( 240 , 10 , as_cmap = True)\n    _ = sns.heatmap(corr,cmap=cmap,square=True, cbar_kws = {'shrink': .9}, fmt= '.1f', ax=ax, annot=True)\n    \nplot_corr(data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Cleaning the dataset\n\nWe have to find a way to effectively remove or replace the null values so that models can be applied."},{"metadata":{},"cell_type":"markdown","source":"The education column could be very subjective, has many null values and has no correlation to the other feature, so I'm going to drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['education'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm not sure what to do with the glucose column since so many values are missing, but glucose is very important when predicting heart disease so I'll impute the values with a SimpleImputer. Imputation is when you replace the null values with the mean or any other value. Total cholesterol, BMI and cigs per day is useful, so we'll impute them too."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nmy_imputer.fit(data)\nimputed_data = pd.DataFrame(my_imputer.transform(data))\n\n# Imputation removed column names; put them back\nimputed_data.columns = data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the training and testing data"},{"metadata":{},"cell_type":"markdown","source":"In order to actually test the accuracy of the models, we must set some data aside for testing and some data to train the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = imputed_data['TenYearCHD']\nX = imputed_data.drop(['TenYearCHD'], axis = 1)\n\n# divide\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a set of features that are standardized\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_std = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_std = pd.DataFrame(scaler.transform(X_test))\n\nX_train_std.columns = X_train.columns\nX_test_std.columns = X_test.columns\n\nX_train_std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Normalization"},{"metadata":{},"cell_type":"markdown","source":"A problem with this dataset that's also mentioned in another kernel (https://www.kaggle.com/lauriandwu/machine-learning-heart-disease-framingham/comments) is that there is no balance between people with and without heart disease, so models will be more likely to predict that a patient doesn't have heart disease without a thourough analysis with this data. I'm going to try to deal with this unbalance by splitting the training and testing data and then balancing the training data. I will undersample to balance the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle df\nshuffled_df = imputed_data.sample(frac=1,random_state=4)\n\n# Put all the fraud class in a separate dataset.\nCHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n\n#Randomly select 492 observations from the non-fraud (majority class)\nnon_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=611,random_state=42)\n\n# Concatenate(join) both dataframes again\nnormalized_df = pd.concat([CHD_df, non_CHD_df])\n\n# plot new count\nsns.countplot(normalized_df.TenYearCHD, palette=\"OrRd\")\nplt.box(False)\nplt.xlabel('Heart Disease No/Yes',fontsize=11)\nplt.ylabel('Patient Count',fontsize=11)\nplt.title('Count Outcome Heart Disease after Resampling\\n')\n#plt.savefig('Balance Heart Disease.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When you get to the model training section, you may notice that I don't use these normalized values for training. I tried using the normalized values and the 4 types of accuracies were almost around the same percentages, which is good, however all the accuracies were between 50%~70%. So, I decided to not use the normalized data."},{"metadata":{},"cell_type":"markdown","source":"# 6. Feature selection"},{"metadata":{},"cell_type":"markdown","source":"The features in this dataset seem all important, however reducing features to the top 5 or 10 can increase the accuracy score. I'm going to score models with limited features and full feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 6 features\nselect_feature = SelectKBest(chi2, k=10).fit(X_train, y_train)\n\ndfscores = pd.DataFrame(select_feature.scores_)\ndfcolumns = pd.DataFrame(X_train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\ntop_featureScores = featureScores.nlargest(10,'Score')\nprint(top_featureScores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing feature selection\nplt.figure(figsize=(20,5))\nsns.barplot(x='Specs', y='Score', data=featureScores, palette = \"GnBu_d\")\nplt.title('Feature importance', fontsize=16)\nplt.xlabel('\\n Features', fontsize=14)\nplt.ylabel('Importance \\n', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to compare using 6 features and all features in each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a dataset with only the top 10 features\nbest_feature_names = top_featureScores['Specs']\n\nX_best_train = X_train[best_feature_names]\nX_best_test = X_test[best_feature_names]\ny_best_train = y_train[best_feature_names]\ny_best_test = y_test[best_feature_names]\n\nX_best_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Applying models\n***\nThe models used:\n* Linear regression\n* Random forest\n* KNN\n* XGBoost  \n\nI will be using a grid search to tune the parameters of each model."},{"metadata":{},"cell_type":"markdown","source":"## 1. **Logistic Regression**"},{"metadata":{},"cell_type":"markdown","source":"### Tuning the parameters of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLR_model= LogisticRegression()\n\ntuned_parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n              'penalty':['l1','l2']\n                   }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using a gridsearch to tune parameters\nfrom sklearn.model_selection import GridSearchCV\n\nGS = GridSearchCV(LR_model, tuned_parameters,cv=10)\n\nGS.fit(X_train_std, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(GS.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making a new model with the best parameters\nLR_model= LogisticRegression(C=0.1, penalty='l2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardized predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\n#Fitting the model to the standardized data\nLR_model.fit(X_train_std, y_train)\n\n#Using the model to predict\ny_pred = LR_model.predict(X_test_std)\ny_pred\n\n#Getting the accuracies\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for LogReg is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for LogReg is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for LogReg is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for LogReg is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-standardized predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#Fitting the model to the non standardized data\nLR_model.fit(X_train, y_train)\n\n#Using the model to predict\ny_pred = LR_model.predict(X_test)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for LogReg is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for LogReg is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for LogReg is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for LogReg is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limited Feature Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#Fitting the model to the standardized data\nLR_model.fit(X_best_train, y_train)\n\n#Using the model to predict\ny_pred = LR_model.predict(X_best_test)\ny_pred\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for LogReg is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for LogReg is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for LogReg is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for LogReg is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standardized data has a slightly higher accuracy and the limited feature model has a lower accuracy than the standardized model. Overall, the full, standardized model predicts the best with linear regression."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2. Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### Tuning Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#initiating a new model\nrf = RandomForestClassifier(random_state=1)\n\n#making a list of parameters for the grid search to compare\ntuned_parameters = {'n_estimators': [100, 500, 1000]}\n\n#Making a grid search model\nGS_rf=GridSearchCV(rf, tuned_parameters, cv=10)\n\n#Fitting the grid search\n# GS_rf.fit(X_train_std, y_train)\n\n#Printing the best features\n# print(GS_rf.best_params_)\n\n# I commented the last lines of code out so that I wouldn't have to run it again since I already know the best paramters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding the best parameters\nrf = RandomForestClassifier(random_state=1, n_estimators = 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nrf.fit(X_train_std, y_train)\n\n#making predictions\ny_pred = rf.predict(X_test_std)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for RandomForest is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for RandomForest is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for RandomForest is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for RandomForest is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nrf.fit(X_train, y_train)\n\n#making predictions\ny_pred = rf.predict(X_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for RandomForest is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for RandomForest is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for RandomForest is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for RandomForest is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limited"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nrf.fit(X_best_train, y_train)\n\n#making predictions\ny_pred = rf.predict(X_best_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for RandomForest is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for RandomForest is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for RandomForest is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for RandomForest is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. KNN"},{"metadata":{},"cell_type":"markdown","source":"### Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n#Initializing the model\nknn = KNeighborsClassifier()\n\n#The parameters\nmy_params={'n_neighbors': [1,2,5,10,15, 20, 21, 25, 30]}\n\n#creating the gridsearch\nGS_knn = GridSearchCV(knn, my_params, cv=10)\n\n#fitting the model\nGS_knn.fit(X_train_std, y_train)\n\n#finding the best parameters\nprint(GS_knn.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a tuned model\nknn = KNeighborsClassifier(n_neighbors=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nknn.fit(X_train_std, y_train)\n\n#making predictions\ny_pred = knn.predict(X_test_std)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for KNN is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nknn.fit(X_train, y_train)\n\n#making predictions\ny_pred = knn.predict(X_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for KNN is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limited"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nknn.fit(X_best_train, y_train)\n\n#making predictions\ny_pred = knn.predict(X_best_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for KNN is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the KNN model, the limited feature model has the highest accuracy and is overall the best model."},{"metadata":{},"cell_type":"markdown","source":"## 4. XGBoost"},{"metadata":{},"cell_type":"markdown","source":"### **Tuning parameters**"},{"metadata":{},"cell_type":"markdown","source":"I'm not using gridsearch to tune the XGBoost model since it kind of self-tunes. I'm going to change some parameters and see if the accuracy increases. Here's more info on the XGBoost model: https://www.kaggle.com/dansbecker/xgboost."},{"metadata":{},"cell_type":"markdown","source":"### Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n#creating a new model with experimental parameters\nXGB = XGBRegressor(n_estimators=1000, early_stopping_rounds=5, learning_state=0.01, objective='binary:hinge')\n\n#fitting the model\nXGB.fit(X_train_std, y_train)\n\n#making predcitions\ny_pred = XGB.predict(X_test_std)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for XGB is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for XGB is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for XGB is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? True Positive Rate = True Positive/actual yes\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for XGB is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new model with experimental parameters\nXGB = XGBRegressor(n_estimators=1000, early_stopping_rounds=5, learning_state=0.05, objective='binary:hinge')\n\n#fitting the model\nXGB.fit(X_train, y_train)\n\n#making predcitions\ny_pred = XGB.predict(X_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for XGB is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for XGB is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for XGB is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for XGB is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limited"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new model with experimental parameters\nXGB = XGBRegressor(n_estimators=1000, early_stopping_rounds=5, learning_state=0.05, objective='binary:hinge')\n\n#fitting the model\nXGB.fit(X_best_train, y_train)\n\n#making predcitions\ny_pred = XGB.predict(X_best_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for XGB is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for XGB is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for XGB is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for XGB is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, the XGBoost does not perform better than the other models. Out of the three types of data, the limited one has the highest accuracy."},{"metadata":{},"cell_type":"markdown","source":"## 5. Support Vector Machine (SVM)"},{"metadata":{},"cell_type":"markdown","source":"I decided to test if the normalized data would improve all other scores other than accuracy for the SVM. It did, but the accuracy score went form 84% to 65.3%, so I decided not to use the normalized data."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ny = normalized_df['TenYearCHD']\nX = normalized_df.drop(['TenYearCHD'], axis = 1)\n\n# divide\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=14)\n\n#Creating a set of features that are standardized\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_std = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_std = pd.DataFrame(scaler.transform(X_test))\n\nX_train_std.columns = X_train.columns\nX_test_std.columns = X_test.columns\n\nX_train_std\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n#initialize model\nsvm = SVC()\n\n#fit model\nsvm.fit(X_train_std, y_train)\n\n#make predictions\ny_pred = svm.predict(X_test_std)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for SVM is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-Standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize model\nsvm = SVC()\n\n#fit model\nsvm.fit(X_train, y_train)\n\n#make predictions\ny_pred = svm.predict(X_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for SVM is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limited"},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize model\nsvm = SVC()\n\n#fit model\nsvm.fit(X_best_train, y_train)\n\n#make predictions\ny_pred = svm.predict(X_best_test)\n\n# check accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n\n# f1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n\n# Precision score\nprecision = precision_score(y_test, y_pred)\nprint(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n\n# recall score\nrecall = recall_score(y_test, y_pred)\nprint(f\"The recall score for SVM is: {round(recall,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"I'm going to compare the accuracies from the best of each model using a bar plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.font_manager\nmatplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n\n# Making a dataframe of the accuracies\na = {'Random Forest Classifier': [84.89], 'K-Nearest Neighbours': [84.89], 'Logistic Regression': [85.5], 'XGBoost':[83.3], 'SVM':[84.7]}\naccuracies = pd.DataFrame(data=a)\n\n# making bar plot comparing the accuracies of the models\nsns.set(font_scale=1)\nax = accuracies.plot.bar(\n    figsize= (10, 5),\n    fontsize=14)\nplt.xticks(rotation=0, fontsize=14)\nplt.xlabel('Models', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nx_labels = ['']\nxticks = [-0.20, -0.1, 0.02, 0.14, ]\nax.set_xticks(xticks)\nax.set_xticklabels(x_labels, rotation=0)\naxbox = ax.get_position()\nplt.legend(loc = (axbox.x0 + 0.65, axbox.y0 + 0.70), fontsize=14)\nplt.title('Accuracies of Each Model Predicting CHD')\nax.set_facecolor('xkcd:white')\nax.set_facecolor(('#ffffff'))\nax.spines['left'].set_color('black')\nax.spines['bottom'].set_color('black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Will you develop CHD?"},{"metadata":{},"cell_type":"markdown","source":"Enter you information as the variables to see if you'll get CHD. I've inputted my values (and average values for values I could not get at the moment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictors = []\nparameters=['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male']\n\nage = 17 ##\"Patient's age: >>>\nmy_predictors.append(age)\nmale = 0#\"Patient's gender. male=1, female=0:\nmy_predictors.append(male)\ncigsPerDay =0 ##\"Patient's smoked cigarettes per day:\nmy_predictors.append(cigsPerDay)\nsysBP = 100##\"Patient's systolic blood pressure:\nmy_predictors.append(sysBP)\ndiaBP = 80##\"Patient's diastolic blood pressure\nmy_predictors.append(diaBP)\ntotChol = 130##\"Patient's cholesterin level:\nmy_predictors.append(totChol)\nprevalentHyp =0 ##\"Was Patient hypertensive? Yes=1, No= 0\nmy_predictors.append(prevalentHyp)\ndiabetes = 0##\"Did Patient have diabetes? Yes=1, No=0\nmy_predictors.append(diabetes)\nglucose = 100##\"What is the Patient's glucose level?\nmy_predictors.append(diabetes)\nBPMeds = 0##\"Has Patient been on Blood Pressure Medication? Yes=1, No=0\nmy_predictors.append(BPMeds)\n\n#adding the data to the dataset\nmy_data = dict(zip(parameters, my_predictors))\nmy_df = pd.DataFrame(my_data, index=[0])\n\nmy_y_pred = knn.predict(my_df)\nprint('Result:')\nif my_y_pred == 1:\n    print(\"The patient will develop a heart disease.\")\nif my_y_pred == 0:\n    print(\"The patient will not develop a heart disease.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yay! I won't get heart disease."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}