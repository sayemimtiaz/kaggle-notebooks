{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Table of Contents</h1>\n\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#1\" role=\"tab\" aria-controls=\"settings\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#2\" role=\"tab\" aria-controls=\"settings\">2. Exploratory Data Analysis <span class=\"badge badge-primary badge-pill\">2</span></a>\n<a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. Missing Values<span class=\"badge badge-primary badge-pill\">3</span></a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. Approaching Categorical Features<span class=\"badge badge-primary badge-pill\">4</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. Evaluation Metrics<span class=\"badge badge-primary badge-pill\">5</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6. Model<span class=\"badge badge-primary badge-pill\">6</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. Hyperparameter Tuning<span class=\"badge badge-primary badge-pill\">7</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">8. Voting Classifier<span class=\"badge badge-primary badge-pill\">8</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#9\" role=\"tab\" aria-controls=\"settings\">9. References<span class=\"badge badge-primary badge-pill\">9</span></a>"},{"metadata":{},"cell_type":"markdown","source":"In this notebook i will try to predict who want to leave credit card services. So that he or she can proactively go to the customer to provide them better services and turn customer's decisions in the opposite direction  "},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n\n\nThe process involves applying for a credit card, getting approved, meeting a minimum spend within a set amount of time, earning a large welcome bonus, and canceling the card before the next annual fee is due. Once this is complete, the process is simply repeated again and again, hence the term churning.\n\n\n![image.png](https://smartcdn.prod.postmedia.digital/financialpost/wp-content/uploads/2016/04/0428cards.jpg)\n"},{"metadata":{},"cell_type":"markdown","source":"## Import important libraries and packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, log_loss, confusion_matrix\nfrom xgboost import XGBClassifier\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nimport missingno\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndf = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About Features in data\n\n *   CLIENTNUM : Client number. Unique identifier for the customer holding the account\n *   Attrition_Flag : Internal event (customer activity) variable - if the account is closed then 1 else 0             \n *   Customer_Age : Demographic variable - Customer's Age in Years              \n *   Gender : Demographic variable - M=Male, F=Female                   \n *   Dependent_count : Demographic variable - Number of dependents           \n *   Education_Level : Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)            \n *   Marital_Status : Demographic variable - Married, Single, Divorced, Unknown           \n *   Income_Category : Demographic variable - Annual Income Category of the account holder (< 40K, 40K - 60K, 60K - 80K, 80K-120K, >)           \n *   Card_Category : Product Variable - Type of Card (Blue, Silver, Gold, Platinum)             \n *   Months_on_book : Period of relationship with bank           \n *  Total_Relationship_Count : Total no. of products held by the customer \n *  Months_Inactive_12_mon : No. of months inactive in the last 12 months      \n *  Contacts_Count_12_mon : No. of Contacts in the last 12 months    \n *  Credit_Limit : Credit Limit on the Credit Card              \n *  Total_Revolving_Bal : Total Revolving Balance on the Credit Card       \n *  Avg_Open_To_Buy : Open to Buy Credit Line (Average of last 12 months)           \n *  Total_Amt_Chng_Q4_Q1 : Change in Transaction Amount (Q4 over Q1)     \n *  Total_Trans_Amt : Total Transaction Amount (Last 12 months)            \n *  Total_Trans_Ct : Total Transaction Count (Last 12 months)           \n *  Total_Ct_Chng_Q4_Q1 : Change in Transaction Count (Q4 over Q1)       \n *  Avg_Utilization_Ratio : Average Card Utilization Ratio    "},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Exploratory Data Analysis</h1><a id = \"2\" ></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.countplot(x = df['Attrition_Flag'], edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.countplot(x = df['Card_Category'], hue = df['Gender'], edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.countplot(x= df['Education_Level'], edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.countplot(x = df['Marital_Status'], edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.countplot(x = df['Income_Category'], edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Missing Values</h1><a id = \"3\" ></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.bar(df, color = 'red', figsize = (15,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot we can say this data do not have any missing values but inplace of missing values many attributes have value \"Unknown\" which is same as missing values. Giving Nan values a seprate class (Unknown) in categorical variables is a great technique to deal with missing values in categorical variables "},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Approaching Categorical Features</h1><a id = \"4\" ></a>\n\nCategorical variables/features are any feature type can be classified into two major\ntypes:\n*  Nominal\n*  Ordinal\n\n**Nominal variables** are variables that have two or more categories which do not\nhave any kind of order associated with them. For example, if gender is classified\ninto two groups, i.e. male and female, it can be considered as a nominal variable.\n\n**Ordinal variables** on the other hand, have “levels” or categories with a particular\norder associated with them. For example, an ordinal categorical variable can be a\nfeature with three different levels: low, medium and high. Order is important.\n\nOrdinal variables in this data are :-\n\n1. Income_Category\n2. Card_Category\n3. Education_Level\n\nIn this notebook i will seprately encode ordinal variables first"},{"metadata":{"trusted":true},"cell_type":"code","source":"Income_Category_map = {\n    'Less than $40K' : 0,\n    '$40K - $60K'    : 1,\n    '$60K - $80K'    : 2,\n    '$80K - $120K'   : 3,\n    '$120K +'        : 4,\n    'Unknown'        : 5\n}\n\n\nCard_Category_map = {\n    'Blue'     : 0,\n    'Silver'   : 1,\n    'Gold'     : 2,\n    'Platinum' : 3\n}\n\n\nAttrition_Flag_map = {\n    'Existing Customer' : 0,\n    'Attrited Customer' : 1\n}\n\nEducation_Level_map = {\n    'Uneducated'    : 0,\n    'High School'   : 1,\n    'College'       : 2,\n    'Graduate'      : 3,\n    'Post-Graduate' : 4,\n    'Doctorate'     : 5,\n    'Unknown'       : 6\n}\n\n\ndf.loc[:, 'Income_Category'] = df['Income_Category'].map(Income_Category_map)\ndf.loc[:, 'Card_Category'] = df['Card_Category'].map(Card_Category_map)\ndf.loc[:, 'Attrition_Flag'] = df['Attrition_Flag'].map(Attrition_Flag_map)\ndf.loc[:, 'Education_Level'] = df['Education_Level'].map(Education_Level_map)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label Encoding** refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.\n![](https://ekababisong.org/assets/seminar_IEEE/LabelEncoder.png)  \n\nWe can do label Encoding From LabelEncoder of scikit-Learn but to do so first we have to impute missing values in data "},{"metadata":{},"cell_type":"markdown","source":"In this Notebook i am going to use scikit-Learn LabelEncoder Due to following reasons\n\n1. Label Encoder encode data on basis of count and this data do not have lots of ordinal features\\ \n    \n2. To use label encoder first we have to create NULL values as new category and in Our data this task is already done"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlbe = LabelEncoder()\n\ncat_cols = [x for x in df.columns if df[x].dtype == 'object']\n\nfor c in cat_cols:\n    df.loc[:, c] = lbe.fit_transform(df.loc[:, c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac = 1).reset_index(drop = True) #To shuffle data\n\nX = df.drop(['Attrition_Flag', 'CLIENTNUM'], axis = 1)\ny = df.Attrition_Flag","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Evaluation Metrics</h1><a id = \"5\" ></a>\n\nI think before selecting an optimal model for given data first we have to analayze target feature. Target Feature can be discrete in case of classification problem or continuous in case of Regression Problem\n\n- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n\n     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n\n\n![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.countplot(x = y, edgecolor = 'black', saturation = 0.55)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the target is **skewed** and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_auc_curve(X, y, model):\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 11)\n    \n    model.fit(x_train, y_train)\n    y_predict = model.predict_proba(x_test)\n    y_predict_proba = y_predict[:, 1]\n    \n    fpr , tpr, _ = roc_curve(y_test, y_predict_proba)\n    \n    plt.figure(figsize = (15,8))\n    plt.plot(fpr, tpr, 'b+', linestyle = '-')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.fill_between(fpr, tpr, alpha = 0.5 )\n    auc_score = roc_auc_score(y_test, y_predict_proba)\n    plt.title(f'ROC AUC Curve having \\n AUC score : {auc_score}')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# funtion to plot learning curves\n\ndef plot_learning_curve(X, Y, model):\n    \n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 11)\n    train_loss, test_loss = [], []\n    \n    for m in range(100,len(x_train),100):\n        \n        model.fit(x_train.iloc[:m,:], y_train[:m])\n        y_train_prob_pred = model.predict_proba(x_train.iloc[:m,:])\n        train_loss.append(log_loss(y_train[:m], y_train_prob_pred))\n        \n        y_test_prob_pred = model.predict_proba(x_test)\n        test_loss.append(log_loss(y_test, y_test_prob_pred))\n        \n    plt.figure(figsize = (15,8))\n    plt.plot(train_loss, 'r-+', label = 'Training Loss')\n    plt.plot(test_loss, 'b-', label = 'Test Loss')\n    plt.xlabel('Number Of Batches')\n    plt.ylabel('Log-Loss')\n    plt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Model</h1><a id = \"6\" ></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross Validation Score\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndef compute_CV(X, y, model, k):\n    \n    kf = StratifiedKFold(n_splits = k)\n    auc_scores = []\n    i = 0\n    \n    for idx in kf.split(X = X, y = y):\n        \n        train_idx, val_idx = idx[0], idx[1]\n        \n        i += 1 \n        X_train = X.iloc[train_idx, :]\n        y_train = y[train_idx]\n        X_val = X.iloc[val_idx, :]\n        y_val = y[val_idx]\n        \n        model.fit(X_train, y_train)\n        y_predict = model.predict_proba(X_val)\n        y_predict_prob = y_predict[:,1]\n        \n        auc_score = roc_auc_score(y_val, y_predict_prob)\n        print(f'AUC Score of {i} Fold is : {auc_score}')\n        auc_scores.append(auc_score)\n    print('-----------------------------------------------')\n    print(f'Average AUC Score of {k} Folds is : {np.mean(auc_scores)}')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression)** is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n\n\n**[Logistic regression Deep Intuition](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)**\n\n![](https://i.stack.imgur.com/bQVzF.png)\n\n**[Sklearn User guide for Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nstd = StandardScaler()\n\ndata1 = std.fit_transform(X)\n\nX_std = pd.DataFrame(data1, columns = X.columns)\n\nclf1 = LogisticRegression()\n\ncompute_CV(X_std, y, clf1, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[Support Vector Machine (SVM)](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)** : The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n\n**[Support Vector Machine (SVM) Deep Intuition](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)**\n\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)\n\n**[Sklearn User guide for Support Vector Machine (SVM)](https://miro.medium.com/max/921/1*06GSco3ItM3gwW2scY6Tmg.png)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\n\nfrom sklearn.svm import SVC\n\nclf2 = SVC(probability = True, C = 100, kernel = 'rbf')\n\ncompute_CV(X_std, y, clf2, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Hyperparameter Tunning</h1><a id = \"7\" ></a>\n\nThe parameters that the model has here are known as hyper-parameters, i.e. the parameters that control the training/fitting process of the model.\n\nLet’s say there are three parameters a, b, c in the model, and\nall these parameters can be integers between 1 and 10. A “correct” combination of\nthese parameters will provide you with the best result. So, it’s kind of like a suitcase\nwith a 3-dial combination lock. However, in 3 dial combination lock has only one\ncorrect answer. The model has many right answers. So, how would you find the\nbest parameters? A method would be to evaluate all the combinations and see which\none improves the metric. We go through all the parameters from 1 to 10. So, we have a\ntotal of 1000 (10 x 10 x 10) fits for the model. Well, that might be expensive because\nthe model can take a long time to train. Let's visit some efficient methods\n\n* [Random Search](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/) : Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\\\n   [Scikit-Learn User Guide for Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n   \n   \n* [Grid Search](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/) : Define a search space as a grid of hyperparameter values and evaluate every position in the grid.\\\n   [Scikit-Learn User Guide for Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n   \n   \nVisit below given link for deep intuition of Random search and Grid search\n\n[Hyperparameter Optimization With Random Search and Grid Search](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/)\n\nIn this notebook i am using **Bayesian optimization with gaussian process**\n\n[Hyperparameter Tuning With Bayesian Optimization](https://machinelearningmastery.com/what-is-bayesian-optimization/)\n\n"},{"metadata":{},"cell_type":"markdown","source":"Bayesian optimization algorithm need a function they can optimize. Most of the time, it’s about the minimization of this function, like we minimize loss.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(params, param_names, x, y, model):\n   \n\n    # convert params to dictionary\n    params = dict(zip(param_names, params))\n\n    # initialize model with current parameters\n    \n    if model == 'rf':\n        clf = RandomForestClassifier(**params)\n    \n    elif model == 'xgb':\n        clf = XGBClassifier(tree_method = 'hist', **params)\n       \n    # initialize stratified k fold\n    kf = StratifiedKFold(n_splits = 5)\n    \n    i = 0\n    \n    # initialize auc scores list\n    auc_scores = []\n    \n    #loop over all folds\n    for index in kf.split(X = x, y = y):\n        train_index, test_index = index[0], index[1]\n        \n        \n        \n        x_train = x.iloc[train_index,:]\n        y_train = y[train_index]\n\n\n        x_test = x.iloc[test_index,:]\n        y_test = y[test_index]\n        \n        #fit model\n        clf.fit(x_train, y_train)\n        \n        y_pred = clf.predict_proba(x_test)\n        y_pred_pos = y_pred[:,1]\n        \n        auc = roc_auc_score(y_test, y_pred_pos)\n        print(f'Current parameters of fold number {i} -> {params}')\n        print(f'AUC score of test {i} f {auc}')\n\n        i = i+1\n        auc_scores.append(auc)\n        \n    return -1 * np.mean(auc_scores)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[Random Forest Classifier](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)** : Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction\n\n**[Random Forest Classifier Deep Intuition](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)**\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png)\n\n**[Sklearn User guide for Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)**"},{"metadata":{},"cell_type":"markdown","source":"So, let’s say, you want to find the best parameters for best accuracy and obviously, the more the accuracy is better. Now we cannot minimize the accuracy, but we can minimize it when we multiply it by -1. This way, we are minimizing the negative of accuracy, but in fact, we are maximizing accuracy. Using Bayesian optimization with gaussian process can be accomplished by using [gp_minimize function from scikit-optimize (skopt) library](https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html). Let’s take a look at how we can tune the parameters of our xgboost model using this\nfunction."},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a parameter space\n\nparam_spaces = [space.Integer(100, 2000, name = 'n_estimators'),\n                space.Integer(5,25, name = 'max_depth')\n]\n\n# make a list of param names this has to be same order as the search space inside the main function\nparam_names = ['n_estimators', 'max_depth']\n\n# by using functools partial, i am creating a new function which has same parameters as the optimize function except \n# for the fact that only one param, i.e. the \"params\" parameter is required. \n# This is how gp_minimize expects the optimization function to be. \n# You can get rid of this by reading data inside the optimize function or by defining the optimize function here.\n\noptimize_function = partial(optimize, param_names = param_names, x = X, y = y, model = 'rf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# output of this cell is very large that's why it is hidden\n\nresult = gp_minimize(optimize_function, dimensions = param_spaces, n_calls = 10, n_random_starts = 5, verbose = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params_rf = dict(zip(param_names, result.x))\nprint(f'Best Parameters for RandomForestClassifier are  : {best_params_rf}')\nprint(f'Best AUC score : {-result.fun}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf3 = RandomForestClassifier(**best_params_rf)\n\ncompute_CV(X, y, clf3, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[Extreme Gradient Boosting (XGBoost)](https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/)** : xgboost is short for eXtreme Gradient Boosting package. It is an efficient and scalable implementation of gradient boosting framework by (Friedman, 2001) (Friedman et al., 2000). The package includes efficient linear model solver and tree learning algorithm. It supports various objective functions, including regression, classification and ranking. The package is made to be extendible, so that users are also allowed to define their own objectives easily.\n\n![](https://miro.medium.com/max/777/1*l4PN8hyAO4fMLxUbIxcETA.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a parameter space\n\nparam_spaces = [space.Integer(100, 2000, name = 'n_estimators'),\n                space.Real(0.01,100, name = 'min_child_weight'),\n                space.Real(0.01,1000, name = 'gamma'),\n                space.Real(0.1, 1, prior = 'uniform', name = 'colsample_bytree'),\n]\n\n# make a list of param names this has to be same order as the search space inside the main function\nparam_names = ['n_estimators' ,'min_child_weight', 'gamma', 'colsample_bytree']\n\n# by using functools partial, i am creating a new function which has same parameters as the optimize function except \n# for the fact that only one param, i.e. the \"params\" parameter is required. \n# This is how gp_minimize expects the optimization function to be. \n# You can get rid of this by reading data inside the optimize function or by defining the optimize function here.\n\noptimize_function = partial(optimize, param_names = param_names, x = X, y = y, model = 'xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# output of this cell is very large that's why it is hidden\n\nresult = gp_minimize(optimize_function, dimensions = param_spaces, n_calls = 10, n_random_starts = 5, verbose = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params_xgb = dict(zip(param_names, result.x))\nprint(f\"Best Parameters for XGBClassifier are : {best_params_xgb}\")\nprint(f\"Best AUC score {result.fun}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGboost\nclf4 = XGBClassifier(use_label_encoder = False, eval_metric = 'logloss', **best_params_xgb)\n\ncompute_CV(X, y, clf4, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Voting Classifier</h1><a id = \"8\" ></a>\n\nA collection of several models working together on a single set is called an ensemble. The method is called Ensemble Learning. It is much more useful use all different models rather than any one.\n\n![](https://miro.medium.com/max/891/1*I7NsQXwyR36XK62s1iDNzQ.png)\n\nVoting is one of the simplest way of combining the predictions from multiple machine learning algorithms. Voting classifier isn’t an actual classifier but a wrapper for set of different ones that are trained and valuated in parallel in order to exploit the different peculiarities of each algorithm.\n\n\n**[Voting Classifier Deep Intuition](https://towardsdatascience.com/how-voting-classifiers-work-f1c8e41d30ff)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Voting Classifier\n\nfrom sklearn.ensemble import VotingClassifier\n\nclf5 = VotingClassifier(\n               estimators = [('lr', clf1), ('svm', clf2), ('rf', clf3), ('xgb', clf4)],\n               voting = 'soft')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms (That's why I used 4 different classifiers). This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n\nThere is a hyperparameter voting in VotingClassifier we can set it hard or soft \n\nIn hard voting, the VotingClassifier counts the number of each Class instance and then assigns to a test instance a class that was voted by majority of the classifiers.\n\nIf all classifiers in VotingClassifier are able to estimate class probabilities (i.e., they have a predict_proba() method), then we can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All we need to do is to set voting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is not the case of the SVC class by default, so we need to set its probability hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add a predict_proba() method)."},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_CV(X_std, y, clf5, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curve(X_std, y, clf5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_auc_curve(X_std, y, clf5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot confusion matrix\n\nx_train, x_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2, random_state = 11)\n\nclf5.fit(x_train, y_train)\ny_predict = clf5.predict(x_test)\n\nplt.figure(figsize = (15,8))\nsns.heatmap(confusion_matrix(y_test, y_predict), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Reference</h1><a id = \"9\" ></a>\n\n\n1. [Learning Curves](https://www.kaggle.com/nkitgupta/who-will-leave-a-job)\n\n2. [ROC AUC Curves](https://www.kaggle.com/nkitgupta/who-will-leave-a-job)\n\n3. [Evaluation Metrics](https://www.kaggle.com/nkitgupta/who-will-leave-a-job)\n\n4. [XGB Classifier](https://mran.microsoft.com/web/packages/xgboost/vignettes/xgboost.pdf)\n\n5. [Hard Voting versus Soft Voting](https://stats.stackexchange.com/questions/320156/hard-voting-versus-soft-voting-in-ensemble-based-methods)\n\n6. [Why Normalization is not required for tree based models](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/160613)\n\n7. [Kaggle notebook for Categorical Encoding Methods](https://www.kaggle.com/arashnic/an-overview-of-categorical-encoding-methods)\n\n8. [How to Implement Bayesian Optimization from Scratch in Python](https://machinelearningmastery.com/what-is-bayesian-optimization/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}