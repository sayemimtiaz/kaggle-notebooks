{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Inroduction**\nThe goal of this problem is to use the power of Machine Learning algorithms to take the dataset of past measurements of Breast Cancer and apply some analysis to understand which most features that can be asign of a Breast Cancer? Also, to predict the likelihood of future patients to be diagnosed as sick.\nSo given important measurements of a future patient we can train a ML algorithm to predict if he/she carries a Breast Cancer easily and accurately."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started \nImport the basic liberaries used in this project\n* Pandas\n* Numpy\n* seaborn\n* Matplotlib \n\nfor this project I have build a python file called `functions` and will use it in `Model Selection` section"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport warnings\n\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the datset into data \ndata = pd.read_csv('../input/data.csv')\n\n# drop uncessary columns \ndata.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True)\n\n#print the number of columns and rows\nprint(\"This dataset contains {} rows and {} columns\".format(data.shape[0], data.shape[1]))\n\n# change the target to numerical to help us in statistics\ndata['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\nIn this section of this project, I will make a cursory investigation about the dataset and provide some observations. Also I'm gooing to Familiarize myself with the data through an `explorative` process and it is a fundamental process to help us better understand the data and justify the final results. So we are going to walk through the following\n* Calculate `Statistics` for numerical features\n* Get information about the dataset and its dtypes to detect null values\n* Plot Histograms to `Visualize Feature Distributions` in the dataset(Detect Skewness)"},{"metadata":{},"cell_type":"markdown","source":"### - Calculate Statistics for numerical features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Calculate Statistics for numerical features\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the difference between the `median` and `mean` it seems there are some features that have `skewness` that need to be transformed "},{"metadata":{},"cell_type":"markdown","source":"### - Get information about the dataset and its dtypes to detect null values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### CONCLUSION: the dataset contains only 1 categorical column and the rest are numericals, also the dataset is clean and have no null values and ready for the preprocessing stage"},{"metadata":{},"cell_type":"markdown","source":"### - Plot Histograms to Visualize Feature Distributions in the dataset and Detect Skewness "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset to target and features\nfeatures = data.drop('diagnosis', axis = 1)\ntarget = data['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 38))\nplt.suptitle('Histograms for Numeric Features in the dataset', fontsize = 20)\nL = list(data)\nfor i in range(data.shape[1]):\n    plt.subplot(11, 3, i + 1)\n    sns.distplot(data[L[i]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Calculate the `skewness` of each feature to make sure the features are normally or sub-normally distributed and fix them if not normally distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew\n\nfeatures_list = list(features)\nSkew_D = {}\n\n# Claculate the skewness of each feature and store them in Skew_D\nfor f in features_list:\n    Skew_D[f] = skew(features[f], bias = False)\n    \n# Store the features that have high skewned\nHigh_skewed_features = []\nfor i in Skew_D:\n    if (Skew_D[i] > 1) or (Skew_D[i] < -1):\n        High_skewed_features.append(i)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Plot the features that have high skewness to visualize the skewness using histograms "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"features[High_skewed_features].hist(figsize = (15, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Testing\nIn this section of this project I'm going to find the correlations in the dataset. Because detecting these correlations can help us in `Feature Selection` process in order to minimize the number of features in the dataset.\n\n#### NOTE: I will calculate the correlation between each pair of attributes (correlation matrix). Then I'll plot the correlation matrix to get an idea of which variables have a high correlation with each other."},{"metadata":{},"cell_type":"markdown","source":"### Calculate coffecients of corelations between each pair of input features"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data_corr = data.corr()\n\nplt.figure(figsize = (18, 18))\nsns.heatmap(data_corr, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"highest_corr = data_corr['diagnosis'].sort_values(ascending = False)[1: 20]\nhighest_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now I'm going to plot barplot (diagnosis VS highest_corr features) to visualize the correlation between the target and the most effective features. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 35))\nplt.suptitle('Barplots for Diagnosis versus highest correlated features in the dataset, B: 0 , M: 1', fontsize = 25)\n\nL = highest_corr.index\nfor i in range(len(L)):\n    plt.subplot(8, 4, i + 1)\n    sns.barplot(data = data, x = 'diagnosis', y = L[i])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier Detection\nIn this section I'm going to apply some techniques to detect the outliers in the dataset. Outlier can be source of information in the dataset on the other hand it maybe lead to bad results or biased result. So we need to detect them and treat them in the `Preprocessing` stage. "},{"metadata":{},"cell_type":"markdown","source":"The presence of outliers can often skew results. There are many techniques for how to detect and deal with the outliers in a dataset. *outlier step* is calculated as `factor` multiplied the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# hold all indices of outliers\noutliers_index = set()\n\n# factor for calculating the step\nfactor = 4.5 \n\n\nfor f in list(features): \n    Q1 = np.percentile(data[f], q = 25)\n    Q3 = np.percentile(data[f], q = 75)\n    step = (Q3 - Q1) * factor\n    \n    for i in range(len(data)):\n        if (data[f].loc[i] > (Q3 + step)) | (data[f].loc[i] < (Q1 - step)):\n            outliers_index.add(i)\n            \nprint(\"There {} detected outliers\".format(len(outliers_index)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Delete the outliers\nBecause the number of outliers is not high we can drop them "},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data = data.drop(list(outliers_index), axis = 0)\n\n#Define features and Target again\nfeatures = clean_data.drop('diagnosis', axis = 1)\ntarget = clean_data['diagnosis']\n\nprint(\"The number of rows after deleting outliers is: {}\".format(len(clean_data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n#### In this section of this project I'm going to apply:\n* Apply `Feature Scaling` (Transformation). This will fix the skewness of the data \n* Split the dataset into `Trainset` and `Testset`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# prepare the final data for the model\nfinal_features = scaler.fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(final_features, target, test_size = 0.2, \n                                                    shuffle = True, random_state = 0)\n\nprint(\"training set size: {}, testing set size: {}\".format(len(X_train), len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\n#### In this Process I'm going to apply the following:\n* Pick up the most `appropriate models` that best-fit the problem I'm trying to solve.\n* Define the required metrics that need to evaluate the model performance.\n* Filter these models by testing them by ploting `learing curve` for each one."},{"metadata":{},"cell_type":"markdown","source":"In this section I planned to list some of the best Machine Learning Classifiers and see which ones are appropriate for this problem, and after that I will filter these models to pick the best one that give me the best accuracy.\n##### These models are:\n1. Support Vector Machine (SVM)\n2. Logistic Regression\n3. Random Forest Classifier\n4. Multi-Layer Perceptron (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier \n\n# Build these models\nsvc_model = SVC(random_state= 40) \nlogistic_model = LogisticRegression(random_state= 40)\nrandom_model = RandomForestClassifier(random_state= 40)\nmlp_model = MLPClassifier(random_state= 40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now I'm going to plot `learning curves` and apply `Cross Validation` to help me filter \nthese models and pick the most appropriate one.\nlearning curve give initial intution about which model will overfit and which will underfit and which one will do good job."},{"metadata":{},"cell_type":"markdown","source":"### Note:\nThe following cell contains some functions that have built personally to assest me when ploting `Learning Curves` and applying `GridSearchCV`, `Cross-Validation` or calculating `confusion matrix`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\n\n\ndef plot_learning_curve(estimators, X, y, train_sizes, scorer, cv):\n    \n    #calculate required number of rows in the figure \n    n_rows = np.ceil(len(estimators) / 2)\n    \n    #calculate the width of the figure\n    y_length = n_rows * 5 + 5\n    \n    # Create the figure window\n    fig = plt.figure(figsize=(10, y_length))\n    \n    for i, est in enumerate(estimators):\n        sizes, train_scores, test_scores = learning_curve(est, X, y, \n                                                          cv = cv, train_sizes = train_sizes, scoring = scorer)\n        \n        #print the done precentage\n        print(\"Precentage of work done: {}%\".format((i + 1) * 100 / len(estimators)))\n        \n        #get estimator name for title setting\n        est_name = est.__class__.__name__\n        \n        # average train_scores and test_scores\n        train_mean = np.mean(train_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n        \n        #Create subplots\n        ax = fig.add_subplot(n_rows, 2, i + 1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        \n        #add texts \n        ax.set_title(est_name)\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n       \n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 1.8), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Learning Performances for Multiple Models', fontsize = 16, y = 1.03)\n    fig.show()\n\ndef multi_cross_val(estimators, X, y, cv, scoring):\n    \n    scores = []\n    \n    for est in estimators:\n        S = cross_val_score(est, X, y, cv =cv, scoring = scoring)\n        scores.append(S)\n        \n    return scores\n\ndef cal_confusion_matrix(y_true, pred):\n    POS = 0\n    true_pos = 0\n    \n    NEG = 0\n    true_neg = 0\n    for i, element in enumerate(y_true):\n\n        if element == 1:\n            POS += 1\n            if pred[i] == 1:\n                true_pos += 1\n        else:\n            NEG += 1\n            if pred[i] == 0 :\n                true_neg += 1\n\n    false_neg = POS - true_pos\n    false_pos = NEG - true_neg\n    \n    return ['True Positive', 'False Positive', 'False Negative','True Negative'], [true_pos, false_pos, false_neg, true_neg]\n    \ndef multi_grid_search(estimators, X, y, params, cv, scoring):\n    \n    grids = []\n    \n    for i, est in enumerate(estimators):\n        \n        #Define the grid search object\n        grid_obj = GridSearchCV(est, param_grid = params[i], cv = cv, scoring = scoring)\n        grid_obj.fit(X, y)\n        grids.append(grid_obj)\n        #print the done precentage\n        print(\"Precentage of work done: {}%\".format((i + 1) * 100 / len(estimators)))\n   \n    #return grid_obj\n    return grids\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import make_scorer, f1_score\n\n\ncv= KFold(n_splits = 10, shuffle = True, random_state = 0) \ntrain_sizes= [20, 40, 80, 180, 300, 390]\nscorer= make_scorer(f1_score)\n\nplot_learning_curve([svc_model, logistic_model, random_model, mlp_model], X_train, y_train, train_sizes, scorer, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply cross_val_score for SVC and MLPClassifier\nscores = multi_cross_val([svc_model, logistic_model, random_model, mlp_model], X_train, y_train, cv, scorer)\n\n#get the average of the scores\nscores = np.mean(scores, axis = 1)\n\nprint(\"The average scores for SVC is: {} and for LogisticRegression is: {}\".format(scores[0], scores[1]))\nprint(\"The average score for RandomForest is: {} and for MLPClassifier is: {}\".format(scores[2], scores[3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above learning curves and Cross Validation I will pick `SVC` and `LogisticRegression` because they both gives high accuracy in learning curves and Cross Validation.\nSo I'm going to take these two models and filter them later in the project."},{"metadata":{},"cell_type":"markdown","source":"# Fine Tune the Model\nIn this sectoin of this project I'm going to fine tune the model's hyperparameters using `Grid Search` Technique in order to improve the performance of the model.\nFor this purpose I have built a method called `multi_grid_search` in functions file in order to apply GrideSearchCV for multiple models to apply it for the filtered models."},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare svc's paramters\nsvc_params = {'C': [1, 2, 2.5, 3], 'kernel': ['linear', 'poly', 'rbf']}\n\n#prepare mlp's prarmeters\nlogistic_params = {'penalty': ['l1','l2'], 'C': [0.09, 0.1, 0.5, 1, 2], 'max_iter': [75, 100, 200, 500]}\n\n#apply GrideSearchCv for both models\ngrids = multi_grid_search([svc_model, logistic_model], X_train, y_train, [svc_params, logistic_params], cv, scorer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the best svc\nbest_svc = grids[0].best_estimator_\n\n# get the best logistic model \nbest_logistic = grids[1].best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\nIn this section of the project I'm going to measure the performance of the final model(s) to make sure the final one can go to the light."},{"metadata":{},"cell_type":"markdown","source":"In this section I'm going to implement the following:\n* Implement cross validation and test the models on the testset in order to pick the best one of them.\n* Plot confusion Matrix to detect `false positive` and `false_negative`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply cross_val_score for SVC and MLPClassifier\nscores = multi_cross_val([best_svc, best_logistic], X_train, y_train, cv, scorer)\n\n#get the average of the scores\nscores = np.mean(scores, axis = 1)\n\nprint(\"The average score for SVC is: {} and for LogisticRegression is: {}\".format(scores[0], scores[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you noticed the SVC beats the LogisticRegression model by 1%, but further I'm going to test both on testset to make sure that SVC still generalize well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on testset \nsvc_pred = best_svc.predict(X_test)\nlogistic_pred = best_logistic.predict(X_test)\n\n#calculate f1_score for both predictions to decide the winner\nsvc_score = f1_score(y_test, svc_pred)\nlogistic_score = f1_score(y_test, logistic_pred)\n\nprint(\"The test score for SVC is: {} and for LogisticRegression is: {}\".format(svc_score, logistic_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above evaluations on trainset and testset, `SVC` have won on trainset, but LogisticRegression beats it on testset. That's make it hard to decide which is better so we can use both algorithms."},{"metadata":{},"cell_type":"markdown","source":"**(Optional)** Now I'm going to try eliminate the less important featreus and keep the highest coorelated ones, trying to simplify the model. This setp will lead to less acurate model but if we came to trade-off between accuracy and speed of the model we may choose to reduce the number of features. So i'll give it a try."},{"metadata":{},"cell_type":"markdown","source":"## Refine the model input features\nIn this section I will remove the least important features trying to speed-up the model perormance, this setp might seems not necessary in my case because I have a small dataset. But I'll do in the seek of figure out what would be the result."},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy \n\nscaler = StandardScaler()\n\n# prepare the final data for the model\nfinal_features_reduced = scaler.fit_transform(features[highest_corr.index])\n\n\nX_train_reduced, X_test_reduced, y_train, y_test = train_test_split(final_features_reduced,\n                                                                                    target, test_size = 0.2, \n                                                                                    shuffle = True, random_state = 0)\n\nsvc = copy.copy(best_svc)\nlogistic = copy.copy(best_logistic)\nsvc.fit(X_train_reduced, y_train)\nlogistic.fit(X_train_reduced, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply cross_val_score for SVC and MLPClassifier\nscores = multi_cross_val([svc, logistic], X_train_reduced, y_train, cv, scorer)\n\n#get the average of the scores\nscores = np.mean(scores, axis = 1)\n\nprint(\"The average score for SVC is: {}, and for Logistic is: {} \".format(scores[0], scores[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on testset \nsvc_pred_reduced = svc.predict(X_test_reduced)\nlogistic_pred_reduced = logistic.predict(X_test_reduced)\n\n# Calculate f1_score for both predictions to decide the winner\nsvc_score = f1_score(y_test, svc_pred_reduced)\nlogistic_score = f1_score(y_test, logistic_pred_reduced)\n\nprint(\"The test score for SVC is: {} , and for Logistic is: {}\".format(svc_score, logistic_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That's Good! \nAs we see after removing the less important features the model's accuracy decreased a little on both trainset and testset.\nThis trade-off in this size of the dataset not important because we have small dataset, but if we increased the number of rows to be high (like 20,000 - 50,000) we may prefere to eliminate these less important features\n---"},{"metadata":{},"cell_type":"markdown","source":"# Accuracy Visualization\n### Final Step\nIn this final step I'm going to calculate the `confusion matrix` in order to detect the two types of errors `False Positive` and `False Negative`. \nFor sure we care much about `False Negatives` because it's more risky as it sends the patient to home pretending he/she is safe and don't suffer anything, and because of this this type of error more risky so we wish to cut it out."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# apply first on trainset\nelements, train_confusion = cal_confusion_matrix(y_train, best_svc.predict(X_train))\n\n# apply first on trainset\nelements, test_confusion = cal_confusion_matrix(y_test, best_svc.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the confution Matrix for trainset and testset using seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build dataframe for train_confusion\nconfusion_train = pd.DataFrame(index = ['Predict Positive', 'Predict Negative'], \n                          columns = ['Actual Positive', 'Actual Negative'])\n\n#Assign values for corresponding rows and columns\nconfusion_train['Actual Positive'] = train_confusion[0], train_confusion[2]\nconfusion_train['Actual Negative'] = train_confusion[1], train_confusion[3]\n\n#Build dataframe for test_confusion\nconfusion_test = pd.DataFrame(index = ['Predict Positive', 'Predict Negative'], \n                          columns = ['Actual Positive', 'Actual Negative'])\n\n#Assign values for corresponding rows and columns\nconfusion_test['Actual Positive'] = test_confusion[0], test_confusion[2]\nconfusion_test['Actual Negative'] = test_confusion[1], test_confusion[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot confusion for trainset\nplt.figure(figsize = (15, 8))\nplt.suptitle(\"Confusion Matrix for Trainset\", fontsize = 30)\nsns.heatmap(confusion_train, annot = True, fmt = 'd')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot confusion for trainset\nplt.figure(figsize = (15, 8))\nplt.suptitle(\"Confusion Matrix for Testset\", fontsize = 30)\nsns.heatmap(confusion_test, annot = True, fmt = 'd')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n##### The total `False Postives` for trainset and testset is `7`  and got zero `False Negatives` so that's great result.\n---"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}