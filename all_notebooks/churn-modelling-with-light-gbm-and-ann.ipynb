{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Churn Modelling with Light GBM and ANN"},{"metadata":{},"cell_type":"markdown","source":"Jonathan Lices Martín\n\nIn this notebook we're gonna try to understand the basic implementation of an ANN with some different methods, and compare our results with one of the most popular methods in Kaggle competitions, Light GBM. So we have some interesting objectives from now, let's do it!"},{"metadata":{},"cell_type":"markdown","source":"## Understanding the problem and the data"},{"metadata":{},"cell_type":"markdown","source":"This dataset is prepared to try to predict/determine if a bank's client will leave it or not, by using information like credit score, salary, etc. So, since we have to say wether the client is going to leave the bank or not, we expect a **binary** output from our ANN.\n\nLike a great data scientist would say, the first step is to explore the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import packages and libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\n\nimport lightgbm as lgb\n\nimport keras\nfrom keras.models import Sequential \nfrom keras.layers import Dense \nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data with pandas\n\ndata = pd.read_csv(\"../input/churn-modelling/Churn_Modelling.csv\")\ndata_copy = data.copy() # Just in case\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we've imported the data succesfully. It's time to explore it and try to understand the dataset. To start with, let's try to describe every column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset columns\n\nprint(\"The names of the columns are:\", data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset statistical description\n\ndata.describe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have some relevant information here, and the rest, maybe we won't need it at all, so we can delete it. The column names are really explicit, so we can easily infer the what are we seeing in this dataset. Our objective now is to preprocess the data. "},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"One of the first things we can think about when we are going to do a Machine Learning project is whether the dataset is complete or not; that is to say, do we have **missing values**?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values\n\ntotal = data.isnull().sum().sort_values(ascending=False)\nporcentage = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, porcentage], axis=1, keys=['Total', 'Porcentage'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have a complete dataset, now we can delete the non-relevant information. To do this we'll juist simply drop that columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing non-relevant information\n\nnot_featured_cols = [\"RowNumber\", \"CustomerId\", \"Surname\"]\ndata = data.drop(not_featured_cols, axis = 1)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"Can we expect some correlation between the data? Which characteristic is more important? These are some of the questions we have to answer. Let's see it with more detail with a correlation plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation plot\n\ncorr = data.corr()\n\nsns.set()\nfig, ax = plt.subplots(figsize = (15,15))\nax = sns.heatmap(corr, annot = True, linewidths = 1.0)\nax.set_title(\"Correlation Plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have strong correlations at all. We can continue our data exploration by studying some information that may be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing columns\n\nfig = sns.countplot(data[\"Geography\"])\nfig.set_title(\"Geopgraphy Counting\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns.countplot(data[\"Gender\"])\nfig.set_title(\"Gender Counting\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display min and max age.\n\nprint(\"The maximum age is:\", data[\"Age\"].max())\nprint(\"The minimum age is:\", data[\"Age\"].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we hace people from their 18 to their 92, there are more men than women, and the half of the dataset is from France. So we can expect an european bank originated in France, with some offices in Spain and Germany.\n\nNow, we are prepared to process the data and build our models."},{"metadata":{},"cell_type":"markdown","source":"## Building the models"},{"metadata":{},"cell_type":"markdown","source":"The first thing we have to do is prepare the data to be able to build a model. We hava some categorical data here, so let's use OneHotEncoder to solve this problem. But first, we have to split the data in two groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data\n\nX = data.iloc[:, :10].values\ny = data.iloc[:, 10].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features\n\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) # 'Geography' \nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) # 'Gender'\n\n\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"Churn_Modelling\", # Name for transormation\n        OneHotEncoder(categories='auto'), # Class we want transform\n        [1] # Columns\n        )\n    ], remainder='passthrough'\n)\nX = transformer.fit_transform(X)\nX = X[:, 1:] # Avoiding multicollinearity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last but not least, splitting the data in training and testing groups\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size = 0.2, \n                                                    random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ANN implementation"},{"metadata":{},"cell_type":"markdown","source":"Now we can build our Multi-layer perceptron or Artificial Neural Network. Bur, what are artificial neural networks?\n\n> *Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)*\n\nActually, this notebook is not as interested in the theory as in the practice, so let's build our model."},{"metadata":{},"cell_type":"markdown","source":"Another interesting method is using Autokeras. You can learn more about this [here](https://towardsdatascience.com/automl-creating-top-performing-neural-networks-without-defining-architectures-c7d3b08cddc)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling data\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model\n\nclassifier = Sequential()\n\n# First layer\nclassifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation = \"relu\", input_dim = 11))\nclassifier.add(Dropout(rate = 0.1))\n\n# Second layer\nclassifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation = \"relu\"))\nclassifier.add(Dropout(rate = 0.1))\n\n# Output layer\nclassifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n\n# Compiler\nclassifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LET'S TRAIN!\n\nclassifier.fit(X_train, y_train,  batch_size = 10, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model\n\ny_pred = classifier.predict(X_test) \ny_pred = (y_pred > 0.5)\n\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm)\n\ndef plot_confusion_matrix(df_confusion, title='Confusion matrix'):\n    sns.set()\n    ax= plt.subplot()\n    sns.heatmap(df_confusion, annot=True, ax = ax, cmap='coolwarm')\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n    ax.set_title('Confusion Matrix');\nplot_confusion_matrix(cm_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see the importance of scaling the data. "},{"metadata":{},"cell_type":"markdown","source":"## Light GBM Implementation"},{"metadata":{},"cell_type":"markdown","source":"If you are in Kaggle right now, it's probably that you heared something about Light GBM, but what is this? \n\n> *Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks. Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. [analyticsvidhya](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)*\n\nLet's try to make that implementation. Remember we've already scaled the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model specifically for LGBM\n\ntraining_data = lgb.Dataset(data = X_train, label = y_train)\nparams = {'num_leaves': 31, 'num_trees': 100, 'objective': 'binary'}\nparams['metric'] = ['auc', 'binary_logloss']\nclassifier = lgb.train(params = params,\n                       train_set = training_data,\n                       num_boost_round = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions with test set\n\nprob_pred = classifier.predict(X_test)\ny_pred = np.zeros(len(prob_pred))\nfor i in range(0, len(prob_pred)):\n    if prob_pred[i] >= 0.5:\n       y_pred[i] = 1\n    else:  \n       y_pred[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm)\nplot_confusion_matrix(cm_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the accuracy\n\naccuracy = accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy: {:.0f} %\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-FOLD CROSS VALIDATION\n\nparams = {'num_leaves': 31, 'num_trees': 100, 'objective': 'binary'}\nparams['metric'] = ['auc']\ncv_results = lgb.cv(params = params,\n                    train_set = training_data,\n                    num_boost_round = 10,\n                    nfold = 10)\naverage_auc = np.mean(cv_results['auc-mean'])\nprint(\"Average AUC: {:.0f} %\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we've obtained the best accuracy with Light GBM. That is to say, we've made a simple model (in programming terms) and obtained better results. Maybe a ANN can do it better, but we should have searched the best params for it, and to be honest, in a real job we don't have that much time!\n\nThanks for reading my notebook and hope it was useful for you. \n\nPlease upvote if you like it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}