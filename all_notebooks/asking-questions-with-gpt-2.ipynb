{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn -U","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom datetime import datetime\nimport warnings\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport umap\nfrom nltk.stem import WordNetLemmatizer\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here I will be finetuning GPT2 on the askreddit questions data"},{"metadata":{},"cell_type":"markdown","source":"# Some light EDA\n\nReddit users are a seemingly curious bunch. The community known as [Ask Reddit](https://www.reddit.com/r/AskReddit/) boasts over 30M members and even has its own [wikipedia entry](https://en.wikipedia.org/wiki/AskReddit), according to which:\n> The subreddit describes its focus as \"to ask and answer questions that elicit thought-provoking discussions\"\n\nHere we will explore a small portion of the questions posted to the AskReddit forum.\n\n## What is reddit asking?\n\ntake a sample of questions from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/askreddit-questions-and-answers/reddit_questions.csv\", delimiter=\";\")\nfor i in df.sample(n=10)['text']:\n    print(i, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It really seems like reddit is a place to ask almost anything. Can we find some patterns in the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = list(df['text'])\nquestions = [q.replace('\"\"', '\"') for q in questions]\ncounts = {}\nfor question in questions:\n    for word in word_tokenize(question.lower()):\n        if word.isalpha():\n            try:\n                counts[word] += 1\n            except KeyError:\n                counts[word] = 1\nsorted_count = {k: v for k, v in sorted(counts.items(), key=lambda item: item[1])[::-1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.bar(x=list(sorted_count.keys())[:50], y=list(sorted_count.values())[:50], \n       title=\"50 most common words\", \n       labels={\"x\":\"word\", \"y\":\"count\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perhaps unsurprisingly, this looks very similar the most common words in the eglish language, except with words like \"what\", \"how\", \"why\" and of course \"reddit\" appearing more frequently than in common parlance. Does excluding stop words shed more light on what reddit users are asking?"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_non_stop_words = []\ntop_non_stop_vals = []\nfor word in sorted_count.keys():\n    if word not in stopwords.words('english'):\n        top_non_stop_words.append(word)\n        top_non_stop_vals.append(sorted_count[word])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.bar(x=top_non_stop_words[:50], y=top_non_stop_vals[:50],\n       title=\"50 most common non stop-words\", \n       labels={\"x\":\"word\", \"y\":\"count\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reddit sure likes talking about reddit."},{"metadata":{},"cell_type":"markdown","source":"We've already seen that the word 'what' is the second most common word in the whole dataset. How frequently do users ask \"what\" questions compared to other questions?"},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = [\"who\", \"what\", \"where\", \"why\", \"when\", \"how\"]\nvals = [counts[k] for k in keys]\nvals, keys = zip(*(sorted(zip(vals, keys)))[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.bar(x=list(keys), y=list(vals),\n       title=\"word count\",\n       labels={\"x\":\"word\", \"y\":\"count\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## question length"},{"metadata":{"trusted":true},"cell_type":"code","source":"l = [len(q) for q in questions]\nsns.displot(l)\nplt.vlines(np.median(l), 0, 4500, colors='r')\nplt.title(\"question length\")\nplt.show()\nprint(\"median:\", np.median(l))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It takes most people around 79 characters to ask a question, but some questions require a short prose to truly get their point across such as:"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions[np.argmax(l)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reddit really is a place to ask anything."},{"metadata":{},"cell_type":"markdown","source":"## Interest in topics over time\n\n(anything measuring term fequency, especially over time, should be taken with a whole heap of salt as the dataset is most likely affected by sampling bias)"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = sns.histplot(df['timestamp'])\n\nxticklabels = []\nfor t in p.get_xticks():\n    dt = datetime.fromtimestamp(t)\n    xticklabels.append(f\"{dt.day}/{dt.month}/{dt.year}\")\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    p.set_xticklabels(xticklabels, rotation = 45, ha=\"right\")\nplt.title(\"questions posted over time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that in early 2020, when people around the word were more bored and confused than ever before, there was a massive spike in questions posted to AskReddit. But this may be due to sampling biases."},{"metadata":{"trusted":true},"cell_type":"code","source":"def interest_over_time(terms, bins=100):\n    ts = []\n    for index, row in df.iterrows():\n        text = row['text'].lower()\n        tokens = word_tokenize(text)\n        for t in terms:\n            if t in tokens:\n                ts.append(row['timestamp'])\n    p = sns.histplot(ts, bins=bins)\n    \n    xticklabels = []\n    for t in p.get_xticks():\n        dt = datetime.fromtimestamp(t)\n        xticklabels.append(f\"{dt.day}/{dt.month}/{dt.year}\")\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        p.set_xticklabels(xticklabels, rotation = 45, ha=\"right\")\n    \n    plt.title(f\"interest in {'/'.join(terms)} over time\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interest_over_time([\"coronavirus\", \"covid\", \"sars-cov-2\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"coronavirus\" in t or \"covid\" in t or \"cov-2\" in t:\n        break\nrow['text'], row['datetime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interest_over_time([\"trump\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"trump\" in word_tokenize(t):\n        break\nrow['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Searching terms such as 'trump' can have misleading results as the word has multiple meanings."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"donald trump\" in t:\n        break\nrow['text'], row['datetime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interest_over_time([\"biden\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"biden\" in t:\n        break\nrow['text'], row['datetime']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... clearly."},{"metadata":{},"cell_type":"markdown","source":"## upvotes"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(df['votes'], cut=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['votes'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The votes are heavily skewed by just a few posts recieving tens of thousands of votes vs the median (50 percentile) votes of just 6. This is likely due to higher voted posts being shown to more users creating a self-reinforcing feedback."},{"metadata":{},"cell_type":"markdown","source":"Here is a sample of questions plotted against number of votes and dates posted to explore.\n\nThere is a weakly positive trend in the upper limit of votes with recency, but overall, newer questions seem to be clustering towards the lower end of the votes axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = df.sample(n=1000)\npx.scatter(sample, x=\"timestamp\", y=\"votes\", hover_name=\"text\", log_y=True, hover_data=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_date(timestamp):\n    dt = datetime.fromtimestamp(timestamp)\n    return f\"{dt.day}/{dt.month}/{dt.year}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['x'] = pd.cut(df['timestamp'], bins=100)\npx.bar(x=[to_date(v) for v in df.groupby(\"x\").median()['timestamp'].values],\n          y=df.groupby(\"x\").mean()['votes'].values, title=\"Median upvotes per post over time\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that with the increasing number to questions submitted from mid 2019 to early 2020, there was a corresponding decrease in the average votes per question. Are people just posting boring questions?"},{"metadata":{},"cell_type":"markdown","source":"# Embeddings\n\nHere I am exploring different sentence embedding & visualisation strategies\n\n* Vectorise each sentence in TF-IDF form\n* Dimensionality reduction with LSA for visualisation\n\n\n* Vectorise each sentence in TF-IDF form\n* Dimensionality reduction with LSA and UMAP for visualisation\n\n\n* Vectorise each sentence with universal sentence encoder\n* Dimensionality reduction with PCA for visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# when running in a kaggle kernel these plots might cause lag\n# or crashes when plotting the entire dateset, taking just a sample here\nsample = df.sample(frac=0.25)\nsamples_qs = list(sample['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_types = []\nw = ['who', 'what', 'where', 'why', 'how', 'when']\n\nfor q in samples_qs:\n    q = word_tokenize(q.lower())\n    f = False\n    for i in w:\n        if i in q:\n            f = True\n            q_types.append(i)\n            break\n    if not f:\n        q_types.append(\"other\")\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform([q.lower() for q in samples_qs])\nsvd = TruncatedSVD(n_components=2)\ny = svd.fit_transform(X)\n\npx.scatter(x=y[:, 0], y=y[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=q_types,\n              title=\"latent semantic analysis reduction of TF-IDF, coloured by question type\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are you in the 1% of? An oddly appropriate embedding for that question.\n\n\nExploring some of the data above, you can see that this has captured some of the semantics of the questions being asked, but it doesn't really give much interesting insight. A caveat of TF-IDF embedding seems to be that the length of the text is a significant factor."},{"metadata":{},"cell_type":"markdown","source":"UMAP is another dimensionality reduction algorithm used for visualisation.\n\nHere, the sparse TF-IDF representation is reduced to a more manageable 100 dimensions before UMAP to improve the efficiency of UMAP. (this may still be quite slow)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nlemmatised = [lemmatizer.lemmatize(q.lower()) for q in samples_qs]\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(lemmatised)\nsvd = TruncatedSVD(n_components=100)\ny = svd.fit_transform(X)\n\nreducer = umap.UMAP()\nscaled = StandardScaler().fit_transform(y)\nembedding = reducer.fit_transform(scaled)\n\npx.scatter(x=embedding[:, 0], y=embedding[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=sample['timestamp'],\n              title=\"UMAP of lemmatised TF-IDF, coloured by votes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the plot above, there seem to be some clusters of semantically similar questions."},{"metadata":{},"cell_type":"markdown","source":"## Universal Sentence Encoder\n\nThe universal sentence encoder is a more sophisticated embedding algirithm from Google. The pretrained neural network is optimised for sentence encoding and outputs a single 512 dimension vector per input."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\ny = pca.fit_transform(model(samples_qs).numpy())\npx.scatter(x=y[:, 0], y=y[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=sample['timestamp'],\n              title=\"PCA of encoded, coloured by age\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the plot above it seems that newer posts tend to cluster towards the mean of the projected embedding. Obviously more thorough analysis is needed before any conclusions can be drawn, but it's fun to explore the interactive plot :)"},{"metadata":{},"cell_type":"markdown","source":"# Asking questions with GPT-2\n\nI will be using the pretrained distilled gpt2 model from ðŸ¤— [Hugging Face](https://huggingface.co/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers -U","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel, GPT2Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#download pretrained model\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token \nmodel = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pretrained text generation\n\nsee https://huggingface.co/blog/how-to-generate for a very nice explanation of text generation methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"for prompt in [\"What is\", \"Who is\", \"Why do\", \"Where are\", \"How do\", \"When is\"]:\n\n    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n\n    sample_outputs = model.generate(\n        input_ids,\n        do_sample=True, \n        max_length=50, \n        top_p=0.92, \n        top_k=0,\n        num_return_sequences=3\n    )\n\n    print(100 * '-' + \"\\nPrompt:\", prompt)\n    print(\"Output:\\n\" + 100 * '-')\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# only train on questions less than 150 chars long\nq2 = [q for q in questions if len(q) < 150]\n# encode using pretrained tokenizer\ntrain_encodings = tokenizer(q2, truncation=True, padding='max_length', max_length=150)#, return_tensors='tf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nepochs = 2\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    train_encodings['input_ids']\n    ).shuffle(1000)\\\n    .batch(batch_size, drop_remainder=True)\\\n    .prefetch(tf.data.experimental.AUTOTUNE)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps = len(train_dataset)\n\nfor e in range(epochs):\n    losses = []\n    start_time = time.time()\n    for i, inputs in train_dataset.enumerate():\n        i = i.numpy()\n        with tf.GradientTape() as tape:\n            loss_value = model(inputs, labels=inputs)['loss']\n\n        grad = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n\n        losses.append(tf.reduce_mean(loss_value).numpy())\n\n        total_time = time.time() - start_time\n        time_per_step = total_time / (i+1)\n        eta = time_per_step * (steps - (i+1))\n        m, s = divmod(eta, 60)\n        h, m = divmod(m, 60)\n\n        print(f\"\\repoch {e+1}/{epochs}, step {i+1}/{steps}, loss:{np.mean(losses):.5f}, ETA:{int(h)}h{int(m)}m{int(s)}s\", end=\"\", flush=True)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## fine tuned text generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for prompt in [\"What is\", \"Who is\", \"Why do\", \"Where are\", \"How do\", \"When is\", \"Why does my\", \"What is your\", \"Where do\", \"If you could\"]:\n\n    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n\n    sample_outputs = model.generate(\n        input_ids, \n        do_sample=True, \n        max_length=50, \n        top_p=0.92, \n        top_k=0,\n        num_return_sequences=3\n    )\n\n    print(100 * '-' + \"\\nPrompt:\", prompt)\n    print(\"Output:\\n\" + 100 * '-')\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}