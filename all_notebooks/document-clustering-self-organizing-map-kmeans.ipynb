{"cells":[{"metadata":{},"cell_type":"markdown","source":"I am phyo thu htet, a student from Myanmar.\n\nI just want to participate to help in fighing covid-19 as much as I can. Because I strongly believe that the things  that I do can be a small stone but I would be happy if it was a part of a building.\nLike this, hope to be a small help or contribution to fight covid-19.\nI love my country. I love my world.\n\n\nFighting all."},{"metadata":{},"cell_type":"markdown","source":"\n![Result](https://raw.githubusercontent.com/PhyoThuHtetHarry/covid-19-literature-kaggle-pth/master/Screenshot%20from%202020-04-16%2023-26-30.png)\n### <center>Tool that I create</center>\n## <center> Still Trying to deploy on server</center>"},{"metadata":{},"cell_type":"markdown","source":"# Document Clustering\n# Content\n* [Why Document Clustering?](# Why Document Clustering?)\n* [Data Preprocessing](# Data Preprocessing)\n* [TF_IDF](# TF_IDF)\n* [Doc2Vec](# Doc2Vec)\n* [PCA](# PCA)\n* [Clustering](# Clustering Approaches)\n  * [1. SOM Clustering](#1.SOM Clustering)\n    * [1.1 SOM Clustering Reinforced with Doc2Vec](#1.1 SOM Clustering Reinforced with Doc2Vec)\n    * [1.2 Pros and Cons of SOM Clustering](#1.3 Pros and Cons of SOM Clustering)\n   * [2. KMeans Clustering](#2. KMeans Clustering)\n     * [2.1 KMeans Clustering Reinforced with TF_IDF](#2.1 KMeans Clustering Reinforced with TF_IDF)\n       * [2.1.1 Evaluation of KMeans Clustering Reinforced with TF_IDF](#2.1.1 Evaluation of KMeans Clustering Reinforced with TF_IDF)\n     * [2.2 KMeans Clustering Reinforced with Doc2Vec](#2.2 KMeans Clustering Reinforced with Doc2Vec)\n       * [2.2.1 Evaluation of KMeans Clustering Reinforced with Doc2Vec](#2.2.1 Evaluation of KMeans Clustering Reinforced with Doc2Vec)\n*  [Classification](# Classification)\n  * [Data Preprocessing for text](#5.1 Data Preprocessing for text)\n  * [Train Test Split](#5.2 Train Test Split)\n  * [DNN Model and Training](#5.3 Training)\n     * [Discussion on the effect of word embedding](#5.3.1 Discussion on the effect of word embedding)\n  * [Evaluation of Classification](#5.4 Evaluation)\n  * [Pros and Cons of DNN Classification](## 5.5 Pros and Cons of 5.1 Approaches)\n* [Summary](#6.Summary)\n    \n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Why Document Clustering?\n\n\n**Def of Document Clustering **\n(Ref:https://www.igi-global.com/dictionary/xml-document-clustering/8184)\nThe task of organizing a collection of documents, whose classification is unknown, into meaningful groups (clusters) that are homogeneous according to some notion of proximity (distance or similarity) among documents. \n\nFor the key questions like\n\n    Resources to support skilled nursing facilities and long term care facilities.\n    Mobilization of surge medical staff to address shortages in overwhelmed communities\n    Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies\n    Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\n    Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\n    Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\n    Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\n    Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\n    Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.\n    Guidance on the simple things people can do at home to take care of sick people and manage disease.\n    Oral medications that might potentially work.\n    Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\n    Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\n    Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\n    Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\n    Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\n"},{"metadata":{},"cell_type":"markdown","source":"Clustering of documents would be the key fundametal coreas the first step considering from the view point of technical dimension. My idea is based on three different factors\n\n(1) Document Clustering will provide meaningful clusters of documents that will be useful for finding facts about virus\n\n(2) We can delivers label for further processing approaches like classificaiton\n\n(3) A stimulus reinforced with 'AI' technology for detecting useful information would be one of the crucial mechanisms to address the escalation of virulent Corana Virus which has negative butterfly effect on the whole society."},{"metadata":{},"cell_type":"markdown","source":"\n![Doc Clustering](https://miro.medium.com/max/1000/1*HgXA9v1EsqlrRDaC_iORhQ.png)\n*Image Credits : [mc.ai](https://mc.ai/a-complete-multi-class-text-classifier-step-by-step/) under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n### <center >Fig: Example of Document Clustering </center>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Preprocessing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**pandas** - for data handling\n\n**matplotlib** - for visualization\n\n**numpy** - for vector, matrix (creation, operation)\n\n**re** - for regular expressions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n#read data set\nmeta_data = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv')\nprint('Original Size of Data:',meta_data.shape)\n\n#drop rows with null values (based on abstract attribute)\nmeta_data.dropna(subset = ['abstract'],axis = 0, inplace = True)\nprint('Data Size after dropping rows with null values (based on abstract attribute):',meta_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nDuplicate Values are also needed to be handled with care.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#handling duplicate data (based on 'sha','title' and 'abstract')\nprint(meta_data[meta_data.duplicated(subset=['sha','title','abstract'], keep=False) == True])\nmeta_data.drop_duplicates(subset=['sha','title','abstract'],keep ='last',inplace=True)\nprint('Data Size after dropping duplicated data (based on abstract attribute):',meta_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A large number of null values can be found. They are, however, can be useful in some ways.\nSo instead of deleting them, I will replace with 'No Information Available'."},{"metadata":{"trusted":true},"cell_type":"code","source":"#3\n#function to deal with null values\n#'No Information Available' will be replaced \ndef dealing_with_null_values(dataset):\n    dataset = dataset\n    for i in dataset.columns:\n        replace = []\n        data  = dataset[i].isnull()\n        count = 0\n        for j,k in zip(data,dataset[i]):\n            if (j==True):\n                count = count+1\n                replace.append('No Information Available')\n            else:\n                replace.append(k)\n        print(\"Num of null values (\",i,\"):\",count)\n        dataset[i] = replace\n    return dataset\n\nmeta_data = dealing_with_null_values(meta_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA (Principal Component Analysis)\n![PCA](https://i2.wp.com/www.sportscidata.com/wp-content/uploads/2019/08/Principal_Component_Analysis_print.png?fit=1024%2C683&ssl=1)\n*Image Credits : [SportSciData](https://www.sportscidata.com/perform-principal-component-analysis-pca-of-training-load-data/) under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n\n\nIt is a statistical approach and uses an orthogonal transformation.\nOur text representation (such as **bag of words**, **tf-idf**, **doc2vec**) resprents text in the form of mutli dimensional matrix which involves a lot of features in the feature space. To address for dimension reduction, PCA can be used. It extract features (which is different from **feature elimination**) and reduce dimension."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ndef pca_fun(n_components, data):\n    pca = PCA(n_components=n_components).fit(data)\n    data = pca.transform(data)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF_IDF (Term Frequency Inverse Data Frequency)\n\n![TF-IDF](https://miro.medium.com/max/1304/1*tWBNjDdKuUoxzk48jxNSPw.png)\n*Image Credits : [Medium](https://medium.com/@ashiddk95/tf-idf-term-frequency-inverse-document-frequency-algorithm-5b16ea86eeff?source=rss-------1) under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n\nNLP (Natural Language Processing) is a sub field of Artifical Intelligence. The texual representaion can not be worked with Machine Learning Algorithm and must be converted into some form of numeric representation. The most common approach to deal with that case is Bag of Words (BOW). But BOW can not perform so well. So another apporach, TF-IDF is used.\n\n**Term Frequency (TF)** Equation can be described as\n\n\nTF(i, j) = number of i in the j / total number of words in j\n\nWhere, i = word, j = document\n       \n\n**Inverse Data Frequency (IDF)**\n\nIDF (i) = log(number of j / number of j that contains i)\n\nWhere, i = word, j = document\n       \nThe score of TFIDF is by multiplying these two equations.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf(data):\n    tfidf = TfidfVectorizer( stop_words='english',use_idf=True)\n    tfidf_matrix = tfidf.fit_transform(data)\n    return tfidf_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a matrix with tfidf for the column abstract\ntfidf_matrix = tfidf(meta_data['abstract'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to explore which documents have more similar respresentaiton, consine simliartiy can be used\nfrom sklearn.metrics.pairwise import linear_kernel\ncosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()\n\n# 10 most related documents indices\nrelated_docs_indices = cosine_similarities.argsort()[:-11:-1]\nprint(\"Related Document:\",related_docs_indices)\n\n# Cosine Similarties of related documents\nprint(\"Cosine Similarites of related documents\",cosine_similarities[related_docs_indices])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at two most similar document\nmeta_data.iloc[0]['abstract']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(meta_data.iloc[0]['abstract'])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.iloc[17863]['abstract']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(meta_data.iloc[17863]['abstract'])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Doc2Vec\n\n![Doc Clustering](https://1.bp.blogspot.com/-StWQLiJs0Ac/XaBkHpQkabI/AAAAAAAAB3U/tPRUALJDz5wf_f4T45wrDh_XJt45p0v2ACLcBGAsYHQ/s320/Capture.PNG)\n*Image Credits : [Think Infi](https://www.thinkinfi.com/2019/10/doc2vec-implementation-in-python-gensim.html) under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n\nDoc2Vec is one of the great mechanisims for representation of text in documents and it is based on word2vec model.\n**Gensims Library for Doc2Vec\nSource: genism PyPI**\nGensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.\nFeatures\n\n    All algorithms are memory-independent w.r.t. the corpus size (can process input larger than RAM, streamed, out-of-core),\n    Intuitive interfaces\n        easy to plug in your own input corpus/datastream (trivial streaming API)\n        easy to extend with other Vector Space algorithms (trivial transformation API)\n    Efficient multicore implementations of popular algorithms, such as online Latent Semantic Analysis (LSA/LSI/SVD), Latent Dirichlet Allocation (LDA), Random Projections (RP), Hierarchical Dirichlet Process (HDP) or word2vec deep learning.\n    Distributed computing: can run Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers.\n    Extensive documentation and Jupyter Notebook tutorials..\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.models import Doc2Vec\n\ndef doc2vec():\n    document_tagged = []\n    tagged_count = 0\n    for _ in meta_data['abstract'].values:\n        document_tagged.append(gensim.models.doc2vec.TaggedDocument(_,[tagged_count]))\n        tagged_count +=1 \n    d2v = Doc2Vec(document_tagged)\n    d2v.train(document_tagged,epochs=d2v.epochs,total_examples=d2v.corpus_count)\n    return d2v.docvecs.vectors_docs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore the data representation of doc2vec for abstract column.\n# Visualization the doc2vec representation\n%time doc2vec = doc2vec()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(16,16))\nsns.heatmap(doc2vec,cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering Approaches"},{"metadata":{},"cell_type":"markdown","source":"# 1. Self Organization Map\nSource: Wikipedia\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\nA self-organizing map showing U.S. Congress voting patterns. The input data was a table with a row for each member of Congress, and columns for certain votes containing each member's yes/no/abstain vote. The SOM algorithm arranged these members in a two-dimensional grid placing similar members closer together. The first plot shows the grouping when the data are split into two clusters. The second plot shows average distance to neighbours: larger distances are darker. The third plot predicts Republican (red) or Democratic (blue) party membership. The other plots each overlay the resulting map with predicted values on an input dimension: red means a predicted 'yes' vote on that bill, blue means a 'no' vote. The plot was created in Synapse.\n\nThis makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling"},{"metadata":{},"cell_type":"markdown","source":"![SOM](https://miro.medium.com/max/1310/1*QG7afWQKjY3IpezhNQMzBg.png)\n*Image Credits : [towards data science](https://towardsdatascience.com/self-organizing-maps-ff5853a118d4) license*"},{"metadata":{},"cell_type":"markdown","source":"### Self Organizing Map Algorithm\n(Ref: http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/som.html)\n\n1. Each node's weights are initialized.\n2. A vector is chosen at random from the set of training data.\n3. Every node is examined to calculate which one's weights are most like the input vector. 4. The winning node is commonly known as the Best Matching Unit (BMU).\n5. Then the neighbourhood of the BMU is calculated. The amount of neighbors decreases over time.\n6. The winning weight is rewarded with becoming more like the sample vector. The nighbors also become more like the sample vector. The closer a node is to the BMU, the more its weights get altered and the farther away the neighbor is from the BMU, the less it learns.\n7. Repeat step 2 for N iterations.\n\n\n\n**BMU (Best Matching Unit)**\nThe weight with the shortest distance is the winner.\nIt calculaes the distance form each weight to the vector. Euclidean Distance is mostly used."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install somoclu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We need to install somoclu library\n\n\nimport somoclu\n\ndef som(data):\n    som = somoclu.Somoclu(50, 50, data=data, maptype=\"toroid\")\n    %time som = som.train(data)\n    print(\"Comonent Planes\")\n    return som","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 1.1 SOM Clustering Reinforced with Doc2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# It will takes about 5minutes to run\n\n# Step1: representing the text data with doc2vec\n# We have already created doc2vec vector in doc2vec section\ndata = doc2vec\n\n# Step2: Reduce to 2 dimension\ndata = pca_fun(2,data)\n\n# Step3: Self Organizing Map\n\nsom = som(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring Component Planes\n#labels is 0 to n(document-0, document-1,...documentn) \nlabels = range(0,data.shape[0])\n\n# Step4: Exploring Content Planes\nsom.view_component_planes()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 5: Exploring clusters by SOM reinforced with  Doc2Vec\nsom.view_umatrix(bestmatches = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# 1.2 Pros and Cons of SOM Clustering Approach\n**Pros:** \n\nSelf-organizing Scheme (Self Learn)\n\n**Cons:**\n\nThe model does not understand how data is created\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 2. KMeans"},{"metadata":{},"cell_type":"markdown","source":"![KMeans](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/19344/versions/1/screenshot.jpg)\n*Image Credits : [MathWorks](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.mathworks.com%2Fmatlabcentral%2Ffileexchange%2F19344-efficient-k-means-clustering-using-jit&psig=AOvVaw13pD0ZS65epY4h-WaIakWy&ust=1585760496184000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCNiW07GYxegCFQAAAAAdAAAAABAD) under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n\n(Source Wikipedia) Clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids.\n\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n\nThe algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing KMeans library of sklearn\nfrom sklearn.cluster import KMeans\n\ndef kmeans(n_clusters):\n    kmean_model = KMeans(n_clusters = n_clusters,random_state=0)\n    return kmean_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(KMeans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I have explored with many other cluster numbers for Kmeans. According to my research, 6 is the most relevant one."},{"metadata":{},"cell_type":"markdown","source":"# 2.2 KMeans Clustering Reinforced With Doc2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have already created doc2vec vector representation in the section of doc2vec, the assign the value to X\nX = doc2vec\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans5 = KMeans(5)\n\n%time km5 = kmeans5.fit_predict(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans6 = KMeans(6)\n%time km6 = kmeans6.fit_predict(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans7 = KMeans(7)\n%time km7 = kmeans7.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 2.1.1 Evaluation of KMeans clustering reinforced with Doc2Vec(within cluster, between cluster, silhouette)\n"},{"metadata":{},"cell_type":"markdown","source":"**Within Cluster:**\n    \n    Within cluster should be minimum.\n    \n    \n    \n**Between Cluster:**\n    \n    Between cluster should be maximum as much as possible.\n    \n    \n    \n    \nSilhouette:\n    \n    \n    Silhouette score ranges from -1 and 1. The larger, the better."},{"metadata":{},"cell_type":"markdown","source":"### Within Cluster and Between Cluster Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [kmeans5, kmeans6, kmeans7]\ndef plot_WCSS_BCSS(models, data):\n    fig, ax = plt.subplots(1, 2, figsize=(12,5))\n    \n    ## Plot WCSS\n    wcss = [mod.inertia_ for mod in models]\n    n_clusts = [5,6,7]\n    \n    ax[0].bar(n_clusts, wcss,color='orange', edgecolor='black', linewidth=1)\n    ax[0].set_xlabel('Number of clusters')\n    ax[0].set_ylabel('WCSS')\n    ax[0].set_title('Within Cluster Analysis')\n    \n    \n    ## Plot BCSS \n    n_1 = (float(data.shape[0]) * float(data.shape[1])) - 1.0\n    tss = n_1 * np.var(data)\n    bcss = [tss - x for x in wcss]\n    ax[1].bar(n_clusts, bcss,edgecolor='black')\n    ax[1].set_xlabel('Number of clusters')\n    ax[1].set_ylabel('BCSS')\n    ax[1].set_title('Between Cluster Analysis')\n    plt.show()\n    \n\nplot_WCSS_BCSS(models,X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Silhouette Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\ndef plot_silhouette(kms,data,nclusts):\n    \n    silhouette = []\n    for i in kms:\n        score = silhouette_score(data,i)\n        print(score)\n        silhouette.append(score)\n    \n    \n    plt.bar(nclusts, silhouette,color = 'green')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Silhouette Score')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time plot_silhouette([km5,km6,km7],X,[5,6,7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data['cluster_doc2vec_kmeans'] = kmeans6.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimension reduction with PCA\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3).fit(X)\ndata = pca.transform(X)\ncentroids =  pca.transform(kmeans6.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\ncolor_code= ['#FFFF00', '#008000', '#0000FF', '#800080','#5e5d58','#e817d7']\ncolor = [color_code[i] for i in list(meta_data['cluster_doc2vec_kmeans'])]\nplt.figure(figsize=(50,20))\naxis = Axes3D(plt.figure())\naxis.scatter(data[:, 0], data[:, 1],data[:, 2],c = color)\naxis.scatter(centroids[:, 0], centroids[:, 1], centroids[:,2], marker='*', s=1500, c='#000000')\nplt.title(\"Doc2Vec Matrix with 6 clusters_3Dimension\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2).fit(X)\ndata = pca.transform(X)\ncentroids =  pca.transform(kmeans6.cluster_centers_)\nplt.scatter(data[:, 0], data[:, 1],c = color)\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='#000000')\nplt.title(\"Doc2Vec Matrix with 6 clusters_2Dimension\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe =pd.DataFrame()\ndataframe['cluster'] = meta_data['cluster_doc2vec_kmeans']\ndataframe['x'] =data[:, 0]\ndataframe['y'] =data[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.to_csv('output.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.to_csv('Cluster.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import parallel_coordinates\n\nfig = plt.figure(figsize=(12, 10))\ntitle = fig.suptitle(\"Parallel Coordinates\", fontsize=18)\nfig.subplots_adjust(top=0.93, wspace=0)\n\npc = parallel_coordinates(dataframe, \n                          'cluster', \n                          color=('skyblue', 'firebrick'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 Pros and Cons of KMeans Clustering"},{"metadata":{},"cell_type":"markdown","source":"\n**Pros**: Doc2vec is one of the best representations for text.\n\n**Cons**: KMeans need to have prior knowledge in the number of clusters (but can be explored with 'within cluster score', 'between cluster score','silhouette score' to choose the possible number that leads to the best for choosing cluster numbers)"},{"metadata":{},"cell_type":"markdown","source":"# 5. Classification[](http://)"},{"metadata":{},"cell_type":"markdown","source":"In General, Machine Learing has three basic approaches\n\n(1) Supervised Learning\n\n(2) Unsupervised Learning\n\n(3) Reinforcement Learning\n\nClassification is a subset of Supervised Learing and needs 'categorical labels'.\nWe get cluster numbers from clustering approaches. Results from KMeans Clustering Apporach reinforced with Doc2Vec Representation will be used here to perform further processing."},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Data Preprocessing for text"},{"metadata":{},"cell_type":"markdown","source":"Text Processing that is different from image classification is that we need to consider 'how to represent' for textual data.\nBut luckly, the texts to process here is English. **Generally**, we don't need to consider cases like 'word segmentation','sentence segmentaion' and other stuffs. In processing English Data, the following processed will be used\n\n(1) Word Indexing\n\n(2) Sentence Indexing\n\n(3) Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nsentences = list(meta_data['abstract'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.Word Indexing\ntokenizer = Tokenizer(oov_token=\"Out of vocab\")\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n# 2. Sentence Indexing\nsentence_indexing = tokenizer.texts_to_sequences(sentences)\n# 3. Padding\npadded_sentences = pad_sequences(sentence_indexing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# After preprocessing, we will use 20000 of data for training and ther rest for testing\ntrain_sentences = []\ntest_sentences = []\ntrain_labels =[]\ntest_labels=[]\n#train_data_len = len(padded_sentences)*0.7\ncount =0\nfor i,j in zip (padded_sentences,meta_data['cluster_doc2vec_kmeans']):\n    count = count+1\n    if(count<=20000):\n        train_sentences.append(i)\n        train_labels.append(j)\n    else:\n        test_sentences.append(i)\n        test_labels.append(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore the data\ntrain_sentences[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.3 Training"},{"metadata":{},"cell_type":"markdown","source":"Machine learning is the subfield of the Aritificial Intelligence Technology. Deep Learing is derived form Machine Learning. DNN(Deconvolutional Neural Network) is one of the architectures of deep learning neural network algorithms. DNN is the feedforward neural network where data flow from input layers to ouputlayers passing through hidden layers.\nIn my architecture,\n\n(1) Input Layer, One Hidden Layer and Output Layer is used \n\n(2) Word Embedding Layer is added (the representation of vectors where similar representaion for the similiar words)\n\n(3) Epochs(iteration in training dataset) = 10"},{"metadata":{},"cell_type":"markdown","source":"![ANN](https://miro.medium.com/max/1700/0*a_tr0gvjHtW9haUo.png)\n*Image Credits : [mio.medium.com] under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license*\n### <center >Fig:Example of Document Clustering </center>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n#instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### DNN Architecture\n### For now,since I use only one hidden layer, it is shallow neural network.\n\nActivation Function For Hidden Layer: 'relu'\n\nActivation Function For Output Layer: 'softmax'\n\nOptimizer: 'adam'\n\nNote : Embedding Layer is used(really powerful representation of text)\n\nEpoch: 20"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport tensorflow.keras as keras\n\n\nclass myCallback(keras.callbacks.Callback):\n\n    def on_epoch_end(self, epoch, logs={}):\n    \n        if(logs.get('accuracy')>0.998):\n        \n            print(\"\\nReached 99.8% accuracy so cancelling training!\")\n            \n            self.model.stop_training = True\n\n\n    \ncallbacks = myCallback()\ndnn_model = keras.Sequential([\n        keras.layers.Embedding(1000,16,input_length=19363),\n        keras.layers.Flatten(),\n        keras.layers.Dense(1000, activation='relu'),\n        keras.layers.Dense(6, activation='softmax') \n        ])\n\ndnn_model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \nhistory = dnn_model.fit(np.asarray(train_sentences), np.asarray(train_labels), epochs=20,callbacks = [callbacks]) \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('dnn_modelwithoutembedding.h5')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.3.1 Discussion on the effect of word embedding"},{"metadata":{},"cell_type":"markdown","source":"I want to share what I explore about word embedding layer in training.\n\nWithout Word Embedding\n\n    epochs = 100\n    and others same parameters deliver the result of accuracy (31.61%) while training with word embedding \n    (using 20 epochs) provides 0.999 (99.81%) on 13 epochs\n    Reached 99.8% accuracy so cancelling training!\nReached 99.8% accuracy so cancelling training!\n20000/20000 [==============================] - 44s 2ms/sample - loss: 0.1324 - accuracy: 0.9981\n\n\n\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"e =dnn_model.layers[0]\nweights = e.get_weights()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_word_index = dict([value,key] for(key,value) in word_index.items())\nimport io\nout_v = io.open('../embedding-tsv/vecsv.tsv','w')\nout_m = io.open('../embedding-tsv/meta.tsv','w')\n\nfor word_num in range(1,790):\n    word = reverse_word_index[word_num]\n    print(word)\n    embeddings = weights[word_num]\n    print(embeddings)\n    out_m.write(word+ \"\\n\")\n    out_v.write('\\t'.join([str(x)for x in embeddings])+\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can explore the vector represntation of word embedding here by using vecsv.tsv and metadata.tsv\nvia https://projector.tensorflow.org/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from IPython.display import Image\nImage(filename='../images-data/embedding.png') "},{"metadata":{},"cell_type":"markdown","source":"# 5.4 Evaluation of Classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy score is not enough, we need to perform recall, fscore, precision\n# I create a confusion matrix and calculate scores from that precision\ndef confusion_matrix_and_score_pth(y,ypredicted):\n    \n    #The lenth of y and ypredicted must have the same length\n    if(len(y)!=len(ypredicted)):\n        return \"Found input variables with inconsistent numbers of samples\",len(y),len(ypredicted)\n    \n    uniq_element     = (set(y)|set(ypredicted))\n    \n    \n    #Creating a dictionary in order to be sure in adding to the matrix\n    dictionary = {}\n    count = 0\n    \n    for _ in uniq_element:\n        dictionary[_] = count\n        count = count+1\n    \n    #Changing class labels\n    y = [dictionary.get(_) for _ in y]\n    ypredicted = [dictionary.get(_) for _ in ypredicted]\n    #print(y)\n    #print(ypredicted)\n    \n    #Declaring necessary variables\n    length          = len(uniq_element)\n    confusion_matrix = np.zeros((length,length))\n    correct_tokens   = 0\n    recall           = 0\n    precision        = 0\n    \n    #Creating confusion matrix\n    for i,j in zip(y,ypredicted):\n        confusion_matrix[i][j] += 1\n     \n    #Calculating accuracy, recall and f1 score  \n    vertical_summation = confusion_matrix.sum(axis=0)\n    horizontal_summation = confusion_matrix.sum(axis=1)\n    for _ in range (confusion_matrix.shape[0]):\n        correct_tokens += confusion_matrix[_][_]\n        if (vertical_summation[_] != 0):\n            recall += confusion_matrix[_][_]/vertical_summation[_]\n        if (horizontal_summation[_]  != 0):\n            precision += confusion_matrix[_][_]/horizontal_summation[_]\n    \n    accuracy  = correct_tokens/np.sum(confusion_matrix)\n    recall    = recall/confusion_matrix.shape[0]\n    precision = precision/confusion_matrix.shape[0]\n    \n    f_score = (2*recall*precision)/(recall+precision)\n    print(\"Input Class Labels:Class labels for this method\",dictionary)\n    print('Confusion matrix is')\n    print(confusion_matrix)\n    print(\"Accuracy Score is \", accuracy*100,'%')\n    print(\"Recall Score is \", recall*100,'%')\n    print(\"Precision Score is \", precision*100,'%')\n    print(\"FScore is \", f_score*100,'%')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain_labels= np.asarray(train_labels)\n\nypredicted_train_labels = dnn_model.predict_classes(np.asarray(train_sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypredicted = dnn_model.predict_classes(np.asarray(test_sentences)) \n\ny = np.array(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_loss_graph(val,history):\n    plt.plot(history.history[val])\n    plt.xlabel('Epochs')\n    plt.ylabel(val)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_loss_graph('accuracy',history)\n\nacc_loss_graph('loss',history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix on training data\n\nconfusion_matrix_and_score_pth(ytrain_labels,ypredicted_train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix on testing data\n\nconfusion_matrix_and_score_pth(y,ypredicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\n### So DNN is prone to over fitting "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Pros and Cons of DNN Classification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pros**: Work really well if the word embedding layer is used\n\n**Cons**: Prone to overfitting"},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"### 1. TF_IDF Representation is really time consuming. So I can't put all my works on this note book. Doc2Vec representaion is more suitable than tf-idf\n### 2. '6 clusters' is the most relevant one for KMeans Approach.\n### 3. Deep Neural Network is Prone to overfitting.\n### 4. The performance of the accuracy can be really improved if word embedding layer is used during training.\n"},{"metadata":{},"cell_type":"markdown","source":"# \n(Hope to be useful )\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}