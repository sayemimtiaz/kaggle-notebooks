{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Competition: COVID-19 Open Research Dataset Challenge (CORD-19)\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n\n# Task: What is known about transmission, incubation, and environmental stability?\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568\n\n****\n\nThis notebook uses the spaCy natural language processing library to rank thousands of research papers by their semantic similarities to a specific question specified in the task. A semantic similarity score is a **number betwwen `0` and `1`** with a higher score indicting greater similarity. The results are found to be relevant to the goals of the task. The top results are presented at the end of this notebook.\n\nAn analysis of semantic similarity scores is accomplished using histograms and boxplots. It is found that the distribution of scores may be considered approximately normal.\n\nA major advantage of this approach is its simplicity while there are no readily apparent disadvantages to this approach. Computing the semantic similarity between a question and a research paper is accomplished with the following *\"simple\"* (thanks to spaCy) technique:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SpaCy setup.\nimport spacy\nspacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_lg\")\nnlp.max_length = 2e6\n\n# Compare two documents\ndoc1 = nlp(\"I like fast food\")\ndoc2 = nlp(\"I like pizza\")\nsimilarity = doc1.similarity(doc2)\n\nsimilarity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Notebook setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"!spacy info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.matcher import Matcher\nfrom IPython.core.display import display, HTML\nimport re\nimport matplotlib.pyplot as plt\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Data\n\nThe dataset used in this notebook is preprocessed and contains similarity scores for all records.\n\nThe complete dataset can be found here: https://www.kaggle.com/hvillacorte/covid19-literature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_json(\"../input/covid19-literature/df_covid_3.json\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(f\"\"\"<h2>Data description</h2><br/>\n<ul>\n    <li>\n        The dataset contains metadata and content from \n        <b>{data.shape[0]:,} scientific research papers</b>.\n    </li>\n    <li>\n        <b>Each record contains the following variables:\n        </b> {\", \".join(data.columns.values)}\n    </li>\n    <li>\n        The <code>body_text</code> feature contains the \n        sceintific content and ranges in size from \n        <b>{data.body_word_count.min():,} to \n        {data.body_word_count.max():,} words</b> with an\n        <b>average of {round(data.body_word_count.mean()):,}\n        words</b>.\n    </li>\n</ul>\"\"\"))\n\ndisplay(HTML(\"<p><b>Here is a random sample of\"\n             \" four records from the dataset:</b></p>\"))\ndata.sample(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List of questions\n\nThese questions are derived from the task then coded as a list of Python dictionaries."},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = [\n    {\"question\": \"Range of incubation periods for the disease in humans\"\n                 \" (and how this varies across age and health status) and\"\n                 \" how long individuals are contagious, even after recovery.\"},\n    {\"question\": \"Prevalence of asymptomatic shedding and transmission\"\n                 \" (e.g., particularly children).\"},\n    {\"question\": \"Seasonality of transmission.\"},\n    {\"question\": \"Physical science of the coronavirus (e.g., charge distribution,\"\n                 \" adhesion to hydrophilic/hydrophobic surfaces, environmental\"\n                 \" survival to inform decontamination efforts for affected areas\"\n                 \" and provide information about viral shedding).\"},\n    {\"question\": \"Persistence and stability on a multitude of substrates and\"\n                 \" sources (e.g., nasal discharge, sputum, urine,\"\n                 \" fecal matter, blood).\"},\n    {\"question\": \"Persistence of virus on surfaces of different materials\"\n                 \" (e,g., copper, stainless steel, plastic).\"},\n    {\"question\": \"Natural history of the virus and shedding of it from\"\n                 \" an infected person.\"},\n    {\"question\": \"Implementation of diagnostics and products to improve\"\n                 \" clinical processes.\"},\n    {\"question\": \"Disease models, including animal models for infection,\"\n                 \" disease and transmission.\"},\n    {\"question\": \"Tools and studies to monitor phenotypic change and\"\n                 \" potential adaptation of the virus.\"},\n    {\"question\": \"Immune response and immunity.\"},\n    {\"question\": \"Effectiveness of movement control strategies to prevent secondary\"\n                 \" transmission in health care and community settings.\"},\n    {\"question\": \"Effectiveness of personal protective equipment (PPE) and its\"\n                 \" usefulness to reduce risk of transmission in\"\n                 \" health care and community settings.\"},\n    {\"question\": \"Role of the environment in transmission.\"}\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate keywords\n\nKeywords are generated from each question then added to each question's dictionary. spaCy is used to filter keywords based on certain lexical attributes. These keywords are not used in the computation of semantic similarity scores as they are already computed. These keywords will be hightlighted when the top results are displayed at the end of this notebook. This is simply meant to be a visual aid and *evidence of task completion and accuracy of the methods used*."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\"<h3>Questions and their keywords</h3>\"))\n\nfor i, q in enumerate(questions):\n    qdoc = nlp(q[\"question\"])\n    key_tokens = [token.text.lower() for token in qdoc\n                  if token.pos_ not in [\"PUNCT\",\"SYM\",\"PART\",\"ADV\"]\n                  and not token.is_stop\n                  and len(token.text) > 2]\n        \n    questions[i][\"keywords\"] = key_tokens\n    keywords = \", \".join(key_tokens)\n    display(HTML(f\"\"\"<p><strong>Question #{i+1}:</strong> {qdoc.text}<br/>\n                        <strong>Keywords:</strong> {keywords}\n                     </p>\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of similarity scores"},{"metadata":{},"cell_type":"markdown","source":"## Histograms"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_histograms(df, variables, n_rows, n_cols):\n    \"\"\"Plots histograms\n    :param df: pandas dataframe\n    :param variables: columns to plot\n    :param n_rows: number of rows\n    :param n_cols: number of columns\n    :see: https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid#answer-29530596\n    \"\"\"\n    fig=plt.figure(figsize=(30,n_rows*8))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=100,ax=ax)\n        ax.set_title(\n            f\"Distribution of question #{i+1} similarity scores\"\n            , fontdict={\"fontsize\":34}\n        )\n    plt.show()\n\ndraw_histograms(\n    data,\n    [f\"q{i}_similarity\" for i, q in enumerate(questions)]\n    , math.ceil(len(questions)/2)\n    , 2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of each question's semantic similarity scores shows an overall negative skew with an aproximately normal distribution in the higher ranges for the majority of the distributions."},{"metadata":{},"cell_type":"markdown","source":"## Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,20))\nplt.title(\n    \"Quantile-based distributions of each question's similarity scores\"\n    , fontdict={\"fontsize\":34}\n)\ndata.boxplot(column=[f\"q{i}_similarity\" for i, q in enumerate(questions)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplots indicate that the vast majority of the similarity scores are in the upper ranges and that the high negative skewness of the distributions is due to low outliers in the lower ranges. The outliers may be dismissed and it may be said that the distributions are for the most part approximately normal. Outliers may possibly be attributed to non-english text, missing text or corrupted text, or they may be genuinely very dissimilar. This may be further investigated, however, we are really only interested in identifying the top results. Further efforts to improve the dataset may include language detection and translation, which may be looked into in the future.\n\nAdditonally, high outliers in the boxplots indicate that in many cases the top results are comparatively very similar to the question."},{"metadata":{},"cell_type":"markdown","source":"# Top results\n\nThe top results, along with a table of contents, will now be displayed. Keywords and numerical tokens and phrases are highlighted as follows.\n\n* <mark style=\"background:yellow\">keyword</mark>\n* <mark style=\"background:lightblue\">number with time interval</mark>\n* <mark style=\"background:lightgreen\">percentage</mark>\n\nThe number of results that are to be displayed is controlled by the variable `num_results` in the folowing code block. To increase or decrease the number of results just edit the value of the variable `num_results` as desired then rerun the code below (there is no need to run the entire notebook again, just the code below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust this variable as desired to increase\n# or decrease the number of results displayed.\nnum_results = 3\n\n# Generate table of contents.\ndisplay(HTML('<h2 id=\"toc\">Table of Contents</h2>'))\nfor i, q in enumerate(questions):\n    display(HTML(\n        f'<b>{i+1}</b>. <a href=\"#q{i}\">{q[\"question\"]}</a><br/>'\n    ))\n    \n# Generate the display for the top results per question.\nfor i, q in enumerate(questions):\n    sim_id = f\"q{i}_similarity\"\n    \n    # Create the PhraseMatcher object. The tokenizer is the first argument.\n    # Use attr = 'LOWER' to make consistent capitalization\n    matcher = PhraseMatcher(nlp.vocab, attr='LEMMA')\n    patterns = [\n        nlp(term) for term in q[\"keywords\"]\n        + [\"covid-19\",\"covid19\",\"coronavirus\",\"corona\"]\n    ]\n    matcher.add(\"MENU\",            # Just a name for the set of rules we're matching to\n            None,              # Special actions to take on matched words\n            *patterns)\n    \n    display(HTML(\n        f\"\"\"<h1 id=\"q{i}\">{q[\"question\"]}</h1>\n        <a href=\"#toc\" title=\"Table of Contents\">Back to top ↑</a>\"\"\"\n    ))\n\n    \n    dat = data.sort_values(by=f\"q{i}_similarity\", ascending=False)\n    \n    for r in dat[:num_results].iterrows():\n        i = r[0]\n        row = r[1]\n        \n        doc = nlp(row[\"body_text\"])\n        \n        \"\"\"\n        # Highlight sentences that meet a question similarity threshold.\n        sentences = [{\"q\": sent, \"sim\": nlp(q[\"question\"]).similarity(sent)}\n                                 for sent in doc.sents]        \n        sentence_similarities = [s[\"sim\"] for s in sentences]        \n        threshold = np.quantile(sentence_similarities, .75)\n        for s in sentences:\n            if s[\"sim\"] >= .9:\n                row.body_text = row.body_text.replace(\n                    s[\"q\"].text\n                    , f\"<mark>{s['q']}</mark>\")\n        \"\"\"\n        \n        matches = matcher(doc)        \n\n        if matches:\n            excerpt = row[\"body_text\"]\n            \n            for m in matches:\n                match = doc[m[1]:m[2]].text\n                excerpt = re.sub(\n                    f'([ -])(?!<mark style=\"background:yellow\">){match}'\n                    '(?!</mark>)([ ,.?!-])'\n                    , f'\\\\1<mark style=\"background:yellow\">{match}</mark>\\\\2'\n                    , excerpt)\n        else:\n            if row[\"body_text\"]:\n                excerpt = row[\"body_text\"]\n            elif row[\"abstract\"]:\n                excerpt = row[\"abstract\"]\n            else:\n                excerpt = \"NO TEXT FOUND!\"\n                \n        excerpt = re.sub(\n            ' (?!<mark style=\"background:lightblue\">)([0-9.-−]+)?'\n            ' (days?|hours?|weeks?|months?|years?)([ ,.?!])'\n            , ' <mark style=\"background:lightblue\">\\\\1 \\\\2</mark>\\\\3'\n            , excerpt)\n        \n        excerpt = re.sub(\n            '(?!<mark style=\"background:lightgreen\">)([0-9.-]+) ?(%)'\n            , '<mark style=\"background:lightgreen\">\\\\1 \\\\2</mark>'\n            , excerpt)\n        \n        display(HTML(f\"\"\"<p>\n                <strong>Title:</strong> {row[\"title\"]}<br/>\n                <strong>Authors:</strong> {row[\"authors\"]}<br/>\n                <strong>Journal:</strong> {row[\"journal\"]}<br/>\n                <strong>ID:</strong> {row[\"paper_id\"]}<br/>\n                <strong>Similarity score:</strong> {row[f\"{sim_id}\"]}<br/>\n                <strong>Number of keyword matches:</strong> {len(matches)}<br/>\n                <strong>Question:</strong> {q[\"question\"]}<br/>\n                <a href=\"#toc\" title=\"Table of Contents\">Back to top ↑</a>\n            </p>\n            <blockquote>{excerpt}</blockquote>\"\"\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}