{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport sys\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"titles = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\ntitles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(titles.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Data wrangling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"description_category = titles[['listed_in','description']]\ndescription_category['listed_in'] = description_category['listed_in'].apply(lambda x: x.split(', '))\ndescription_category.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['category_1', 'category_2', 'category_3']\ncat = pd.DataFrame(description_category['listed_in'].to_list(), columns = columns)\ncat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_1_genres = cat.category_1.unique()\ncategory_2_genres = cat.category_2.unique()\ncategory_3_genres = cat.category_3.unique()\n\ngenres = np.concatenate([category_1_genres, category_2_genres, category_3_genres])\ngenres = list(dict.fromkeys(genres))\ngenres = [x for x in genres if x is not None]\nlen(genres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = pd.concat([cat,pd.DataFrame(columns = list(genres))])\ncat.fillna(0, inplace = True)\ncat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 0\nfor genre in cat['category_1']:\n    if genre != 0:\n        cat.loc[row, genre] = 1\n    row = row + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 0\nfor genre in cat['category_2']:\n    if genre != 0:\n        cat.loc[row, genre] = 1\n    row = row + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 0\nfor genre in cat['category_3']:\n    if genre != 0:\n        cat.loc[row, genre] = 1\n    row = row + 1\n    \ncat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description_category = pd.concat([description_category['description'], \n                                     cat.loc[:,\"Children & Family Movies\":]],\n                                    axis=1)\ndescription_category.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **EDA - Exploratory data analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot = pd.DataFrame()\nbar_plot['cat'] = description_category.columns[1:]\nbar_plot['count'] = description_category.iloc[:,1:].sum().values\nbar_plot.sort_values(['count'], inplace=True, ascending=False)\nbar_plot.reset_index(inplace=True, drop=True)\nbar_plot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.set(font_scale = 1.5)\nsns.set_style('whitegrid') \n\n\npal = sns.color_palette(\"Blues_r\", len(bar_plot))\nrank = bar_plot['count'].argsort().argsort()  \n\nsns.barplot(bar_plot['cat'], bar_plot['count'], palette=np.array(pal[::-1])[rank])\nplt.axhline(threshold, ls='--', c='red')\nplt.title(\"Most commons categories\", fontsize=24)\nplt.ylabel('Number of titles', fontsize=18)\nplt.xlabel('Genre', fontsize=18)\nplt.xticks(rotation='vertical')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_categories = pd.DataFrame()\nmain_categories = bar_plot[bar_plot['count']>200]\ncategories = main_categories['cat'].values\ncategories = np.append(categories,'Others')\nnot_category = []\ndescription_category['Others'] = 0\n\nfor i in description_category.columns[1:]:\n    if i not in categories:\n        description_category['Others'][description_category[i] == 1] = 1\n        not_category.append(i)\n\ndescription_category.drop(not_category, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_cat = pd.DataFrame()\nmost_common_cat['cat'] = description_category.columns[1:]\nmost_common_cat['count'] = description_category.iloc[:,1:].sum().values\nmost_common_cat.sort_values(['count'], inplace=True, ascending=False)\nmost_common_cat.reset_index(inplace=True, drop=True)\nmost_common_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.set(font_scale = 1.5)\nsns.set_style('whitegrid') \n\n\npal = sns.color_palette(\"Blues_r\", len(most_common_cat))\nrank = most_common_cat['count'].argsort().argsort()  \n\nsns.barplot(most_common_cat['cat'], most_common_cat['count'], palette=np.array(pal[::-1])[rank])\nplt.axhline(threshold, ls='--', c='red')\nplt.title(\"Most commons categories\", fontsize=24)\nplt.ylabel('Number of titles', fontsize=18)\nplt.xlabel('Genre', fontsize=18)\nplt.xticks(rotation='vertical')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rowSums = description_category.iloc[:,1:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1.5)\nsns.set_style('whitegrid') \nplt.figure(figsize=(10,6))\n\nsns.barplot(multiLabel_counts.index, multiLabel_counts.values)\nplt.title(\"Number of categories per title\", fontsize=24)\nplt.ylabel('Number of titles', fontsize=18)\nplt.xlabel('Number of categories', fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = description_category.copy()\nboxplot['len'] = description_category.description.apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\",rc={\"font.size\":13,\"axes.labelsize\":13})\n\nplt.figure(figsize=(9, 4))\n\nax = sns.boxplot(x='len', data=boxplot, orient=\"h\", palette=\"Set2\")\nplt.ylabel('')\nplt.xlabel('Words')\nplt.title(\"Distribution of the word frequency\", fontsize=13)\nplt.tight_layout(h_pad=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n\nplt.figure(figsize=(40,25))\ntext = description_category.description.values\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\nplt.axis('off')\nplt.title(\"Common words on the description\",fontsize=40)\nplt.imshow(cloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['OneVsAll', 'BinaryRelevance', 'ClassifierChain', 'MultipleOutput','DNN', 'CNN', 'LSTM']\nresults = pd.DataFrame(columns = columns)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seeds = [1, 43, 678, 90, 135]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = results.copy()\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract(sentence):\n    # specific\n    sentence = re.sub(r\"won't\", \"will not\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n\n    # general\n    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    return sentence\n\ndef cleanPunc(sentence): \n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent\n\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\"\", sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stopwords) + \")\\\\W\", re.I)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description_category['description'] = description_category['description'].str.lower()\ndescription_category['description'] = description_category['description'].apply(decontract)\ndescription_category['description'] = description_category['description'].apply(cleanPunc)\ndescription_category['description'] = description_category['description'].apply(keepAlpha)\ndescription_category['description'] = description_category['description'].apply(removeStopWords)\ndescription_category['description'] = description_category['description'].apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(description_category['description'], \n                                                    description_category[description_category.columns[1:]], \n                                                    test_size=0.3, \n                                                    random_state=seeds[4], \n                                                    shuffle=True)\nvectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\nvectorizer.fit(X_train)\n\nX_train = vectorizer.transform(X_train)\nX_test = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n\nLR_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1))])\naccuracy = 0\nfor category in description_category.columns[1:]:\n    print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    LR_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = LR_pipeline.predict(X_test)\n    accuracy = accuracy + accuracy_score(y_test[category], prediction)\n    print('AUC ROC score is {}'.format(roc_auc_score(y_test[category],prediction)))\n    print(\"\\n\")\n\nprint('Test averaged AUC ROC is {}'.format(accuracy/len(description_category.columns[1:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nNB_pipeline = Pipeline([('clf', OneVsRestClassifier(MultinomialNB(\n                    fit_prior=True, class_prior=None)))])\naccuracy = 0\nfor category in description_category.columns[1:]:\n    print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    NB_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = NB_pipeline.predict(X_test)\n    accuracy = accuracy + accuracy_score(y_test[category], prediction)\n    print('AUC ROC is {}'.format(roc_auc_score(y_test[category],prediction)))\n    print(\"\\n\")\n\nprint('Test averaged AUC ROC is {}'.format(accuracy/len(description_category.columns[1:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nSVC_pipeline = Pipeline([('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1))])\n\naccuracy = 0\nfor category in description_category.columns[1:]:\n    print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    SVC_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = SVC_pipeline.predict(X_test)\n    accuracy = accuracy + roc_auc_score(y_test[category], prediction)\n    print('F1-score is {}'.format(roc_auc_score(y_test[category],prediction)))\n    print(\"\\n\")\n\nprint('Test averaged f1-score is {}'.format(accuracy/len(description_category.columns[1:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF_pipeline = Pipeline([('clf', OneVsRestClassifier(RandomForestClassifier(), n_jobs=1))])\n\naccuracy = 0\nfor category in description_category.columns[1:]:\n    print('**Processing {} titles...**'.format(category))\n    \n    # Training logistic regression model on train data\n    RF_pipeline.fit(X_train, y_train[category])\n    \n    # calculating test accuracy\n    prediction = RF_pipeline.predict(X_test)\n    accuracy = accuracy + roc_auc_score(y_test[category], prediction)\n    print('AUC ROC is {}'.format(roc_auc_score(y_test[category],prediction)))\n    print(\"\\n\")\n\nprint('Test averaged AUC ROC is {}'.format(accuracy/len(description_category.columns[1:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'OneVsAll'] = accuracy/len(description_category.columns[1:])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = BinaryRelevance(GaussianNB())\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\naccuracy_score(y_test,predictions)\nprint('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'BinaryRelevance'] = roc_auc_score(y_test,predictions.toarray())\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = ClassifierChain(LogisticRegression())\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\n\nprint('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'ClassifierChain'] = roc_auc_score(y_test,predictions.toarray())\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf = MultiOutputClassifier(KNeighborsClassifier()).fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\nprint('AUC ROC is {}'.format(roc_auc_score(y_test,predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'MultipleOutput'] = roc_auc_score(y_test,predictions)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Deep Learning**"},{"metadata":{},"cell_type":"markdown","source":"**Word Embedding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"description_category['description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=5000, lower=True)\ntokenizer.fit_on_texts(description_category['description'])\nsequences = tokenizer.texts_to_sequences(description_category['description'])\nx = pad_sequences(sequences, maxlen=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, \n                                                    description_category[description_category.columns[1:]], \n                                                    test_size=0.3, \n                                                    random_state=seeds[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_cat['class_weight'] = len(most_common_cat) / most_common_cat['count']\nclass_weight = {}\nfor index, label in enumerate(categories):\n    class_weight[index] = most_common_cat[most_common_cat['cat'] == categories]['class_weight'].values[0]\n    \nmost_common_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = y_train.shape[1]\nmax_words = len(tokenizer.word_index) + 1\nmaxlen = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nimport tensorflow as tf\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 20, input_length=maxlen))\n#model.add(Dropout(0.2))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(num_classes, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\ncallbacks = [\n    ReduceLROnPlateau(),\n    #EarlyStopping(patience=10),\n    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)\n]\n\n\nhistory = model.fit(X_train, y_train,\n                    class_weight=class_weight,\n                    epochs=30,\n                    batch_size=32,\n                    validation_split=0.3,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = model\nmetrics = cnn_model.evaluate(X_test, y_test)\nprint(\"{}: {}\".format(cnn_model.metrics_names[1], metrics[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'DNN'] = metrics[1]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n\nfilter_length = 300\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 20, input_length=maxlen))\n#model.add(Dropout(0.5))\nmodel.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n\ncallbacks = [\n    ReduceLROnPlateau(),\n    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n]\n\nhistory = model.fit(X_train, y_train,\n                    class_weight=class_weight,\n                    epochs=30,\n                    batch_size=32,\n                    validation_split=0.3,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = model\nmetrics = cnn_model.evaluate(X_test, y_test)\nprint(\"{}: {}\".format(model.metrics_names[1], metrics[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.loc[4,'CNN'] = metrics[1]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GloVe - LSTM** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\n\nglove_file = open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\nembedding_matrix = zeros((max_words, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from keras.layers import Input\nfrom keras.layers import Flatten, LSTM\nfrom keras.models import Model\n\ndeep_inputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(max_words, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\nLSTM_Layer_1 = LSTM(128)(embedding_layer)\ndense_layer_1 = Dense(21, activation='sigmoid')(LSTM_Layer_1)\nmodel = Model(inputs=deep_inputs, outputs=dense_layer_1)\n\ncallbacks = [\n    ReduceLROnPlateau(),\n    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n]\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\nhistory = model.fit(X_train, y_train.values,\n                    class_weight=class_weight,\n                    batch_size=32, \n                    epochs=30, \n                    validation_split=0.3,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = model.evaluate(X_test, y_test)\nprint(\"{}: {}\".format(model.metrics_names[1], metrics[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"results.loc[4,'LSTM'] = metrics[1]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Results**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def highlight_max(data, color='yellow'):\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_max = data == data.max()\n        return [attr if v else '' for v in is_max]\n    else:  # from .apply(axis=None)\n        is_max = data == data.max().max()\n        return pd.DataFrame(np.where(is_max, attr, ''),\n                            index=data.index, columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def highlight_min(data, color='yellow'):\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_min = data == data.min()\n        return [attr if v else '' for v in is_min]\n    else:  # from .apply(axis=None)\n        is_min = data == data.min().min()\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.style.apply(highlight_min, color='red', axis=None).apply(highlight_max, color='lightgreen', axis=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import LinearSegmentedColormap\n\n#cm = sns.light_palette(\"green\", as_cmap=True)\ncm = LinearSegmentedColormap.from_list(\n    name='test', \n    #colors=['red','white','green','white','red']\n    colors=['tomato','orange','white','lightgreen','green']\n)\n\nt = results.apply(pd.to_numeric).style.background_gradient(cmap=cm)\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def background_gradient(s, m=None, M=None, cmap=cm, low=0, high=0):\n\n    if m is None:\n        m = 0.88\n    if M is None:\n        M = 0.8923\n    rng = M - m\n    norm = colors.Normalize(m - (rng * low),\n                            M + (rng * high))\n    normed = s.apply(lambda x: norm(x.values))\n\n    cm = plt.cm.get_cmap(cmap)\n    c = normed.applymap(lambda x: colors.rgb2hex(cm(x)))\n    ret = c.applymap(lambda x: 'background-color: %s' % x)\n    return ret\n\n\nresults.style.apply(background_gradient, axis=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}