{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analysis of Airbnb data from Munich, Germany\n\n![https://images.portal.muenchen.de/upload/media/000/000/218/415/resized/0750x0310/marienplatz-750.jpg](https://images.portal.muenchen.de/upload/media/000/000/218/415/resized/0750x0310/marienplatz-750.jpg)\n\n**Motivation: ** Being relatively new to the field of data science, I am enthusiastic about the information that can be obtained from data. Especially in how businesses can use data to make decisions! For my training I chose an Airbnb data set to be examined in this notebook - and because I live in Munich, Germany I find that data particularly interesting.\n\n**Dataset: ** The used data sets were created on November 25th, 2019 and contain detailed listings data, review data and calendar data of current Airbnb listings in Munich, Germany. This data was created by Murray Cox and his Inside Airbnb project which can be found here:  http://insideairbnb.com/get-the-data.html\n\nThe data set can also be found here: https://www.kaggle.com/chriskue/munich-airbnb-data\n\n**Methodology: ** For my analysis I will use the *CRISP-DM* Methodology. CRIPS-DM stands for \"cross-industry process for data mining\". It provides a structured approach to planning a data mining project. The process consists of six steps:\n\n1. Business Understanding\n1. Data Understanding\n1. Data Preparation\n1. Modeling\n1. Evaluation\n1. Deployment\n\nThe underlying Jupyter notebook can be found on Github (https://github.com/noema-git/airbnb-analysis)\n\n**Let's get started ...**"},{"metadata":{},"cell_type":"markdown","source":"# 1. Business Understanding  \nThe phase of the business understanding is about defining the specific goals and requirements for data mining. The result of this phase is the formulation of the task and the description of the planned rough procedure. In this phase, goals and specific questions are defined. Users and analysts exchange information on tasks and expectations. Appropriate procedures for the task are discussed and defined. In this phase, the criteria for success are finally set.\n\nAirbnb is a community-based platform that supports magical travel that is local, authentic and unique. Airbnb has more than 5 million listings and operates in 191 countries and more than 81,000 cities. To date, the Airbnb community has hosted nearly half-a-billion guests through a model designed to support healthy travel. Moreover, it cultivates a sharing-economy by allowing property owners to rent out private flats worldwide. (Source: https://news.airbnb.com/en-us/an-update-about-our-community-in-san-francisco/)\n\n\n**In order to get a better insight into the business model of Airbnb, especially in Munich, I am particularly interested in the answers to the following three questions:**\n1. When is the most expensive time of the year to visit Munich and how much do the price spike?\n2. What are the most expensive neighbourhoods in Munich? \n3. What factors influence the price most?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import all the libraries which will be needed later\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.utils import shuffle\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Understanding\nAs part of the data understanding, an attempt is made to get a first overview of the available data and their quality. The data quality is analyzed and evaluated. Problems with the quality of the existing data in relation to the task defined in the previous phase must be identified. Afterwards you become familiar with the data in the phase of data understanding. The quality and reliability of the data is also checked in this phase. For this purpose, all observations are named and appropriate corrections are implemented.\n\n## 2.1. Load the relevant dataset  \nThe available csv data sets from Inside Airbnb (http://data.insideairbnb.com/germany/bv/munich/2019-11-25/data/) was stored in a separately dataset (https://www.kaggle.com/chriskue/munich-airbnb-data) for further analysis.\n\nThe data set contains the following files:\n*     **listings.csv**:  descriptions and review score\n*     **calendar.csv**:  listing id, price and availability for the upcoming year\n*     **reviews.csv**:   unique id for each reviewer and detailed comments"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"df_calendar = pd.read_csv(\"../input/munich-airbnb-data/calendar.csv\");\ndf_listings = pd.read_csv(\"../input/munich-airbnb-data/listings.csv\");\ndf_reviews = pd.read_csv(\"../input/munich-airbnb-data/reviews.csv\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Understand the underlaying data\nTo get a better understanding of the data we will look at the features and the overall quality of the data ( e.g. missing values)."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1. Dataset \"calendar.csv\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at a concise summary of the DataFrame 'calendar'\ndf_calendar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the first 5 rows of the data set\ndf_calendar.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all features in this data set and show the number of missing values\nobj = df_calendar.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 / df_calendar['listing_id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary after analysing the dataset \"calendar\"**  \nThe data set consists of 7 features and a total rows of 4.190.565.\nThe overall quality is good, only the features 'price' and 'adjusted_price' have missing data (171).\n\nFor further analysis the following data cleaning is required:\n- Drop feature 'adjusted_price'.\n- Remove the rows with missing data in 'price' (number of affected rows is low ~0.004%).\n- The feature 'price' needs to be converted to numerical value\n- The feature 'date' needs to be converted to datetime format\n- The feature 'available' needs to be converted into Boolean data type"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2. Dataset \"listing.csv\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at a concise summary of the DataFrame 'listings'\ndf_listings.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the first 5 rows of the data set\ndf_listings.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all features in this data set and show the number of missing values\nobj = df_listings.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 / df_listings['id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count distinct observations per feature\ndf_listings.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary after analysing the dataset \"listings\"**  \nThe dataset consists of 106 features and a total rows of 11.481.  \nThe overall quality is not for all features good. It must be considered how to deal with the remaining missing values. Filling does not look very promising in all cases, so the features should be droped from a threshold.\nSome features have only constant values and don't help us any further.\n\nFor further analysis the following data cleaning is required:  \n- Drop features with constant values \n- Drop features with more than 50% missing data\n- Fill missing numerical data with mean value\n- Convert features to useable data type (e.g. price)\n- Drop features that do not provide us with any useful information (for our specific questions) \n\nAppendix: Consider whether the outliers in feature 'price' should be dropped."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.3 Dataet \"reviews.csv\"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at a concise summary of the DataFrame 'reviews'\ndf_reviews.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the first 5 rows of the data set\ndf_reviews.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all features in this data set and show the number of missing values\nobj = df_reviews.isnull().sum()\nfor key,value in obj.iteritems():\n    percent = round((value * 100 / df_reviews['id'].index.size),3)\n    print(key,\", \",value, \"(\", percent ,\"%)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the number reviews over time to see any patterns\ndf_reviews_plot = df_reviews.groupby('date')['id'].count().reset_index()\ndf_reviews_plot[\"rolling_mean\"] = df_reviews_plot.id.rolling(window=30).mean()\ndf_reviews_plot['date'] = pd.to_datetime(df_reviews_plot['date'])\n\nplt.figure(figsize=(20, 10));\nplt.plot(df_reviews_plot.date, df_reviews_plot.rolling_mean);\n\nplt.title(\"Number of reviews by date (monthly mean)\");\nplt.xlabel(\"time\");\nplt.ylabel(\"reviews\");\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary after analysing the dataset \"reviews\"**  \nThe dataset consists of 6 features and a total of 175.561 rows.   \nThe overall quality is good, only the feature 'comments' has missing data (74).  \nAt first sight the data set cannot be used to answer the questions. Nonetheless, the number of reviews shows an interesting pattern which should also be examined. \nMaybe the feature 'id' can be connected to the other data sets 'calendar' or 'listings'. \nIf I used the data set for my further analysis, I would drop missing data in 'comments' (number of affected rows is low ~0.04%) and convert the feature 'date' in to the 'DateTime' data type."},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Preparation\nThe data preparation is used to create a final data set that forms the basis for the next phase of the modeling. In data preparation you then create the data set used for the further analyzes. Variables are encoded or transformed if necessary. Appropriate procedures for missing data can be used. Experience has shown that a large part of the time is required for this phase. Only if the data is valid and reliable can CRISP-DM predictive analytics deliver reliable results."},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Preparation for Question 1\n### \"When is the most expensive time of the year to visit Munich and how much do the price spike?\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data to a new DataFrame for further clean up\ndf_calendar_clean = df_calendar.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean up the data set \"calendar\" as the previous analysis pointed out\n\n# Drop \"adjusted_price\"\ndf_calendar_clean = df_calendar_clean.drop(\"adjusted_price\", axis = 1)\n\n# Remove missing values\ndf_calendar_clean.dropna(how='all', inplace=True)\n\n# Convert the data type of feature 'date' from object to DateTime\ndf_calendar_clean['date'] = pd.to_datetime(df_calendar_clean['date'])\n\n# clean up the format of the 'price' values. Maybe not the best solution - but will do the job\ndf_calendar_clean['price'] = df_calendar_clean['price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n\n# Convert the feature 'available' to boolean data type\n# (This conversion is actually not necessary for further analysis)\ndf_calendar_clean['available'] = df_calendar_clean['available'].replace({'t': True, 'f':False})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group the data by mean price per date\ndf_calendar_clean = df_calendar_clean.groupby('date')['price'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the mean price over time\nplt.figure(figsize=(20, 10));\nplt.plot(df_calendar_clean.date, df_calendar_clean.price);\n\nplt.title(\"Mean price by date\");\nplt.xlabel(\"date\");\nplt.ylabel(\"price\");\nplt.grid();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new feature 'month'\ndf_calendar_clean[\"month\"] = df_calendar_clean[\"date\"].dt.month\n\n# Show a Boxplot to see the price distribution per month\nplt.figure(figsize=(20, 10))\nboxplot = sns.boxplot(x = 'month',  y = 'price', data \n                      = df_calendar_clean).set_title('Distribution of price per month');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new feature 'weekday'\ndf_calendar_clean[\"weekday\"] = df_calendar_clean[\"date\"].dt.weekday_name\n\n# Show a Boxplot to see the price distribution per weekday\nplt.figure(figsize=(20, 10))\nsns.boxplot(x = 'weekday',  y = 'price', data \n            = df_calendar_clean).set_title('Distribution of price per weekday');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the statistics per weekday\ndf_calendar_clean.groupby(['weekday'])['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the statistics per per week of the year\ndf_calendar_clean[\"week\"] = df_calendar_clean[\"date\"].dt.week\ndf_calendar_clean.groupby(['week'])['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show a Boxplot to see the price distribution per week\nplt.figure(figsize=(20, 10))\nsns.boxplot(x = 'week',  y = 'price', data \n            = df_calendar_clean).set_title('Distribution of price per week');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary for Question 1: \"When is the most expensive time of the year to visit Munich and how much do the price spike?\"\n\nRegardless of the location or other properties of the apartment, the following information results:\n- **There are two periods in the year in which the price differs significantly.**  \n    The mean price in the year is ~114 USD.  \n    a) In weeks 39 and 40 the price increases on average by ~13% to ~129 USD. It looks like there is an Oktoberfest effect at Airbnb.  \n    b)  In weeks 48 and 49 the price drops on average by 21% to ~94 USD. The drop in price is not obvious and requires additional research. \n- **The mean price rises slightly at the weekends (Friday and Saturday).**  \n    During the week the mean price is around ~112 USD and on the weekend around ~114 USD; an increase of ~ 2%.   \n- **The price of apartments also rises during certain times of the year.**  \n    It is believed that both the German holidays and large exhibitions could have an impact on this.  \n    But this information cannot be obtained from the examined data set, and would needs additional analysis (e.g. in comparison with the holiday calendar)\n\n**Answer:**  \nThe most expensive time of the year 2020 is between the end of September and the beginning of October during the Oktoberfest.\nThe mean price spikes around +15 USD."},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Preparation for Question 2/3\n### \"What are the most expensive neighbourhoods in Munich?\" and \"What factors influence the price most?\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data to a new DataFrame for further clean up\ndf_listings_clean = df_listings.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean up the data set \"listings\" as the previous analysis pointed out\n\n# Drop features which are not used further \nfeatures_to_drop = ['listing_url', 'picture_url','host_url', 'host_thumbnail_url', 'host_picture_url',\n                    'name', 'summary', 'space', 'neighborhood_overview', 'transit', 'interaction', 'description',\n                    'host_name', 'host_location', 'host_neighbourhood', 'street', 'last_scraped', 'zipcode',\n                    'calendar_last_scraped', 'first_review', 'last_review', 'host_since', 'calendar_updated',\n                    'experiences_offered', 'state', 'country', 'country_code', 'city', 'market',\n                    'host_total_listings_count', 'smart_location']\ndf_listings_clean.drop(features_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove constant features by finding unique values per feature \ndf_listings_clean = df_listings_clean[df_listings_clean.nunique().where(df_listings_clean.nunique()!=1).dropna().keys()]\n\n# Drop features with 50% or more missing values\nmore_than_50 = list(df_listings_clean.columns[df_listings_clean.isnull().mean() > 0.5])\ndf_listings_clean.drop(more_than_50, axis=1, inplace=True)\n\n# Clean up the format values. Maybe not the best solution - but will do the job.\ndf_listings_clean['price'] = df_listings_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\ndf_listings_clean['extra_people'] = df_listings_clean['extra_people'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\ndf_listings_clean['cleaning_fee'] = df_listings_clean['cleaning_fee'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n        \n# Convert rates type from string to float and remove the % sign\ndf_listings_clean['host_response_rate'] = df_listings_clean['host_response_rate'].str.replace('%', '').astype(float)\ndf_listings_clean['host_response_rate'] = df_listings_clean['host_response_rate'] * 0.01\n    \n# Covert boolean data from string data type to boolean\nboolean_features = ['instant_bookable', 'require_guest_profile_picture', \n                'require_guest_phone_verification', 'is_location_exact', 'host_is_superhost', 'host_has_profile_pic', \n                'host_identity_verified']\ndf_listings_clean[boolean_features] = df_listings_clean[boolean_features].replace({'t': True, 'f': False})\n\n## Fill numerical missing data with mean value\nnumerical_feature = df_listings_clean.select_dtypes(np.number)\nnumerical_columns = numerical_feature.columns\n\nimp_mean = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nimp_mean = imp_mean.fit(numerical_feature)\n\ndf_listings_clean[numerical_columns] = imp_mean.transform(df_listings_clean[numerical_columns])\n     \n# Remove all remaining missing values  \ndf_listings_clean.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show price statistic for each neighbourhood  \ndf_listings_clean.groupby([\"neighbourhood_cleansed\"])[\"price\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature 'mean' with the mean price per neighbourhood\ndf_listings_clean['mean']=df_listings_clean.groupby('neighbourhood_cleansed')['price'].transform(lambda r : r.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listings_plot = df_listings_clean\ndf_listings_plot = df_listings_plot.groupby('neighbourhood_cleansed')[['price']].mean()\ndf_listings_plot = df_listings_plot.reset_index()\ndf_listings_plot = df_listings_plot.sort_values(by='price',ascending=False)\ndf_listings_plot.plot.bar(x='neighbourhood_cleansed', y='price', color='blue', rot=90, figsize = (20,10)).set_title('Mean Price per Neighbourhood');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we also have the geo data (latitude and longitude) of the apartments we can create a map\nfig = px.scatter_mapbox(df_listings_clean, color=\"mean\", lat='latitude', lon='longitude',\n                        center=dict(lat=48.137154, lon=11.576124), zoom=10,\n                        mapbox_style=\"stamen-terrain\",width=1000, height=800);\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary for Question 2: \"What are the most expensive neighbourhoods in Munich?\"\n\nFrom the analysis, there is a clear difference in the costs between the different neighbourhoods in Munich.   \nIn general - the closer the apartment is to the city center or the Munich fair (Messe Munich), the higher the price.  \nFurthermore the outliers must also be taken into account, since there are individual apartments in which the price deviates considerably.\n\n**Answer:**  \nThe TOP 3 expensive neighbourhoods in Munich (average) are   \n- Altstadt-Lehel \n- Trudering-Riem\n- Allach-Untermenzing"},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling\nAs part of the modeling, the methods of data mining suitable for the task are applied to the data set created in the data preparation. Typical for this phase are the optimization of the parameters and the creation of several models. In modeling, you perform the procedures necessary to answer the questions. Usually, different parameters have to be varied and different models created. If predictive models are formed, one speaks of CRISP-DM predictive analytics. There are a number of possible data mining methods for this step, the applicability of which largely depends on the question."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data to a new DataFrame for encoding \ndf_listings_encoded = df_listings_clean.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the remaining features and the data type\ndf_listings_encoded.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers of the feature 'price\" - drop all higher than 90% quantile\noutliers = df_listings_encoded[\"price\"].quantile(0.90)\ndf_listings_encoded = df_listings_encoded[df_listings_encoded[\"price\"] < outliers]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode features for use in machine learing model\n\n# Encode feature 'amenities' and concat the data\ndf_listings_encoded.amenities = df_listings_encoded.amenities.str.replace('[{\"\"}]', \"\")\ndf_amenities = df_listings_encoded.amenities.str.get_dummies(sep = \",\")\ndf_listings_encoded = pd.concat([df_listings_encoded, df_amenities], axis=1) \n\n# Encode feature 'host_verification' and concat the data\ndf_listings_encoded.host_verifications = df_listings_encoded.host_verifications.str.replace(\"['']\", \"\")\ndf_verification = df_listings_encoded.host_verifications.str.get_dummies(sep = \",\")\ndf_listings_encoded = pd.concat([df_listings_encoded, df_verification], axis=1)\n    \n# Encode feature 'host_response_time'\ndict_response_time = {'within an hour': 1, 'within a few hours': 2, 'within a day': 3, 'a few days or more': 4}\ndf_listings_encoded['host_response_time'] = df_listings_encoded['host_response_time'].map(dict_response_time)\n\n# Encode the remaining categorical feature \nfor categorical_feature in ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'neighbourhood', \n                            'cancellation_policy']:\n    df_listings_encoded = pd.concat([df_listings_encoded, \n                                     pd.get_dummies(df_listings_encoded[categorical_feature])],axis=1)\n        \n# Drop features\ndf_listings_encoded.drop(['amenities', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', \n                          'host_verifications', 'neighbourhood','cancellation_policy','security_deposit',\n                          'id', 'host_id', 'mean', 'latitude', 'longitude'],\n                         axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last check if there are any missing values in the data set\nsum(df_listings_encoded.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the data to ensure a good distribution\ndf_listings_encoded = shuffle(df_listings_encoded)\n\nX = df_listings_encoded.drop(['price'], axis=1)\ny = df_listings_encoded['price']\n\n# Split the data into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Initalize the model\nmodel = RandomForestRegressor(max_depth=15, n_estimators=100, criterion='mse', random_state=42)\n# Fit the model on training data\nmodel.fit(X, y)\n        \n# Predict results\nprediction = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Evaluation\n\nThe evaluation ensures an exact comparison of the created data models with the task and selects the most suitable model. In the evaluation phase, you compare the models created from CRISP-DM predictive analytics. Various parameters of model quality are used. Often a balance is made between the goodness of adaptation and the complexity of the models as well as their applicability. Based on the results, phases are repeated or the last phase of the CRISP-DM model is initiated.\n\nFor the evaluation we look at the coefficient of determination (r squared value) of the training set and the test set, so we can compare the quality of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the result - compare r squared of the training set with the test set\n\n# Find R^2 on training set\nprint(\"Training Set:\")\nprint(\"R_squared:\", round(model.score(X_train, y_train) ,2))\n\n# Find R^2 on testing set\nprint(\"\\nTest Set:\")\nprint(\"R_squared:\", round(model.score(X_test, y_test), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot of th actual vs predicted data\nplt.figure(figsize=(10, 10))\nplt.grid()\nplt.xlim((0, 200))\nplt.ylim((0, 200))\nplt.plot([0,200],[0,200], color='#AAAAAA', linestyle='dashed')\nplt.scatter(y_test, prediction, alpha=0.5)\ncoef = np.polyfit(y_test,prediction,1)\npoly1d_fn = np.poly1d(coef) \nplt.plot(y_test, poly1d_fn(y_test))\nplt.title('Actual vs. Predicted data');\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Predicted values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort the importance of the features\nimportances = model.feature_importances_\n    \nvalues = sorted(zip(X_train.columns, model.feature_importances_), key=lambda x: x[1] * -1)\nfeature_importances = pd.DataFrame(values, columns = [\"feature\", \"score\"])\nfeature_importances = feature_importances.sort_values(by = ['score'], ascending = False)\n\nfeatures = feature_importances['feature'][:10]\ny_feature = np.arange(len(features))\nscore = feature_importances['score'][:10]\n\n# Plot the importance of a feature to the price\nplt.figure(figsize=(20,10));\nplt.bar(y_feature, score, align='center');\nplt.xticks(y_feature, features, rotation='vertical');\nplt.xlabel('Features');\nplt.ylabel('Score');\nplt.title('Importance of features (TOP 10)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary for Question 3: \"What factors influence the price most?\"\n\nWith a coefficient of determination (R^2) of 0.87, the model and the prediction seems accurate enough to predict the price of an apartment. Moreover, the coefficient of determination of the training data is the same.\n\n**Answer:**  \nThe TOP 5 factors of an apartment that have the greatest influence on the price are:\n- Accommodates\n- Entire home/ Apartment\n- Extra people\n- Number of reviews\n- Guests included"},{"metadata":{},"cell_type":"markdown","source":"# 6. Deployment\n\nThe deployment is the final phase of the CRISP-DM process.   \nIn this phase, the results obtained are processed in order to present them and feed them into the client's decision-making process. And in the last step of the CRISP-DM model, the results obtained are summarized, processed and presented in an understandable way\n\nThis blog post and the Repo on Github (https://github.com/noema-git/airbnb-analysis) is the deployment of this work. To improve the quality of the model the CRSIP-DM cycle should be with adjusted parameters run through again.\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/240px-CRISP-DM_Process_Diagram.png](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/240px-CRISP-DM_Process_Diagram.png)"},{"metadata":{},"cell_type":"markdown","source":"# Summary and conclusion"},{"metadata":{},"cell_type":"markdown","source":"This notebook uses data from the Munich, Germany area of Airbnb and has been analyzed to answer the following questions. \n\n### When is the most expensive time of the year to visit Munich and how much do the price spike?\n**Answer:**   \n The most expensive time of the year 2020 is between the end of September and the beginning of October during the Oktoberfest. The average price spikes around +15 USD.\n### What are the most expensive neighbourhoods in Munich?  \n**Answer:**   \nThe TOP 3 expensive neighbourhoods in Munich (in average) are:\n- Altstadt-Lehel\n- Trudering-Riem\n- Allach-Untermenzing\n\n### What factors influence the price most?\n**Answer:**\nThe TOP 5 factors of an apartment that have the greatest influence on the price are:\n- Accommodates\n- Entire home/ Apartment\n- Extra people\n- Number of reviews\n- Guest included\n\nThis analysis was done as a project within the Udacity Data Science Nanodegree.   \nAny kind of optimization is very welcome. I really appreciate any feedback to improvement.  \n\nThe underlying Jupyter Notebook to this evaluation can be found on GitHub (https://github.com/noema-git/airbnb-analysis)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}