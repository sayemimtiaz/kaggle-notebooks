{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID 19 NLP Notebook"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfilelist = []\n\n# LOAD .json file paths into array\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        extension = os.path.splitext(filename)[1];\n        if extension == '.json':\n            filelist.append(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nprint('Total JSON files', len(filelist))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load files into Pandas DataFrame\n\nWe load text from JSON files into DataFrame for easier manipulation. Note: I am loading first 100 files only (total 13k) to test the concept."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import json\n\n# Create DataFrame -- TODO add more cols if needed\ndf = pd.DataFrame(columns=[ \"paper_id\", \"text\"])\n\n\n# FILTER if includes string in the body text\nfilter_by_string = 'vaccine'\n\n# NOTE: trying on 100 first files only\n# TODO: bump this number when ready\nfor file in filelist[:100]:\n    f = open(file)\n    raw = f.read()\n    loaded_json = json.loads(raw)\n\n    # Create body text (joined)\n    collector = ''\n    for item in loaded_json['body_text']:\n        collector = collector + ' ' +item['text']\n\n    # Try to filter\n    if (filter_by_string in collector):\n        entry = {\n        'paper_id': loaded_json['paper_id'],\n        'text': collector\n        }\n        df = df.append(entry, ignore_index=True)    \n\n\nprint ('Total files including word', filter_by_string, len(df))\ndf.sample(3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Word Frequency\n\n\nLet's try to see the most common word using `PorterStemmer` from NLTK."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n\nfreqTable = dict()\ndef create_frequency_table(text_string):\n    stopWords = set(stopwords.words(\"english\"))\n    words = word_tokenize(text_string)\n    ps = PorterStemmer()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n## Iterate and create Frequency table\nfor txt in df['text']:\n    create_frequency_table(txt)\n            \n\nfreq = pd.DataFrame.from_dict(freqTable, orient='index')\nfreq.reset_index(level=0, inplace=True)\nfreq.columns = ['word', 'frequency']\n\nfreq['word_length'] = freq['word'].apply(lambda x: len(x))\n\n# Sort & Remove short ones\nfreq = freq.where(freq['word_length'] > 5).sort_values('frequency', ascending=0).reset_index()\nfreq = freq.drop('index', axis='columns')\nfreq = freq[0:30] # we care about top 30\nfreq.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nouns with SpaCy\n\nFind noun phrases with spacy."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n\nfreq_table2 = dict()\nfor txt in df['text']:\n    doc = nlp(txt)\n    nouns = [chunk.text for chunk in doc.noun_chunks]\n    for word in nouns:\n        if word in freq_table2:\n            freq_table2[word] += 1\n        else:\n            freq_table2[word] = 1\n\n# for token in doc:\n#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n#             token.shape_, token.is_alpha, token.is_stop)\n\nfreq = pd.DataFrame.from_dict(freq_table2, orient='index')\nfreq.reset_index(level=0, inplace=True)\nfreq.columns = ['word', 'frequency']\n\n\n# Sort & Remove short ones\nfreq = freq.sort_values('frequency', ascending=0).reset_index()\nfreq = freq.drop('index', axis='columns')\nfreq = freq[0:30] # we care about top 30\nfreq.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}