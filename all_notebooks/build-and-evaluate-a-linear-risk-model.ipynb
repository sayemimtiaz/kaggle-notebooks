{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Outline\n\n- [1. Import Packages](#1)\n- [2. Load Data](#2)\n- [3. Explore the Dataset](#3)\n- [4. Mean-Normalize the Data](#4)\n- [5. Build the Model](#Ex-2)\n- [6. Evaluate the Model Using the C-Index](#6)\n- [7. Evaluate the Model on the Test Set](#7)\n- [8. Improve the Model](#8)\n- [9. Evalute the Improved Model](#9)"},{"metadata":{},"cell_type":"markdown","source":"## Overview of this Notebook\n\nIn this Notebook, we'll build a risk score model for retinopathy in diabetes patients using logistic regression.\n\nAs we develop the model, we will learn about the following topics:\n\n- Data preprocessing\n  - Log transformations\n  - Standardization\n- Basic Risk Models\n  - Logistic Regression\n  - C-index\n  - Interactions Terms\n  \n### Diabetic Retinopathy\nRetinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina.\nThis often leads to vision changes or blindness.\nDiabetic patients are known to be at high risk for retinopathy. \n    \n### Logistic Regression    \nLogistic regression is an appropriate analysis to use for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy.\nLogistic Regression is one of the most commonly used algorithms for binary classification. It is used to find the best fitting model to describe the relationship between a set of features (also referred to as input, independent, predictor, or explanatory variables) and a binary outcome label (also referred to as an output, dependent, or response variable). Logistic regression has the property that the output prediction is always in the range $[0,1]$. Sometimes this output is used to represent a probability from 0%-100%, but for straight binary classification, the output is converted to either $0$ or $1$ depending on whether it is below or above a certain threshold, usually $0.5$.\n\nIt may be  confusing that the term regression appears in the name even though logistic regression is actually a classification algorithm, but that's just a name it was given for historical reasons."},{"metadata":{},"cell_type":"markdown","source":"<a name='1'></a>\n## 1.  Import Packages\n\nWe'll first import all the packages that we need for this notebook. \n\n- `numpy` is the fundamental package for scientific computing in python.\n- `pandas` is what we'll use to manipulate our data.\n- `matplotlib` is a plotting library."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name='2'></a>\n## 2. Load Data\n\nFirst we will load in the dataset that we will use for training and testing our model.\n\n- Run the next cell to load the data that is stored in csv files.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# This function creates randomly generated data\n# X, y = load_data(6000)\n\n# For stability, load data from files that were generated using the load_data\nX = pd.read_csv('../input/ai-for-medical-prognosis-diabetes-datasets/X_data.csv',index_col=0)\ny_df = pd.read_csv('../input/ai-for-medical-prognosis-diabetes-datasets/y_data.csv',index_col=0)\ny = y_df['y']\ndf = pd.concat([X, y_df], axis=1)\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`X` and `y` are Pandas DataFrames that hold the data for 6,000 diabetic patients. "},{"metadata":{},"cell_type":"markdown","source":"<a name='3'></a>\n##  3. Explore the Dataset\n\nThe features (`X`) include the following fields:\n* Age: (years)\n* Systolic_BP: Systolic blood pressure (mmHg)\n* Diastolic_BP: Diastolic blood pressure (mmHg)\n* Cholesterol: (mg/DL)\n    \nWe can use the `head()` method to display the first few records of each.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target (`y`) is an indicator of whether or not the patient developed retinopathy.\n\n* y = 1 : patient has retinopathy.\n* y = 0 : patient does not have retinopathy."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we build a model, let's take a closer look at the distribution of our training data. To do this, we will split the data into train and test sets using a 75/25 split.\n\nFor this, we can use the built in function provided by sklearn library.  See the documentation for [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the histograms of each column of `X_train` below: "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X.columns:\n    X_train_raw.loc[:,col].hist()\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew.\n\nMany statistical models assume that the data is normally distributed, forming a symmetric Gaussian bell shape (with no skew) more like the example below."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\ndata = np.random.normal(50,12, 5000)\nfitting_params = norm.fit(data)\nnorm_dist_fitted = norm(*fitting_params)\nt = np.linspace(0,100, 100)\nplt.hist(data, bins=60, density=True)\nplt.plot(t, norm_dist_fitted.pdf(t))\nplt.title('Example of Normally Distributed Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data.\n\nLet's plot the log of the feature variables to see that it produces the desired effect."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X_train_raw.columns:\n    np.log(X_train_raw.loc[:, col]).hist()\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data is more symmetric after taking the log."},{"metadata":{},"cell_type":"markdown","source":"<a name='4'></a>\n## 4. Mean-Normalize the Data\n\nLet's now transform our data so that the distributions are closer to standard normal distributions.\n\nFirst we will remove some of the skew from the distribution by using the log transformation.\nThen we will \"standardize\" the distribution so that it has a mean of zero and standard deviation of 1. Recall that a standard normal distribution has mean of zero and standard deviation of 1. "},{"metadata":{},"cell_type":"markdown","source":"* We will write a function that first removes some of the skew in the data, and then standardizes the distribution so that for each data point $x$,\n$$\\overline{x} = \\frac{x - mean(x)}{std(x)}$$\n* Keep in mind that we want to pretend that the test data is \"unseen\" data. \n    * This implies that it is unavailable to us for the purpose of preparing our data, and so we do not want to consider it when evaluating the mean and standard deviation that we use in the above equation. Instead we want to calculate these values using the training data alone, but then use them for standardizing both the training and the test data.\n    * For a further discussion on the topic, see this article [\"Why do we need to re-use training parameters to transform test data\"](https://sebastianraschka.com/faq/docs/scale-training-test.html). "},{"metadata":{},"cell_type":"markdown","source":"#### Note\n- For the sample standard deviation, we will calculate the unbiased estimator:\n$$s = \\sqrt{\\frac{\\sum_{i=1}^n(x_{i} - \\bar{x})^2}{n-1}}$$\n- In other words, if you numpy, set the degrees of freedom `ddof` to 1.\n- For pandas, the default `ddof` is already set to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef make_standard_normal(df_train, df_test):\n    \"\"\"\n    In order to make the data closer to a normal distribution, take log\n    transforms to reduce the skew.\n    Then standardize the distribution with a mean of zero and standard deviation of 1. \n  \n    Args:\n      df_train (dataframe): unnormalized training data.\n      df_test (dataframe): unnormalized test data.\n  \n    Returns:\n      df_train_normalized (dateframe): normalized training data.\n      df_test_normalized (dataframe): normalized test data.\n    \"\"\"\n    \n     \n    # Remove skew by applying the log function to the train set, and to the test set\n    df_train_unskewed = np.log(df_train)\n    df_test_unskewed = np.log(df_test)\n    \n    #calculate the mean and standard deviation of the training set\n    mean = df_train_unskewed.mean(axis = 0)\n    stdev = df_train_unskewed.std(axis = 0)\n    \n    # standardize the training set\n    df_train_standardized = (df_train_unskewed - mean)/stdev\n    \n    # standardize the test set (see instructions and hints above)\n    #print(f'the mean for test: {np.mean(df_test_unskewed, axis=0)}')\n    mean_ = df_test_unskewed.mean()\n    stdev_ = df_test_unskewed.std()\n    df_test_standardized = (df_test_unskewed - mean)/stdev\n    \n    \n    return df_train_standardized, df_test_standardized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\ntmp_train = pd.DataFrame({'field1': [1,2,10], 'field2': [4,5,11]})\ntmp_test = pd.DataFrame({'field1': [1,3,10], 'field2': [4,6,11]})\ntmp_train_transformed, tmp_test_transformed = make_standard_normal(tmp_train,tmp_test)\n\nprint(f\"Training set transformed field1 has mean {tmp_train_transformed['field1'].mean(axis=0):.4f} and standard deviation {tmp_train_transformed['field1'].std(axis=0):.4f} \")\nprint(f\"Test set transformed, field1 has mean {tmp_test_transformed['field1'].mean(axis=0):.4f} and standard deviation {tmp_test_transformed['field1'].std(axis=0):.4f}\")\nprint(f\"Skew of training set field1 before transformation: {tmp_train['field1'].skew(axis=0):.4f}\")\nprint(f\"Skew of training set field1 after transformation: {tmp_train_transformed['field1'].skew(axis=0):.4f}\")\nprint(f\"Skew of test set field1 before transformation: {tmp_test['field1'].skew(axis=0):.4f}\")\nprint(f\"Skew of test set field1 after transformation: {tmp_test_transformed['field1'].skew(axis=0):.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform training and test data \nWe will use the function that we just implemented to make the data distribution closer to a standard normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = make_standard_normal(X_train_raw, X_test_raw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After transforming the training and test sets, we'll expect the training set to be centered at zero with a standard deviation of $1$.\n\nWe will avoid observing the test set during model training in order to avoid biasing the model training process, but let's have a look at the distributions of the transformed training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X_train.columns:\n    X_train[col].hist()\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name='5'></a>\n## 5. Build the Model\n\nNow we are ready to build the risk model by training logistic regression with our data."},{"metadata":{},"cell_type":"markdown","source":"We will implement the lr_model function to build a model using logistic regression with the LogisticRegression class from sklearn.\nSee the documentation for  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef lr_model(X_train, y_train):\n    \n    \n    # import the LogisticRegression class\n    from sklearn.linear_model import LogisticRegression\n    \n    # create the model object\n    model = LogisticRegression()\n    \n    # fit the model to the training data\n    model.fit(X_train,y_train)\n    \n    \n    #return the fitted model\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test Our Work\n\nNote: the `predict` method returns the model prediction *after* converting it from a value in the $[0,1]$ range to a $0$ or $1$ depending on whether it is below or above $0.5$."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test\ntmp_model = lr_model(X_train[0:3], y_train[0:3] )\nprint(tmp_model.predict(X_train[4:5]))\nprint(tmp_model.predict(X_train[5:6]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've tested our model, we can go ahead and build it. Note that the `lr_model` function also fits  the model to the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_X = lr_model(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name='6'></a>\n## 6. Evaluate the Model Using the C-index\n\nNow that we have a model, we need to evaluate it. We'll do this using the c-index. \n* The c-index measures the discriminatory power of a risk score. \n* Intuitively, a higher c-index indicates that the model's prediction is in agreement with the actual outcomes of a pair of patients.\n* The formula for the c-index is\n\n$$ \\mbox{cindex} = \\frac{\\mbox{concordant} + 0.5 \\times \\mbox{ties}}{\\mbox{permissible}} $$\n\n* A permissible pair is a pair of patients who have different outcomes.\n* A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome.\n* A tie is a permissible pair where the patients have the same risk score."},{"metadata":{},"cell_type":"markdown","source":"\n\n* We will implement the `cindex` function to compute c-index.\n* `y_true` is the array of actual patient outcomes, 0 if the patient does not eventually get the disease, and 1 if the patient eventually gets the disease.\n* `scores` is the risk score of each patient.  These provide relative measures of risk, so they can be any real numbers. By convention, they are always non-negative.\n* Here is an example of input data and how to interpret it:\n```Python\ny_true = [0,1]\nscores = [0.45, 1.25]\n```\n    * There are two patients. Index 0 of each array is associated with patient 0.  Index 1 is associated with patient 1.\n    * Patient 0 does not have the disease in the future (`y_true` is 0), and based on past information, has a risk score of 0.45.\n    * Patient 1 has the disease at some point in the future (`y_true` is 1), and based on past information, has a risk score of 1.25."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cindex(y_true, scores):\n    '''\n\n    Input:\n    y_true (np.array): a 1-D array of true binary outcomes (values of zero or one)\n        0: patient does not get the disease\n        1: patient does get the disease\n    scores (np.array): a 1-D array of corresponding risk scores output by the model\n\n    Output:\n    c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs\n    '''\n    n = len(y_true)\n    assert len(scores) == n\n\n    concordant = 0\n    permissible = 0\n    ties = 0\n    \n    \n    # use two nested for loops to go through all unique pairs of patients\n    for i in range(n):\n        for j in range(i+1, n): #choose the range of j so that j>i\n            \n            # Check if the pair is permissible (the patient outcomes are different)\n            if y_true[i]!= y_true[j]:\n                # Count the pair if it's permissible\n                permissible += 1\n\n                # For permissible pairs, check if they are concordant or are ties\n\n                # check for ties in the score\n                if scores[i]== scores[j]:\n                    # count the tie\n                    ties +=1\n                    # if it's a tie, we don't need to check patient outcomes, continue to the top of the for loop.\n                    continue\n\n                # case 1: patient i doesn't get the disease, patient j does\n                if y_true[i] == 0 and y_true[j] == 1:\n                    # Check if patient i has a lower risk score than patient j\n                    if scores[i] < scores[j]:\n                        # count the concordant pair\n                        concordant +=1\n                    # Otherwise if patient i has a higher risk score, it's not a concordant pair.\n                    # Already checked for ties earlier\n\n                # case 2: patient i gets the disease, patient j does not\n                if y_true[i] ==1 and y_true[j] == 0:\n                    # Check if patient i has a higher risk score than patient j\n                    if scores[i] > scores[j]: \n                        #count the concordant pair\n                        concordant +=1\n                    # Otherwise if patient i has a lower risk score, it's not a concordant pair.\n                    # We already checked for ties earlier\n\n    # calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs.\n    c_index = (concordant + 0.5* ties)/permissible\n    \n    \n    return c_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\ny_true = np.array([1.0, 0.0, 0.0, 1.0])\n\n# Case 1\nscores = np.array([0, 1, 1, 0])\nprint('Case 1 Output: {}'.format(cindex(y_true, scores)))\n\n# Case 2\nscores = np.array([1, 0, 0, 1])\nprint('Case 2 Output: {}'.format(cindex(y_true, scores)))\n\n# Case 3\nscores = np.array([0.5, 0.5, 0.0, 1.0])\nprint('Case 3 Output: {}'.format(cindex(y_true, scores)))\ncindex(y_true, scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\nWe will check our implementation of the for loops. \n- There is way to make a mistake on the for loops that cannot be caught with unit tests.\n- Bonus: Can you think of what this error could be, and why it can't be caught by unit tests?"},{"metadata":{},"cell_type":"markdown","source":"## 7. Evaluate the Model on the Test Set\n\nNow, we can evaluate our trained model on the test set.  \n\nTo get the predicted probabilities, we use the `predict_proba` method. This method will return the result from the model *before* it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). "},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model_X.predict_proba(X_test)[:, 1]\nc_index_X_test = cindex(y_test.values, scores)\nprint(f\"c-index on test set is {c_index_X_test:.4f}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the coefficients to see which variables (patient features) are having the most effect. We can access the model coefficients by using `model.coef_`"},{"metadata":{"trusted":true},"cell_type":"code","source":"coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns)\ncoeffs.T.plot.bar(legend=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question: \n> __Which three variables have the largest impact on the model's predictions?__"},{"metadata":{},"cell_type":"markdown","source":"<a name='8'></a>\n## 8. Improve the Model\n\nWe can try to improve our model by including interaction terms. \n* An interaction term is the product of two variables. \n    * For example, if we have data \n    $$ x = [x_1, x_2]$$\n    * We could add the product so that:\n    $$ \\hat{x} = [x_1, x_2, x_1*x_2]$$\n    "},{"metadata":{},"cell_type":"markdown","source":"We will write code below to add all interactions between every pair of variables to the training and test datasets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef add_interactions(X):\n    \"\"\"\n    Add interaction terms between columns to dataframe.\n\n    Args:\n    X (dataframe): Original data\n\n    Returns:\n    X_int (dataframe): Original data with interaction terms appended. \n    \"\"\"\n    features = X.columns\n    m = len(features)\n    X_int = X.copy(deep=True)\n\n    \n    # 'i' loops through all features in the original dataframe X\n    for i in range(m):\n        \n        # get the name of feature 'i'\n        feature_i_name = features[i]\n        \n        # get the data for feature 'i'\n        feature_i_data = X_int[feature_i_name]\n        \n        # choose the index of column 'j' to be greater than column i\n        for j in range(i+1, m):\n            \n            # get the name of feature 'j'\n            feature_j_name = features[j]\n            \n            # get the data for feature j'\n            feature_j_data = X_int[feature_j_name]\n            \n            # create the name of the interaction feature by combining both names\n            # example: \"apple\" and \"orange\" are combined to be \"apple_x_orange\"\n            feature_i_j_name = f\"{feature_i_name}_x_{feature_j_name}\"\n            \n            # Multiply the data for feature 'i' and feature 'j'\n            # store the result as a column in dataframe X_int\n            X_int[feature_i_j_name] = feature_i_data * feature_j_data\n        \n    \n\n    return X_int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test\nprint(\"Original Data\")\nprint(X_train.loc[:, ['Age', 'Systolic_BP']].head())\nprint(\"Data w/ Interactions\")\nprint(add_interactions(X_train.loc[:, ['Age', 'Systolic_BP']].head()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! we have correctly implemented `add_interactions`, we will use it to make transformed version of `X_train` and `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_int = add_interactions(X_train)\nX_test_int = add_interactions(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Evaluate the Improved Model\n\nNow we can train the new and improved version of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_X_int = lr_model(X_train_int, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate our new model on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_X = model_X.predict_proba(X_test)[:, 1]\nc_index_X_int_test = cindex(y_test.values, scores_X)\n\nscores_X_int = model_X_int.predict_proba(X_test_int)[:, 1]\nc_index_X_int_test = cindex(y_test.values, scores_X_int)\n\nprint(f\"c-index on test set without interactions is {c_index_X_test:.4f}\")\nprint(f\"c-index on test set with interactions is {c_index_X_int_test:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should see that the model with interaction terms performs a bit better than the model without interactions.\n\nNow let's take another look at the model coefficients to try and see which variables made a difference. Plot the coefficients and report which features seem to be the most important."},{"metadata":{"trusted":true},"cell_type":"code","source":"int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns)\nint_coeffs.T.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Questions:\n> __Which variables are most important to the model?__<br>\n> __Have the relevant variables changed?__<br>\n> __What does it mean when the coefficients are positive or negative?__<br>\n\nYou may notice that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease.\n\nTo understand the effect of interaction terms, let's compare the output of the model we've trained on sample cases with and without the interaction. Run the cell below to choose an index and look at the features corresponding to that case in the training set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"index = index = 3432\ncase = X_train_int.iloc[index, :]\nprint(case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_case = case.copy(deep=True)\nnew_case.loc[\"Age_x_Cholesterol\"] = 0\nnew_case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Output with interaction: \\t{model_X_int.predict_proba([case.values])[:, 1][0]:.4f}\")\nprint(f\"Output without interaction: \\t{model_X_int.predict_proba([new_case.values])[:, 1][0]:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SUMMARY\nWe see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}