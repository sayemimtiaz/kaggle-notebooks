{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\ndf_angry = pd.read_csv('/kaggle/input/emotion/Emotion(angry).csv')\ndf_happy = pd.read_csv('/kaggle/input/emotion/Emotion(happy).csv')\ndf_sad = pd.read_csv('/kaggle/input/emotion/Emotion(sad).csv')\ndf_main = pd.concat([df_angry, df_happy, df_sad])\ndf_main = df_main.reset_index(drop=True)\n\n# Remove duplicates\ndf_main = df_main.drop_duplicates(subset=['content', 'sentiment'])\ndf_main = df_main.reset_index(drop=True)\n\n# Remove empty data\ndf_main['content'].replace('', np.nan, inplace=True)\ndf_main = df_main.dropna(subset = ['content'])\ndf_main = df_main.reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(df_main.head())\nprint('\\n')\nprint(df_main.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examples of angry content:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"angry_text = df_main[df_main[\"sentiment\"] == 'angry'][\"content\"].values\nprint(angry_text[0])\nprint(\"\\n\")\nprint(angry_text[1])\nprint(\"\\n\")\nprint(angry_text[2])\nprint(\"\\n\")\nprint(angry_text[3])\nprint(\"\\n\")\nprint(angry_text[4])\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examples of happy content:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"happy_text = df_main[df_main[\"sentiment\"] == 'happy'][\"content\"].values\nprint(happy_text[0])\nprint(\"\\n\")\nprint(happy_text[1])\nprint(\"\\n\")\nprint(happy_text[2])\nprint(\"\\n\")\nprint(happy_text[3])\nprint(\"\\n\")\nprint(happy_text[4])\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examples of sad content:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sad_text = df_main[df_main[\"sentiment\"] == 'sad'][\"content\"].values\nprint(sad_text[0])\nprint(\"\\n\")\nprint(sad_text[1])\nprint(\"\\n\")\nprint(sad_text[2])\nprint(\"\\n\")\nprint(sad_text[3])\nprint(\"\\n\")\nprint(sad_text[4])\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tot = df_main.shape[0]\nvc = df_main['sentiment'].value_counts()\n\nnum_angry = vc['angry']\nnum_happy = vc['happy']\nnum_sad = vc['sad']\n\nslices = [num_angry, num_happy, num_sad]\nlabeling = ['Angry','Happy', 'Sad']\nexplode = [0.1, 0.1, 0.1]\nplt.pie(slices,explode=explode,shadow=True,autopct='%1.1f%%',labels=labeling,wedgeprops={'edgecolor':'black'})\nplt.title('Sentiment of Content')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To clean the data and make word clouds, I took some inspiration from the following kernel:\n\nhttps://www.kaggle.com/moezabid/disaster-tweets-nlp\n\nI highly reccommend you checking it out - they did some great work! ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import re\nimport string\nfrom textblob import TextBlob\nfrom tqdm.notebook import tqdm\n\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}\n\n\ndef clean(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower()\n    text = re.sub('\\[.*?\\]','',text)\n    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n    text = re.sub('<,*?>+\"','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n    text = re.sub('\\n','',text)\n    text = re.sub('\\w*\\d\\w*','',text)\n    text = re.sub(\"xa0'\", '', text)\n    text = re.sub(u\"\\U00002019\", \"'\", text) # IMPORTANT: Their apostrophe character was not the usual one...\n    words = text.split()\n    for i in range(len(words)):\n        if words[i].lower() in contractions.keys():\n            words[i] = contractions[words[i].lower()]\n    text = \" \".join(words)\n    #text = TextBlob(text).correct()\n    return text\n\ndf_main['content'] = df_main['content'].apply(lambda x: clean(x))\n\n# Remove empty data\ndf_main['content'].replace('', np.nan, inplace=True)\ndf_main = df_main.dropna(subset = ['content'])\ndf_main = df_main.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS \n\nplt.style.use('fivethirtyeight')\nstopwords = set(STOPWORDS) \nstop_word= list(stopwords) + ['http','co','https','wa','amp','รป','ร','HTTP','HTTPS']\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='angry']['content']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Angry Content',fontsize=40)\n\nwordcloud2 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='happy']['content']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Happy Content',fontsize=40)\n\nwordcloud3 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='sad']['content']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Sad Content',fontsize=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word clouds are about what I would expect them to be, but still interesting to visualize.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\ndf_main['word_count'] = df_main['content'].apply(lambda x: len(x.split()))\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n\ndf_angry = df_main[df_main['sentiment'] == 'angry']\nword = df_angry['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red', kde=False)\nax1.set_title('Angry')\n\ndf_happy = df_main[df_main['sentiment'] == 'happy']\nword = df_happy['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green', kde=False)\nax2.set_title('Happy')\n\ndf_sad = df_main[df_main['sentiment'] == 'sad']\nword = df_sad['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='blue', kde=False)\nax3.set_title('Sad')\n\nfig.suptitle('Average word length by sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the super long statuses are all Angry (makes sense; I'd imagine an angry rant could drone on and on)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Count Vectorizer + Tf-Idf","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's lemmatize and stem the text, and also just remove stop words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\n\ndef remove_stopwords(text):\n    text = text.split()\n    words = [w for w in text if w not in stopwords.words('english')]\n    return \" \".join(words)\n\ndf_main['content_no_sw'] = df_main['content'].apply(lambda x : remove_stopwords(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize,word_tokenize\n\n\nlemmatizer = WordNetLemmatizer()\nstatuses = df_main['content'].values.copy()\n\nfor i in range(len(statuses)):\n    a = statuses[i]\n    sentences = sent_tokenize(statuses[i])\n    word_list = []\n    for sent in sentences:\n        words = word_tokenize(sent)\n        for word in words:\n            if words not in word_list:\n                word_list.append(word)\n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    statuses[i] = ' '.join(w for w in word_list)\n    \nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfor i in range(len(statuses)):\n    sentences = sent_tokenize(statuses[i])\n    word_list = []\n    for sent in sentences: \n        words = word_tokenize(sent)\n        for word in words: \n            if words not in word_list:\n                word_list.append(word)\n    word_list = [porter.stem(w) for w in word_list if w not in stop_words]\n    statuses[i] = ' '.join(w for w in word_list)\n\n    \ndf_main['content_lemm_stem_no_sw'] = statuses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, plot_confusion_matrix\n\ndef show_cm(classifier, X_test, y_test):\n    plt.style.use('default')\n    class_names = clf.classes_\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\n    for title, normalize in titles_options:\n        disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n        plt.title(title)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import f1_score\n\ncount_vectorizer0 = CountVectorizer(ngram_range = (1,2))\ncount_vectorizer1 = CountVectorizer(ngram_range = (1,2))\ncount_vectorizer2 = CountVectorizer(ngram_range = (1,2))\n\nvecs0 = count_vectorizer0.fit_transform(df_main['content'])\nvecs1 = count_vectorizer1.fit_transform(df_main['content_no_sw'])\nvecs2 = count_vectorizer2.fit_transform(df_main['content_lemm_stem_no_sw'])\n\nclf0 = linear_model.RidgeClassifier().fit(vecs0, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No changes\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf0.score(vecs0, df_main[\"sentiment\"]))\nshow_cm(clf0, vecs0, df_main['sentiment'])\n\nclf1 = linear_model.RidgeClassifier().fit(vecs1, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No stop words\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf1.score(vecs1, df_main[\"sentiment\"]))\nshow_cm(clf1, vecs1, df_main['sentiment'])\n\nclf2 = linear_model.RidgeClassifier().fit(vecs2, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No stop words, lemmatized and stemmed\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf2.score(vecs2, df_main[\"sentiment\"]))\nshow_cm(clf2, vecs2, df_main['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = clf1.predict(vecs1)\nerror_a_h = 0\nerror_a_s = 0\nerror_h_a = 0\nerror_h_s = 0\nerror_s_a = 0\nerror_s_h = 0\nfor i in range(len(predict)):\n    prediction = predict[i]\n    actual = df_main.loc[i, 'sentiment']\n    if actual == 'angry' and prediction == 'happy' and error_a_h == 0:\n        print(\"Angry status mislabeled as Happy:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_a_h += 1\n    elif actual == 'angry' and prediction == 'sad' and error_a_s == 0:\n        print(\"Angry status mislabeled as Sad:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_a_s += 1\n    elif actual == 'happy' and prediction == 'angry' and error_h_a == 0:\n        print(\"Happy status mislabeled as Angry:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_h_a += 1\n    elif actual == 'happy' and prediction == 'sad' and error_h_s == 0:\n        print(\"Happy status mislabeled as Sad:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_h_s += 1\n    elif actual == 'sad' and prediction == 'angry' and error_s_a == 0:\n        print(\"Sad status mislabeled as Angry:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_s_a += 1\n    elif actual == 'sad' and prediction == 'happy' and error_s_h == 0:\n        print(\"Sad status mislabeled as Happy:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_s_h += 1\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}