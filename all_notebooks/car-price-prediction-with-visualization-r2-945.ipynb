{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"- Predict used car price by various regression models\n\n- Regression Models:\n  - Linear Regression\n  - Multivariate Adaptive Regression Splines\n  - Decision Tree Regressor\n  - XGBoost Regressor\n  - Deep Neural Network\n\n- Performace Metrics:\n  - R-Squared\n  - Mean Absolute Error"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Libraries and Load Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom pyearth import Earth\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load Data\n\ndf = pd.read_csv(\"/kaggle/input/cars-dataset-audi-bmw-ford-hyundai-skoda-vw/cars_dataset.csv\")\nprint(\"Shape of Dataset:\", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration and Preprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Data Information\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Descriptive statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Remove Outliers in Target Variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the distribution of car price \nsns.distplot(df['price'],color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is right-skewed. Let's remove outliers using the IQR as the criteria."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find IQR\nQ1 = df['price'].quantile(0.25)\nQ3 = df['price'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove outliers with a criteria: 1.5 x IOR\ndf = df[~((df['price'] < (Q1 - 1.5 * IQR)) |(df['price'] > (Q3 + 1.5 * IQR)))]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the distribution of price: outliers removed\nsns.distplot(df['price'], color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Exploration of Categorical Variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the list of car models\nprint(df['model'].unique().tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the frequency of each car model\nfor index, value in df['model'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since great many car models are contained, let's ignore car models in prediction."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the value counts of transmission\nfor index, value in df['transmission'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the value counts of fuelType\nfor index, value in df['fuelType'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show the value counts of Make\nfor index, value in df['Make'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Representing categorical data using violin plots\nfig = plt.figure(figsize=(16,4))\nplt.subplot(1,3,1)\nsns.violinplot(x = 'transmission', y = 'price', data = df, palette=\"winter\")\nplt.subplot(1,3,2)\nsns.violinplot(x = 'fuelType', y = 'price', data = df, palette=\"winter\")\nplt.subplot(1,3,3)\nsns.violinplot(x = 'Make', y = 'price', data = df, palette=\"winter\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Exploration of Continuous Variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a list of continuous variables\ncont = [\"year\", \"price\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"]\n\n# Create a dataframe of continuous variables\ndf_cont = df[cont]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualize correlation between continuous variables\n\n# Compute the correlation matrix\ncorr = df_cont.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 6\nfig = plt.figure(figsize=(12,12))\n# Correlations between each variable\ncorrmat = df_cont.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(6, \"price\")[\"price\"].index\n# Calculate correlation\nfor i in np.arange(1,6):\n    regline = df_cont[cols[i]]\n    ax = fig.add_subplot(3,2,i)\n    sns.regplot(x=regline, y=df['price'], scatter_kws={\"color\": \"royalblue\", \"s\": 3},\n                line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Data Preparation for Modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split X and y\nX = df.drop(['model', 'price'], axis=1)\ny = df['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create dummies for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\n# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\n# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Scale the features\n\n# Store column names since the column names will be lost after scaling\ncols = X.columns\n\n# Scale the features and convert it back to a dataframe\nX = pd.DataFrame(scale(X))\n\n# Write in the column names again\nX.columns = cols\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Regression"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model based on the assumption of linear regression:\n\n# Assumption 1. The error terms are normally distributed with mean approximately 0.\n\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50, color=\"blue\")\nfig.suptitle('Error Terms', fontsize=14)                  \nplt.xlabel('y_test-y_pred', fontsize=12)                  \nplt.ylabel('Index', fontsize=12)                          \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first assumption seems to be met."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assumption 2: Homoscedasticity, i.e. the variance of the error term (y_true-y_pred) is constant.\n\nc = [i for i in range(len(y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\", alpha=0.4)\nfig.suptitle('Error Terms', fontsize=14)               \nplt.xlabel('Index', fontsize=12)                      \nplt.ylabel('ytest-ypred', fontsize=12)                \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second assumption seems to be met."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assumption 3: There is little correlation between the predictors. i.e., Multicollinearity:\n\npredictors = ['year', 'mileage', 'tax', 'mpg', 'engineSize','transmission_Manual','transmission_Other', \n              'transmission_Semi-Auto', 'fuelType_Electric','fuelType_Hybrid', 'fuelType_Other', 'fuelType_Petrol', \n              'Make_Ford','Make_Hyundai', 'Make_audi', 'Make_skoda', 'Make_toyota', 'Make_vw']\n\n# Compute the correlation matrix\ncors = X.loc[:, list(predictors)].corr()\n\n# Generate a mask for the upper triangle\nmask_2 = np.triu(np.ones_like(cors, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cors, mask=mask_2, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are highly correlated. So let's check the multicolliearity by VIF."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All VIF values are lower than 5. So, there is no possibility of multicollinearity."},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Multivariate Adaptive Regression Splines (MARS)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate the model\nmars_model = Earth()\n\n# By default, we do not need to set any of the algorithm hyperparameters.\n# The algorithm automatically discovers the number and type of basis functions to use.\n\n# Fit the model\nmars_model.fit(X_train, y_train)\n\n# Making predictions\nmars_y_pred = mars_model.predict(X_test)\n\n# Performance Metrics\nmars_r2 = r2_score(y_test, mars_y_pred)\nmars_mae = mean_absolute_error(y_test, mars_y_pred)\n\n# Show the model performance\nprint(\"MARS R2: \", mars_r2)\nprint(\"MARS MAE: \", mars_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Decision Tree Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate the model\ndt_model = DecisionTreeRegressor()\n\n# Grid search\ndt_gs = GridSearchCV(dt_model,\n                     param_grid = {'max_depth': range(1, 11),\n                                   'min_samples_split': range(10, 60, 10)},\n                     cv=5,\n                     n_jobs=1,\n                     scoring='neg_mean_squared_error')\n\ndt_gs.fit(X_train, y_train)\n\nprint(dt_gs.best_params_)\nprint(-dt_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate the best model\ndt_model_best = DecisionTreeRegressor(max_depth=10, min_samples_split=20)\n\n# Fit the best model\ndt_model_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make predictions\ndt_y_pred = dt_model_best.predict(X_test)\n\n# Performance metrics\ndt_r2 = r2_score(y_test, dt_y_pred)\ndt_mae = mean_absolute_error(y_test, dt_y_pred)\n\n# Show the model performance\nprint(\"DT R2: \", dt_r2)\nprint(\"DT MAE: \", dt_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.4. XGB Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate the model\nxgb_model = xgb.XGBRegressor()\n\n# Grid search\n#xgb_gs = GridSearchCV(xgb_model,\n#                      param_grid = {'max_depth': range(8, 15),\n#                                   'min_samples_split': range(1, 11, 3)},\n#                      cv=5,\n#                      n_jobs=1,\n#                      scoring='neg_mean_squared_error')\n                      \n#xgb_gs.fit(X_train, y_train)\n\n#print(xgb_gs.best_params_)\n#print(-xgb_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate the best model\nxgb_model_best = xgb.XGBRegressor(max_depth=10, min_samples_split=10)\n\n# Fit the best model\nxgb_bst = xgb_model_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make predictions\nxgb_y_pred = xgb_bst.predict(X_test)\n\n# Performance metrics\nxgb_r2 = r2_score(y_test, xgb_y_pred)\nxgb_mae = mean_absolute_error(y_test, xgb_y_pred)\n\n# Show the model performance\nprint(\"XGB R2: \", xgb_r2)\nprint(\"XGB MAE: \", xgb_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5. Deep Neural Network"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Define a DNN\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initiate DNN\ndnn = KerasRegressor(build_fn=create_model, epochs=300, batch_size=20, verbose=1)\n\n# Fit DNN\ndnn_history = dnn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualize the DNN learning\nloss_train = dnn_history.history['loss']\nepochs = range(1,301)\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_train, 'royalblue', label='Training loss', linewidth=3)\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make predictions\ndnn_y_pred = dnn.predict(X_test)\n\n# Performance metrics\ndnn_r2 = r2_score(y_test, dnn_y_pred)\ndnn_mae = mean_absolute_error(y_test, dnn_y_pred)\n\n# Show the model performance\nprint(\"DNN R2: \", dnn_r2)\nprint(\"DNN MAE: \", dnn_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results_table = pd.DataFrame([[np.mean(lr_r2), np.mean(lr_mae)],\n                             [np.mean(mars_r2), np.mean(mars_mae)],\n                             [np.mean(dt_r2), np.mean(dt_mae)],\n                             [np.mean(xgb_r2), np.mean(xgb_mae)],\n                             [np.mean(dnn_r2), np.mean(dnn_mae)]],\n                            columns=['R2', 'MAE'],\n                            index=[\"Linear Regression\",\"MARS\",\"Decision Tree\",\"XGBoost\",\"DNN\"])\npd.options.display.precision = 3\nresults_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_table = pd.DataFrame({\"Linear Regression: Predicted Price\": y_pred,\n                           \"MARS: Predicted Price\": mars_y_pred,\n                           \"Decision Tree: Predicted Price\": dt_y_pred,\n                           \"XGBoost: Predicted Price\": xgb_y_pred,\n                           \"DNN: Predicted Price\": dnn_y_pred,\n                          \"Actual Price\": y_test})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualize the predicted price and actual price\nfig = plt.figure(figsize=(10,10))\nplt.subplot(3,2,1)\nsns.regplot(x = 'Linear Regression: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,2)\nsns.regplot(x = 'MARS: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,3)\nsns.regplot(x = 'Decision Tree: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,4)\nsns.regplot(x = 'XGBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,5)\nsns.regplot(x = 'DNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_bst.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(6,6))\nsorted_idx = xgb_bst.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb_bst.feature_importances_[sorted_idx], color=\"royalblue\", alpha=0.9)\nplt.xlabel(\"Xgboost Feature Importance\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}