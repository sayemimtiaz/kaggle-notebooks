{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sales of Summer Cloth in E-Commerce Wish\n\nThe task was to predict the number of units sold of the products. The problem has been solved through a regression model. \n\nThe impetus has been given to data preprocessing as I have been reading up on it quite a lot and I wanted to implement and see it for myself. \n\nFeature Selection has also been done to get the best features that would help us in our predictions.\nGridSeach has been performed on the best model to get more optimised parameters of our model. \nAn attempt at model boosting has been done to get even better predictions. You can follow through the steps or skip to the part you want to see through below:\n\n* [Importing Data](#import)\n* [Data Preprocessing](#preprocess)\n* [Correlation between features](#correlation)\n* [Feature Selection](#selection)\n* [Regression Models](#regmodel)\n* [GridSearchCV](#gridsearch)\n* [Final Model](#finalmodel) (by model boosting method)"},{"metadata":{},"cell_type":"markdown","source":"<a id='import'></a>\n\n# Importing Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Basic \nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/summer-products-with-rating-and-performance_2020-08.csv')\nsales.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='preprocess'></a>\n\n# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Data Preprocessing is a key step in the path to making models that can predict/classify depending on the dataset we have and the question we aim to answer. At some level, this requires you to be aware of the background of your data and the question you intend to answer. There is a lot of underlying inferences that can be extracted while answering the actual question. This is possible due to the data preprocessing phase. \n\nI have looked up quite a few online articles about what different methods are involved and when should we apply what. It has been an interesting read and I have attempted to apply what I learnt here.\n\nBelow are some steps that are used at times in the Data Preprocessing stage. The list is not exhaustive, these are some of the methods that were used in this dataset\n\n1. [Removing the null values](#1)\n2. [Transform categorical variables](#2)  (as necessary)\n3. [Removing the features that have 1 unique value](#3)\n4. [Engineer new feature](#4) (if there is a possibility)\n5. [Remove unnecessary features](#5)\n6. Binning data (if required) (was not required here)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n\n## Removing the null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.isnull().sum()[sales.isnull().sum() !=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the 5 rating count features with null values can be replaced with 0. We are doing so because if the value could be null (for that rating) because no customer wanted to rate it so.\n\n\n* Product color, Origin Country and Product Size variation will be dealt with in the next section. \n\n\n* 'has_urgency_banner' feature tells us whether or not the product has an urgency banner. We can see that the count of null values for 'has_urgency_banner' and 'urgency_text' is the same. This implies that the products that do not have an urgency text have been given a null value in 'has_urgency_banner'. Therefore, we can say that this feature can become a categorical variable: \n    * 1 denoting it has an urgency text\n    * 0 denoting it does not have an urgency text (Replace null with 0)\n    \n  \n* Merchant name, info subtitle, profile picture will be dealt in a separate section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# rating features\nsales['rating_five_count'].replace(np.nan, 0, inplace=True)\nsales['rating_four_count'].replace(np.nan, 0, inplace=True)\nsales['rating_three_count'].replace(np.nan, 0, inplace=True)\nsales['rating_two_count'].replace(np.nan, 0, inplace=True)\nsales['rating_one_count'].replace(np.nan, 0, inplace=True)\n\n# urgency banner\nsales['has_urgency_banner'].replace(np.nan, 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n\n## Transform categorical variables"},{"metadata":{},"cell_type":"markdown","source":"* In this section, I will curate and reduce the different values that are present under some of the features so that there is some uniformity in the dataset and the sparsity is reduced. You can go ahead and see further.\n\n* Transforming categorical variables has a major step of changing it into a ***one hot encoding format***. I have done that a little ahead in this notebook. If you want to go through that, you can click [here](#7)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.dtypes[sales.dtypes == 'object']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'title', 'title_orig', 'merchant_title', 'merchant_name', 'merchant_info_subtitle', 'merchant_profile_picture', 'product_url', 'product_picture', 'product_id', 'theme', 'crawl_month', 'shipping_option_name', 'currency_buyer', 'urgency_text' will be dealt with in separate sections further on. \n\n\n* 'tags' will be used to engineer a new feature later on.\n\n\n* 'product_color', 'product_variation_size_id' and 'origin_country' will be used in this category to reduce the number of categories in each feature."},{"metadata":{},"cell_type":"markdown","source":"### 1. Product Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_color'].value_counts()\ncount[count>3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will club the colors into some basic colors and not keep so many different shades of colours.\n\nThe code in this section and results arequite long and will seem redundant because I have tries to segregate all the singular colours into a paarticular basic colour. I have done so as I wanted to be exhaustive about it. \n\nAnother way to go about it would be address only some colours that appear more times than the rest while clubbing the few occurrences of other colours under the 'others' category. (Read ahead if you did not quite understand what I tried to say here!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'].replace('armygreen', 'green', inplace=True)\nsales['product_color'].replace('winered', 'red', inplace=True)\nsales['product_color'].replace('navyblue', 'blue', inplace=True)\nsales['product_color'].replace('lightblue', 'blue', inplace=True)\nsales['product_color'].replace('khaki', 'green', inplace=True)\nsales['product_color'].replace('gray', 'grey', inplace=True)\nsales['product_color'].replace('rosered', 'red', inplace=True)\nsales['product_color'].replace('skyblue', 'blue', inplace=True)\nsales['product_color'].replace('coffee', 'brown', inplace=True)\nsales['product_color'].replace('darkblue', 'blue', inplace=True)\nsales['product_color'].replace('rose', 'red', inplace=True)\nsales['product_color'].replace('fluorescentgreen', 'green', inplace=True)\nsales['product_color'].replace('navy', 'blue', inplace=True)\nsales['product_color'].replace('lightpink', 'pink', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_color'].value_counts()\ncount[count==3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'].replace('orange-red', 'red', inplace=True)\nsales['product_color'].replace('Black', 'black', inplace=True)\nsales['product_color'].replace('lightgreen', 'green', inplace=True)\nsales['product_color'].replace('White', 'white', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_color'].value_counts()\ncount[count==2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'].replace('wine', 'red', inplace=True)\nsales['product_color'].replace('Pink', 'pink', inplace=True)\nsales['product_color'].replace('Army green', 'green', inplace=True)\nsales['product_color'].replace('coralred', 'red', inplace=True)\nsales['product_color'].replace('lightred', 'red', inplace=True)\nsales['product_color'].replace('apricot', 'orange', inplace=True)\nsales['product_color'].replace('navy blue', 'blue', inplace=True)\nsales['product_color'].replace('burgundy', 'red', inplace=True)\nsales['product_color'].replace('silver', 'grey', inplace=True)\nsales['product_color'].replace('camel', 'brown', inplace=True)\nsales['product_color'].replace('lakeblue', 'blue', inplace=True)\nsales['product_color'].replace('lightyellow', 'yellow', inplace=True)\nsales['product_color'].replace('watermelonred', 'red', inplace=True)\nsales['product_color'].replace('coolblack', 'black', inplace=True)\nsales['product_color'].replace('applegreen', 'green', inplace=True)\nsales['product_color'].replace('mintgreen', 'green', inplace=True)\nsales['product_color'].replace('dustypink', 'pink', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_color'].value_counts()\ncount[count==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'].replace('ivory', 'white', inplace=True)\nsales['product_color'].replace('lightkhaki', 'green', inplace=True)\nsales['product_color'].replace('lightgray', 'grey', inplace=True)\nsales['product_color'].replace('darkgreen', 'green', inplace=True)\nsales['product_color'].replace('RED', 'red', inplace=True)\nsales['product_color'].replace('tan', 'brown', inplace=True)\nsales['product_color'].replace('jasper', 'red', inplace=True)\nsales['product_color'].replace('nude', 'white', inplace=True)\nsales['product_color'].replace('army', 'brown', inplace=True)\nsales['product_color'].replace('light green', 'green', inplace=True)\nsales['product_color'].replace('offwhite', 'white', inplace=True)\nsales['product_color'].replace('Blue', 'blue', inplace=True)\nsales['product_color'].replace('denimblue', 'blue', inplace=True)\nsales['product_color'].replace('Rose red', 'red', inplace=True)\nsales['product_color'].replace('lightpurple', 'purple', inplace=True)\nsales['product_color'].replace('prussianblue', 'blue', inplace=True)\nsales['product_color'].replace('offblack', 'black', inplace=True)\nsales['product_color'].replace('violet', 'purple', inplace=True)\nsales['product_color'].replace('gold', 'yellow', inplace=True)\nsales['product_color'].replace('wine red', 'red', inplace=True)\nsales['product_color'].replace('rosegold', 'red', inplace=True)\nsales['product_color'].replace('claret', 'red', inplace=True)\nsales['product_color'].replace('army green', 'green', inplace=True)\nsales['product_color'].replace('lightgrey', 'grey', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_color'].value_counts()\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'].replace(np.nan, 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'black', 'white', 'blue', 'red', 'green', 'yellow', 'pink', 'grey', 'purple', 'orange', 'brown', 'beige' are the basic colours we will go forward with. \n\n* Replaced np.nan with 'others' already.\n\n* We will add a category 'dual' for products that have two colours (in the format __ & __).\n\n* We will add a category 'others' for products that are multi-coloured or have a print on them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def color(col):\n    ls = ['black', 'white', 'blue', 'red', 'green', 'yellow', 'pink', 'grey', 'purple', 'orange', 'brown', 'beige']\n    if col not in ls:\n        if '&' in col:\n            return 'dual'\n        else:\n            return 'others'\n    return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_color'] = sales['product_color'].apply(color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'product_color', data = sales, order = sales['product_color'].value_counts().iloc[:].index)\nplt.xlabel('Product Colour')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Product Variation Size ID"},{"metadata":{},"cell_type":"markdown","source":"* The categories which we will have are: XXXS, XXS, XS, S, M, L, XL, XXL, XXXL, XXXXL, XXXXXL, Others\n\n* The code will be lengthy here too but only for the purpose of this being an exhaustive data cleaning. However, there are some names that have only a single occurrence and as you will see later on that it is 84 in number and will become very tedious to categorize. Thus, we will club all these under 'Others'.\n\n* I am filtering out the categories other than 'Others' first and later will club everyhting under 'Others' using a function *'size_name'*\n\n* All the null values will also be under the category 'Others'."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_variation_size_id'].value_counts()\ncount[count>3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_variation_size_id'].replace('S.', 'S', inplace=True)\nsales['product_variation_size_id'].replace('Size S', 'S', inplace=True)\nsales['product_variation_size_id'].replace('XS.', 'XS', inplace=True)\nsales['product_variation_size_id'].replace('s', 'S', inplace=True)\nsales['product_variation_size_id'].replace('M.', 'M', inplace=True)\nsales['product_variation_size_id'].replace('2XL', 'XXL', inplace=True)\nsales['product_variation_size_id'].replace('Size XS', 'XS', inplace=True)\nsales['product_variation_size_id'].replace('Size-XS', 'XS', inplace=True)\nsales['product_variation_size_id'].replace('4XL', 'XXXXL', inplace=True)\nsales['product_variation_size_id'].replace('SIZE XS', 'XS', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_variation_size_id'].value_counts()\ncount[count==3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_variation_size_id'].replace('SizeL', 'L', inplace=True)\nsales['product_variation_size_id'].replace('Size-S', 'S', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_variation_size_id'].value_counts()\ncount[count==2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_variation_size_id'].replace('5XL', 'XXXXXL', inplace=True)\nsales['product_variation_size_id'].replace('3XL', 'XXXL', inplace=True)\nsales['product_variation_size_id'].replace('S(bust 88cm)', 'S', inplace=True)\nsales['product_variation_size_id'].replace('Size4XL', 'XXXXL', inplace=True)\nsales['product_variation_size_id'].replace('Size -XXS', 'XXS', inplace=True)\nsales['product_variation_size_id'].replace('SIZE-XXS', 'XXS', inplace=True)\nsales['product_variation_size_id'].replace('Size M', 'M', inplace=True)\nsales['product_variation_size_id'].replace('size S', 'S', inplace=True)\nsales['product_variation_size_id'].replace('S Pink', 'S', inplace=True)\nsales['product_variation_size_id'].replace('Size S.', 'S', inplace=True)\nsales['product_variation_size_id'].replace('Suit-S', 'S', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = sales['product_variation_size_id'].value_counts()\ncount.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def size_name(size):\n    ls = [\"XXXS\", \"XXS\", \"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", \"XXXL\", \"XXXXL\", \"XXXXXL\"]\n    if size in ls:\n        return size\n    return \"Others\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['product_variation_size_id'].replace(np.nan, 'Others', inplace=True)\nsales['product_variation_size_id'] = sales['product_variation_size_id'].apply(size_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'product_variation_size_id', data = sales, order = sales['product_variation_size_id'].value_counts().iloc[:].index)\nplt.xlabel('Product Variation Size ID')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Origin country"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['origin_country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will keep categories 'CN', 'US' and 'Others' (this will have VE, SG, AT and GB).\n\n* We are doing so because the occurrences of countries apart from CN and US are really less and will interfere with the predictions of the model.\n\n* Allthe null values are categorized under 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def origin_name(country):\n    ls = [\"VE\", \"SG\", \"GB\", \"AT\"]\n    if country in ls:\n        return \"Others\"\n    return country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['origin_country'].replace(np.nan, \"Others\", inplace=True)\nsales['origin_country'] = sales['origin_country'].apply(origin_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'origin_country', data = sales, order = sales['origin_country'].value_counts().iloc[:].index)\nplt.xlabel('Origin Country')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n\n## Columns with 1 unique value"},{"metadata":{},"cell_type":"markdown","source":"* Columns with only 1 unique value will not add any value to our model, so it would be best to drop them out."},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = sales.nunique()\nls[ls==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(labels = ['currency_buyer', 'theme', 'crawl_month'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n\n## Engineer a new feature"},{"metadata":{},"cell_type":"markdown","source":"* We will import the CSV file that has all the unique categories of tags sorted by count.\n\n* The aim is to find out the percentage of total number of tags available for a particular product. Our new feature will be 'tags_percentage'.\n\n* The reason behind engineering this feature is that the more number of tags a product has, the more it will turn up in searches. The probability of its units being sold more in number will be high.\n\n* We will drop the 'tags' feature thereafter because we do not need it for the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"collect_tags = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/unique-categories.sorted-by-count.csv')\nprint('Total number of tags: ', collect_tags.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return percentage of tags present for a product\n\ndef tag_number(tags):\n    ls = tags.split(',')\n    return len(ls)/collect_tags.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['tags_percentage'] = sales['tags'].apply(tag_number)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(labels = ['tags'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n\n## Remove unnecessary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.dtypes[sales.dtypes == 'object']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Columns: title, title_orig, merchant_profile_picture, product_url, product_picture, product_id, merchant_id, merchant_info_subtitle, merchant_name, merchant_title, shipping_option_name, urgency_text\n\n* These will be dropped for now, as the likelihood of these affecting the number of units sold is pretty less. For some of the features present above, a corresponding feature already exists in the dataset that provides more information relevant to the model we want to make. \n\n* The rating count column will also be removed for now as we already have features of the distribution of rating count across (5/4/3/2/1) which gives us a more detailed information than 'rating count'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(labels = ['title', 'title_orig', 'merchant_profile_picture', 'product_url', 'product_picture', 'product_id', 'merchant_id', \n                     'merchant_info_subtitle', 'merchant_name', 'merchant_title', 'shipping_option_name', 'urgency_text'], \n           axis=1, \n           inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(labels = ['rating_count'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='correlation'></a>\n\n# Correlation between features\n\n* We will check for correlation of all features with the number of units sold. \n\n* For the three categorical variables (product colour, variation size and origin country) however, we will do a separate check of correlation (using the one hot encoded format) with the units sold. This has been done because it will be difficult to visualise efficiently otherwise. "},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n\n### Categorical Variables: One hot encoding\n\n* We will change our categorical variables to one hot encoding format. "},{"metadata":{},"cell_type":"markdown","source":"### 1. Product Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"# product color\ndummies_color = pd.get_dummies(sales['product_color'], drop_first=True) # give us the one hot ecoded features\ndummies_color.drop(labels = 'others', axis=1, inplace=True) # remove the 'others' feature as n-1 encoded features represents n features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Product Variation Size ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"# product variation size id\ndummies_variation = pd.get_dummies(sales['product_variation_size_id'])\ndummies_variation.drop(labels = ['Others'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Origin Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies_origin = pd.get_dummies(sales['origin_country'])\ndummies_origin.drop(labels=['Others'], axis = 1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenating all the one hot encoded features for the three categorical variables above\n\nfeat_onehot = pd.concat([dummies_color, dummies_variation, dummies_origin, sales['units_sold']], axis=1)\nfeat_onehot.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_onehot_corr = feat_onehot.corr()\n\nfeat_onehot_corr['units_sold'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the above result we can safely say that the dependency of units sold on the product color, variation size or origin country is very unlikely. \n\n* For the same reason, we will DROP these three features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.drop(labels = ['product_color', 'product_variation_size_id', 'origin_country'], \n           axis=1, \n           inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The correlation between the rest of the features and units of the product sold"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_corr = sales.corr()\n\nplt.figure(figsize = (18, 16))\nsns.heatmap(sales_corr, annot=True, cmap='Blues_r')\nplt.title('Correlation between features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_corr['units_sold'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see above the correlation all features hold with the units sold. The method for correlation is *pearson*.\n\n* We will use the **SelectKBest method** to capture the best features for the model. "},{"metadata":{},"cell_type":"markdown","source":"<a id='selection'></a>\n\n# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating the independent and dependent variables\n\ny = sales['units_sold']\nX = sales.drop(labels = ['units_sold'], axis = 1)\nprint(\"Shape of X is {} and that of y is {}\".format(X.shape, y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n\nprint('Shape of training set ', X_train.shape)\nprint('Shape of test set ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SelectKBest\n\n* Selects features according to the k highest scores.\n\n* Scoring function used here is Mutual Info Regression\n\n### Scoring Function: Mutual Info Regression\n\n* We could have used the default scoring function: f_regression but that captures linear dependencies better.\n\n* mutual_info_regression can capture any type of dependency between variables which is what we would need here. Check out the comparison [here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html)."},{"metadata":{},"cell_type":"markdown","source":"* **Mutual Information Regression**: \n\n    * Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n    * The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances. \n    \nSource: [Link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\n\n# feature selection\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=mutual_info_regression, k='all')\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.tick_params(color='white', labelcolor='white')\nplt.xlabel('Features', color='white')\nplt.ylabel('Score of Features', color='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We shall select the best 8 features for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=mutual_info_regression, k=8)\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features\n\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n\nprint('Shape of Training set with the best features: ', X_train_fs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = fs.get_support(indices=True)\n\nprint('Best columns that we are using for our model\\n')\nfor i in cols:\n    print (sales.columns[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='regmodel'></a>\n\n# Regression Model"},{"metadata":{},"cell_type":"markdown","source":"We will try out models:\n1. Linear Regression\n2. Polynomial Regression\n3. SVR\n4. Decision Forest Regression\n5. Random Forest Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Regression Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressors = [LinearRegression(),\n             DecisionTreeRegressor(random_state=1),\n             RandomForestRegressor(n_estimators = 10, random_state=1)]\n\ndf = pd.DataFrame(columns = ['Name', 'Train Score', 'Test Score', 'Mean Absolute Error', 'Mean Squared Error', \n                             'Cross Validation Score (Mean Accuracy)', 'R2 Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for regressor in regressors:\n    regressor.fit(X_train_fs, y_train)\n    y_pred = regressor.predict(X_test_fs)\n    \n    # print classifier name\n    s = str(type(regressor)).split('.')[-1][:-2]\n    \n    # Train Score\n    train = regressor.score(X_train_fs, y_train)\n    \n    # Test Score\n    test = regressor.score(X_test_fs, y_test)\n    \n    # MAE score\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    # MSE Score\n    mse = mean_squared_error(y_test, y_pred)\n    \n    accuracy = cross_val_score(estimator = regressor, X = X_train_fs, y = y_train, cv=10)\n    cv = accuracy.mean()*100\n    \n    r2 = r2_score(y_test, y_pred)\n    \n    df = df.append({'Name': s, 'Train Score': train, 'Test Score': test, 'Mean Absolute Error': mae, \n                    'Mean Squared Error': mse, 'Cross Validation Score (Mean Accuracy)': cv,\n                   'R2 Score': r2},\n                  ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Polynomial Features\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree = 3)\nX_train_poly = poly_reg.fit_transform(X_train_fs)\nX_test_poly = poly_reg.fit_transform(X_test_fs)\n\n# Fitt PolyReg to training set\nregressor = LinearRegression()\nregressor.fit(X_train_poly, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_poly)\n\ndf = df.append({'Name': str(type(regressor)).split('.')[-1][:-2] + ' (Poly)', \n                'Train Score': regressor.score(X_train_poly, y_train), \n                'Test Score': regressor.score(X_test_poly, y_test), \n                'Mean Absolute Error': mean_absolute_error(y_test, y_pred), \n                'Mean Squared Error': mean_squared_error(y_test, y_pred), \n                'Cross Validation Score (Mean Accuracy)': cross_val_score(estimator = regressor, X = X_train_fs, y = y_train, cv=10).mean()*100,\n                'R2 Score': r2_score(y_test, y_pred)},\n                  ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# Applying feature scaling for this\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train_fs)\nX_test_sc = sc.fit_transform(X_test_fs)\n\nregressor = SVR(kernel='rbf')\nregressor.fit(X_train_sc, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_sc)\n\ndf = df.append({'Name': str(type(regressor)).split('.')[-1][:-2], \n                'Train Score': regressor.score(X_train_sc, y_train), \n                'Test Score': regressor.score(X_test_sc, y_test), \n                'Mean Absolute Error': mean_absolute_error(y_test, y_pred), \n                'Mean Squared Error': mean_squared_error(y_test, y_pred), \n                'Cross Validation Score (Mean Accuracy)': cross_val_score(estimator = regressor, X = X_train_sc, y = y_train, cv=10).mean()*100,\n                'R2 Score': r2_score(y_test, y_pred)},\n                  ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='gridsearch'></a>\n\n# GridSearchCV\n\nWe will perform GridSearch on Random Forest Regression that has already given us best results out of the pool of models we tried."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nreg = RandomForestRegressor(random_state=1)\n\nparam_grid = { \n    'n_estimators': np.arange(4, 30, 2),\n    'max_depth' : [4,5,6,7,8],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_reg = GridSearchCV(estimator=reg, param_grid=param_grid, cv= 5)\nCV_reg.fit(X_train_fs, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now check the results from the regressor with the best results."},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = RandomForestRegressor(n_estimators=18, random_state=1, max_depth=4)\n\nregressor.fit(X_train_fs, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_fs)\n\ndf = df.append({'Name': str(type(regressor)).split('.')[-1][:-2] + ' (after GridSearchCV)', \n                'Train Score': regressor.score(X_train_fs, y_train), \n                'Test Score': regressor.score(X_test_fs, y_test), \n                'Mean Absolute Error': mean_absolute_error(y_test, y_pred), \n                'Mean Squared Error': mean_squared_error(y_test, y_pred), \n                'Cross Validation Score (Mean Accuracy)': cross_val_score(estimator = regressor, X = X_train_fs, y = y_train, cv=10).mean()*100,\n                'R2 Score': r2_score(y_test, y_pred)},\n                  ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='finalmodel'></a>\n\n# Final Model: Model Boosting\n\n**MODEL BOOSTING:** We have used VotingRegressor to boost our results. \n\nVotingRegressor: A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. Click [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) for more details.\n\nThe voting regressor uses *linear regressor* and the best possible *random forest regressor* to give predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\nregressor = VotingRegressor([('lr',LinearRegression()), ('rf', RandomForestRegressor(n_estimators=18, random_state=1, max_depth=4))])\n\nregressor.fit(X_train_fs, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_fs)\n\ndf = df.append({'Name': str(type(regressor)).split('.')[-1][:-2], \n                'Train Score': regressor.score(X_train_fs, y_train), \n                'Test Score': regressor.score(X_test_fs, y_test), \n                'Mean Absolute Error': mean_absolute_error(y_test, y_pred), \n                'Mean Squared Error': mean_squared_error(y_test, y_pred), \n                'Cross Validation Score (Mean Accuracy)': cross_val_score(estimator = regressor, X = X_train_fs, y = y_train, cv=10).mean()*100,\n                'R2 Score': r2_score(y_test, y_pred)},\n                  ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The specifications of the most optimum model:\n\nVotingRegressor: \n1. Linear Regressor\n2. Random Forest Regressor (n_estimators=18, max_depth=4)\n\nwith results:\n\n* Train Score: 0.88\n* Test Score: 0.83\n* MAE: 1451.06\n* MSE: 8.88e+06\n* CV Score (Mean Accuracy): 77.16\n* R2 Score: 0.83"},{"metadata":{},"cell_type":"markdown","source":"***Hey! ***\n\n***Do comment and let me know about scope of improvement in this.***\n\n***Do upvote if you like my notebook. It will be much motivation!!***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}