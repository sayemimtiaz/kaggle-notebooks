{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: black ; color : #CCCC00; text-align: center; border-radius: 100px 100px;padding:10px\">Object Detetction with YOLO </h1>\n<br>\n\n<img src=\"https://pjreddie.com/media/image/yologo_2.png\">\n<span style=\"font-size:15px\">\nYou only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev.\n</span><br>\n<hr style=\"border:2px solid gray\">\n<!-- <b><span class=\"label label-default\" style=\"color:#03e8fc;font-size:30px;border-radius:6px;\"> Performance of YOLO with other Detectors</span></b><br> -->\n\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Performance of YOLO with other Detectors</span></h1>\n\n<span style=\"font-size:15px\">\nYOLOv3 is extremely fast and accurate. In mAP measured at .5 IOU YOLOv3 is on par with Focal Loss but about 4x faster. Moreover, you can easily tradeoff between speed and accuracy simply by changing the size of the model, no retraining required!</span>\n<br><br>\n<span style=\"background-color:black\">\n<img src=\"https://pjreddie.com/media/image/map50blue.png\" style=\"background-color: black\">\n</span><br>\n\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Working of YOLO</span></h1>\n<br>\n<span style=\"font-size:15px\">\nPrior detection systems repurpose classifiers or localizers to perform detection. They apply the model to an image at multiple locations and scales. High scoring regions of the image are considered detections.\nYOLO uses a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities.</span><br>\n\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">How is YOLO different from other detectors from R-CNN family</span></h1><br>\n<span style=\"font-size:15px\">\nYOLO model has several advantages over classifier-based systems. It looks at the whole image at test time so its predictions are informed by global context in the image. It also makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image. This makes it extremely fast, more than 1000x faster than R-CNN and 100x faster than Fast R-CNN. See our paper for more details on the full system.</span>\n\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">How is YOLO different from other detectors from R-CNN family</span></h1><br>\n<span style=\"font-size:15px\">\nIn this notebook, we will discover :\n<br><br>\n1) How to develop a YOLOv3 model for object detection on new photographs.\nAfter completing this tutorial, you will know:YOLO-based Convolutional Neural Network family of models for object detection and the most recent variation called YOLOv3.\n\n2) The best-of-breed open source library implementation of the YOLOv3 for the Keras deep learning library.\nHow to use a pre-trained YOLOv3 to perform object localization and detection on new photographs.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Object detection with Yolov3</span></h1><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Create and Load pretrained model&nbsp;&nbsp;&nbsp;&nbsp;</h1> \n<span style=\"font-size:15px\">\nSteps:\n1) These were trained using the DarkNet code base on the MSCOCO dataset. Download the model weights and place them into your current working directory with the filename “yolov3.weights.”\n\n2) We need to define a Keras model that has the right number and type of layers to match the downloaded model weights. The model architecture is called a “DarkNet” and was originally loosely based on the VGG-16 model.\n\n3) We will create 2 helper function - one for loading the yolov3 pretrained weights and other for creating convolution blocks.","metadata":{}},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Create block of layers</span></h1><br>","metadata":{}},{"cell_type":"code","source":"import struct\nimport numpy as np\nfrom keras.layers import Conv2D\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers.merge import add, concatenate\nfrom keras.models import Model\nimport glob","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:14.974784Z","iopub.execute_input":"2021-08-08T11:21:14.975276Z","iopub.status.idle":"2021-08-08T11:21:21.643951Z","shell.execute_reply.started":"2021-08-08T11:21:14.975163Z","shell.execute_reply":"2021-08-08T11:21:21.643001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _conv_block(inp, convs, skip=True):\n    x = inp\n    count = 0\n    for conv in convs:\n        if count == (len(convs) - 2) and skip:\n            skip_connection = x\n        count += 1\n        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n        x = Conv2D(conv['filter'],\n                   conv['kernel'],\n                   strides=conv['stride'],\n                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n                   name='conv_' + str(conv['layer_idx']),\n                   use_bias=False if conv['bnorm'] else True)(x)\n        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n    return add([skip_connection, x]) if skip else x","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:24.578445Z","iopub.execute_input":"2021-08-08T11:21:24.579068Z","iopub.status.idle":"2021-08-08T11:21:24.590606Z","shell.execute_reply.started":"2021-08-08T11:21:24.579013Z","shell.execute_reply":"2021-08-08T11:21:24.589396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Make YOLO model</span></h1><br>","metadata":{}},{"cell_type":"code","source":"def make_yolov3_model():\n    input_image = Input(shape=(None, None, 3))\n    # Layer  0 => 4\n    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n    # Layer  5 => 8\n    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n    # Layer  9 => 11\n    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n    # Layer 12 => 15\n    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n    # Layer 16 => 36\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n    skip_36 = x\n    # Layer 37 => 40\n    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n    # Layer 41 => 61\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n    skip_61 = x\n    # Layer 62 => 65\n    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n    # Layer 66 => 74\n    for i in range(3):\n        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n    # Layer 75 => 79\n    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n    # Layer 80 => 82\n    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n    # Layer 83 => 86\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_61])\n    # Layer 87 => 91\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n    # Layer 92 => 94\n    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n    # Layer 95 => 98\n    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_36])\n    # Layer 99 => 106\n    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n    return model\n ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:26.542276Z","iopub.execute_input":"2021-08-08T11:21:26.542708Z","iopub.status.idle":"2021-08-08T11:21:26.588028Z","shell.execute_reply.started":"2021-08-08T11:21:26.542675Z","shell.execute_reply":"2021-08-08T11:21:26.586546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Read and load the weight object</span></h1><br>","metadata":{}},{"cell_type":"code","source":"class WeightReader:\n    def __init__(self, weight_file):\n        with open(weight_file, 'rb') as w_f:\n            major,= struct.unpack('i', w_f.read(4))\n            minor,= struct.unpack('i', w_f.read(4))\n            revision, = struct.unpack('i', w_f.read(4))\n            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n                w_f.read(8)\n            else:\n                w_f.read(4)\n            transpose = (major > 1000) or (minor > 1000)\n            binary = w_f.read()\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype='float32')\n \n    def read_bytes(self, size):\n        self.offset = self.offset + size\n        return self.all_weights[self.offset-size:self.offset]\n \n    def load_weights(self, model):\n        for i in range(106):\n            try:\n                conv_layer = model.get_layer('conv_' + str(i))\n                print(\"loading weights of convolution #\" + str(i))\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer('bnorm_' + str(i))\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n                    beta  = self.read_bytes(size) # bias\n                    gamma = self.read_bytes(size) # scale\n                    mean  = self.read_bytes(size) # mean\n                    var   = self.read_bytes(size) # variance\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n                if len(conv_layer.get_weights()) > 1:\n                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel])\n            except ValueError:\n                print(\"no convolution #\" + str(i))\n \n    def reset(self):\n        self.offset = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:28.70427Z","iopub.execute_input":"2021-08-08T11:21:28.704731Z","iopub.status.idle":"2021-08-08T11:21:28.722902Z","shell.execute_reply.started":"2021-08-08T11:21:28.704688Z","shell.execute_reply":"2021-08-08T11:21:28.72155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Instantiate</span></h1><br>","metadata":{}},{"cell_type":"code","source":"# define the model\nmodel = make_yolov3_model()\n# load the model weights\nweight_reader = WeightReader('../input/yolov3/yolov3.weights')\n# set the model weights into the model\nweight_reader.load_weights(model)\n# save the model to output folder\nmodel.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:30.989402Z","iopub.execute_input":"2021-08-08T11:21:30.989956Z","iopub.status.idle":"2021-08-08T11:21:37.773656Z","shell.execute_reply.started":"2021-08-08T11:21:30.989902Z","shell.execute_reply":"2021-08-08T11:21:37.772631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Prepare the input image for model prediction&nbsp;&nbsp;&nbsp;&nbsp;</h1> \n\nWe need to load our new photograph and prepare it as suitable input to the model. The model expects inputs to be color images with the square shape of 416×416 pixels.\n\nWe can use the load_img() Keras function to load the image and the target_size argument to resize the image after loading. We can also use the img_to_array() function to convert the loaded PIL image object into a NumPy array, and then rescale the pixel values from 0-255 to 0-1 32-bit floating point values.","metadata":{}},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Load the saved yolov3 model</span></h1><br>","metadata":{}},{"cell_type":"code","source":"from numpy import expand_dims\nfrom keras.models import load_model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n \n# load and prepare an image\ndef load_image_pixels(filename, shape):\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    # convert to numpy array\n    image = img_to_array(image)\n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image /= 255.0\n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height\n \n# load yolov3 model\nmodel = load_model('./model.h5')\n# define the expected input shape for the model\ninput_w, input_h = 416, 416","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:41.667292Z","iopub.execute_input":"2021-08-08T11:21:41.667811Z","iopub.status.idle":"2021-08-08T11:21:44.081153Z","shell.execute_reply.started":"2021-08-08T11:21:41.667764Z","shell.execute_reply":"2021-08-08T11:21:44.080076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Load the sample image</span></h1><br>","metadata":{}},{"cell_type":"code","source":"## load a image\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimg=cv2.imread('../input/car-object-detection/data/training_images/vid_4_10000.jpg')\nplt.imshow(img)\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:48.254538Z","iopub.execute_input":"2021-08-08T11:21:48.254917Z","iopub.status.idle":"2021-08-08T11:21:48.771674Z","shell.execute_reply.started":"2021-08-08T11:21:48.254881Z","shell.execute_reply":"2021-08-08T11:21:48.770791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_cvt=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img_cvt)\nprint(img_cvt.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:48.81252Z","iopub.execute_input":"2021-08-08T11:21:48.813211Z","iopub.status.idle":"2021-08-08T11:21:49.056587Z","shell.execute_reply.started":"2021-08-08T11:21:48.813169Z","shell.execute_reply":"2021-08-08T11:21:49.055297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\n\nclass BoundBox:\n    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        self.objness = objness\n        self.classes = classes\n        self.label = -1\n        self.score = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n\n        return self.label\n\n    def get_score(self):\n        if self.score == -1:\n            self.score = self.classes[self.get_label()]\n\n        return self.score\n\ndef _sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\ndef decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n    grid_h, grid_w = netout.shape[:2]\n    nb_box = 3\n    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n    nb_class = netout.shape[-1] - 5\n    boxes = []\n    netout[..., :2]  = _sigmoid(netout[..., :2])\n    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n\n    for i in range(grid_h*grid_w):\n        row = i / grid_w\n        col = i % grid_w\n        for b in range(nb_box):\n            # 4th element is objectness score\n            objectness = netout[int(row)][int(col)][b][4]\n            if(objectness.all() <= obj_thresh): continue\n            # first 4 elements are x, y, w, and h\n            x, y, w, h = netout[int(row)][int(col)][b][:4]\n            x = (col + x) / grid_w # center position, unit: image width\n            y = (row + y) / grid_h # center position, unit: image height\n            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n            # last elements are class probabilities\n            classes = netout[int(row)][col][b][5:]\n            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n            boxes.append(box)\n    return boxes\n\ndef correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n    new_w, new_h = net_w, net_h\n    for i in range(len(boxes)):\n        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n\ndef _interval_overlap(interval_a, interval_b):\n    x1, x2 = interval_a\n    x3, x4 = interval_b\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n            return 0\n        else:\n            return min(x2,x4) - x3\n\ndef bbox_iou(box1, box2):\n    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n    intersect = intersect_w * intersect_h\n    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n    union = w1*h1 + w2*h2 - intersect\n    return float(intersect) / union\n\ndef do_nms(boxes, nms_thresh):\n    if len(boxes) > 0:\n        nb_class = len(boxes[0].classes)\n    else:\n        return\n    for c in range(nb_class):\n        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n        for i in range(len(sorted_indices)):\n            index_i = sorted_indices[i]\n            if boxes[index_i].classes[c] == 0: continue\n            for j in range(i+1, len(sorted_indices)):\n                index_j = sorted_indices[j]\n                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n                    boxes[index_j].classes[c] = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:51.066647Z","iopub.execute_input":"2021-08-08T11:21:51.067035Z","iopub.status.idle":"2021-08-08T11:21:51.101034Z","shell.execute_reply.started":"2021-08-08T11:21:51.066995Z","shell.execute_reply":"2021-08-08T11:21:51.099261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load and prepare an image\ndef load_image_pixels(filename, shape):\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    # convert to numpy array\n    image = img_to_array(image)\n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image /= 255.0\n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height\n\n# get all of the results above a threshold\ndef get_boxes(boxes, labels, thresh):\n    v_boxes, v_labels, v_scores = list(), list(), list()\n    # enumerate all boxes\n    for box in boxes:\n        # enumerate all possible labels\n        for i in range(len(labels)):\n            # check if the threshold for this label is high enough\n            if box.classes[i] > thresh:\n                v_boxes.append(box)\n                v_labels.append(labels[i])\n                v_scores.append(box.classes[i]*100)\n                # don't break, many labels may trigger for one box\n    return v_boxes, v_labels, v_scores\n\n# draw all results\ndef draw_boxes(filename, v_boxes, v_labels, v_scores):\n    # load the image\n    data = pyplot.imread(filename)\n    # plot the image\n    pyplot.imshow(data)\n    # get the context for drawing boxes\n    ax = pyplot.gca()\n    # plot each box\n    for i in range(len(v_boxes)):\n        box = v_boxes[i]\n        # get coordinates\n        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n        # calculate width and height of the box\n        width, height = x2 - x1, y2 - y1\n        # create the shape\n        rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n        # draw the box\n        ax.add_patch(rect)\n        # draw text and score in top left corner\n        label = \"%s (%.2f)\" % (v_labels[i], v_scores[i])\n        pyplot.text(x1, y1, label, color='white')\n    # show the plot\n    pyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:51.890986Z","iopub.execute_input":"2021-08-08T11:21:51.891388Z","iopub.status.idle":"2021-08-08T11:21:51.903783Z","shell.execute_reply.started":"2021-08-08T11:21:51.891353Z","shell.execute_reply":"2021-08-08T11:21:51.902658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load yolov3 model\nmodel = load_model('./model.h5')\n# define the expected input shape for the model\ninput_w, input_h = 416, 416\n# define our new photo\nphoto_filename = '../input/car-object-detection/data/training_images/vid_4_10000.jpg'\n# load and prepare image\nimage, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n# make prediction\nyhat = model.predict(image)\n# summarize the shape of the list of arrays\nprint([a.shape for a in yhat])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:53.739284Z","iopub.execute_input":"2021-08-08T11:21:53.739684Z","iopub.status.idle":"2021-08-08T11:21:58.325139Z","shell.execute_reply.started":"2021-08-08T11:21:53.73965Z","shell.execute_reply":"2021-08-08T11:21:58.32427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the anchors\nanchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n# define the probability threshold for detected objects\nclass_threshold = 0.6\nboxes = list()\nfor i in range(len(yhat)):\n    # decode the output of the network\n    boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:58.328967Z","iopub.execute_input":"2021-08-08T11:21:58.330754Z","iopub.status.idle":"2021-08-08T11:21:59.206542Z","shell.execute_reply.started":"2021-08-08T11:21:58.330711Z","shell.execute_reply":"2021-08-08T11:21:59.205254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correct the sizes of the bounding boxes for the shape of the image\ncorrect_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n# suppress non-maximal boxes\ndo_nms(boxes, 0.5)\n# define the labels\nlabels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n# get the details of the detected objects\nv_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:21:59.209169Z","iopub.execute_input":"2021-08-08T11:21:59.209644Z","iopub.status.idle":"2021-08-08T11:22:06.507254Z","shell.execute_reply.started":"2021-08-08T11:21:59.209595Z","shell.execute_reply":"2021-08-08T11:22:06.506077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize what we found\nfor i in range(len(v_boxes)):\n    print(v_labels[i], v_scores[i])\n# draw what we found\ndraw_boxes(photo_filename, v_boxes, v_labels, v_scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:22:06.509026Z","iopub.execute_input":"2021-08-08T11:22:06.509345Z","iopub.status.idle":"2021-08-08T11:22:06.756055Z","shell.execute_reply.started":"2021-08-08T11:22:06.509314Z","shell.execute_reply":"2021-08-08T11:22:06.754756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Create a function to predict the bounding box with class probability in test set</span></h1><br>","metadata":{}},{"cell_type":"code","source":"def predict_bb_image(image):\n    # define the expected input shape for the model\n    input_w, input_h = 416, 416\n    # define our new photo\n    photo_filename = image\n    # load and prepare image\n    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n    # make prediction\n    yhat = model.predict(image)\n    # summarize the shape of the list of arrays\n    print([a.shape for a in yhat])\n    \n    # define the anchors\n    anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n    # define the probability threshold for detected objects\n    class_threshold = 0.6\n    boxes = list()\n    for i in range(len(yhat)):\n        # decode the output of the network\n        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n        \n    # correct the sizes of the bounding boxes for the shape of the image\n    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n    # suppress non-maximal boxes\n    do_nms(boxes, 0.5)\n    # define the labels\n    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n        \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n        \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n        \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n        \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n        \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n        \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n        \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n        \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n        \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n    \n    # get the details of the detected objects\n    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n    \n    # summarize what we found\n    for i in range(len(v_boxes)):\n        print(v_labels[i], round(v_scores[i],2))\n    # draw what we found\n    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:22:06.758188Z","iopub.execute_input":"2021-08-08T11:22:06.758644Z","iopub.status.idle":"2021-08-08T11:22:06.774933Z","shell.execute_reply.started":"2021-08-08T11:22:06.758593Z","shell.execute_reply":"2021-08-08T11:22:06.773655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Test Set prediction</span></h1><br>","metadata":{}},{"cell_type":"code","source":"j=0\nfor i in range(len(glob.glob('../input/car-object-detection/data/testing_images/*.jpg'))):\n    if j<=10:\n        predict_bb_image(glob.glob('../input/car-object-detection/data/testing_images/*.jpg')[i])\n        j+=1\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:22:06.77707Z","iopub.execute_input":"2021-08-08T11:22:06.777487Z","iopub.status.idle":"2021-08-08T11:23:45.251671Z","shell.execute_reply.started":"2021-08-08T11:22:06.777445Z","shell.execute_reply":"2021-08-08T11:23:45.250577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Train Set prediction</span></h1><br>","metadata":{}},{"cell_type":"code","source":"j=0\nfor i in range(len(glob.glob('../input/car-object-detection/data/training_images/*.jpg'))):\n    if j<=10:\n        predict_bb_image(glob.glob('../input/car-object-detection/data/training_images/*.jpg')[i])\n        j+=1\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-08T11:23:45.254261Z","iopub.execute_input":"2021-08-08T11:23:45.254598Z","iopub.status.idle":"2021-08-08T11:25:24.771778Z","shell.execute_reply.started":"2021-08-08T11:23:45.254563Z","shell.execute_reply":"2021-08-08T11:25:24.770923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}