{"cells":[{"metadata":{"_uuid":"c6153a8bb0c589d8eabe600c2b2f0e09c47ec104"},"cell_type":"markdown","source":"# Forest Cover Type\n## Supervised Learning, Classification"},{"metadata":{"_uuid":"fbdf0157d392771fa3ec278f296c8c3a8dda3f60"},"cell_type":"markdown","source":"----\n## Table of Contents\n\n- [Description](#description)\n- [Getting Started](#Getting-Started)\n    - [Explaination of the Data](#Explaination-of-the-data)\n- [Data Exploration](#Data-Exploration)\n    - [Feature Statistics](#Feature-Statistics)\n        - [Feature Describe](#Feature-Describe)\n        - [Feature Skew](#Feature-Skew)\n        - [Class Distribution](#Class-Distribution)\n    - [Feature Visualization](#Feature-Visualization)\n        - [Feature Spread](#Feature-Spread)\n        - [Feature Distribution](#Feature-Distribution)\n        - [Feature Comparison](#Feature-Comparison)\n        - [Feature Correlation](#Feature-Correlation)       \n- [Data Engineering](#Data-Engineering)\n    - [Observation Cleaning](#Observation-Cleaning)\n        - [Handling Missing Values](#Handling-Missing-Values)\n        - [Handling-Duplicates](#Handling-Duplicates)\n    - [Dimentionality Reduction](#Dimentionality-Reduction)\n        - [Extra-Trees Classifier](#Extra-Trees-Classifier)\n        - [Random Forest Classifier](#Random-Forest-Classifier)\n        - [AdaBoost Classifier](#AdaBoost-Classifier)\n        - [Gradient Boosting Classifier](#Gradient-Boosting-Classifier)\n    - [Train-Test Split](#Train-Test-Split)\n    - [Feature Scaling](#Feature-Scaling)\n- [Model Evaluations](#Model-Evaluations)\n    - [Benchmark Model](#Benchmark-Model)\n    - [K-Nearest Neighbors](#1.-K-Nearest-Neighbors)\n    - [Random Forest Classifier](#2.-Random-Forest-Classifier)\n    - [Stochastic Gradient Descent Classifier](#3.-Stochastic-Gradient-Descent-Classifier)\n    - [Extra Trees Classifier](#4.-Extra-Trees-Classifier)\n    - [Logistic Regression](#5.-Logistic-Regression)\n    - [Choosing Model](#Choosing-Model)\n- [Testing Model](#Testing-Model)\n- [Conclusion](#Conclusion)\n- [Notes](#Notes)\n-----\n-----"},{"metadata":{"_uuid":"d2dbc397fdd71e86e34878750d6a20b313903878"},"cell_type":"markdown","source":"## Description\n\nA short description I want to give of how I am going to solve this project before starting. Our goal in this project is to classify which forest type it is from the data given.\n\n- This study area includes 4 Wilderness Areas located in the Roosevelt National Forest of Northern Colorado. These area represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological process rather than forest management practices.\n\n- Each observation is 30m x 30m forest cover type determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from the data originally obtained from US Geological Survey (USGS) and USFS data.\n\n- I have been given a total of 54 attributes/features, (excluding 1 target variable) these attributes contain Binary and Quantative attributes, and I need to predict which Forest Cover-Type is it from the given features.\n\n- I will first explore the data, visualize it, know what the data wants to tell us. Remove any missing values and features that have null values and scale the data within a specific range.\n\n- Also perform dimensionality reduction procedure where I will use 4 models to tell us which are useful in order to predict the target variable, and then using features which gives us hgih score in the most models. Those 4 Models are:\n    \n    - Extra Trees Classifier (ETC)\n    - Random Forest (RF)\n    - AdaBoost Classifier (ADBC)\n    - Gradient Boosting Classifier (GBC)\n    \n\n- Split the data 75%-25%, train-test set respectively. Will use 10 K-fold Cross Validation on train set.\n\n- Feed the training data to the Naive Bayes (Our Benchmark Model) and evaluate the result.\n\n- Training will be done on the Solution Models that I have chose, those are:-\n\n    - K-Nearest Neighbour (KNN), \n    - Random Forest (RF),\n    - Stochastic Gradient Descent Classifier (SGDC),\n    - Extra Trees Classifier (ETC),\n    - Logistic Regression (LG)\n    \n \n- Scores will be evaluated with Accuracy and F1 score metrics.\n\n- Choosing the best model from above based on metrics scores and testing that model on the test set.\n\n- Conclusions\n\nDetailed Info regarding on how I am going to approach this problem and data summary is given in `proposal.pdf` file. [Visit](https://github.com/JuzerShakir/Forest_Cover-Type/blob/master/proposal.pdf)"},{"metadata":{"trusted":true,"_uuid":"c23a4c26c069a9151f203274c101b91c82f57223"},"cell_type":"code","source":"# Importing required libraries for the project\nimport sys # for python library version\nimport numpy as np # for scientific computing\nimport pandas as pd # for data anaysis\nimport matplotlib # for visualization\nimport seaborn as sns # for visualization\nimport sklearn # ML Library","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a9cf6048e2f324262e707a03c680d390b7e5a3cb"},"cell_type":"code","source":"print('Python: {}'.format(sys.version))  # Python version\nprint('numpy: {}'.format(np.__version__))  # Numpy version\nprint('pandas: {}'.format(pd.__version__))  # Pandas version\nprint('matplotlib: {}'.format(matplotlib.__version__))  # Matplotlib version\nprint('seaborn: {}'.format(sns.__version__))  # seaborn version\nprint('sklearn: {}'.format(sklearn.__version__))  # sklearn version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b4a217c74b8ed299b93013912984a92d9b32fc"},"cell_type":"code","source":"# No warning of any kind please!\nimport warnings\n# will ignore any warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13aafbab5226a970e1ade36ef5a92425f0219949"},"cell_type":"markdown","source":"------\n------"},{"metadata":{"_uuid":"5ef093651bc233141ac744cfec622f38b83b62fc"},"cell_type":"markdown","source":"## Getting Started\nFirst thing first, we need to import the dataset and have a peak at it...."},{"metadata":{"trusted":true,"_uuid":"f81c8aa211576678a63cf4bad2cdeec488a14319"},"cell_type":"code","source":"# importing the dataset to a variable\ndata = pd.read_csv(\"../input/covtype.csv\")\n\n# displaying first 3 observations\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae6c531515a7f5448e589f9d7387a7e9a4a6bcc7"},"cell_type":"markdown","source":"We can see that the data has been imported successfully but there are missing column names. We need to give column names in order to keep track of columns and make sense of features and data we have.\n\nThe column names are given here on [Kaggle](https://www.kaggle.com/uciml/forest-cover-type-dataset)"},{"metadata":{"trusted":true,"_uuid":"da0d9244f931bcf2f5dcded6af7a5b89896d362b"},"cell_type":"code","source":"# since the data doesn't have column names, we will provide it in a form of list\nfeature_names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n                 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', \n                'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7',\n                'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', \n                 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', \n                 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', \n                 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n\n# Feeding column names to the data\ndata.columns = feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b41cb0569e2f5d0aef94b7f0a54544ed0e06ea12"},"cell_type":"code","source":"# displaying first 5 observation\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3097d0eaa31161d843afa760c34332650a32864e"},"cell_type":"markdown","source":"Hmm, Now that makes sense. \nNow we need to know the number of observations and features we have."},{"metadata":{"trusted":true,"_uuid":"ae99cba57754807d6e8d9fe820bf4dd19abc31b5"},"cell_type":"code","source":"# dimensions of the data\n# where x will be no. of observation\n# and y will be features including 1 target variable\nx, y = data.shape\n\nprint('We have ', x, ' number of observations and ', y-1, ' features for this dataset to predict type of forest cover.')  # removing count of a target variable in 'y'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e457f4ef4aeb4c751d977aaba88ee91c23cb5d51"},"cell_type":"markdown","source":"Let's look at the datatypes of each feature and see if it needs any processing if the feature is not in its appropriate form.."},{"metadata":{"trusted":true,"_uuid":"87ed13438cda688d2762b240cb84daefb2107f9d"},"cell_type":"code","source":"# datatypes of features\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec034d074db7e1282b76952558e8df83709dde8b"},"cell_type":"markdown","source":"Well since all are numeric integer and should be so, then we do not need to do any convertions here."},{"metadata":{"_uuid":"37c46e2f9c96d17839297288e3e1090e7c957212"},"cell_type":"markdown","source":"---------"},{"metadata":{"_uuid":"cc393d413dd5efeed2db961e90624b9ad31953fd"},"cell_type":"markdown","source":"### Explaination of the data\n\nOur dataset has `54` features and `1` target variable `'Cover_Type'`. From `54` features, `10` are `numeric` and `44` are `catrgorical`. From `44` categorical, `40` are of `Soil_Type` and `4` of `Wilderness_Area`.\n\nWe have been provided the names of all `Soil_Type` and `Wilderness_Areas` for this dataset. The table below lists all the names with respect to their feature names in the column:\n\nThis information is available on [Kaggle](https://www.kaggle.com/uciml/forest-cover-type-dataset), [UCI](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info) and in my [Proposal](https://github.com/JuzerShakir/Forest_Cover-Type/blob/master/proposal.pdf), but for convenience I have documented here too."},{"metadata":{"_uuid":"cab56dd6117c59986baa836460c8f56dd4fc32f4"},"cell_type":"markdown","source":"| Feature Name | Names |\n| ------------ | ----- |\n| Wilderness_Area1 | Rawah Wilderness Area |\n| Wilderness_Area2 | Neota Wilderness Area |\n| Wilderness_Area3 | Comanche Wilderness Area |\n| Wilderness_Area4 | Cache La Poudre Wilderness Area |\n| Soil_Type1 | Cathedral family - Rock outcrop complex, extremely stony |\n| Soil_Type2 | Vanet - Ratake families complex, very stony |\n| Soil_Type3 | Haploborolis - Rock outcrop complex, rubbly |\n| Soil_Type4 | Ratake family - Rock outcrop complex, rubbly |\n| Soil_Type5 | Vanet family - Rock outcrop complex, rubbly |\n| Soil_Type6 | Vanet - Wetmore families - Rock outcrop complex, stony |\n| Soil_Type7 | Gothic family |\n| Soil_Type8 | Supervisor - Limber families complex |\n| Soil_Type9 | Troutville family, very stony |\n| Soil_Type10 | Bullwark - Catamount families - Rock outcrop complex, rubbly |\n| Soil_Type11 | Bullwark - Catamount families - Rock land complex, rubbly |\n| Soil_Type12 | Legault family - Rock land complex, stony |\n| Soil_Type13 | Catamount family - Rock land - Bullwark family complex, rubbly |\n| Soil_Type14 | Pachic Argiborolis - Aquolis complex |\n| Soil_Type15 | _unspecified in the USFS Soil and ELU Survey_ |\n| Soil_Type16 | Cryaquolis - Cryoborolis complex |\n| Soil_Type17 | Gateview family - Cryaquolis complex |\n| Soil_Type18 | Rogert family, very stony |\n| Soil_Type19 | Typic Cryaquolis - Borohemists complex |\n| Soil_Type20 | Typic Cryaquepts - Typic Cryaquolls complex |\n| Soil_Type21 | Typic Cryaquolls - Leighcan family, till substratum complex |\n| Soil_Type22 | Leighcan family, till substratum, extremely bouldery |\n| Soil_Type23 | Leighcan family, till substratum, - Typic Cryaquolls complex. |\n| Soil_Type24 | Leighcan family, extremely stony |\n| Soil_Type25 | Leighcan family, warm, extremely stony |\n| Soil_Type26 | Granile - Catamount families complex, very stony |\n| Soil_Type27 | Leighcan family, warm - Rock outcrop complex, extremely stony |\n| Soil_Type28 | Leighcan family - Rock outcrop complex, extremely stony |\n| Soil_Type29 | Como - Legault families complex, extremely stony |\n| Soil_Type30 | Como family - Rock land - Legault family complex, extremely stony |\n| Soil_Type31 | Leighcan - Catamount families complex, extremely stony |\n| Soil_Type32 | Catamount family - Rock outcrop - Leighcan family complex, extremely stony |\n| Soil_Type33 | Leighcan - Catamount families - Rock outcrop complex, extremely stony |\n| Soil_Type34 | Cryorthents - Rock land complex, extremely stony |\n| Soil_Type35 | Cryumbrepts - Rock outcrop - Cryaquepts complex |\n| Soil_Type36 | Bross family - Rock land - Cryumbrepts complex, extremely stony |\n| Soil_Type37 | Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony |\n| Soil_Type38 | Leighcan - Moran families - Cryaquolls complex, extremely stony |\n| Soil_Type39 | Moran family - Cryorthents - Leighcan family complex, extremely stony |\n| Soil_Type40 | Moran family - Cryorthents - Rock land complex, extremely stony |\n"},{"metadata":{"_uuid":"58f1f6d93603f55c94b8d17e6d24c70b1a5b8221"},"cell_type":"markdown","source":"Yeah! I know! Sorry, I didn't get to pick the names! But just given here for reference for curious people who might be wondering what names do Wilderness and Soil Area the forest have. \n\nWe will stick to the current feature names and not change it for ease! Later, if necessory, we might take a look at this if we see some feature catch our interests.\n\nJust to clarify, the categorical variable we have here is 2. And these alone have 44 features. But an observation can only have presence of any 2 feature from 44, 1 for Soil and 1 for Wilderness. So if an observation has `'1'` in `Wilderness_Area4` and `'1'` in `Soil_Type12`, it means that it's respective Soil and Wilderness is present, while all other 42 features will have `'0'` hence its absence. And this is what a categorical feature means. And also these are `one-hot encoded` for us, so thanks to the authors!\n\nTalking about numeric features, `Aspect` and `Slope` have measurement in `degrees` while 3 `Hillshade..` features have values range from `0 to 255` index, describing summer solstice. Remaining 5 out of 10 numerical features have measurement in `Meters`. \n\nThe target variable `Cover_Type` ranges bewtween integer value `1 - 7` and each number is a key reprsenting names of different forest type. Let's look at what number represents which forest cover types."},{"metadata":{"_uuid":"180020429225dac2306f249dcc3e00d1994e9871"},"cell_type":"markdown","source":"| Key | Name |\n| --- | ---- |\n| 1 | Spruce / Fir |\n| 2 | Lodgepole Pine |\n| 3 | Ponderosa Pine |\n| 4 | Cottonwood / Willow |\n| 5 | Aspen |\n| 6 | Douglas-fir |\n| 7 | Krummholz |"},{"metadata":{"_uuid":"1cb6f64c3ae4923d7ef22c7196fc9e2fdf6ec0fb"},"cell_type":"markdown","source":"We will later take a look how many observations are there for each class.\n\nEnough of explanations, and start exploring the data statistically...\n\n-------\n-------"},{"metadata":{"_uuid":"004afe60ff7586a9011db37bfb8788ad042486f3"},"cell_type":"markdown","source":"## Data Exploration\n\n### Feature Statistics\n\n#### Feature Describe\n\nWe will split the data in 2 parts. First part will contain all `numerical features` and second part will contain all `binary or categorical features` of the data. The target variable `Cover_Type` is excluded.\n\n**We will look at the statistics of numerical features and extract useful info out of it.**"},{"metadata":{"trusted":true,"_uuid":"517f116f687880ff98b3e9844c4078376db0b886"},"cell_type":"code","source":"# Extracting all numerical features from data\nnum_fea = data.iloc[:, :10]\n\n# extracting all binary features from data\nbinary_fea = data.iloc[:, 10:-1]\n\n\n# statistics of numerical features\nnum_fea.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9bfe09472ba919cabec9a1da0e191028caffa7"},"cell_type":"markdown","source":"- Mean of the feature vary from as low as 14 to as high as 2959. Different features taking on different ranges of values. Describe `'Range'` you ask, thats what `Standard deviation` is here for.\n\n- Standard deviation tells us how spread the data is from the mean, here we can see `Horizontal_Distance_To_Roadways` is the most spread out data followed by `Horizontal_Distance_To_Fire_Points` and `Elevation`. The most densed and near to mean is `Slope` followed by all 3 features of `Hillshade`. [Take a look at plot #1](#Feature-Visualization) in Feature Visualization section.\n\n- All the features have minimum value of `0` except `Elevation` and `Vertical_Distance_To_Hydrology` features. Where `Elevation` has the highest minimum value and `Vertical_Distance_To_Hydrology` has the lowest, being negative.\n\n- We will document and visualize in detail later for each feature of how spread or dense the data value is between min-25%, 25%-50%, 50%-75% and 75%--max. These are called the percentile. 25% percentile denotes first quaritle, 50% percentile is the median and 75% percentile is the third quartile. [Take a look at plot #1](#Feature-Visualization) in Feature Visualization section.\n\n- `Hillshade`s features have similar maximum value of `254` while `Horizontal_Distance_To_Fire_Points` has the highest followed by `Horizontal_Distance_To_Roadways` feature and they also have the highest ranges of all features. `Slope` having lowest maximum value and also being lowest in range followed by `Apsect` feature. \n\n\nThe reason some features are so widely spread and having high values and some features don't is because 5 out of 10 variables are measured in meters, includes (`'Elevation', 'Horizontal_Distance_To_Hydrology' , Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', Horizontal_Distance_To_Fire_Points'`), so it makes sense that these have high values and ranges. Features like `Aspect` and `Slope` are measured in degrees so its maximum value can't go above `360`. While `Hillshade`s features can take on max value of `255`. All these were discussed before.\n\nTo help understand all these visually, [take a look at plot #1](#Feature-Visualization) and [take a look at plot #4.1](#Feature-Visualization) in Feature Visualization section."},{"metadata":{"_uuid":"89cc6c15f5f35e9908ce5ee91d6ac89084c20ad5"},"cell_type":"markdown","source":"**Taking statistical look at binary or categorical features.**"},{"metadata":{"trusted":true,"_uuid":"490a49eba5e9c159094aef8bdff4e2bdbfefe151"},"cell_type":"code","source":"# statistics of binary or categorical features\nbinary_fea.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abeb8d9f23c0157a5c273ac4fb0e4738e309b17a"},"cell_type":"markdown","source":"- Since here all the values can only be either 0 and 1. The mean can tell us useful information here. `Wilderness_Area1` has the highest mean followed by `Wilderness_Area3`, this means that `Wilderness_Area1` has the most presence in the data compared to other Wilderness Areas. In other words, most observation have features either `Wilderness_Area1` or `Wilderness_Area3`. The least amount of observation will be seen from `Wilderness_Area2`. We will visualize this and `Soil_Type` features later for better understanding these data [Take a look at plot #3](#Feature-Visualization) in Feature Visualization section.\n\n- One more thing to notice here is that when we add all the mean of `Wildernesss_Area`s `0.448864 + 0.051434 + 0.436074 + 0.063627` we get result `0.999999` which is approximately `1`. This actually makes sense because all the observations can be from any one Wilderness area. Crosschecking, I have also programmed to check whether any obsevation has 2 Wilderness Area and Soil type presence at the same time or None for assurance that our data is in appropriate form in [Observation Cleaning](#Observation-Cleaning) section.\n\n- Hence if we look at this in the probability perspective we can say that, the next observation that we get has `44.8%` probability that its been taken from `Wilderness_Area1`, `43.6%` probability that it's taken from `Wilderness_Area3` and so on for others. [Take a look at plot #2](#Feature-Visualization) in Feature Visualization section.\n\n- We can document same for `Soil_Type`s too.  [Take a look at plot #3](#Feature-Visualization) and [plot #4.2](#Feature-Visualization) in Feature Visualization section.\n\n\nBy looking at these statistics of two different data types and since the features have different spreads and uneven amount of distribution, we will feature scale these so that all the feature have similar ranges between 0 and 1. Some algorithm are very sensitive to high values hence giving us inapprpraite results while some algorithms are not. Do be on safe side we will feature scale it and will do this in `Data Engineering` Section."},{"metadata":{"_uuid":"ab9cb6f70cbc38121974afb4a32e29c3ea128a7a"},"cell_type":"markdown","source":"#### Feature Skew"},{"metadata":{"_uuid":"95f67f05a3d5f199e81e060afd545fa358e296c6"},"cell_type":"markdown","source":"**Exploring Skewness of each features:**"},{"metadata":{"trusted":true,"_uuid":"087913f47fc0b265446df53e8f333f8ae731026e"},"cell_type":"code","source":"# skew distribution\ndata.skew()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd615b783900d750be78fdea35c845900b0ac80c"},"cell_type":"markdown","source":"- We can see that `Soil_Type15` has the highest positive skewness meaning the mass of the distribution is concentrated to the left and has long tail to the right followed by `Soil_Type7 , 36, 38`. This is also called `right skewed distribution`. Where mode of the feature is to the left most followed by median and mean. This means that mostly all of the observations have will have 0 value for this feature  [Take a look at plot #3](#Feature-Visualization) in Feature Visualization section\n\n- In general looking at skew scores of `Soil`s it seems like we can reduce our dimensions by removing some `Soil Type`s only if they dont have any different information to give our models and improving its performance. There's a way we can evaluate this as we will see in later sections.\n\n- `Elevation` and `Hillshade`'s having negatively skewed distibution, it's the opposite of the positively skewed distribution, where mode is to the right most followed by meadian and mean.\n\n- ML algothims can be very sensitive to such ranges of data and can give us inappropriate or weak results. Feature Scaling will handle these as discussed earlier."},{"metadata":{"_uuid":"1d8d4bc1a01e0875a9708868ed81f4b39a7dcfe6"},"cell_type":"markdown","source":"#### Class Distribution:\n\nLet's take a look how each class is distributed.."},{"metadata":{"trusted":true,"_uuid":"30506c921bad72b0e978e3be74aeecfbd8b9db21"},"cell_type":"code","source":"# grouping by forest cover type and calculating total occurance\ndata.groupby('Cover_Type').size()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baaa87336525d00deca735eb8da721bad765ad26"},"cell_type":"markdown","source":"- We have uneven samples of forest cover type, where `Lodgepole Pine (2)` has the highest no. of observation followed by `Spruce (1)`. Only these 2 cover types add up to `495,141` number of observations out of `581,011` total which covers approx `85.2%` of data.\n\n- But we do have enoough samples to train the model learning different patterns of each forest cover types. We will see how models performs with these uneven amount of distributions in [Model Evaluation](#Model_Evaluation) section."},{"metadata":{"_uuid":"80c1e8a9a2af88844c337fe8d9dff4bb0c8e5a8f"},"cell_type":"markdown","source":"Enough of looking at bunch of numbers! Lets Visualize them...!"},{"metadata":{"_uuid":"931ba4724cdeac5fcb66fa34b630a338bd5746f5"},"cell_type":"markdown","source":"--------"},{"metadata":{"_uuid":"b20d68e598095b5afd9b031be7c647cdd26857bb"},"cell_type":"markdown","source":"### Feature Visualization"},{"metadata":{"_uuid":"68181539a7f46bd60f6720ea8f4eb42801f85c95"},"cell_type":"markdown","source":"#### Feature Spread"},{"metadata":{"_uuid":"04090a80a828be9bf2ae27b875de8e6c8514f608"},"cell_type":"markdown","source":"Visualizing the spread and outliers of the data of numerical features."},{"metadata":{"trusted":true,"_uuid":"239cdf46968dfcc0362ba078e06704d84fddc5a4"},"cell_type":"code","source":"#####    1    ######\n# Box and whiskers plot\n# Spread of numerical features\n\n# importing pyplot module from matplotlib to plt\nplt = matplotlib.pyplot\n\n# plot bg\nsns.set_style(\"whitegrid\")\n\n#Size of the plot\nplt.subplots(figsize=(21, 14))\n\n# setting color of the plot\ncolor = sns.color_palette('pastel')\n\n# Using seaborn to plot it horizontally with 'color'\nsns.boxplot(data = num_fea, orient='h', palette=color)\n\n# Uncomment below code to visualize where every single data observation of the features lie in the plot \n#sns.swarmplot(data = num_fea)  #WARNING THIS WILL TAKE LOTS OF TIME DEPENDING ON CPU POWER AND RAM YOU HAVE  !!\n\n# Title of the graph\nplt.title('Spread of data in Numerical Features', size = 20)\n\n# Horizontal axis Label\nplt.xlabel('No.of Observation', size = 17)\n# Vertical axis Label\nplt.ylabel('Features', size = 17)\n\n# x-axis label size\nplt.xticks(size = 17)\n#y-axis label size\nplt.yticks(size = 15)\n\n# removing the top and right axes spines, which are not needed\nsns.despine()\n\n# display plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0cc698bd8e2e10bed0677df54e721122d0514c1"},"cell_type":"markdown","source":"- As explained by me in [feature statistics](#Feature-Statistics), `Slope` is the most squeezed box plot feature! It's densely packed taking on least range compared to all features. Having little range means `mean` and `median` will be quite close and we saw that before in the table, it has a difference of approx 1. It does have a few outliers though.\n\n- `Aspect` feature is the only one which do not have any outliers having a range of 360. Since both `Aspect` and `Slope` are measured in degrees, `Aspect` takes on much bigger range than `Slope` because it has lowest max score, hence `Aspect` is much less densed than `Slope`. The `first 50%` of the data, from `min to meadian` is more densed than the `last 50%`, its more spread out.\n\n- `Hillshade`s feature also having similar plot like `Slope` including many outliers and taking on smaller range. Similiar plot is for `Vertical_Distance_To_Hydrology` except here the minimum value is negative as we had seen in the table.\n\n- `Elevation` and `Horizontal_Distance_To_Hydrology` are the only features that doesn't have minimum value of 0. `Elevation` instead is plotted in middle having many outliers too.\n\n- `Horizontal_Distance_To_Roadways` is the most spread data of all features because it has the highest standard deviation score followed by `Horizontal_Distance_To_Fire_Points` though this feature has the maximum value. We can see visually only how spread these are and which one is most. `Horizontal_Distance_To_Fire_Points` may be having largest number of outliers I guess from this plot. If we compare these two features, the last 50% of the data of `Horizontal_Distance_To_Roadways` is much more spread and less dense compared to `Horizontal_Distance_To_Fire_Points` , hence having high standard deviation score."},{"metadata":{"_uuid":"9d507a4411e6e2afe727f5d2d79dcba6dc87517f"},"cell_type":"markdown","source":"#### Feature Distribution"},{"metadata":{"_uuid":"925730e2be99f63f6fd926f6b2e5fd51f5b06ef2"},"cell_type":"markdown","source":"Now, lets plot how `Wilderness_Area`s are distributed. As we saw earlier in [feature statistics](#Feature-Statistics), the mean of `Wilderness_Area1` and `Wilderness_Area3` were highest which meant there presence were high. Now lets make this assumption more concretely by visualizing it. "},{"metadata":{"trusted":true,"_uuid":"ac17c13a158c27e27d598aadabe796f6cf7c48b8"},"cell_type":"code","source":"####    2    #####\n# Bar plot\n# Wilderness Area Count\n\n# Splitting binary_fea data in 2\n# Wild_data will have wilderness data\n# Soil_Data will have Soil data\n\n# Splitting\nWild_data, Soil_data = binary_fea.iloc[:,:4], binary_fea.iloc[:,4:]\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# list of colors\nflatui = [\"#e74c3c\", \"#34495e\", \"#2ecc71\",\"#3498db\"]\n# using seaborn, passing color to palette\ncolor = sns.color_palette(flatui)\n\n# Sum the data, plot bar with given size using color defined\nWild_data.sum().plot(kind='bar', figsize=(10, 8), color='#34a028')\n\n# Title of the graph\nplt.title('No. of observations of Wilderness Areas', size = 20)\n\n# Horizontal axis Label\nplt.xlabel('Wilderness Areas', size = 17)\n# Vertical axis Label\nplt.ylabel('No.of Observation', size = 17)\n\n# x-axis label size, setting label rotations\nplt.xticks(rotation = 'horizontal', size = 14)\n# y-axis label size\nplt.yticks(size = 14)\n\n# removing the top and right axes spines, which are not needed\nsns.despine()\n\n# display plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7206f2b5a0e34a9ab44289bb3a7d4aac3b09c489"},"cell_type":"markdown","source":"And here's the visual proof, `Wilderness_Area1` has the most presence followed by `Wilderness_Area3`, both have quite close observations and so were their mean value. `Wilderness_Area2` having the least observation. Lets see their exact values for precision."},{"metadata":{"trusted":true,"_uuid":"69d88a0d2908b49b9d8370df474962a43de41c94"},"cell_type":"code","source":"# total count of each Wilderness Area\nWild_data.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d25e36fd26fad374001fb8c6699cfe40e8c3238c"},"cell_type":"markdown","source":"- The difference of observations between `Wilderness_Area1` and `Wilderness_Area3` is approximately `7k`. \n\n- `Wilderness_Area2` and `Wilderness_Area4` may seem to have less observation but its not, as they have `~29k` and `~36k` observations respectively. Which is very good amount of observations."},{"metadata":{"_uuid":"3f55103ffa0a61a3a124c872bef86985e31787cc"},"cell_type":"markdown","source":"**Now Let's see similar visualization for `Soil Types`**"},{"metadata":{"trusted":true,"_uuid":"608ab7e79cef0a52aec5394d5eaf6d71c79d844a"},"cell_type":"code","source":"####    3.1    #####\n# Bar plot\n# Soil Type Count\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n\n# Sum the data, plot horizontal bar with given size using color defined\nSoil_data.sum().plot(kind='bar', figsize=(24, 12), color='#a87539')\n\n# Title of the graph\nplt.title('No. of observations of Soil Types', size = 20)\n\n# Horizontal axis Label\nplt.xlabel('Soil Types', size = 17)\n# Vertical axis Label\nplt.ylabel('No.of Observation', size = 17)\n\n# x-axis label size, setting label rotations\nplt.xticks(rotation = 65, size = 15)\n# y-axis label size\nplt.yticks(size = 15)\n\n# removing the top and right axes spines, which are not needed\nsns.despine()\n\n# display plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b682d2a495d39be5ea2f48a9ffadbb372a6da46c"},"cell_type":"code","source":"# Statistical description of Highest observation of Soil Type seen\nSoil_data.loc[:,'Soil_Type29'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a582f35c06bf4644d60e119801d8985765bf71a7"},"cell_type":"markdown","source":"- Looking at the visualization above the first thing I notice that there are visualization of `normal distribution`, `bimodal distribution`, `Unimodal Distribution` and `Left and Right-skewed distribution` showing up in pieces. In short we see all kinds of distributions here!!\n\n- Distribution Observation from Left-Right:\n    - The left-most appears to have mostly Normal distribution (`Soil_Type1` - `Soil_Type6`).\n    - Unimodal Distribution (`Soil_Type7` - `Soil_Type8`)\n    - We see a bimodal distribution (`Soil_Type19` - `Soil_Type14`).\n    - Left Skewed Distribution (`Soil_Type15` - `Soil_Type21`).\n    - Normal Distribution (`Soil_Type22` - `Soil_Type24`).\n    - Right Skewed Distribution (`Soil_Type25` - `Soil_Type28`)\n    - Mixture of Right Skewed and Bimodal (`Soil_Type29` - `Soil_Type33`).\n    - Normal Distribution (`Soil_Type34` - `Soil_Type35`).\n    - Normal Distribution (`Soil_Type36` - `Soil_Type37`).\n    - Right Skewed Distribution (`Soil_Type38` - `Soil_Type40`)\n    \n    \n- The most observation is seen from `Soil_Type29` followed by `Soil_Type23`, `Soil_Type32` and `Soil_Type33`. As from statistical analysis done of `Soil_Type29`, it shows that the mean is `~0.198` which mean it alone has presence in approximately `20%` of observations in our data. It also had the least skewed value of all in `Soil Types` as we had seen earlier in Data Exploration.\n\n**Let's see the exact number of observations of `Soil Types` in descending order.**"},{"metadata":{"trusted":true,"_uuid":"1ee58e397e74be34e0a73217d97136b7b748d7d2"},"cell_type":"code","source":"####    3.2    #####\n# Horizontal Bar plot\n# Soil Type Count\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# sum Soil data values, and pass it as a series \nsoil_sum = pd.Series(Soil_data.sum())\n\n# will sort values in descending order\nsoil_sum.sort_values(ascending = False, inplace = True)\n\n# plot horizontal bar with given size using color defined\nsoil_sum.plot(kind='barh', figsize=(23, 17), color= '#a87539')\n\n# horizontal bar flips columns in ascending order, this will filp it back in descending order\nplt.gca().invert_yaxis()\n\n# Title of the graph\nplt.title('No. of observations of Soil Types', size = 20)\n\n# Horizontal axis Label\nplt.xlabel('No.of Observation', size = 17)\n# Vertical axis Label\nplt.ylabel('Soil Types', size = 17)\n\n# x-axis label size, setting label rotations\nplt.xticks(rotation = 'horizontal', size = 15)\n# y-axis label size\nplt.yticks(size = 16)\n\n# removing the top and right axes spines, which are not needed\nsns.despine()\n\n# display plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12c88014cf7b31af98e13ac8a7b20e0b4370e36b"},"cell_type":"code","source":"# Exact counts of observations of Soil Type\nsoil_sum","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a3ce52221c831db3ec501dbe67ceabdd782ab54"},"cell_type":"markdown","source":"- The least observation are of `Soil_Type15` of `3`. Thats why it had most skewed value of all `Soil Types` of `~440` which now makes sense since this feature has value of `0` for all except for `3` observations hence making it densly concentrated towards `0` and long flat tail to the right having a form of `positively skewed distribution` or `Right Skewed Distribution`. We saw this in [Feature\nSkew](#Feature-Skew) section.\n- `Soil_Type29` has the highest, `115,246` observations."},{"metadata":{"_uuid":"f2b8b3f883761d245a9b85cf31b5dbc75c3de329"},"cell_type":"markdown","source":"-------"},{"metadata":{"_uuid":"c6f7ea204a3d325f17679394094f6f7f522ae44b"},"cell_type":"markdown","source":"#### Feature Comparison"},{"metadata":{"_uuid":"c0db91c48d55fd77cd40a8183f703958147b9f1a"},"cell_type":"markdown","source":"**Next let's compare each feature in our data to our target variable, visualizing how much dense and distributed each target variable's class is compared to the feature. We will use [Violin Plot](https://datavizcatalogue.com/methods/violin_plot.html) to visualize this, a combination of Box Plot and Density Plot (Histogram).**"},{"metadata":{"trusted":true,"_uuid":"051592bc069648b4dcdc2cac41ef56d476907b29"},"cell_type":"code","source":"#######    4.1    ########\n# Violin Plot (Box + Density)\n# Comparing numerical features with target variable\n\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# setting target variable\ntarget = data['Cover_Type']\n\n# features to be compared with target variable\nfeatures = num_fea.columns\n\n\n# loop for plotting Violin Plot for each features in the data\nfor i in range(0, len(features)):\n    \n    #figure size\n    plt.subplots(figsize=(16, 11))\n    \n    # Plot violin for i feature for every class in target \n    sns.violinplot(data=num_fea, x=target, y = features[i])\n    \n    # x-axis label size\n    plt.xticks(size = 15)\n    # y-axis label size\n    plt.yticks(size = 16)\n\n    # Horizontal axis Label\n    plt.xlabel('Forest Cover Types', size = 17)\n    # Vertical axis Label\n    plt.ylabel(features[i], size = 17)\n  \n    # display plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a484619e2b85dcbb7271a8b271b0ce3195b710f"},"cell_type":"markdown","source":"We can say so much by looking at these plots of each features. Their medians, interquartile range, skewness, density etc. I will only brief those that are intresting and have valueable information to tell us:\n\n- `Elevation` takes on different range of values for different forest classes. Most of the forest at the elevation between `2000m - 25000m` are of `class 4 forest type` while `class 3 forest type` has fewer presence of such `elevation`. `Class 7 forest type` have the observations of most elevated trees ranging from as low as `~2800m` to as high as `~3800m`. The `'max'` value in `elevation` belongs to `class 7 forest type`. This is the most important feature since every feature tells different story to different classes of forest cover type hence an important feature for our algorithm.\n\n- `Aspect` is the feature that has normal distribution for each class.\n\n- `Slope` feature takes on lower values compared to most features as its measured in degrees and least to `Aspect` which is also measured in degrees. It has the `least maximum` value of all features and by looking the plot above we can say that it belongs to `Forest Cover Type 2`. All classes have dense slope observations between `0-20 degrees`.\n\n- `Horizontal distance to hydrology` has the `right or positively skewed distribution` where most of the values for all classes are towards `0-50m`.\n\n- `Vertical distance to hydrology` is also `positively skewed distribution` but this takes on values much closer to `0` for all classes for most observations.The `highest value` in this feature belongs to `Forest cover type 2`. And this feature also has the `least minimum value` of all features and that also belongs to `class 2 forest type`, hence `class 2` having most range of data observations compared to all other classes.\n\n- `Hillshade_9am` and `Hillshade_Noon` are `left or negatively skewed distributions` where they take on max value between `200-250 index value` for most observations in each class. While `Hillshade_3pm` has `normal distribution` for all classes.\n\n\n**Now lets see similar visualize for `Wilderness Areas`**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"38c65d90498abcf8033d9644821f7f2baf3c6acf"},"cell_type":"code","source":"#######    4.2    ########\n# Violin Plot (Box + Density)\n# Comparing Wilderness features with target variable\n\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# setting target variable\ntarget = data['Cover_Type']\n# features to be compared with target variable\nfeatures = Wild_data.columns\n\n\n# loop for plotting Violin Plot for each features in the data\nfor i in range(0, len(features)):\n    \n    #figure size\n    plt.subplots(figsize=(13, 9))\n    \n    # Plot violin for i feature for every class in target\n    sns.violinplot(data = Wild_data, x=target, y = features[i])\n    \n    # x-axis label size\n    plt.xticks(size = 15)\n    # y-axis label size\n    plt.yticks(size = 16)\n\n    # Horizontal axis Label\n    plt.xlabel('Forest Cover Types', size = 17)\n    # Vertical axis Label\n    plt.ylabel(features[i], size = 17)\n\n    # display plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45a8c774942fc19257fab952877ad5f05e6c4544"},"cell_type":"markdown","source":"The reason these plots look so different than before is because these features take on value ranging between `0 and 1`. \n\n- `Wilderness Area 1` belong to `forest cover type 1, 2, 5 and 7` while `wilderness area 3` shows presence in `all classes` except `Forest Cover Type 4`.\n\n- `Wilderness Area 2 and 4` have less observations, their dense is less on `1` on all classes compared to other two `Wilderness Areas 1 and 3`."},{"metadata":{"_uuid":"fb8d4a01c7d9b9f6bb7584c3f9329ba372cda845"},"cell_type":"markdown","source":"\n\n**Now lets visualize `Soil Type`...**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"42a8e5bbdffde0c834179615ae59642364a25b94"},"cell_type":"code","source":"#######    4.3    ########\n# Violin Plot (Box + Density)\n# Comparing Soil features with target variable\n\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# setting target variable\ntarget = data['Cover_Type']\n# features to be compared with target variable\nfeatures = Soil_data.columns\n\n\n# loop for plotting Violin Plot for each features in the data\nfor i in range(0, len(features)):\n    \n    #figure size\n    plt.subplots(figsize=(13, 9))\n    \n    # Plot violin for i feature for every class in target    \n    sns.violinplot(data=Soil_data, x=target, y = features[i])\n    \n    # x-axis label size\n    plt.xticks(size = 15)\n    # y-axis label size\n    plt.yticks(size = 16)\n\n    # Horizontal axis Label\n    plt.xlabel('Forest Cover Types', size = 17)\n    # Vertical axis Label\n    plt.ylabel(features[i], size = 17)\n  \n    # display plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e67c0a7b67e96ca6046195f9202d26eefa378fc"},"cell_type":"markdown","source":"- `Soil Type 4` is the only Soil Type that has presence in `all classes` of `forest cover types`.\n\n- `Soil Type 7, 15 and 37` belong to `forest class 2, 6 and 7` respectively. They also happen to have fewest observations in all Soil Types as seen [here](#Feature-Visualization). Having observations which has presence of either `Soil Type 7, 15 and 37` has most likely chance of being present in `forest class type 2, 6 and 7` respectively. I think this is an important feature though they have less observations but they do give us a valuable information here. But this could be opposite too, yes they give valueable information to those classes but other features might be giving more info to those class too. Its a debate we can do but only models can tell us which feature has more impact in predicting.\n\n- `Forest Cover Type 4` seems to have less presence compared to all classes for `Soil Types` having least observation too."},{"metadata":{"_uuid":"bc5b126d7b83edb4eda1218ef284b7379ae4d9ea"},"cell_type":"markdown","source":"#### Feature Correlation"},{"metadata":{"_uuid":"bdaa6faf3d61801f78a5ce742df487e0d06cd79a"},"cell_type":"markdown","source":"**Now Let us see how much each features are correlated with each other...**\n\nSince part of our data is binary, we will exclude binary data from our dataset and only find correlation matrix of numerical data becuase correlation requires continous data."},{"metadata":{"trusted":true,"_uuid":"9425847f90161867b5e2664bd44c449ab6df41eb"},"cell_type":"code","source":"######    5    #######\n# Correlation Plot\n# Correlation of each feature\n\n# fig size\nplt.subplots(figsize=(15, 10))\n\n# Compute the correlation matrix\nnum_fea_corr = num_fea.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(num_fea_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Generates heatmap masking the upper triangle and shrinking the cbar\nsns.heatmap(num_fea_corr, mask=mask, center=0, square=True, annot=True, annot_kws={\"size\": 15}, cbar_kws={\"shrink\": .8})\n\n# x-axis label size\nplt.xticks(size = 13)\n# y-axis label size\nplt.yticks(size = 13)\n\n# display plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5bc025290f7258833e73d1a4ea1f73700761dc3"},"cell_type":"markdown","source":"- Features that have less or no correlation are colored `black` while features with positive correlation are colored `orange` and `blue` for negative correlation.\n\n- As we can see from the above figure, correlation values of the features are given in their respective boxes. \n\n- `Hillshade_3pm and Hillshade_9am` show highly `negative correlation` while `hillshade_3pm and Aspect` show highest positive correlation.\n\n- `Hillshade_3pm and Aspect` also had almost normal distribution compared to forest cover types classes. ([Plot 4.1](#Feature-Visualization))\n\n- Other features which have correlations are `Vertical and Horizonal Distance to Hydrology`, `Hillshade_3m and Hillshade_Noon`, `Hillshade_9am and Aspect` and `Hillshade_Noon and Slope`. So in total we have `6` pairs of correlation.\n\n- Less Correlated value tell us that the features have different valueable information to tell us and model, hence important features for predictions."},{"metadata":{"_uuid":"66f82cd2cc5c10c85d33071e730fd4e911ac65c4"},"cell_type":"markdown","source":"**Plotting scatter plots of all features that have correlation greater than 0.5 with each other.**"},{"metadata":{"trusted":true,"_uuid":"7118e30947c801f650fe70134fb3bc6aa1902d80"},"cell_type":"code","source":"#####    6    #####\n# Scatter Plots\n# Correlation that have greater than 0.5\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# giving list of lists\n# inner lists conatains pairs of feature which have high correlation\nlist_data_cor = [['Aspect','Hillshade_3pm'], ['Aspect', 'Hillshade_9am'], ['Slope', 'Hillshade_Noon'], ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology'], \n                 ['Hillshade_3pm', 'Hillshade_9am'], ['Hillshade_3pm', 'Hillshade_Noon']]\n\n\n# Looping through outer list\n# taking 2 features from inner list\nfor i,j in list_data_cor:\n    \n    # fig size\n    plt.subplots(figsize=(15, 12))\n    \n    #plot 1 feature on x axis and other on y axis, each point shows which cover forest they belong to\n    sns.scatterplot(data = data, x = i, y = j, hue=\"Cover_Type\", legend = 'full', palette='rainbow_r')\n\n    # x-axis label size\n    plt.xticks(size = 15)\n    # y-axis label size\n    plt.yticks(size = 15)\n\n    # Horizontal axis Label\n    plt.xlabel(i, size = 17)\n    # Vertical axis Label\n    plt.ylabel(j, size = 17)\n  \n    # display plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa18f82603475d2d131ce8324b28332d70cb8964"},"cell_type":"markdown","source":"Wooaahh..! Those patterns..!\n\n- `Hillshade_3pm and Aspect` represent relationship of a `sigmoid function`. The data points at the boundaries of the figure mostly belong to `forest cover type class 1` while `class 3` takes on most of datapoints in the figure followed by `forest cover type class 6`. The datapoints when `Hillshade_3pm` is `0` belongs to `class 1,2,3 or 7` regardless of what `Aspect` values it has.\n\n- The figure `Hillshade_9am and Aspect` also represent relationship of a `sigmoid function` just its flipped over the y-axis. `Class type 3` has the highest observation here followed by the `class type 1 and 6`.\n\n- `Hillshade_Noon and Slope` have a horizontal `'V' shaped` representation. Lower degrees represent `class 4 and 6` while high degree values represent `class 1, 2 and 7` also we can see decrease in `Hillshade_Noon` value as slope increases and it geographically makes sense.\n\n- `Vertical and Horizontal Distance to Hydrology` represent a `linear` but spreaded out type, not a single line fit to all datapoints. `Class type 7 and 2` have more observation here and spreaded out while `class type 3 and 6` are densely packed between the range `0-800m` of `Horizontal Distance to Hydrology`.\n\n- ` Hilshade_9am and Hillshade_3pm` figure represents relationship of a sliced-out part of a circle where top most of the datapoints belong to `class 3` and middle and bottom area belong to rest of the classes.\n\n- `Hillshade_Noon and Hillshade_3pm` have similar observation as described before just a difference here is that it's flipped over y-aixs. We also see similar patterns of datapoints too as before."},{"metadata":{"_uuid":"e8a4ec84d23115dc32e723a666e71c382368954e"},"cell_type":"markdown","source":"**Now lets move on to the next part of the project, Data Engimeering** where I am going to :\n\n- Delete feature which has `'0'` value for all observation.\n- Delete observation which has null values in any of its features.\n- Deleting duplicate entries but keeping first.\n- Take a look at if any observations is present in more than one type in same category of Wilderness and Soil Type.\n- Reducing features by keeping best.\n- Scaling values in specific range.\n- Peform Train-Test Split."},{"metadata":{"_uuid":"c9be6929855f1e8b548708f06e9427d16b78f5b6"},"cell_type":"markdown","source":"-------\n-------"},{"metadata":{"_uuid":"730edf25f52367d16103d54d24168aa39087fe94"},"cell_type":"markdown","source":"## Data Engineering"},{"metadata":{"_uuid":"7aaddde3b0b9ff12a39811ad0c6c36b7daead6e7"},"cell_type":"markdown","source":"### Observation Cleaning"},{"metadata":{"_uuid":"2a09337f710e935e330e15353ca252a6c94b2607"},"cell_type":"markdown","source":"There's a possibility where we can have an observation where `Soil Type` and `Wilderness Area` are recorded as present for more than one type or maybe none.\n\nBelow code will show us if we have any..\n\n**Checking for Wilderness Area.**"},{"metadata":{"trusted":true,"_uuid":"785d29d74e4ea67985a79ef27048fb1100e772c9"},"cell_type":"code","source":"# Checking if any observation have more than 1 presence of Wilderness area at same time or None\n\n# Count for more than 1 presence\nmore_count = 0\n# Count for none presence\nnone_count = 0\n# total count\ntotal = 0\n\n#looping through each row of wilderness area column\nfor index, row in Wild_data.iterrows():\n    # adding the values of each column of that row\n    total = row.sum(axis=0)\n    \n    #checking greater than 1\n    if total > 1:\n        # if found, increment count by 1\n        more_count =+ 1\n        # reset the total\n        total = 0\n        # do not execute code below, start from top\n        break\n        \n    #checking for none   \n    if total == 0:\n        # if found, increment count by 1\n        none_count =+ 1\n        # reset the total\n        total = 0      \n\n# priting results found\nprint('We have ', more_count, ' observations that shows presence in more than 1 Wilderness Area.')\nprint('We have ' ,none_count, ' observations that shows no presence in any Wilderness Area.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"976d1ce99cb8477eb9259730453cc60ac0ce0c6b"},"cell_type":"markdown","source":"**Checking for Soil Type.**"},{"metadata":{"trusted":true,"_uuid":"34e7b5f26369d47f3312d4c0971c84235fff984d"},"cell_type":"code","source":"# Checking if any observation have more than 1 presence of Soil Type area at same time or None\n\n# Count for more than 1 presence\nmore_count = 0\n# Count for none presence\nnone_count = 0\n# total count\ntotal = 0\n\n#looping through each row of Soil Type area column\nfor index, row in Soil_data.iterrows():\n    # adding the values of each column of that row\n    total = row.sum(axis=0)\n    \n    #checking greater than 1\n    if total > 1:\n        # if found, increment count by 1\n        more_count =+ 1\n        # reset the total\n        total = 0\n        # do not execute code below, start from top\n        break\n        \n    #checking for none   \n    if total == 0:\n        # if found, increment count by 1\n        none_count =+ 1\n        # reset the total\n        total = 0      \n\n# priting results found\nprint('We have ', more_count, ' observations that shows presence in more than 1 Soil Type Area.')\nprint('We have ' ,none_count, ' observations that shows no presence in any Soil Type Area.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bb87199b517b986e4dd37c59b576e2a8d4821f1"},"cell_type":"markdown","source":"So we have **None** to worry about. Thanks to the authors of the data. \n\nAnother way to approach this problem would be to simply add total counts of each types in Wilderness Area and Soil Type Categories and check if each category is equal to the number of observation in the data we have..."},{"metadata":{"_uuid":"c04d46cd88d96c2a6f815940f7204f12df7459cc"},"cell_type":"markdown","source":"#### Handling Missing Values"},{"metadata":{"_uuid":"30a770d4d5b9fe430bf0949d113be90be5636d97"},"cell_type":"markdown","source":"**Removing Observation which has any Missing Values in it....**"},{"metadata":{"trusted":true,"_uuid":"88174ead1644522488204d144643f952b5fd3dbd"},"cell_type":"code","source":"# will delete observation if it has any missing values in any of the features.\ndata.dropna()\n\n# shape of the data after deleting missing entries\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ea6860bb77b8df4db572c52ffac030ecb65787e"},"cell_type":"markdown","source":"NO Missing Values...!! That's great!"},{"metadata":{"_uuid":"5c69b208625b39192738071e4c1426e523eab17d"},"cell_type":"markdown","source":"#### Handling Duplicates"},{"metadata":{"trusted":true,"_uuid":"5466f55a5c9765266154bf1e00403a9cd69a2450"},"cell_type":"code","source":"# deleting duplicates, except the first observation\ndata.drop_duplicates(keep='first')\n\n# shape of the data after deleting duplicate entries\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"790e0905694e90285817d5b14c5c4480e2183b1f"},"cell_type":"markdown","source":"NO Duplicates too..! Neat!"},{"metadata":{"_uuid":"9fd77faa755997a01ff0b3ca1ecbe5c0edb2bbe3"},"cell_type":"markdown","source":"--------"},{"metadata":{"_uuid":"fd84c8866cafdec1beecc653ced15698ff6f1258"},"cell_type":"markdown","source":"### Dimentionality Reduction\n\n- Since we already have lots of observation now to train the model, we also happen to have lots of features. This will make algorithm run very slowly, have difficulty in learning and also tend to overfit in training set and do worse in testing.\n\n- We also see above in visualization section that `Wilderness Area` and `Soil Type` Area have no category that has no observations of it. So every feature has presence or values of an observations so we can't just delete any feature since it may have an important informations for our models in predicting classes.\n\n- To approach such a problem, we need to see how each feature has an impact on prediciting classes, and the best way to do this is by asking the models only.\n\n- Classifiers like `Extra Trees, Random Forest, Gradient Boosting Classifiers and AdaBoost` offer an attribute called `'feature_importance_'` with which we can see that which feature has more importance compared to others and by how much.\n\nSo now let's run all the 4 classifiers on our entire model, train from it and give us which feature for that was important in terms of predicting classes."},{"metadata":{"_uuid":"21eefe45e2af30a9d8c174084929353726c26aa5"},"cell_type":"markdown","source":"#### Extra-Trees Classifier"},{"metadata":{"trusted":true,"_uuid":"8cf138dc99bc38539cae5bffb42bb7b8dab6d57c"},"cell_type":"code","source":"# importing model for feature importance\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# passing the model\nmodel = ExtraTreesClassifier(random_state = 53)\n\n# feeding all our features to var 'X'\nX = data.iloc[:,:-1]\n# feeding our target variable to var 'y'\ny = data['Cover_Type']\n\n# training the model\nmodel.fit(X, y)\n\n# extracting feature importance from model and making a dataframe of it in descending order\nETC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['ETC']).sort_values('ETC', ascending=False)\n\n# removing traces of this model\nmodel = None\n\n# show top 10 features\nETC_feature_importances.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff9a378543641ec06d804de0da6d599766e5d5a"},"cell_type":"markdown","source":"#### Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"b3332d562ecd0c325bfe6b4395c2764485a5f189"},"cell_type":"code","source":"# importing model for feature importance\nfrom sklearn.ensemble import RandomForestClassifier\n\n# passing the model\nmodel = RandomForestClassifier(random_state = 53)\n\n# training the model\nmodel.fit(X, y)\n\n# extracting feature importance from model and making a dataframe of it in descending order\nRFC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['RFC']).sort_values('RFC', ascending=False)\n\n# removing traces of this model\nmodel = None\n\n# show top 10 features\nRFC_feature_importances.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bc1dbf0c99fb53de6b938dad13ad2d809ce1145"},"cell_type":"markdown","source":"#### AdaBoost Classifier"},{"metadata":{"trusted":true,"_uuid":"7390642bfbdd6e08bc07732a6921b252de6f7e42"},"cell_type":"code","source":"# importing model for feature importance\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# passing the model\nmodel = AdaBoostClassifier(random_state = 53)\n\nmodel.fit(X, y)\n\n# extracting feature importance from model and making a dataframe of it in descending order\nADB_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['ADB']).sort_values('ADB', ascending=False)\n\n# removing traces of this model\nmodel = None\n\nADB_feature_importances.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdddf448e34a5a9b4599f24c28f8cb3349599c2d"},"cell_type":"markdown","source":"#### Gradient Boosting Classifier"},{"metadata":{"trusted":true,"_uuid":"3d174f89d949cbb850928843e8dd155a6e318650"},"cell_type":"code","source":"# importing model for feature importance\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# passing the model\nmodel = GradientBoostingClassifier(random_state = 53)\n\n# training the model\nmodel.fit(X, y)\n\n# extracting feature importance from model and making a dataframe of it in descending order\nGBC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['GBC']).sort_values('GBC', ascending=False)\n\n# removing traces of this model\nmodel = None\n\n# show top 10 features\nGBC_feature_importances.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3955681321daa39e6311c4494b653a35b3ea072b"},"cell_type":"markdown","source":"There they are, each classifier giving its top choice of features.\n\n- We can see that `RFC` and `ETC` show similar results, yes there are features which show-up different ranks but not of a great difference. Each feature show a little similar numbers.\n\n- `GBC` also happens to show similar results but little different that those `RFC` and `ETC` classifier's results.\n\n- `ADBC`, show a unique and very interesting results. The top `8` features are alone enough to predict classes and highest taken is by `Wilderness Area4` followed by `Elevation` which was being followed in other classifiers!! This is interesting because `Wilderness Area4` isn't even present in the top `10` except of `RFC` which had showed about `4.38%` importance which is very different than `ADBC`'s result score of `44&`\n\n- `Elevation` do take on similar dominance in predicting class, being around `22-24%` for every classifier except for `GBC', a whooping ~66% dominance is shown for it!!\n\n- `Hillshade` features are seen on top 10 list of every classifier except for `ADB`. `ETC` and `RFC` show all `Hillshade` features having similar dominance while `GBC` shows a percent less.\n\n- In above Visualization section of [Correlation](#Feature-Correlation), we saw that `Hillshade` features had nice correlation with each other also other features like `SLope`, `Aspect`, `Horizontal and Vertical Distance to Hydrology` showed high correlations values. They also show dominance here in predicting, meaning they might had correlated but they have very useful information in predicting target variable.\n\n- `Elevation`, `Vertical and Horizontal distance to Hydrology` show presence in top 10 for all classifiers, hence important features.\n\n- `Horizontal Distance to Roadway`s and `Fire Points` had highest standard deviation score including outliers, making up in the list, it might be that different ranges of each feature represent different class types.\n\n- `Aspect`, `Slope` and` Hillshade` s features had least standard deviation and `slope` and `hillshade` s taking on least range of values and also making top in the list except `Slope` and `Aspect` dont show up in top 10 in `GBC`.\n\n- All these classification tell us one thing in common, Numerical Features dominate when it comes to predicting forest classes.\n\n- All that being said, I will now go with features that show up in the top in most classifiers. Top `15-20` would be a reasonable choice."},{"metadata":{"_uuid":"0bb0dee23e0b92e712a93f75aa2b6b8913313f25"},"cell_type":"markdown","source":"**Comparing the top 24 features evaluated by `Random Forest` and `Extra Tree Classifier` side by side**\n\n| Features by RF | Features by ETC |\n| --------- |  --------- | \n| Elevation |  Elevation | \n| Horizontal_Distance_To_Roadways |  Horizontal_Distance_To_Roadways | \n| Horizontal_Distance_To_Fire_Points | Horizontal_Distance_To_Fire_Points | \n| Horizontal_Distance_To_Hydrology |  Horizontal_Distance_To_Hydrology | \n| Vertical_Distance_To_Hydrology | Vertical_Distance_To_Hydrology |\n| Aspect | Aspect |\n| Wilderness_Area4 |  Hillshade_Noon | \n| Hillshade_Noon | Hillshade_3pm | \n| Hillshade_3pm |  Hillshade_9am | \n| Hillshade_9am |  Slope | \n| Slope |  Wilderness_Area4 | \n| Soil_Type22 |  Soil_Type22 | \n| Soil_Type10 |  Soil_Type10 | \n| Soil_Type38 |  Soil_Type4 | \n| Soil_Type4 |  Soil_Type23 | \n| Soil_Type39 |  Soil_Type38 | \n| Soi_Type12 |  Wilderness_Area3 |\n| Wilderness_Area3 |  Soil_Type39 |\n| Soil_Type_23 |  Wilderness_Area1 | \n| Wilderness_Area1 |  Soil_Type12 | \n| Soil_Type2 |  Soil_Type2 | 0\n| Soil_Type40 |  Soil_Type40 |\n| Soil_Type32 |  Wilderness_Area2 | \n| Wilderness_Area2 |  Soil_Type13 | "},{"metadata":{"_uuid":"f7deac036ea9b111af2b9e7d5eab0700e9335293"},"cell_type":"markdown","source":"Out of these top 24 features, I would take like to go with **Top 20**. There's no reasonable justification to why I chose this but my intuition say that these will do the job. I could have taken results of AdaBoost having just 8 features but I think that would be an underestimation after what we see in visualizations and other 3 classification results. "},{"metadata":{"trusted":true,"_uuid":"7679b7412edf89f36359fca0158af405401e2f2e"},"cell_type":"code","source":"## feeding top 20 features in a variable as dataframe including target variable\n\n## AdaBoost Sample\n#sample = data[['Wilderness_Area4', 'Elevation','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4', 'Soil_Type4', 'Soil_Type10' 'Cover_Type']]\n\nsample = data[['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4',\n            'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope','Soil_Type22','Soil_Type10','Soil_Type4','Soil_Type34','Soil_Type34','Wilderness_Area3','Soil_Type12',\n            'Soil_Type2','Wilderness_Area1', 'Cover_Type']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0aee1384df53c3d74b1e5189f2f5e18c28d9621e"},"cell_type":"markdown","source":"Finding Useful features and having reasonable dimentions we are ready to move on to our nest step of feature scaling."},{"metadata":{"_uuid":"31d2fe259e529e82dec6001d1e416bcf6bf69d48"},"cell_type":"markdown","source":"---------"},{"metadata":{"_uuid":"c7e7b3817d01b5a824fa4ab53534cebc2ebb54bd"},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"_uuid":"a3e76292409a7be4622bc1a89d3e28854f60f232"},"cell_type":"markdown","source":"One last step before we move to splitting our data to Train-Test Split is to scale the features to some specific range. This is called Feature Scaling. We will scale all feature values to specific range of `0 to 1`. but before we do this we will split the feature and target variables because we dont want to scale our target variable.."},{"metadata":{"trusted":true,"_uuid":"94dd620ef1c0d2277b84e0435600a1239ee5c7f9"},"cell_type":"code","source":"# importing feature scaling function\nfrom sklearn.preprocessing import MinMaxScaler\n\n# passing range to the function and then save it\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# feeding sample features to var 'X'\nX = sample.iloc[:,:-1]\n\n# feeding our target variable to var 'y'\ny = sample['Cover_Type']\n\n# apply feature scaling to all features\nX_scaled = scaler.fit_transform(X)\n#s_sample_2 = scaler.fit_transform(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c3647be4c4022286a645d02254f383bb02d119"},"cell_type":"code","source":"# our data after feature scaling\nX_scaled","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70877295e65baa6a8d90cd4ec842a2d633ec5232"},"cell_type":"markdown","source":"-------"},{"metadata":{"_uuid":"039edf6631a5074438ad0dc7b8fe6185807f47bc"},"cell_type":"markdown","source":"### Train-Test Split"},{"metadata":{"_uuid":"1c70a3bd61a1b74b40c1e1cdcc10b68c393cd534"},"cell_type":"markdown","source":"Now our data is ready to be splitted into **75%-25% train-test set respectively**."},{"metadata":{"trusted":true,"_uuid":"f9eb1be29e956047dfd9bf2fb0c496c46e800575"},"cell_type":"code","source":"# importing train-test function\nfrom sklearn.model_selection import train_test_split\n\n# split the data in 75%-25% train-test respectively with fixed state\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 53)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf789da5886f1cc03bafb53c160667c27b5676a5"},"cell_type":"code","source":"# number of training observation\nprint(X_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4372f76d330400cbe980fc3af200f66695b32b98"},"cell_type":"markdown","source":"Lots of data to train and test on."},{"metadata":{"_uuid":"5be6aa1d04fd73277e0b2d5bd7bc1512aff187ee"},"cell_type":"markdown","source":"-------"},{"metadata":{"_uuid":"1b35ae63947a1c6806f51ddf3a777eb1c7a9eb9d"},"cell_type":"markdown","source":"-----------"},{"metadata":{"_uuid":"0386ec0d4bf3ae002a9bc59c5a0549b356986f5f"},"cell_type":"markdown","source":"## Model Evaluations\n"},{"metadata":{"_uuid":"e2327ec9315d1ead247185a6c3f816d482f465bd"},"cell_type":"markdown","source":"Now its time to feed our data to the models to see how each models performs using 2 different `evaluation metrics` **`accuracy`** and **`f1 score`** and see which model performs the best. \n\nBut before that, we will train our data on training set and test the performance of the Benchmark model we discussed about in the start of the project. I will use 10 K-Fold CV to test the performance of our model. I had choosen Naive Bayes Classifier as my benchmark model and I am going to use **`Multimonial Naive Bayes classifier`** since we have a claasification problem to solve.\n\nThe `Evaluation Metric` I am going to use are `f1 score` and `accuracy` to see how well our model performs.\n\n- `Accuracy` is the measure of the correct predicted data divided by total number of observations hence giving a value ranging between `0 and 1`, while `0` is no correctly predicted class whereas `1` is all correctly predicted class. We can multiply the result by `100` to get the accuracy score in terms of percent.\n\n- `F1 score` is more useful than accuracy specially in the case where you have uneven amount of class distribution as in our case. It's the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. \n\n-  `Accuracy` works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, its better to look at both Precision and Recall or `F1 score`. \n\nFirst I will define a function which will train the `models` using training data and calculate model's performance using `accuracy` and `f1 score`. One sets of instruction for all `models`!"},{"metadata":{"trusted":true,"_uuid":"a9fc71b79e1f9e929e829b5de3307f3b71da25eb"},"cell_type":"code","source":"### defining function for training models and measuring performance \n\n# to measure performance\nfrom sklearn.model_selection import cross_val_score\n\n# for calculating time elapsed\nimport time\n\n# fucntion\ndef model_evaluation(clf):\n    \n    # passing classifier to a variable\n    clf = clf\n    \n    # records time\n    t_start = time.time()\n    # classifier learning the model\n    clf = clf.fit(X_train, y_train)\n    # records time\n    t_end = time.time()\n    \n    \n    # records time\n    c_start = time.time()     \n    # Using 10 K-Fold CV on data, gives peroformance measures\n    accuracy  = cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy')\n    f1_score = cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'f1_macro')\n    # records the time\n    c_end = time.time()    \n    \n    \n    # calculating mean of all 10 observation's accuracy and f1, taking percent and rounding to two decimal places\n    acc_mean = np.round(accuracy.mean() * 100, 2)\n    f1_mean = np.round(f1_score.mean() * 100, 2)\n    \n    \n    # substracts end time with start to give actual time taken in seconds\n    # divides by 60 to convert in minutes and rounds the answer to three decimal places\n    # time in training\n    t_time = np.round((t_end - t_start) / 60, 3)\n    # time for evaluating scores\n    c_time = np.round((c_end - c_start) / 60, 3)\n    \n    \n    # Removing traces of classifier\n    clf = None\n    \n    \n    # returns performance measure and time of the classifier \n    print(\"The accuracy score of this classifier on our training set is\", acc_mean,\"% and f1 score is\", f1_mean,\"% taking\", t_time,\"minutes to train and\", c_time,\n          \"minutes to evaluate cross validation and metric scores.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183741b7b793758880b2e94bfe3159145a6763bf"},"cell_type":"markdown","source":"------"},{"metadata":{"_uuid":"ba532a1c45a727a91339e5248954c2a75f4fa7db"},"cell_type":"markdown","source":"### Benchmark Model"},{"metadata":{"_uuid":"266f5bb038635e49bb6622b4465c8d401fdcb763"},"cell_type":"markdown","source":"Now lets see the performance of `MultinomialNB classifier` on given training data."},{"metadata":{"trusted":true,"_uuid":"63089d78950b2444869be0342060382125d72190"},"cell_type":"code","source":"# importing Multinomial classifier, one of the Naive Bayes classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# passing the model to function to get performance measures\nmodel_evaluation(MultinomialNB())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0945fc84cffcade8eddbffedad94bea9094da224"},"cell_type":"markdown","source":"It performed quite well, atleast what I had expected in terms of accuracy but it performs poorly in `precision` and `recall` and that's what `f1 score` evaluates for us. \n\nLet's now move on to measure performance on the models that I have chose for this problem, they are:\n\n    1. K-Nearest Neighbour (KNN) \n    2. Random Forest (RF)\n    3. Stochastic Gradient Descent Classifier (SGDC)\n    4. Extraa Trees Classifier (ETC)\n    5. Logistic Regression (LR)\n\n**I have evaluated below models without tuning any parameters and using just default set by sklearn except for `n_job`s and `random_state` where `n_jobs` is set to `-1` to use all cpu's and `ranodom_state` is set to `53` for getting even results across runs. I have chosen default parameters just to see how each models perform on this data with its default parameters (good for newbies) and if results aren't accurate as expected then I might change it. But for now I will stick to default for training.**"},{"metadata":{"_uuid":"de5b540b37146f1f7e226e05fd61ad12982099d0"},"cell_type":"markdown","source":"### 1. K-Nearest Neighbors"},{"metadata":{"trusted":true,"_uuid":"f2ef59089d1aaf933edf1f2fe6255db17d8a45b7"},"cell_type":"code","source":"# importing K-Nearest Neighbors Classifier function\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel_evaluation(KNeighborsClassifier(n_jobs=-1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f620b54ae5fb4d0d92ecc254a5be0357cfa877"},"cell_type":"markdown","source":"### 2. Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"e68d9b34df3cbdb3c4aa67b97b080748a84b75cb"},"cell_type":"code","source":"# importing Random Forest function\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_evaluation(RandomForestClassifier(n_jobs=-1, random_state = 53))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfcabd9e0abe1e1dad5e1f31039b4cdc2d48dcad"},"cell_type":"markdown","source":"### 3. Stochastic Gradient Descent Classifier"},{"metadata":{"trusted":true,"_uuid":"b22307e70235bf309645c86c0c1c8686a0841d10"},"cell_type":"code","source":"# importing Stochastic Gradient Descent Classifier function\nfrom sklearn.linear_model import SGDClassifier\n\nmodel_evaluation(SGDClassifier(n_jobs=-1, random_state = 53))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"964c4009365aeff4d83840516e5faa875dffb96f"},"cell_type":"markdown","source":"### 4. Extra Trees Classifier"},{"metadata":{"trusted":true,"_uuid":"c9825ee325a864fdc4870857ad7cc122f2f2cf9c"},"cell_type":"code","source":"# importing AdaBoost classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel_evaluation(ExtraTreesClassifier(n_jobs=-1, random_state = 53))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01d97bbb56062d87f35d09a2f341ab22d0f877ea"},"cell_type":"markdown","source":"### 5. Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"57f4c8a5e85fc8421f1a7e5630da0f0a59ec39a4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel_evaluation(LogisticRegression(n_jobs = -1, random_state = 53))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b869e7742908462e0304a363c38d6c152df5ccbf"},"cell_type":"markdown","source":"Seeing the results now and before moving to next step of choosing the model let me give all the values of all the parameters of the models used above that is given by default by sklearn.\n\n**Parameters Values for KNN:**\n    - n_neighbors = 5, \n    - weights = uniform, \n    - algorithm = auto, \n    - leaf_size = 30, \n    - p = 2, \n    - metric = minkowski, \n    - metric_params = None, \n    - n_jobs = -1\n    \n**Parameters Values for RFC:**\n    - n_estimators = 10, \n    - criterion = gini, \n    - max_depth = None, \n    - min_samples_split = 2, \n    - min_samples_leaf = 1, \n    - min_weight_fraction_leaf = 0.0, \n    - max_features = auto, \n    - max_leaf_nodes = None, \n    - min_impurity_decrease = 0.0, \n    - min_impurity_split = None, \n    - bootstrap = True, \n    - oob_score = False, \n    - n_jobs = -1, \n    - random_state = 53, \n    - verbose = 0, \n    - warm_start = False, \n    - class_weight = None\n\n**Parameters Values for SGDC:**\n    - loss = hinge, \n    - penalty = l2, \n    - alpha = 0.0001, \n    - l1_ratio = 0.15, \n    - fit_intercept = True, \n    - max_iter = None, \n    - tol = None, \n    - shuffle = True, \n    - verbose = 0, \n    - epsilon = 0.1, \n    - n_jobs = -1, \n    - random_state = 53, \n    - learning_rate = optimal, \n    - eta0 = 0.0, \n    - power_t = 0.5, \n    - class_weight = None, \n    - warm_start = False, \n    - average = False, \n    - n_iter = None\n\n**Parameters Values for ETC:**\n    - n_estimators = 10, \n    - criterion = gini, \n    - max_depth = None, \n    - min_samples_split = 2, \n    - min_samples_leaf = 1, \n    - min_weight_fraction_leaf = 0.0, \n    - max_features = auto, \n    - max_leaf_nodes = None, \n    - min_impurity_decrease = 0.0, \n    - min_impurity_split = None, \n    - bootstrap = False, \n    - oob_score = False, \n    - n_jobs = -1, \n    - random_state = 53, \n    - verbose = 0, \n    - warm_start = False, \n    - class_weight = None\n    \n**Parameters Values for LR:**    \n    - penalty = l2, \n    - dual = False, \n    - tol = 0.0001, \n    - C = 1.0, \n    - fit_intercept = True, \n    - intercept_scaling = 1, \n    - class_weight = None, \n    - random_state = 53, \n    - solver = liblinear, \n    - max_iter = 100, \n    - multi_class = ovr, \n    - verbose = 0, \n    - warm_start = False, \n    - n_jobs = 1"},{"metadata":{"_uuid":"c617b840ac36515cb33ac051959ab842a6231c6d"},"cell_type":"markdown","source":"-------"},{"metadata":{"_uuid":"fcc88fa840f4b4fe684b646f4f6545e8248123a9"},"cell_type":"markdown","source":"### Choosing Model\n\nOut of 5 Models evaluated above and benchmark model, which performs better? Lets see all the scores of all the models in a table below:"},{"metadata":{"_uuid":"311fb26937f445d78b4cf8954fb8f08bd339f503"},"cell_type":"markdown","source":"| Model | Accuracy | F1 Score | Train Time (m) | Evaluation Time (m) |\n| ----- | -------- | -------- | ---------- | --------------- |\n| MNB | 56.22% | 23.07 | 0.003 | 0.077  -  0.078 |\n| **KNN** | **92.39%** | **87.07%** | **3.423  -  3.548** | **100.721  -  110.443** |\n| **RF** | **93.37%** | **89.09%** | **0.092** | **3.764  -  3.853** |\n| **SGDC** | 69.34% | 30.77% | 0.026 - 0.031 | 1.302  -  1.448 |\n| **ETC** | **92.93%** | **89.14%** | **0.046  -  0.063** | **2.485  -  2.633** |\n| **LR** | 69.7% | 36.88% | 0.942 | 16.366 |"},{"metadata":{"_uuid":"d5cf76da5cc3b4340eef2b8d78726d7d82ebac76"},"cell_type":"markdown","source":"**Here it is, default params have done the job! Clearly all models beating our benchmark model.**\n\n- `KNN` performs here stunningly awesome, getting accuracy `92.39%` and `87.07%` of F1 score. Though it takes highest training and evaluation time there but it has gotten results very well.\n\n- `Random Forest` here gets highest accuracy result of `93.37%` taking seconds to train and couple of minutes running cross validation and metrics results. Also given its flexibility it has perofrmed so well with default params.\n\n- `Stochastic Gradient Descent` has shocked me with its accuracy and F1 score results. In proposal I had mentioned that this and Random Forest would be really intresting to see as their results might be really high and close enough and would be tough call to pick but it turned out to be opposite now. Might some tuning in parameter would get a little better result but I dont think it would beat Random Forest or Extra Trees Classifier and that was the whole point as discussed before, to see results based on default params of models. SGDC has least training and evaluation time.\n\n- `Extra Trees` gets highest F1 score result of `89.14%` hasn't disappointed with its metrics results and runtime, both RFC and this have close call on accuracy having less than a percent difference and similar runtimes.\n\n- `Logistic Regression` has similar results as `SGDC` but has better F1 score and takes more time in evaluation. But for LR, an expected results atleast for me.\n\n- Benchmark model used here `Multinomial Naive Bayes`, has been beaten by all models above that I chose for solution both in terms of accuracy and F1 score.\n\nChoosing best classifiers among these 3 which are bold in table is little tight atleast between `RF` and `ETC`. `KNN` have preformed well predicting classes, `n_neighbors`, which is the parameter of `KNN`, set to `5` by default has done it but it just took alot of evaluation time also train time was highest among all but its reasonable in general atleast for this particular data. \n\nSo to pick one model I would consider not only having best accuracy score but also `F1 score` since `F1 scores` are more important as they give us an weighted average score of both `precision` and `recall`, where `precision` is intuitively the ability of the classifier not to label as positive a sample that is negative and `recall` it's the number of positive prediction divided by the number of positive class values and `F1 score` is the balance of both of these. Let us dig deep into f1 before we choose our final model.\n\nSo all that said, hows it calculated, whats the formula? Let us see....\n\n$$F1=2 \\cdot \\frac{ \\cdot }{ + }$$\n\nwhere `Precision` is calculated by..\n\n$$Precision = \\frac{TP}{TP + FP}$$\n\nand `Recall` is calculated by....\n\n$$Recall = \\frac{TP}{TP + FN}$$\n\n`ETC` do have here high F1 score by 0.005 but it has lower accuracy score of 0.43 compared to `RFC`.\n\nSo with that said, **I will be picking `Random Forest` as my final model to evaluate on the test set and see its performance on it.** Will it perform better or same? Lets check it out!"},{"metadata":{"_uuid":"bf5627ae643b0773955c46e2822e6755b48e62d5"},"cell_type":"markdown","source":"-------\n-------"},{"metadata":{"_uuid":"9f1a65af158d35be9e4c4af5dd9d6676f4b17b83"},"cell_type":"markdown","source":"## Testing Model"},{"metadata":{"_uuid":"6c21391930ef667ef702d567b9d658174686903a"},"cell_type":"markdown","source":"Since I had chosen default params above in `Random Forest` in training set, I will be **tuning important param** that might give us more better results. Later I will discuss about it."},{"metadata":{"trusted":true,"_uuid":"f6c62ee45a6e5bd5bcf49ee11530697308139274"},"cell_type":"code","source":"# importing EM scores for model performance measure\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# definning best chosen classifier\nclf = RandomForestClassifier(n_estimators = 50, random_state = 53)\n\n# training our model\nclf = clf.fit(X_train, y_train)\n\n# predicting unseen data\npredict = clf.predict(X_test)\n\n# calculating accuracy\naccuracy = accuracy_score(y_test, predict)\n\n# calculating f1 score\nf1_score = f1_score(y_test, predict, average = 'macro')\n\n# taking precentage and rounding to 3 places\naccuracy = np.round(accuracy * 100, 3)\nf1_score = np.round(f1_score * 100, 3)\n\n# cleaning traces\nclf = None\n\n# results\nprint(\"The accuracy score of our final model Random Forest Classifier on our testing set is\", accuracy,\"% and f1 score is\", f1_score,\"%.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07f450f20a5d5406cd49d264a776b12740126272"},"cell_type":"markdown","source":"**Better results of what we got in training. `n_estimator` parameter has done the job!**\n\nThe default value of `n_estimator` was 10 which was used in training data. `n_estimators` is the number of trees we want to build before taking the maximum voting or averages of predictions. Higher number of trees gives us better performance but makes code run little slower. Choosing as high value makes our predictions stronger and more stable. `50` is reasonable value and increasing more will take more proceesing power but would give you increasing scores but at not that much of a difference than with `50`. \n\nBelow I have given all the values that I have tried on `n_estimators` and `accuracy` and `f1` score got:\n\n| n_estimator | accuracy | F1 score |\n| ----------- | -------- | -------- |\n| 10 | 93.6% | 89.4% |\n| 15 | 94.257% | 90.6% |\n| 20 | 94.545% | 90.867% |\n| 25 | 94.644% | 91.067% |\n| 30 | 94.758% | 91.225% |\n| 50 | 94.923% | 91.453% |\n\nAs we see from above table, if we compare `RFC` with our training result with same `n_estimators` of `10`, we see that it has performed better by `.3` and `.4` in `accuracy` and `F1 score` respectively. And as we increase our `n_estimators` we see consistant increase in both scores but at some level it starts to slow down that consistancy as we see between `n_estimators` `30` and `50`. \n\nAll that said our model in all has performed really well predicting classes even with oneven amount of distribution but thanks to the authors for giving us huge dataset of all classes for making prediction and our job to evaluate easier!"},{"metadata":{"_uuid":"0ba9aac05ce9ba2c6402c64dd140c515f4fa9d79"},"cell_type":"markdown","source":"------\n------"},{"metadata":{"_uuid":"5e4cb9ba2481445286cb2b5320676abfa43f4d72"},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{"_uuid":"586dfefd8b347f6fe53b4983350ce051617fbae3"},"cell_type":"markdown","source":"Coming back from where we started in proposal, can we predict which `Forest Cover Type` it is, given `elevation, hydrologic, sunlight and wilderness area` data? **Yes we can! Tree models are your best friends in predicting forest cover types but when you got lots of time, your nearest neighbor isn't a bad choice to make too!!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}