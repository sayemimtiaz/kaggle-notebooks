{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Concepts and Examples Covered\nThe aim of this notebook is to provide example code on how to complete the following using **PIPELINES**:\n\n- Splitting a dataset into training and test sets\n- Encoding categorical data using ColumnTransformer (OrdinalEncoder and OneHotEncoder)\n- Hyperparameter grid search for SVM and kNN models\n- Evaluating a final ML model on a test set"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports, seeing if any rows/columns are missing data\nimport numpy as np, pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import plot_roc_curve, plot_confusion_matrix\n\ndata= pd.read_csv('/kaggle/input/customer-analytics/Train.csv')\nprint('Number of columns/rows with any missing values:', data.isnull().sum().sum())\nprint('Dataset size:', np.shape(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this dataset, columns 2-11 represent features and column 12 represents the target for each row/set of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset into a training and test set, ensuring the same proportions for the target are in both the training and test sets\nX= data.iloc[:, 1:-1]\nY= data.iloc[:, -1]\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size= 0.2, stratify= Y, random_state= 24)\n\nprint('Number of training examples:', len(X_train))\nprint('Number of test examples:', len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to use all features in a ML model, we need to encode the categorical features\n# first get the cardinality of each of the categorical features (to help decide how we will encode these features)\nnum_feats= X_train.select_dtypes(include= ['int64', 'float32']).columns\ncat_feats= X_train.select_dtypes(include= 'object').columns\nprint(X_train[cat_feats].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Gender, Product_importance, and Mode_of_Shipment, we will use ordinal encoding. For Warehouse_block, we will use one-hot encoding\n# for any numerical features, we will scale them so their values are between 0 and 1\nprint(X_train['Gender'].value_counts())\nprint(X_train['Product_importance'].value_counts())\nprint(X_train['Mode_of_Shipment'].value_counts())\ntransforms= [('num_t', MinMaxScaler(), list(num_feats)), ('warehouse', OneHotEncoder(categories= 'auto', sparse= False), ['Warehouse_block']), ('gender', OrdinalEncoder(categories= [['M', 'F']]), ['Gender']), ('importance', OrdinalEncoder(categories= [['low', 'medium', 'high']]), ['Product_importance']), ('shipment', OrdinalEncoder(categories= [['Ship','Flight','Road']]), ['Mode_of_Shipment'])]\n\ncol_transforms= ColumnTransformer(transforms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, we have our dataset split into training and test sets. We also have all of our feature transformations set up and ready to be used in a pipeline. Next, we will create a ML model (SVM) and then define our pipeline before executing our hyperparameter grid search using our pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model, pipeline, and hyperparameter settings to test\nmodel_svm= SVC()\npipeline_svm= Pipeline(steps= [('prep', col_transforms), ('mod', model_svm)])\nparams_svm= {'mod__C': [0.5, 1, 10], 'mod__kernel': ['linear', 'rbf'], 'mod__class_weight': [None, 'balanced'], 'mod__random_state': [24]}\nsearch_svm= GridSearchCV(pipeline, param_grid= params, cv=20, n_jobs= -1, scoring= 'roc_auc')\nsearch_svm.fit(X_train, Y_train) # will take awhile to run... reduce number hyperparameters to test to reduce time!\nprint(search_svm.best_params_)\nprint(search_svm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the hyperparameter grid search, we now have the best hyperparameter settings for our SVM model. The 20-fold cross-validation score (ROC-AUC) for these settings is 0.729. I chose this particular scoring function because of the class imbalance in what we are trying to predict (3549 instances of class 0 vs. 5250 instances of class 1). For reference, a perfect score is 1. Since there is room for improvement for this score, let's try another model before we get into evaluating our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_nn= KNeighborsClassifier()\npipeline_nn= Pipeline(steps= [('prep', col_transforms), ('mod', model_nn)])\nparams_nn= {'mod__n_neighbors': [3, 5, 7]}\nsearch_nn= GridSearchCV(pipeline, param_grid= params, cv=20, n_jobs= -1, scoring= 'roc_auc')\nsearch_nn.fit(X_train, Y_train) # will take awhile to run... reduce number hyperparameters to test to reduce time!\nprint(search_nn.best_params_)\nprint(search_nn.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see we get a worse result for the kNN model. You can continue to try different models and hyperparameter settings until you find a model that gives you a score you're happy with, but for the sake of this notebook, we will use the SVM model as our best model and evaluate its performance on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_mod= SVC(C= 0.5, class_weight= 'balanced', kernel= 'rbf', random_state= 24)\npipeline_fin= Pipeline(steps= [('prep', col_transforms), ('mod', model_nn)])\npipeline_fin.fit(X_train, Y_train)\npredicts= pipeline_fin.predict(X_test)\nplot_roc_curve(pipeline_fin, X_test, Y_test); plot_confusion_matrix(pipeline_fin, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These visualizations illustrate how many test examples were misclassified - 435 examples with a ground truth label of 1 were misclassified as belonging to class 0 whereas 375 examples with a ground truth label of 0 were misclassified as belonging to class 1.\n\n# Conclusions\nThis notebook provided example code on how to implement pipelines to conduct feature transformations and a hyperparameter grid search. Pipelines are super handy and can help reduce the risk of accidental data leakage - for the final model, we fit the pipeline on the training data and then made predictions on our test set. Leave your feedback, questions, and anything else you'd like to see from me in the comments!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}