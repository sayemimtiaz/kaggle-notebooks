{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\n#plt.style.use('ggplot')\n#ggplot is R based visualisation package that provides better graphics with higher level of abstraction","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Data Science and ML Pipeline","metadata":{"_uuid":"fb40452ad5ae975fd33307735fd47c33f1aca999"}},{"cell_type":"code","source":"#Loading the dataset\ndiabetes_data = pd.read_csv('/kaggle/input/dataset-dental-panoramic/dataset_dental_panoramic .csv')\n\n#Print the first 5 rows of the dataframe.\ndiabetes_data.head()","metadata":{"_uuid":"3977359aa7f65cc9d96fdf0ede4e99116fb314c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA and statistical analysis\n","metadata":{"_uuid":"e19cd0e1959c38d52ed8c437b42f3ccd61e92fb4"}},{"cell_type":"code","source":"## gives information about the data types,columns, null value counts, memory usage etc\n## function reference : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\ndiabetes_data.info(verbose=True)","metadata":{"scrolled":true,"_uuid":"650e7357fe6d26e6a557cc3088d9b1147b6093ea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## basic statistic details about the data (note only numerical columns would be displayed here unless parameter include=\"all\")\n## for reference: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html#pandas.DataFrame.describe\ndiabetes_data.describe()\n\n## Also see :\n##to return columns of a specific dtype: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes","metadata":{"_uuid":"0682a8c7acbdb75ad600df7b7cd732f4d5b0a17f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes_data.describe().T","metadata":{"_uuid":"de488e0ee803ee6d4084c92df3f548eb6a051d43","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values","metadata":{"_uuid":"a5c66431403ac1246f52e4256b09b660fc273c25"}},{"cell_type":"code","source":"diabetes_data_copy = diabetes_data.copy(deep = True)\ndiabetes_data_copy[['dissimilarity_0', 'dissimilarity_45', 'dissimilarity_90', 'dissimilarity_135', \n                 'correlation_0', 'correlation_45', 'correlation_90', 'correlation_135',\n                 'homogeneity_0', 'homogeneity_45', 'homogeneity_90', 'homogeneity_135',\n                 'contrast_0', 'contrast_45', 'contrast_90', 'contrast_135',\n                 'ASM_0', 'ASM_45', 'ASM_90', 'ASM_135',\n                 'energy_0', 'energy_45', 'energy_90', 'energy_135']] = diabetes_data_copy[['dissimilarity_0', 'dissimilarity_45', 'dissimilarity_90', 'dissimilarity_135', \n                 'correlation_0', 'correlation_45', 'correlation_90', 'correlation_135',\n                 'homogeneity_0', 'homogeneity_45', 'homogeneity_90', 'homogeneity_135',\n                 'contrast_0', 'contrast_45', 'contrast_90', 'contrast_135',\n                 'ASM_0', 'ASM_45', 'ASM_90', 'ASM_135',\n                 'energy_0', 'energy_45', 'energy_90', 'energy_135']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(diabetes_data_copy.isnull().sum())","metadata":{"_uuid":"506fa58605d641694883cad75bb3683dafdb0356","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### To fill these Nan values the data distribution needs to be understood","metadata":{"_uuid":"20bf4545b4d80cd00061d08d3cc2206d0b80f376","trusted":true}},{"cell_type":"code","source":"p = diabetes_data.hist(figsize = (20,20))","metadata":{"_uuid":"d5ef22db6afea0d63189599fb4abbb2b3f53ccb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aiming to impute nan values for the columns in accordance with their distribution","metadata":{"_uuid":"337635cc5b94f7bf6b1a95f5d4043e67604c2c8a"}},{"cell_type":"code","source":"diabetes_data_copy['dissimilarity_0'].fillna(diabetes_data_copy['dissimilarity_0'].mean(), inplace = True)\ndiabetes_data_copy['dissimilarity_45'].fillna(diabetes_data_copy['dissimilarity_45'].mean(), inplace = True)\ndiabetes_data_copy['dissimilarity_90'].fillna(diabetes_data_copy['dissimilarity_90'].median(), inplace = True)\ndiabetes_data_copy['dissimilarity_135'].fillna(diabetes_data_copy['dissimilarity_135'].median(), inplace = True)\ndiabetes_data_copy['correlation_0'].fillna(diabetes_data_copy['correlation_0'].mean(), inplace = True)\ndiabetes_data_copy['correlation_45'].fillna(diabetes_data_copy['correlation_45'].mean(), inplace = True)\ndiabetes_data_copy['correlation_90'].fillna(diabetes_data_copy['correlation_90'].median(), inplace = True)\ndiabetes_data_copy['correlation_135'].fillna(diabetes_data_copy['correlation_135'].median(), inplace = True)\ndiabetes_data_copy['homogeneity_0'].fillna(diabetes_data_copy['homogeneity_0'].mean(), inplace = True)\ndiabetes_data_copy['homogeneity_45'].fillna(diabetes_data_copy['homogeneity_45'].mean(), inplace = True)\ndiabetes_data_copy['homogeneity_90'].fillna(diabetes_data_copy['homogeneity_90'].median(), inplace = True)\ndiabetes_data_copy['homogeneity_135'].fillna(diabetes_data_copy['homogeneity_135'].median(), inplace = True)\ndiabetes_data_copy['contrast_0'].fillna(diabetes_data_copy['contrast_0'].mean(), inplace = True)\ndiabetes_data_copy['contrast_45'].fillna(diabetes_data_copy['contrast_45'].mean(), inplace = True)\ndiabetes_data_copy['contrast_90'].fillna(diabetes_data_copy['contrast_90'].median(), inplace = True)\ndiabetes_data_copy['contrast_135'].fillna(diabetes_data_copy['contrast_135'].median(), inplace = True)\ndiabetes_data_copy['ASM_0'].fillna(diabetes_data_copy['ASM_0'].mean(), inplace = True)\ndiabetes_data_copy['ASM_45'].fillna(diabetes_data_copy['ASM_45'].mean(), inplace = True)\ndiabetes_data_copy['ASM_90'].fillna(diabetes_data_copy['ASM_90'].median(), inplace = True)\ndiabetes_data_copy['ASM_135'].fillna(diabetes_data_copy['ASM_135'].median(), inplace = True)\ndiabetes_data_copy['energy_0'].fillna(diabetes_data_copy['energy_0'].mean(), inplace = True)\ndiabetes_data_copy['energy_45'].fillna(diabetes_data_copy['energy_45'].mean(), inplace = True)\ndiabetes_data_copy['energy_90'].fillna(diabetes_data_copy['energy_90'].median(), inplace = True)\ndiabetes_data_copy['energy_135'].fillna(diabetes_data_copy['energy_135'].median(), inplace = True)\n","metadata":{"_uuid":"0568a737f6529e711d3e1ffa5493f0fa749efff3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting after Nan removal ","metadata":{"_uuid":"1bf99a7a106d92072207b4a5071098ac32c03902"}},{"cell_type":"code","source":"p = diabetes_data_copy.hist(figsize = (20,20))","metadata":{"_uuid":"6b54f61be1a5b6fc75a90bfdcdfdb1df18a38594","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## observing the shape of the data\ndiabetes_data.shape","metadata":{"_uuid":"8322614f5de4888d713c0468a43ae2a3eb1b8862","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## data type analysis\n#plt.figure(figsize=(5,5))\n#sns.set(font_scale=2)\nsns.countplot(y=diabetes_data.dtypes ,data=diabetes_data)\nplt.xlabel(\"count of each data type\")\nplt.ylabel(\"data types\")\nplt.show()","metadata":{"_uuid":"c69e3cf2bce422520047daa7b02b688e42c197e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## null count analysis\nimport missingno as msno\np=msno.bar(diabetes_data)\n","metadata":{"_uuid":"1d5a4e88b691d25e8c661e3e292f5d81d33c576e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the balance of the data by plotting the count of outcomes by their value\ncolor_wheel = {1: \"#0392cf\", \n               2: \"#7bc043\"}\ncolors = diabetes_data[\"outcome\"].map(lambda x: color_wheel.get(x + 1))\nprint(diabetes_data.outcome.value_counts())\np=diabetes_data.outcome.value_counts().plot(kind=\"bar\")\n","metadata":{"_uuid":"009e21b28bb9fcb64bd0b841d0aa09597b9bbea8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas.tools.plotting import scatter_matrix\np=scatter_matrix(diabetes_data,figsize=(25, 25))","metadata":{"_uuid":"f100c23bcf63fa58180813ff594dfff591bb20ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=sns.pairplot(diabetes_data_copy, hue = 'outcome')","metadata":{"_uuid":"3adefc9ab47271b11f13687a245df6ced9f1b312","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","metadata":{"_uuid":"2971ad528058ac82e54eec7b42b814644cef4195","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","metadata":{"_uuid":"0b000ace1a6558c57c0b485c66a5bdb84063174b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling the data \ndata Z is rescaled such that Î¼ = 0 and ð›” = 1, and is done through this formula:\n![](https://cdn-images-1.medium.com/max/800/0*PXGPVYIxyI_IEHP7.)\n\n\n#### to learn more about scaling techniques\nhttps://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\nhttps://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/","metadata":{"_uuid":"5e5142438fcc7973b722337deb24ba2061e14316"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop([\"outcome\"],axis = 1),),\n        columns=['dissimilarity_0', 'dissimilarity_45', 'dissimilarity_90', 'dissimilarity_135', \n                 'correlation_0', 'correlation_45', 'correlation_90', 'correlation_135',\n                 'homogeneity_0', 'homogeneity_45', 'homogeneity_90', 'homogeneity_135',\n                 'contrast_0', 'contrast_45', 'contrast_90', 'contrast_135',\n                 'ASM_0', 'ASM_45', 'ASM_90', 'ASM_135',\n                 'energy_0', 'energy_45', 'energy_90', 'energy_135'])","metadata":{"_uuid":"2cf6c9ff22d3a7af35e399406e7565055ca6af36","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"_uuid":"e10c79fe13861fb0fbe714c977f93bb8ed90a5fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = diabetes_data.drop(\"outcome\",axis = 1)\ny = diabetes_data_copy.outcome","metadata":{"_uuid":"9d9bd9ecd9612fb32f0629a8a3c0a85a14b034cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Train Split and Cross Validation methods\n\n\n\n***Train Test Split*** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n\n![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n\n***Cross Validation***: When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques\n\n![](https://cdn-images-1.medium.com/max/1600/1*4G__SV580CxFj78o9yUXuQ.png)\n\n\n***About Stratify*** : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n\nFor Reference : https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6","metadata":{"_uuid":"f87d51b88980792d54d8f3ba60c2c7b91d03cad3","trusted":true}},{"cell_type":"code","source":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)","metadata":{"_uuid":"f14c88ae0a061de30566d336608d195ba89993d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","metadata":{"_uuid":"a7081050c51df07b8af1cd18c9be61f041a97fb8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","metadata":{"_uuid":"ee126a72ca24e54ee78bfac94a21dfac1a3edee1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","metadata":{"_uuid":"8bbfda9d066c354f974dcb1180c3348aaa915c4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result Visualisation","metadata":{"_uuid":"fe08768381ea8011d90ae58149c8e41b0a707da2"}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","metadata":{"_uuid":"2a5c0b4fde15148a049fa340a58f5b4fa421e614","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The best result is captured at k = 11 hence 11 is used for the final model","metadata":{"_uuid":"1db31455aba31edc524091fa0914743a284034c5","trusted":true}},{"cell_type":"code","source":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(11)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","metadata":{"_uuid":"277c1bb9c48cca13536ac8ba71604818d323fae0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## trying to plot decision boundary ","metadata":{"_uuid":"22878f26662e0a863c48cf43030c9e6ab57d98fc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value = 20000\nwidth = 20000\nplot_decision_regions(X.values, y.values, clf=knn, legend=2, \n                      filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value, 8: value, 9: value, 10: value, 11: value, 12: value, 13: value, 14: value, 15: value, 16: value, 17: value, 18: value, 19: value, 20: value, 21: value, 22: value, 23: value},\n                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width, 8: width, 9: width, 10: width, 11: width, 12: width, 13: width, 14: width, 15: width, 16: width, 17: width, 18: width, 19: width, 20: width, 21: width, 22: width, 23: width},\n                      X_highlight=X_test.values)\n\n# Adding axes annotations\n#plt.xlabel('sepal length [cm]')\n#plt.ylabel('petal length [cm]')\nplt.title('KNN with Diabetes Data')\nplt.show()","metadata":{"_uuid":"e67d02ae6dbab28b27cfbd968d3717e406de53bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Performance Analysis","metadata":{"_uuid":"ab1e49d83f39a6ddc780c394d3a052b49508c6ac","trusted":true}},{"cell_type":"markdown","source":"## 1. Confusion Matrix\n\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n![](https://cdn-images-1.medium.com/max/1600/0*-GAP6jhtJvt7Bqiv.png)\n\n\n\n### ***In the famous cancer example***:\n\n\n###### Cases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\n\n###### Cases in which the doctor predicted NO (they do not have the disease), and they donâ€™t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\n\n###### Cases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as â€œType I errorâ€.\n\n###### Cases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as â€œType II errorâ€.\n\n![](https://cdn-images-1.medium.com/max/1600/0*9r99oJ2PTRi4gYF_.jpg)\n\nFor Reference: https://medium.com/@djocz/confusion-matrix-aint-that-confusing-d29e18403327","metadata":{"_uuid":"0b471c49636c633d40442a208c5983d607e4fa2c"}},{"cell_type":"code","source":"#import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"_uuid":"d09044f60af8405e7334c2062404336d0849e871","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","metadata":{"_uuid":"cb0b2fc2a33afeed856e2b3cc986ffb9e3e3ae7b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report\n\nReport which includes Precision, Recall and F1-Score.\n\n\n#### Precision Score\n        TP â€“ True Positives\n        FP â€“ False Positives\n\n        Precision â€“ Accuracy of positive predictions.\n        Precision = TP/(TP + FP)\n        \n   \n#### Recall Score\n        FN â€“ False Negatives\n\n        Recall(sensitivity or true positive rate): Fraction of positives that were correctly identified.\n        Recall = TP/(TP+FN)\n        \n#### F1 Score\n        F1 Score (aka F-Score or F-Measure) â€“ A helpful metric for comparing two classifiers.\n        F1 Score takes into account precision and the recall. \n        It is created by finding the the harmonic mean of precision and recall.\n\n        F1 = 2 x (precision x recall)/(precision + recall)\n        \n        \n        \n> > ***Precision*** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.\n> > \n> > Precision = TP/TP+FP\n> > \n> > ***Recall (Sensitivity)*** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? A recall greater than 0.5 is good.\n> > \n> > Recall = TP/TP+FN\n> > \n> > ***F1 score*** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, itâ€™s better to look at both Precision and Recall. \n> > \n> > F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n        \n> ****","metadata":{"_uuid":"893c30414255c296b13d0d421086bb1a24cdd22c","trusted":true}},{"cell_type":"code","source":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","metadata":{"_uuid":"6ac998149c1f0dd304b807707f0dc44dd2b2ffb3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","metadata":{"_uuid":"20b2083d2eaf2fca599eb6f2ef8803be0b1ac5d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()","metadata":{"_uuid":"379eefad0181f1f57ffbb3634ab6d132af17464f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","metadata":{"_uuid":"6c92773e49532f6133b23d511058202bb77ff2cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","metadata":{"_uuid":"e369c794253b71d3daa1444fb7d11872fb8a110c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"0899b30f7683581d675a185468972b2165eec04e","trusted":true},"execution_count":null,"outputs":[]}]}