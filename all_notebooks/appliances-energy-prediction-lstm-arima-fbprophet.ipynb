{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, model_selection, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the dataset in dataframe\ndata = pd.read_csv(\"../input/KAG_energydata_complete.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of rows in dataset is - ' , data.shape[0])\nprint('The number of columns in dataset is - ' , data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of null values in all columns\ndata.isnull().sum().sort_values(ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 75% of the data is usedfor the training of the models and the rest is used for testing\ntrain, test = train_test_split(data,test_size=0.25,random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide the columns based on type for clear column management \n\ncol_time=[\"date\"]\n\ncol_temp = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\"]\n\ncol_hum = [\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_5\",\"RH_6\",\"RH_7\",\"RH_8\",\"RH_9\"]\n\ncol_weather = [\"T_out\", \"Tdewpoint\",\"RH_out\",\"Press_mm_hg\",\n                \"Windspeed\",\"Visibility\"] \ncol_light = [\"lights\"]\n\ncol_randoms = [\"rv1\", \"rv2\"]\n\ncol_target = [\"Appliances\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate dependent and independent variables \nfeature_vars = train[ col_time + col_temp + col_hum + col_weather + col_light + col_randoms ]\ntarget_vars = train[col_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of values in lights column\nfeature_vars.lights.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Due to lot of zero enteries this column is of not much use and will be ignored in rest of the model\n_ = feature_vars.drop(['lights'], axis=1 , inplace= True) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_vars.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= data.date  ,  mode = \"lines\", y = data.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption pattern' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding column to mark weekdays (0) and weekends(1) for time series evaluation , \n# decided not to use it for model evaluation as it has least impact\n\ndata['WEEKDAY'] = ((pd.to_datetime(data['date']).dt.dayofweek)// 5 == 1).astype(float)\n# There are 5472 weekend recordings \ndata['WEEKDAY'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find rows with weekday \ntemp_weekday =  data[data['WEEKDAY'] == 0]\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= temp_weekday.date  ,  mode = \"lines\", y = temp_weekday.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption pattern on weekdays' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find rows with weekend \n\ntemp_weekend =  data[data['WEEKDAY'] == 1]\n\n# To understand the timeseries variation of the applaince energy consumption\nvisData = go.Scatter( x= temp_weekend.date  ,  mode = \"lines\", y = temp_weekend.Appliances )\nlayout = go.Layout(title = 'Appliance energy consumption pattern on weekend' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of all the features to understand the distribution\nfeature_vars.hist(bins = 20 , figsize= (12,16)) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# focussed displots for RH_6 , RH_out , Visibility , Windspeed due to irregular distribution\nf, ax = plt.subplots(2,2,figsize=(12,8))\nvis1 = sns.distplot(feature_vars[\"RH_6\"],bins=10, ax= ax[0][0])\nvis2 = sns.distplot(feature_vars[\"RH_out\"],bins=10, ax=ax[0][1])\nvis3 = sns.distplot(feature_vars[\"Visibility\"],bins=10, ax=ax[1][0])\nvis4 = sns.distplot(feature_vars[\"Windspeed\"],bins=10, ax=ax[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of values in Applainces column\nf = plt.figure(figsize=(12,5))\nplt.xlabel('Appliance consumption in Wh')\nplt.ylabel('Frequency')\nsns.distplot(target_vars , bins=10 ) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliance column range with consumption less than 200 Wh\nprint('Percentage of the appliance consumption is less than 200 Wh')\nprint(((target_vars[target_vars <= 200].count()) / (len(target_vars)))*100 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the weather , temperature , applainces and random column to see the correlation\ntrain_corr = train[col_temp + col_hum + col_weather +col_target+col_randoms]\ncorr = train_corr.corr()\n# Mask the repeated values\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n  \nf, ax = plt.subplots(figsize=(16, 14))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, annot=True, fmt=\".2f\" , mask=mask,)\n    #Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n    #Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n    #show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# Function to get top correlations \n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_corr, 40))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split training dataset into independent and dependent varibales\ntrain_X = train[feature_vars.columns]\ntrain_y = train[target_vars.columns]\ntrain_X.drop(['date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boruta algorith for feature selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom boruta import BorutaPy\nfrom datetime import datetime\n\nX=train_X.values\ny=train_y.values\ny=y.ravel()\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n\nrfc = RandomForestRegressor(n_estimators=100, max_depth=6, criterion='mse')\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2)\nstart_time = timer(None)\nboruta_selector.fit(X, y)\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"boruta_selector.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"boruta_selector.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split testing dataset into independent and dependent varibales\ntest_X = test[feature_vars.columns]\ntest_y = test[target_vars.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Due to conlusion made above below columns are removed\ntrain_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\"],axis=1 , inplace=True)\n\n# Due to conlusion made above below columns are removed\ntest_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\",\"date\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\n# Create test and training set by including Appliances column\n\ntrain = train[list(train_X.columns.values) + col_target ]\n\ntest = test[list(test_X.columns.values) + col_target ]\n\n# Create dummy test and training set to hold scaled values\n\nsc_train = pd.DataFrame(columns=train.columns , index=train.index)\n\nsc_train[sc_train.columns] = sc.fit_transform(train)\n\nsc_test= pd.DataFrame(columns=test.columns , index=test.index)\n\nsc_test[sc_test.columns] = sc.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Appliances column from traininig set\n\ntrain_X =  sc_train.drop(['Appliances'] , axis=1)\ntrain_y = sc_train['Appliances']\n\ntest_X =  sc_test.drop(['Appliances'] , axis=1)\ntest_y = sc_test['Appliances']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n\nmodels = [\n           ['RandomForest ',RandomForestRegressor()],\n           ['ExtraTreeRegressor :',ExtraTreesRegressor()]\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run all the proposed models and update the information in a list model_data\nimport time\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\nmodel_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model.random_state = 78\n    curr_model_data[\"Name\"] = name\n    start = time.time()\n    curr_model.fit(train_X,train_y)\n    end = time.time()\n    curr_model_data[\"Train_Time\"] = end - start\n    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(train_y,curr_model.predict(train_X))\n    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(test_y,curr_model.predict(test_X))\n    curr_model_data[\"Test_RMSE_Score\"] = sqrt(mean_squared_error(test_y,curr_model.predict(test_X)))\n    model_data.append(curr_model_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(model_data)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{\n              'max_depth': [80, 150, 200,250],\n              'n_estimators' : [100,150,200,250],\n              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n            }]\nreg = ExtraTreesRegressor(random_state=40)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\ngrid_search.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuned parameter set\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best possible parameters for ExtraTreesRegressor\ngrid_search.best_estimator_\n\n# R2 score on training set with tuned parameters\n\ngrid_search.best_estimator_.score(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score on test set with tuned parameters\ngrid_search.best_estimator_.score(test_X,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score on test set with tuned parameters\n\nnp.sqrt(mean_squared_error(test_y, grid_search.best_estimator_.predict(test_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get sorted list of features in order of importance\nfeature_indices = np.argsort(grid_search.best_estimator_.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = grid_search.best_estimator_.feature_importances_\nindices = np.argsort(importances)[::-1]\nnames = [train_X.columns[i] for i in indices]\n# Create plot\nplt.figure(figsize=(10,6))\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(train_X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(train_X.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nfrom sklearn.model_selection import train_test_split\nfrom numpy import concatenate\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom pandas import to_datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert series to supervised learning\ndef series_to_supervised(dataset, n_in=1, n_out=1, dropnan=True):\n    num_vars = 1 if type(dataset) is list else dataset.shape[1]\n    dataframe = DataFrame(dataset)\n    cols, names = list(), list()\n    \n    # input sequence (t-n, ....t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(dataframe.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(num_vars)]\n    # forecast sequence (t, t+1 .... t+n)\n    for i in range(0, n_out):\n        cols.append(dataframe.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(num_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(num_vars)]\n    \n    # put it all together \n    agg = concat(cols, axis=1)\n    agg.columns = names\n    \n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature=[\"RH_out\",\"RH_8\",\"RH_1\",\"T3\",\"RH_3\",\"T2\",\"Press_mm_hg\",\"RH_2\",\"RH_7\",\"T8\",\"RH_6\",\"RH_4\",\"RH_5\",\"T_out\",\"RH_9\",\n             \"T4\",\"T7\",\"Tdewpoint\",\"Windspeed\",\"T1\",\"T5\"]\ndata1 = data[col_target + col_time + feature]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata1[\"date\"]=pd.to_datetime(data1[\"date\"])\ndata1 = data1.set_index(['date'], drop=True)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values=data1.values\nvalues.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize features\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed = series_to_supervised(scaled, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.drop(reframed.columns[[22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = reframed.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = values[:,:21]\nY = values[:,21]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.3)\n\n# reshape input to be 3D [samples, timesteps, features]\nX_Train = X_Train.reshape((X_Train.shape[0], 1, X_Train.shape[1]))\nX_Test = X_Test.reshape((X_Test.shape[0], 1, X_Test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network architecture\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(X_Train.shape[1], X_Train.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n\n# fit\nhistory = model.fit(X_Train, Y_Train, epochs=70, batch_size=10, validation_data=(X_Test, Y_Test), verbose=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.plot(history.history['loss'], label='Train')\npyplot.plot(history.history['val_loss'], label='Test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sca=DataFrame(scaled)\nsca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_test_mse = model.evaluate(X_Test, Y_Test, batch_size=1)\nprint('Test MSE: %f'%lstm_test_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_pred_test_lstm = model.predict(X_Test)\ny_train_pred_lstm = model.predict(X_Train)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(Y_Train, y_train_pred_lstm)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(Y_Test, y_pred_test_lstm)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_y_pred_test = model.predict(X_Test)\nplt.figure(figsize=(10, 6))\nplt.plot(Y_Test, label='True')\nplt.plot(y_pred_test_lstm, label='LSTM')\nplt.title(\"LSTM's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Appliances scaled')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\nyhat = model.predict(X_Test)\nX_Test = X_Test.reshape((X_Test.shape[0], 21))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, X_Test[:, -21:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\nY_Test = Y_Test.reshape((len(Y_Test), 1))\ninv_y = np.concatenate((Y_Test, X_Test[:, -21:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ARIMA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom scipy import stats\nimport statsmodels.api as sm\nimport warnings\nfrom itertools import product\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-poster')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/KAG_energydata_complete.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df[['date','Appliances']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unix-time to \ndf.Timestamp = pd.to_datetime(df.date)\n\n# Resampling to daily frequency\ndf.index = df.Timestamp\ndf = df.resample('D').mean()\n\n# Resampling to monthly frequency\ndf_month = df.resample('M').mean()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[65, 17])\nplt.suptitle('Appliances enegy consumption', fontsize=22)\n\nplt.subplot(221)\nplt.plot(df.Appliances, '-', label='By Days')\nplt.legend()\n\nplt.subplot(222)\nplt.plot(df_month.Appliances, '-', label='By Months')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dickeyâ€“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df.Appliances)[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 18, 8\n\ndecomposition = sm.tsa.seasonal_decompose(df.Appliances, model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(df.Appliances,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = sm.tsa.statespace.SARIMAX(df.Appliances,\n                                order=(0, 1, 1),\n                                seasonal_order=(1, 0, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\n\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2016-04-20'), dynamic=False)\npred_ci = pred.conf_int()\n\nax = df.Appliances['2016':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Appliances consumption')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = df.Appliances['2016-04-20':]\n\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=15)\npred_ci = pred_uc.conf_int()\n\nax = df.Appliances.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Appliances consumption')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FBPROPHET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom fbprophet import Prophet\n\ndata = data.rename(columns={'date': 'ds', 'Appliances': 'y'})\nmodel = Prophet(interval_width=0.95)\nmodel.fit(data)\n\nfuture = model.make_future_dataframe(periods=10, freq='H')\nforecast = model.predict(future)\n\nplt.figure(figsize=(30, 6))\nmodel.plot(future, xlabel = 'Date', ylabel = 'comsumption')\nplt.title('Appliances consumption');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}