{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nimport datetime as dt\nfrom statistics import median\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport math\nfrom math import pi\nfrom scipy.spatial.distance import cdist\nfrom IPython.display import display, HTML\n\nstyle.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Aims:**\n\n1.  Group field players from FIFA20 based on their skill attributes using a ***KMeans clustering*** algorithm.\n    * only using players rated > 75\n    * no goalkeepers\n        > excluding any goalkeeper skill attributes\n    * only using base skill attributes\n        > excluding 'general' skill attributes (overall & potential), excluding the main 6 skill attributes (pace, shooting, passing, dribbling, defending, physical)\n2.  Plot and visualize differences between these groups\n3.  Find \"best\" players in each cluster\n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading in data\nplayer_data = pd.read_csv('/kaggle/input/fifa-20-complete-player-dataset/players_20.csv')\n\n# Cleaning & Dropping Data\n\n# dropping player ratings for each individual position as they are already contained in other cells or could be determined from other columns\nplayer_data.drop(['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb'], 1, inplace=True)\n\n# dropping uninteresting columns that won't be used later on\nplayer_data.drop(['sofifa_id','player_url','long_name', 'loaned_from'], 1, inplace=True)\n\n# Only keep the first position listed for each player in the column player_position (their best position)\nplayer_data['player_positions'] = player_data['player_positions'].str.split(',').str[0]\n\n# Dropping all Goalkeepers from data\nplayer_data = player_data[(player_data['player_positions'] != 'GK' )]\n\n# Only keeping players with a rating higher than 75\nplayer_data = player_data[(player_data['overall']  > 75)]\n# player_data = player_data[(player_data['overall'] < 67 )]\n\n# Dropping all GK skill-related columns, since we don't want to group field players based on their goalkeeping abilities\nplayer_data.drop(['gk_diving',\t'gk_handling',\t'gk_kicking',\t'gk_reflexes',\t'gk_speed',\t'gk_positioning',\n                  'goalkeeping_diving',\t'goalkeeping_handling',\t'goalkeeping_kicking',\t'goalkeeping_positioning',\t'goalkeeping_reflexes'], 1, inplace=True)\n\n# filling all missing values with a 0, so they'll be handled as an outlier\nplayer_data.fillna(0, inplace=True)\n\n# Even if many columns still contained in player_data won't be used for our clustering, I saved a copy into the variable original_data to plot some of those \n# columns (e.g.: height, age, work_rate, etc..) based on their cluster group.\noriginal_data = pd.DataFrame.copy(player_data)\n\n# Dropping all other columns except for skill attributes\nplayer_data.drop(['player_positions'], 1, inplace=True)\nplayer_data.drop(['overall'], 1, inplace=True)\nplayer_data.drop(['short_name','age','dob','height_cm','weight_kg','nationality','club','potential','value_eur','wage_eur',\n                  'preferred_foot','international_reputation','weak_foot','skill_moves','work_rate','body_type','real_face','release_clause_eur',\n                  'player_tags','team_position','team_jersey_number','joined','contract_valid_until','nation_position','nation_jersey_number'], 1, inplace=True)\nplayer_data.drop(['player_traits'], 1, inplace=True)\n\n\nplayer_data.drop(['pace', 'shooting', 'dribbling', 'passing', 'physic', 'defending'], 1, inplace=True)\n\nprint('Length of dataset:', len(player_data))\n\nfig, ax = plt.subplots(figsize=(10,10)) \nplt.title('Correlation heatmap of skill attributes in dataset')\nsns.heatmap(player_data.corr())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Determine optimal number of clusters to run KMeans algorithm**\n\nBefore plotting and visualizing differences between the clusters which KMeans generates I wanted to determine what would be the ideal number of clusters to specify for the KMeans algorithm. \n\nThe first method called the 'Elbow Method' consists of plotting the within-cluster sum of squares against number of clusters, and then visually determining at which point the plot makes a bend, looking like an elbow, hence its name. \n\nThe second method is called a 'Silhouette Analysis' and visualises how similar objects are to their own assigned cluster compared to the other clusters. The silhouette coefficient is a measure of how seperable the clusters are, ranging from +1 to -1 with a positive score indicating that the object is close to the center of its own cluster and far away from other cluster centers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining and scaling X data (all skill attributes for each player)\n\nX = np.array(player_data)\nX = preprocessing.scale(X)\n\nWCSS = []\nK = range(1,30)\nfor k in K:\n  kmeansmodel = KMeans(n_clusters=k)\n  kmeansmodel.fit(X)\n  # distortions.append(sum(np.min(cdist(X, SpectralClusteringModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n  WCSS.append(kmeansmodel.inertia_)\n\n# Plot the elbow\nfig = plt.figure(figsize=(25,10))\nax1 = fig.add_subplot(1,2,1)\nax1.plot(K, WCSS, 'bx-')\nplt.xlabel('k')\nplt.xticks(list(K), list(K))\nplt.ylabel('Within Cluster Sum of Squares')\nplt.title('The Elbow Method showing the optimal k')\n\nprice_series = pd.Series(WCSS)\nax2 = fig.add_subplot(1,2,2)\nWCSS_as_pdframe = pd.Series(WCSS)\nax2.plot(WCSS_as_pdframe.pct_change())\nplt.xlabel('k')\nplt.xticks(list(K), list(K))\nplt.ylabel('Percent Change')\nplt.title('Percent change using Elbow Method')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The present example shows how the elbow method can be much harder to implement in practice than in theory. After also plotting the percent change for the within-cluster sum of squares with increasing numbers of clusters, it seems like 5 clusters would be an appropriate choice based on this method, however other options like 3 or 8 could be reasonable aswell"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_samples, silhouette_score\n\nK = range(2,15)\nfor i, k in enumerate(K):\n  fig, ax1 = plt.subplots(1, 1)\n  fig.set_size_inches(18, 7)\n  \n  # Run the Kmeans algorithm\n  clf = KMeans(n_clusters=k)\n  labels = clf.fit_predict(X)\n  centroids = clf.cluster_centers_\n\n  # Get silhouette samples\n  silhouette_vals = silhouette_samples(X, labels)\n\n  # Silhouette plot\n  y_ticks = []\n  y_lower, y_upper = 0, 0\n  for i, cluster in enumerate(np.unique(labels)):\n      cluster_silhouette_vals = silhouette_vals[labels == cluster]\n      cluster_silhouette_vals.sort()\n      y_upper += len(cluster_silhouette_vals)\n      ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n      ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n      y_lower += len(cluster_silhouette_vals)\n\n  # Get the average silhouette score and plot it\n  avg_score = np.mean(silhouette_vals)\n  ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n  ax1.set_yticks([])\n  ax1.set_xlim([-0.1, 1])\n  ax1.set_xlabel('Silhouette coefficient values')\n  ax1.set_ylabel('Cluster labels')\n  ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n  \n  plt.tight_layout()\n  plt.suptitle(f'Silhouette analysis using k = {k}',\n                fontsize=16, fontweight='semibold', y=1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The silhouette plots for cluster sizes ranging from 2 to 14 show that both average and individual silhouette scores remain considerably low. This may indicate that the data is relatively continuous between clusters rather than strictly seperable. I still decided to go ahead with using KMeans though, using 5 clusters as suggested by the Elbow method. "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters_ = 5\nclf = KMeans(n_clusters=n_clusters_, n_init=100)\nclf.fit(X)\n\ncentroids = clf.cluster_centers_\nlabels = clf.labels_\n\n# creating a new column in original_data, assigning each player their cluster labels\noriginal_data['cluster_group'] = np.nan\nfor i in range(len(X)):\n  original_data['cluster_group'].iloc[i] = labels[i]\n\nprint('Cluster Sizes:')\nfor i in range(n_clusters_):\n  print(len(original_data[(original_data['cluster_group']==i)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualizing differences and similarities between clusters**\n\nThe first bar plots show the 3 most common field positions in each cluster group and the most common one will from here on onwards be used as the name for each cluster group. Furthermore, the top 5 rated players and their respective preferred field position for each cluster group are also written onto the plots. Interestingly, the most common field position is different for each cluster, suggesting that the KMeans technique was relatively successful at seperating the data into self-similar clusters. However some cluster assignments seem surprising, for example the fact that Kevin de Bruyne, Harry Kane and Sergio Ramos are all part of the same cluster.\n\nThe second set of bar plots show which clubs have the most players for each cluster group."},{"metadata":{"trusted":true},"cell_type":"code","source":"if n_clusters_ <= 4:\n  sizer = n_clusters_\nelse: \n  sizer = 4\n\nnrows_ = math.ceil(1*(n_clusters_/4))\nncols_ = 4\n\nfig = plt.figure(figsize=(sizer*7,3*nrows_))\nplt.subplots_adjust(top = 2)\n\ndef most_common_player_position(cluster_label):\n  cluster = original_data[(original_data['cluster_group']==cluster_label)]\n  v_counts = pd.value_counts(cluster['player_positions'])\n  position = v_counts.index[0]\n  return position\n\ndef bar_chart_value_counts(column, nr_columns, rotate_xlabels, text):\n  fig = plt.figure(figsize=(sizer*7,6*nrows_))\n  plt.subplots_adjust(hspace = 0.4)\n  for group in range(n_clusters_):\n    cluster = original_data[(original_data['cluster_group']==group)]\n    v_counts = pd.value_counts(cluster[column])\n    v_counts = v_counts[:nr_columns]\n    ax1 = fig.add_subplot(nrows_, ncols_, group+1)\n    ax1.bar(v_counts.index, v_counts.values)\n    ax1.set(title=most_common_player_position(group), ylabel='Number of Players')\n    if rotate_xlabels == True:\n      for label in ax1.xaxis.get_ticklabels():\n        label.set_rotation(45)\n    if text == True:\n      barchartindex = int(len(v_counts)/2)\n      ax1.text(v_counts.index[barchartindex], 0.9*v_counts.values[0],\n               'Top rated players:', style='italic',\n               bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 7})\n      nr_names = 5\n      for name in range(nr_names):\n        top_name = cluster['short_name'].values[name]\n        position = cluster['player_positions'].values[name]\n        top_name = top_name + '    ' + position\n        y_index = (0.82 - (name*0.05)) * v_counts.values[0]\n        ax1.text(v_counts.index[barchartindex], y_index, str(top_name))\n\nbar_chart_value_counts('player_positions', 3, True, True)\nbar_chart_value_counts('club', 6, True, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I plotted the overall and potential skill ratings for each cluster group, showing that these measures seem to be quite similar across clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot_per_clustergroup(variable_of_interest):\n  fig, ax = plt.subplots(figsize=(10,10))  \n  sns.boxplot(x=original_data['cluster_group'], y=pd.to_numeric(original_data[variable_of_interest], downcast=\"float\"))\n  xlabellist = []\n  for pos in range(n_clusters_):\n    xlabellist.append(most_common_player_position(pos)) \n  plt.xticks(list(range(0, n_clusters_)), xlabellist)\n\nboxplot_per_clustergroup('overall')\nboxplot_per_clustergroup('potential')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next up I wanted to plot the average main and base skill attributes of each cluster group to get a better idea about which skill attributes define the 5 clusters. I did this using radial plots, as they are also used to visualize player stats in the Fifa games"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken and adapted from https://python-graph-gallery.com/392-use-faceting-for-radar-chart/\n \n# Set data\nshooting = []\npassing = []\ndribbling = []\ndefending = []\nphysic = []\npace = []\n\nattribute_names = 'shooting', 'passing', 'dribbling', 'defending', 'physic', 'pace'\nattribute_means = [shooting, passing, dribbling, defending, physic, pace]\n\n\ndef get_attribute_per_cluster_means(attribute_names_list, attribute_means_list):\n  cluster_names = []\n  for group in range(n_clusters_):\n    cluster = original_data[(original_data['cluster_group']==group)]\n    cluster_names.append(most_common_player_position(group))\n    for i, attribute in enumerate(attribute_names_list):\n      mean = np.average(cluster[attribute])\n      attribute_means_list[i].append(mean)\n  dictionary = {'cluster': cluster_names}\n  for i in range(len(attribute_names_list)):\n    dictionary.update({attribute_names_list[i]: attribute_means_list[i]})\n  df_means = pd.DataFrame(dictionary)\n  return df_means\n\ndf_means_main = get_attribute_per_cluster_means(attribute_names, attribute_means)\n\n\n# Radar Chart Function \n# ------- PART 1: Define a function that do a plot for one line of the dataset!\n \ndef make_spider(df_means,row, title, color):\n  # number of variable\n  categories=list(df_means)[1:]\n  N = len(categories)\n  \n  # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n  angles = [n / float(N) * 2 * pi for n in range(N)]\n  angles += angles[:1]\n  \n  # Initialise the spider plot\n  ax = plt.subplot(nrows_, ncols_,row+1, polar=True, )\n  \n  # If you want the first axis to be on top:\n  ax.set_theta_offset(pi / 2)\n  ax.set_theta_direction(-1)\n  \n  # Draw one axe per variable + add labels labels yet\n  plt.xticks(angles[:-1], categories, color='grey', size=8)\n  \n  # Draw ylabels\n  ax.set_rlabel_position(0)\n  plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n  plt.ylim(0,100)\n  \n  # Ind1\n  values=df_means.loc[row].drop('cluster').values.flatten().tolist()\n  values += values[:1]\n  ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n  ax.fill(angles, values, color=color, alpha=0.4)\n  \n  # Add a title\n  plt.title(title, size=11, color=color, y=1.1)\n \n# ------- PART 2: Apply to all individuals\n# initialize the figure\nmy_dpi=96\nplt.figure(figsize=(1750/my_dpi, 1750/my_dpi), dpi=my_dpi)\nif n_clusters_ <= 8:\n  bottom_ = 0.45\n  top_ = 0.9\n  hspace_ = 0.05\nif n_clusters_ > 8 & n_clusters_ <= 12:\n  bottom_ = 0.45\n  top_ = 0.9\n  hspace_ = 0.4\nplt.subplots_adjust(bottom= bottom_, top=top_, hspace = hspace_)\n \n# Create a color palette:\nmy_palette = plt.cm.get_cmap(\"Set2\", len(df_means_main.index))\n \n# Loop to plot\nfor row in range(len(df_means_main.index)):\n  make_spider(df_means=df_means_main, row=row, title='Cluster '+most_common_player_position(row), color=my_palette(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attacking_crossing = []\nattacking_finishing = []\nattacking_heading_accuracy = []\nattacking_short_passing = []\nattacking_volleys = []\nskill_dribbling = []\nskill_curve = []\nskill_fk_accuracy = []\nskill_long_passing = []\nskill_ball_control = []\nmovement_acceleration = []\nmovement_sprint_speed = []\nmovement_agility = []\nmovement_reactions = []\nmovement_balance = []\npower_shot_power = []\npower_jumping = []\npower_stamina = []\npower_strength = []\npower_long_shots = []\nmentality_aggression = []\nmentality_interceptions = []\nmentality_positioning = []\nmentality_vision = []\nmentality_penalties = []\nmentality_composure = []\ndefending_marking = []\ndefending_standing_tackle = []\ndefending_sliding_tackle = []\n\nattribute_names = ['attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing',\n                   'attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',\n                   'movement_acceleration','movement_sprint_speed', 'movement_agility','movement_reactions','movement_balance',\n                   'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength','power_long_shots',\n                   'mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure',\n                   'defending_marking','defending_standing_tackle','defending_sliding_tackle']\nattribute_means = [attacking_crossing,attacking_finishing,attacking_heading_accuracy,attacking_short_passing,\n                   attacking_volleys,skill_dribbling,skill_curve,skill_fk_accuracy, skill_long_passing, skill_ball_control,\n                   movement_acceleration,movement_sprint_speed, movement_agility,movement_reactions,movement_balance,\n                   power_shot_power, power_jumping, power_stamina, power_strength,power_long_shots,\n                   mentality_aggression,mentality_interceptions,mentality_positioning,mentality_vision,mentality_penalties,mentality_composure,\n                   defending_marking,defending_standing_tackle,defending_sliding_tackle]\n\ndf_means_main = get_attribute_per_cluster_means(attribute_names, attribute_means)\n\nmy_dpi=120\nplt.figure(figsize=(25,25), dpi=my_dpi)\nif n_clusters_ <= 8:\n  bottom_ = 0.6\n  top_ = 0.9\n  hspace_ = 0.1\nif n_clusters_ > 8 & n_clusters_ <= 12:\n  bottom_ = 0.45\n  top_ = 0.9\n  hspace_ = 0.1\nplt.subplots_adjust(bottom= bottom_, top=top_, hspace = hspace_, wspace=0.5)\n \n# Loop to plot\nfor row in range(0, len(df_means_main.index)):\n  make_spider(df_means=df_means_main, row=row, title='Cluster '+most_common_player_position(row), color=my_palette(row))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly I also plotted histograms of each cluster for the Height, Weight, Value and Age, aswell as the overall median to compare within cluster frequencies against it."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = 'height_cm', 'weight_kg', 'value_eur', 'age'\nxlabels = 'Height in cm', 'Weight in kg', 'Value in â‚¬', 'Age'\n\nmedians = []\nplottingarrays = []\nfor i in range(len(xlabels)):\n  medians.append([])\n  plottingarrays.append([])\n  for _ in range(n_clusters_):\n    plottingarrays[i].append([])\n\nfor col, savingarray, mediandata in zip(columns, plottingarrays, medians):\n  med = median(np.array(original_data[col]))\n  mediandata.append(med)\n  for group, array in zip(range(n_clusters_), savingarray):\n    cluster = original_data[(original_data['cluster_group']==group)]\n    att_data = np.array(cluster[col])\n    array.append(att_data)\n\ncolours = 10*['g', 'b', 'c', 'y', 'k']\n\nfor pdata, xlabel, c, m in zip(plottingarrays, xlabels, colours, medians):\n  fig = plt.figure(figsize=(sizer*7,3*nrows_))\n  for data, group in zip(pdata, range(n_clusters_)):\n    ax1 = fig.add_subplot(nrows_,ncols_, group+1)\n    ax1.hist(data, bins='auto', histtype='bar', rwidth=0.8, color=c, alpha=0.45, linewidth=4)\n    ax1.axvline(m)\n    plt.xlabel(xlabel)\n    plt.title(most_common_player_position(group))\n  plt.subplots_adjust(hspace = 0.5)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Finding \"best\" players for each cluster**\n\n1. Find the defining attributes for each cluster\n    * only those that have a **significantly higher** mean than the mean of the other clusters together (**as determined via t-test**) \n2. Sum together the F-values obtained from the t-test and divide each F-value by that sum to get a weighting for each defining cluster attribute\n3. Multiply the cluster specific attributes for each player by their weighting factor and then sum those products together to obtain a cluster specific score for each player\n4. For each cluster find the top players based on the cluster-specific weighted score. Also find top players in each cluster for skillsets required for other clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"attribute_names = ['attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing',\n                   'attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',\n                   'movement_acceleration','movement_sprint_speed', 'movement_agility','movement_reactions','movement_balance',\n                   'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength','power_long_shots',\n                   'mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure',\n                   'defending_marking','defending_standing_tackle','defending_sliding_tackle']\n\nfrom scipy.stats import f_oneway, ttest_ind\n\nFvals_list = []\nindex_list = []\nimportant_feature_names_per_cluster = []\nsorted_Fval_array = []\nfor i in range(n_clusters_):\n  Fvals_list.append([])\n  index_list.append([])\n\n\nfor i, F_list, i_list in zip(range(n_clusters_), Fvals_list, index_list):\n  for ii, index in zip(attribute_names, range(len(attribute_names))):\n    cluster = original_data[(original_data['cluster_group']==i)]\n    other_clusters = original_data[(original_data['cluster_group']!=i)]\n    cluster_attribute = np.array(cluster[ii])\n    mean_cluster = np.average(cluster_attribute)\n    other_clusters_attribute = np.array(other_clusters[ii])\n    mean_other_clusters = np.average(other_clusters_attribute)\n    F_onewayResult, p_onewayResult = ttest_ind(cluster_attribute, other_clusters_attribute)\n    if mean_cluster > mean_other_clusters:\n      if p_onewayResult < (0.05/(len(attribute_names)*n_clusters_)):\n        F_list.append(F_onewayResult)\n        i_list.append(ii)\n\n\nfor list_index in range(len(index_list)):\n  ff = np.array(Fvals_list[list_index])\n  sorted_F_vals = sorted(ff, reverse=True)\n  ind = index_list[list_index]\n  sorted_feature_index = [x for _,x in sorted(zip(ff,ind))]\n  important_feature_names_per_cluster.append(sorted_feature_index)\n  sorted_Fval_array.append(sorted_F_vals)\n\nfor i in range(n_clusters_):\n  fig = plt.figure()\n  ax1 = fig.add_subplot(1,1,1)\n  ax1.bar(list(range(len(sorted_Fval_array[i]))), sorted_Fval_array[i])\n  plt.xticks(list(range(len(important_feature_names_per_cluster[i]))), important_feature_names_per_cluster[i])\n  for label in ax1.xaxis.get_ticklabels():\n    label.set_rotation(90)\n\n  plt.title(f'{most_common_player_position(i)}- most important feature: {important_feature_names_per_cluster[i][0]}')\n\n\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_Fvals = []\nfor i in range(n_clusters_):\n  scaled_Fvals.append([])\nfor i, each_list in enumerate(Fvals_list):\n  scaled_list = each_list/np.sum(each_list)\n  scaled_Fvals[i].append(scaled_list)\n\n\nweighted_scores_by_cluster_dfs = []\nfor i in range(n_clusters_):\n  weighted_scores_by_cluster_dfs.append([])\n\nfor group in range(n_clusters_):\n  cluster = original_data[(original_data['cluster_group']==group)]\n  for jj, i_list in enumerate(index_list):\n    df_for_weighting = cluster[i_list]\n    weight_col_list = []\n    for ii, col in enumerate(i_list):\n      df_for_weighting[f'cluster_{most_common_player_position(jj)}_{col}_weighted'] = df_for_weighting[col]*scaled_Fvals[jj][0][ii]\n      weight_col_list.append(f'cluster_{most_common_player_position(jj)}_{col}_weighted')  \n    df_for_weighting['weighted_cluster_specific_score'] = df_for_weighting[weight_col_list].sum(axis = 1)\n    df_for_weighting = df_for_weighting.sort_values(by='weighted_cluster_specific_score', ascending=False)\n    weighted_scores_by_cluster_dfs[group].append(df_for_weighting['weighted_cluster_specific_score'])\n\n\nfor main_cluster in weighted_scores_by_cluster_dfs:\n  for i, cluster in enumerate(main_cluster):\n    for wscore, Uindex in zip(cluster.values, cluster.index):\n      original_data.loc[Uindex, f'{most_common_player_position(i)}_score'] = wscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_names_savarrs = []\nfor i in range(n_clusters_):\n  top_names_savarrs.append([])\n\nfor i in range(n_clusters_):\n  cluster = original_data[(original_data['cluster_group']==i)]\n  for j in range(n_clusters_):\n    columns = ['short_name','player_positions',f'{most_common_player_position(j)}_score','overall']\n    cluster_ranked_names = cluster[columns].sort_values(by=f'{most_common_player_position(j)}_score', ascending=False)\n    cluster_ranked_names = cluster_ranked_names.round(2)\n    top_names = cluster_ranked_names.reset_index()\n    top_names.drop(['index'], axis=1, inplace=True)\n    top_names_savarrs[i].append(top_names)\n\n\nfor i, cluster_savarr in enumerate(top_names_savarrs):\n  all_topnames_per_cluster_df = pd.concat([savarr for savarr in cluster_savarr], axis=1)\n  print('Cluster Group:',most_common_player_position(i))\n  head_df = all_topnames_per_cluster_df.head(10)\n  display(HTML(head_df.to_html()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These tables for each cluster group give a pretty good idea of the \"best\" players in each cluster for each cluster-specific skillset."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}