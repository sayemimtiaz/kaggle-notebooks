{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Read in data from pandas\nimport pandas as pd\n\n# This is used for fast string concatination\nfrom io import StringIO\n\n# Use nltk for valid words\nimport nltk\n# Need to make hash 'dictionaries' from nltk for fast processing\nimport collections as co\n\n\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\n\n# Read the input\nd = pd.read_csv(\"../input/consumer_complaints.csv\") # the consumer dataset is now a Pandas DataFrame\n# Only interested in data with consumer complaints\nd=d[d['consumer_complaint_narrative'].notnull()]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# We want a very fast way to concat strings.\n#  Try += if you don't believe this method is faster.\ns=StringIO()\nd['consumer_complaint_narrative'].apply(lambda x: s.write(x))\n\nk=s.getvalue()\ns.close()\nk=k.lower()\nk=k.split()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Next only want valid strings\nwords = co.Counter(nltk.corpus.words.words())\nstopWords =co.Counter( nltk.corpus.stopwords.words() )\nk=[i for i in k if i in words and i not in stopWords]\ns=\" \".join(k)\nc = co.Counter(k)\n"},{"cell_type":"markdown","metadata":{},"source":"## At this point we have k,s and c\n**k** Array of words, with stop words removed\n\n**s** Concatinated string of all comments\n\n**c** Collection of words"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Take a look at the 14 most common words\nc.most_common(14)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"s[0:100]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print(k[0:10],\"\\n\\nLength of k %s\" % len(k))"},{"cell_type":"markdown","metadata":{},"source":"## Word Cloud\nAt this point we have some data, so it might be a good idea to take a look at it.\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from wordcloud import WordCloud\n\n# Read the whole text.\ntext = s\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nimport matplotlib.pyplot as plt\n\n\n# take relative word frequencies into account, lower max_font_size\nwordcloud = WordCloud(background_color=\"white\",max_words=len(k),max_font_size=40, relative_scaling=.8).generate(text)\nplt.figure()\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"## Taking a look at their stories\nThese stories claim to involve identity theft and or fraud.\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's get some text involving identity theft\nsearchS='victim of identity theft'\nvi = d[d['consumer_complaint_narrative'].str.find(searchS) >= 0]\nd['victim']=None\nd['e']=1\nd['m']=None  # This will be for 'Closed with monetary relief'\nd['victim'] = d[d['consumer_complaint_narrative'].str.find(searchS) >= 0]\nd['m']=d[d['company_response_to_consumer'] == 'Closed with monetary relief']\n\n\n# Take a look at some sample stories  mindex to mindex_inc\n# Adjust this, to see different stories\nmindex=20\nmindex_inc=5+mindex\nsi=StringIO()\nvi['consumer_complaint_narrative'].iloc[mindex:mindex_inc].apply(lambda x: si.write(x+'\\n___\\n\\n'))\n\nt=si.getvalue()\nsi.close()\nprint(t)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# We might be missing data on just fraud...\n# Search for all cases of theft or fraud\nsearchS0='victim'\nsearchS1='identity'\nsearchS_OR=['theft','fraud']\n\nvi2 = d[(d['consumer_complaint_narrative'].str.find(searchS0) >= 0) &\n        (d['consumer_complaint_narrative'].str.find(searchS1) >= 0) &\n       ( (d['consumer_complaint_narrative'].str.find(searchS_OR[0]) >= 0) |\n        (d['consumer_complaint_narrative'].str.find(searchS_OR[1]) >= 0))\n        ]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# vi2.count()\n\ng=vi2.groupby(['issue'])\ngg=g.count().reset_index()\ngg.sort_values(by='e',inplace=True)\ngg=g['e','victim','m'].count().reset_index()\ngg.sort_values(by='e',inplace=True, ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Taking a look at common complaints\n# Need to format this...but note only 9 cases where it\n# was \"Closed with monetary relief\"  m==1\n\n#gg.head(4)\nwith pd.option_context('display.max_rows', 10, 'display.max_columns', 4):\n    print(gg)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages\n# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats\n# For example, here's several helpful packages to load in \n\nlibrary(ggplot2) # Data visualization\nlibrary(readr) # CSV file I/O, e.g. the read_csv function\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nsystem(\"ls ../input\")\n\n# Any results you write to the current directory are saved as output."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}