{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DnD 5e Monsters CR Analysis\nDnD 5e has a plethora of monsters in its universe. A DM who is designing a combat encounter for their players usually builds the encounter by using monsters with a **Challenge Rating (CR)** that is similar to the adventuring party's average level. However, many DMs like to create their own monsters to bring something unique to their encounter. But how do you decide a custom monster's CR? To answer this question and to better understand which characteristics are driving factors I will be building a machine learning model to predict a monster's CR."},{"metadata":{},"cell_type":"markdown","source":"# Data Files\nI'll be using and modifying the Dataset uploaded by [mrpantherson][1]. This data was scraped from [AideDD][2] and additional information on the dataset can be found [here][3]. In addition a second anonymous dataset was used to update some missing information and it can be found [here][4]\n\n[1]: https://www.kaggle.com/mrpantherson\n[2]: https://www.aidedd.org/\n[3]: https://www.kaggle.com/mrpantherson/dnd-5e-monsters\n[4]: https://docs.google.com/spreadsheets/d/1FIjaz6S0JXrXaCVhHEDeq-nH7xHzlqAx6inuRbDjhjU/edit?usp=sharing"},{"metadata":{},"cell_type":"markdown","source":"# Formatting the Data\nThe 'Name' columns in both datasets need both be reformatted so that they can merge correctly. Additionally some of the columns need to have their data type changed to reflect their numerical nature. The target column 'cr' needs to have certain values modified before its data type can be changed (some of the values are fractions denoted as strings like '1/4'). 'cr' also needs to be seperated from the dataframe since it is the target."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read in datasets\nmonsters = pd.read_csv('/kaggle/input/dnd-5e-monsters/dnd_monsters.csv')\nability_scores = pd.read_csv('/kaggle/input/dnd-monsters-ability-scores/DnD Monster Ability Scores.csv')\n\n# reformat the name column in both datasets so dataframes can merge properly on 'Name'\nability_scores['Name'] = ability_scores['Name'].str.lower()\nability_scores['Name'] = ability_scores['Name'].str.replace('-',' ')\nmonsters = monsters.rename(columns = {'name': 'Name'})\nmonsters['Name'] = monsters['Name'].str.replace('-',' ')\n\n# remove obsolete columns that will be filled from other dataset\nmonsters.drop(['str', 'dex', 'con', 'int', 'wis', 'cha'], axis=1, inplace=True)\n\n# set Int64 data type to reflect the numerical nature of these columns\nability_scores = ability_scores.astype({'STR': 'Int64', 'DEX': 'Int64', 'CON': 'Int64', 'INT': 'Int64', 'WIS': 'Int64', 'CHA': 'Int64'})\n\n# convert the string fractions to decimal and set 'cr' as the float64 data type\nmonsters.loc[monsters['cr'] == '1/4', 'cr'] = .25\nmonsters.loc[monsters['cr'] == '1/2', 'cr'] = .5\nmonsters.loc[monsters['cr'] == '1/8', 'cr'] = .125\nmonsters.cr = monsters.cr.astype('float64')\n\n# remove rows with 0 or null for 'cr' (they are considered errors) and seperate target from predictors\nmonsters = monsters.drop(monsters.loc[monsters['cr']==0].index)\nmonsters = monsters.drop(monsters.loc[monsters['cr'].isna()].index)\nmonsters_full = monsters[monsters.columns] #for later use\ncr = monsters.cr\nmonsters.drop(['cr'], axis=1, inplace=True)\n\n\n# convert binary categorical variable to 1's and 0's\nmonsters.loc[monsters['legendary'] == 'Legendary', 'legendary'] = 1\nmonsters['legendary'].fillna(0, inplace=True)\n\n# merge data sets on 'Name' column\nmonsters = monsters.merge(ability_scores, on='Name', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features\nImportant information for each column from the monsters dataframe is listed below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the features identifying their total unique values, number of missing values, and data type\nprint(pd.concat([monsters.nunique(),monsters.isna().sum(), monsters.dtypes], \n                axis=1).rename(columns={0:'Unique_Values',1:'Missing_Values', 2:'Data_Type'}))\nprint('Total Rows:', len(monsters.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Columns to ignore\n'name', 'url', 'align', and 'source' will not be useful in this model. Correlations could be identified between both 'align' and 'source' to 'cr', but they do not serve as useful predictors."},{"metadata":{"trusted":true},"cell_type":"code","source":"monsters.drop(['Name', 'url', 'align', 'source'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Columns to adjust\n'type' could be a useful categorical attribute to the model. However there is a problem: subtypes are included in parenthesis in certain rows, causing the column to have too high a cardinality for effective one-hot encoding. See below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(monsters['type'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cardinality can be greatly reduced by removing the subtypes in parenthesis of each string value"},{"metadata":{"trusted":true},"cell_type":"code","source":"monsters['type'] = monsters['type'].str.split(' ').str[0]\nmonsters['type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The types of monsters have lost some specificity but they can now be used as a feature in the model."},{"metadata":{},"cell_type":"markdown","source":"## Columns that remain unchanged\n'ac', 'hp', 'speed', the ability scores ('STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA'), and 'legendary' were unchanged besides data type formatting.\n\n\n\n# Missing Values\nsome of the columns in the dataframe are missing values. The columns with categorical data are not actually missing data but the missing numerical values in 'STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA' need to be addressed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# shows the features with both their total missing values and percentage of data missing\nscore_names = ['STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA']\nprint('Original:', pd.concat([\n    monsters[score_names].isna().sum(),\n    monsters[score_names].isna().sum().divide(len(monsters.index)).map(lambda n: '{:.2%}'.format(n))],axis=1).\n      rename(columns={0:'Missing_Values',1:'Percent_of_Data_Missing'}))\nprint('Total Rows:', len(monsters.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-NN Imputation of missing values\n\nTo fill these missing values KNNImputer will be utilized. All features other than the ones removed previously will be utilized to help improve the imputation. For the categorical features, drop_first will be enabled when encoding to avoid Dummy Variable Trap. The Dataframe will also be normalized via MinMaxScaler to reduce bias during imputation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import imputer and scaler from sklearn\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# encode categorical variables, dropping the first of each\ncat_variables = monsters[['type', 'size', 'speed']]\ncat_dummies = pd.get_dummies(cat_variables, drop_first=True)\n\n# create the dataframe for imputation\nimp_df = monsters.drop(['type', 'size', 'speed'], axis=1)\nimp_df = pd.concat([imp_df, cat_dummies], axis=1)\n\n# apply scaler to imputation dataframe\nscaler = MinMaxScaler()\nimp_df = pd.DataFrame(scaler.fit_transform(imp_df), columns=imp_df.columns)\n\n# apply imputation to missing values in dataframe and check for sucess\nimputer = KNNImputer(n_neighbors=5)\nimp_df = pd.DataFrame(imputer.fit_transform(imp_df), columns=imp_df.columns)\n\n# revert scaler, round imputed values to integers and change the data type to int64\nimp_df = pd.DataFrame(scaler.inverse_transform(imp_df), columns=imp_df.columns)\nimp_df[score_names] = round(imp_df[score_names],0)\nimputed_scores = imp_df[['STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA']].astype('int64', errors='ignore')\n\n# update main dataframe with imputed values\nmonsters = pd.concat([monsters.drop(score_names, axis=1), imputed_scores], axis=1)\nprint(pd.concat([monsters.nunique(),monsters.isna().sum(), monsters.dtypes], \n                axis=1).rename(columns={0:'Unique_Values',1:'Missing_Values', 2:'Data_Type'}))\nprint('Total Rows:', len(monsters.index))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Model\nA gradient boosting model (XGBoost) will be used to predict cr. The model will be evaluated using k-fold cross validation MAE as the small size of the data prevents a test set being extracted. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import XGBoost, sklearn functions, and numpy sort\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom numpy import sort\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\n\n\n# Specify features (currently all are included besides those mentioned in 'Columns to Ignore')\nfeature_cols = ['ac', 'hp', 'legendary', 'STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA',  'type', 'size', 'speed']\n\n# encode data from desired features\nfeatures = pd.get_dummies(monsters[feature_cols])\n\n# split validation set from training data\nm_train, m_val, cr_train, cr_val = train_test_split(features, cr, train_size=.8, test_size=.2, random_state=0)\n\n# define model\nmodel = XGBRegressor(n_estimators=1000, learning_rate=.01)\nmodel.fit(m_train, cr_train, early_stopping_rounds=50, eval_set=[(m_val, cr_val)], verbose=False)\n\n# generate MAEs\nmae = -1*cross_val_score(model, features, cr, cv=10, scoring='neg_mean_absolute_error')\n\n# generate accuracy score\nacc = model.score(m_val, cr_val)\n\n# print model accuracy and average MAE\nprint(\"Average MAE across 5 folds:\", mae.mean(),'\\nAccuracy:',\"{:.3%}\".format(acc))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance and Selection\nThe feature importances will be identified and then used for feature selection using SelectFromModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# disables SettingWithCopyWarning from displaying\npd.set_option('mode.chained_assignment', None)\n\n# identify feature importances\ntemp = model.feature_importances_\ncolumn_names = m_train[0:0]\ncolumn_names.loc[len(column_names)] = temp\nfeature_importance = column_names.transpose().rename(columns= {0: 'feature importance'}).sort_values(by=['feature importance'], axis=0, ascending=False)\nprint(feature_importance)\n\n# plot F-scores of each feature\nplt.rcParams[\"figure.figsize\"] = (14, 7)\nplot_importance(model)\nplt.show()\n\n# iterate by thresholds of feature importance to determine their effect on the model ## NOTE: this section currently does not work as intended and needs to be revisited\n# thresholds = sort(model.feature_importances_)\n# for thresh in thresholds:\n    \n#     # select features using threshold\n#     selection = SelectFromModel(model, threshold=thresh, prefit=True)\n#     select_m_train = selection.transform(m_train)\n    \n#     # train selection model\n#     selection_model = XGBRegressor(n_estimators=1000, learning_rate=.01)\n#     selection_model.fit(select_m_train, cr_train)\n\n#     # genearte and display mae of selection model\n#     mae = -1*cross_val_score(selection_model, features, cr, cv=10, scoring='neg_mean_absolute_error')\n#     print(\"Thresh=%.3f, n=%d, mae: %.3f\" % (thresh, select_m_train.shape[1], mae.mean()))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nThe results of the model were mostly unsurprising. Many have anecdotally claimed that \"CR is measured only by ac and hp\" and it seems these claims have some merit. While ac interestingly does not have much importance to the model hp stands out as the dominating feature. A potential reason for such high feature importance is that hp tends to scale linearly with the cr of most monsters, while other features do not have anywhere near as strong a relationship. cr and ac also have a linear correlation as seen below but it is not as strong as cr and hp."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.lmplot(x= 'cr', y= 'hp', data=monsters_full, fit_reg=True).fig.suptitle(\"hp and cr\")\nsns.lmplot(x= 'cr', y= 'ac', data=monsters_full, fit_reg=True).fig.suptitle(\"ac and cr\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}