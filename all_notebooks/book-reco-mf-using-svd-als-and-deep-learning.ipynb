{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"**Matrix Factorization - there is more than one way to do it**\n\nThe purpose of this notebook is to illustrate one of the most widely used ways of generating recommendations using low rank matrix factorization using three alternate methods of doing it.\n* Singular Value Decomposition (SVD)\n* Alternating Least Squares\n* Deep Learning (the example uses TF2.x, but one can use Pytorch to generate similar results)\n\nMatrix Factorization involves decomposing a large matrix (usally an User-Item matrix in the context of item personalization) into two smaller latent factor matrices. The dot product of these two matrices approximates the already known ratings and at the same time is able to produce ratings for user-item combinations which had no values in the original matrix"},{"metadata":{},"cell_type":"markdown","source":"Note: results may vary in each run as I have not seeded everything, but core essence of the notebook remains the same."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image,display\nImage(\"../input/mat-fact/Diagram-of-matrix-factorization.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an ongoing work and I will add more details to this. Please upvote if you found this useful.\n"},{"metadata":{},"cell_type":"markdown","source":"**Credits\n* http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf\n* https://github.com/jeffheaton/t81_558_deep_learning\n* https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization\n* https://beckernick.github.io/matrix-factorization-recommender/\n* https://www.kaggle.com/vikashrajluhaniwal/matrix-factorization-recommendation-using-pyspark"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nfrom scipy.sparse.linalg import svds\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport matplotlib.image as mpimgimport \nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\nfrom statistics import mean\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom random import shuffle  \nfrom zipfile import ZipFile\n##Deep Learning specific stuff\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Concatenate\nfrom tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import model_to_dot\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dropout, Flatten,Activation,Input,Embedding\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import dot\nfrom tensorflow.keras.models import Model\n\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/book*.csv\"):\n    df = pd.read_csv(file)\n    if book_rating.empty:\n        book_rating = df\n    else:\n        book_rating.append(df, ignore_index=True)\n\n        \nuser_rating = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/user_rating*.csv\"):\n    df = pd.read_csv(file)\n    if user_rating.empty:\n        user_rating = df\n    else:\n        user_rating.append(df, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating.shape,user_rating.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at the two tables"},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the purpose of illustrating Collaborative filetring, we can use user_rating as it is in the format where we have Users who have rated specific books. We will skip the cases where there is no book name and no rating as well (as shown in the first few rows)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of unique users in the user_rating table :\"+str(user_rating['ID'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ratings are given in sentences, let's convert them to numeric field on a scale of 0-5 (where 0=No rating)"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nuser_rating['Rating_numeric'] = le.fit_transform(user_rating.Rating.values)\nuser_rating.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Training and Validation set for the recommender system**\n\nfor this purpose, we first take only those customers whose ratings are > 0 i.e. they have given some sort of rating\nand then take a random sample out of them i.e. we hide few of the previously rated books as the test set. Let's see an \nexample of how this is to be done."},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_pos = user_rating[user_rating['Rating_numeric']>0]\nuser_rating_zero = user_rating[user_rating['Rating_numeric']==0]\npos_rating_summary=user_rating_pos[['ID', 'Rating_numeric']].groupby(['ID']).agg(['count'])\npos_rating_summary.columns = ['_'.join(col) for col in pos_rating_summary.columns.values]\npos_rating_summary.reset_index(inplace=True)\nplt.figure(figsize = (8, 8))\nplt.title('Number of Ratings Density plot')\nsns.kdeplot(pos_rating_summary['Rating_numeric_count'], color=\"blue\", shade=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a long tail, where some customers do provide a large number of ratings, whereas bulk of the people seem to have more than 5 ratings at least. Let's do a quick check to confirm this"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kk = (len(pos_rating_summary[pos_rating_summary['Rating_numeric_count']>=5])/len(pos_rating_summary))*100\nprint(\"Percentage of people with 5 or more reviews: \"+ str(kk))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will choose our test train split from these set of users who have atleast 5 or more reviews. This will ensure that if we take a 80-20 split, we have atleast 1 book that is held out for test predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"eligible_customer_list = pos_rating_summary[pos_rating_summary['Rating_numeric_count']>=5].ID.tolist()\nuser_rating_eligible = user_rating_pos[user_rating_pos.ID.isin(eligible_customer_list)]\nuser_rating_NotEligible = user_rating_pos[~user_rating_pos.ID.isin(eligible_customer_list)]\nuser_rating_NotEligible.shape,user_rating_eligible.shape,user_rating_pos.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Create test and train split\nsplit_idx = np.random.rand(len(user_rating_eligible)) < 0.8\nuser_rating_train_temp = user_rating_eligible[split_idx]\nuser_rating_test = user_rating_eligible[~split_idx]\nuser_rating_train = user_rating_train_temp.append(user_rating_NotEligible, ignore_index=True)\nuser_rating_train.shape, user_rating_train_temp.shape,user_rating_NotEligible.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Pre-processing for Matrix Factorization**\n\nFor using SVD to do Matrix Factorization we need to convert it into wide format from Long format."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nb_users  = user_rating_train['ID'].nunique()\nnb_books = user_rating_train['Name'].nunique()\n\nprint(\"There are %d unique users and %d unique books; so we need to prepare \" \n      \"an matrix of size %d by %d.\" %(nb_users, nb_books, nb_users, nb_books))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_wide = user_rating_train.pivot(index = 'ID', columns ='Name', values = 'Rating_numeric').fillna(0)\nuser_rating_wide.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_matrix = user_rating_wide.to_numpy()\nuser_ratings_mean = np.mean(user_rating_matrix, axis = 1)\nuser_rating_matrix_dm = user_rating_matrix - user_ratings_mean.reshape(-1, 1) ##Normalizing the ratings here, one can try a version without\n                                                                              ##this and see if the recommendations are any better/worse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do matrix factorization using SVD now"},{"metadata":{"trusted":true},"cell_type":"code","source":"## k is a hyperparam here\n## U and V are User latent matrix and V is Books latent matrix in our case\nU, sigma, V = svds(user_rating_matrix_dm, k = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma = np.diag(sigma)\nall_user_predicted_ratings = np.dot(np.dot(U, sigma), V) + user_ratings_mean.reshape(-1, 1)\npreds_df = pd.DataFrame(all_user_predicted_ratings, columns = user_rating_wide.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"kk=pd.DataFrame(user_rating_wide.index.values)\nkk.rename(columns={ kk.columns[0]: \"ID\" }, inplace = True)\npreds_df=pd.concat([kk,preds_df],axis=1)\npreds_df.rename(columns={ preds_df.columns[0]: \"ID\" }, inplace = True)\npreds_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compare the predictions**\n\nWe will compare th overlap of recommendations at Top-k with the hold out set and see how the recommender system performed in general.\n\nWhy do we do choose such a measure, instead of other metrics like MAE to compare absolute ratings?\n\n\nLet's look at a sample book recommender at one of India's largest e-retailer (flipkart)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/bookrecoflp/Book-reco-flipkart.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you may see that actual ratings are whole numbers between (1-5), and we have filtered for 4-start reviews only in the example above. The ordering of the recommendation is not just a function of average rating. From the point of deploying the recommendation in production, its always done in a way where a multiple books are shown at once (8-12 typically based on the UI). Thus in many practical applications, one is interested if the top-K (in our example case k=8) overlap and as long as we can get maximum coverage, the relative order doesn't matter.\n\n\n\np.s. * - the opinion on evaluation metric in this case is purely based on my personal exp of having worked on many large scale industrial recommender systems that span across Manufacturing, QSR, Restaurant, e-commerce and Finanacial services. Others may disgaree to this."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"##Function for checking overlap for one specific user\ndef check_overlap(UserId,top_recos_to_check):\n    samp_cust = user_rating_test[user_rating_test['ID']==UserId]\n    samp_cust.sort_values(by='Rating_numeric', ascending=False, inplace=True)\n    book_name_testcust = samp_cust.Name.unique().tolist()\n    available_actual_ratings = samp_cust.shape[0]\n    rows_to_fetch = min(available_actual_ratings,top_recos_to_check)\n    preds_df_sampcust = preds_df[preds_df['ID']==UserId]\n    if preds_df_sampcust.shape[0] ==0:\n        pass\n    elif preds_df_sampcust.shape[0] >0:\n        preds_check_cust = preds_df_sampcust.T\n        preds_check_cust.reset_index(inplace=True)\n        preds_check_cust.rename(columns={ preds_check_cust.columns[0]: \"Name\" }, inplace = True)\n        #preds_check_cust = preds_df_sampcust_T[preds_df_sampcust_T['Name']!='ID']\n        preds_check_cust.rename(columns={ preds_check_cust.columns[1]: \"Ratings_normalized_predicted\" }, inplace = True)\n        preds_check_cust.sort_values(by='Ratings_normalized_predicted', ascending=0, inplace=True)\n        preds_check_cust_check = preds_check_cust[preds_check_cust.Name.isin(book_name_testcust)]\n        actual_rating = samp_cust.iloc[0:rows_to_fetch,:]\n        pred_rating = preds_check_cust_check.iloc[0:rows_to_fetch,:]\n        overlap = pd.Series(list(set(actual_rating.Name).intersection(set(pred_rating.Name))))\n        pct_overlap = (len(overlap)/rows_to_fetch)*100\n        #print(\"Percentage of overlap in top\"+str(top_recos_to_check)+\" for User ID - \"+str(UserId)+\" : \"+str(pct_overlap))\n        return pct_overlap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_user_list = user_rating_test.ID.unique().tolist()\noverlap_summary={}\ntop_recos_to_check =10\nfor users in test_user_list:\n    if check_overlap(users,top_recos_to_check) is not None:\n        overlap_summary[users]= check_overlap(users,top_recos_to_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_summary = sorted(overlap_summary.items(), key=lambda x: x[1], reverse=True)\nmax_overlap = np.array(list(overlap_summary.values())).max()\nmin_overlap = np.array(list(overlap_summary.values())).min()\nmean_overlap = np.array(list(overlap_summary.values())).mean()\nprint(\"Max overlap in top\" +str(top_recos_to_check)+ \" books \"+str(max_overlap))\nprint(\"Min overlap in top \"+str(top_recos_to_check)+ \" books \"+str(min_overlap))\nprint(\"Average overlap in top \"+str(top_recos_to_check)+ \" books \"+str(mean_overlap))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, on an average we are able to recommend the right books decent number of the times. This was set up with very little effort. Let's see if we can improve this further by using Alternating Least Squares.\n\nFor this we will use Pyspark where there is a very good implementation of ALS"},{"metadata":{},"cell_type":"markdown","source":"**Alternating Least Squares**"},{"metadata":{},"cell_type":"markdown","source":"In any matrix factorization problem, we are trying to find out a relatively small number *k* and approximate each user *u* with a \n*k* dimensional vector *xu* and each book *i* with a *k* dimensional vector *yi* . These vectors are referred as factors. Then to predict user *u*s rating for book *i* , we simply predict *r~xu.Tyi* . In matrix notation it looks like as follows:\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/alspics/x-y-individual.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To estimate complete ratings, this is formulated as an optimization excercise (given below). Here we minimize the least squared errors of the observed ratings (along with regularization term)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/alspics/als-loss-function.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that this objective function is non-convex and is NP-hard to optimize. Gradient descent can be used as an approximate approach here, however it turns out to be slow and costs lots of iterations. Note however, that if we fix the set of variables X and treat them as constants, then the objective is a convex function of Y and vice versa. In ALS, we fix Y and optimize X, then fix X and optimize Y , and repeat until convergence (algo shown below in picture)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/alspics/als-algo.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark       #installing pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext, SQLContext   # required for dealing with dataframes\nimport numpy as np\nfrom pyspark.ml.recommendation import ALS      # for Matrix Factorization using ALS ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SparkContext()      # instantiating spark context \nsqlContext = SQLContext(sc) # instantiating SQL context ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the ALS implementation in pyspark, user id and book ids need to be in Integer format. Let's create book id and create the required dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"book_names_train = user_rating_train[['Name']]\nbook_names_test = user_rating_test[['Name']]\nbook_name = pd.concat([book_names_train,book_names_test],axis=0)\nbook_name.drop_duplicates(inplace=True)\nbook_name['Book_id']= book_name.index.values\nbook_name.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_train = pd.merge(user_rating_train,book_name, on='Name',how='left')\nuser_rating_test = pd.merge(user_rating_test,book_name, on='Name',how='left')\nuser_rating_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_train_ = user_rating_train[['ID','Book_id','Rating_numeric']]\nuser_rating_test_ = user_rating_test[['ID','Book_id','Rating_numeric']]\nuser_rating_train_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_train_.to_csv('user_rating_train.csv', index=False)\nuser_rating_test_.to_csv('user_rating_test.csv', index=False)\nuser_rating_train_sp = sqlContext.read.csv('user_rating_train.csv',header = True, inferSchema = True)\nuser_rating_test_sp = sqlContext.read.csv('user_rating_test.csv',header = True, inferSchema = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating_train_sp.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"als = ALS(userCol=\"ID\",itemCol=\"Book_id\",ratingCol=\"Rating_numeric\",rank=20, maxIter=10, seed=0, )\nmodel = als.fit(user_rating_train_sp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.userFactors.show(5, truncate = False)  # displaying the latent features for five user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.transform(user_rating_test_sp[[\"ID\",\"Book_id\"]]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratesAndPreds = user_rating_test_sp.join(other=predictions,on=['ID','Book_id'],how='inner').na.drop() \nratesAndPreds.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the columns into numpy arrays for direct and easy calculations \nrating = np.array(ratesAndPreds.select(\"Rating_numeric\").collect()).ravel()\nprediction = np.array(ratesAndPreds.select(\"prediction\").collect()).ravel()\nprint(\"RMSE : \", np.sqrt(np.mean((rating - prediction)**2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avp_als= ratesAndPreds.toPandas()\navp_als.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_overlap(UserId,top_recos_to_check):\n    samp_cust = avp_als[avp_als['ID']==UserId][['ID','Rating_numeric','Book_id']]\n    samp_cust.sort_values(by='Rating_numeric', ascending=False, inplace=True)\n    available_actual_ratings = samp_cust.shape[0]\n    rows_to_fetch = min(available_actual_ratings,top_recos_to_check)\n    preds_df_sampcust = avp_als[avp_als['ID']==UserId][['ID','prediction','Book_id']]\n    preds_df_sampcust.sort_values(by='prediction', ascending=False, inplace=True)\n    actual_rating = samp_cust.iloc[0:rows_to_fetch,:]\n    pred_rating = preds_df_sampcust.iloc[0:rows_to_fetch,:]\n    overlap = pd.Series(list(set(actual_rating.Book_id).intersection(set(pred_rating.Book_id))))\n    pct_overlap = (len(overlap)/rows_to_fetch)*100\n    #print(\"Percentage of overlap in top\"+str(top_recos_to_check)+\" for User ID - \"+str(UserId)+\" : \"+str(pct_overlap))\n    return pct_overlap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_user_list = avp_als.ID.unique().tolist()\noverlap_summary={}\ntop_recos_to_check =10\nfor users in test_user_list:\n    overlap_summary[users]= check_overlap(users,top_recos_to_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_summary = sorted(overlap_summary.items(), key=lambda x: x[1], reverse=True)\nmax_overlap = np.array(list(overlap_summary.values())).max()\nmin_overlap = np.array(list(overlap_summary.values())).min()\nmean_overlap = np.array(list(overlap_summary.values())).mean()\nprint(\"Max overlap in top\" +str(top_recos_to_check)+ \" books \"+str(max_overlap))\nprint(\"Min overlap in top \"+str(top_recos_to_check)+ \" books \"+str(min_overlap))\nprint(\"Average overlap in top \"+str(top_recos_to_check)+ \" books \"+str(mean_overlap))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, that's a huge improvement from where we started using plain vanilla Matrix Factorization using SVD.\n\nLet's see if we doing Matrix factorization using a Deep Neural network does anything better."},{"metadata":{},"cell_type":"markdown","source":"**Let's do MF using Deep Learning**\n\nBefore we jump into creating the network to mimic Matrix Factorization, let's understand the building blocks in a bit more details. The most crucial piece in this is the Embedding layer. The piece that we are referring to is 'tf.keras.layers.Embedding'.\n\nPeople most often use Embedding layer in case of sequences, but that doesn't always have to be the case.\nHere the first argument is input_dim, i.e. input dimension size. In our case when we create embedding for users\nthe input dimension is the number of unique users, which we encode into a lower numbered vectors (it's almost similar to \ndimension reduction execrcise). In our case there isn't anything for input_length as that is useful for sequences (like in NLP \nuse cases). Think of the embedding layer as a look up table. Let's understand this in bit more details."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nembedding_layer = Embedding(input_dim=10, output_dim=4, input_length=2)\nmodel.add(embedding_layer)\nmodel.compile('adam', 'mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n\nNow, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = np.array([\n    [1,2]\n])\n\npred = model.predict(input_data)\n\nprint(input_data.shape)\nprint(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see two length-4 vectors that Tensorflow looked up for each of the input integers. Recall that Python arrays are zero-based. Tensorflow replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, Tensorflow replaced the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer.get_weights()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values above are random parameters that Tensorflow generated as starting points. In our case the network trains these embeddings to learn values that are able to minimize the loss function. In our case we create embeddings for users and books, and then combine them using dot product (i.e. tf.keras.layers.dot) to simulate a case where we are trying minimize mean squared error to replicate the actual ratings. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"##Reading the datafiles again to avoid any confusion\nbook_rating = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/book*.csv\"):\n    df = pd.read_csv(file)\n    if book_rating.empty:\n        book_rating = df\n    else:\n        book_rating.append(df, ignore_index=True)\n\n        \nuser_rating_temp = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/user_rating*.csv\"):\n    df = pd.read_csv(file)\n    if user_rating_temp.empty:\n        user_rating_temp = df\n    else:\n        user_rating_temp.append(df, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_map = user_rating_temp[['Name']]\nbook_map.drop_duplicates(subset=['Name'],keep='first',inplace=True)\nbook_map['Book_Id']=book_map.index.values\nuser_rating_temp = pd.merge(user_rating_temp,book_map, on='Name', how='left')\nuser_rating = user_rating_temp[user_rating_temp['Name']!='Rating'] ##Dropping users who have not rated any books\nuser_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nuser_rating['Rating_numeric'] = le.fit_transform(user_rating.Rating.values)\nusers = user_rating.ID.unique()\nbooks = user_rating.Book_Id.unique()\n##Train-test split  - keeping 80%-20% for simplicity. But one can create a k-fold set up for better accuracy as well\nuserid2idx = {o:i for i,o in enumerate(users)}\nbookid2idx = {o:i for i,o in enumerate(books)}\nuser_rating['ID'] = user_rating['ID'].apply(lambda x: userid2idx[x])\nuser_rating['Book_Id'] = user_rating['Book_Id'].apply(lambda x: bookid2idx[x])\ny=user_rating['Rating_numeric']\nX=user_rating.drop(['Rating_numeric'],axis=1)\n####\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(X_train.shape , X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(dropout,latent_factors):\n    n_books=len(user_rating['Book_Id'].unique())\n    n_users=len(user_rating['ID'].unique())\n    n_latent_factors=latent_factors  # hyperparamter to deal with. \n    user_input=Input(shape=(1,),name='user_input',dtype='int64')\n    user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding',embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=42))(user_input)\n    user_vec =Flatten(name='FlattenUsers')(user_embedding)\n    user_vec=Dropout(dropout)(user_vec)\n    book_input=Input(shape=(1,),name='book_input',dtype='int64')\n    book_embedding=Embedding(n_books,n_latent_factors,name='book_embedding',embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=42))(book_input)\n    book_vec=Flatten(name='FlattenBooks')(book_embedding)\n    book_vec=Dropout(dropout)(book_vec)\n    sim=dot([user_vec,book_vec],name='Similarity-Dot-Product',axes=1)\n    nn_inp=Dense(128,activation='relu')(sim)\n    nn_inp=Dropout(dropout)(nn_inp)\n    nn_inp=Dense(64,activation='relu')(nn_inp)\n    nn_inp=BatchNormalization()(nn_inp)\n    nn_inp=Dense(1,activation='relu')(nn_inp)\n    nn_model =Model([user_input, book_input],nn_inp)\n    return nn_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model = build_model(0.4,65)\nnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model.compile(optimizer=Adam(lr=1e-4),loss='mse')\nbatch_size=128\nepochs=5\nHistory = nn_model.fit([X_train.ID,X_train.Book_Id],y_train, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([X_test.ID,X_test.Book_Id],y_test),\n                              verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"preds = nn_model.predict([X_test.ID,X_test.Book_Id])\navp = (preds,y_test)\ndf_id = pd.DataFrame(np.array(X_test.ID))\ndf_Book_id = pd.DataFrame(np.array(X_test.Book_Id))\ndf_actual_rating = pd.DataFrame(np.array(y_test))\ndf_preds = pd.DataFrame(preds)\ndfList = [df_id, df_Book_id,df_actual_rating,df_preds]  # List of your dataframes\navp = pd.concat(dfList,ignore_index=True,axis=1)\n#new_df = pd.concat([new_df,df_preds],ignore_index=True,axis=1)\navp.rename(columns={ avp.columns[0]: \"ID\" }, inplace = True)\navp.rename(columns={ avp.columns[1]: \"Book_Id\" }, inplace = True)\navp.rename(columns={ avp.columns[2]: \"Rating_numeric\" }, inplace = True)\navp.rename(columns={ avp.columns[3]: \"Pred_Rating\" }, inplace = True)\navp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def check_overlap(UserId,top_recos_to_check):\n    samp_cust = avp[avp['ID']==UserId][['ID','Rating_numeric','Book_Id']]\n    samp_cust.sort_values(by='Rating_numeric', ascending=False, inplace=True)\n    available_actual_ratings = samp_cust.shape[0]\n    rows_to_fetch = min(available_actual_ratings,top_recos_to_check)\n    preds_df_sampcust = avp[avp['ID']==UserId][['ID','Pred_Rating','Book_Id']]\n    preds_df_sampcust.sort_values(by='Pred_Rating', ascending=False, inplace=True)\n    actual_rating = samp_cust.iloc[0:rows_to_fetch,:]\n    pred_rating = preds_df_sampcust.iloc[0:rows_to_fetch,:]\n    overlap = pd.Series(list(set(actual_rating.Book_Id).intersection(set(pred_rating.Book_Id))))\n    pct_overlap = (len(overlap)/rows_to_fetch)*100\n    #print(\"Percentage of overlap in top\"+str(top_recos_to_check)+\" for User ID - \"+str(UserId)+\" : \"+str(pct_overlap))\n    return pct_overlap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_user_list = avp.ID.unique().tolist()\noverlap_summary={}\ntop_recos_to_check =10\nfor users in test_user_list:\n    overlap_summary[users]= check_overlap(users,top_recos_to_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_summary = sorted(overlap_summary.items(), key=lambda x: x[1], reverse=True)\nmax_overlap = np.array(list(overlap_summary.values())).max()\nmin_overlap = np.array(list(overlap_summary.values())).min()\nmean_overlap = np.array(list(overlap_summary.values())).mean()\nprint(\"Max overlap in top\" +str(top_recos_to_check)+ \" books \"+str(max_overlap))\nprint(\"Min overlap in top \"+str(top_recos_to_check)+ \" books \"+str(min_overlap))\nprint(\"Average overlap in top \"+str(top_recos_to_check)+ \" books \"+str(mean_overlap))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"That's a huge improvement in the accuracy of recommendations by just switching to a different way of doing low rank matrix factorization using a deep neural network. Now we are able to recommend almost 3 out of 4 books correctly to the user.\n\nFurther improvments can be made by playing around with network architecture."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}