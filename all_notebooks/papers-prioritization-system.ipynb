{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prioritizing research papers based on abstracts\n### The aim of this project is to make an easy way to prioritize and recommend papers to researchers in a fast way to save the researchers' time and make them able to direct all their efforts to innovate a new way to fight COVID-19. We will analyze the papers with highest priority according to our system.\n\n#### We will start first by data exploration and cleaning then make a hypothesis based on our data exploration and domain knowledge, after that we will be implementing your solution using NLP and ML libraries like NLTK, TensorFlow and Scikit-Learn"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install modin[ray]\nfrom gensim.models.word2vec import FAST_VERSION\nprint(FAST_VERSION)\nimport multiprocessing\nimport numpy as np\n# import pandas as pd\nimport modin\nimport modin.pandas as pd\nfrom matplotlib import pyplot as plt\nimport os\nimport string\n\nimport nltk\nimport gensim\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n\nfrom gensim.test.utils import common_corpus, common_dictionary\nfrom gensim.similarities import MatrixSimilarity\n\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.similarities import Similarity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of files in the directory, json files for each paper, a description file and a metadata file for each paper. We will explore the metadata file to see whether we can find some useful features that serve our purpose or not."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"meta = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nprint(\"Cols names: {}\".format(meta.columns))\nmeta.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nmeta.isna().sum().plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the two features 'Microsoft Academic Paper ID', 'WHO #Covidence' have a very huge number of missing values as obvious from the histogram so we will remove them to regulate the scale of the histogram's frequencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_dropped = meta.drop(['Microsoft Academic Paper ID', 'WHO #Covidence'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nmeta_dropped.isna().sum().plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above histogram we can see there is a small number of papers with missing urls and a considerable number with missing doi and abstracts. We are interested only in papers that have abstracts and either doi or url to be able to recommend them to the researcher. So let's explore some statistics about the papers with missing abstracts and remove them if possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"miss = meta['abstract'].isna().sum()\nprint(\"The number of papers without abstracts is {:0.0f} which represents {:.2f}% of the total number of papers\".format(miss, 100* (miss/meta.shape[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see from these previous results the percentage of missing abstracts is considerable but we can ignore it as it is one of the most important features in our approach. Now lets see the number of missing doi and the number of missing urls from the papers without missing abstracts."},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_papers = meta[meta['abstract'].notna()]\nprint(\"The total number of papers is {:0.0f}\".format(abstracts_papers.shape[0]))\nmissing_doi = abstracts_papers['doi'].isna().sum()\nprint(\"The number of papers without doi is {:0.0f}\".format(missing_doi))\nmissing_url = abstracts_papers['url'].isna().sum()\nprint(\"The number of papers without url is {:0.0f}\".format(missing_url))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will need to extract the year of publication to give high priority to the papers with earlier publication data as they are more likely to be cited in newer papers and also because the might not be sufficient for today's usage (ex: papers from 1955)"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_papers = abstracts_papers[abstracts_papers['publish_time'].notna()]\nabstracts_papers['year'] = pd.DatetimeIndex(abstracts_papers['publish_time']).year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see if the papers with urls have doi or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_url_data = abstracts_papers[abstracts_papers[\"url\"].notna()]\nprint(\"The total number of papers with abstracts, urls, but missing doi = {:.0f}\".format( missing_url_data.doi.isna().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above data exploration, we will need to remove the data that doesn't have both url so that research scientists can find the paper recommended to them. We will only choose papers with abstracts and either doi or url as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_papers = abstracts_papers[abstracts_papers[\"url\"].notna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove stopwords: \nnow we are going to remove the stop words which are common words as (the, a, an, etc.) we will also remove punctuation, we will then revert the words to its basis using the famous stemming algorithm (Porter algorithm) before using the TF-IDF techniques. We will also take care while removing punctuation as we don't want to lose the meaning of words like (non-medical) while using the dash but we will convert it to (nonmedical) instead to keep its meaning with respect to the search algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"porter = PorterStemmer()\nlancaster=LancasterStemmer()\n\nabstracts_only = abstracts_papers['abstract']\ntokenized_abs = []\n\nfor abst in abstracts_only:\n    tokens_without_stop_words = remove_stopwords(abst)\n    tokens_cleaned = sent_tokenize(tokens_without_stop_words)\n    words = [porter.stem(w.lower()) for text in tokens_cleaned for w in word_tokenize(text) if (w.translate(str.maketrans('', '', string.punctuation))).isalnum()]\n    tokenized_abs.append(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will vectorize the corpus using TF-IDF, the TF-IDF technique to be computed to be used to compute the similarity between the query and the abstracts for prioritization in later steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = []\ndictionary = gensim.corpora.Dictionary(tokenized_abs)\ncorpus = [dictionary.doc2bow(abstract) for abstract in tokenized_abs]\ntf_idf = gensim.models.TfidfModel(corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will need to apply the same cleaning steps, that we applied before to the corpus, to the query itself so that we can get consistent results. We will do the following: Removing stop words, removing punctuation and stemming. then we will map the words to their integer ids using the dictionary of words computed before."},{"metadata":{"trusted":true},"cell_type":"code","source":"def query_tfidf(query):\n    \n    query_without_stop_words = remove_stopwords(query)\n    tokens = sent_tokenize(query_without_stop_words)\n\n    query_doc = [porter.stem(w.lower()) for text in tokens for w in word_tokenize(text) if (w.translate(str.maketrans('', '', string.punctuation))).isalnum()]\n\n    # mapping from words into the integer ids\n    query_doc_bow = dictionary.doc2bow(query_doc)\n    query_doc_tf_idf = tf_idf[query_doc_bow]\n    \n    return query_doc_tf_idf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use the cosine similarity algorithm from gensim, we tried to use text vectorizer instead of the TF-IDF but the TF-IDF gave better results. We will store the similarity in the same dataframe with the abstracts and sort from highest similarity to the lowest one."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rankings(query):\n    t = time.time()\n    query_doc_tf_idf = query_tfidf(query)\n    print(time.time()-t , 0)\n    t = time.time()\n    index_temp = get_tmpfile(\"index\")\n    print(time.time()-t , 1)\n    t = time.time()\n    len_dictionary = len(dictionary)\n    print(time.time()-t , 2)\n    t = time.time()\n    index = Similarity(index_temp, tf_idf[corpus], num_features=len_dictionary)\n    print(time.time()-t , 3)\n    t = time.time()\n    similarities = index[query_doc_tf_idf]\n    print(time.time()-t , 4)\n\n    # Storing similarity in the dataframe and sort from high to low simmilatiry\n    t = time.time()\n    abstracts_papers[\"similarity\"] = similarities\n    print(time.time()-t , 5)\n    t = time.time()\n#     abstracts_papers_sorted = abstracts_papers.sort_values(by ='similarity' , ascending=False)\n    abstracts_papers_sorted = sorted(abstracts_papers, reverse=True)\n    print(time.time()-t , 6)\n    t = time.time()\n    abstracts_papers_sorted.reset_index(inplace = True)\n    print(time.time()-t , 7)\n    t = time.time()\n    top20 = abstracts_papers_sorted.head(20)\n    print(time.time()-t , 8)\n    t = time.time()\n    norm_range = top20['year'].max() - top20['year'].min()\n    print(time.time()-t , 9)\n    t = time.time()\n    top20[\"similarity\"] -= (abs(top20['year'] - top20['year'].max()) / norm_range)*0.1\n    print(time.time()-t , 10)\n    t = time.time()\n    top20 = top20.sort_values(by ='similarity' , ascending=False)\n    print(time.time()-t , 11)\n    t = time.time()\n    top20.reset_index(inplace = True)\n    print(time.time()-t , 12)\n    \n    return top20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also reorder the top 20 papers by penalizing the older papers using the equation: similarity - 0.1*abs(x - newest_year_puplication) / (newest_year_puplication - oldest_year_puplication)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\n# query = \"COVID-19 (corona) non-pharmaceutical interventions, Methods to control the spread in communities, barriers to compliance and how these vary among different populations\"\nt = time.time()\ntop = rankings(input())\nprint(time.time()-t)\n\nfor abstract in range(10):\n    print(top.abstract[abstract])\n    print('\\n>>>>>>>>>>>>>>>>>>>>>>\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will print the top 10 abstracts from the similarity algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"for paper in range(10):\n    print(top.url[paper])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of the most related research papers\n### Controlling the spread in communities:\nWe will now discuss the methods to control the spread of the virus in the communities based on our top five papers with respect to the search query: (\"COVID-19 (corona) non-pharmaceutical interventions, Methods to control the spread in communities, barriers to compliance and how these vary among different populations\")"},{"metadata":{},"cell_type":"markdown","source":"Non-pharmaceutical intervention is often recommended to control the spread of pandemic[1] but it is debatable whether it is acceptable by the public or not and what barrier would we face when trying to apply concepts like social distancing, wearing masks or even convincing people to wash their hands more often to prevent the outbreak of a disease. We will discuss the details of the first results recommended by our system above.\n\nIt seems that there are many barriers that prevents some of the public to abide to some non-pharmaceutical intervention, for example: In the first paper, the study evaluates the acceptability of non-pharmaceutical interventions (as washing hands, social distancing,  as methods to fight respiratory diseases whether it was declared pandemic as H1N1 or non-pandemic. The study showed that a group of people were aware of the importance of non-pharmaceutical interventions to protect themselves and the society while others saw many barriers as stigma,social interaction or even economic feasibility. to make people adopt the non-pharmaceutical interventions habits we need to increase their awareness of the importance of those methods to protect them and the ones they care for and also to break the barriers concerning them about these methods.[2]\n\nThe above discussion and results show us the importance of evaluating some Non-pharmaceutical intervention methods and try to see if they could be effective in facing influenza outbreaks, this might convince more people to follow those methods, for example: The second paper discusses the non-medical intervention in the case of Influenza outbreak in one  of the schools. Many non-pharmaceutical intervention measures were tested including the following:\n1. Closure of the school\n2. Making a field hospital to isolate influenza patients from other patients in hospitals.\n3. Public Health education campaign to increase the public awareness including instructions about:\n    ```css\n        1. Mask usage\n    2. Handshaking\n    3. Isolation of individuals \n     ```\n\nA high attack rate was found among the students maybe due to the close contact between the infected students and the rest of the students in the school although there is no mention of any obvious direct or indirect dependency between the attack rate and the close contact between the students. The study also stated that interventions could prevent the spread of the virus from the school to the outside world (other schools and places in the neighbourhood)[1]"},{"metadata":{},"cell_type":"markdown","source":"#### Citations:\n\n[1] Teasdale E, Santer M, Geraghty AW, Little P, Yardley L. Public perceptions of non-pharmaceutical interventions for reducing transmission of respiratory infection: systematic review and synthesis of qualitative studies. BMC Public Health. 2014;14:589. Published 2014 Jun 11. doi:10.1186/1471-2458-14-589\n\n[2] Sonthichai C, Iamsirithaworn S, Cummings D, et al. Effectiveness of Non-pharmaceutical Interventions in Controlling an Influenza A Outbreak in a School, Thailand, November 2007. Outbreak Surveill Investig Rep. 2011;4(2):611.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}