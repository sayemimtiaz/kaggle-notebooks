{"cells":[{"metadata":{"id":"zdhxlbyhfQxB"},"cell_type":"markdown","source":"This notebook is my final submission to [Analytics Vidhya HackLive2](https://datahack.analyticsvidhya.com/contest/hacklive-2-guided-community-hackathon/) with **RANK 15** .\n\nI have also shown some additional implementations here which were not used for final submission.    \nA large part of this notebook is credited to Nikhil's Webinar. I have extended it with additional Feature Engineering and Model Tuning."},{"metadata":{},"cell_type":"markdown","source":"The objective of the competition was to predict the number of likes obtained by a Youtube video based on given features. The Evaluation Metric was **Root Mean Square Logarithmic Error**.\n\nTo tackle this I have used extensive Feature Engineering and state of the art Gradient Boosting Models."},{"metadata":{},"cell_type":"markdown","source":"> > ### <font color='green'>If you like my approach then do consider Upvoting the Kernel. It keeps me motivated.</font>"},{"metadata":{"id":"rA6i2im_gUSd"},"cell_type":"markdown","source":"# Contents\n\n1. [Mount Drive](#Mount_Drive) \n2. [Import Libraries](#Import_Libraries)\n3. [Define Functions](#Define_Functions)\n4. [Load Data](#Load_Data)\n5. [EDA and Data Preprocessing](#EDA_and_Data_Preprocessing)  \n  5.1 [Target Distribution](#Target_Distribution)  \n  5.2 [Data Processing](#Data_Processing)  \n6. [Model](#Model)    \n  6.1 [Linear Regression](#Linear_Regression)  \n  6.2 [XGBRegressor](#XGBRegressor)  \n  6.3 [LGBMRegressor](#LGBMRegressor)  \n  6.4 [CatBoostRegressor](#CatBoostRegressor)  \n7. [Feature Engineering](#Feature_Engineering)  \n  7.1 [Helper Functions](#Helper_Functions)  \n  7.2 [DateTime Features](#DateTime_Features)  \n  7.3 [Channel Title](#Channel_Title)  \n  7.4 [Views and Comment Counts](#Views_and_Comment_Counts)  \n  7.5 [Text Data](#Text_Data)  \n  7.6 [NLP](#NLP)      \n  -       7.6.1 [Text Preprocessing](#Text_Preprocessing)  \n  -       7.6.2 [Count](#Count)    \n  -       7.6.3 [Sentiment Analysis](#Sentiment_Analysis)    \n  -       7.6.4 [Capitalized Words](#Capitalized_Words)    \n8. [Feature Engineering Models](#Feature_Engineering_Models)  \n  8.1 [LGBMRegressor](#FE_LGBMRegressor)  \n  8.2 [CatBoostRegressor](#FE_CatBoostRegressor)  \n  8.3 [XGBRegressor](#FE_XGBRegressor)  \n9. [Model Ensemble](#Model_Ensemble)"},{"metadata":{"id":"LXjAQvllabJO"},"cell_type":"markdown","source":"# 1. Mount Drive <a id = \"Mount_Drive\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Since this notebook has been made using Google Colab, you may find some instances particular to Colab. I have commented them. The benefit of this is that for people who are willing to execute this using Colab can directly download, uncomment and use it without requiring much changes. This way it is compatile to both Kaggle and Colab Environment."},{"metadata":{"id":"MMq1h5IfXQTt","trusted":false},"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/MyDrive')\n\n# from google.colab import files","execution_count":null,"outputs":[]},{"metadata":{"id":"Ve4P1X0ZaelS"},"cell_type":"markdown","source":"# 2. Import Libraries <a id = \"Import_Libraries\"></a>"},{"metadata":{"id":"lnSXMKaKYKa0","trusted":true},"cell_type":"code","source":"#Import Libraries\n\nimport pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-dark')\n\n#Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n\n#NLP\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#Text Processing\nimport re\nimport nltk\nnltk.download('popular')\n\n#Language Detection\n!pip install langdetect\nimport langdetect\n\n#Sentiment\nfrom textblob import TextBlob\n\n#ner\nimport spacy\n\n#Vectorizer\nfrom sklearn import feature_extraction, manifold\n\n#Word Embedding\nimport gensim.downloader as gensim_api\n\n#Topic Modeling\nimport gensim\n\n#Model\nimport tensorflow as tf\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.optimizers import Adam\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n!pip install catboost\nfrom catboost import CatBoostRegressor\n\n#Validation\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"1cvNlUNIalw2"},"cell_type":"markdown","source":"# 3. Define Functions <a id = \"Define_Functions\"></a>"},{"metadata":{},"cell_type":"markdown","source":"First we define some helper functions that will help us in different parts of the code. Some function operations may seem similar, I have added them so that you can pick whichever you feel comfortable with.\n\n1.  **predict()** :   \nTo calculate the predictions of the model. We have to pass the *model* along with the desired features *model_features* which are being used. Then the model predictions are evaluated. We have used *mean_squared_error* here because the target features have been log transformed already. \n\n2. **run_gradient_boosting()** :  \nThis is the most important function. We will use this to run our boosting algorithms. First we will perform StratifiedKFold on the data. Then we define the train and validation sets for each fold. Next, we transform the data (This is an optional step). after this we store the feature importances, make predictions and evaluate the model.\n\n3. **av_metric()**:  \nFunction to provide evaluation based on *Root Mean Square Logarithmic Error*.\n\n4. **download_preds()**:  \nFunction to download the model predictions on Test Data to your local machine.\n\n5. **download()**:  \nFunction to calculate Model Predictions and the download them to your local machine.\n\n6. **join_df()**:  \nTo join the train and test set for Data Preprocessing. I have also shown an alternate way to do this without using the function."},{"metadata":{"id":"CcWVTQdzYzsA","trusted":true},"cell_type":"code","source":"#Hackathon Metric\ndef predict(model, model_features):\n  pred_train = model.predict(X_train[model_features])\n  pred_val = model.predict(X_val[model_features])\n\n  print(f\"Train RMSLE = {1000 * np.sqrt(mean_squared_error(y_train, pred_train))}\")\n  print(f\"Test RMSLE = {1000 * np.sqrt(mean_squared_error(y_val, pred_val))}\")\n\ndef run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[TARGET_COL]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET_COL], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(12, 6))\n\n  return oofs, preds, fi\n\ndef av_metric(y_true, y_pred):\n  return 1000 * np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef download_preds(preds_test, file_name = 'hacklive_sub.csv'):\n\n  ## 1. Setting the target column with our obtained predictions\n  submission[TARGET_COL] = preds_test\n\n  ## 2. Saving our predictions to a csv file\n\n  submission.to_csv(file_name, index = False)\n\n  ## 3. Downloading and submitting the csv file\n  from google.colab import files\n  files.download(file_name)\n\n#Download Submission File\ndef download(model, model_features, file_name = 'hacklive_sub.csv'):\n\n  pred_test = model.predict(test[model_features])\n  pred_test = np.expm1(pred_test)\n\n  #Setting the target column with our obtained predictions\n  submission[TARGET_COL] = pred_test\n\n  #Saving our predictions to a csv file\n  submission.to_csv(file_name, index = False)\n  \n  #Downloadingthe csv file\n  files.download(file_name)\n\ndef join_df(train, test):\n\n  df = pd.concat([train, test], axis=0).reset_index(drop = True)\n  features = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\n  df[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))\n\n  return df, features","execution_count":null,"outputs":[]},{"metadata":{"id":"rPfre8RzapuO"},"cell_type":"markdown","source":"# 4. Load Data <a id = \"Load Data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"First we load our data set and assign the ID and TARGET columns. We also define our features based on their types."},{"metadata":{"id":"y70Lwu0QY8Dt","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/av-guided-hackathon/test.csv')\ntrain = pd.read_csv('../input/av-guided-hackathon/train.csv')\nsubmission = pd.read_csv('../input/av-guided-hackathon/sample_submission_cxCGjdN.csv')\n\nID_COL, TARGET_COL = 'video_id', 'likes'\n\nnum_cols = ['views', 'dislikes', 'comment_count']\ncat_cols = ['category_id', 'country_code']\ntext_cols = ['title', 'channel_title', 'tags', 'description']\ndate_cols = ['publish_date']\n\n# features = [c for c in train.columns if c not in [ID_COL, TARGET_COL]] ","execution_count":null,"outputs":[]},{"metadata":{"id":"Cj9TOvsPfUhK","outputId":"756ef9df-720e-41a6-c53c-deceba11305c","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"uBeCN3cCau0g"},"cell_type":"markdown","source":"# 5. EDA and Data Preprocessing <a id = \"EDA_and_Data_Preprocessing\"></a>"},{"metadata":{"id":"SVznftIua69W"},"cell_type":"markdown","source":"## 5.1 Target Distribition <a id = \"Target_Distribution\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df, features = join_df(train, test)","execution_count":null,"outputs":[]},{"metadata":{"id":"I2C30EXyZTLE","trusted":true},"cell_type":"code","source":"#Likes Distribution\n_ = df[TARGET_COL].plot(kind = 'density', title = 'Likes Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{"id":"tvVkQ1bUhutu"},"cell_type":"markdown","source":"Target Column is Highly Right Skewed, so we apply log transform."},{"metadata":{"id":"5eozN2CtezAN","trusted":true},"cell_type":"code","source":"#Log Likes Distribution\n_ = pd.Series(np.log1p(df[TARGET_COL])).plot(kind = 'density', title = 'Log Likes Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{"id":"UPwiJqi2h94l","trusted":true},"cell_type":"code","source":"#Likes Boxplot\n_ = df[TARGET_COL].plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Likes Boxplot', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"jY7AMLndiSaK","trusted":true},"cell_type":"code","source":"#Log Likes BoxPlot\n_ = pd.Series(np.log1p(df[TARGET_COL])).plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Likes Boxplot', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"sui33H6pireR"},"cell_type":"markdown","source":"Log Transform helps us to deal with outliers in Target Variable."},{"metadata":{"id":"qNp93EuHFrlD"},"cell_type":"markdown","source":"## 5.2 Data Processing <a id = \"Data_Processing\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Here we perform some basic data preprocessing. Since we are dealing with categorical features we use One Hot Encoding."},{"metadata":{"id":"d22et4eCikna","outputId":"9bae0bc9-4772-4b65-acea-8a5965f463f4","trusted":true},"cell_type":"code","source":"#Combine Train and Test set for Data Cleaning\ntrain['set'] = 'train'\ntest['set'] = 'test'\ndf = pd.concat([test, train])\n\n#One Hot Encoding\ndf = pd.get_dummies(df, columns=cat_cols)\n\n#Filling Null Values\ndf = df.fillna(-999)\n\n#Apply Log Transform to Numerical Columns\ndf[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))\n\n#Separating Train and Test Data\ntrain = df[df['set']=='train']\ntest = df[df['set']=='test']\ntrain = train.drop('set', 1)\ntest = test.drop('set', 1)\ntest = test.drop('likes', 1)\n\n#Define Features\nfeatures = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in text_cols + date_cols]","execution_count":null,"outputs":[]},{"metadata":{"id":"2hwfeRhnA0I6","trusted":true},"cell_type":"code","source":"#Train Test Split\nX = train[features]\ny = train[TARGET_COL]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = 23)","execution_count":null,"outputs":[]},{"metadata":{"id":"kBGepr_VpAp4"},"cell_type":"markdown","source":"# 6. Model <a id = \"Model\"></a>"},{"metadata":{"id":"w3WtuCyMLfp8"},"cell_type":"markdown","source":"## 6.1 Linear Regression <a id = \"Linear_Regression\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n\nThe linear equation assigns one scale factor to each input value or column, called a coefficient and represented by the capital Greek letter Beta (B). One additional coefficient is also added, giving the line an additional degree of freedom (e.g. moving up and down on a two-dimensional plot) and is often called the intercept or the bias coefficient."},{"metadata":{"id":"1_N27TOkBqHp","outputId":"df250801-4594-415d-8b02-3dde69a02952","trusted":true},"cell_type":"code","source":"#Linear Regression\nmodel = LinearRegression()\n\nmodel.fit(X_train[num_cols], y_train)\n\npredict(model, num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"4Gdc3Pb7M5_r","trusted":true},"cell_type":"code","source":"# download(model = model, model_features = num_cols, file_name = 's6_lr1.csv')\n# !mv s6_lr1.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"1_ebs-PepkJw"},"cell_type":"markdown","source":"## 6.2 XGBRegressor <a id = \"XGBRegressor\"></a>"},{"metadata":{},"cell_type":"markdown","source":"XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman.\nXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Before we learn about trees specifically, let us start by reviewing the basic elements in supervised learning.\n\nXGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems."},{"metadata":{"id":"HkiI68i3pWnO","trusted":true},"cell_type":"code","source":"model = XGBRegressor(n_estimators = 1000,\n                    max_depth = 6,\n                    learning_rate = 0.05,\n                    colsample_bytree = 0.5,\n                    random_state=1452,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds, fi = run_gradient_boosting(model, fit_params, train, test, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q_CWLAQut88e","trusted":true},"cell_type":"code","source":"xgb_preds_t = np.expm1(xgb_preds)\n# download_preds(xgb_preds_t, file_name = 's9_xgb1.csv')\n# !mv s9_xgb1.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"tdhqLAccMG9T"},"cell_type":"markdown","source":"## 6.3 LGBMRegressor <a id = \"LGBMRegressor\"></a>"},{"metadata":{},"cell_type":"markdown","source":"LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n\nIt uses two novel techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB) which fulfills the limitations of histogram-based algorithm that is primarily used in all GBDT (Gradient Boosting Decision Tree) frameworks. The two techniques of GOSS and EFB described below form the characteristics of LightGBM Algorithm. They comprise together to make the model work efficiently and provide it a cutting edge over other GBDT frameworks"},{"metadata":{"id":"omU4r43kLn5U","trusted":true},"cell_type":"code","source":"model = LGBMRegressor(n_estimators = 5000,\n                        learning_rate = 0.01,\n                        colsample_bytree = 0.76,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds, fi = run_gradient_boosting(clf = model, fit_params = fit_params, train = train, test = test, features = cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"oECPvKJ0MQvZ","trusted":true},"cell_type":"code","source":"lgb_preds_t = np.expm1(lgb_preds)\n# download_preds(lgb_preds_t, file_name = 's7_lgbm2.csv')\n# !mv s7_lgbm2.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"NjrYN9ZNRBDb"},"cell_type":"markdown","source":"## 6.4 CatBoostRegressor <a id = \"CatBoostRegressor\"></a>"},{"metadata":{},"cell_type":"markdown","source":"CatBoost is a machine learning algorithm that uses gradient boosting on decision trees.\n\nIt is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy"},{"metadata":{"id":"n1FWlbV4RAm9","trusted":true},"cell_type":"code","source":"model = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.01,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=2054,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(model, fit_params, train, test, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"yV7njJkTRKJ7","trusted":true},"cell_type":"code","source":"cb_preds_t = np.expm1(cb_preds)\n# download_preds(cb_preds_t, file_name = 's8_cb1.csv')\n# !mv s8_cb1.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"FORjHjLvTQtM"},"cell_type":"markdown","source":"# 7. Feature Engineering <a id = \"Feature_Engineering\"></a>"},{"metadata":{},"cell_type":"markdown","source":"This is the most important part of the code. \n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."},{"metadata":{"id":"ENq5fW84TfMr"},"cell_type":"markdown","source":"## 7.1 Helper Functions <a id = \"Helper_Functions\"></a>"},{"metadata":{},"cell_type":"markdown","source":"1. **split_df_and_get_features()**:  \nFunction to split dataframe to original train and test set."},{"metadata":{"id":"PONHnmhRTT3K","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/av-guided-hackathon/test.csv')\ntrain = pd.read_csv('../input/av-guided-hackathon/train.csv')\n\ndef split_df_and_get_features(df, train_nrows):\n\n  train, test = df[:train_nrows].reset_index(drop = True), df[train_nrows:].reset_index(drop = True)\n  features = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\n  \n  return train, test, features\n\ndf, features = join_df(train, test)\n\ncat_cols = ['category_id', 'country_code', 'channel_title']\n\n#Label Encoding\ndf[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"YBzD7rfYUQ0U"},"cell_type":"markdown","source":"## 7.2 DateTime Features <a id = \"DateTime Features\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Dates and times are rich sources of information that can be used with machine learning models. However, these datetime variables do require some feature engineering to turn them into numerical data.\n\nIn this dataset we have been provided with one dateTime feature i.e., *publish_date*. We use it to generate more features as shown here."},{"metadata":{"id":"H_a1vlY-T4s8","trusted":true},"cell_type":"code","source":"df['publish_date'] = pd.to_datetime(df['publish_date'], format='%Y-%m-%d')\ndf['publish_date_days_since_start'] = (df['publish_date'] - df['publish_date'].min()).dt.days\n\ndf['publish_date_day_of_week'] = df['publish_date'].dt.dayofweek\ndf['publish_date_year'] = df['publish_date'].dt.year\ndf['publish_date_month'] = df['publish_date'].dt.month\ndf['publish_date_week'] = df['publish_date'].dt.isocalendar().week  ","execution_count":null,"outputs":[]},{"metadata":{"id":"3T3ADn-RUjhR"},"cell_type":"markdown","source":"## 7.3 Channel Title <a id = \"Channel_Title\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The title of the channel is definitely influential in determining the success of the video. We use it and combine it with other available features to create new features."},{"metadata":{"id":"wVVn60u8UZq7","trusted":true},"cell_type":"code","source":"df['channel_title_num_videos'] = df['channel_title'].map(df['channel_title'].value_counts())\ndf['publish_date_num_videos'] = df['publish_date'].map(df['publish_date'].value_counts())\ndf['channel_in_n_countries'] = df.groupby('channel_title')['country_code'].transform('nunique')","execution_count":null,"outputs":[]},{"metadata":{"id":"KSHv-LkKU2nM"},"cell_type":"markdown","source":"## 7.4 Views, Comment and Dislikes Counts <a id = \"Views_and_Comment_Counts\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Next we use our Numerical Features to create new features. We rely on the basic statistics obtained from these features for our model."},{"metadata":{"id":"BPYX6HKNUqAr","trusted":true},"cell_type":"code","source":"#Grouping Features\n\ndf['channel_title_mean_views'] = df.groupby('channel_title')['views'].transform('mean')\ndf['channel_title_max_views'] = df.groupby('channel_title')['views'].transform('max')\ndf['channel_title_min_views'] = df.groupby('channel_title')['views'].transform('min')\n\ndf['channel_title_mean_comments'] = df.groupby('channel_title')['comment_count'].transform('mean')\ndf['channel_title_max_comments'] = df.groupby('channel_title')['comment_count'].transform('max')\ndf['channel_title_min_comments'] = df.groupby('channel_title')['comment_count'].transform('min')\n\ndf['channel_title_mean_dislikes'] = df.groupby('channel_title')['dislikes'].transform('mean')\ndf['channel_title_max_dislikes'] = df.groupby('channel_title')['dislikes'].transform('max')\ndf['channel_title_min_dislikes'] = df.groupby('channel_title')['dislikes'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"id":"And7FSHIVDcT"},"cell_type":"markdown","source":"## 7.5 Text Data <a id = \"Text_Data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"To evaluate Text Data we use two approaches.\n1. Evaluate the Length of the text fields.\n2. Create a bag of words."},{"metadata":{"id":"0KtyCRibU-f0","trusted":true,"collapsed":true},"cell_type":"code","source":"#Evaluate the length of Text Fields\n\ndf['title_len'] = df['title'].apply(lambda x: len(x))\ndf['description_len'] = df['description'].apply(lambda x: len(x))\ndf['tags_len'] = df['tags'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"17QXR3P7VIiU","trusted":true},"cell_type":"code","source":"#Generate Bag of Words\n\nTOP_N_WORDS = 50\n\nvec = CountVectorizer(max_features = TOP_N_WORDS)\ntxt_to_fts = vec.fit_transform(df['description']).toarray()\ntxt_to_fts.shape\n\nc = 'description'\ntxt_fts_names = [c + f'_word_{i}_count' for i in range(TOP_N_WORDS)]\ndf[txt_fts_names] = txt_to_fts\n\ntrain, test, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"id":"WHCD9uydmamR","outputId":"a257c835-35b2-4320-c239-19c5d540fb55","trusted":false},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"id":"P4NQbvktpEr2"},"cell_type":"markdown","source":"## 7.6 NLP <a id = \"NLP\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In this section we dive deeper into Natural Language Processing. If you use the feature engineering before this section only, then also your model will perform fairly good. This section provides added enhancements."},{"metadata":{"id":"iXT3OSPOpvr5"},"cell_type":"markdown","source":"### 7.6.1 Text Preprocessing <a id = \"Text_Preprocessing\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The **utils_preprocess_text()** function performs basic text processing tasks. "},{"metadata":{"id":"rK92IGC5pLTT","trusted":true},"cell_type":"code","source":"def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"3l9C8fgZq9_d","trusted":true},"cell_type":"code","source":"#Clean Text\n\ndf[\"clean_title\"] = df[\"title\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, ))\ndf[\"clean_tags\"] = df[\"tags\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, ))\ndf[\"clean_description\"] = df[\"description\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, ))","execution_count":null,"outputs":[]},{"metadata":{"id":"PGStfeeoXuVk"},"cell_type":"markdown","source":"### 7.6.2 Count <a id = \"Count\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Here we create new features based on counts of certain parameters."},{"metadata":{"id":"ifvqt-u89U1g","trusted":true},"cell_type":"code","source":"#Word Count\ndf['clean_title_word_count'] = df[\"clean_title\"].apply(lambda x: len(str(x).split(\" \")))\ndf['clean_tags_word_count'] = df[\"clean_tags\"].apply(lambda x: len(str(x).split(\" \")))\ndf['clean_description_word_count'] = df[\"clean_description\"].apply(lambda x: len(str(x).split(\" \")))\n\n#Character Count\ndf['clean_title_char_count'] = df[\"clean_title\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndf['clean_tags_char_count'] = df[\"clean_tags\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndf['clean_description_char_count'] = df[\"clean_description\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n\n#Sentence Count    \ndf['clean_description_sentence_count'] = df[\"clean_description\"].apply(lambda x: len(str(x).split(\".\")))\n\n#Average Word Length\ndf['clean_title_avg_word_length'] = df['clean_title_char_count'] / df['clean_title_word_count']\ndf['clean_tags_avg_word_length'] = df['clean_tags_char_count'] / df['clean_tags_word_count']\ndf['clean_description_avg_word_length'] = df['clean_description_char_count'] / df['clean_description_word_count']\n\n#Average Sentence Length\ndf['clean_description_avg_sentence_length'] = df['clean_description_word_count'] / df['clean_description_sentence_count']","execution_count":null,"outputs":[]},{"metadata":{"id":"jsHqOc-CX1ob"},"cell_type":"markdown","source":"### 7.6.3 Sentiment Analysis <a id = \"Sentiment_Analysis\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information."},{"metadata":{"id":"zEgytYdcP8Lv","trusted":true},"cell_type":"code","source":"#Sentiment Analysis\n\ndf[\"clean_title_sentiment\"] = df['clean_title'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf[\"clean_tags_sentiment\"] = df['clean_tags'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf[\"clean_description_sentiment\"] = df['clean_description'].apply(lambda x: TextBlob(x).sentiment.polarity)","execution_count":null,"outputs":[]},{"metadata":{"id":"pAuCjJNhYVGu"},"cell_type":"markdown","source":"### 7.6.4 Capitalized Words <a id = \"Capitalized_Words\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Here we create new features based on whether there were any capitalized words in the given features."},{"metadata":{"id":"8aa9SUFtZGiA","trusted":true},"cell_type":"code","source":"#Capitalized Word\n\ndef contains_capitalized_word(s):\n    for w in s.split():\n        if w.isupper():\n            return 1\n    return 0\n\ndf[\"clean_title_capital\"] = df['clean_title'].apply(contains_capitalized_word)\ndf[\"clean_description_capital\"] = df['clean_description'].apply(contains_capitalized_word)","execution_count":null,"outputs":[]},{"metadata":{"id":"YWy8GpaNL2E9","trusted":true},"cell_type":"code","source":"train, test, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date', 'clean_title', 'clean_tags', 'clean_description']]","execution_count":null,"outputs":[]},{"metadata":{"id":"mOB2a7gIXpCw"},"cell_type":"markdown","source":"# 8. Feature Engineering Models <a id = \"Feature_Engineering_Models\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Now, we apply our models with Hyperparameter Tuning to our Feature Engineered Data."},{"metadata":{"id":"6HsFvHWZXzZL"},"cell_type":"markdown","source":"## 8.1 LGBMRegressor <a id = \"FE_LGBMRegressor\"></a>"},{"metadata":{"id":"EiuHRzeQVdeE","outputId":"c78baff1-f6d7-4a8e-85d7-10f03401bb74","trusted":true},"cell_type":"code","source":"model = LGBMRegressor(n_estimators = 5000,\n                        learning_rate = 0.05,    \n                        colsample_bytree = 0.65,\n                        metric = 'None',\n                        num_leaves = 50, \n                       \n                       )\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds_50, fi = run_gradient_boosting(model, fit_params, train, test, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"3-lGzkLFWD_V","outputId":"76d11c2c-d7ad-41e8-916f-673d5f60181d","trusted":true},"cell_type":"code","source":"lgb_preds_t = np.expm1(lgb_preds)\n# download_preds(lgb_preds_t, file_name = 's27 _lgbm_featureEng_colsample0.75.csv')    \n# !mv s27_lgbm_featureEng_colsample0.75.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"_ny_7AWeX4Np"},"cell_type":"markdown","source":"## 8.2 CatBoostRegressor <a id = \"FE_CatBoostRegressor\"></a>"},{"metadata":{"id":"uvVTl4mwWuLo","trusted":true},"cell_type":"code","source":"model = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.01,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=2054,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(model, fit_params, train, test, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"oi9ckZeHYBDJ","trusted":true},"cell_type":"code","source":"cb_preds_t = np.expm1(cb_preds)\n# download_preds(cb_preds_t, file_name = 's11_cb_featureEng.csv')\n# !mv s11_cb_featureEng.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"5tKCxsdYY8m2"},"cell_type":"markdown","source":"## 8.3 XGBRegressor <a id = \"FE_XGBRegressor\"></a>"},{"metadata":{"id":"P8LXu_7sYij7","trusted":true},"cell_type":"code","source":"model = XGBRegressor(n_estimators = 1000,\n                    max_depth = 6,\n                    learning_rate = 0.05,\n                    colsample_bytree = 0.5,\n                    random_state=1452,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds, fi = run_gradient_boosting(model, fit_params, train, test, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"j5wq1143ZEvD","trusted":true},"cell_type":"code","source":"xgb_preds_t = np.expm1(xgb_preds)    \n# download_preds(xgb_preds_t, file_name = 's28_xgb_featureEng.csv')\n# !mv s28_xgb_featureEng.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]},{"metadata":{"id":"V4vFzIJkarjb"},"cell_type":"markdown","source":"# 9. Model Ensemble <a id = \"Model_Ensemble\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In this section we ensemble the predictions from all three models using a LightGBM Model."},{"metadata":{"id":"8A7NfjbnbnQz","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/av-guided-hackathon/test.csv')\ntrain = pd.read_csv('../input/av-guided-hackathon/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"j0ofYAAJaZOF","trusted":true},"cell_type":"code","source":"train_new = train[[ID_COL, TARGET_COL]]\ntrain_new[TARGET_COL] = np.log1p(train_new[TARGET_COL])\n\ntest_new = test[[ID_COL]]\n\ntrain_new['lgb'] = lgb_oofs\ntest_new['lgb'] = lgb_preds\n\ntrain_new['cb'] = cb_oofs\ntest_new['cb'] = cb_preds\n\ntrain_new['xgb'] = xgb_oofs\ntest_new['xgb'] = xgb_preds\n\nfeatures = [c for c in train_new.columns if c not in [ID_COL, TARGET_COL]]","execution_count":null,"outputs":[]},{"metadata":{"id":"ikBhdf2Sbo1N","trusted":true},"cell_type":"code","source":"model = LGBMRegressor(n_estimators = 5000,\n                        learning_rate = 0.05,\n                        colsample_bytree = 0.65,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nens_oofs, ens_preds, fi = run_gradient_boosting(model, fit_params, train_new, test_new, features)","execution_count":null,"outputs":[]},{"metadata":{"id":"0pD0ynGab7UK","outputId":"4a9c520f-d088-44be-bdb2-da975c236832","trusted":true},"cell_type":"code","source":"ens_preds_t = np.expm1(ens_preds)\n# download_preds(ens_preds_t, file_name = 's13_ens1.csv')\n# !mv s13_ens1.csv '/content/MyDrive/My Drive/Data Science/Analytics Vidhya/Hacklive 2 Guided Community Hackathon/Submission Files'","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}