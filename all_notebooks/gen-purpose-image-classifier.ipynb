{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport shutil\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The classifier function is a general purpose image classifier that can be adapted to most image classification tasks.\nIt enables you to select one of 6 pre-trained models. The comments next to the function parameters should  provide\nthe information necessary to set the values for your application. Note the function uses a custom calback that \nadjust the learning rate. Initially the learning rate is adjusted by monitoring accuracy. Once the model accuracy\nexceeds the level set by parameter threshold, the learning rate is adjusted based on monitoring validation loss."},{"metadata":{},"cell_type":"markdown","source":"### Import needed modules"},{"metadata":{},"cell_type":"markdown","source":"### surpress annoying tensorflow warningsÂ¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the classifier function and defaut parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifier(my_dir,                   # main directory \n               save_dir,                 # directory to store the trained model in\n               subject,                  # string defining classification subject. Ex 'birds' or 'cars' etc\n               model_type = 'Mobilenet', # select from Mobilenet','MobilenetV2', 'VGG19','InceptionV3', 'ResNet50V2', 'NASNetMobile', 'DenseNet201,'EfficientNetB3'3\"\n               structure = 3,            # 3 for train,test, valid directories, 2 for train, test directories 1 for a single directory dir  \n               v_split=.10,              # only valid if structure =1 or 2. Then it is percentage of training images that will be used for validation\n               epochs = 15,              # number of epochs to run the model for\n               freeze = False,           # if False all layers of the model are trained for epoch epochs, if True layersof basemodel are not trained\n               fine_tune_epochs = 10,    # only used when freeze=True, then all model layers are trained for this value epochs after  initial epochs\n               height = 224,             # height of images to be used by the model\n               width = 224,              # width of images to be used by the model      \n               bands = 3,                # bands in image 3 for rgb 1 for grey scale \n               batch_size = 32,          # batch size used in generators\n               lr = .001,                # initial learning rate\n               patience = 1,             # number of epochs with no performance improvement before learning rate is adjusted\n               stop_patience = 3,        # number of times learning rate can be adjusted with performance improvement before stopping training\n               threshold = .90,           # float value is training accuracy<threshold adjust lr on accuracy, above threshold adjust on validation loss\n               dwell=False,               # if True when there is no performance improvement model weights set back to best_weights\n               factor = .5,              # float <1 factor to multiply current learning rate by if performance does not improve\n               dropout = .4,             # dropout float<1 defines dropout factor\n               print_code=10,             # Integer if 0 no file classifications are printed, if >0 is the maximum number misclassified \n                                         # file classifications that will be printed out\n               neurons_a= 128,           # number of neurons in Dense layer between base model and top classification layer\n               metrics=['accuracy']):    # create a list of desired metrics Note 'accuracy' metric is automatically added to the list of metrics \n    \n    if 'accuracy' not in metrics:\n        metrics.append('accuracy')\n    # **********NOTE***** if your train, test or validation directories have names different than\n    # 'train', or 'test' or ' valid' then use your directory names in the os.path.join statements\n    if structure == 3: # define train, test and valid directories where image files are stored within the main dir directory\n        train_dir, test_dir, valid_dir =  os.path.join(my_dir, 'train'),  os.path.join(my_dir, 'test'),  os.path.join(my_dir, 'valid')\n    elif structure==2: # define train and test directories, no validation directory\n        train_dir=os.path.join(my_dir,'train') \n        test_dir= os.path.join(my_dir, 'test')\n        valid_dir = None\n    else: # the data is in a single directory the dir directory set train directory to my_dir directory\n         train_dir, test_dir, valid_dir = my_dir, my_dir, my_dir  \n    # create the data generators\n    train_gen, test_gen, valid_gen=make_gens(structure, train_dir, test_dir, valid_dir,height, width, bands, batch_size, v_split)\n    # display some training images\n    show_training_samples(train_gen)\n    #create a list of classes from the sub directory names\n    class_list=os.listdir(train_dir) # list of class directories within the train directory\n    #determine the number of classes\n    class_count=0\n    for klass in class_list: # iterate through train_dir list, each directory within it is a new class, ignore any files\n        klass_path=os.path.join(train_dir, klass)\n        if os.path.isdir(klass_path):\n            class_count=class_count + 1    \n    # make the model basedon input parameters\n    model=Models().make_model( model_type,neurons_a, class_count, width, height, bands, lr, freeze, dropout, metrics )\n    # determine class weights to handle imbalanced data sets\n    class_weight=get_weight_dict(train_dir)\n    # create a custom callback using input parameters\n    callbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,dwell=dwell, model_name=model_type, freeze=freeze, end_epoch=epochs - 1 )]\n    # determine batch sizes for generators and steps per epoch for model.fit\n    if structure ==3:# all three generators derived from train, test, valid directory contents\n        valid_batch_size, valid_steps = get_bs(valid_dir, batch_size)\n        test_batch_size, test_steps = get_bs(test_dir, batch_size)\n    elif structure ==2: # train and test generators determined from train and test directories , split train data\n                        # into a training generator  and a validation generator\n        valid_steps= len(valid_gen.labels)//batch_size + 1\n        valid_batch_size= batch_size\n        test_batch_size, test_steps=get_bs(test_dir, batch_size) \n    else: # all data is in directory dir\n        valid_steps= len(valid_gen.labels)//batch_size + 1\n        valid_batch_size=batch_size\n        test_steps=valid_steps\n        test_batch_size=batch_size\n    results=model.fit(x=train_gen,  epochs=epochs, verbose=2, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n    tr_plot(results, 0)  # plot the loss and accuracy metrics\n    model.set_weights(LRA.best_weights) # load the best weights saved during training\n    e_dict=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=None, return_dict=True)\n    acc=display_eval_metrics(e_dict)\n    msg=f'accuracy on the test set is {acc:5.2f} %'\n    print_in_color(msg, (0,255,0),(55,65,80))\n    save_id=str (model_type +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n    save_loc=os.path.join(save_dir, save_id)\n    model.save(save_loc)\n    preds=model.predict(test_gen, batch_size=test_batch_size, verbose=0, steps=None)\n    print_info( test_dir, test_gen, preds, print_code, save_dir, subject )\n    if freeze: # Start Fine Tunning\n        msg='Starting Fine Tuning of Model data from last epoch is shown below'\n        print_in_color(msg, (255,255,0), (55,65,80))\n        for layer in model.layers:  # make all layers trainable\n            layer.trainable==True\n        start_epoch=epochs\n        total_epochs=start_epoch + fine_tune_epochs\n        # train model for fine tunning\n        data=model.fit(x=train_gen,  epochs=total_epochs, verbose=2, callbacks=callbacks,  validation_data=valid_gen,  initial_epoch=start_epoch)\n        tr_plot(data, start_epoch)  # plot training graph\n        model.set_weights(LRA.best_weights) # load best weights from fine tunning\n        save_id=str (model_type +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n        save_loc=os.path.join(save_dir, save_id)\n        model.save(save_loc)\n        e_dict=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=None, return_dict=True)\n        acc=display_eval_metrics(e_dict)\n        msg=f'accuracy of fine tuned model on the test set is {acc:5.2f} %'\n        print_in_color(msg, (0,255,0),(55,65,80))\n        preds=model.predict(test_gen, batch_size=test_batch_size, verbose=0, steps=test_steps)\n        #show_images( test_dir, test_gen, preds, max_images)\n        print_info( test_dir, test_gen, preds, print_code, save_dir, subject )  \n\n    acc=display_eval_metrics(e_dict)\n    msg=' Process complete'\n    print_in_color (msg, (0,255,0), (55,65,80))\n    return ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function to display sample training images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_training_samples(gen):\n    class_dict=gen.class_indices\n    new_dict={}\n    # make a new dictionary with keys and values reversed\n    for key, value in class_dict.items(): # dictionary is now {numeric class label: string of class_name}\n        new_dict[value]=key        \n    images,labels=next(gen) # get a sample batch from the generator\n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):\n        plt.subplot(5, 5, i + 1)\n        image=(images[i]+1 )/2 # scale images between 0 and 1 becaue pre-processor set them between -1 and +1\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=new_dict[index]\n        plt.title(class_name, color='blue', fontsize=16)\n        plt.axis('off')\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function to create data generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_gens ( structure, train_dir, test_dir, valid_dir,height, width, bands, batch_size, v_split):\n    def scaler(x):\n        y=x/127.5-1\n        return y\n    if bands==3:\n        color_mode = 'rgb'\n    else:\n        color_mode = 'grayscale'\n    if structure ==3:\n        train_gen=ImageDataGenerator(preprocessing_function=scaler,\n                                         horizontal_flip=True).flow_from_directory( train_dir, target_size=(width, height),\n                                                                                   batch_size=batch_size, seed=123,\n                                                                                   class_mode='categorical', color_mode=color_mode,shuffle=True)   \n        valid_batch_size, valid_steps=get_bs( valid_dir, batch_size)\n        valid_gen=ImageDataGenerator(preprocessing_function=scaler,\n                                        horizontal_flip=False).flow_from_directory(valid_dir,target_size=(width, height),\n                                                                                   batch_size=valid_batch_size,\n                                                                                   class_mode='categorical',color_mode=color_mode, shuffle=False)\n        test_batch_size, test_steps=get_bs(test_dir, batch_size)\n        test_gen=ImageDataGenerator(preprocessing_function=scaler,\n                                       horizontal_flip=False).flow_from_directory(test_dir, target_size=(width, height),\n                                                                                  batch_size=test_batch_size,\n                                                                                  class_mode='categorical',color_mode=color_mode, shuffle=False )\n\n        return train_gen, test_gen, valid_gen\n    if structure ==2:\n        train_gen=ImageDataGenerator(preprocessing_function=scaler,validation_split=v_split,\n                                     horizontal_flip=True).flow_from_directory( train_dir, target_size=(width, height), \n                                                                               batch_size=batch_size, seed=123,\n                                                                               class_mode='categorical', color_mode=color_mode, subset='training') \n        valid_gen=ImageDataGenerator(preprocessing_function=scaler, validation_split=v_split,\n                                    horizontal_flip=False).flow_from_directory( train_dir, target_size=(width, height),\n                                                                               batch_size=batch_size, seed=123, shuffle=False,\n                                                                               class_mode='categorical', color_mode=color_mode, subset='validation') \n        test_batch_size, test_steps=get_bs(test_dir, batch_size)                                                                                                                         \n        test_gen=ImageDataGenerator(preprocessing_function=scaler,\n                                    horizontal_flip=False).flow_from_directory(test_dir, target_size=(width, height),\n                                                                               batch_size=test_batch_size,\n                                                                               class_mode='categorical',color_mode=color_mode, shuffle=False ) \n        return train_gen, test_gen, valid_gen\n    train_gen=ImageDataGenerator(preprocessing_function=scaler,validation_split=v_split,\n                                     horizontal_flip=True, ).flow_from_directory( train_dir, target_size=(width, height), \n                                                                               batch_size=batch_size, seed=123,\n                                                                               class_mode='categorical', color_mode=color_mode, subset='training') \n    valid_gen=ImageDataGenerator(preprocessing_function=scaler, validation_split=v_split,\n                                    horizontal_flip=False).flow_from_directory( train_dir, target_size=(width, height),\n                                                                               batch_size=batch_size, seed=123, shuffle=False,\n                                                                               class_mode='categorical', color_mode=color_mode, subset='validation')\n    \n    \n    test_gen=valid_gen     \n    return train_gen, test_gen, valid_gen","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function to determine batch size and steps per epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bs(dir,b_max):\n    # dir is the directory containing the samples, b_max is maximum batch size to allow based on your memory capacity\n    # you only want to go through test and validation set once per epoch this function determines needed batch size ans steps per epoch\n    length=0\n    dir_list=os.listdir(dir)\n    for d in dir_list:\n        d_path=os.path.join (dir,d)\n        length=length + len(os.listdir(d_path))\n    batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=b_max],reverse=True)[0]  \n    return batch_size,int(length/batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the Models class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Models:         \n    def make_model(self, model_type,neurons_a, class_count, width, height, bands, lr, freeze, dropout, metrics ):\n        self.model_type=model_type\n        self.class_count=class_count\n        self.width=width\n        self.height=height\n        self.bands=bands\n        self.lr=lr   \n        self.freeze=freeze\n        self.dropout=dropout\n        self.metrics=metrics\n        self.neurons_a=neurons_a        \n        img_shape=(self.width, self.height,self.bands)        \n        model_list=['Mobilenet','MobilenetV2', 'VGG19','InceptionV3', 'ResNet50V2', 'NASNetMobile', 'DenseNet201','EfficientNetB0','Xception','InceptionResNetV2']\n        if self.model_type not in model_list:\n            msg=f'ERROR the model name you specified {self.model_type} is not an allowed model name'\n            print_in_color(msg, (255,0,0),(55,65,80))\n            return None       \n        if self.model_type=='Mobilenet':\n            base_model=tf.keras.applications.mobilenet.MobileNet( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet',dropout=.4) \n        elif self.model_type=='MobilenetV2':\n            base_model=tf.keras.applications.MobileNetV2( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet')        \n        elif self.model_type=='VGG19':\n            base_model=tf.keras.applications.VGG19( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet' )\n        elif self.model_type=='InceptionV3':\n            base_model=tf.keras.applications.InceptionV3( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet' )\n        elif self.model_type=='NASNetMobile':\n            base_model=tf.keras.applications.NASNetMobile( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet' )\n        elif self.model_type=='DenseNet201':\n            base_model=tf.keras.applications.densenet.DenseNet201( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet' )\n        elif self.model_type=='EfficientNetB0':\n            base_model=tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n        elif self.model_type=='Xception':\n            base_model=tf.keras.applications.Xception(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')  \n        elif self.model_type=='InceptionResNetV2':\n            base_model=tf.keras.applications.InceptionResNetV2(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n        else:\n            base_model=tf.keras.applications.ResNet50V2( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet')\n        \n        if self.freeze:\n            for layer in base_model.layers:#train top 20 layers of base model\n                layer.trainable=False            \n        x=base_model.output\n        x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n        x = Dense(self.neurons_a, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu', kernel_initializer= tf.keras.initializers.GlorotUniform(seed=123))(x)\n        x=Dropout(rate=dropout, seed=123)(x) \n        #x = Dense(self.neurons_b, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                #bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n        #x=Dropout(rate=dropout, seed=123)(x) \n        output=Dense(self.class_count, activation='softmax',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123))(x)\n        model=Model(inputs=base_model.input, outputs=output)\n        \n        model.compile(Adamax(lr=self.lr), loss='categorical_crossentropy', metrics=self.metrics) \n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function that prints a string msg in specified foreground and background rgb colors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define a function that calculates the class_weight dictionary for use in model.fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weight_dict(dir):    \n    most_samples=0\n    class_weight={}\n    class_list=os.listdir(dir) # dir is the directory with the training samples organized by class    \n    for c in (class_list): # iterate through class directories, find number of samples in each class then find class with highest number of samples\n        c_path=os.path.join(dir,c)\n        if os.path.isdir(c_path):            \n            length=len(os.listdir(c_path)) # determine number of samples in the class directory\n            if length>most_samples:\n                most_samples=length   \n    for i,c in enumerate(class_list): #iterate through class directories, find number of samples in each and divide total_samples by length\n        c_path=os.path.join(dir,c)\n        if os.path.isdir(c_path):\n            length=len(os.listdir(c_path)) # number of samples inclass directory\n            class_weight[i]=most_samples/length   \n            #print (i,most_samples, class_weight[i])   \n    return class_weight\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create subclass of callback class as custom callback to adjust learning rate and save best weights\npatience is an integer that specifies how many consecutive epoch can occur until learning rate is adjusted\nthreshold is a float. It specifies that if training accuracy is above this level learning rate will be adjusted based on validation loss\nfactor is a float <1 that specifies the factor by which the current learning rate will be multiplied by\nclass variable LRA.best_weights stores the model weights for the epoch with the lowest validation loss\nafter train set the model weights with model.load_weights(LRA.best_weights) then do predictions on the test set"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"class LRA(keras.callbacks.Callback):\n    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,end_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        self.count=0 # initialize counter that counts epochs with no improvement\n        self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n        self.end_epoch=end_epoch # value of the number of epochs to run\n        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed\n        msg=' '\n        if freeze==True:\n            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n        else:\n            msgs=f' Starting training using base model { model_name} training all layers '            \n        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n            \n    def on_epoch_begin(self, epoch, logs=None): # just used to print data from previous epoch\n        if epoch != 0:\n            msgs=f'for epoch {epoch} '\n            msgs=msgs + LRA.msg\n            print_in_color(msgs, (255,255,0), (55,65,80))\n            \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            if acc>self.highest_tracc: # training accuracy improved in the epoch\n                LRA.msg= f' training accuracy improved from  {self.highest_tracc:7.4f} to {acc:7.4f} learning rate held at {lr:10.8f}'\n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss             \n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1:\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss\n                    msgs=f' training accuracy {acc:7.4f} < highest accuracy of {self.highest_tracc:7.4f} '\n                    LRA.msg=msgs + f' for {self.patience } epochs, lr adjusted to {self.lr:10.8f}'                    \n                else:\n                    self.count=self.count +1 # increment patience counter\n                    LRA.msg=f' training accuracy {acc:7.4f} < highest accuracy of {self.highest_tracc:7.4f} '\n                    #print_in_color(msg, (255,255,0), (55,65,80))\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved\n                msgs=f' validation loss improved from {self.lowest_vloss:8.5f} to {v_loss:8.5}, saving best weights'\n                LRA.msg=msgs + f' learning rate held at {self.lr:10.8f}'\n                #print_in_color(msg, (0,255,0), (55,65,80))\n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0                    \n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    self.lr=self.lr * self.factor # adjust the learning rate\n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted                    \n                    msgs=f' val_loss of {v_loss:8.5f} > {self.lowest_vloss:8.5f} for {self.patience} epochs'\n                    LRA.msg=msgs + f', lr adjusted to {self.lr:10.8f}'\n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter\n                    LRA.msg=f' validation loss of {v_loss:8.5f} > {self.lowest_vloss:8.5f}'\n                    #print_in_color(msg, (255,255,0), (55,65,80)) \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        if epoch==self.end_epoch:\n            print_in_color(LRA.msg, (255,255,0), (55,65,80)) # print out data for the final epoch\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            LRA.msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(LRA.msg, (0,255,0), (55,65,80))\n            self.model.stop_training = True # stop training","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### define function to plot training results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function to print metrics from model.evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_eval_metrics(e_data):\n    msg='Model Metrics after Training'\n    print_in_color(msg, (255,255,0), (55,65,80))\n    msg='{0:^24s}{1:^24s}'.format('Metric', 'Value')\n    print_in_color(msg, (255,255,0), (55,65,80))\n    for key,value in e_data.items():\n        print (f'{key:^24s}{value:^24.5f}')\n    acc=e_data['accuracy']* 100\n    return acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define function to print out information on misclassified images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_info( test_dir, test_gen, preds, print_code, save_dir, subject ):\n    class_dict=test_gen.class_indices\n    labels= test_gen.labels\n    file_names= test_gen.filenames \n    error_list=[]\n    true_class=[]\n    pred_class=[]\n    prob_list=[]\n    new_dict={}\n    error_indices=[]\n    y_pred=[]\n    for key,value in class_dict.items():\n        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n    # store new_dict as a text fine in the save_dir\n    classes=list(new_dict.values())     # list of string of class names\n    dict_as_text=str(new_dict)\n    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n    dict_path=os.path.join(save_dir,dict_name)    \n    with open(dict_path, 'w') as x_file:\n        x_file.write(dict_as_text)    \n    errors=0      \n    for i, p in enumerate(preds):\n        pred_index=np.argmax(p)        \n        true_index=labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred\n            error_list.append(file_names[i])\n            true_class.append(new_dict[true_index])\n            pred_class.append(new_dict[pred_index])\n            prob_list.append(p[pred_index])\n            error_indices.append(true_index)            \n            errors=errors + 1\n        y_pred.append(pred_index)    \n    if print_code !=0:\n        if errors>0:\n            if print_code>errors:\n                r=errors\n            else:\n                r=print_code           \n            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n            print_in_color(msg, (0,255,0),(55,65,80))\n            for i in range(r):\n                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(error_list[i], pred_class[i],true_class[i], ' ', prob_list[i])\n                print_in_color(msg, (255,255,255), (55,65,60))\n                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n        else:\n            msg='With accuracy of 100 % there are no errors to print'\n            print_in_color(msg, (0,255,0),(55,65,80))\n    if errors>0:\n        plot_bar=[]\n        plot_class=[]\n        for  key, value in new_dict.items():        \n            count=error_indices.count(key) \n            if count!=0:\n                plot_bar.append(count) # list containg how many times a class c had an error\n                plot_class.append(value)   # stores the class \n        fig=plt.figure()\n        fig.set_figheight(len(plot_class)/3)\n        fig.set_figwidth(10)\n        plt.style.use('fivethirtyeight')\n        for i in range(0, len(plot_class)):\n            c=plot_class[i]\n            x=plot_bar[i]\n            plt.barh(c, x, )\n            plt.title( ' Errors by Class on Test Set')\n    if len(classes)<= 20:\n        # create a confusion matrix and a test report        \n        y_true= np.array(labels)        \n        y_pred=np.array(y_pred)        \n        cm = confusion_matrix(y_true, y_pred )\n        clr = classification_report(y_true, y_pred)\n        length=len(classes)\n        if length<8:\n            fig_width=8\n            fig_height=8\n        else:\n            fig_width=length\n            fig_height=length\n        plt.figure(figsize=(fig_width, fig_height))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(length)+.5, classes)\n        plt.yticks(np.arange(length)+.5, classes)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()    \n        print(\"Classification Report:\\n----------------------\\n\", clr)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example of use "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"s_dir = r'../input/wild-edible-plants/dataset/resized'  # main directory that stores the data - function returns dictionary of trained model metrics \nclassifier( my_dir=s_dir,\n            save_dir=r'./',          # directory to store the trained model and text version of class dictionary of the form {index, class name}\n            subject='sports',         # classifying bird species\n            model_type ='MobilenetV2',  # select from Mobilenet','MobilenetV2', 'VGG19','InceptionV3', 'ResNet50V2', 'NASNetMobile',\n                                     #'DenseNet201,'EfficientNetB0','InceptionResNetV2'\n            structure = 1,           # 3 for train,test, valid directories, 2 for train, test directories 1 for a single directory dir  \n            v_split=.2,              # only valid if structure =1 or 2. Then it is percentage of training images that will be used for validation\n            epochs = 20,               # number of epochs to run the model for\n            freeze = False,           # if False all layers of the model are trained for epoch epochs, if True layersof basemodel are not trained\n            fine_tune_epochs= 10,     # only used when freeze=True, then all model layers are trained for this value epochs after  initial epochs\n            height = 224,             # height of images to be used by the model\n            width = 224,              # width of images to be used by the model      \n            bands = 3,                # bands in image 3 for rgb 1 for grey scale\n            batch_size = 32,          # batch size used in generators\n            lr = .001,                # initial learning rate\n            patience = 1,             # number of epochs with no performance improvement before learning rate is adjusted\n            stop_patience = 4,        # number of times learning rate can be adjusted with performance improvement before stopping training\n            threshold = .95,           # float value is training accuracy<threshold adjust lr on accuracy, above threshold adjust on validation loss\n            dwell=False,               # if True model weights are set back to best_weights if performance did not improve for current epoch\n            factor = .5,              # float <1 factor to multiply current learning rate by if performance does not improve\n            dropout = .4,             # dropout float<1 defines dropout factor\n            print_code=50 ,           # max number of misclassified files to print out \n            neurons_a= 1024,          # number of neurons in dense layer between base model  and final top classification layer           \n            metrics=[])               # create a list of desired metrics Note 'accuracy' metric is automatically added to the list of metrics  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}