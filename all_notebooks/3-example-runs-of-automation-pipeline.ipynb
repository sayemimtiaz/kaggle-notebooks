{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"import shutil, os, folium, warnings\nfrom shapely.geometry import Point\nimport pandas as pd, numpy as np \nfrom collections import Counter\nfrom statistics import median\nimport geopandas as gpd\nwarnings.filterwarnings('ignore')\n\n## define base paths \nct_base_path = \"../input/census-tracts/cb_2017_<NUM>_tract_500k/cb_2017_<NUM>_tract_500k.shp\"\n_base_dir = \"../input/data-science-for-good/cpe-data/\"\nexternal_datasets_path = \"../input/external-datasets-cpe/\"\n_root_dir = \"CPE_ROOT/\"\n\n## define the new directory names and mandatory shape files \nmandatory_shapefiles = [\"shp\", \"shx\", \"dbf\", \"prj\"]\nnew_dirs = [\"shapefiles\", \"events\", \"metrics\", \"metrics_meta\"]\n\n## Utility function to cleanup the environment\ndef _cleanup_environment():\n    if os.path.exists(_root_dir):\n        !rm -r CPE_ROOT\n        pass\n    return None\n\n## Function to create a new repository structure \ndef _create_repository_structure():            \n    ## refresh environment \n    _cleanup_environment()\n    \n    ## list of all departments whose raw data is available\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    \n    ## master folder\n    os.mkdir(_root_dir) \n    for dept in depts:\n\n        ## every department folder \n        os.mkdir(_root_dir + \"/\" + dept)         \n        for _dir in new_dirs:\n        \n            ## sub directories for - shapefiles, acsdata, metrics, metrics-meta\n            os.mkdir(_root_dir + \"/\" + dept + \"/\" + _dir + \"/\")            \n    print (\"Status : Directory Structured Created\")\n    \ndept_37_27_prj = 'PROJCS[\"NAD_1983_StatePlane_Texas_Central_FIPS_4203_Feet\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS_1980\",6378137,298.257222101]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"False_Easting\",2296583.333333333],PARAMETER[\"False_Northing\",9842499.999999998],PARAMETER[\"Central_Meridian\",-100.3333333333333],PARAMETER[\"Standard_Parallel_1\",30.11666666666667],PARAMETER[\"Standard_Parallel_2\",31.88333333333333],PARAMETER[\"Latitude_Of_Origin\",29.66666666666667],UNIT[\"Foot_US\",0.30480060960121924],AUTHORITY[\"EPSG\",\"102739\"]]'\n\n## create a config to handle the errors in raw shape files\nmissing_shape_meta = {\n    \"Dept_37-00027\" : {\"prj\" : dept_37_27_prj}\n}\n\n## Function to fix / cleanup the errors in shapefile types\ndef _fix_errors_shapefiles(_path, dept):\n    \"\"\"\n    :params:\n    _path : root path containig the shape files \n    dept : selected dept if it is called only for a particular department\n    \"\"\"\n    \n    if dept not in missing_shape_meta:\n        return False\n    \n    ## Fix the errors in raw corresponding shape files\n    for extension, content in missing_shape_meta[dept].items():\n        if extension == \"prj\": \n            # Step1: Add missing prj file\n            with open(_path + \"department.prj\", 'w') as outfile:\n                outfile.write(content)\n            \n            # Step2: Fix CRS of shape file\n            df = gpd.read_file(_path + 'department.shp')\n            df.to_file(filename = _path + 'department.shp', \n                       driver='ESRI Shapefile', crs_wkt = content)\n\n        elif extension == \"shx\":\n            ## This function can be extended for other shape filetypes\n            ## the corresponding logic can be added in these blocks \n            pass\n    return True\n\n## Function to standardize the shape files\ndef _standardize_shapefiles():\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    for dept in depts:    \n        ## Step1: Configure the old and new path\n        shp_dir = dept.replace(\"Dept_\",\"\") + \"_Shapefiles/\"\n        old_pth = _base_dir + dept + \"/\" + shp_dir\n        new_pth = _root_dir + dept + \"/\" + \"shapefiles/\"\n\n        ## Step2: Standardize the file names and move to new path \n        _files = os.listdir(old_pth)\n        for _file in _files:\n            if _file[-3:].lower() not in mandatory_shapefiles:\n                continue\n            ext = \".\".join(_file.split(\".\")[1:]).lower()\n            new_name = \"department.\" + ext\n            shutil.copy(old_pth+_file, new_pth+new_name)\n\n        ## Step3: Fix Erroroneus shapefiles\n        fix_flag = _fix_errors_shapefiles(new_pth, dept)\n        \n    print (\"Status : Shapefile Standardization Complete\")\n    return None\n\n\n## cleaned names corresponding to given raw metric names\nacs_metrics_dic = { 'owner-occupied-housing' : 'housing', 'education-attainment' : 'education', 'employment' : 'employment', 'education-attainment-over-25' : 'education25', 'race-sex-age' : 'race-sex-age', 'poverty' : 'poverty', 'income' : 'income' }\nmetrics_names = list(acs_metrics_dic.values())\n\n## function to cleanup and move the ACS data\ndef _standardize_acs():\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    for dept in depts:  \n        ## Step1: Configure the old and new path\n        acs_dir = dept.replace(\"Dept_\",\"\") + \"_ACS_data\"\n        old_dirs = os.listdir(_base_dir + dept +\"/\"+ acs_dir)\n        new_dirs = [f.replace(dept.replace(\"Dept_\",\"\"),\"\") for f in old_dirs]\n        new_dirs = [f.replace(\"_ACS_\",\"\") for f in new_dirs]\n        \n        ## Step2: Move all ACS datafiles\n        for j, metric in enumerate(old_dirs):\n            metric_files = os.listdir(_base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric)\n            _file = [f for f in metric_files if \"metadata\" not in f][0]\n            _meta = [f for f in metric_files if \"metadata\" in f][0]\n\n            ## Step3: Standardize / Cleanup the name \n            for name, clean_name in acs_metrics_dic.items():\n                if \"25\" in metric:\n                    cname = \"education25\"\n                if name in metric:\n                    cname = clean_name     \n\n            ## Step4.1 : Move Metric File\n            old_path = _base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric +\"/\"+ _file\n            new_path = _root_dir + dept +\"/metrics/\" + cname + \".csv\"\n            shutil.copy(old_path, new_path)\n\n            ## Step4.2 : Move Metrics meta files\n            old_path = _base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric +\"/\"+ _meta\n            new_path = _root_dir + dept +\"/metrics_meta/\" + cname + \".csv\"\n            shutil.copy(old_path, new_path)\n\n    print (\"Status : Standardization of Metrics complete\")\n    \n    \ndef _run_standardization_pipeline():\n    _create_repository_structure()\n    _standardize_shapefiles()\n    _standardize_acs()\n\n\n## Provide the config file for the departments\ndepts_config = {\n    'Dept_23-00089' : {'_rowid' : \"DISTRICT\", \"ct_num\" : \"18\"},  \n    'Dept_49-00035' : {'_rowid' : \"pol_dist\", \"ct_num\" : \"06\"},  \n    'Dept_24-00013' : {'_rowid' : \"OBJECTID\", \"ct_num\" : \"27\"},  \n    'Dept_24-00098' : {'_rowid' : \"gridnum\",  \"ct_num\" : \"27\"},   \n    'Dept_49-00033' : {'_rowid' : \"number\",   \"ct_num\" : \"06\", \"center_ll\" : [34.0883787,-118.37781]},    \n    'Dept_11-00091' : {'_rowid' : \"ID\",       \"ct_num\" : \"25\"},         \n    'Dept_49-00081' : {'_rowid' : \"company\",  \"ct_num\" : \"06\"},   \n    'Dept_37-00049' : {'_rowid' : \"Name\",     \"ct_num\" : \"48\"},      \n    'Dept_37-00027' : {'_rowid' : \"CODE\",     \"ct_num\" : \"48\"},     \n    'Dept_49-00009' : {'_rowid' : \"objectid\", \"ct_num\" : \"53\"}, \n}\n\n\n## Function to read a shapefile\ndef _read_shape_gdf(_dept):\n    shape_pth = _root_dir + _dept + \"/shapefiles/department.shp\"\n    ## ensure that CRS are consistent\n    shape_gdf = gpd.read_file(shape_pth).to_crs(epsg=4326)\n    return shape_gdf\n\n## Read the CT File\ndef _read_ctfile(_dept):\n    ## find the corresponding CT number from the config\n    _ct = depts_config[_dept][\"ct_num\"]\n    ## generate the base CT path \n    ct_path = ct_base_path.replace(\"<NUM>\", _ct)\n    ## load the geo data frame for CT \n    state_cts = gpd.read_file(ct_path).to_crs(epsg='4326')\n    return state_cts\n\n## Function to get the centroid of a polygon\ndef _get_latlong_point(point):\n    _ll = str(point).replace(\"POINT (\",\"\").replace(\")\", \"\")\n    _ll = list(reversed([float(_) for _ in _ll.split()]))\n    return _ll\n\n## Function to plot a shapefile\ndef _plot_shapefile_base(shape_gdf, _dept, overlapped_cts = {}):\n    ## obtain the center most point of the map \n    \n    if \"center_ll\" not in depts_config[_dept]:\n        center_pt = shape_gdf.geometry.centroid[0]\n        center_pt = _get_latlong_point(center_pt)\n    else:\n        center_pt = depts_config[_dept][\"center_ll\"]\n    \n    ## initialize the folium map \n    mapa = folium.Map(center_pt,  zoom_start=10, tiles='CartoDB dark_matter')\n    if len(overlapped_cts) == 0:\n        ## only the base map\n        folium.GeoJson(shape_gdf).add_to(mapa)\n    else:\n        ## overlapped map\n        ct_style = {'fillColor':\"red\",'color':\"red\",'weight':1,'fillOpacity':0.5}\n        base_style = {'fillColor':\"blue\",'color':\"blue\",'weight':1,'fillOpacity':0.5}\n        folium.GeoJson(overlapped_cts, style_function = lambda feature: ct_style).add_to(mapa)\n        folium.GeoJson(shape_gdf, style_function = lambda feature: base_style).add_to(mapa)\n    return mapa\n\n\n## Find Overlapping Census Tracts\ndef find_overlapping_cts(dept_gdf, state_cts, _identifier, _threshold = 10.0):\n    \"\"\"\n    :params:\n    dept_gdf : the geo dataframe loaded from shape file for the department \n    state_cts : the geo dataframe of the corresponding ct file\n    _identifier : the unique row identifier for the department \n    _threshold : the overlapping threshold percentage to consider \n    \"\"\"\n    \n    \n    ## Step 1: Initialize\n    olaps_percentages, overlapped_idx = {}, []\n    for i, row in dept_gdf.iterrows():\n        if row[_identifier] not in olaps_percentages: \n            olaps_percentages[row[_identifier]] = {}\n\n        ## Step 2: Find overlap bw district and ct layer\n        layer1 = row[\"geometry\"] # district layer\n        for j, row2 in state_cts.iterrows():\n            layer2 = row2[\"geometry\"] # ct layer\n            layer3 = layer1.intersection(layer2) # overlapping layer\n            \n            ## Step 3: Save overlapping percentage\n            overlap_percent = layer3.area / layer2.area * 100\n            if overlap_percent >= _threshold: \n                olaps_percentages[row[_identifier]][row2[\"GEOID\"]] = overlap_percent\n                overlapped_idx.append(j)\n    \n    ## Step 4: Find unique overlapping census tracts\n    overlapped_idx = list(set(overlapped_idx))\n    overlapped_cts = state_cts.iloc[overlapped_idx]\n    # print (\"Status : Overlapped CTs Found: \", len(overlapped_cts))\n    return overlapped_cts, olaps_percentages\n\n## function to convert overlapping percentages dictionary to a dataframe \ndef _prepare_olaps_df(olaps_percentages):\n    temp = pd.DataFrame()\n    distid, ct, pers = [], [], []\n    for k, vals in olaps_percentages.items():\n        for v, per in vals.items():\n            distid.append (k)\n            ct.append(v)\n            pers.append(round(per, 2))\n    temp[\"DistId\"] = distid\n    temp[\"CensusTract\"] = ct\n    temp[\"Overlap %\"] = pers\n    return temp\n\n## Specific Metrics and their measures \nmetrics_config = {\n            'race-sex-age': {'metrics':['race','age','sex'], \"measure\":\"proportion\"},\n            'income':       {'metrics':['median_income'],    \"measure\":\"median\"},\n            'poverty':      {'metrics':['below_poverty'],    \"measure\":\"proportion\"},\n            'employment':   {'metrics':['ep_ratio', 'unemp_ratio'], \"measure\" : \"mean\"}\n            }\n\n## Cleaned Column Names \n_column_names = {\"race\" : { \"HC01_VC43\" : \"total_pop\",\n                            \"HC01_VC49\" : \"white_pop\",\n                            \"HC01_VC50\" : \"black_pop\",\n                            \"HC01_VC56\" : \"asian_pop\",\n                            \"HC01_VC88\" : \"hispanic_pop\"},\n                \"age\" : {\n                            \"HC01_VC12\" : \"20_24_pop\", \n                            \"HC01_VC13\" : \"25_34_pop\", \n                            \"HC01_VC14\" : \"35_44_pop\", \n                            \"HC01_VC15\" : \"45_54_pop\", \n                            \"HC01_VC16\" : \"55_59_pop\", \n                },\n                \"sex\": {\n                            \"HC01_VC04\" : \"male_pop\",\n                            \"HC01_VC05\" : \"female_pop\",\n                },\n                \"median_income\" : {\n                            \"HC02_EST_VC02\" : \"pop_income\",\n                            \"HC02_EST_VC04\" : \"whites_income\",\n                            \"HC02_EST_VC05\" : \"blacks_income\",\n                            \"HC02_EST_VC07\" : \"asian_income\",\n                            \"HC02_EST_VC12\" : \"hispanic_income\",\n                },\n                \"below_poverty\" : {\n                            \"HC02_EST_VC01\" : \"below_pov_pop\"},\n                 \"ep_ratio\" : {\n                             \"HC03_EST_VC15\" : \"whites_ep_ratio\",\n                             \"HC03_EST_VC16\" : \"blacks_ep_ratio\"\n                  },\n                 \"unemp_ratio\" : {\n                             \"HC04_EST_VC15\" : \"whites_unemp_ratio\",\n                             \"HC04_EST_VC16\" : \"blacks_unemp_ratio\"}\n                }\n\n\n## Function to perform basic pre-processing on metrics data \ndef _cleanup_metrics_data(_dept):\n    metrics_df = {}\n    for _metric in metrics_names: ## metrics_name is deinfed in config \n        mpath = _root_dir + _dept + \"/metrics/\" + _metric + \".csv\"\n        mdf = pd.read_csv(mpath, low_memory=False).iloc[1:]\n        mdf = mdf.reset_index(drop=True).rename(columns={'GEO.id2':'GEOID'})\n        metrics_df[_metric] = mdf\n    \n    ## returns metrics_df that contains all the dataframe for ACS metrics \n    return metrics_df\n\n## Function to Flatten the details\ndef _flatten_gdf(df, _identifier):\n    relevant_cols = [_identifier]\n    flatten_df = df[relevant_cols]\n    for c in df.columns:\n        if not c.startswith(\"_\"):\n            continue\n        _new_cols = list(df[c].iloc(0)[0].keys())\n        for _new_col in _new_cols:\n            _clean_colname = _column_names[c[1:]][_new_col]\n            flatten_df[_clean_colname] = df[c].apply(lambda x : x[_new_col]\\\n                                                if type(x) == dict else 0.0)\n            relevant_cols.append(_clean_colname)\n    return flatten_df[relevant_cols]\n\n\n## Function that enriches the information using overlapped percentage\ndef _enrich_info(idf, percentages, m_df, columns, m_measure):\n    \"\"\"\n    :params:\n    idf : unique identifier for the police department information\n    percentages : The overalapped CTs and their percentages\n    m_df : the dataframe of the metric containing all the information\n    columns : the corresponding column names of the metric, defined in config\n    m_measure : the measure (mean, median, proportion) to perform\n    \"\"\"\n    \n    ## define the updated_metrics object that will store the estimated information\n    updated_metrics = {}\n    \n    ## return None if no overlapping CTs\n    if len(percentages[idf]) == 0:\n        return ()\n    \n    ## Iterate in all Districts with the overlapped CTs and percentage\n    for idd, percentage in percentages[idf].items(): \n        ## find the corresponding row for an overlapped CT in the metric data \n        ct_row = m_df[m_df[\"GEOID\"] == idd]\n        for rcol in columns:\n            if rcol not in updated_metrics:\n                updated_metrics[rcol] = []\n            \n            ## Perform the necessary calculation to find the estimated number \n            try:\n                actual_value = ct_row[rcol].iloc(0)[0].replace(\"-\",\"\")\n                actual_value = actual_value.replace(\",\",\"\")\n                actual_value = float(actual_value.replace(\"+\",\"\"))\n                if m_measure == \"proportion\":\n                    updated_value = actual_value * percentage / 100\n                else:\n                    updated_value = actual_value\n                updated_metrics[rcol].append(updated_value)\n            except Exception as E:\n                pass\n    \n    ## Update the information in updated_metrics\n    for rcol in columns:\n        if len(updated_metrics[rcol]) == 0:\n            updated_metrics[rcol] = 0\n        else:\n            if m_measure == \"proportion\":\n                updated_metrics[rcol] = sum(updated_metrics[rcol])\n            elif m_measure == \"median\":\n                updated_metrics[rcol] = median(updated_metrics[rcol])\n            elif m_measure == \"mean\":\n                _mean = float(sum(updated_metrics[rcol])) / len(updated_metrics[rcol])\n                updated_metrics[rcol] = _mean\n    return updated_metrics\n\n\n## Master Function to process the ACS info in dept df\ndef _process_metric(metrics_df, dept_df, _identifier, olaps_percentages, metric_name):\n    \"\"\"\n    :params:\n    metrics_df : the complete dataframe containing the metrics data\n    dept_df : the geodataframe for police shape files \n    _identifier : the row identifier column corresponding to the police dept shape file \n    olaps_percentages : the overlapping percentage object calculated in previous step\n    metric_name : Name of the metric, example - education / poverty / income \n    \"\"\"\n    \n    m_df = metrics_df[metric_name]\n    m_measure = metrics_config[metric_name][\"measure\"]\n    for flag in metrics_config[metric_name]['metrics']:\n        cols = list(_column_names[flag].keys())\n        dept_df[\"_\"+flag] = dept_df[_identifier].apply(lambda x : \\\n                            _enrich_info(x, olaps_percentages, m_df, cols, m_measure))\n    return dept_df \n\n\nsubject_race_csv_content = \"\"\"W\tWhite\nW(White)\tWhite\nWhite\tWhite\nB\tBlack\nB(Black)\tBlack\nBlack\tBlack\nBlack or African American\tBlack\nBlack, Black\tBlack\nUnk\tUnknown\nUnknown\tUnknown\nUNKNOWN\tUnknown\nNo Data\tUnknown\nNO DATA ENTERED\tUnknown\nnot recorded\tUnknown\nNot Specified\tUnknown\nP\tPacific Islander\nPacific Islander\tPacific Islander\nO\tOther\nOther\tOther\nOther / Mixed Race\tOther\nNative Am\tNative American\nNative Amer\tNative American\nNative American\tNative American\nLatino\tLatino\nH\tHispanic\nH(Hispanic)\tHispanic\nHispanic\tHispanic\nHispanic or Latino\tHispanic\nA\tAsian\nA(Asian or Pacific Islander)\tAsian\nAsian\tAsian\nAsian or Pacific islander\tAsian\nAmerican Ind\tAmerican Indian\nAmerican Indian/Alaska Native\tAmerican Indian\"\"\"\n\nsubject_gender_csv_content = \"\"\"F\tFemale\nFemale\tFemale\nFEMALE\tFemale\nM\tMale\nM, M\tMale\nMale\tMale\nMALE\tMale\nNo Data\tUnknown\nnot recorded\tUnknown\nNot Specified\tUnknown\nUnk\tUnknown\nUnknown\tUnknown\nUNKNOWN\tUnknown\n-\tUnknown\"\"\"\n\n\n## utility function to get the map of raw -> standardized\ndef _get_map(content):\n    _map = {}\n    for line in content.split(\"\\n\"):\n        raw = line.split(\"\t\")[0]\n        standardized = line.split(\"\t\")[1]\n        _map[raw] = standardized\n    return _map\n\n## utility function to get the frequency count of elements \ndef _get_count(x):\n    return dict(Counter(\"|\".join(x).split(\"|\")))\n\n## utility function to cleanup the name \ndef _cleanup_dist(x):\n    try:\n        x = str(int(float(x)))\n    except Exception as E:\n        x = \"NA\"\n    return x \n\n## Create the raw-standardized maps after reading the csv content as shown in image above \nsubject_race_map = _get_map(subject_race_csv_content)\nsubject_gender_map = _get_map(subject_gender_csv_content)\n\ncolumn_config = {\n    \"SUBJECT_RACE\" : { \"variations\": [\"SUBJECT_RACT\"],  \"values_map\" : subject_race_map },\n    \"SUBJECT_GENDER\" : { \"variations\": [],  \"values_map\" : subject_gender_map },\n    }\n\n\n## master function to standardize the column names and values\ndef _standardize_columns(datadf):\n    for col, col_dict in column_config.items():\n        col_dict[\"variations\"].append(col)\n        _map = col_dict[\"values_map\"]\n        for colname in col_dict[\"variations\"]:\n            if colname in datadf.columns:\n                datadf[col] = datadf[colname].apply(lambda x : _map[x] if x in _map else \"-\")\n                \n    ## Standardize Date Column, add Year and Month\n    if \"INCIDENT_DATE\" in datadf.columns:\n        datadf[\"INCIDENT_DATE\"] = pd.to_datetime(datadf[\"INCIDENT_DATE\"])\n        datadf[\"INCIDENT_YEAR\"] = datadf[\"INCIDENT_DATE\"].dt.year\n        datadf[\"INCIDENT_MONTH\"] = datadf[\"INCIDENT_DATE\"].dt.month\n    \n    if \"LOCATION_DISTRICT\" in datadf.columns:\n        datadf[\"LOCATION_DISTRICT\"] = datadf[\"LOCATION_DISTRICT\"].astype(str)    \n\n    return datadf\n\n## Function to standardize the events data file\ndef _standardize_filename(_dept):\n    _file = [f for f in os.listdir(_base_dir + _dept) if f.endswith(\".csv\")][0]\n    old_path = _base_dir + _dept + \"/\" + _file\n    new_path = _root_dir + _dept + \"/events/\" + _file\n    shutil.copy(old_path, new_path)\n    return _file\n\ndef _process_events(pol_config):\n    ## load the given police incidents file and cleanup some missing info\n    ppath = _root_dir + _dept + \"/events/\" + pol_config[\"police_file\"]\n    events_df = pd.read_csv(ppath, low_memory=False)[1:]\n    events_df = _standardize_columns(events_df)\n\n    ## Slice the data for the given years, if given by user\n    years_to_process = pol_config[\"years_to_process\"]\n    if len(years_to_process) != 0: \n        events_df = events_df[events_df['INCIDENT_YEAR'].isin(years_to_process)]\n    \n    ## Aggregate the events by every district of the department\n    police_df = events_df.groupby(\"LOCATION_DISTRICT\")\n\n    ## [Extendable] Obtain the distribution by gender, race etc\n    police_df = police_df.agg({\"SUBJECT_GENDER\" : lambda x : _get_count(x),\\\n                               \"SUBJECT_RACE\"   : lambda x : _get_count(x)})\n    police_df = police_df.reset_index()\n    police_df = police_df.rename(columns={\n                    \"SUBJECT_GENDER\" : pol_config['event_type'] + \"_sex\",\\\n                    \"SUBJECT_RACE\" : pol_config['event_type'] + \"_race\"})\n    return police_df, events_df \n\n\ndef _load_external_dataset(pol_config):\n    ## load the dataset \n    _path = external_datasets_path + pol_config[\"path\"]\n    events2 = pd.read_csv(_path, parse_dates=[pol_config[\"date_col\"]])\n\n    ## basic standardization\n    events2['year'] = events2[pol_config[\"date_col\"]].dt.year\n    years_to_process = pol_config[\"years_to_process\"]\n    events2 = events2[events2['year'].isin(years_to_process)]\n    events2[pol_config[\"race_col\"]] = events2[pol_config[\"race_col\"]].fillna(\"\")\n    events2[pol_config[\"gender_col\"]] = events2[pol_config[\"gender_col\"]].fillna(\"\")\n    \n    ## Aggregate and cleanup\n    events2[\"LOCATION_DISTRICT\"] = events2[pol_config['identifier']].apply(\n                                                lambda x : _cleanup_dist(x))\n    temp_df = events2.groupby(\"LOCATION_DISTRICT\").agg({\n                                pol_config['gender_col'] : lambda x : _get_count(x),\\\n                                pol_config['race_col'] : lambda x : _get_count(x)})\n    \n    ## cleanup the column names\n    temp_df = temp_df.reset_index().rename(columns={\n                                pol_config['gender_col'] : pol_config[\"event_type\"]+\"_sex\", \n                                pol_config['race_col'] : pol_config[\"event_type\"]+\"_race\"})\n    return temp_df\n\n\ndef _save_final_data(enriched_df, police_df, events_df):\n    enriched_df.to_csv(_root_dir +\"/\"+ _dept + \"/enriched_df.csv\", index = False)\n    police_df.to_csv(_root_dir +\"/\"+ _dept + \"/police_df.csv\", index = False)\n    events_df.to_csv(_root_dir +\"/\"+ _dept + \"/events/events_df.csv\", index = False)\n\ndef _execute_district_pipeline(_dept, _police_config1, _police_config2=None):\n    print (\"Selected Department: \", _dept)\n    \n    ## department shape file\n    print (\". Loading Shape File Data\")\n    dept_shape_gdf = _read_shape_gdf(_dept)\n    base_plot = _plot_shapefile_base(dept_shape_gdf, _dept, overlapped_cts = {})    \n\n    ## finding overlapped CTs percentages\n    print (\".. Finding Overlapping CTs\")\n    _identifier = depts_config[_dept][\"_rowid\"]\n    state_cts = _read_ctfile(_dept)\n    overlapped_cts, olaps_percentages = find_overlapping_cts(dept_shape_gdf, state_cts, _identifier)\n    overlapped_plot = _plot_shapefile_base(dept_shape_gdf, _dept, overlapped_cts)\n    \n    ## Adding the Metrics Data\n    print (\"... Loading ACS Metrics Data\")\n    metrics_df = _cleanup_metrics_data(_dept)\n\n    ## Add Metrics to the dept df\n    print (\".... Enrichment of ACS Metrics with Overlapped Data\")\n    dept_enriched_gdf = dept_shape_gdf.copy(deep=True)\n    for metric_name in metrics_config.keys():\n        dept_enriched_gdf = _process_metric(metrics_df, dept_enriched_gdf, _identifier, \n                                            olaps_percentages, metric_name=metric_name)\n    \n    ## Find Enriched DF\n    enriched_df = _flatten_gdf(dept_enriched_gdf, _identifier)\n    enriched_df = enriched_df.rename(columns={_identifier : \"LOCATION_DISTRICT\"})\n    \n    ## Processing Police DF\n    if _police_config1 != None:\n        print (\"..... Standardizing the Police Events\")\n        police_file1 = _standardize_filename(_dept)\n        _police_config1[\"police_file\"] = police_file1\n        police_df, events_df = _process_events(_police_config1)\n    else:\n        police_df, events_df = pd.DataFrame(), pd.DataFrame()\n    \n    ## Adding any other external Police Data \n    if _police_config2 != None:\n        print (\"..... Standardizing the External Data\")\n        external_df = _load_external_dataset(_police_config2)\n        police_df = police_df.merge(external_df, on=\"LOCATION_DISTRICT\")\n    \n    ## Save Final Data\n    print (\"...... Saving the Final Data in New Repository\")\n    _save_final_data(enriched_df, police_df, events_df)\n    \n    response = {\n                \"dept_shape_gdf\" : dept_shape_gdf,\n                \"base_plot\" : base_plot,\n                \"olaps_percentages\" : _prepare_olaps_df(olaps_percentages),\n                \"overlapped_plot\" : overlapped_plot,\n                \"dept_enriched_gdf\" : dept_enriched_gdf,\n                \"enriched_df\" : enriched_df,\n                \"police_df\" : police_df,\n                \"events_df\" : events_df\n                }\n    return response\n\nfrom IPython.display import display, HTML\ndef _view_output(pipeline_resp, dpt):\n    display(HTML(\"<h3>Pipeline Output: \" + dpt + \"</h3>\"))\n    display(HTML(\"<b>GeoDataframe: \" + dpt+ \"</b>\"))\n    display(pipeline_resp[\"dept_shape_gdf\"].head())\n    display(HTML(\"<b>Districts Map: \"+ dpt+ \"</b>\"))\n    display(pipeline_resp[\"base_plot\"])\n    display(HTML(\"<b>Overlapped Tracts & Percentages : \"+ dpt+ \" </b>\"))\n    display(pipeline_resp[\"olaps_percentages\"].head(10))\n    display(HTML(\"<b>Overlapped Tracts: \"+ dpt+ \"</b>\"))\n    display(pipeline_resp[\"overlapped_plot\"])\n    display(HTML(\"<b>Final Enriched Data : \"+ dpt+ \"</b>\"))\n    display(pipeline_resp[\"enriched_df\"].head())\n    if len(pipeline_resp[\"police_df\"]) > 0:\n        display(HTML(\"<b>Final Police Incidents : \"+ dpt+ \"</b>\"))\n        display(pipeline_resp[\"police_df\"].head())\n    display(HTML(\"<hr>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d489998686350a8599f3fa62f989d322870979b"},"cell_type":"markdown","source":"# <font color=\"#703bdb\">Part 3. Example Runs of Automation Pipeline</font> <hr>\n\n<a href=\"http://policingequity.org/\">Center of Policing Equity</a> is a research and action think tank that works collaboratively with law enforcement, communities, and political stakeholders to identify ways to strengthen relationships with the communities they serve. CPE is also the home of the nationâ€™s first and largest <a href=\"http://policingequity.org/national-justice-database/\">database</a> tracking national statistics on police behavior. \n\nThe main aim of CPE is to bridge the divide created by communication problems, suffering and generational mistrust, and forge a path towards public safety, community trust, and racial equity. This kernel series is my contribution to the <a href=\"https://www.kaggle.com/center-for-policing-equity/data-science-for-good\">Data Science for Good: Center for Policing Equity</a>. The contribution is focused on providing a generic, robust, and automated approach to integrate, standardize the data and further diagnose disparities in policing, shed light on police behavior, and provide actionable recommendations. \n\nFollowing are parts of Kernels Submissions in order:  \n\n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-solution-workflow-science-of-policing-equity/\">Part 1: Solution Workflow - The Science of Policing Equity </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-automation-pipeline-integration-processing\">Part 2: Data Integration and Processing : Automation Pipeline</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-example-runs-of-automation-pipeline\">Part 3: Example Runs of Automation Pipeline </a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-1-analysis-report-minneapolis-24-00013\">Part 4.1: Analysis Report - Minneapolis Police Department (24-00013) </a>   </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/4-2-analysis-report-lapd-49-00033\">Part 4.2: Analysis Report - Los Angles Police Department (49-00033) </a>   </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/4-3-analysis-report-officer-level-analysis\">Part 4.3: Analysis Report - Indianapolis Officer Level Analysis (23-00089) </a>   </li></ul>\n\nThe complete overview of the solution is shared in the *first kernel*. It explains the process and flow of automation, standardization, processing, and analysis of data. In the *second kernel*, the first component of the solution pipeline : data integration and processing is implemented. It processes both core level data as well as department level data. In the *third kernel*, this pipeline is executed and run for several departments. After all the standardized and clean data is produced, it is analysed with different formats of the Analysis Framework in 4.1, 4.2 and 4.3 kernels. In *kernel 4.1*, core analysis is done along with link with crime rate and poverty data. In *kernel 4.2*, core analysis is done along with statistical analysis. In *kernel 4.3*, officer level analysis is done. \n\n<hr>\n\nThis kernel, is the third of the series. In this kernel, the execution of the pipeline run for varios police department is done. \n\n## <font color=\"#703bdb\">Kernel Conents </font> \n\n<ul>\n    <li><a href=\"#1\"> 1.  Pipeline Run :  Core Data Integration and Processing   </a>  </li>\n    <li><a href=\"#2\">2.  Pipeline Run :  LAPD Department 49-00033  </a>  </li>\n    <li><a href=\"#3\">3.  Pipeline Run :  Minneapolis Department 24-00013  </a>  </li>\n    <li><a href=\"#4\">4.  Pipeline Run :  San Fransisco Department - Dept : 49-00081   </a>  </li>\n    <li><a href=\"#5\">5.  Pipeline Run :  Oakland Department - Dept : 49-00035   </a>  </li>\n    <li><a href=\"#6\">6.  Pipeline Run :  Indianapolis Department 23-00089  </a>  </li>\n    <li><a href=\"#7\">7.  Pipeline Run :   Dallas Department 37-00049  </a>  </li>\n    <li><a href=\"#8\">8.  Pipeline Run :  Minneapolis Department 24-00098  </a>  </li>\n</ul>\n\n<a id=\"1\"></a>\n## <font color=\"#703bdb\">1. Pipeline Run : Core Data Integration and Processing  </font> <hr>\n\nFirst, we will run the core part of the pipeline (component A) - The core data processing and standardization. This is common for all the departments. "},{"metadata":{"trusted":true,"_uuid":"430e6ac146c5621ee824aa90bd95e840994b22e8"},"cell_type":"code","source":"_run_standardization_pipeline()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273f953a0c7c940ab494e278c17f1a00f05ffa08"},"cell_type":"markdown","source":"Next, we will run the component B for selected departments. To execute we will define two params: \n \n> **_dept :** \"Dept_49-00033\"  \n> **police_config1:** config file for given police incidents data  \n> **police_config2:** any external police incidents data to be integrated  \n\n<a id=\"2\"></a>\n## <font color=\"#703bdb\">2. Pipeline Run : LAPD Department - Dept : 49-00033  </font> <hr>"},{"metadata":{"trusted":true,"_uuid":"049632743be6471e6916977ec5dd7bb6e5a09eab"},"cell_type":"code","source":"## select department \n_dept = \"Dept_49-00033\"\n\n## given police data config \n_police_config1 = { 'event_type' : 'arrest', \"years_to_process\" : []}\n\n# ## external police data config\n_police_config2 = {  'path' : \"la_stops/vehicle-and-pedestrian-stop-data-2010-to-present.csv\", \n                     'event_type' : 'vstops',\n                     'identifier' : \"Officer 1 Division Number\" , \n                     'gender_col' : 'Sex Code', \n                     'race_col' : 'Descent Code', \n                     'date_col' : \"Stop Date\", \n                     'years_to_process' : [2015] }\n\n## call the trigger for the given department and their configurations\npipeline_resp = _execute_district_pipeline(_dept, _police_config1, _police_config2)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52206e58ccda43f503dc5c1af51c88f3c12a5531"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n## <font color=\"#703bdb\">3. Pipeline Run : Minneapolis Department - Dept : 24-00013  </font> <hr>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"075c1f2dbf43fa291cb205af141f16e85193c413"},"cell_type":"code","source":"_dept = \"Dept_24-00013\"\n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : [2012, 2013, 2014, 2015, 2016, 2017]}\n_police_config2 = { \"path\" : \"minneapolis_stops/Minneapolis_Stops.csv\", \"years_to_process\" : [2016] , 'identifier' : \"policePrecinct\" , 'gender_col' : 'gender', 'race_col' : 'race', 'date_col' : 'responseDate', 'event_type' : 'vstops'}\n\npipeline_resp = _execute_district_pipeline(_dept, _police_config1, _police_config2)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9491a81116cee91b236f9d86f3f8f8a6a53c24af"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## <font color=\"#703bdb\">4. Pipeline Run : San Fransisco Department - Dept : 49-00081  </font> <hr>\n\nThe police data for this department does not contains race and gender related info, so as of now the pipeline will ignore it. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3b843223ca6474ec54e605472bdd8cebf83eb3ab"},"cell_type":"code","source":"_dept = \"Dept_49-00081\" \n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : []}\npipeline_resp = _execute_district_pipeline(_dept, _police_config1 = None)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2524ebd91d10e082f3b56d90830ed02fb86c2caa"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## <font color=\"#703bdb\">5. Pipeline Run : Oakland Department - Dept : 49-00035  </font> <hr>"},{"metadata":{"trusted":true,"_uuid":"1550ed40eef30e56cbb32169bf216c047a5c1c92"},"cell_type":"code","source":"_dept = \"Dept_49-00035\" \n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : []}\npipeline_resp = _execute_district_pipeline(_dept, _police_config1 = None)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5154fc0d5d340aeb907f56ff6fcbf544e3887106"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n## <font color=\"#703bdb\">6. Pipeline Run : Indianapolis Department - Dept : 23-00089  </font> <hr>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"99b99db9c267b4428caad6b14fdac469d0123e1b"},"cell_type":"code","source":"_dept = \"Dept_23-00089\"\n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : []}\npipeline_resp = _execute_district_pipeline(_dept, _police_config1)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e5c74863a0fe4f763e1579f3e461a1cf6fe8ae8"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n## <font color=\"#703bdb\">7. Pipeline Run : Dallas Department - Dept : 37-00049  </font> <hr>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"288549c72a8b83234d2ee63f37a562499a471acc"},"cell_type":"code","source":"_dept = \"Dept_37-00049\"\n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : [2016]}\npipeline_resp = _execute_district_pipeline(_dept, _police_config1)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34f00444f6a1d31f2e5a2282b8f1af5b8bc80064"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n## <font color=\"#703bdb\">8. Pipeline Run : Minneapolis Department - Dept : Dept_24-00098  </font> <hr>"},{"metadata":{"trusted":true,"_uuid":"30f4841db129fa69fe09ddd00fb705e7c6bdc246"},"cell_type":"code","source":"_dept = \"Dept_24-00098\"\n_police_config1 = { 'event_type' : 'uof', \"years_to_process\" : [2015, 2016, 2017]}\npipeline_resp = _execute_district_pipeline(_dept, _police_config1)\n_view_output(pipeline_resp, _dept)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"993356234b582c98b3d3bdb1df4101ac751e6a54"},"cell_type":"markdown","source":"In this kernel, I explained how the pipeline which was developed in the previous kernel can be scaled to process multiple departments. We obtained the ouputs of these pipeline run. As the next step, we will write the final part of the pipeline - Analysis Component, and see the examples reports. \n\n<a href=\"https://www.kaggle.com/shivamb/4-1-analysis-report-minneapolis-24-00013\">Next Kernel</a> - Analysis Framework and Report : Minneapolis Police Department (24-00013) "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}