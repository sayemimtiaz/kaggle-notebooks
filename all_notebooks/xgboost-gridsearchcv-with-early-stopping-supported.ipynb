{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One nice feature about XGBoost is the possibility to do early-stopping. However, this functionality is not integrated well with scikit-learn's API. So I wrote a custom function which allows us to do grid search and early-stopping at the same time.\n\nBy the way, if you choose not to specify `early_stopping_rounds`, this custom function would reduce to an ordinary grid searcher.\n\nThe logic and implementation of this custom function might not be ideal and any suggestion is welcome."},{"metadata":{"trusted":true},"cell_type":"code","source":"from copy import deepcopy\nfrom itertools import product\nfrom collections import defaultdict\n\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def GridSearchCV_XGB_early_stoppping(param_grid, fit_params, scorer, cv, X, y):\n    \"\"\"This function performs grid search for the best set of parameters of XGBoost model with early stopping.\n\n    Args:\n        param_grid (dict): The parameter ranges for which the function searches.\n        fit_params (dict): The fitting parameters for XGBoost.\n        scorer (_PredictScorer): The sklearn's scorer instance.\n        cv (model_selection._split): The sklearn's split instance.\n        X (DataFrame): The input data matrix.\n        y (Series): The ground truth label.\n        \n    Returns:\n        dict: The best set of parameters found via grid search.\n    \"\"\"\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy()\n    if isinstance(y, pd.Series):\n        y = y.to_numpy()\n        \n    param_names, param_values = zip(*list(param_grid.items()))\n\n    cv_best_iterations = defaultdict(list)\n    cv_results = defaultdict(list)\n\n    for train_index, test_index in cv.split(X, y):\n        X_in, X_out = X[train_index], X[test_index]\n        y_in, y_out = y[train_index], y[test_index]\n\n        fit_params_cv = deepcopy(fit_params)\n        fit_params_cv['eval_set'] = [(X_out, y_out)]\n\n        for value_combination in product(*param_values):\n            param_grid_cv = tuple(zip(param_names, value_combination))\n            xgboost = XGBRegressor(**dict(param_grid_cv))\n\n            xgboost.fit(X_in, y_in, **fit_params_cv)\n            best_iteration = xgboost.get_num_boosting_rounds() if 'early_stopping_rounds' not in fit_params_cv else xgboost.best_iteration\n            cv_best_iterations[param_grid_cv].append(best_iteration)\n\n            score = scorer(xgboost, X_out, y_out)\n            cv_results[param_grid_cv].append(score)\n        \n    best_params_xgb, score_list = max(cv_results.items(), key=lambda x: np.array(x[1]).mean())\n\n    # Note that our XGBoost model may stop early,\n    # so we calculate the mean of the actual number of estimators in each fold,\n    # in place of the originally planned n_estimators after finishing cross validation.\n    n_estimators = int(round(np.array(cv_best_iterations[best_params_xgb]).mean()))\n    \n    best_params_xgb = dict(best_params_xgb)\n    best_params_xgb['n_estimators'] = n_estimators\n\n    print (\"Best score: {:.3f}\".format(np.array(score_list).mean()))\n    print (\"Best Parameters: {}\".format(best_params_xgb))\n    \n    return best_params_xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try it on a real [dataset](https://www.kaggle.com/c/home-data-for-ml-course)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nseed = 0\n\n# Read the data\nX = pd.read_csv('../input/house-prices-data/train.csv', index_col='Id')\n\n# For simplicity, we drop all categorical features\nX = X.select_dtypes(exclude=['object'])\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import KFold\n\nparam_grid = {\n    'objective': ['reg:squarederror'],\n    'n_estimators': [200, 500, 1000, 1500, 2000],\n    'max_depth': [2],\n    'learning_rate': [0.1],\n    'random_state': [seed]\n}\n\nfit_params = {\n    'eval_metric': \"rmse\",\n    'early_stopping_rounds': 100,\n    'verbose': False\n}\n\nscorer = make_scorer(mean_absolute_error, greater_is_better=False)\nkf = KFold(n_splits=5, shuffle=True, random_state=seed)\n\nbest_params_xgb = GridSearchCV_XGB_early_stoppping(param_grid, fit_params, scorer, kf, X_train, y_train)\n\nbest_xgb = XGBRegressor(**best_params_xgb)\nbest_xgb.fit(X_train, y_train, eval_metric=fit_params['eval_metric'], verbose=False)\nbest_score = scorer(best_xgb, X_valid, y_valid)\nprint (\"The best score for XGBoost on validation set is {:.3f}\".format(best_score))\n\nnp.save('best_params_xgb.npy', best_params_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can load the model back with previously found parameters:\nfinal_xgb = XGBRegressor(**np.load('best_params_xgb.npy', allow_pickle=True).item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook ends here and please kindly give a vote if you find it helpful. You are also welcome to check my other notebooks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}