{"cells":[{"metadata":{},"cell_type":"markdown","source":"<blockquote>\nMass-customization is related to optimizing the balance between flexibility, strongly required by the customer-focused industries and manufacturing efficiency, which is critical for market competitiveness. In the conventional industries, process in which custom product design is made, validated and manufactured is long and expensive. Some of the common approaches for addressing those issues are parametric product modeling and Finite Element Analysis (FEA). However, the costs involved are still relatively high, because of the very special expertise needed and the cost of the specialized software. Also, the specific design of the product cannot be validated in a real-time, which often leads to making hard compromises between the specific customer requirements and structural properties of the product in its exploitation. In this paper, we propose the novel methodology for real-time structural analysis assistance for custom product design. We introduce the concept of so-called compiled FEA model, a Machine Learning (ML) model, consisting of dataset of characteristic product parameters and associated physical properties, selected ML algorithms and the sets of associated hyperparameters. The case study of creating a compiled FEA model for the case of internal orthopedic fixator is provided.\n</blockquote>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"The contemporary production has shown significant progress in adopting disrupting technologies such as rapid prototyping, cloud-based storage, enhanced interoperability of diverse enterprise information systems in the value chain and last but not the least, Internet of Things. Two of the most qualitative effects of such digitalisation to the production processes are more efficient mass-customization (Da Silveira et al, 2001) and streamlined collaboration-based value chain (Simatupang and Sridharan, 2005). While latter unleashed the vast, diverse real-time data about operations, logistics and product lifecycle, the former pushed the trend of servitization (Vandermerwe and Rada, 1988) over the limits. This trend created the opportunities for enhanced collaboration in a product value chain and affordable use of high-end services in the whole production span, starting from structural product analysis to marketing automation and micro-customer segmentation."},{"metadata":{},"cell_type":"markdown","source":"In this paper, we explore the practical impact of using the new disrupting technologies (namely, Machine Learning and cloud-based integration) to resolving the problem of cost- and time-efficient validation of the design of the custom product, based on product family generic design. Such generic design is often represented by the parametric model of the complex product geometry, with other associated relevant features, such as exploitation and environment properties, material properties and other."},{"metadata":{},"cell_type":"markdown","source":"Customized product design problem is often solved by parametric modeling. Instead of designing the custom product instance from scratch (or by adapting the existing model to new desired properties), designers can choose the appropriate values of the previously defined, critical features of the product family geometric and structural properties, namely, the product parameters. Those choices are made based on the different criteria, including customer requirements, part and material market cost and availability, product pricing policies, exploitation conditions, manufacturability and others. The set of the product’s parameters combined with the other fixed properties is called a parametric product model. The benefits of such a product modeling approach for customization are numerous. The parameters can be used to adapt the product design to the different aspects of its exploitation, as well as manufacturing, such as manufacturability (if the design service is outsourced), cost (including materials and manufacturing complexity), assembly restrictions and customer-focused requirements, such as usability and customer-tailored design (for example, special medical devices that need to be fully adopted to the patient's physionomy and physiology), among others."},{"metadata":{},"cell_type":"markdown","source":"In any stage of the custom product design, the single design instance may be validated. Such validation can be relatively simple and quick (for example, inspection of the product visual properties by the customer) but sometimes, very troublesome and significant cost-incurring, such as testing of the product instance physical properties and its integrity in the exploitation conditions. Some of the physical properties that can be of great importance for the design are deformation, stress and product mass."},{"metadata":{},"cell_type":"markdown","source":"Testing in exploitation conditions is often replaced by simulating those conditions by using Finite Element Analysis (FEA) method (Cook, 2007)."},{"metadata":{},"cell_type":"markdown","source":"FEA could help to calculate the physical properties of the product in exploitation, before its prototyped and tested in realistic environments. For example, it could highlight the occurrence of critical stresses and deformations in the product areas. Unfortunately, the structural analysis of the customized products is often removed from the design pipeline due to the mass-customization related time and cost pressures (FEA software annual subscription rates are as high as tens of thousands of dollars), long duration (complex product FEA alone, even without considering FEA model preparation, can last for hours, even days), high-level expertise requirements and consequently, high cost of the service."},{"metadata":{},"cell_type":"markdown","source":"We addressed the above issues by assuming the following scenario (see Fig.1). A manufacturing company maintains the parametric model of the product family design. Upon customer request, the designer needs to create this model’s instance, so this instance meets all the given requirements. Instead of launching the FEA on the specific instance, the designer is assisted in a real-time by the software which is using the model we call the “compiled” generic FEA model. This software is integrated with the CAD package designer use."},{"metadata":{},"cell_type":"markdown","source":"<figure>\n    <img src=\"https://novafabrika.com/notebooks/compiled_models/concept.png\" width=\"450\">\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.1 Concept of using compiled FEA models for real-time assistance in product design and validation</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"Compiled FEA model is based on the physical properties (for example, level of mechanical stresses in critical product areas, product mass and similar) of the number of “characteristic” data instances. Characteristic data instances dataset is relatively large collection of product model parameter (lengths, widths, distances, material properties, etc.) values in the selected regions, associated with previously calculated mechanical features (such as stresses and product mass). Those properties are calculated once (by using FEA software) for each of the parameters' instances, for the whole product family and then used to fit the prediction function, derived by using a Machine Learning (ML) algorithm (Michie, 1968). Therefore, compiled FEA model is actually serialized ML model and it involves dataset with characteristic instances, selected ML algorithm and best performing hyperparameters. From the performance point of view, predicting the physical properties based on the specific set of the parametric model values is trivial and such service can be executed in a real-time, during the custom product design. More important, no additional cost is incurred."},{"metadata":{},"cell_type":"markdown","source":"Key hypothesis of the research work behind this paper is that based on the above dataset, ML models can be developed for predicting physical properties of the custom product which was instantiated by selecting the appropriate design parameters with sufficient accuracy. Another hypotheses, which will not be addressed in this paper is that multi-criteria optimization methods (Marler and Arora, 2004) can be used to identify all local optimums, namely, to identify the characteristic instances from the dataset that are associated with best combination of physical properties. Some initial work addressing the optimization problem has been already done (Korunović, 2019)."},{"metadata":{},"cell_type":"markdown","source":"The concept of the solution has been proposed by the authors and previously published (Korunović and Zdravković, 2019). In this paper, the concept is further elaborated and demonstrated by considering realistic design and exploitation aspects (dataset), with improved methodology description, analysis of the results and their visualization."},{"metadata":{},"cell_type":"markdown","source":"The remainder of the paper is structured as it follows. First, a novel methodology for facilitating real-time assistance in validating the custom product design is presented. Then, the methodology is demonstrated in the case study of validating the design of the internal fixator medical device. Finally, the guidelines for the implementation of the methodology and its use in daily practice is provided."},{"metadata":{},"cell_type":"markdown","source":"# Methodology"},{"metadata":{},"cell_type":"markdown","source":"The process in which compiled FEA model is built consists of two major activities: design of experiment and training the prediction model."},{"metadata":{},"cell_type":"markdown","source":"Design-of-experiment feature of the selected FEA tool is used to create the dataset of characteristic product instances, based on the selected product family parametric model."},{"metadata":{},"cell_type":"markdown","source":"Then, ML prediction model is created by fitting the selected ML algorithm with the dataset above, where design parameters are considered as input and physical properties as output features. Prediction model is developed by using Python programming language. Its development follows the typical ML pipeline, namely correlation analysis, feature selection, algorithm selection and optimization of the selected algorithm hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"Correlation analysis aim is to reduce the problem dimensionality. For very complex products, number of design parameters can be measured in hundreds. While creating compiled model for such a product is one-time job and thus it does not have a significant effect to a process, prediction (including neccessary data pre-processing) may come with a computational cost and consequently slower performance which could affect user experience. By selecting the most relevant product geometrical properties, we can address this problem. Two-way correlation analysis will be performed. First, correlation of the individual parameters with the physical properties will be assessed by looking at the Pearson coefficients. Second, Recursive Feature Elimination (RFE) (Guyon et al, 2002) method will be used to assess the combined relevance of all n-tuples of input features to each of the individual output features."},{"metadata":{},"cell_type":"markdown","source":"Different ML algorithms will be tested in order to choose the one with the least Mean Absolute Error (MAE) - a key indicator for assessing the accuracy. Selected algorithms are linear regression (LinearRegression), K-Nearest Neighbors (KNeighborsRegressor), Support Vector Machine Regressor (SVR), Decision Tree (DecisionTreeRegressor) and two ensemble methods, namely Random Forest (RandomForestRegressor) and Gradient Boosting (GradientBoostingRegressor)."},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbors (Altman, 1990) is a non-parametric method used since the beginning of 1970-ties. It is so-called instance-based method; it stores all available cases/instances and classifies new cases based on a similarity measure (namely, a distance function). Support Vector Machine (SVM) belongs to the group of kernel methods (Cortes and Vapnik, 1995). It was initially developed for two-group classification problems. Decision Tree or in this case so-called regression tree is the method in which observations about some item, represented as branches are used to make decisions about its target values, represented as leaves. Random Forest (Breiman, 2001) belong to a group of ensemble methods that combine a number of decision trees, and then adopt a mean forecast of the predictions of the individual trees. Random Forest is today considered as one of the most powerful algorithms in the Machine Learning without considering Artificial Neural Networks, namely deep learning architectures. Gradient Boosting (Friedman, 2001) adopts the idea of boosting - an optimization of a suitable cost function (Breiman, 1997), where an ensemble of weak prediction models, namely decision trees are staged one after another."},{"metadata":{},"cell_type":"markdown","source":"Some of the selected algorithms, namely K-Nearest Neighbors and Support Vector Machine regressor require that before training data is normalized (scaled in (0,1) range). Feature scaling is required in order to reduce the training time and improve the prediction accuracy."},{"metadata":{},"cell_type":"markdown","source":"Those algorithms will be used to develop respective prediction models and test their accuracies. The algorithms with the best performances, as validated by K-fold Cross Validation method will be selected, trained and produced models will be serialized - those models is actually what we call compiled models for real-time structural analysis assistance. Standard deviations of the output features will be used as reference values for assessing the accuracies."},{"metadata":{},"cell_type":"markdown","source":"K-Fold Cross validation is a method which produce reliable prediction accuracy metrics for a given dataset. Instead of a single split between data for training and testing, it does k-1 splits where each of the folds/sections of data is used as a test set. Hence, the model will be validated in k test runs, each of which will produce an accuracy measure. Mean of those values is then adopted as accuracy of the prediction model."},{"metadata":{},"cell_type":"markdown","source":"Validation is being carried out for predicting each of the output features, namely, product physical properties. It is expected that the performance of the models based on different estimators will differ for some of the physical properties data. Thus, all models, associated with the set of the optimal settings will be serialized. Obviously, those with the best performances for the specific physical properties will be used for prediction."},{"metadata":{},"cell_type":"markdown","source":"Each of the estimators used is associated with the set of so-called hyper-parameters, which define its different properties, related to the way how the model is trained and validated. The best prediction performance for the given dataset is achieved only with a unique set of their values. This set, for each of the estimators and each of the output features is typically determined in a process called Grid Search optimization (Lerman, 1980). Grid Search calculates accuracy score (per defined scoring function using the specific metric) across defined hyper-parameter space (defined by the value ranges and/or enumerations), most desirably by using K-Fold Cross Validation method. The search through the combination of the hyper-parameter value from the defined space can be exhaustive (all combinations) or randomized."},{"metadata":{},"cell_type":"markdown","source":"All steps that involve use of estimators, namely RFE and training models with data, must be carried out with the same conditions. This applies to using not only same parameters (k) but also same data in different steps of K-Fold Cross Validation. This is especially important for models which are trained with small number of instances - n-tuples of parameters and physical properties; in those cases, the results (especially in the optimization step) can be misleading. The exception from this rule may be the optimization process for the regression problems, in case that NMAE is selected as a model metrics (which is the case here). Then, Mean Squared Error (MSE) or R^2 regression score could be used."},{"metadata":{},"cell_type":"markdown","source":"Following key Python libraries will be used in developing and using ML prediction model:\n- Numpy is a library for handling large, multi-dimensional arrays and matrices and a set of associated mathematical functions for their manipulation.\n- Pandas is a library for data manipulation and analysis, released in 2008. It features a DataFrame object, which is most often used in manipulating data in software which implements some form of Machine Learning. \n- Sckit-learn (Pedregosa et al, 2011), initially released in 2007 is Machine Learning library consisting of number of classification, regression and clustering algorithms, ensemble methods, data pre-processing tools, metrics, feature engineering tools and other.\n- Matplotlib is a plotting library.\n- Pickle is a Python module which implements serializing and de-serializing Python object structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, make_scorer, r2_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFECV\nimport collections\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Case study"},{"metadata":{},"cell_type":"markdown","source":"The compiled FEA model is developed for the case of orthopedic device – internal fixator, used in subtrochanteric fractures of thigh bone (femur). This is the case of highly customizable product which needs to be fitted to the different requirements arising from patient physical and physiological properties, some of many different types of fractures, etc. The process in which this fitting is carried out is out of the scope of the research behind this paper. The fixator parametric model has been created by using SolidWorks CAD software. In this case, it is defined by 6 relevant geometry parameters and fixed design. The illustration of the model is provided on Figure 1 below."},{"metadata":{},"cell_type":"markdown","source":"<figure>\n    <img src=\"https://novafabrika.com/notebooks/compiled_models/fiksator.png\" width=\"500\">\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.2 Parametric model of internal fixator product</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"Design Explorer module of ANSYS FEA software, which was used for calculation, features the design-of-experiment functionality. Namely, it is used to generate the set of values of input parameters that defines the collection of characteristic product instances. These values are then used to create the CAD model instances in SolidWorks and send them back to ANSYS for calculation of physical properties. Central Composite Design (CCD)/Face Centered/Enhanced method was applied in planning the experiment (dataset generation), meaning that extreme values of the parameters are also included in the dataset (Korunović and Zdravković, 2020)."},{"metadata":{},"cell_type":"markdown","source":"Created dataset (referred to as DOE6) is used as input to the typical ML pipeline. This dataset, with six parameters counts 89 rows. Small number of data instances was used in a case study for the practical reasons (single instance calculation of physical properties by ANSYS takes time) as well as because of strong representativeness of data generated by design-of-experiment feature, namely balanced distribution of parameters value over the given range."},{"metadata":{"trusted":true},"cell_type":"code","source":"df6=pd.read_csv(\"../input/geometry-and-physical-properties-of-fixator/DOE6.csv\", \n                skiprows=4, \n                names=['Name',\n                       'Bar length',\n                       'Bar diameter',\n                       'Bar end thickness',\n                       'Radius trochanteric unit',\n                       'Radius bar end',\n                       'Clamp distance',\n                       'Total Deformation Maximum',\n                       'Equivalent Stress',\n                       'P9',\n                       'P10',\n                       'P11',\n                       'Fixator Mass'], \n                usecols=['Bar length',\n                         'Bar diameter',\n                         'Bar end thickness',\n                         'Radius trochanteric unit',\n                         'Radius bar end',\n                         'Clamp distance',\n                         'Total Deformation Maximum',\n                         'Equivalent Stress',\n                         'Fixator Mass'])\ndf6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df6.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of the output features data in the dataset is illustrated by using boxplots in the figure below."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (14,14)\nplt.rcParams.update({'font.size': 11})\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\nplt.subplots_adjust(wspace=0.4, hspace=0.3)\n\ndefdict={'DOE6':df6['Total Deformation Maximum'].values}\nax[0].set_title('Total Deformation Max')\nax[0].boxplot(defdict.values(), widths=(0.4))\nax[0].set_xticklabels(defdict.keys())\nax[0].grid(color='gray', ls = '-.', lw = 0.2)\nax[0].set_ylabel('mm')\n\nstrdict={'DOE6':df6['Equivalent Stress'].values}\nax[1].set_title('Equivalent Stress')\nax[1].boxplot(strdict.values(), widths=(0.4))\nax[1].set_xticklabels(strdict.keys())\nax[1].grid(color='gray', ls = '-.', lw = 0.2)\nax[1].set_ylabel('MPa')\n\nmasdict={'DOE6':df6['Fixator Mass'].values}\nax[2].set_title('Fixator Mass')\nax[2].boxplot(masdict.values(), widths=(0.4))\nax[2].set_xticklabels(masdict.keys())\nax[2].grid(color='gray', ls = '-.', lw = 0.2)\nax[2].set_ylabel('kg')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<figure>\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.3 Distribution of the physical properties values in the dataset (boxplots)</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"Standard deviations for Total Deformation Maximum, Equivalent Stress and Fixator Mass are as it follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('std(def) = ' + str(df6['Total Deformation Maximum'].values.std()))\nprint('std(str) = ' + str(df6['Equivalent Stress'].values.std()))\nprint('std(mas) = ' + str(df6['Fixator Mass'].values.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation analysis"},{"metadata":{},"cell_type":"markdown","source":"Analysis of data correlation was carried out by considering Pearson linear correlation and Recursive Feature Elimination (RFE) methods. The aim of the analysis is to determine if the dimensionality of the problem can be reduced, namely if it is reasonable to exclude some of the input variables from the training dataset."},{"metadata":{},"cell_type":"markdown","source":"Calculation and presentation of the correlation matrix with Python is straighforward."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:,.3f}'.format\ndf6.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It was found that there existed significant linear correlation between: \n- Bar length and total deformation (p=-0.950)\n- Bar length and fixator mass (p=0.899)\n- Bar length and equivalent stress (p=-0.655)\n- Bar end thickness and equivalent Stress (p=-0.633)\n\nNotable linear correlation is found between:\n- Bar diameter and Total Deformation Maximum (p=-0.250)\n- Bar diameter and Fixator mass (p=0.397)\n\nOther input values did not have notable linear correlation with output variables, namely deformation, stress and mass. This implies that some features could be removed from the model, namely: radius trochanteric unit, radius bar end and clamp distance."},{"metadata":{},"cell_type":"markdown","source":"The correlation of the individual geometrical features with the physical properties is also illustrated with the scatter plots, displayed below."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (14,14)\nplt.rcParams.update({'font.size': 12})\n\nfig, ax = plt.subplots(3, 3, figsize=(17, 12))\nplt.subplots_adjust(wspace=0.4, hspace=0.3)\n\nax[0,0].set_title('p=-0.950')\nax[0,0].plot(df6['Bar length'].values,df6['Total Deformation Maximum'].values,'g.')\nax[0,0].set_xlabel('Bar length (mm)')\nax[0,0].set_ylabel('Total Deformation Max (mm)')\nax[0,0].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[0,1].set_title('p=-0.655')\nax[0,1].plot(df6['Bar length'].values,df6['Equivalent Stress'].values,'g.')\nax[0,1].set_xlabel('Bar length (mm)')\nax[0,1].set_ylabel('Equivalent Stress (MPa)')\nax[0,1].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[0,2].set_title('p=0.899')\nax[0,2].plot(df6['Bar length'].values,df6['Fixator Mass'].values,'g.')\nax[0,2].set_xlabel('Bar length (mm)')\nax[0,2].set_ylabel('Fixator Mass (Kg)')\nax[0,2].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[1,0].set_title('p=-0.250')\nax[1,0].plot(df6['Bar diameter'].values,df6['Total Deformation Maximum'].values,'y.')\nax[1,0].set_xlabel('Bar diameter (mm)')\nax[1,0].set_ylabel('Total Deformation Max (mm)')\nax[1,0].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[1,1].set_title('p=-0.051')\nax[1,1].plot(df6['Bar diameter'].values,df6['Equivalent Stress'].values,'r.')\nax[1,1].set_xlabel('Bar diameter (mm)')\nax[1,1].set_ylabel('Equivalent Stress (MPa)')\nax[1,1].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[1,2].set_title('p=0.397')\nax[1,2].plot(df6['Bar diameter'].values,df6['Fixator Mass'].values,'y.')\nax[1,2].set_xlabel('Bar diameter (mm)')\nax[1,2].set_ylabel('Fixator Mass (Kg)')\nax[1,2].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[2,0].set_title('p=-0.038')\nax[2,0].plot(df6['Bar end thickness'].values,df6['Total Deformation Maximum'].values,'r.')\nax[2,0].set_xlabel('Bar end thickness (mm)')\nax[2,0].set_ylabel('Total Deformation Max (mm)')\nax[2,0].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[2,1].set_title('p=-0.633')\nax[2,1].plot(df6['Bar end thickness'].values,df6['Equivalent Stress'].values,'g.')\nax[2,1].set_xlabel('Bar end thickness (mm)')\nax[2,1].set_ylabel('Equivalent Stress (MPa)')\nax[2,1].grid(color='gray', ls = '-.', lw = 0.2)\n\nax[2,2].set_title('p=0.048')\nax[2,2].plot(df6['Bar end thickness'].values,df6['Fixator Mass'].values,'r.')\nax[2,2].set_xlabel('Bar end thickness (mm)')\nax[2,2].set_ylabel('Fixator Mass (Kg)')\nax[2,2].grid(color='gray', ls = '-.', lw = 0.2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<figure>\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.4 Correlation of the individual geometrical features with the physical properties (Pearson)</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"The issue of the linear correlation based on Pearson coefficient is that it can help in assessing only the relevance of  individual input features for prediction of the output ones. In other words, while one specific input feature may have very low correlation with the output, it may appear that in combination with the other ones, its changes may significantly affect the output features."},{"metadata":{},"cell_type":"markdown","source":"Thus, to complement the correlation analysis, a Recursive Feature Elimination (RFE) method is applied with goal to explore the relevance ranking of the subsets of the input features. RFE is a method which performs a backward feature elimination. The algorithm begins with the set of all features and successively eliminates the feature which induces the smallest effect to the output features. It can be applied by using the selected algorithms, in our case - simple linear regression, SVM, Decision Trees, Random Forest and Gradient Boosting regressors. KNN is excluded because its regressor does not expose the attributes relevant to RFE."},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef getModels():\n    models = []\n    models.append(('LRE', LinearRegression()))\n    models.append(('KNN', KNeighborsRegressor()))\n    models.append(('SVR', SVR(kernel='linear')))\n    models.append(('DTR', DecisionTreeRegressor()))\n    models.append(('RFR', RandomForestRegressor()))\n    models.append(('GBR', GradientBoostingRegressor()))\n    return models\n\ndef getRanking(X,y,model):\n    fit=RFECV(model, cv=3).fit(X,y)\n    return fit.ranking_, fit.support_\n\ndef getRFERanks(X,y):\n    models=getModels()\n    ranks= collections.defaultdict(dict)\n    for x in range(y.shape[1]):\n        for modelname, model in models:\n            if(modelname!='KNN'):\n                rank, supp=getRanking(X,y[:,x],model)\n                ranks[x+1][modelname]=rank\n    return ranks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df6.values[:,:6]\ny=df6.values[:,6:]\nranks=getRFERanks(X,y)\ndefranks=ranks[1]\nstrranks=ranks[2]\nmasranks=ranks[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RFE method calculate ranks which are the measures of the relevance of individual input features in combination with others for predicting the output features. Ranks are calculated in the range (1,5) where less value means better relevance/correlation. All results are then displayed in bar charts in order to provide effective illustration of the ranks by the different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,5)\nplt.rcParams.update({'font.size': 12})\nfig, ax = plt.subplots(1, 3, figsize=(17, 5))\nplt.subplots_adjust(wspace=0.2, hspace=0.8)\n\nx = np.arange(6)\nwidth = 0.15\n\nax[0].set_xlabel('Input features')\nax[0].set_ylabel('Rank of corrn with Total Deformation Max (1-5)')\nax[0].bar(x, defranks['LRE'], width=width, label='LinReg', color='r')\nax[0].bar(x+width, defranks['SVR'], width=width, label='SVR', color='y')\nax[0].bar(x+2*width, defranks['DTR'], width=width, label='DecTre', color='b')\nax[0].bar(x+3*width, defranks['RFR'], width=width, label='RanFor', color='c')\nax[0].bar(x+4*width, defranks['GBR'], width=width, label='GBoost', color='m')\nax[0].legend()\n\nax[1].set_xlabel('Input features')\nax[1].set_ylabel('Rank of corr with Equivalent Stress (1-5)')\nax[1].bar(x, strranks['LRE'], width=width, label='LinReg', color='r')\nax[1].bar(x+width, strranks['SVR'], width=width, label='SVR', color='y')\nax[1].bar(x+2*width, strranks['DTR'], width=width, label='DecTre', color='b')\nax[1].bar(x+3*width, strranks['RFR'], width=width, label='RanFor', color='c')\nax[1].bar(x+4*width, strranks['GBR'], width=width, label='GBoost', color='m')\nax[1].legend()\n\nax[2].set_xlabel('Input features')\nax[2].set_ylabel('Rank of corr with Product Mass (1-5)')\nax[2].bar(x, masranks['LRE'], width=width, label='LinReg', color='r')\nax[2].bar(x+width, masranks['SVR'], width=width, label='SVR', color='y')\nax[2].bar(x+2*width, masranks['DTR'], width=width, label='DecTre', color='b')\nax[2].bar(x+3*width, masranks['RFR'], width=width, label='RanFor', color='c')\nax[2].bar(x+4*width, masranks['GBR'], width=width, label='GBoost', color='m')\nax[2].legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<figure>\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.5 Correlation ranks of the individual geometrical features with the physical properties (Recursive Feature Elimination method)</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"The relevance of first three input features is clearly confirmed by the RFE method and all algorithms. The exception is SVM regressor which produced outlier results because data wasn't normalized (requirement for SVR) for RFE estimation. RFE with some of the algorithms suggest some relevance of the input feature #5, namely clamp distance for predicting equivalent stress and deformation mass."},{"metadata":{},"cell_type":"markdown","source":"The behavior and performance of many ML algorithms are referred to as stochastic, because they involve randomness (random state initialization of the models, random selection of data in K-fold Cross Validation, etc.). For that reason, the indicators that are produced by ML models are typically calculated as a statistical measure (for example, mean) of the population of the specific indicator values produced by the ML models in multiple runs. The minor relevance of clamp distance for predicting some of the output features is not continuously visible in multiple RFE runs. Therefore, it will be excluded from the final set of features, together with trochanteric unit radius (feature #3) and bar end radius (feature #4)."},{"metadata":{},"cell_type":"markdown","source":"However, it is important to strongly highlight that the decision to reduce the dimensionality of the parameter set in this specific case would not be practical because the complexity of the product is very low, so the possible savings in the computational performance are infinitesimal. Still, in cases of very complex products with hundreds of parameters, this methodological step could help achieving critical benefits."},{"metadata":{},"cell_type":"markdown","source":"## Compiled models"},{"metadata":{},"cell_type":"markdown","source":"According to the proposed concept, compiled model is actually serialized ML model for predicting the physical properties of the product, based on the parameter values. Three characteristic physical properties need to be predicted by the compiled model: maximum total deformation, maximum equivalent stress and fixator mass. Before the model is compiled, the algorithm with the best performance need to be selected from the pre-selected list that include: linear regression, K-Nearest Neighbors, Support Vector Machine regressor, Decision Tree regressor, Random Forest and Gradient Boosting regressor."},{"metadata":{},"cell_type":"markdown","source":"In the following step, the selected ML algorithms, with default hyperparameters were fitted with the dataset and the outcomes of the resulting models’ accuracies were compared. K-fold cross validation (k=4) was used for validation and Negative Mean Absolute Error (NMAE) was used as indicator. Same KFold object will be used in all relevant steps in order to get comparable data. Object is set not to shuffle data, because randomness in selecting data for the folds with such a small dataset would not be beneficial. Testing produced the results as shown in table below."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_doe6=KFold(n_splits=4, shuffle=False, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef getModels():\n    models = []\n    models.append(('LRE', LinearRegression()))\n    models.append(('KNN', KNeighborsRegressor()))\n    models.append(('SVR', SVR()))\n    models.append(('DTR', DecisionTreeRegressor()))\n    models.append(('RFR', RandomForestRegressor(random_state=7)))\n    models.append(('GBR', GradientBoostingRegressor(random_state=7)))\n    return models\n\nscaler1 = MinMaxScaler()\nscaled_df6=scaler1.fit_transform(df6)\n\nscores=[]\nfor d in range(3):\n    featurescores=[]\n    for modelname, model in getModels():\n        X=df6.values[:,:6]\n        y=df6.values[:,6+d]\n        if(modelname=='KNN' or modelname=='SVR'):\n            X=scaled_df6[:,:6]\n        fscores=cross_val_score(model, X, y, cv=cv_doe6, \n                                             scoring='neg_mean_absolute_error').mean()\n        featurescores.append(fscores)\n    scores.append(featurescores)\n    \ndfres = pd.DataFrame(scores, \n                     columns=['LRE','KNN','SVR','DTR','RFR','GBR'], \n                     index=['Total Deformation Maximum','Equivalent Stress','Fixator mass'])\ndfres.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All NMAE indicators are well within the standard deviations for the considered output features and certainly within the limits of acceptable error in structural analysis of products of this type. Given the high linear correlation, as found by the Pearson coefficients, the expectation that the linear regression method will produce good results is confirmed."},{"metadata":{},"cell_type":"markdown","source":"## Estimator optimization"},{"metadata":{},"cell_type":"markdown","source":"Grid Search method was used for optimization of hyperparameters. While NMAE is used in the table below, for the optimization, another metrics will be used, namely R^2 (coefficient of determination) regression score. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). Grid search optimization is implemented in iterative fashion where the specific set of optimal hyper-parameters is determined for each output feature (physical property) and each estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"def testModel(X, y, model, parameters, feature):\n    grid = GridSearchCV(estimator=model, param_grid=parameters, cv=cv_doe6, scoring=make_scorer(r2_score))\n    grid = grid.fit(X, y=y)\n    print('-------------------------')\n    print(\"Parameters :\", grid.best_params_)\n    print(\"R2 :\", grid.best_score_)\n    print('-------------------------')\n    return grid.best_params_, grid.best_score_ \n    \n    \nparams={'LRE':\n          {'params':\n               {'fit_intercept':[True,False],\n                'normalize': [True,False],\n                'copy_X': [True,False],\n                'n_jobs': [1,2,None]\n               }\n          },\n        'KNN':\n          {'params':\n               {'n_neighbors':[7,13,20],\n                'leaf_size':[1,5,30],\n                'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n                'p':[1,2,3,5],\n                'weights':['uniform', 'distance']\n               }\n          },\n        'SVR':\n          {'params':\n               {'C': [100], \n                'kernel': ['rbf','poly','linear','sigmoid'],\n                'degree':[2,3,5,8],\n                'coef0':[0.1,1],\n                'shrinking':[True,False],\n                'gamma':['scale','auto']\n               }\n          },\n        'DTR':\n          {'params':\n               {'max_features': ['auto', 'sqrt', 'log2'],\n                'splitter':['best','random'],\n                'max_depth':[1,10,100,None],\n                'min_samples_split': [8,9,10],\n                'criterion':['mse', 'friedman_mse', 'mae'],\n                'min_samples_leaf':[4,5,6]\n               }\n          },\n        'RFR':\n          {'params':\n               {'n_estimators': [5,7,10],\n                'max_features': ['auto'],\n                'criterion': ['mae','mse'],\n                'min_samples_split': [2,4],\n                'min_samples_leaf': [1,3,4],\n                'max_leaf_nodes':[6,8,12],\n                'bootstrap':[True,False],\n                'max_depth': [3,4]\n               }\n          },\n        'GBR':\n          {'params':\n               {'loss':['ls', 'lad'],\n                'learning_rate': [0.08,0.1],\n                'subsample'    : [1.0],\n                'criterion': ['mse'],\n                'min_samples_split': [0.7,0.8,4],\n                'min_samples_leaf': [1],\n                'n_estimators' : [100,200,600],\n                'max_depth'    : [None]\n               }\n          }\n       }\nfeatures = {0:'Total Deformation Maximum',1:'Equivalent Stress',2:'Fixator Mass'}\ngsresults= collections.defaultdict(dict)\n\nscaler2 = MinMaxScaler()\nscaled_df6=scaler2.fit_transform(df6)\n\nfor d in features.keys():\n    for modelname, model in getModels():\n        X=df6.values[:,:6]\n        y=df6.values[:,6+d]\n        if(modelname=='KNN' or modelname=='SVR'):\n            X=scaled_df6[:,:6]\n        print(features[d] + ' with ' + modelname)\n        prms,sc=testModel(X, y, model,params[modelname].values(), d)\n        gsresults[features[d]][modelname]={'PARAMS':prms}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid search optimization introduced the sets of hyper parameters which significantly improved the performance of models based on K-Nearest Neighbors and SVR. Notable improvement was made in predicting equivalent stresses with Random Forest and Gradient Boosting estimators."},{"metadata":{},"cell_type":"markdown","source":"Based on the R2 score values, several conclusions can be made. As expected, Random Forest and Gradient Boosting have shown the best general performance. Both of those ensemble methods are most often used for addressing regression and classification problems by practitioners, despite possible overfitting issues. In our case, with balanced distribution of input featues values, overfitting is not a serious concern.\nWith optimized hyper-parameters, Gradient Boosting estimator produces excellent results in predicting fixator mass and total maximum deformation, with R2>0.99 and in prediction equivalent stresses (R2=0.91). Comparable performance was achieved by Random Forest, in predicting fixator mass (R2=0.92) and maximum total deformation (R2=0.95) and by SVR, in predicting maximum total deformation (R2=0.98)."},{"metadata":{},"cell_type":"markdown","source":"Values of NMAE indicator after training the estimators by using the optimal sets of hyper-parameters are shown in the table below."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler6 = MinMaxScaler()\nscaled_df6=scaler6.fit_transform(df6)\n\n\nscores=[]\nfeatures = {0:'Total Deformation Maximum',1:'Equivalent Stress',2:'Fixator Mass'}\nfor d in features.keys():\n    y=df6.values[:,6+d]\n    featurescores=[]\n    for modelname, model in getModels():\n        X=df6.values[:,:6]\n        if(modelname=='KNN' or modelname=='SVR'):\n            X=scaled_df6[:,:6]\n        model.set_params(**gsresults[features[d]][modelname]['PARAMS'])\n        #pickle.dump(model, open('models/'+modelname+'_'+str(d)+'.sav','wb'))\n        featurescores.append(cross_val_score(model, X, y, cv=cv_doe6,\n                                             scoring='neg_mean_absolute_error').mean())\n    scores.append(featurescores)\n\ndfresopt = pd.DataFrame(scores, \n                     columns=['LRE','KNN','SVR','DTR','RFR','GBR'], \n                     index=['Total Deformation Maximum','Equivalent Stress','Fixator mass'])\ndfresopt.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the purpose of convenient comparison, the table with NMAE as predicted by the estimators with default values of the hyper-parameters is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfres.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementation"},{"metadata":{},"cell_type":"markdown","source":"To conclude, the considered research hypotheses have been convincingly confirmed in a case study. The developed ML model can be serialized as compiled FEA model and used in hypothetical CAD tool add-on – container for compiled models of selected product families. CAD model enriched with this add-on can provide real-time structural analysis assistance of custom product design and thus, significantly reduce its time and cost."},{"metadata":{},"cell_type":"markdown","source":"The Figure 3 depicts the design of the infrastructure for the implementation of the proposed solution for real-time assistance in customized product design.\nThe process starts with the development of parametric model and design of experiment. Design of experiment data is used to develop a compiled FEA model, as described above. It is then deployed as a web service resource. Web infrastructure facilitates:\n- the deployment of compiled FEA models and parametric models,\n- management (including versioning) of nongeometric model parameters (in the above example, maximal equivalent stress over the product and product mass)\n- end user authentication and tracking logic and\n- a business model (subscription based, pay per view, etc.) of choice.\n\nIt should be exposed by REST API with authentication and key verification functionalities. \nClient is considered as add-on to one of the commonly used CAD platforms. Add-on facilitates:\n- user login,\n- definition and serialization of non-geometric model parameters (e.g. exploitation and environment effects, material properties)\n- display of user interface with the addon toolbox and visualization of predicted physical properties\n- synchronous REST calls to a web service using associated compiled FEA model, where input is current set of parameters (geometric and nongeometric) and output – predicted physical properties."},{"metadata":{},"cell_type":"markdown","source":"\n<figure>\n    <img src=\"https://novafabrika.com/notebooks/compiled_models/tool.png\" width=\"600\">\n    <figcaption style=\"text-align:center;font-style:italic;font-size: smaller;\">Fig.6 Concept of the integration of CAD system with prediction services based on compiled models</figcaption>\n</figure>"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Mass-customization trend, implying the need for design and manufacturing of custom product designs with efficiency near to mass-production is a new industrial reality. Quite obviously, this trend creates new challenges in manufacturing and custom product design domains. Most of the challenges are due to the efforts in finding the right balance between flexibility, strongly required by the customer-focused industries and efficiency, which is critical for market competitiveness. In more conventional industries, this balance is often searched for by implementing outsourcing practices, even for critical activities in the manufacturing process. Another way is digitalization, which helps to facilitate fast decision making processes and thus, quick responses to the variety of demand and supply circumstances. Today, with the advance of AI methods and tools, it becomes possible to digitalise even knowledge-intensive operations and thus, not only reduce the lead time but also significantly reduce the total cost of product manufacturing."},{"metadata":{},"cell_type":"markdown","source":"The proposed solution aims to solve the problem of long and expensive custom product design process, and in specific, the need for a special (expensive) expertise in building FEA models, lots of computational resources needed and expensive FEA software. "},{"metadata":{},"cell_type":"markdown","source":"Each parametric model is defined by the finite set of parameters, mostly geometrical features. The values of those parameters, in most of the cases vary within the specific range in order to keep the integrity of the design. The level of correlation of those values with the actual physical properties of the product define the guidelines important for the ordering process. This process now includes customization sub-process, in which customer and designer actually negotiate the design that fits the customer's requirements in the best possible way, in a real time, while still maintaining the integrity of the product in the target exploitation conditions and its manufacturability."},{"metadata":{},"cell_type":"markdown","source":"The centerpiece of the proposed novel methodology is so-called compiled FEA model, offering the best approximations of non-geometric parameters, vital for the exploitation behavior and manufacturability of the custom product design. The use of compiled FEA model during geometric parameter tuning facilitates real-time review of the critical nongeometric features and immediate assessment of the designed product physical properties. Moreover, the proposed solution creates opportunities for new collaborative business models, in which the roles of CAD and FEA specialists are separated across the enterprises and FEA can be implemented as online service."},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"- Da Silveira, G., Borenstein, D., Fogliatto, F., 2001, Mass customization: Literature review and research directions. International Journal of Production Economics. 72(1), pp.1-13\n- Simatupang, T.M., Sridharan, R., 2005, The collaboration index: a measure for supply chain collaboration. International Journal of Physical Distribution & Logistics Management. 35(1)\n- Vandermerwe, S., Rada, J., 1988, Servitization of business: Adding value by adding services. European Management Journal. 6(4), pp.314-324\n- Cook, R.D., 2007, Concepts and applications of finite element analysis. John Wiley & Sons\n- Michie, D., 1968, 'Memo' functions and machine learning. Nature. 218(5136), pp.19-22\n- Marler, R.T., Arora, J.S., 2004, Survey of multi-objective optimization methods for engineering. Structural and Multidisciplinary Optimization. 26(6), pp.369–395\n- Friedman, J.H., 2001, Greedy Function Approximation: A Gradient Boosting Machine. The Annals of Statistics. 29(5), pp.1189-1232\n- Lerman, P.M., 1980, Fitting Segmented Regression Models by Grid Search. Applied Statistics. 29(1), pp. 77-84\n- Guyon, I., Weston, J., Barnhill, S., Vapnik, V., 2002, Gene Selection for Cancer Classification using Support Vector Machines. Machine Learning. 46(2002), pp.389-422\n- Breiman, L., 2001, Random forests. Machine Learning, 45(1), pp.5–32\n- Korunović, N., Marinković, D., Trajanović, M., Zehn, M., Mirković, M., Affatato, S., 2019, In Silico Optimization of Femoral Fixator Position and Configuration by Parametric CAD Model. Materials. 12(14), pp.2326\n- Altman, S., 1990, An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression. The American Statistician. 46(3), pp.175-185\n- Cortes, C., Vapnik, V., 1995, Support-vector networks. Machine Learning. 20(1995), pp.273–297\n- Breiman, L., 1997, Arcing the Edge. Technical Report 486. Statistics Department, University of California, Berkeley\n- Korunović, N., Zdravković, M., 2019, Real-time structural analysis assistance in customized product design. In: Trajanović, M., Zdravković, M., Konjović, Z. (Eds.) ICIST 2019 Proceedings Vol.1, pp.217-220, 2019\n- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Perrot, M., Duchesnay, E., 2011, Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research. 12(85), pp.2825−2830\n- Korunović, N., Zdravković, M., 2020, Dataset: Geometry and physical properties of fixator, Kaggle, doi: 10.34740/KAGGLE/DSV/1114146"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}