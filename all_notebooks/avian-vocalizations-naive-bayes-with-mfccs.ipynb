{"cells":[{"metadata":{},"cell_type":"markdown","source":"Trains a Naive Bayes classifier on [melspectrograms](https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html) and [Mel-frequency Cepstral Coefficients (MFCCs)](https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html) generated from the [Avian Vocalizations from CA & NV, USA](https://www.kaggle.com/samhiatt/xenocanto-avian-vocalizations-canv-usa) dataset, to be used as a benchmark classification model.\n\nThis version was forked from [Avian Vocalizations: Benchmark Naive Bayes](https://www.kaggle.com/samhiatt/avian-vocalizations-benchmark-naive-bayes?scriptVersionId=19012978) and adds MFCCs to the feature space."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" `imageio_ffmpeg` contains a pre-built `ffmpeg` binary, needed for mp3 decoding by `librosa`. \n    It is installed as a custom package on Kaggle. If no `ffmpeg` binary is found in `/usr/local/bin` \n    then create a softlink to the `imageio_ffmpeg` binary. \n\"\"\"\nimport os\nif not os.path.exists(\"/usr/local/bin/ffmpeg\"): \n    import imageio_ffmpeg\n    os.link(imageio_ffmpeg.get_ffmpeg_exe(), \"/usr/local/bin/ffmpeg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa as lr\n# from librosa import feature\nfrom librosa.display import specshow\nfrom glob import glob\nimport os\nfrom IPython.display import Audio\nfrom matplotlib import pyplot as plt\n# from zipfile import ZipFile\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nimport keras\nfrom keras.utils import to_categorical\n\nimport re\ndef parse_shape(shape_str):\n    a,b = re.search('\\((\\d+), (\\d+)\\)',shape_str).groups()\n    return int(a), int(b)\n\ndef log_clipped(a):\n    return np.log(np.clip(a,.0000001,a.max()))\n\ndef get_full_path(sample): return os.path.join(sounds_dir, sample['file_name'])\nsounds_dir = \"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto-ca-nv/\"\n# sounds_dir = \"../input/xeno-canto-ca-nv/\"\n\ndf = pd.read_csv(\"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto_ca-nv_index.csv\")\n# df = pd.read_csv(\"../input/xeno-canto_ca-nv_index.csv\")\nfiles_list = glob(os.path.join(sounds_dir,\"*.mp3\"))\nprint(\"%i mp3 files in %s\"%(len(files_list), sounds_dir))\nprint(\"%i samples in index.\"%len(df))\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melspec_dir = \"../input/avian-vocalizations-melspectrograms-log-norm/\"\nmelspec_features_dir = melspec_dir + \"/melspectrograms_logscaled_normalized/features\"\n\nshapes_df = pd.read_csv(\"../input/avian-vocalizations-spectrograms-and-mfccs/feature_shapes.csv\")\n# shapes_df.head(2) \n\nlabel_encoder = LabelEncoder().fit(df['english_cname'] )\nn_classes = len(label_encoder.classes_)\nprint(\"The dataset contains %i distinct species labels.\"%n_classes)\nprint(\"%i mp3s found in %s\"%(len(glob(melspec_features_dir+\"*.mp3\")), melspec_features_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/train_file_ids.csv\",\n                       index_col=0)\ntest_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/test_file_ids.csv\",\n                      index_col=0)\nX_train = np.array(train_df.index)\ny_train = np.array(train_df.label)\nprint(\"Training data shape:\",X_train.shape)\nX_test = np.array(test_df.index)\ny_test = np.array(test_df.label)\nprint(\"Test data shape:    \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a Data Generator to generate fixed-length samples from random windows within clips"},{"metadata":{"trusted":true},"cell_type":"code","source":"sg_dir = \"../input/avian-vocalizations-spectrograms-and-mfccs/melspectrograms/features/\"\nsglog_dir = \"../input/avian-vocalizations-melspectrograms-log-norm/melspectrograms_logscaled_normalized/features/\"\nmfcc_dir = \"../input/avian-vocalizations-spectrograms-and-mfccs/mfccs/features/\"\n\nclass AudioFeatureGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size, n_frames=128, n_channels=1,\n                 n_classes=10, shuffle=False, seed=37):\n        'Initialization'\n        self.n_frames = n_frames\n        self.dim = (128, self.n_frames)\n        self.batch_size = batch_size\n        self.labels = {list_IDs[i]:l for i,l in enumerate(labels)}\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.seed = seed\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n\n    def on_epoch_end(self):\n        'Update indexes, to be called after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.seed)\n            self.seed = self.seed+1 # increment the seed so we get a different batch.\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X = np.empty((self.batch_size, 128+20, self.dim[1], self.n_channels))\n        y = np.empty((self.batch_size, self.n_classes), dtype=int) # one-hot encoded labels\n\n        for i, ID in enumerate(list_IDs_temp):\n            sg_lognorm = np.memmap(sglog_dir+'XC%s_melspectrogram_logscaled_normalized.dat'%ID, \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['melspectrogram_shapes'].values[0]),  \n                    dtype='float32', mode='readonly')\n#             sg = np.memmap(sg_dir+'XC%s_melspectrogram.dat'%ID, \n#                     shape=parse_shape(shapes_df[shapes_df.file_id==file_id]['melspectrogram_shapes'].values[0]),  \n#                     dtype='float32', mode='readonly')\n            mfcc = np.memmap(mfcc_dir+'XC%s_mfcc.dat'%ID, \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['mfcc_shapes'].values[0]),  \n                    dtype='float32', mode='readonly')\n            # Normalize MFCCs\n            mfcc = mfcc_scaler.transform(mfcc)\n            \n            # Filter out quiet frames, thanks to:\n            # https://www.kaggle.com/fleanend/extract-features-with-librosa-predict-with-nb\n            # Take mean amplitude M from frame with highest energy\n#             m = sg[:,np.argmax(sg.mean(axis=0))].mean()\n#             # Filter out all frames with energy less than 5% of M\n#             mask = sg.mean(axis=0)>=m/20\n#             sg = sg[:,mask]\n#             sg_lognorm = sg_lognorm[:,mask]\n#             mfcc = mfcc[:,mask]\n            \n            # Pick a random window from the sound file\n            d_len = mfcc.shape[1] - self.dim[1]\n            if d_len<0: # Clip is shorter than window, so pad with mean value.\n                n = int(np.random.uniform(0, -d_len))\n                pad_range = (n, -d_len-n) # pad with n values on the left, clip_length - n values on the right \n#                 sg_cropped = np.pad(sg, ((0,0), pad_range), 'constant', constant_values=sg.mean())\n                sg_lognorm_cropped = np.pad(sg_lognorm, ((0,0), pad_range), 'constant', constant_values=0)\n                mfcc_cropped = np.pad(mfcc, ((0,0), pad_range), 'constant', constant_values=0)\n            else: # Clip is longer than window, so slice it up\n                n = int(np.random.uniform(0, d_len))\n#                 sg_cropped = sg[:, n:(n+self.dim[1])]\n                sg_lognorm_cropped = sg_lognorm[:, n:(n+self.dim[1])]\n                mfcc_cropped = mfcc[:, n:(n+self.dim[1])]\n                \n            # Stack the MFCCs and spectrograms to create a single array\n            X[i,] = np.concatenate([sg_lognorm_cropped.reshape(1,128,self.dim[1],1), \n                                    mfcc_cropped.reshape(1,20,self.dim[1],1)], axis=1)\n            #X[i,] = sg_lognorm_cropped.reshape(1,128,self.dim[1],1)\n            # Overwrite the bottom of X with MFCCs (we don't need the low frequency bands anyway) \n            #X[i,:20] = mfcc_cropped.reshape(1,20,self.dim[1],1)\n            y[i,] = to_categorical(self.labels[ID], num_classes=self.n_classes)\n\n        return X, y\n    \n# MFCC statistics didn't get saved properly in pervious processing steps. Let's just fit a scaler to them here. \nfrom sklearn.preprocessing import StandardScaler\nmfcc_scaler = StandardScaler()\nfor file_id in shapes_df.file_id:\n    mfcc = np.memmap(mfcc_dir+'/XC%s_mfcc.dat'%file_id, \n        shape=parse_shape(shapes_df[shapes_df.file_id==file_id]['mfcc_shapes'].values[0]),  \n        dtype='float32', mode='readonly')\n    mfcc_scaler.partial_fit(mfcc.flatten().reshape(-1, 1))\nprint(\"MFCC scaler:\",mfcc_scaler.mean_, mfcc_scaler.var_, np.sqrt(mfcc_scaler.var_))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\ngenerator = AudioFeatureGenerator(X_train, y_train, batch_size=1, shuffle=True, seed=37, n_frames=128, n_classes=n_classes)\nfor g in islice(generator,0,4): # show a few examples\n    for i,spec in enumerate(g[0]): \n        plt.figure(figsize=(10,4))\n        spec_ax = specshow(spec.squeeze(), x_axis='time', y_axis='mel')\n        plt.title(label_encoder.classes_[np.argmax(g[1][i])])\n        plt.colorbar()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vis_learning_curve(learning):\n    train_loss = learning.history['loss']\n    train_acc = learning.history['acc']\n    val_loss = learning.history['val_loss']\n    val_acc = learning.history['val_acc']\n\n    fig, axes = plt.subplots(1, 2, figsize=(20,4), subplot_kw={'xlabel':'epoch'} )\n    axes[0].set_title(\"Accuracy\")\n    axes[0].plot(train_acc)\n    axes[0].plot(val_acc)\n    axes[0].legend(['training','validation'])\n    axes[1].set_title(\"Loss\")\n    axes[1].plot(train_loss)\n    axes[1].plot(val_loss)\n    axes[1].legend(['training','validation'])\n\n    best_training_epoc = val_loss.index(np.min(val_loss))\n    axes[0].axvline(x=best_training_epoc, color='red')\n    axes[1].axvline(x=best_training_epoc, color='red')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Benchmark Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy by random guess: %.4f\"%(1/len(df['english_cname'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nimport warnings; warnings.simplefilter('ignore')\n\nn_splits = 3\nn_epochs = 50\nsss = StratifiedShuffleSplit(n_splits=n_splits, test_size=1/4, random_state=37)\nscores = []\nn_classes = len(label_encoder.classes_)\nparams = {#'dim': (128,128),\n          'n_frames': 128,\n          'n_classes': n_classes,\n          'n_channels': 1}\nsplit_i=0\nfor cv_train_index, cv_val_index in sss.split(X_train, y_train):\n    split_i+=1\n    training_generator = AudioFeatureGenerator([X_train[i] for i in cv_train_index], [y_train[i] for i in cv_train_index], \n                                               batch_size=64, shuffle=True, seed=37, **params)\n    validation_generator = AudioFeatureGenerator([X_train[i] for i in cv_val_index], [y_train[i] for i in cv_val_index], \n                                                 batch_size=len(cv_val_index), **params)\n    nb = GaussianNB()\n    epoch_scores = []\n    print(\"Training split %i\"%split_i)\n    for epoch in range(n_epochs):\n        n_batches = int(len(cv_train_index)/training_generator.batch_size)\n        for X_batch, y_batch in training_generator:\n            # Take the mean along the spectral band\n            nb.partial_fit(X_batch.reshape(X_batch.shape[:-1]).mean(axis=2), \n                           [np.argmax(y) for y in y_batch], \n                           classes=range(n_classes))\n        training_generator.on_epoch_end()\n        # Test it out\n        X_val_batch, y_val_batch = validation_generator[0]\n        predictions = nb.predict(X_val_batch.reshape(X_val_batch.shape[:-1]).mean(axis=2)) \n        score = accuracy_score([np.argmax(y) for y in y_val_batch], predictions)\n        epoch_scores.append(score)\n        print(\"\\rEpoch %i, score: %.5f, mean: %.5f\"%(epoch+1, score, np.mean(epoch_scores)), end='')\n        \n    scores.append(np.mean(epoch_scores))\n    plt.plot(epoch_scores)\n    plt.title(\"Split %i learning curve, score: %.5f\"%(len(scores), np.mean(epoch_scores)))\n    plt.show()\nprint(\"Cross Validation Accuracy: %.4f\"%(np.mean(scores)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean Validation Accuracy: %.5f, std. dev.: %.5f\"%(np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.41% accuracy. Now we're getting somewhere. The mean amplitude of each frequency band (a 128x1 array) seems to have some predictive power. I think we can do better still."},{"metadata":{},"cell_type":"markdown","source":"What if we feed in all the pixels instead of flattening by taking the mean?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nimport warnings; warnings.simplefilter('ignore')\n\nn_splits = 3\nn_epochs = 50\nn_classes = len(label_encoder.classes_)\nparams = {#'dim': (128,128),\n          'n_frames': 128,\n          'n_classes': n_classes,\n          'n_channels': 1}\nsss = StratifiedShuffleSplit(n_splits=n_splits, test_size=1/4, random_state=37)\nscores = []\nsplit_i=0\nfor cv_train_index, cv_val_index in sss.split(X_train, y_train):\n    split_i+=1\n    training_generator = AudioFeatureGenerator([X_train[i] for i in cv_train_index], [y_train[i] for i in cv_train_index], \n                                               batch_size=64, shuffle=True, seed=37, **params)\n    validation_generator = AudioFeatureGenerator([X_train[i] for i in cv_val_index], [y_train[i] for i in cv_val_index], \n                                                 batch_size=len(cv_val_index), **params)\n    nb = GaussianNB()\n    epoch_scores = []\n    for epoch in range(n_epochs):\n        n_batches = int(len(cv_train_index)/training_generator.batch_size)\n        for X_batch, y_batch in training_generator:\n            nb.partial_fit(X_batch.reshape(X_batch.shape[0],X_batch.shape[1]*X_batch.shape[2]), \n                           [np.argmax(y) for y in y_batch], \n                           classes=range(n_classes),)\n        training_generator.on_epoch_end()\n        # Test it out\n        X_val_batch, y_val_batch = validation_generator[0]\n        predictions = nb.predict(X_val_batch.reshape(X_val_batch.shape[0],X_val_batch.shape[1]*X_val_batch.shape[2])) \n        score = accuracy_score([np.argmax(y) for y in y_val_batch], predictions)\n        epoch_scores.append(score)\n        print(\"\\rEpoch %i, score: %.5f, mean: %.5f\"%(epoch+1, score, np.mean(epoch_scores)), end='')\n    scores.append(np.mean(epoch_scores))\n    plt.plot(epoch_scores)\n    plt.title(\"Split %i learning curve, score: %.5f\"%(len(scores), np.mean(epoch_scores)))\n    plt.show()\nprint(\"Cross Validation Accuracy: %.4f\"%(np.mean(scores)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}