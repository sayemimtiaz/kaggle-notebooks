{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **SMS Spam Collection v. 1**\n\nThe SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam.\n\nComposition\nThis corpus has been collected from free or free for research sources at the Internet:\n\nA collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/.\n\nA subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/.\n\nA list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf.\n\n\n* text classification problem(NLP)\n* binary outputs: spam vs ham (2 possible outputs)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are two parts in this study.\n1. Using Tensorflow and build RNN model\n2. Using sklearn,nltk,re library to preprocess the text data and build multiple classifier models and compare the accuracy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# [Part1] Tensorflow + RNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \n             \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \n             \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \n             \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n             \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \n             \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \n             \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \n             \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \n             \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \n             \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \n             \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\",\n             \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \n             \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport csv\nimport tensorflow as tf\nimport numpy as np\nlabels = []\nmessages = []\nwith open(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\", 'r') as csvfile:\n  csvreader = csv.reader(csvfile, delimiter=',')\n  next(csvreader)\n  for row in csvreader:\n    labels.append(row[0])\n    sentence = row[1]\n    for word in stopwords:\n      token = \" \" + word + \" \"\n      sentence = sentence.replace(token, \" \")\n    \n    messages.append(sentence)\nprint(\"Total data number: \",len(messages))\nprint(messages[0])\n\n\ntraining_ratio = 0.8\ntrain_size = int(training_ratio * len(messages))\ntrain_messages = messages[:train_size]\ntrain_labels = labels[:train_size]\nvalid_messages = messages[train_size:]\nvalid_labels = labels[train_size:]\nprint(\"training data: \", len(train_messages))\nprint(\"validation data: \",len(valid_messages))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words = 1000, oov_token='<oov>', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True)\ntokenizer.fit_on_texts(train_messages)\nword_index = tokenizer.word_index \nprint(word_index)\ntrain_sequences = tokenizer.texts_to_sequences(train_messages)\ntrain_padded = pad_sequences(train_sequences,  maxlen=85, padding='post', truncating='post')#maxlen=85 can be known after run without maxlen\nvalid_sequences = tokenizer.texts_to_sequences(valid_messages)\nvalid_padded = pad_sequences(valid_sequences,maxlen=85, padding='post', truncating='post')\nprint(train_sequences[0])\nprint(train_padded.shape)\nprint(valid_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\ntrain_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\nvalid_label_seq = np.array(label_tokenizer.texts_to_sequences(valid_labels))\nprint(train_label_seq.shape)\nprint(valid_label_seq.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000, 16, input_length=85),\n    #tf.keras.layers.Flatten(),\n    #tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nnum_epochs = 30\nhistory = model.fit(train_padded, train_label_seq, epochs=num_epochs, \n                    batch_size=128,\n                    validation_data=(valid_padded, valid_label_seq)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(valid_padded, valid_label_seq)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Part2] sklearn,nltk,re + multiple classifiers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\",delimiter=',')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert categorical variable into dummy/indicator variables\ndata['Category'] = pd.get_dummies(data['Category'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk as nlp\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\nsentences_final = []\nfor sentences in data[\"Message\"]:\n    sentences = re.sub(\"[^a-zA-Z]\",\" \",sentences)\n    sentences = sentences.lower()   # buyuk harftan kucuk harfe cevirme\n    sentences = nlp.word_tokenize(sentences)\n    lemma = nlp.WordNetLemmatizer()\n    Stopwords = stopwords.words(\"english\")\n    for word in sentences:\n        if not word in Stopwords:\n            lemma.lemmatize(word)\n    sentences = \" \".join(sentences)   \n    sentences_final.append(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer \nmax_features = 5000 #5000 most common words\nvect = CountVectorizer(max_features = max_features, stop_words = \"english\")\nsparce_matrix = vect.fit_transform(sentences_final).toarray() # fit CountVectorizer and converting features/target into numeric vector\n#print(\"the most using {} words: {}\".format(max_features,vect.get_feature_names()))\n\n# split dataset into train/test\ny = data['Category']  \nx = sparce_matrix\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 52)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading all classifiers\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nsvc = SVC(kernel = 'linear')\nmnb = MultinomialNB(alpha =0.2)\ngnb  = GaussianNB()\nlr = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=100,random_state=52)\nabc = AdaBoostClassifier(n_estimators =100,random_state=52)\nknn = KNeighborsClassifier(n_neighbors = 2)\n\n# define a dictionary of classifier\nclassifier={'SVM': svc , 'MultinomialNB': mnb,'GaussianNB': gnb,'logistic': lr,'RandomForest': rfc,'Adaboost': abc, 'KNN':knn}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\nfor label, model in classifier.items():\n    model.fit(X_train,y_train)\n    accuracy.append((label,[model.score(X_test,y_test)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_df = pd.DataFrame(accuracy)\naccuracy_df.columns = [\"Classifier\",\"Accuracy\"]\naccuracy_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MultinomialNB has the best model with the accuracy of 98.8%. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}