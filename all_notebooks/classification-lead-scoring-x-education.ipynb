{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Importing the necessary packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\npd.set_option('display.max_rows',150)\npd.set_option('display.max_columns',150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead = pd.read_csv('../input/leadscore/Leads.csv')\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shape of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are total 9240 records and 37 columns in this dataframe."},{"metadata":{},"cell_type":"markdown","source":"### Dataframe Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treating incorrect datatype"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Converted' is a binary categorical variable but with datatype as 'int64'\nlead['Converted'] = lead['Converted'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Converted'].dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for the missing values"},{"metadata":{},"cell_type":"markdown","source":"#### A few categorical features have a label - 'Select', which is the default option of selecting the value of a feature and this means a lead has not selected any value; and hence we are good to treat this as a missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing 'Select' with NaN value\nlead.replace({'Select' : np.nan},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating whether all the 'Select' values got replaced or not\nres = lead.isin(['Select']).any().any()\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()/len(lead.index)), 2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observations:-\n1) There are no missing values for the target variable - Converted\n2) Prospect ID and Lead Number uniquely identify a lead from a customer, and these columns have no missing values.\n3) Certain columns like - How did you hear about X Education, Lead Profile, Lead Quality, Asymmetrique Activity Index, Asymmetrique Profile Index, Asymmetrique Activity Score and Asymmetrique Profile Score have missing value percentage higher then 45, and the nature being subjective too for these features, we are good to drop these fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the columns where the missing value percentage is greater than 45\nmax_null_percentage = 45\nlead = lead.loc[:, ((lead.isnull().sum() * 100 / len(lead)) < max_null_percentage)]\nlead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"So now we have 30 columns, after deleting those 7 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us analyse the 'Country' column\n\nlead['Country'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"'Country' is a highly skewed column, with over 95% data belonging to India. Hence this column can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.drop(['Country'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"'City' has about 40% missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us treat 'City'\n\nlead['City'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Around 58% leads are from Mumbai. However, 40% missing values cannot be imputed with 'Mumbai'. So, we can club those missing cities with 'Other Cities'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing all the missing values for 'City' with 'Other Cities'\n\nlead['City'].fillna('Other Cities',inplace=True)\nlead['City'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Specialization' has around 36.5% of missing values.\n\nlead['Specialization'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are no dominant values in the 'Specialization' field. Hence, the missing values can be imputed with 'No Specialization'"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Specialization'].fillna('No Specialization',inplace=True)\nlead['Specialization'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The column 'Tags' has a little over 36% missing values.\n\nlead['Tags'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The feature 'Tags' has 26 unique values, which may be classified as random inputs taken from the leads. Hence, this column can be ignored."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the column 'Tags'\n\nlead.drop(['Tags'],axis=1,inplace=True)\nlead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"So now we have 28 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values for 'What matters most to you in choosing a course'\nlead['What matters most to you in choosing a course'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The column - 'What matters most to you in choosing a course' is highly skewed, since over 99% records belong to the category 'Better Career Prospects'. Hence, this column can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the column 'What matters most to you in choosing a course'\n\nlead.drop(['What matters most to you in choosing a course'],axis=1,inplace=True)\nlead.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values for 'What is your current occupation'\nlead['What is your current occupation'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Again to remain unbias towards the dataset, the 29.11% missing values are to be imputed with 'Unknown'"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['What is your current occupation'].fillna('Unknown',inplace=True)\nlead['What is your current occupation'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us again check the percentage of missing values in the dataset\n\nround(100*(lead.isnull().sum()/len(lead.index)), 2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Columns such as - 'TotalVisits', 'Page Views Per Visit', 'Last Activity' and 'Lead Source' have less than 2% missing values, hence such rows can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"lead = lead.dropna(axis=0, subset=['TotalVisits','Page Views Per Visit','Last Activity','Lead Source'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us re-check the percentage of missing values in the dataset\n\nround(100*(lead.isnull().sum()/len(lead.index)), 2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Now our dataset has no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape #checking shape again","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Incorrect Label Treatment for the categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = lead.dtypes[lead.dtypes == 'object'].index.values\nfor col in columns : \n    print('Levels in ',col,' are ' , lead[col].unique(),'\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In the column 'Lead Source', although they mean the same, but 'Google' and 'google' are two different distinct values and this needs to be fixed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing 'google' with 'Google' in 'Lead Source'\nlead['Lead Source'] = lead['Lead Source'].str.replace('google','Google')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns having only one label\ncol_label_drop = ['Magazine','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content',\n                 'I agree to pay the amount through cheque']\nlead.drop(columns=col_label_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Now we are left with 22 columns."},{"metadata":{},"cell_type":"raw","source":"The columns - 'Last Activity' and 'Last Notable Activiy' are seem to be having similar labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Last Activity'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Last Notable Activity'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Except the label 'Modified', the column 'Last Notable Activity' is equivalent to 'Last Activity'. Moreover, the purpose of that label is not understood. Hence, going forward, we will be using 'Last Activity', thereby dropping 'Last Notable Activity'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the column - 'Last Notable Activity'\n\nlead.drop(columns = ['Last Notable Activity'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"So we have 21 columns now."},{"metadata":{},"cell_type":"raw","source":"'Newspaper' and 'Newspaper Article' seems to have the same characteristics. Let us have a look."},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Newspaper'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Newspaper Article'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Both the features have same number of labels in equivalent proportions. Hence, we can drop any one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the column - 'Newspaper Article'\n\nlead.drop(columns = ['Newspaper Article'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the labels for 'Lead Origin'\n\nlead['Lead Origin'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The labels 'Lead Add Form' and 'Lead Import' have very low counts in compare to other 2 labels. Let us group them into one label as - 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labeling those lead origins having very low lead counts as 'Others' \n\nlead_origin = lead['Lead Origin'].value_counts(normalize=True)\nlow_lead_origin = lead_origin[lead_origin < 0.07].index\n\nlead['Lead Origin'].replace(low_lead_origin,'Others',inplace=True)\nlead['Lead Origin'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the labels for 'Lead Source'\n\nlead['Lead Source'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Many labels are there which contribute less than 10%, hence those can be clubbed as 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_source = lead['Lead Source'].value_counts(normalize=True)\nlow_lead_source = lead_source[lead_source < 0.1].index   \n\nlead['Lead Source'].replace(low_lead_source,'Others',inplace=True)\nlead['Lead Source'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the labels for 'Last Activity'\n\nlead['Last Activity'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Many labels are there which contribute less than 10%, hence those can be clubbed as 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_activity = lead['Last Activity'].value_counts(normalize=True)\nlow_lead_activity = lead_activity[lead_activity < 0.1].index    \n\nlead['Last Activity'].replace(low_lead_activity,'Others',inplace=True)\nlead['Last Activity'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the labels for 'Specialization'\n\nlead['Specialization'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Many labels are there which contribute less than 5%, hence those can be clubbed as 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_spec = lead['Specialization'].value_counts(normalize=True)\nlow_lead_spec = lead_spec[lead_spec < 0.05].index               \n\nlead['Specialization'].replace(low_lead_spec,'Others',inplace=True)\nlead['Specialization'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the outliers for the continuous variables at 25%, 50%, 75%, 90%, 95% and 99%\n\nlead[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']].describe(percentiles=[.25, .5, .75, .90, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Huge differences are observed between 99% and max values for all the continuous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us cap the outliers at the 99th percentile\n\ncap1 = lead['TotalVisits'].quantile(0.99)\ncap2 = lead['Total Time Spent on Website'].quantile(0.99)\ncap3 = lead['Page Views Per Visit'].quantile(0.99)\n\ncond1 = lead['TotalVisits'] > cap1\ncond2 = lead['Total Time Spent on Website'] > cap2\ncond3 = lead['Page Views Per Visit'] > cap3\n\nlead.loc[cond1,'TotalVisits' ] = cap1\nlead.loc[cond1,'Total Time Spent on Website' ] = cap2\nlead.loc[cond1,'Page Views Per Visit' ] = cap3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-checking for the outliers for the continuous variables at 25%, 50%, 75%, 90%, 95% and 99%\n\nlead[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']].describe(percentiles=[.25, .5, .75, .90, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Total % of records retained = (Total no. of records after data cleaning / Total no. of records in the original data) * 100\n                               = (9074 / 9240) * 100\n                               = 98.2 %"},{"metadata":{},"cell_type":"markdown","source":"### Checking the balancing nature of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Converted'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"62% leads are uncoverted."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataset w.r.t. successful or unsuccessful convertion of leads\nconv = lead[lead['Converted']==1]\nunconv = lead[lead['Converted']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(column_name, title_name, df1=lead, df2=conv, figsize=(7,4), kind='bar', normalize_df_appl=False, k=1):\n    plt.figure(figsize=figsize)\n    plt.subplot(1,2,1)\n    (df1[column_name].value_counts(normalize=normalize_df_appl)*k).plot(kind=kind)\n    plt.title('Total Leads by {0}'.format(title_name), fontsize=12)\n    \n    plt.subplot(1,2,2)\n    (df2[column_name].value_counts(normalize=True)*100).plot(kind=kind)\n    plt.title('Converted by {0} (%)'.format(title_name), fontsize=12)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Lead Origin'\n\nplot_bar('Lead Origin','Lead Origin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observations:-\n1) Convertion rate from APIs are not that great.\n2) Convertion rate from other origins, apart from Landing Page Submission and API, are much better."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Lead Source'\n\nplot_bar('Lead Source','Lead Source')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nThe conversion rate of the sources like Direct Traffic and Olark Chat, are not great at all."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Do Not Email'\n\nplot_bar('Do Not Email','Do Not Email')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nThose leads who do not wish to get communicated over mail, have lower conversion rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Last Activity'\n\nplot_bar('Last Activity','Last Activity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observations:-\n1) Activities like 'SMS Sent' has the highest convertion rate\n2) all remaining activities like 'Email Opened', 'Olark Chat' and 'Others' has failed to convert at a signifant rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Specialization'\n\nplot_bar('Specialization','Specialization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"36.5% missing values have been imputed by 'No Specialization'\n'Others' category has good conversion chances while 'No Specialization' has less"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'What is your current occupation'\n\nplot_bar('What is your current occupation','current occupation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nWorking Professionals have a tremendous convertion rates."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'A free copy of Mastering The Interview'\n\nplot_bar('A free copy of Mastering The Interview','Mastering The Interview')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nConvertion rate of the leads those who want free copies of 'Mastering The Interview' is less."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining function for continuous variable univariate analysis using disribution plot\ndef plot_distplot(column_name, title_name):\n    plt.figure(figsize=(15,5))\n    plt.title('{0} - All leads vs Converted'.format(title_name), fontsize=12)\n    sns.distplot(lead[column_name],hist=False,label='All leads')\n    sns.distplot(conv[column_name],hist=False,label='Converted')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'TotalVisits'\nplot_distplot('TotalVisits','TotalVisits')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nMore the number of visits on the website, higher the convertion rate of the leads. Typically, convertion rate is lower if a lead visits less than 5 times."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Total Time Spent on Website'\nplot_distplot('Total Time Spent on Website','Total Time Spent on Website')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nConvertion rate of the leads who have spent atleast 600 units of time on the website, is much higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis for 'Page Views Per Visit '\nplot_distplot('Page Views Per Visit','Page Views Per Visit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nNo such clear trend is observed."},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Variable Mapping & Creating Dummy Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check once more the labels of all the categorical features\n\ncolumns = lead.dtypes[lead.dtypes == 'object'].index.values\nfor col in columns : \n    print('Levels in ',col,' are ' , lead[col].unique(),'\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The binary categorical variables (Yes/No) are - 'Do Not Email', 'Do Not Call', 'Search', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations' and 'A free copy of Mastering The Interview'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping the binary categorical variables(yes/no) to 1/0\n\nvarlist =  ['Do Not Email', 'Do Not Call', 'Search', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'A free copy of Mastering The Interview']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\nlead[varlist] = lead[varlist].apply(binary_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables for the remaining categorical features and removing the 'Others' category since it is \n# not very intuitive from model understanding perspective & adding them to the original dataframe\n\ndummy1 = pd.get_dummies(lead['Lead Origin'],prefix='Origin')      \ndummy1 = dummy1.drop(columns=['Origin_Others'])\nlead = pd.concat([lead,dummy1], axis=1)\n\ndummy2 = pd.get_dummies(lead['Lead Source'],prefix='Source')\ndummy2 = dummy2.drop(columns='Source_Others')\nlead = pd.concat([lead,dummy2], axis=1)\n\ndummy3 = pd.get_dummies(lead['Last Activity'],prefix='Activity')\ndummy3 = dummy3.drop(columns='Activity_Others')\nlead = pd.concat([lead,dummy3], axis=1)\n\ndummy4 = pd.get_dummies(lead['Specialization'],prefix='Spec')\ndummy4 = dummy4.drop(columns='Spec_Others')\nlead = pd.concat([lead,dummy4], axis=1)\n\ndummy5 = pd.get_dummies(lead['What is your current occupation'],drop_first=True,prefix='Occupation')\nlead = pd.concat([lead,dummy5], axis=1)\n\ndummy6 = pd.get_dummies(lead['City'],drop_first=True,prefix='City')\nlead = pd.concat([lead,dummy6], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the features whose dummy variables have been created\n\nlead = lead.drop(['Lead Origin','Lead Source','Last Activity','Specialization','What is your current occupation','City'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Finally, after all the data cleaning and data preparation operations, we have 9074 rows and 39 columns."},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top Correlations\ndef correlation(dataframe) : \n    cor0=dataframe.corr()\n    type(cor0)\n    cor0.where(np.triu(np.ones(cor0.shape),k=1).astype(np.bool))\n    cor0=cor0.unstack().reset_index()\n    cor0.columns=['VAR1','VAR2','CORR']\n    cor0.dropna(subset=['CORR'], inplace=True)\n    cor0.CORR=round(cor0['CORR'],2)\n    cor0.CORR=cor0.CORR.abs()\n    cor0.sort_values(by=['CORR'],ascending=False)\n    cor0=cor0[~(cor0['VAR1']==cor0['VAR2'])]\n    return pd.DataFrame(cor0.sort_values(by=['CORR'],ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlations for Converted Leads \n\nconvertedCondition= lead['Converted']==1\ncorrelation(lead[convertedCondition])[1:30:2].style.hide_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlations for Unconverted Leads \n\nunconvertedCondition= lead['Converted']==0\ncorrelation(lead[unconvertedCondition])[1:30:2].style.hide_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataset into 2 seperate dataframes - one with 'Prospect ID' and 'Lead Number', to be used for the eventual \n# lead scoring purpose, and another without those 2 features for model building purpose\n\nlead_org = lead.copy()\nlead_org = lead_org.drop(lead_org.iloc[:,2:],axis=1) \nlead = lead.drop(['Prospect ID','Lead Number'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test-Train Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = lead\ny = lead.pop('Converted')\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\n# fitting and transforming train set\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selction Using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Currently we have 36 columns. "},{"metadata":{},"cell_type":"raw","source":"Approach - We will follow a mixed approach for feature elimination. Initially, using RFE, 15 features will be selected. Then, manually, with respect to p-value and VIF, other features will be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(logreg, 15)             \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 15 Columns selected by rfe\n\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics"},{"metadata":{},"cell_type":"markdown","source":"#### Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the first dataframe model with RFE selected variables\nX_train_1 = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_1 = sm.add_constant(X_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_1,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_1_ = X_train_1.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_1_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be following the below rule to eliminate the features one by one, as per the priorities mentioned by the following sequence:-\n\n* We will first check the summary and VIF\n* If a variable has got high p-value(>0.05) as well as high VIF(>5), we need to drop that first\n* If a variable has got high p-value(>0.05) but low VIF(<5), then we need to drop such\n* Still if we have a variable with low p-value(<0.05) but high VIF(>5), we need to drop such at the very end"},{"metadata":{},"cell_type":"raw","source":"In Model 1, 'Newspaper' is the least significant variable, with the highest p-value and a low VIF. Hence, dropping this feature."},{"metadata":{},"cell_type":"markdown","source":"#### Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Newspaper'  \n\nX_train_2 = X_train_1.drop(['const','Newspaper'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_2 = sm.add_constant(X_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_2,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_2_ = X_train_2.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_2_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 2, 'Occupation_Housewife' has the highest p-value, with a low VIF. Hence, this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"#### Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Occupation_Housewife'\n\nX_train_3 = X_train_2.drop(['const','Occupation_Housewife'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_3 = sm.add_constant(X_train_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_3,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_3_ = X_train_3.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_3_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Lets drop 'City_Tier II Cities' next, on account of highest p-value, in Model 3."},{"metadata":{},"cell_type":"markdown","source":"#### Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'City_Tier II Cities'\n\nX_train_4 = X_train_3.drop(['const','City_Tier II Cities'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_4 = sm.add_constant(X_train_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_4,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_4_ = X_train_4.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_4_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 4, 'Occupation_Unemployed' is the only insignificant feauture, with the highest VIF. Hence, this feature should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"#### Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Occupation_Unemployed'\n\nX_train_5 = X_train_4.drop(['const','Occupation_Unemployed'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_5 = sm.add_constant(X_train_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_5,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_5_ = X_train_5.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_5_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 5, there are no insignificant features with p-value > 0.05. But 'Origin_API' has VIF > 5. Hence, this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"#### Model 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Origin_API'\n\nX_train_6 = X_train_5.drop(['const','Origin_API'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nX_train_6 = sm.add_constant(X_train_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the logistic model\n\nlgm = sm.GLM(y_train,X_train_6,family = sm.families.Binomial()).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lgm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_6_ = X_train_6.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_6_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Model 6 is best model achieved, having all the features statistically significant, with no multi-collinearity among them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Features\n\nFinal_cols = X_train_6.columns.values\nFinal_col = np.delete(Final_cols,0)   # Removing the 'const' variable for future use\nFinal_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgm = sm.GLM(y_train,X_train_6,family = sm.families.Binomial()).fit()  #as obtained previously\ny_train_pred = lgm.predict(X_train_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conv_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'Lead_Pred' with 1 if Conv_Prob > 0.5 else 0\n\ny_train_pred_final['Lead_Pred'] = y_train_pred_final['Conv_Prob'].map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix for Train Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final['Converted'], y_train_pred_final['Lead_Pred'])\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy for Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = metrics.accuracy_score(y_train_pred_final['Converted'], y_train_pred_final['Lead_Pred'])\nprint('Accuracy on Train Data : ', round(100*accuracy,2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Metrics beyond simple accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsepositiverate = FP/(FP + TN)\npositivepredictivevalue = TP/(TP +FP )\nnegativepredictivevalue = TN/(TN + FN)\nprint('Sensitivity : ', round(100*sensitivity,2),'%')\nprint('Specificity : ',  round(100*specificity,2),'%')\nprint('False Positive Rate : ',  round(100*falsepositiverate,2),'%')\nprint('Positive Predictive Power : ',  round(100*positivepredictivevalue,2),'%')\nprint('Negative Predictive Power : ',  round(100*negativepredictivevalue,2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final['Converted'], y_train_pred_final['Conv_Prob'], drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final['Converted'], y_train_pred_final['Conv_Prob'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"AUC=0.87"},{"metadata":{},"cell_type":"markdown","source":"#### Finding Optimal Cutoff Point"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final['Conv_Prob'].map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final['Converted'], y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the curve above, 0.38 seems to be the optimum cutoff probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final['Conv_Prob'].map( lambda x: 1 if x > 0.38 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\naccu = metrics.accuracy_score(y_train_pred_final['Converted'], y_train_pred_final['final_predicted'])\nprint('Accuracy on Train set at Optimum Cut Off : ', round(100*accu,2),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for the train set at the Optimum Cut-Off\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final['Converted'], y_train_pred_final['final_predicted'])\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsepositiverate = FP/(FP + TN)\npositivepredictivevalue = TP/(TP +FP )\nnegativepredictivevalue = TN/(TN + FN)\nprint('Sensitivity : ', round(100*sensitivity,2),'%')\nprint('Specificity : ',  round(100*specificity,2),'%')\nprint('False Positive Rate : ',  round(100*falsepositiverate,2),'%')\nprint('Positive Predictive Power : ',  round(100*positivepredictivevalue,2),'%')\nprint('Negative Predictive Power : ',  round(100*negativepredictivevalue,2),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve for cut off probability of 0.38\ndraw_roc(y_train_pred_final['Converted'], y_train_pred_final['final_predicted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"AUC=0.78"},{"metadata":{},"cell_type":"markdown","source":"#### Precision and Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_train_pred_final['Converted'], y_train_pred_final['Lead_Pred'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_pred_final['Converted'], y_train_pred_final['Lead_Pred'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision and Recall tradeoff"},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final['Converted'], y_train_pred_final['Conv_Prob'])\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The cut-off point from the precision-recall curve is  about 0.4."},{"metadata":{},"cell_type":"markdown","source":"### Predictions on the Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and transform operations are done on the training data but only transform operation will be done on the test data\n\nX_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aligning X_test with the final features\n\nX_test = X_test[Final_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = lgm.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_final = pd.DataFrame({'Converted':y_test, 'Conv_Prob':y_test_pred, 'LeadID':y_test.index})\ny_test_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions with optimal cut off = 0.38\n\ny_test_pred_final['final_predicted'] = y_test_pred_final['Conv_Prob'].map(lambda x: 1 if x > 0.38 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy of the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = metrics.accuracy_score(y_test_pred_final['Converted'], y_test_pred_final['final_predicted'])\nprint('Accuracy on Test Data : ', round(100*accuracy,2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix for Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion3 = metrics.confusion_matrix(y_test_pred_final['Converted'], y_test_pred_final['final_predicted'])\nprint(confusion3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion3[1,1] # true positive \nTN = confusion3[0,0] # true negatives\nFP = confusion3[0,1] # false positives\nFN = confusion3[1,0] # false negatives\nsensitivity = TP/(FN + TP)\nspecificity = TN/(FP + TN)\nfalsepositiverate = FP/(FP + TN)\npositivepredictivevalue = TP/(TP +FP )\nnegativepredictivevalue = TN/(TN + FN)\nprint('Sensitivity : ', round(100*sensitivity,2),'%')\nprint('Specificity : ',  round(100*specificity,2),'%')\nprint('False Positive Rate : ',  round(100*falsepositiverate,2),'%')\nprint('Positive Predictive Power : ',  round(100*positivepredictivevalue,2),'%')\nprint('Negative Predictive Power : ',  round(100*negativepredictivevalue,2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_test_pred_final['Converted'], y_test_pred_final['final_predicted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"AUC=0.78"},{"metadata":{},"cell_type":"markdown","source":"### Lead Scoring"},{"metadata":{},"cell_type":"raw","source":"Lead Scoring is done to generate a score against each lead, for a possiblity of getting successfully converted. Higher the score of a lead. the greater is the chance for its successful conversion. This will minimise the effort of the sales team to identify those leads who are having higher chances for a successful convertion."},{"metadata":{"trusted":true},"cell_type":"code","source":"conversionprob = pd.concat([y_test_pred_final['Conv_Prob'],y_train_pred_final['Conv_Prob']],axis=0)\nconvprob = pd.DataFrame(conversionprob)  #creating a dataframe instead of a pandas series\nconvprob.reset_index(inplace=True)\nconvprob.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_org.reset_index(inplace=True)\nlead_org.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = pd.merge(lead_org,convprob,on='index',how='outer')\nleads['Lead Score'] = round(leads['Conv_Prob']*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.sort_values('Lead Score',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation\n\nWe have arrived at a very decent model for the the convertion of leads using 10 variables. It can predict the potential leads (leads who eventually get converted) correctly upto 78%. We also have the corresponding lead scores of conversion.\n\nThe final relationship between log Odds of Conversion Probability and lead features is    \n  \n`logOdds(Conversion Probability)` = 0.3199 - 1.2623 * `Do Not Email` + 0.8939 `Total Time Spent on Website` - 1.893 `Origin_Landing Page Submission` + 0.3538 `Source_Olark Chat` + 0.6283 `Activity_Email Opened` - 0.9875 `Activity_Olark Chat Conversation` + 1.8478 `Activity_SMS Sent` - 1.1953 `Spec_No Specialization` - 1.3795 `Occupation_Unknown` + 2.3786 `Occupation_Working Professional`\n  \nwhere `Total Time Spent on Website` is standardized to $\\mu=0,\\sigma=1$\n\n\nInterpreting the top features affecting Conversion Probability :   \n- Leads who are `Working Professionals` have 2.38 times higher log odds of conversion than those with other professions (such as student, housewife, unemployed etc.) combined\n- Leads having `SMS Sent` as the last registered activity have 1.8 times higher log odds of conversion than leads having all other last activity (such as email opened, page visited etc.) combined \n- Leads having `Email Opened` as the last registered activity have 0.6 times higher log odds of conversion than leads having all other last activity (such as email marked spam, email link opened, page visited on website etc.) combined \n- Leads who landed on the companys website through `Olark Chat` as the source, have 0.35 times higher log odds of conversion compared to other lead sources (such as google, facebook etc.) combined\n- Leads whose origin was a `Landing Page Submission` have 1.9 times lesser log odds of conversion than other lead sources (such as API etc.) combined\n- Leads who have not provided details of their Occupation or in other words, have `Unknown Occupation` have 1.37 times lower log odds of conversion compared to all the leads who have filled their occupation (as either unemployed, working etc.) combined\n- Leads who chose not to receive email updates (`Do Not Email`) have 1.26 times lower log odds of conversion compared to leads who would like email updates\n- Leads with `No Specialization` have 1.2 times lower log odds of conversion than all leads having some specialization (either Banking, Healthcare, Finance etc.) combined\n- Leads with `Olark Chat conversation` as the last activity registered have 0.99 times lower log odds of conversion compared to leads having all other last activity (such as email opened, sms sent etc.) combined\n    "},{"metadata":{},"cell_type":"markdown","source":"### Business Goals/ Action to be taken\n\n* The sales team should target those leads on high priority, who are working professionals, who spend significant amount of time on the website, wishes to communicate over mail and with whom the last method of contact was SMS sent\n\n* The sales team should provide low importance to those leads, who have not mentioned either of their occupation or specialization, and do not wishes to communicate over mail"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}