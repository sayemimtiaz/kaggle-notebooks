{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Single Cortical Neurons as Deep Artificial Neural Networks\n\nIntegrate and Fire ([I&F](https://en.wikipedia.org/wiki/Biological_neuron_model#Integrate-and-fire)) neuron models are the simplest type of [biological neuron models](https://en.wikipedia.org/wiki/Biological_neuron_model). \nThey contain only a cell membrane, a threshold spike generation mechansim, a simple post spike reset mechanism, and simple current based synapses.  \nThey contain slighly more \"biological detail\" than \"perceptrons\" (which are also known as \"the artificial neurons that we use in state of the art neural networks\").  \nThey are considered to be a vast over-simplification of the biolgical truth but are still widely used in neuroscience.  \n\nThis script replicates the first introductory figure (Figure 1) in the paper \"[Single Cortical Neurons as Deep Artificial Neural Networks](https://www.biorxiv.org/content/10.1101/613141v2)\".\nThe goal of this script is to give an overview of the full methodology used in the paper but on a simple neuron model.\nLater in the paper (and in the kaggle notebooks listed below) we perform the same methodology on state of the art and the most detailed models on biological neurons we neuroscientists have today. We than can use this simple experiment as a control case to demostrate the if neurons were simple, it would have been easy to find\n\nI've previously created a short twitter thread to simply explain the key results of the full paper: [twitter thread](https://twitter.com/DavidBeniaguev/status/1131890349578829825). Welcome to have a look for a brief visual summery.\n\nThe TD;LR version:\nbiological neurons are more complex than the neurons we use in our artificial neural networks by quite a bit, but at the same time they are not unimaginably more complex so we can still have a firm handle of what they actually are - something equivalent to a deep neural network.\n\n### What this script contains\n* Generates train and test datasets that simulate (directly in python) an I&F neuron model (takes ~15 minutes)\n* Trains a 1 hidden unit fully connected neural network (FCN) to fit the data (takes ~10 minutes)\n* Visualizes first layer learned weights of the FCN as spatio-temporal heat maps\n* Makes a prediction using the 1 unit FCN model on the test dataset\n* Evaluates model performance by comparing the model's prediction to the I&F simulation\n* Combines everything together to form Figure 1 in the paper (not exactly identical, but pretty much the same)\n\n### Additional resources\n* [A dataset of a cortical neuron simulation](https://www.kaggle.com/selfishgene/single-neurons-as-deep-nets-nmda-test-data)\n* [A script to explore cortical neuron simulation dataset](https://www.kaggle.com/selfishgene/exploring-a-single-cortical-neuron)\n* [A script to replicate main paper result](https://www.kaggle.com/selfishgene/single-neuron-as-deep-net-replicating-key-result)\n* [A GitHub repo for all simulation fitting and analysis code](https://github.com/SelfishGene/neuron_as_deep_net)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\nfrom scipy import signal\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import roc_curve, auc\nfrom keras.models import Model\nfrom keras.optimizers import Nadam\nfrom keras.layers import Input, Conv1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.regularizers import l2\nfrom keras import initializers\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some helper functions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#%% some helper functions\n\ndef bin2dict(bin_spikes_matrix):\n    spike_row_inds, spike_times = np.nonzero(bin_spikes_matrix)\n    row_inds_spike_times_map = {}\n    for row_ind, syn_time in zip(spike_row_inds,spike_times):\n        if row_ind in row_inds_spike_times_map.keys():\n            row_inds_spike_times_map[row_ind].append(syn_time)\n        else:\n            row_inds_spike_times_map[row_ind] = [syn_time]\n\n    return row_inds_spike_times_map\n\n\ndef dict2bin(row_inds_spike_times_map, num_segments, sim_duration_ms):\n    \n    bin_spikes_matrix = np.zeros((num_segments, sim_duration_ms), dtype='bool')\n    for row_ind in row_inds_spike_times_map.keys():\n        for spike_time in row_inds_spike_times_map[row_ind]:\n            bin_spikes_matrix[row_ind,spike_time] = 1.0\n    \n    return bin_spikes_matrix\n\n\ndef generate_input_spike_trains_for_simulation(sim_duration_ms=6000, num_exc_segments=80, num_inh_segments=20,\n                                               num_exc_spikes_per_100ms_range=[0,100], num_exc_inh_spike_diff_per_100ms_range=[-100,100]):\n\n    # randomly sample inst rate (with some uniform noise) smoothing sigma\n    keep_inst_rate_const_for_ms = inst_rate_sampling_time_interval_options_ms[np.random.randint(len(inst_rate_sampling_time_interval_options_ms))]\n    keep_inst_rate_const_for_ms += int(2 * inst_rate_sampling_time_interval_jitter_range * np.random.rand() - inst_rate_sampling_time_interval_jitter_range)\n    \n    # randomly sample smoothing sigma (with some uniform noise)\n    temporal_inst_rate_smoothing_sigma = temporal_inst_rate_smoothing_sigma_options_ms[np.random.randint(len(temporal_inst_rate_smoothing_sigma_options_ms))]\n    temporal_inst_rate_smoothing_sigma += int(2 * temporal_inst_rate_smoothing_sigma_jitter_range * np.random.rand() - temporal_inst_rate_smoothing_sigma_jitter_range)\n    \n    num_inst_rate_samples = int(np.ceil(float(sim_duration_ms) / keep_inst_rate_const_for_ms))\n    \n    # create the coarse inst rates with units of \"total spikes per tree per 100 ms\"\n    num_ex_spikes_per_100ms   = np.random.uniform(low=num_exc_spikes_per_100ms_range[0], high=num_exc_spikes_per_100ms_range[1], size=(1, num_inst_rate_samples))\n    num_inh_spikes_low_range  = np.maximum(0, num_ex_spikes_per_100ms + num_exc_inh_spike_diff_per_100ms_range[0])\n    num_inh_spikes_high_range = num_ex_spikes_per_100ms + num_exc_inh_spike_diff_per_100ms_range[1]\n    num_inh_spikes_per_100ms  = np.random.uniform(low=num_inh_spikes_low_range, high=num_inh_spikes_high_range, size=(1, num_inst_rate_samples))\n    num_inh_spikes_per_100ms[num_inh_spikes_per_100ms < 0] = 0.0001\n\n    # convert to units of \"per_1um_per_1ms\"\n    ex_bas_spike_rate_per_1um_per_1ms   = num_ex_spikes_per_100ms   / (num_exc_segments  * 100.0)\n    inh_bas_spike_rate_per_1um_per_1ms  = num_inh_spikes_per_100ms  / (num_inh_segments  * 100.0)\n\n    # kron by space (uniform distribution across branches per tree)\n    ex_spike_rate_per_seg_per_1ms   = np.kron(ex_bas_spike_rate_per_1um_per_1ms  , np.ones((num_exc_segments,1)))\n    inh_spike_rate_per_seg_per_1ms  = np.kron(inh_bas_spike_rate_per_1um_per_1ms , np.ones((num_inh_segments,1)))\n\n    # add some spatial multiplicative randomness (that will be added to the sampling noise)\n    ex_spike_rate_per_seg_per_1ms  = np.random.uniform(low=0.5, high=1.5, size=ex_spike_rate_per_seg_per_1ms.shape ) * ex_spike_rate_per_seg_per_1ms\n    inh_spike_rate_per_seg_per_1ms = np.random.uniform(low=0.5, high=1.5, size=inh_spike_rate_per_seg_per_1ms.shape) * inh_spike_rate_per_seg_per_1ms\n\n    # kron by time (crop if there are leftovers in the end) to fill up the time to 1ms time bins\n    ex_spike_rate_per_seg_per_1ms  = np.kron(ex_spike_rate_per_seg_per_1ms , np.ones((1, keep_inst_rate_const_for_ms)))[:,:sim_duration_ms]\n    inh_spike_rate_per_seg_per_1ms = np.kron(inh_spike_rate_per_seg_per_1ms, np.ones((1, keep_inst_rate_const_for_ms)))[:,:sim_duration_ms]\n    \n    # filter the inst rates according to smoothing sigma\n    smoothing_window = signal.gaussian(1.0 + 7 * temporal_inst_rate_smoothing_sigma, std=temporal_inst_rate_smoothing_sigma)[np.newaxis,:]\n    smoothing_window /= smoothing_window.sum()\n    seg_inst_rate_ex_smoothed  = signal.convolve(ex_spike_rate_per_seg_per_1ms,  smoothing_window, mode='same')\n    seg_inst_rate_inh_smoothed = signal.convolve(inh_spike_rate_per_seg_per_1ms, smoothing_window, mode='same')\n    \n    # sample the instantanous spike prob and then sample the actual spikes\n    ex_inst_spike_prob = np.random.exponential(scale=seg_inst_rate_ex_smoothed)\n    ex_spikes_bin      = np.random.rand(ex_inst_spike_prob.shape[0], ex_inst_spike_prob.shape[1]) < ex_inst_spike_prob\n\n    inh_inst_spike_prob = np.random.exponential(scale=seg_inst_rate_inh_smoothed)\n    inh_spikes_bin      = np.random.rand(inh_inst_spike_prob.shape[0], inh_inst_spike_prob.shape[1]) < inh_inst_spike_prob\n\n    all_spikes_bin = np.vstack((ex_spikes_bin, inh_spikes_bin))\n\n    return all_spikes_bin\n\n\ndef simulate_integrate_and_fire_cell(presynaptic_input_spikes, synaptic_weights, membrane_time_const=20, v_reset=-95, v_threshold=-50, current_to_voltage_mult_factor=5):\n    temporal_filter_length = int(7 * membrane_time_const) + 1\n    syn_filter = signal.exponential(M=temporal_filter_length,center=0,tau=membrane_time_const,sym=False)[np.newaxis,:]\n    syn_local_currents = signal.convolve(presynaptic_input_spikes, syn_filter, mode='full')[:,:presynaptic_input_spikes.shape[1]]\n    soma_current       = signal.convolve(syn_local_currents, np.flipud(synaptic_weights), mode='valid')\n    \n    # make simulations\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    for t in range(len(soma_voltage)):\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * syn_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return soma_voltage, output_spike_times_in_ms\n\n\ndef generate_multiple_simulations(input_generation_func, cell_simulation_func, num_simulations):\n    \n    num_synapses, sim_duration_ms = input_generation_func().shape\n    X = np.zeros(((num_synapses, sim_duration_ms, num_simulations)), dtype=np.bool)\n    y_spikes = np.zeros(((sim_duration_ms, num_simulations)), dtype=np.bool)\n    y_soma   = np.zeros(((sim_duration_ms, num_simulations)), dtype=np.float32)\n    for sim_ind in range(num_simulations):\n        presynaptic_input_spikes = input_generation_func()\n        soma_voltage, output_spike_times_in_ms = cell_simulation_func(presynaptic_input_spikes)\n        \n        X[:,:,sim_ind] = presynaptic_input_spikes\n        y_spikes[output_spike_times_in_ms,sim_ind] = 1.0\n        y_soma[:,sim_ind] = soma_voltage\n\n    return X, y_spikes, y_soma\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simulate Integrate and Fire (I&F) to create a large dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% collect a large dataset of {input,output} \"recordings\" from an Integrate and Fire (I&F) simulation\n\nrandom_seed = 1234\nnp.random.seed(random_seed)\n\n# simulation params\nnum_ex_synapses  = 80\nnum_inh_synapses = 20\nnum_synapses     = num_ex_synapses + num_inh_synapses\n\nv_reset     = -75\nv_threshold = -55\ncurrent_to_voltage_mult_factor = 2\nmembrane_time_const = 20\n\n# create synaptic weights vector \"w\"\nsynaptic_weights = np.ones((num_synapses, 1))\nexc_inds  = range(num_ex_synapses)\ninh_inds = list(set(range(num_synapses)) - set(exc_inds))\nsynaptic_weights[exc_inds] *=  1.0\nsynaptic_weights[inh_inds] *= -1.0\n\nsim_duration_ms  = 6000\nsim_duration_sec = sim_duration_ms / 1000.0\n\ninst_rate_sampling_time_interval_options_ms   = [25, 30, 35, 40, 50, 60, 70, 80, 90, 100]\ntemporal_inst_rate_smoothing_sigma_options_ms = [40, 60, 80, 100]\n\ninst_rate_sampling_time_interval_jitter_range   = 20\ntemporal_inst_rate_smoothing_sigma_jitter_range = 20\n\nnum_exc_spikes_per_100ms_range = [0, 50]\nnum_exc_inh_spike_diff_per_100ms_range = [-50, -15]\n\nnum_simulations_train = 4500\nnum_simulations_test  = 1500\n\ndataset_generation_start_time = time.time()\n\ninput_generation_func = lambda  : generate_input_spike_trains_for_simulation(sim_duration_ms=sim_duration_ms,\n                                                                             num_exc_segments=num_ex_synapses, num_inh_segments=num_inh_synapses,\n                                                                             num_exc_spikes_per_100ms_range=num_exc_spikes_per_100ms_range,\n                                                                             num_exc_inh_spike_diff_per_100ms_range=num_exc_inh_spike_diff_per_100ms_range)\ncell_simulation_func  = lambda x: simulate_integrate_and_fire_cell(x, synaptic_weights, membrane_time_const=membrane_time_const,\n                                                                   v_reset=v_reset, v_threshold=v_threshold, current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\nX_train, y_spike_train, y_soma_train = generate_multiple_simulations(input_generation_func, cell_simulation_func, num_simulations_train)\nX_test , y_spike_test , y_soma_test  = generate_multiple_simulations(input_generation_func, cell_simulation_func, num_simulations_test )\n\ny_soma_train[y_soma_train > v_threshold] = v_threshold + 0.1\ny_soma_test[y_soma_test   > v_threshold] = v_threshold + 0.1\n\ndataset_generation_duration_sec = time.time() - dataset_generation_start_time\nprint('dataset generation took %.2f minutes' %(dataset_generation_duration_sec / 60))\nprint('each simulation took %.3f seconds to generate' %(dataset_generation_duration_sec / (num_simulations_train + num_simulations_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot some validatory figures"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% plot some validatory figures\n\n# calculate ISI CV\nISIs_train = []\nsim_inds, spike_times = np.nonzero(y_spike_train.T)\nfor curr_sim_ind in np.unique(sim_inds):\n    curr_ISIs = np.diff(spike_times[sim_inds == curr_sim_ind])\n    ISIs_train += list(curr_ISIs)\nISI_CV_train = np.array(ISIs_train).std() / np.array(ISIs_train).mean()\n\ninput_exc_inst_rate = X_train[:num_ex_synapses,:,:].mean() * 1000\ninput_inh_inst_rate = X_train[num_ex_synapses:,:,:].mean() * 1000\n\n# summerize key statstics\nprint('-------------------------------------------')\nprint('train exc input firing rate = %.3f [Hz]' %(input_exc_inst_rate))\nprint('train inh input firing rate = %.3f [Hz]' %(input_inh_inst_rate))\nprint('-------------------------------------------')\nprint('train output firing rate = %.3f [Hz]' %(y_spike_train.mean() * 1000))\nprint('test  output firing rate = %.3f [Hz]' %(y_spike_test.mean() * 1000))\nprint('-------------------------------------------')\nprint('train output ISI Coefficient of Variation = %.3f' %(ISI_CV_train))\nprint('-------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show some generated input spike raster plots\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_ms_raster = 2000\n\n# input raster plots\nplt.figure(figsize=(20,10))\nplt.subplots_adjust(left=0.03,right=0.97,top=0.97,bottom=0.03,hspace=0.2)\nplt.subplot(6,1,1); plt.spy(X_train[:,:num_ms_raster,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample'); plt.axis('off')\nplt.subplot(6,1,2); plt.spy(X_train[:,:num_ms_raster,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample'); plt.axis('off')\nplt.subplot(6,1,3); plt.spy(X_train[:,:num_ms_raster,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample'); plt.axis('off')\nplt.ylabel('synaptic index')\nplt.subplot(6,1,4); plt.spy(X_test[:,:num_ms_raster ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample'); plt.axis('off')\nplt.subplot(6,1,5); plt.spy(X_test[:,:num_ms_raster ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample'); plt.axis('off')\nplt.subplot(6,1,6); plt.spy(X_test[:,:num_ms_raster ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample'); plt.axis('off')\nplt.xlabel('time [ms]');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show some generated output spike trains"},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary spikes\nplt.figure(figsize=(20,13))\nplt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.32)\nplt.subplot(6,1,1); plt.plot(y_spike_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.subplot(6,1,2); plt.plot(y_spike_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.subplot(6,1,3); plt.plot(y_spike_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.subplot(6,1,4); plt.plot(y_spike_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.subplot(6,1,5); plt.plot(y_spike_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.subplot(6,1,6); plt.plot(y_spike_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.xlabel('time [ms]');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show some generated somatic voltage traces"},{"metadata":{"trusted":true},"cell_type":"code","source":"# somatic voltage\nplt.figure(figsize=(20,13))\nplt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.32)\nplt.subplot(6,1,1); plt.plot(y_soma_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.subplot(6,1,2); plt.plot(y_soma_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.subplot(6,1,3); plt.plot(y_soma_train[:,np.random.randint(num_simulations_train)], markersize=3); plt.title('train sample')\nplt.ylabel('voltage [mV]')\nplt.subplot(6,1,4); plt.plot(y_soma_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.subplot(6,1,5); plt.plot(y_soma_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.subplot(6,1,6); plt.plot(y_soma_test[: ,np.random.randint(num_simulations_test) ], markersize=3); plt.title('test sample')\nplt.xlabel('time [ms]');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot ISI and somatic voltage distributions for all generated data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot ISI distribution and somatic voltage distribution\nplt.figure(figsize=(20,12))\nplt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.12)\nplt.subplot(2,1,1); plt.hist(ISIs_train, bins=range(0,2000,5))\nplt.title('Inter spike interval (ISI) distribution')\n\nplt.subplot(2,1,2); plt.hist(y_soma_train.ravel(), bins=150)\nplt.title('Soma voltage distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper function to create a temporally convolutional network"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#%% helper function to create a temporally convolutional network\n\ndef create_temporaly_convolutional_model(max_input_window_size, num_segments, num_syn_types, filter_sizes_per_layer, num_filters_per_layer,\n                                         activation_function_per_layer, l2_regularization_per_layer,\n                                         strides_per_layer, dilation_rates_per_layer, initializer_per_layer):\n    \n    # define input and flatten it\n    binary_input_mat = Input(shape=(max_input_window_size, num_segments * num_syn_types), name='input_layer')\n        \n    for k in range(len(filter_sizes_per_layer)):\n        num_filters   = num_filters_per_layer[k]\n        filter_size   = filter_sizes_per_layer[k]\n        activation    = activation_function_per_layer[k]\n        l2_reg        = l2_regularization_per_layer[k]\n        stride        = strides_per_layer[k]\n        dilation_rate = dilation_rates_per_layer[k]\n        initializer   = initializer_per_layer[k]\n        \n        initializer = initializers.TruncatedNormal(stddev=initializer)\n        first_layer_bias_initializer = initializers.Constant(value=0.1)\n\n        if k == 0:\n            x = Conv1D(num_filters, filter_size, activation=activation, bias_initializer=first_layer_bias_initializer, kernel_initializer=initializer,\n                       kernel_regularizer=l2(l2_reg), strides=stride, dilation_rate=dilation_rate, padding='causal', name='layer_%d' %(k + 1))(binary_input_mat)\n        else:\n            x = Conv1D(num_filters, filter_size, activation=activation, kernel_initializer=initializer, kernel_regularizer=l2(l2_reg),\n                       strides=stride, dilation_rate=dilation_rate, padding='causal', name='layer_%d' %(k + 1))(x)\n        #x = BatchNormalization(name='layer_%d_BN'%(k+1))(x)\n\n    output_spike_init_weights = initializers.TruncatedNormal(stddev=0.05)\n    output_spike_init_bias    = initializers.Constant(value=-2.5)\n    output_soma_init  = initializers.TruncatedNormal(stddev=0.05)\n\n    output_spike_predictions = Conv1D(1, 1, activation='sigmoid', kernel_initializer=output_spike_init_weights, bias_initializer=output_spike_init_bias,\n                                                                  kernel_regularizer=l2(1e-8), padding='causal', name='spikes')(x)\n    output_soma_voltage_pred = Conv1D(1, 1, activation='linear' , kernel_initializer=output_soma_init, kernel_regularizer=l2(1e-8), padding='causal', name='soma')(x)\n\n    temporaly_convolutional_network_model = Model(inputs=binary_input_mat, outputs=[output_spike_predictions, output_soma_voltage_pred])\n\n    optimizer_to_use = Nadam(lr=0.0003)\n    temporaly_convolutional_network_model.compile(optimizer=optimizer_to_use, loss=['binary_crossentropy','mse'], loss_weights=[1.0, 0.003])\n    temporaly_convolutional_network_model.summary()\n\n    return temporaly_convolutional_network_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define network architecture\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% define network architecture\n\nmax_input_window_size = 500\nnum_segments  = 100\nnum_syn_types = 1\n\nnetwork_name = '1_layer_TCN'\n\nnetwork_depth = 1\nfilter_sizes_per_layer        = [80] * network_depth\nnum_filters_per_layer         = [1] * network_depth\ninitializer_per_layer         = [0.25] * network_depth\nactivation_function_per_layer = ['linear'] * network_depth\nl2_regularization_per_layer   = [1e-8] * network_depth\nstrides_per_layer             = [1] * network_depth\ndilation_rates_per_layer      = [1] * network_depth\n\n# define model\ntemporal_conv_net = create_temporaly_convolutional_model(max_input_window_size, num_segments, num_syn_types, filter_sizes_per_layer, num_filters_per_layer,\n                                                         activation_function_per_layer, l2_regularization_per_layer,\n                                                         strides_per_layer, dilation_rates_per_layer, initializer_per_layer)\n\n# prepare data for training\nX_train_for_TCN  = np.transpose(X_train,axes=[2,1,0])\ny1_train_for_TCN = y_spike_train.T[:,:,np.newaxis]\ny2_train_for_TCN = y_soma_train.T[:,:,np.newaxis] - y_soma_train.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#%% train model\n\nnum_train_subsets = 250\nnum_epochs_per_subset = 1\nbatch_size = 16\nval_split_ratio = 0.05\n\nnum_iter_per_epoch = int(((1 - val_split_ratio) * X_train_for_TCN.shape[0]) / batch_size) + 1\n\nnum_iterations    = [0]\ntrain_spikes_loss = [np.nan]\nvalid_spikes_loss = [np.nan]\ntrain_soma_loss   = [np.nan]\nvalid_soma_loss   = [np.nan]\n\nmodel_fitting_start_time = time.time()\n\nfor k in range(num_train_subsets):\n    start_time_ind = np.random.randint(100, X_train_for_TCN.shape[1] - max_input_window_size - 1)\n    end_time_ind   = start_time_ind + max_input_window_size\n    \n    print('%d: selected timepoint range = [%d, %d]' %(k + 1, start_time_ind, end_time_ind))\n    \n    history = temporal_conv_net.fit(X_train_for_TCN[:,start_time_ind:end_time_ind,:],\n                                    [y1_train_for_TCN[:,start_time_ind:end_time_ind,:], y2_train_for_TCN[:,start_time_ind:end_time_ind,:]],\n                                    epochs=num_epochs_per_subset, batch_size=batch_size, validation_split=val_split_ratio)\n\n    num_iterations.append(num_iterations[-1] + num_iter_per_epoch)\n    train_spikes_loss.append(history.history['spikes_loss'][0])\n    train_soma_loss.append(history.history['soma_loss'][0])\n    valid_spikes_loss.append(history.history['val_spikes_loss'][0])\n    valid_soma_loss.append(history.history['val_soma_loss'][0])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fitting_duration_min = (time.time() - model_fitting_start_time) / 60\nprint('model fitting took %.2f minutes' %(model_fitting_duration_min))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% show learning curves\n\nplt.figure(figsize=(15,18))\nplt.subplots_adjust(left=0.08,right=0.95,top=0.95,bottom=0.06,hspace=0.3)\n\nplt.subplot(4,1,1); plt.title('spikes loss', fontsize=14)\nplt.plot(np.array(num_iterations).ravel(), np.array(train_spikes_loss).ravel())\nplt.plot(np.array(num_iterations).ravel(), np.array(valid_spikes_loss).ravel())\nplt.legend(['train', 'valid'], fontsize=14)\nplt.ylabel('spikes log-loss', fontsize=12)\n\nplt.subplot(4,1,2); plt.title('soma loss', fontsize=14)\nplt.plot(np.array(num_iterations).ravel(), np.array(train_soma_loss).ravel())\nplt.plot(np.array(num_iterations).ravel(), np.array(valid_soma_loss).ravel())\nplt.legend(['train', 'valid'], fontsize=14)\nplt.ylabel('Soma voltage MSE', fontsize=12)\n\nplt.subplot(4,1,3); plt.title('spikes loss (log scale)', fontsize=14)\nplt.semilogy(np.array(num_iterations).ravel(), np.array(train_spikes_loss).ravel())\nplt.semilogy(np.array(num_iterations).ravel(), np.array(valid_spikes_loss).ravel())\nplt.legend(['train', 'valid'], fontsize=14)\nplt.ylabel('spikes log-loss', fontsize=12)\n\nplt.subplot(4,1,4); plt.title('soma loss (log scale)', fontsize=14)\nplt.semilogy(np.array(num_iterations).ravel(), np.array(train_soma_loss).ravel())\nplt.semilogy(np.array(num_iterations).ravel(), np.array(valid_soma_loss).ravel())\nplt.legend(['train', 'valid'], fontsize=14)\nplt.ylabel('Soma voltage MSE', fontsize=12)\nplt.xlabel('num train batches', fontsize=14);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show first layer weights of trained model\ndisplay also temporal cross section below\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% show first layer weights with temporal cross section below\n\nxytick_labels_fontsize = 16\ntitle_fontsize = 30\nxylabels_fontsize = 25\nlegend_fontsize = 26\n\nfirst_layer_weights = temporal_conv_net.get_weights()[0][:,:,0].T\n\n# correct positivity for presentation if necessary\nis_excitation_negative = first_layer_weights[:num_ex_synapses,-20:].sum() < 0\nif is_excitation_negative:\n    first_layer_weights = -first_layer_weights\n\nexc_max_avg_w_value = first_layer_weights[:num_ex_synapses,:].mean(axis=0).max()\ninh_min_avg_w_value = first_layer_weights[num_ex_synapses:,:].mean(axis=0).min()\n\n# make sure the range is symmetric for visualization purposes\nvmin_max_range = [1.05 * inh_min_avg_w_value, -1.05 * inh_min_avg_w_value]\n\nfig = plt.figure(figsize=(9,17))\ngs = gridspec.GridSpec(3, 1)\ngs.update(left=0.15, right=0.85, bottom=0.08, top=0.95, hspace=0.08)\nax1 = plt.subplot(gs[:2,0])\nax2 = plt.subplot(gs[2,0])\n\nax1.set_title('layer 1 spatio-temporal filter', fontsize=title_fontsize)\nax1.imshow(first_layer_weights,cmap='jet', vmin=vmin_max_range[0], vmax=vmin_max_range[1])\nax1.set_xticks([])\nax1.set_ylabel('syn index', fontsize=xylabels_fontsize)\n\nfor ytick_label in ax1.get_yticklabels():\n    ytick_label.set_fontsize(xytick_labels_fontsize)\n\ntime_axis_weights = -np.arange(first_layer_weights.shape[1])\nax2.set_title('temporal cross sections', fontsize=title_fontsize)\nax2.plot(time_axis_weights, np.flipud(first_layer_weights[:num_ex_synapses,:].T),c='r')\nax2.plot(time_axis_weights, np.flipud(first_layer_weights[num_ex_synapses:,:].T),c='b')\nex_synapses_patch = mpatches.Patch(color='red', label='exc syn')\ninh_synapses_patch = mpatches.Patch(color='blue', label='inh syn')\nax2.legend(handles=[ex_synapses_patch, inh_synapses_patch], fontsize=legend_fontsize, loc='upper left')\nax2.set_xlim(time_axis_weights.min(),time_axis_weights.max())\nax2.set_xlabel('time before prediction moment [ms]', fontsize=xylabels_fontsize)\nax2.set_ylabel('weight', fontsize=xylabels_fontsize)\nax2.set_ylim(vmin_max_range[0], vmin_max_range[1])\n\nfor ytick_label in ax2.get_yticklabels():\n    ytick_label.set_fontsize(xytick_labels_fontsize)\nfor xtick_label in ax2.get_xticklabels():\n    xtick_label.set_fontsize(xytick_labels_fontsize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create spike predictions on test set from the fitted network output\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% create spike predictions on test set from the fitted network output\n\nprediction_start_time = time.time()\n\noverlap_size = 120\n\nX_test_for_TCN = np.transpose(X_test,axes=[2,1,0])\ny1_test_for_TCN = y_spike_test.T[:,:,np.newaxis]\ny2_test_for_TCN = y_soma_test.T[:,:,np.newaxis] - y_soma_train.mean()\n\ny1_test_for_TCN_hat = np.zeros(y1_test_for_TCN.shape)\ny2_test_for_TCN_hat = np.zeros(y2_test_for_TCN.shape)\n\nnum_test_splits = int(2 + (X_test_for_TCN.shape[1] - max_input_window_size) / (max_input_window_size - overlap_size))\n\nfor k in range(num_test_splits):\n    start_time_ind = k * (max_input_window_size - overlap_size)\n    end_time_ind   = start_time_ind + max_input_window_size\n    \n    curr_X_test_for_TCN = X_test_for_TCN[:,start_time_ind:end_time_ind,:]\n    \n    if curr_X_test_for_TCN.shape[1] < max_input_window_size:\n        padding_size = max_input_window_size - curr_X_test_for_TCN.shape[1]\n        X_pad = np.zeros((curr_X_test_for_TCN.shape[0],padding_size,curr_X_test_for_TCN.shape[2]))\n        curr_X_test_for_TCN = np.hstack((curr_X_test_for_TCN,X_pad))\n    curr_y1_test_for_TCN, curr_y2_test_for_TCN = temporal_conv_net.predict(curr_X_test_for_TCN)\n\n    if k == 0:\n        y1_test_for_TCN_hat[:,:end_time_ind,:] = curr_y1_test_for_TCN\n        y2_test_for_TCN_hat[:,:end_time_ind,:] = curr_y2_test_for_TCN\n    elif k == (num_test_splits - 1):\n        t0 = start_time_ind + overlap_size\n        duration_to_fill = y1_test_for_TCN_hat.shape[1] - t0\n        y1_test_for_TCN_hat[:,t0:,:] = curr_y1_test_for_TCN[:,overlap_size:(overlap_size + duration_to_fill),:]\n        y2_test_for_TCN_hat[:,t0:,:] = curr_y2_test_for_TCN[:,overlap_size:(overlap_size + duration_to_fill),:]\n    else:\n        t0 = start_time_ind + overlap_size\n        y1_test_for_TCN_hat[:,t0:end_time_ind,:] = curr_y1_test_for_TCN[:,overlap_size:,:]\n        y2_test_for_TCN_hat[:,t0:end_time_ind,:] = curr_y2_test_for_TCN[:,overlap_size:,:]\n\nprediction_duration_min = (time.time() - prediction_start_time) / 60\nprint('time took to predict is %.3f minutes' %(prediction_duration_min))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show main evaluation metrics on test data performance\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% show main evaluation metrics\n\nplt.close('all')\n\nxytick_labels_fontsize = 16\ntitle_fontsize = 30\nxylabels_fontsize = 25\nlegend_fontsize = 26\n\nfig = plt.figure(figsize=(11,17))\ngs = gridspec.GridSpec(3,1)\ngs.update(left=0.12, right=0.95, bottom=0.05, top=0.92, hspace=0.6)\nax0 = plt.subplot(gs[0,0])\nax1 = plt.subplot(gs[1,0])\nax2 = plt.subplot(gs[2,0])\n\ny_test = y_spike_test\ny_test_hat = y1_test_for_TCN_hat[:,:,0].T\n\n## plot histograms of prediction given ground truth\nax0.hist(y_test_hat[y_test == True ], bins=np.linspace(0,1,100), color='g', alpha=0.8, density=True)\nax0.hist(y_test_hat[y_test == False], bins=np.linspace(0,1,100), color='b', alpha=0.8, density=True)\nax0.set_title('spike probability prediction histograms', fontsize=title_fontsize)\nax0.set_xlabel('predicted spike probability', fontsize=xylabels_fontsize)\nax0.set_ylabel('density', fontsize=xylabels_fontsize)\nax0.legend(['P(prediction|spike)','P(prediction|no spike)'], fontsize=legend_fontsize)\n\nfor tick_label in (ax0.get_xticklabels() + ax0.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\n## plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_test.ravel(), y_test_hat.ravel())\nAUC_score = auc(fpr, tpr)\n\nax1.plot(fpr, tpr)\nax1.set_title('ROC curve (AUC = %.3f)' %(AUC_score), fontsize=title_fontsize)\nax1.set_xlabel('False Positive Rate', fontsize=xylabels_fontsize)\nax1.set_ylabel('True Positive Rate', fontsize=xylabels_fontsize)\nax1.set_ylim(0,1.05)\nax1.set_xlim(-0.03,1)\n\nfor tick_label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\ndesired_false_positive_rate = 0.002\ndesired_fp_ind = np.argmin(abs(fpr - desired_false_positive_rate))\nif desired_fp_ind == 0:\n    desired_fp_ind = 1\nactual_false_positive_rate = fpr[desired_fp_ind]\nprint('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, tpr[desired_fp_ind]))\n\n# organize the data for ploting\ndesired_threshold = thresholds[desired_fp_ind]\nground_truth_output_spikes = y_test.T\npredicted_output_spikes    = y_test_hat.T > desired_threshold\nnum_test_traces            = X_test.shape[2]\n\n## plot the cross correlation between the spike trains\n\nhalf_time_window_size_ms = 50\ndelta_time = 10\n\ninds_inside_delta = range(half_time_window_size_ms - delta_time, half_time_window_size_ms + 1 + delta_time)\n\n# pad both spike train predictions with zeros from both sides\nzero_padding_matrix = np.zeros((num_test_traces,half_time_window_size_ms))\npredicted_output_spikes_padded    = np.hstack((zero_padding_matrix,predicted_output_spikes,zero_padding_matrix))\nground_truth_output_spikes_padded = np.hstack((zero_padding_matrix,ground_truth_output_spikes,zero_padding_matrix))\n\n# calculate recall curve: P(predicted spikes|ground truth=spike)\nrecall_curve = np.zeros(1 + 2 * half_time_window_size_ms)\ntrace_inds, spike_inds = np.nonzero(ground_truth_output_spikes_padded)\nfor trace_ind, spike_ind in zip(trace_inds,spike_inds):\n    recall_curve += predicted_output_spikes_padded[trace_ind,(spike_ind - half_time_window_size_ms):(1 + spike_ind + half_time_window_size_ms)]\nrecall_curve /= recall_curve.sum()\nrecall = recall_curve[inds_inside_delta].sum()\n\ntime_axis_cc = np.arange(-half_time_window_size_ms, half_time_window_size_ms + 1)\n\ntime_in_delta = time_axis_cc[inds_inside_delta]\nrecall_in_delta = recall_curve[inds_inside_delta]\nrecall_patch = mpatches.Patch(color='b', label='area = %.2f' %(recall))\n\nax2.set_title('$P(Prediction | GroundTruth = 1)$', fontsize=title_fontsize)\nax2.plot(time_axis_cc, recall_curve, c='k')\nax2.fill_between(time_in_delta, recall_in_delta, 0, facecolor='b', alpha=0.8)\nax2.legend(handles=[recall_patch],fontsize=legend_fontsize)\nax2.vlines([time_in_delta[0],time_in_delta[-1]], [0,0], [recall_in_delta[0],recall_in_delta[-1]], colors='k', linewidths=3.3)\nax2.set_ylim(0, 1.05 * recall_curve.max())\nax2.set_xlabel('$\\Delta t$ [ms]', fontsize=xylabels_fontsize)\nax2.set_ylabel('density', fontsize=xylabels_fontsize)\n\nfor tick_label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot voltage trace scatter plot and residual distribution\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot voltage trace scatter plot\n\nfig = plt.figure(figsize=(12,15))\ngs = gridspec.GridSpec(3, 1)\ngs.update(left=0.12, right=0.95, bottom=0.05, top=0.92, hspace=0.37)\nax0 = plt.subplot(gs[:2,0])\nax1 = plt.subplot(gs[2,0])\n\nxytick_labels_fontsize = 16\ntitle_fontsize = 25\nxylabels_fontsize = 22\nlegend_fontsize = 26\n\nnum_datapoints_in_scatter = 60000\nmean_soma_voltage = y_soma_train.mean()\nselected_datapoints = np.random.choice(range(len(y2_test_for_TCN.ravel())),size=num_datapoints_in_scatter,replace=False)\nselected_GT = y2_test_for_TCN.ravel()[selected_datapoints] + 0.02 * np.random.randn(num_datapoints_in_scatter) + mean_soma_voltage\nselected_pred = y2_test_for_TCN_hat.ravel()[selected_datapoints] + mean_soma_voltage\n\nsoma_explained_variance_percent = 100.0 * explained_variance_score(y2_test_for_TCN.ravel(),y2_test_for_TCN_hat.ravel())\nsoma_RMSE = np.sqrt(MSE(y2_test_for_TCN.ravel(),y2_test_for_TCN_hat.ravel()))\nsoma_MAE  = MAE(y2_test_for_TCN.ravel(),y2_test_for_TCN_hat.ravel())\n\n\nax0.scatter(selected_GT,selected_pred, s=1.5, alpha=0.8)\nax0.set_title('soma voltage prediction. explained variance = %.2f%s' %(soma_explained_variance_percent,'%'), fontsize=title_fontsize)\nax0.set_xlabel('ground truth soma voltage [mV]', fontsize=xylabels_fontsize)\nax0.set_ylabel('predicted soma voltage [mV]', fontsize=xylabels_fontsize)\nsoma_voltage_lims = np.round([np.percentile(selected_pred,0.2),np.percentile(selected_pred,99.8)]).astype(int)\nvoltage_granularity = 5\nvoltage_setpoint = -56\nvoltage_axis = np.arange(soma_voltage_lims[0],soma_voltage_lims[1])\nvoltage_ticks_to_show = np.unique(((voltage_axis - voltage_setpoint) / voltage_granularity).astype(int) * voltage_granularity + voltage_setpoint)\nvoltage_ticks_to_show = voltage_ticks_to_show[np.logical_and(voltage_ticks_to_show >= soma_voltage_lims[0],\n                                                             voltage_ticks_to_show <= soma_voltage_lims[1])]\nax0.set_xticks(voltage_ticks_to_show)\nax0.set_yticks(voltage_ticks_to_show)\nax0.set_xlim(soma_voltage_lims[0],soma_voltage_lims[1])\nax0.set_ylim(soma_voltage_lims[0],soma_voltage_lims[1])\nax0.plot([-90,-50],[-90,-50], ls='-', c='k')\nax0.spines['top'].set_visible(False)\nax0.spines['right'].set_visible(False)\n\nfor tick_label in (ax0.get_xticklabels() + ax0.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\nax1.hist(y2_test_for_TCN_hat.ravel() - y2_test_for_TCN.ravel(), bins=300, density=True)\nax1.set_title('voltage prediction redisduals. RMSE = %.3f [mV]' %(soma_RMSE), fontsize=title_fontsize)\nax1.set_xlabel('$\\Delta$V [mV]', fontsize=xylabels_fontsize)\nax1.set_xlim(-20,20)\nax1.set_yticks([])\n\nfor tick_label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replication of full figure 1 from paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% figure 1 replication\n\nxytick_labels_fontsize = 16\ntitle_fontsize = 26\nxylabels_fontsize = 17\nlegend_fontsize = 15\n\nnum_spikes_per_simulation = y1_test_for_TCN.sum(axis=1)[:,0]\nsoma_bias_to_add = y_soma_train.mean()\n\n# get a selected trace\npossible_presentable_candidates = np.nonzero(np.logical_and(num_spikes_per_simulation >= 2, num_spikes_per_simulation <= 8))[0]\nselected_trace  = np.random.choice(possible_presentable_candidates)\nzoomin_fraction = [0.25 + 0.21 * np.random.rand(), 0.54 + 0.21 * np.random.rand()]\n\n# selected_trace = 1194\n# zoomin_fraction = [0.1, 0.4]\n\nprint('selected_trace = %s' %(selected_trace))\nprint('zoomin_fraction = %s' %(zoomin_fraction))\n\n# collect everything need for presentation\nspike_trace_GT   = y1_test_for_TCN[selected_trace,:,0]\nspike_trace_pred = y1_test_for_TCN_hat[selected_trace,:,0] > desired_threshold\n\noutput_spike_times_in_ms_GT   = np.nonzero(spike_trace_GT)[0]\noutput_spike_times_in_ms_pred = np.nonzero(spike_trace_pred)[0]\n\nsoma_voltage_trace_GT   = y2_test_for_TCN[selected_trace,:,0] + soma_bias_to_add\nsoma_voltage_trace_pred = y2_test_for_TCN_hat[selected_trace,:,0] + soma_bias_to_add\n\nsoma_voltage_trace_GT[output_spike_times_in_ms_GT] = 40\nsoma_voltage_trace_pred[output_spike_times_in_ms_pred] = 40\n\nsim_duration_ms = spike_trace_GT.shape[0]\ntime_in_sec = np.arange(sim_duration_ms) / 1000.0\n\n# for raster plot (scatter)\nall_presynaptic_spikes_bin = X_test_for_TCN[selected_trace,:,:]\n\nsyn_activation_time, syn_activation_index = np.nonzero(all_presynaptic_spikes_bin)\nex_synapses_inds = syn_activation_index < num_ex_synapses\n\nex_syn_activation_time   = syn_activation_time[ex_synapses_inds] / 1000.0\nex_syn_activation_index  = num_synapses - syn_activation_index[ex_synapses_inds]\ninh_syn_activation_time  = syn_activation_time[~ex_synapses_inds] / 1000.0\ninh_syn_activation_index = num_synapses - syn_activation_index[~ex_synapses_inds]\n\n# set up the grid specs\nplt.close('all')\nfig = plt.figure(figsize=(24,18.5))\n\ngs1 = gridspec.GridSpec(3,1)\ngs1.update(left=0.05, right=0.65, bottom=0.05, top=0.45, wspace=0.01, hspace=0.01)\n\ngs2 = gridspec.GridSpec(12,2)\ngs2.update(left=0.73, right=0.97, bottom=0.07, top=0.97, wspace=0.58, hspace=1.05)\n\nax10 = plt.subplot(gs1[0,0])\nax11 = plt.subplot(gs1[1,0])\nax12 = plt.subplot(gs1[2,0])\n\nax10.axis('off')\nax11.axis('off')\nax12.axis('off')\n\nax31 = plt.subplot(gs2[:5,:])\nax32 = plt.subplot(gs2[5:7,:])\n\na33_left  = plt.subplot(gs2[7:9,0])\na33_right = plt.subplot(gs2[7:9,1])\n\nax34 = plt.subplot(gs2[9:,:])\n\n### left column of the figure\n\n## raster of input exitation and inhibition of the selected trace\nax10.scatter(ex_syn_activation_time, ex_syn_activation_index, s=2, c='r')\nax10.scatter(inh_syn_activation_time, inh_syn_activation_index, s=2, c='b')\nax10.set_xlim(0, sim_duration_sec - 0.01)\nax10.set_ylabel('syn index', fontsize=xylabels_fontsize)\nax10.grid('off')\nax10.set_yticks([])\nax10.set_xticks([])\n\n## ground truth and prediction trace\nax11.plot(time_in_sec,soma_voltage_trace_GT,c='c')\nax11.plot(time_in_sec,soma_voltage_trace_pred,c='m',linestyle=':')\nax11.set_xlim(0,sim_duration_sec)\nax11.set_ylabel('$V_m$ (mV)', fontsize=xylabels_fontsize)\nfor tick_label in (ax11.get_xticklabels() + ax11.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\n# add dashed rectangle\nzoomin_xlims = [zoomin_fraction[0] * sim_duration_sec, zoomin_fraction[1] * sim_duration_sec]\nzoomin_dur_sec = zoomin_xlims[1] - zoomin_xlims[0]\nzoomin_time_in_sec = np.logical_and(time_in_sec >= zoomin_xlims[0], time_in_sec <= zoomin_xlims[1])\nzoomin_ylims = [soma_voltage_trace_GT[zoomin_time_in_sec].min() -2.5, -49]\nzoomin_scalebar_xloc = zoomin_xlims[1] - 0.05 * zoomin_dur_sec\n\nrect_w = zoomin_xlims[1] - zoomin_xlims[0]\nrect_h = zoomin_ylims[1] - zoomin_ylims[0]\nrect_bl_x = zoomin_xlims[0]\nrect_bl_y = zoomin_ylims[0]\ndashed_rectangle = mpatches.Rectangle((rect_bl_x,rect_bl_y),rect_w,rect_h,linewidth=2,edgecolor='k',linestyle='--',facecolor='none')\n\nax11.add_patch(dashed_rectangle)\n\n## zoomin section of ground truth and prediction trace\nax12.plot(time_in_sec,soma_voltage_trace_GT,c='c')\nax12.plot(time_in_sec,soma_voltage_trace_pred,c='m',linestyle=':')\nax12.set_xlim(zoomin_xlims[0],zoomin_xlims[1])\nax12.set_ylim(zoomin_ylims[0],zoomin_ylims[1])\nax12.set_ylabel('$V_m$ (mV)', fontsize=xylabels_fontsize)\nax12.set_xlabel('time (sec)', fontsize=xylabels_fontsize)\n\nfor tick_label in (ax12.get_xticklabels() + ax12.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\n# add scale bar to top plot\nzoomout_scalebar_xloc = 0.95 * sim_duration_sec\n\nscalebar_loc = np.array([zoomout_scalebar_xloc,-25])\nscalebar_size_x = 0.6\nscalebar_str_x = '600 ms'\nscalebar_size_y = 40\nscalebar_str_y = '40 mV'\n\nx = [scalebar_loc[0], scalebar_loc[0] - scalebar_size_x]\ny = [scalebar_loc[1], scalebar_loc[1]]\nax11.plot(x,y,lw=2,c='k')\nax11.text(scalebar_loc[0] - 0.05 * scalebar_size_x, scalebar_loc[1] - 0.15 * scalebar_size_y,\n          scalebar_str_x, color='k', fontsize=15, ha='right', va='top', rotation='horizontal')\n\nx = [scalebar_loc[0], scalebar_loc[0]]\ny = [scalebar_loc[1], scalebar_loc[1] + scalebar_size_y]\nax11.plot(x,y,lw=2,c='k')\nax11.text(scalebar_loc[0] + 0.1 * scalebar_size_x, scalebar_loc[1] + 0.6 * scalebar_size_y,\n          scalebar_str_y, color='k', fontsize=15, ha='left', va='top', rotation='horizontal')\n\n# add scalebar to bottom plot\nscalebar_loc = np.array([zoomin_scalebar_xloc,-58])\nscalebar_size_x = 0.1\nscalebar_str_x = '100 ms'\nscalebar_size_y = 8\nscalebar_str_y = '%d mV' %(scalebar_size_y)\n\nx = [scalebar_loc[0], scalebar_loc[0] - scalebar_size_x]\ny = [scalebar_loc[1], scalebar_loc[1]]\nax12.plot(x,y,lw=2,c='k')\nax12.text(scalebar_loc[0] - 0.15 * scalebar_size_x, scalebar_loc[1] - 0.15 * scalebar_size_y,\n          scalebar_str_x, color='k', fontsize=15, ha='right', va='top', rotation='horizontal')\n\nx = [scalebar_loc[0], scalebar_loc[0]]\ny = [scalebar_loc[1], scalebar_loc[1] + scalebar_size_y]\nax12.plot(x,y,lw=2,c='k')\nax12.text(scalebar_loc[0] + 0.1 * scalebar_size_x, scalebar_loc[1] + 0.6 * scalebar_size_y,\n          scalebar_str_y, color='k', fontsize=15, ha='left', va='top', rotation='horizontal')\n\n### right column of the figure\n\n## ROC curve\na33_left.plot(fpr, tpr, c='k')\na33_left.set_xlabel('False alarm rate', fontsize=xylabels_fontsize)\na33_left.set_ylabel('Hit rate', fontsize=xylabels_fontsize)\na33_left.set_ylim(0,1.05)\na33_left.set_xlim(-0.03,1)\n\na33_left.spines['top'].set_visible(False)\na33_left.spines['right'].set_visible(False)\n\nfor tick_label in (a33_left.get_xticklabels() + a33_left.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\nleft, bottom, width, height = [0.768, 0.327, 0.047, 0.067]\na33_left_inset = fig.add_axes([left, bottom, width, height])\na33_left_inset.plot(fpr, tpr, c='k')\na33_left_inset.set_ylim(0,1.05)\na33_left_inset.set_xlim(-0.001,0.012)\na33_left_inset.spines['top'].set_visible(False)\na33_left_inset.spines['right'].set_visible(False)\na33_left_inset.scatter(actual_false_positive_rate, tpr[desired_fp_ind + 1], c='r', s=100)\n\nprint('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, tpr[desired_fp_ind]))\n\n## cross correlation between the spike trains\na33_right.plot(time_axis_cc, 1000 * recall_curve, c='k')\na33_right.set_ylim(0, 1.05 * 1000 * recall_curve.max())\na33_right.set_xlabel('$\\Delta t$ (ms)', fontsize=xylabels_fontsize)\na33_right.set_ylabel('spike rate (Hz)', fontsize=xylabels_fontsize)\na33_right.spines['top'].set_visible(False)\na33_right.spines['right'].set_visible(False)\na33_right.spines['left'].set_visible(False)\na33_right.spines['bottom'].set_visible(False)\nfor tick_label in (a33_right.get_xticklabels() + a33_right.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)\n\n## weights heatmap\nvmin_max_range = [1.1 * inh_min_avg_w_value, -1.1 * inh_min_avg_w_value]\nvmin_max_range_to_plot = (8 * np.array(vmin_max_range)).astype(int) / 10\n\nweights_images = ax31.imshow(first_layer_weights, cmap='jet', aspect='auto', vmin=vmin_max_range[0], vmax=vmin_max_range[1])\nax31.set_xticks([])\nax31.set_ylabel('Synaptic index', fontsize=xylabels_fontsize)\nfor ytick_label in ax31.get_yticklabels():\n    ytick_label.set_fontsize(xytick_labels_fontsize)\n\n\nax_colorbar = inset_axes(ax31, width=\"50%\", height=\"6%\", loc=2)\ncbar = plt.colorbar(weights_images, cax=ax_colorbar, orientation=\"horizontal\", ticks=[vmin_max_range_to_plot[0], 0, vmin_max_range_to_plot[1]])\nax_colorbar.xaxis.set_ticks_position(\"bottom\")\ncbar.ax.tick_params(labelsize=15)\n\nax31.text(12, 12, 'weight (A.U)', color='k', fontsize=15, ha='left', va='top', rotation='horizontal')\n\n## temporal cross sections of weights\nax32.plot(time_axis_weights, np.flipud(first_layer_weights[:num_ex_synapses,:].T),c='r')\nax32.plot(time_axis_weights, np.flipud(first_layer_weights[num_ex_synapses:,:].T),c='b')\nax32.set_xlim(time_axis_weights.min(),time_axis_weights.max())\nax32.set_xlabel('Time before $t_0$ (ms)', fontsize=xylabels_fontsize)\nax32.set_ylabel('Weight (A.U)', fontsize=xylabels_fontsize)\nax32.set_ylim(vmin_max_range[0], vmin_max_range[1])\nax32.set_yticks([vmin_max_range_to_plot[0],0,vmin_max_range_to_plot[1]])\n\nfor ytick_label in ax32.get_yticklabels():\n    ytick_label.set_fontsize(xytick_labels_fontsize)\nfor xtick_label in ax32.get_xticklabels():\n    xtick_label.set_fontsize(xytick_labels_fontsize)\n\n# place a text box in upper left in axes coords\nax32.text(-25, 0.5 * vmin_max_range_to_plot[1] - 0.1, 'Exc', color='r', fontsize=20, verticalalignment='bottom')\nax32.text(-25, 0.5 * vmin_max_range_to_plot[0], 'Inh', color='b', fontsize=20, verticalalignment='top')\n\n## voltage predction scatter plot\nax34.scatter(selected_GT,selected_pred, s=1.0, alpha=0.8)\nsoma_voltage_lims = np.round([np.percentile(selected_pred,0.2),np.percentile(selected_pred,99.8)]).astype(int)\nvoltage_granularity = 5\nvoltage_setpoint = -56\nvoltage_axis = np.arange(soma_voltage_lims[0],soma_voltage_lims[1])\nvoltage_ticks_to_show = np.unique(((voltage_axis - voltage_setpoint) / voltage_granularity).astype(int) * voltage_granularity + voltage_setpoint)\nvoltage_ticks_to_show = voltage_ticks_to_show[np.logical_and(voltage_ticks_to_show >= soma_voltage_lims[0],\n                                                             voltage_ticks_to_show <= soma_voltage_lims[1])]\nax34.set_xticks(voltage_ticks_to_show)\nax34.set_yticks(voltage_ticks_to_show)\nax34.set_xlim(soma_voltage_lims[0],soma_voltage_lims[1])\nax34.set_ylim(soma_voltage_lims[0],soma_voltage_lims[1])\nax34.plot([-90,-50],[-90,-50], ls='-', c='k')\nax34.set_xlabel('I&F (mV)', fontsize=xylabels_fontsize)\nax34.set_ylabel('ANN (mV)', fontsize=xylabels_fontsize)\nax34.spines['top'].set_visible(False)\nax34.spines['right'].set_visible(False)\n\nfor tick_label in (ax34.get_xticklabels() + ax34.get_yticklabels()):\n    tick_label.set_fontsize(xytick_labels_fontsize)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}