{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic Comments classification Using Deep Learning.\n##### Even if most of it is just preprocessing of data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndir = \"../input/cleaned-toxic-comments/train_preprocessed.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Get the Corpus of all the comments and related Toxicity fields","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(dir)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the dataset contains the following fields,\n1. comment_text: The comments in english\n2. id: The comment Id (not useful)\n3. identity_hate, insult, obscene, severe_toxic, threat, toxic: the types of toxicity in the comment\n4. set: whether the comment is in train set or test set (since this is only training, this column isnt required)\n5. toxicity: the measure of toxicity of the comment.( 0 for non toxic, 1 and above for toxic with increasing severity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Since we are making a simple binary classifier which seperates toxic from non toxic, we can drop the types of toxicity and only keep the \"toxicity\" feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Divide the data set into features and labels:\n> Features = \"comment\"            \n> Labels = \"toxicity\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Features = data['comment_text']\nLabels = np.array([0 if y == 0 else 1 for y in data['toxicity']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing and preprocessing the data\n\n### As we can see, the comments are in string format. Since our models require numerical data to work with, we need to convert these words into some numbers.\n\n#### We can use the Tokenize function provided by keras api to convert the words in our comment_text to unique numbers for each word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_WORDS = 40000 # Maximum number of unique words which need to be tokenized\nMAXLEN = 50 # Maximum length of a sentence/ comment\nPADDING = 'post' # The type of padding done for sentences shorter than the Max len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=NUM_WORDS)\n\n# Fit the tokenizer on the comments \ntokenizer.fit_on_texts(Features)\n\n# Get the word index of the top 20000 words from the dataset\nword_idx = tokenizer.word_index\n\n# Convert the string sentence to a sequence of their numerical values\nFeature_sequences = tokenizer.texts_to_sequences(Features)\n\n# Pad the sequences to make them of uniform length\npadded_sequences = pad_sequences(Feature_sequences, maxlen = MAXLEN, padding = PADDING)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Transformation of sentence::\")\nprint(\"\\n\\nThe normal Sentencen:\\n\")\nprint(Features[2])\nprint(\"\\n\\nThe tokenized sequence:\\n\")\nprint(Feature_sequences[2])\nprint(\"\\n\\nThe padded sequence:\\n\")\nprint(padded_sequences[2])\n\n# Convert to array for passing through the model\nX = np.array(padded_sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the word Embeddings\n### When training a deep learning model on text, an Embedding layer is created which assigns some set of vectors to a word so that similar meaning words get clustered together. Since it is a long and compuatation heavy task, we generally import these vectors and use them.\n\n### These vectors are ususally of a higher dimension example 50 or 100 so that the meaning of the words can be classified properly.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\" style=\"width:800px;height:500px;\">\n\n#### GloVe Visualization provided by stanford","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### There are multiple sources from where a person can get these word embeddings. We will use the one named as \"Global Vector for Word Representation\" or GloVe provided [Here](url:\"https://www.kaggle.com/watts2/glove6b50dtxt\") . You can read more about it on [this page](https://nlp.stanford.edu/projects/glove/)\n### In this notebook we will be using 50 dimensional vector version for the word embeddings.    \n#### The embeddings are stored as : \"word\",    vector[0],     vector[1],      vector[2], ..........,     vector[49]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/glove6b50dtxt/glove.6B.50d.txt\", encoding='utf-8') as f:\n    for x in f:\n        print(x)\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the word is followed by a long list of vectors corresponding to the word 'the' we need to split the word and the vectors and store in a dictionary for easy use","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 50 # number of dimensions of the word embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the word to index dictionary\nword_2_vec = {}\nwith open(\"../input/glove6b50dtxt/glove.6B.50d.txt\", encoding='utf-8') as f:\n    for line in f:\n        \n        # spilt the elements by space\n        elements = line.split()\n        word = elements[0]\n        # convert to np array\n        vecs = np.asarray(elements[1:], dtype='float32')\n        word_2_vec[word] = vecs\n        \nprint(\"Done....\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of words {len(word_2_vec)}\")\nprint(f\"Shape of the vector {len(word_idx)}\")\nprint(f\"Number of max words to be saved {NUM_WORDS}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cool. So we have 400000 unique words with their vectors. Time to convert each word in the comment_text to a sequence of these vectors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Combine the Word Index and the Vectors\n### First we find the minimum from the number of our vocabulary words and number of words that are indexed during the tokenizer fit since we want the words in the embedding matrix to be in our vocabulary AND have a token. It should also have a vector associated with it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the max number of words that exist in word index and vocabulary both\nnum = min(NUM_WORDS, len(word_idx)+ 1)\n\n# Matrix containing the word index and the vector of the word\nembedding_matrix = np.zeros((num, EMBEDDING_DIM))\n\nfor word, idx in word_idx.items():\n    if idx < NUM_WORDS:\n        word_vec = word_2_vec.get(word)\n        if word_vec is not None:\n            embedding_matrix[idx] = word_vec\n            \nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making and training on the model\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Coming to the best part of the project, Training the model\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Before we start on the model, lets first make a class which could help us train multiple models one after the other and compare them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(models, epochs, graph=True, verbose=2):\n    n = 1\n    plt.figure(figsize=(10, 7))\n    \n    histories = []\n    for model in models:\n        print(f\"model number : {n} is training\")\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n        history = model.fit(\n            X, \n            Labels,\n            batch_size=128,\n            epochs=epochs,\n            validation_split=0.2, # 20 percent data reserved for validation to avoid or monitor overfitting/ underfitting\n            verbose=verbose,\n        )\n        histories.append(history)\n        \n        if graph:\n            plt.plot(history.history['val_acc'], label=f\"Model {n}\")\n        n+=1\n            \n    plt.xlabel('Epochs')\n    plt.ylabel('Validation Accuracy')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets start with a very simple model which starts with an embedding layers, goes throught an LSTM and an GlobalAveragePool and then to a Final output layer with a sigmoid activation.\n\n#### For information about the LSTM networks or other RNN networks you can refer to [this video](https://www.youtube.com/watch?v=WCUNPb-5EYI) or try to get [this course](https://www.coursera.org/specializations/natural-language-processing) by Deeplearning.ai on Coursera(highly recommended)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    \n    # Embedding layers that takes in the embedding matrix. Be sure to set trainable to false or else it will mess up your \n    # nicely pre trained vectors\n    tf.keras.layers.Embedding(num, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAXLEN,trainable=False),\n    tf.keras.layers.LSTM(5, return_sequences=True),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets add some more LSTM units and more Dense Layers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2= tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(num, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAXLEN,trainable=False),\n    tf.keras.layers.LSTM(50, return_sequences=True),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(5, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets add some Convolution layers to see how it does","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model3= tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(num, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAXLEN,trainable=False),\n    \n    tf.keras.layers.LSTM(50, return_sequences=True),\n    tf.keras.layers.Conv1D(10,15, activation='relu'),\n\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(5, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finally lets add some Bidirectional LSTMs to the first model to see how it compares ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model4= tf.keras.models.Sequential([\n    \n    # Embedding layers that takes in the embedding matrix. Be sure to set trainable to false or else it will mess up your \n    # nicely pre trained vectors\n    tf.keras.layers.Embedding(num, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAXLEN,trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5, return_sequences=True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel4.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to finally use our train function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [model, model2, model3, model4]\ntrain(models, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### You can test out your own combinations for the models. I hope you learnt something new and useful today. If you have any questions, leave them in the comments and i will try my best to help you out","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}