{"cells":[{"metadata":{},"cell_type":"markdown","source":"**version - info<br>\nversion - loss_fold_8 - loss_fold_9 - f1_class_pT>25GeV ( precision | recall )<br>**\n\nv01 - Attention-aggregation-1<br>\nv01 - 2.002-2.030-2956-0.83(88|79)<br>\nv02 - Attention-aggregation-2<br>\nv02 - 2.069-2.422-3210-0.81(89|75)<br>\nv03 - Attention-aggregation-3<br>\nv03 - 1.799-2.217-2849-0.83(87|80)<br>\nv04 - Attention-aggregation-4<br>\nv04 - 1.785-2.479-3028-0.83(87|79)<br>\nv05 - Edge+Sage-1<br>\nv05 - 1.773-1.866-2779-0.84(89|79)<br>\nv06 - Edge+Sage-2<br>\nv07 - Edge+Sage-3<br>\nv07 - 1.707-2.122-2840-0.84(87|81)<br>\nv08 - Edge+Sage-4<br>\nv08 - 1.941-1.872-2862-0.83(89|78)<br>\nv09 - Edge+Sage-5<br>\nv09 - 1.775-1.810-2734-0.84(89|80)<br>\nv10 - Edge+Sage+CG-1<br>\nv10 - 1.836-1.936-2817-0.84(89|79)<br>\nv11 - Edge+Sage+CG-2<br>\nv11 - 1.930-1.874-2.912-0.83(89|78)<br>\nv12 - Edge+Sage+CG-3<br>\nv12 - 2.591-7.717-6252-0.52(86|38)<br>\nv13 - Edge+Sage+CG-4<br>\nv13 - 1.789-2.464-3233-0.82(90|75)<br>\nv14 - Edge+Sage-6<br>\nv14 - 1.849-1.914-2866-0.84(89|79)<br>\nv15 - Edge+Sage-7<br>\nv15 - 2.487-1.824-3256-0.82(88|76)<br>\nv16 - Edge+CG-1<br>\nv16 - 2.386-1.796-3210-0.81(90|74)<br>\nv17 - Edge+CG-2<br>\nv17 - 1.826-1.802-2767-0.84(89|80)<br>\nv18 - Edge+CG-3<br>\nv19 - NewConv-1 - New rep = W x msg + (1-W) x old rep <br>\nv19 - 1.824-1.992-2824-0.83(89|79)<br>\nv20 - NewConv-2 - New rep = W1 x msg + W2 x old rep <br>\nv20 - 1.758-1.781-2767-0.84(90|79)<br>\nv21 - NewConv-3 - New rep = W1 x msg + W2 x old rep <br>\nv21 - 1.849-1.862-2838-0.84(89|79)<br>\nv22 - NewConv-4 - Update-newconv3 + Msg-v18<br>\nv22 - 2.082-2.172-3011-0.82(89|77)<br>\nv23 - NewConv-5 - Update-newconv3 + Msg-v18<br>\nv23 - 2.363-2.381-3297-0.81(86|76)<br>\nv24 - NewConv-6 - Update-newconv2 + Msg-v18<br>\nv24 - 2.068-1.738-2900-0.84(88|80)<br>\nv25 - NewConv-7 - Update-newconv2 + Msg-v18<br>\nv25 - 1.725-2.395-2879-0.83(87|89)<br>\nv26 - NewConv-8 - Update-newconv2 + Msg-CG<br>\nv26 - 1.792-1.893-2792-0.84(89|79)<br>\nv27 - NewConv-9 - Update-newconv2 + Msg-CG<br>\nv27 - 7.897-1.896-6032-0.54(89|39)<br>\nv28 - NewConv-10 - Update-newconv2 + Msg-CG<br>\nv28 - 4.327-2.059-4387-0.69(82|59)<br>\nv29 - NewConv-11 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-1<br>\nv29 - 1.864-1.848-2816-0.84(89|79)<br>\nv30 - NewConv-12 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-2<br>\nv30 - 1.767-1.894-2802-0.84(89|80)<br>\nv31 - NewConv-13 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-3<br>\nv31 - 2.398-2.001-3109-0.83(88|78)<br>\nv36 - NewConv-14 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-4<br>\nv36 - 2.246-1.837-2936-0.84(88|80)<br>\nv37 - NewConv-15 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-5<br>\nv37 - 1.804-1.830-2766-0.84(89|80)<br>\nv38 - NewConv-16 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-6<br>\nv38 - 1.767-1.894-2802-0.84(89|80)<br>\nv39 - NewConv-17 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-7<br>\nv39 - 1.817-1.818-2782-0.84(89|80)<br>\nv40 - NewConv-18 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-8<br>\nv40 - 1.774-1.853-2731-0.84(89|80)<br>\nv41 - NewConv-19 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-9<br>\nv41 - 1.761-1.845-2742-0.84(90|80)<br>\nv42 - NewConv-20 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-10<br>\nv42 - 1.786-1.736-2773-0.84(90|79)<br>\nv43 - NewConv-21 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-11<br>\nv43 - 1.745-2.674-3054-0.83(87|79)<br>\nv44 - NewConv-22 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-12<br>\nv44 - 1.987-1.809-2835-0.84(89|79)<br>\nv45 - NewConv-23 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-13<br>\nv45 - 1.805-1.796-2765-0.84(89|80)<br>\nv46 - NewConv-24 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-14<br>\nv46 - 1.791-1.840-2748-0.84(89|80)<br>\nv47 - NewConv-25 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-15<br>\nv47 - 1.775-1.803-2743-0.84(90|79)<br>\nv48 - NewConv-26 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-16<br>\nv48 - 1.847-1.938-2840-0.83(89|79)<br>\nv49 - NewConv-27 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-17 + CG Conv-1<br>\nv49 - 2.387-2.214-3146-0.82(87|77)<br>\nv50 - NewConv-28 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-18 + CG Conv-2<br>\nv50 - 2.062-12.562-6385-0.54(90|39)<br>\nv51 - NewConv-29 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-19 + CG Conv-3<br>\nv51 - 1.870-2.287-2943-0.84(88|78)<br>\nv52 - NewConv-30 - New rep = W1 x msg + W2 x old rep - Attention Aggregation-20 + CG Conv-4<br>\nv52 - 1.862-7.686-5813-0.56(89|41)<br>\nv56 - Architecture-Optimization-1<br>\nv56 - 1.782-1.808-2724-0.84(89|80)<br>\nv57 - Architecture-Optimization-2<br>\nv57 - 1.779-1.814-2700-0.84(89|80)<br>\nv58 - Architecture-Optimization-3<br>\nv58 - 1.891-1.777-2798-0.84(89|80)<br>\nv59 - Architecture-Optimization-4<br>\nv59 - 1.809-1.840-2781-0.84(89|80)<br>\nv60 - Architecture-Optimization-5<br>\nv60 - 1.775-1.805-2766-0.84(90|79)<br>\nv61 - Architecture-Optimization-6<br>\nv61 - 1.785-1.836-2740-0.84(89|80)<br>\nv62 - Architecture-Optimization-7<br>\nv62 - 1.794-1.816-2756-0.84(89|80)<br>\nv63 - Architecture-Optimization-8<br>\nv63 - 1.785-2.228-2908-0.83(88|80)<br>\nv64 - Architecture-Optimization-9<br>\nv64 - 1.790-1.790-2750-0.84(89|80)<br>\nv65 - Architecture-Optimization-10<br>\nv65 - 1.763-1.841-2753-0.84(90|79)<br>\nv66 - GoDeep-depth-8<br>\nv66 - 2.055-1.837-2895-0.83(89|78)<br>\nv67 - GoDeep-depth-7<br>\nv67 - 1.786-1.817-2725-0.84(89|80)<br>\nv68 - GoDeep-depth-6<br>\nv68 - 7.897-1.849-5979-0.55(88|40)<br>\nv69 - GoDeep-depth-5<br>\nv69 - 2.024-1.820-2811-0.84(88|79)<br>\nv70 - GoDeep-depth-4<br>\nv70 - 1.828-1.827-2804-0.84(89|79)<br>\nv71 - GoDeep-depth-3<br>\nv71 - 2.138-1.876-2911-0.83(0.89|78)<br>\nv72 - GoDeep-depth-2<br>\nv72 - 1.806-1.842-2696-0.85(89|91)<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Download and install required libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -c \"import torch; print(torch.__version__)\"\n!python -c \"import torch; print(torch.version.cuda)\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### For CPU ###\n\n!pip -qq install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n! pip -qq install torch-scatter==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-sparse==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-cluster==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-spline-conv==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-geometric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### For GPU ###\n\n# ! pip -qq install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n# ! pip -qq install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n# ! pip -qq install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n# ! pip -qq install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n# ! pip -qq install torch-geometric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union, Tuple\nfrom torch_geometric.typing import OptPairTensor, Adj, Size\n\nfrom torch import Tensor\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.nn.conv import MessagePassing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport random\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.nn as gnn\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.data import Dataset, Data, DataLoader\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import softmax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/cmsnewsamples/new-smaples.csv').drop(columns = 'Unnamed: 0')\ndf = df.drop(columns = [i for i in df.columns if '_1' in i])\ndf['non_hits'] = df[[i for i in df.columns if 'mask' in i]].sum(axis=1)\ndf = df[df['non_hits']==0].reset_index(drop=True)\n\ndf['1/pT'] = df['q/pt'].abs()\ndef label(a):\n    if a<=10:\n        return 0\n    if a>10 and a<=30:\n        return 1\n    if a>30 and a<=100:\n        return 2\n    if a>100:\n        return 3\n\ndf['pT'] = 1/df['1/pT']\n    \ndf['pT_classes'] = df['pT'].apply(label)\n\nfeatures = ['emtf_phi_'+str(i) for i in [0,2,3,4]] + ['emtf_theta_'+str(i) for i in [0,2,3,4]] + ['fr_'+str(i) for i in [0,2,3,4]] + ['old_emtf_phi_'+str(i) for i in [0,2,3,4]]\nlabels_1 = ['pT']\nlabels_2 = ['pT_classes']\nlabels_3 = ['vx']\n\nscaler_1 = StandardScaler()\ndf[features] = scaler_1.fit_transform(df[features])\n\nscaler_3 = MinMaxScaler()\ndf[labels_3] = scaler_3.fit_transform(df[labels_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shuffled_list = list(range(len(df)))\nrandom.Random(242).shuffle(shuffled_list)\nshuffled_list = np.array_split(np.array(shuffled_list), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edge_index = torch.tensor([(0,1),(1,2),(2,3),(3,2),(2,1),(1,0)], dtype=torch.long).T\nX_data = df[features].to_numpy()\nY_data = df[labels_1].to_numpy()\ndef process_data(i):\n  graph = X_data[i].reshape(-1,4).T\n  y = Y_data[i]\n  data = Data(x=torch.tensor(graph, dtype=torch.float), y=torch.tensor(y, dtype=torch.float), edge_index=edge_index)\n  return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PyTorch Geometric Dataset Class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TriggerDataset(Dataset):\n    def __init__(self, root, indexes=list(range(len(df))), transform=None, pre_transform=None):\n        super(TriggerDataset, self).__init__(root, transform, pre_transform)\n        self.indexes = indexes\n        self.length = len(self.indexes)\n\n    @property\n    def raw_file_names(self):\n        return ['vgc']\n\n    @property\n    def processed_file_names(self):\n        return ['vghv']\n\n    def download(self):\n        return None\n\n    def process(self):\n        return None\n\n    def len(self):\n        return self.length\n\n    def get(self, idx):\n        return process_data(self.indexes[idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom MPL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MPL(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super(MPL, self).__init__(aggr='add')\n        self.mlp1 = torch.nn.Linear(in_channels*2, out_channels)\n        self.mlp2 = torch.nn.Linear(in_channels, out_channels)\n        self.mlp3 = torch.nn.Linear(2*out_channels, 1)\n        self.mlp4 = torch.nn.Linear(2*out_channels, 1)\n        self.mlp5 = torch.nn.Linear(in_channels,16)\n        self.mlp6 = torch.nn.Linear(out_channels,16)\n        self.mlp7 = torch.nn.Linear(16,1)\n\n    def forward(self, x, edge_index):\n\n        msg = self.propagate(edge_index, x=x)\n        x = F.relu(self.mlp2(x))\n        w1 = F.sigmoid(self.mlp3(torch.cat([x,msg], dim=1)))\n        w2 = F.sigmoid(self.mlp4(torch.cat([x,msg], dim=1)))\n        out = w1*msg + w2*x\n        \n        return out\n\n    def message(self, x_i, x_j, edge_index):\n        msg = F.relu(self.mlp1(torch.cat([x_i, x_j-x_i], dim=1)))\n        w1 = F.tanh(self.mlp5(x_i))\n        w2 = F.tanh(self.mlp6(msg))\n        w = self.mlp7(w1*w2)\n        w = softmax(w, edge_index[0])\n        return msg*w","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GNN Architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MPNN(torch.nn.Module):\n    def __init__(self):\n      super(MPNN, self).__init__()\n      self.conv1 = MPL(4,160 )\n      self.conv2 = MPL(160,32)\n      self.conv3 = MPL(32,64 )\n      self.conv4 = MPL(64,32 )\n      self.lin1 = torch.nn.Linear(32*2, 128)\n      self.lin2 = torch.nn.Linear(128, 16)\n      self.lin3 = torch.nn.Linear(16, 16)\n      self.lin4 = torch.nn.Linear(16, 1)\n      self.global_att_pool1 = gnn.GlobalAttention(torch.nn.Sequential(torch.nn.Linear(32, 1)))\n      self.global_att_pool2 = gnn.GlobalAttention(torch.nn.Sequential(torch.nn.Linear(32, 1)))\n\n    def forward(self, data):\n      x, edge_index, batch = data.x, data.edge_index, data.batch\n      x = F.relu(self.conv1(x, edge_index))\n      x = F.relu(self.conv2(x, edge_index))\n      x1 = self.global_att_pool1(x, batch)\n      x = F.relu(self.conv3(x, edge_index))\n      x = F.relu(self.conv4(x, edge_index))\n      x2 = self.global_att_pool2(x, batch)\n      x = torch.cat([x1, x2], dim=1)\n      x = F.relu(self.lin1(x))\n      x = F.relu(self.lin2(x))\n      x = self.lin3(x)\n      x = self.lin4(x).squeeze(1)\n\n      return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\nepochs = 50\nprogress_bar=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mse(outputs, labels):\n    weights = torch.tensor(labels<80, dtype=torch.float).to(device)*labels + torch.tensor(labels>=80, dtype=torch.float).to(device)*torch.tensor(labels<160, dtype=torch.float).to(device)*labels*2.4 + torch.tensor(labels>=160, dtype=torch.float).to(device)*10\n    error = weights*(((outputs-labels)/labels)**2)\n    return torch.mean(error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef train_fn(val_batch=1, test_batch=2):\n\n    train_loader = DataLoader(TriggerDataset('./',np.concatenate([shuffled_list[j] for j in range(10) if j not in (val_batch, test_batch)])), batch_size=batch_size, shuffle=True, num_workers = 4) \n    val_loader = DataLoader(TriggerDataset('./',shuffled_list[val_batch]), batch_size=batch_size) \n    test_loader = DataLoader(TriggerDataset('./',shuffled_list[test_batch]), batch_size=batch_size)\n\n    model = MPNN().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=1, factor=0.5)\n    \n    m_train_loss = []\n    m_val_loss = []\n    m_test_loss = []\n    min_val_loss = float('inf')\n    for epoch in range(epochs):\n      train_loss = 0\n      val_loss = 0\n      if progress_bar:\n          pbar = tqdm(train_loader)\n      else:\n          pbar = train_loader\n      for data in pbar:\n        data = data.to(device)\n        optimizer.zero_grad()\n        outputs = model(data)\n        labels = data.y\n        loss = mse(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        if progress_bar:\n          pbar.set_description('MSELoss: '+str(loss.cpu().detach().numpy()))\n        train_loss += loss.cpu().detach()/len(train_loader)\n#         return 0\n\n      for data in val_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        outputs = model(data)\n        labels = data.y\n        loss = mse(outputs, labels)\n        val_loss += loss.cpu().detach()/len(val_loader)\n      if val_loss.detach().numpy()<min_val_loss:\n        min_val_loss = val_loss.cpu().detach().numpy()\n        torch.save(model.state_dict(), 'model.pth')\n      lr_scheduler.step(val_loss)\n      print('Epoch: ', str(epoch+1)+'/'+str(epochs),'| Training MSELoss: ', train_loss.numpy(), '| Validation MSELoss: ', val_loss.numpy())\n      m_train_loss.append(train_loss.numpy())\n      m_val_loss.append(val_loss.numpy())\n      if epoch>20 and min(m_val_loss[-7:])>min_val_loss+0.0001:\n        break\n    \n    if progress_bar==False:\n        plt.plot(range(1,len(m_val_loss)+1), m_val_loss, label='val_loss')\n        plt.plot(range(1,len(m_train_loss)+1), m_train_loss, label='train_loss')\n        plt.xlabel('epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.show()\n    model = MPNN().to(device)\n    model.load_state_dict(torch.load('model.pth'))\n    test_loss = 0\n    true = []\n    preds = []\n    for data in test_loader:\n      data = data.to(device)\n      optimizer.zero_grad()\n      outputs = model(data)\n      labels = data.y\n      true += list(labels.detach().numpy())\n      preds += list(outputs.detach().numpy())\n      loss = mse(outputs, labels)\n      test_loss += loss/len(test_loader)\n    print('Test MSELoss: ', test_loss.detach().numpy())\n    OOF_preds = pd.DataFrame()\n    OOF_preds['true_value'] = true\n    OOF_preds['preds'] = preds\n    OOF_preds['row'] = shuffled_list[test_batch]\n    OOF_preds.to_csv('OOF_preds_'+str(val_batch)+'.csv')\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Training fold 8 & 9\n\nfor i in range(8,10):\n    train_fn(val_batch=i, test_batch=(i+1)%10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = os.listdir('/kaggle/working')\ndf = pd.concat([pd.read_csv('/kaggle/working/'+i).drop(columns = ['Unnamed: 0']) for i in files if 'OOF_preds_' in i])\ndf.to_csv('OOF_preds.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf = df.sort_values(by = 'row').reset_index(drop = True)\ndf['True_pT'] = df['true_value']\ndf['Predicted_pT'] = df['preds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cnn = pd.read_csv('../input/cnn-pt-regression-multi1-new-data/OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf_cnn = df_cnn.sort_values(by = 'row').reset_index(drop = True)\ndf_cnn['True_pT'] = df_cnn['true_value']\ndf_cnn['Predicted_pT'] = df_cnn['preds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\ndef MAE(df):\n    MAE1 = []\n    dx = 0.5\n    for i in range(int(2/dx),int(150/dx)):\n        P = df[(df['True_pT']>=(i-1)*dx)&(df['True_pT']<=(i+1)*dx)]\n        try:\n            p = mae(P['True_pT'],P['Predicted_pT'])\n        except:\n            p=0\n        MAE1.append(p)\n    MAE1 = MAE1[:196]\n    return MAE1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dx = 0.5\nMAE1 = MAE(df)\nplt.plot([i*dx for i in range(4,200)],MAE1,label = 'GNN')\nplt.plot([i*dx for i in range(4,200)],MAE(df_cnn),label = 'CNN')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(MAE1[:196]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pT_classes(x):\n    if x>=25:\n        return 'Above 25 GeV'\n    else:\n        return 'Below 25 GeV'\n\nprint(classification_report(df['True_pT'].apply(pT_classes), df['Predicted_pT'].apply(pT_classes)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}