{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Answer Extraction Engine using BERT and TextRank for COVID-19\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/591611/1066108/__results___60_1239.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1586559849&Signature=lzUne8IfFlbvT4sizp4OE3FfqmMSt7f4ixhQXCcdbtKqZBvgxEtTk86%2F%2FT3laJhLyDzlLkOWXS7J2cGCJZK3eTpm7bVVduBcRRXETSwJ5PSUo34G6sdsp4km%2BMCIuBxsW2hc9nBHsuSZtyrIIzuNcpqgQk0JxUtn7NQ0khKViJJ3Liw7TDkTH2l2Z3oY5r2U0RK4AWUsfipAb1MGZlAP8NvwfjeAxMYKDZDozfjZ7rRFn2qBPR11AjqXMioscpjkWusulgMyDsOMgawH%2F6DPb1zCyRQ%2BvY1fFnPkKeOOw%2FcE2wHZnybrT4UoBkXdxv%2Bw93u%2Ftg1OFikDNahM%2F%2FY%2FhA%3D%3D)\n\nThis notebook explores using cdQA (closed domainn QA)  (with BERT inside) to find answer fragments, select relevant paragraphs, then use Textrank to summarize those paragraphs to provide some context. We also graph the commonalities between selected passages. The sytem is intended to be a resource for larger system. For exporation there is an interactive shell,where one can filter documents using regular expressions into relevant subsets (called contexts), then ask batches of of query questions relative to that context.\n\n\nIt also adds dudkduckgo links to directly pop up the documents if possible, using the title as the query key when DOI link is unavailable.\n\n[Closed Domain QA Description ](https://cdqa-suite.github.io/cdQA-website/)\n\n[Closed Domain QA on Github ](https://github.com/cdqa-suite/cdQA)\n\n[Summa Textrank tool on Github ](https://github.com/summanlp/textrank)\n\n[How to create your own Question-Answering system easily with python](https://towardsdatascience.com/how-to-create-your-own-question-answering-system-easily-with-python-2ef8abc8eb5)\n\nWarning: Old coder writing first public notebook, so a learning oppurtunity all around.\nAlso note that for some operations the system is right at 16 GB, so some processing is designed to get around that for testing. It's more about getting the entire data processing and display pipeline right (and share ideas), than making it fully happy in the notebook.\n\n***Please feel free to comment on any ideas for improvement.***"},{"metadata":{},"cell_type":"markdown","source":"# General Idea\n\ncdQA (closed domain Question Answerer) is a Python project that incorporates a BERT-based model trained for answer extraction. To operate cdQA requires a corpus of text presented in a Pandas dataframe, and from this dataframe it builds an in-memory text retrieval engine using either BM25 or TFIDF.\n\nOur goal is to get the metadata.csv file and the content of the JSON files of the COVID-19 corpus into that a Pandas dataframe with the following columns which cdQA knows how to interpret so we can ask it questions:\n\n| title             | paragraphs                                             |\n| ----------------- | ------------------------------------------------------ |\n| The Article Title | [Paragraph 1 of Article, ... , Paragraph N of Article] |\n\n.\n\nThis makes up the \"Pool of articles\" in the following image that describes cdQA.\n\n![Basics of the cdQA pipeline](https://miro.medium.com/max/1161/1*v7s0WvOj-Z-ZwVzWFuzR7Q.png)\n\n\nThe first half of the notebook is setting up the enviroment, setting some configuration and creating that dataframe from the two possible sources (metadata.csv and json files).\n\nNote: In order to save time and memory most of these functions are implemented in \"COVID-19 Corpus Pickle Factory\", which generates python pickle files, which this project can load based on the configuration.\n\nThe last half of the notebook is expermentation with running a set of queries against the database and returning the results in various formats including providing clickable links to the actual articles,and node graphs of the commonalities between selected articles.\n"},{"metadata":{},"cell_type":"markdown","source":"**Install the needed packages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install summa\n!pip install cdqa\n#!pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install summa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import and Header Section\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\nimport urllib\n\nimport pickle\nimport gc\nimport json\nimport re \nimport random\n\nimport os.path\nfrom os import path\n\n# load TextRank summarizer\nfrom summa.summarizer import summarize\nfrom summa.keywords import keywords\n\n# necessary for cdQA\nfrom ast import literal_eval\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\nfrom cdqa.utils.converters import generate_squad_examples\n\n#vaderSentiment\n#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsys.path.insert(0, \"../\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration\n\nA set of flags and parameters to control the rest of the sytem.\nNote: to control the execution of cell between developement or submission, I usually use \"doIt\" as switch for each individual cell.\n\n* corpus_frac - defines fraction of corpus to use, for faster testing\n* use_distilled - use DistilBERT or regular BERT for the Reader\n* use_prior -  load dataframe from a prior constructed csv\n* load_all - use both metadata and JSON files\n* just_meta - use just the metafile\n* save_load - write KB dataframe to disk\n* parse_all - should we parse all info from sources\n* exit_after_load - should we just quit after loading. (used if we are just generating files.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"global parse_all\n\nroot_path = '/kaggle/input/CORD-19-research-challenge'\ncorpus_frac = 0.1 #fraction of corpus to use\nuse_distilled = False # use BERT or DistilBERT \nmakeNew = False\n\nif (makeNew):\n    load_prior = False #\n    load_all = True\n    just_meta = True\n    save_load = True\n    parse_all = False\n    exit_after_load = True\nelse:\n    load_prior = True #\n    load_all = False\n    just_meta = False\n    save_load = False\n    parse_all = False\n    exit_after_load = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create dataframes\nOur gaol is to have all the data in \"corona_df\" and give that to cdQA.\nWe also have a dictionaries indexed by the lowercase titles, which holds either the DOI URL or a DuckDuckGo search URL for the title."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the files saved into a list and then iterate over them like below to extract relevant information\n# hold this information in a dataframe and then move forward from there. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global url_link\nglobal ans_df\nglobal current_task\nurl_link={}\n\n# Just set up a quick blank dataframe to hold all these medical papers. \ncorona_features = {\"doc_id\": [], \"source\": [], \"title\": [],\n                  \"abstract\": [], \"text_body\": []}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\n#links indexed by title\nlinkdict={}\n\n#our output frame\nans_features = {\"query\": [], \"answers\": [], \"keys\": [],\"summary\": [],\"context\":[],\"task\":[]}\nans_df = pd.DataFrame.from_dict(ans_features)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the JSON file names and subsample for developement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cool so dataframe now set up, lets grab all the json file names. \n\n# For this we can use the very handy glob library\n\njson_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\njson_filenames = random.sample(json_filenames,int( len(json_filenames)*corpus_frac))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stop Words and StopSet**\n\nBasically it helps to have a list of common words to eliminate from analysis. Below are two such lists. The first derived from nltk..corpus stopwords and the other from the union of various stop lists found online."},{"metadata":{"trusted":true},"cell_type":"code","source":"#see https://gist.github.com/sebleier/554280  for stoplists\nstoplist1=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\nstoplist2=[\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]\nstopSet1 = set(stoplist1)\nstopSet2 = set(stoplist2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Functions\nThese are the functions used to clean the datase and load the data from the metadata.csv or the JSON directory. So collect more info than they need but that is because we are also collecting datasource parsing methods for use in later projects.\nWe optionally can add additional fragments to the set of paragraphs by running TextRank over text segements that are too large, with the hope of adding percision."},{"metadata":{"trusted":true},"cell_type":"code","source":"#support functions\ndef clean_dataset(text):\n    if (not (isinstance(text, str)) ): return text\n    text=re.sub(\"[\\[].*?[\\]]\", \"\", text)#remove in-text citation\n    text=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',text, flags=re.MULTILINE)#remove hyperlink\n    text=re.sub(r'^a1111111111 a1111111111 a1111111111 a1111111111 a1111111111.*[\\r\\n]*',' ',text)#have no idea what is a11111.. is, but I remove it now\n    text=re.sub(' +', ' ',text ) #remove extra space, but was not able to remove all, see examples in the following cells\n    text=re.sub(r's/ ( *)/\\1/g','',text)\n    \n    return text\n\n\n# Now we just iterate over the files and populate the data frame. \ndef return_corona_df(json_filenames, df, source,linkdict):\n    lim=100000\n    cnt=0\n    global url_link\n    global parse_all\n    \n    for file_name in json_filenames:\n        cnt+=1\n        if (cnt>lim):break\n        if ((cnt % 1000) ==0):\n            print (\"Load JSON {}\".format(cnt))\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\"authors\": None,\n              \"abstract\": None, \"text_body\": None, \"paragraphs\":[],\"bibliography\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n            \n            lowTitle = row['title'].lower()\n            linkdict[lowTitle]=\"0000\"\n            \n            authors = \", \".join([author['first'] + \" \" + author['last'] \\\n                                 for author in data['metadata']['authors'] if data['metadata']['authors']])\n            row['authors'] = authors\n            bibliography = \"\\n \".join([bib['title'] + \",\" + bib['venue'] + \",\" + str(bib['year']) \\\n                                      for bib in data['bib_entries'].values()])\n            row['bibliography'] = bibliography\n            \n            #find any DOI enties\n            for bib in data['bib_entries'].values():\n                bib_title_low=bib['title'].lower()\n               # bib_data[lowTitle] = bib\n                if ('other_ids' in bib):\n                    ids = bib['other_ids']\n                    if('DOI' in ids):\n                        dois = ids['DOI']\n                        for doi in dois:\n                            linkdict[bib_title_low]=doi\n                            #print (\"{} -> {}\".format(lowTitle,doi))\n                    \n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [data['abstract'][x]['text'] for x in range(len(data['abstract']) - 1)]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. For some reason I am getting an index error\n            # In one of the Json files, so rather than have it wrapped in a lovely list\n            # comprehension I've had to use a for loop like a neanderthal. \n            \n            # Needless to say this bug will be revisited and conquered. \n            row['paragraphs']=abstract_list\n            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                    row['paragraphs'].append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Augment the paragraphs with Textrank summaries\n            extra_list=[]\n            summary_threshold=2048\n            #if (len(body)>summary_threshold):\n            #    extra_list.append(\"TR1: \" + summarize(body, ratio=0.1))\n            #    extra_list.append(\"TR2: \" + summarize(body, ratio=0.3))\n            if (len(abstract)>summary_threshold):                \n                extra_list.append(\"TR3: \" + summarize(abstract, ratio=0.3))\n            for subtext in row['paragraphs']:\n                if (len(subtext)>summary_threshold):\n                    extra_list.append(\"TR4: \" + summarize(subtext, ratio=0.3))\n            for subtext in extra_list:\n                row['paragraphs'].append(subtext)\n                \n       \n            #define links\n            searchTitle = row['title']\n            searchTitle = re.sub(r'\\W+',' ', searchTitle)\n            if (len(searchTitle)>160):\n                p =searchTitle.find(' ',128)\n                if (p>0):\n                    searchTitle = searchTitle[0:p]\n            qdict={'q': \"!ducky filetype:pdf \"+searchTitle}\n            if (len(body_list)==0):\n                #not body text -> assume no free pdf on web\n                qdict={'q': \"!ducky \"+searchTitle}\n            url_link[lowTitle]=\"https://duckduckgo.com/?\"+urllib.parse.urlencode(qdict)\n\n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"biorxiv_medrxiv\"\n            elif source == \"c\":\n                row['source'] = \"common_use_sub\"\n            elif source == \"n\":\n                row['source'] = \"non_common_use\"\n            elif source == \"p\":\n                row['source'] = \"pmc_custom_license\"\n                \n            if (not(parse_all)):\n                del row['source']\n                del row['authors']\n                del row['abstract']\n                del row['text_body']\n                del row['bibliography']\n\n            df = df.append(row, ignore_index=True)\n            \n    return df\n    \ndef return_append_metadata_df(df,linkdict):\n    global url_link\n    global parse_all\n    # load the meta data from the CSV file using 3 columns (abstract, title, authors),\n    meta_df=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', usecols=['title','abstract','authors','doi','full_text_file'])\n    #drop duplicates\n    meta_df=meta_df.drop_duplicates()\n    #drop NANs \n    meta_df=meta_df.dropna()\n    # convert abstracts to lowercase\n    #df[\"abstract\"] = df[\"abstract\"].str.lower()   \n    lim=100000\n    cnt=0\n    for index, row in meta_df.iterrows():\n        cnt+=1\n        if (cnt>lim):break\n        if ((cnt % 1000) ==0):\n            print (\"Load Metadata {}\".format(cnt))\n        \n        new_row = {\"doc_id\": None, \"source\": None, \"title\": None,\"authors\": None,\n              \"abstract\": None, \"text_body\": None, \"paragraphs\":[],\"bibliography\": None}\n        new_row['paragraphs'].append( row['abstract'])\n        new_row['title'] = row['title']\n        new_row['authors']=row['authors']\n        new_row['abstract']=row['abstract']\n        new_row['text_body']=row['abstract']\n        new_row['doc_id']=row['doi']\n        new_row['source']=row['full_text_file']\n        \n        if (not(parse_all)):\n                del new_row['source']\n                del new_row['authors']\n                del new_row['abstract']\n                del new_row['text_body']\n                del new_row['bibliography'] \n                \n        linkdict[row['title'].lower()]=row['doi']\n        url_link[row['title'].lower()]='https://doi.org/'+row['doi']\n        df = df.append(new_row,ignore_index=True)\n\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Additional methods**\n\n* filterDF which returns a cdQA dataframe with paragraphs mathcinng the regex\n* predict2 which does what the regular predict does but allows Retriever and Reader queries to be specified seperately\n\nBecause of processing issues when cdQA builds its in memory database/KB we have filterDF also implement memory limits, since two corpus dataframes + DB will exist in memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"def filterDF(source_df,regex):\n    # save the processing and space\n    #if (regex == \"(.*)\"):\n    #    return source_df\n    start_mem = source_df.memory_usage().sum() / 1024**2\n    print('Memory usage of source_df is {:.2f} MB'.format(start_mem))\n    \n    filtered_features = { \"title\":[],\"paragraphs\":[]}\n    filtered_df = pd.DataFrame.from_dict(filtered_features)\n    memcount =0\n    mb = 1024*1024\n    memlimit = 128 * mb\n    for index, row in source_df.iterrows():\n        keep=False\n        plist=[]\n        for p in row['paragraphs']:\n            \n            valid = re.search(regex,p)\n            keep = keep or valid\n            if (valid):\n                plen = len(p)\n                if (memcount+plen < memlimit):\n                    plist.append(p)\n                    memcount += plen\n                    \n        if (keep):\n            filtered_row = {}\n            filtered_row['title']=row['title']\n            filtered_row['paragraphs']=plist #row['paragraphs']\n            filtered_df = filtered_df.append(filtered_row,ignore_index=True )\n    \n    print(\"memcount = {}\".format(memcount))\n    end_mem = filtered_df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return filtered_df\n    \ndef predict2(\n        cdqa_pipeline,\n        queryPre: str = None,\n        query: str = None,\n        n_predictions: int = None,\n        retriever_score_weight: float = 0.35,\n        return_all_preds: bool = False,\n    ):\n\n    best_idx_scores = cdqa_pipeline.retriever.predict(queryPre)\n\n    squad_examples = generate_squad_examples(\n        question=query,\n        best_idx_scores=best_idx_scores,\n        metadata=cdqa_pipeline.metadata,\n        retrieve_by_doc=cdqa_pipeline.retrieve_by_doc,\n        )\n    examples, features = cdqa_pipeline.processor_predict.fit_transform(X=squad_examples)\n    prediction = cdqa_pipeline.reader.predict(\n        X=(examples, features),\n        n_predictions=n_predictions,\n        retriever_score_weight=retriever_score_weight,\n        return_all_preds=return_all_preds,\n        )\n    return prediction\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Network Display**\n\nTextRank is an extractive summarizaiton approach. It attempts to select the N most represenative sententences or words that best contains the information of the original text. It does so by building a graph of the words and sentences and applying PageRank to provide a centrality score for each node. This centrality score approximates the amount of attention that would be given to each node. This is interesting given the current attention being given to transformer based system which also use the attention concept in their operation.\n\nThe code below displays the connction graph between sentences being summarized and the words they share in common.\n\nMihalcea, R., Tarau, P.: “Textrank: Bringing order into texts”. In: Lin, D., Wu, D. (eds.) Proceedings of EMNLP 2004. pp. 404–411. Association for Computational Linguistics, Barcelona, Spain. July 2004."},{"metadata":{"trusted":true},"cell_type":"code","source":"from summa import summarizer,keywords\nfrom summa.summarizer import summarize\n\nimport networkx as nx\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport re\nimport textwrap \ndef word_wrap(value,n):\n   # wrapper = textwrap.TextWrapper(width=n) \n    wrapped = textwrap.wrap(text=value,width=n) \n    return wrapped\n\ndef showNetwork(text):\n    nx_graph = nx.DiGraph()\n    w_nodes=set()\n\n    #print(keywords.keywords(text,scores=True))\n    keyScore={}\n    sentScore={}\n    linked=set()\n    keyPhraseSet = set()\n    #labels={}\n    toks = text.split(' ')\n    tlen = int( len(toks) * 0.5 )\n    \n    # get a set of keywords\n    ks = keywords.keywords(text,scores=True,ratio=0.8)\n    for (key,score) in ks:\n        keyScore[key]=score\n        w_nodes.add(key)\n        #nx_graph.add_node(key, name=key)\n        #labels[key]=key\n\n    #nx_graph.add_node(\"root\", name=\"root\")\n    #nx_graph.add_node(\"term\", name=\"term\")\n\n    # Get a sentence summary\n    ss = summarize(text,words=128,scores=True,split=True)\n    \n    #count how many times a keyword appears in each summary sentence\n    keyCount={}\n    for (key,score) in ks:\n        keyCount[key]=0\n        # ignore stopwords\n        if (key in stopSet1):\n            continue\n        #count appearences in each senntence\n        for (skey,score) in ss:\n            search_key = key.lower().replace(\" \",\".*\")\n            if (re.search(search_key,skey.lower()) ):\n                keyCount[key]+=1\n    \n    # find the set of words            \n    tokSet =set()            \n    for (skey,sscore) in ss:\n        tkey=re.sub('[\\W+]+',' ',skey)\n        toks = tkey.split(' ')\n        for t in toks:\n            # ignore stopwords\n            if (t in stopSet1):\n                continue\n            #generate new entries\n            if (len(t)>3) and (not(t in keyCount)):\n                keyCount[t]=0\n                tokSet.add(t)\n                \n    #count how many times a raw toke appears in the summary sentences\n    for key in tokSet:\n        for (skey,score) in ss:\n            search_key = key.lower().replace(\" \",\".*\")\n            if (re.search(search_key,skey.lower()) ):\n                keyCount[key]+=1\n        \n    #build a graph of the sentences and the keywords/tokens\n    # that occur in two or more sentences\n    for (skey,sscore) in ss:\n        sentScore[skey]=sscore\n        w_nodes.add(skey)\n        nx_graph.add_node(skey, name=skey)\n        #nx_graph.add_edge(skey, \"root\", name=\"\") \n        #labels[skey]=skey\n        \n        #keyphrase to sentences links\n        for (key,kscore) in ks:\n            if (keyCount[key]>1) and (len(key)>2):\n                keyPhraseSet.add(key)\n                search_key = key.lower().replace(\" \",\".*\")\n                if (re.search(search_key,skey.lower()) ) :\n                    nx_graph.add_node(key, \n                                      name=\"{0}\".format(key,kscore)\n                                     , node_color='#00b4d9')\n                    nx_graph.add_edge(key, skey, name=\"\",weight=1)\n                    linked.add(key)\n                    \n        #tokens to sentence links\n        for key in tokSet:\n            if (keyCount[key]>1) and (len(key)>2):\n                search_key = key.lower().replace(\" \",\".*\")\n                if (re.search(search_key,skey.lower()) ) :\n                    nx_graph.add_node(key, name=\"{0}\".format(key,kscore))\n                    nx_graph.add_edge(key, skey, name=\"\",weight=0.1)\n                    linked.add(key)\n    \n    #tokens to keyphrases\n    for (key,kscore) in ks:\n        for tkey in tokSet:\n            if ( (tkey.lower() in key.lower()) \n                 and (tkey in linked) and (key in linked) ):\n                    nx_graph.add_edge(tkey, key, name=\"\",weight=0.9)\n\n    #print(summarize(text, words=50))\n    kgp = keywords.get_graph(text)\n    sgp = summarizer.get_graph(text)\n\n    kn_score ={}\n    for (s_name,o_name) in kgp.edges():\n        kn_score[s_name]=0\n\n    for (s_name,o_name) in kgp.edges():\n        for (key,score) in ks:\n            if (score> kn_score[s_name]) and (s_name in key): \n                kn_score[s_name]=score\n   #add colors\n    ncolors = []\n    for node in nx_graph:\n        if node in keyPhraseSet:\n            ncolors.append(\"red\")\n        else:\n            if node in tokSet:\n                ncolors.append(\"lightgreen\")\n            else:\n                ncolors.append(\"lightblue\")\n    ecolors = []\n    for u,v,d in nx_graph.edges(data=True):   \n        if u in keyPhraseSet:\n            ecolors.append(\"red\")\n        else:\n            if u in tokSet:\n                ecolors.append(\"lightgreen\")\n            else:\n                ecolors.append(\"lightblue\")\n   #plot the graph\n    fig = plt.figure(figsize=(20, 20))\n    #fig = plt.figure(figsize=(20, 20))\n    #ax = fig.add_subplot(111)\n   # select a style\n    #_pos = nx.kamada_kawai_layout(nx_graph)\n    _pos = nx.spring_layout(nx_graph\n                            ,k = 0.6\n                            ,iterations = 100\n                            ,threshold = 0.005)\n\n   #draw the raw nodes and edges\n    _ = nx.draw_networkx_nodes(nx_graph, pos=_pos,\n                               node_color=ncolors,alpha=0.5)\n    _ = nx.draw_networkx_edges(nx_graph, pos=_pos,\n                               edge_color=ecolors,alpha=0.6)\n    \n    #generate new word wrapped labels for the nodes\n    lablesN={}\n    names = nx.get_node_attributes(nx_graph, 'name')\n    for n in names:\n        p = _pos[n]\n        t=names[n]\n        lablesN[n]=\"\\n\".join(word_wrap(t,32))\n        _pos[n] = p\n    y=0\n    for (k,p) in lablesN.items():\n        if (not(k in _pos)):\n            y+=0.1\n            _pos[k]=(0,y)\n            #print (\"lab:{} -> {}\".format(k,_pos[k]))\n            \n    #draw the lables of the nodes and edges\n   # _ = nx.draw_networkx_labels(nx_graph, \n   #                             pos=_pos, \n   #                             fontsize=9,\n   #                             font_color =\"white\",\n   #                             labels=lablesN)\n    \n    #draw node labels one at a time for individual color\n    for node in nx_graph:\n        if node in keyPhraseSet:\n            nx.draw_networkx_labels(nx_graph, \n                                pos=_pos, \n                                font_size=12,\n                                font_color =\"cyan\",\n                                font_weight=\"black\",\n                                labels={node:lablesN[node]})\n        else:\n            if node in tokSet:\n                nx.draw_networkx_labels(nx_graph, \n                    pos=_pos, \n                    font_size=9,\n                    font_color =\"lightgreen\",\n                    alpha = 0.8,\n                    labels={node:lablesN[node]})\n            else:\n                nx.draw_networkx_labels(nx_graph, \n                    pos=_pos, \n                    font_size=12,\n                    font_color =\"white\",\n                    labels={node:lablesN[node]})\n    \n    #_ = nx.draw_networkx_labels(nx_graph, pos=_pos, fontsize=8)\n\n    names = nx.get_edge_attributes(nx_graph, 'name')\n\n    _ = nx.draw_networkx_edge_labels(nx_graph,\n                                     pos=_pos, \n                                     edge_labels=names, \n                                     font_color =\"cyan\",\n                                     fontsize=8)\n    #fig.set_facecolor(\"#00000F\")\n    plt.gca().set_facecolor(\"#00000F\")\n    # SHOW THE PLOT\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"\"\"\n\n Multivariable regression analysis showed that elevated high sensitivity troponin I (OR 2.68, 95%CI 1.31-5.49, P=0.007), neutrophils (OR 1.14, 95%CI 1.01-1.28, P=0.033) and depressed oxygen saturation (OR 0.94, 95%CI 0.89-0.99, P=0.027) on admission were associated with rapid death of patients with COVID-19.\n Elevated high sensitivity troponin, neutrophils and depressed oxygen saturation predicted the rapid death of patients. \n Compared with patients without pneumonia, those with pneumonia were 15 years older and had a higher rate of hypertension, higher frequencies of having a fever and cough, and higher levels of interleukin-6 (14.61 vs. 8.06pg/mL, P=0.040), B lymphocyte proportion (13.0% vs.10.0%, P=0.024), low account (<190/μL) of CD8+ T cells (33.3% vs. 0, P=0.019). \n For example, a large observational report 2 including 1099 patients with confirmed COVID-19 infection indicated that in 173 with severe disease there existed the comorbidities of hypertension (23·7%), diabetes mellitus (16·2%), coronary heart diseases (5·8%), and cerebrovascular disease (2·3%). \n  Even though COVID-19 is highly contagious , control measures have proven to be very effective. \n Meanwhile, numbers of patients with COVID-19 infection had chronic comorbidities , mainly hypertension, diabetes and cardiovascular disease, which is similar to MERS-COV population. \n \n \n \"\"\"\nshowNetwork(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load From Sources and Optionally Save\nNote: need to learn more about storing and retrieving the paragraphs series as a csv since I have trouble getting it back. So for now we just load the corpus as needed for each run."},{"metadata":{"trusted":true},"cell_type":"code","source":"if (just_meta):\n    corona_df = return_append_metadata_df( corona_df,linkdict)\n    if (load_all):\n        corona_df = return_corona_df(json_filenames, corona_df, 'b',linkdict)\nelse:\n    if (load_prior):\n        #load prior CSV \n        #corona_df= pd.read_csv('/kaggle/working/kaggle_covid-19_open_csv_format.csv')\n        #load prior pkl\n        if path.exists('/kaggle/input/covid-19-corpus-pickle-factory/kaggle_covid-19_pickle.pkl'):\n            print(\"Loading pickled KB from /kaggle/input/covid-19-corpus-pickle-factory\")\n            corona_df= pd.read_pickle('/kaggle/input/covid-19-corpus-pickle-factory/kaggle_covid-19_pickle.pkl')\n            #restore the url_link info dictionary\n            with open('/kaggle/input/covid-19-corpus-pickle-factory/url_links.pkl','rb') as handle:\n                url_link = pickle.load(handle)\n        else:\n            print(\"Loading pickled KB from /kaggle/working\")\n            corona_df= pd.read_pickle('/kaggle/working/kaggle_covid-19_pickle.pkl')\n            #restore the url_link info dictionary\n            with open('/kaggle/working/url_links.pkl','rb') as handle:\n                url_link = pickle.load(handle)\n    else:\n        #generate\n        corona_df = return_corona_df(json_filenames, corona_df, 'b',linkdict)\n\nif (save_load): \n        # save\n        corona_out = corona_df.to_csv('kaggle_covid-19_open_csv_format.csv')\n        corona_pkl = corona_df.to_pickle('kaggle_covid-19_pickle.pkl')\n         # Store (serialize) the url_link dictionary\n        with open('url_links.pkl', 'wb') as handle:\n            pickle.dump(url_link, handle, protocol=pickle.HIGHEST_PROTOCOL) \n            \n#if (exit_after_load):\n#    exit(keep_kernel=True)\n    #raise SystemExit(\"Exit after load\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(url_link)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#url_link\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup KB\nWe need to define the version of BERT to use as the Reader, do any final cleaning, setup the pipeline, then index the KB."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cdqa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (use_distilled):\n    # Downloading pre-trained DistilBERT fine-tuned on SQuAD 1.1\n    download_model('distilbert-squad_1.1', dir='./models')\nelse:\n    #download the model\n    download_model(model='bert-squad_1.1', dir='./models')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subsample full corpus to fit in memory\n#corona_df=corona_df.sample(frac = corpus_frac)\ncorona_df.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning**\n\ncdQA is only interested in title and paragraphs so we shrink the table down to that,and get rid of any entries that would make cdQA unhappy."},{"metadata":{"trusted":true},"cell_type":"code","source":"#find duplicate values. (remove also files that do not have a title)\n\ntry:\n    dfdrop= corona_df[corona_df['title'].duplicated() == True]\n    #dfdrop.head()\n    corona_df= corona_df.drop(dfdrop.index)\n    dfdrop2 = corona_df[corona_df.astype(str)['paragraphs'] == '[]']\n    corona_df= corona_df.drop(dfdrop2.index)\nexcept:\n    gc.collect()\n\n    \n#corona_df['text_body'] =corona_df['text_body'].apply(clean_dataset)\n#corona_df['title']     =corona_df['title'].apply(clean_dataset)\n#corona_df['abstract']  =corona_df['abstract'].apply(clean_dataset)\n#corona_df=corona_df[['doc_id','title','abstract','text_body','paragraphs']]\ncorona_df=corona_df[['doc_id','title','paragraphs']]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.mask(corona_df.eq('None')).dropna()\ncorona_df = corona_df.replace(to_replace='None', value=np.nan).dropna()\ncorona_df = corona_df.reset_index(drop=True)\n\n\ncorona_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the data into df\ncorona_df = filter_paragraphs(corona_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create the pipline**\n* Defaults to retriever=\"bm25\". Other option is  retriever=\"tfidf\" \n* Docs claim defaults to retrieve_by_doc = True but code appears to be set to False. Controls if Retriever will rank by documents or paragraphs.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"if (use_distilled):\n    # Loading QAPipeline with CPU version of DistilBERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models/distilbert_qa.joblib')\nelse:\n    # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models/bert_qa.joblib')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Index the KB**\n\nFit the QAPipeline retriever to a list of documents in a dataframe.\nAccepts parameter df, a pandas.Dataframe with the following columns: \"title\", \"paragraphs\""},{"metadata":{"trusted":true},"cell_type":"code","source":"cdqa_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the retriever to the list of documents in the dataframe\nsub_kb = corona_df.sample(frac=corpus_frac)\ntry:\n    #cdqa_pipeline.fit_retriever(df=corona_df)\n    cdqa_pipeline.fit_retriever(df=sub_kb)\nexcept:\n    print(\"Unexpected error:\", sys.exc_info()[0])    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Asking Questions**\n\nThis is where we run queries against the KB we made.\nOne area to explore is creating and updating the set of queries. \nThe format I have selected is \"context keywords|NL Query\".\nThe whole Query (conext + NL Query) is passed to the Retriever, while only the NL Query is passed to the Reader.\nThe context keywords are optional.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sending a question to the pipeline and getting prediction\nquery_list=[ 'What presents a emerging threat to global health?'\n              ,'When did COVID-19 appear?'\n              ,'Where did COVID-19 first appear?'\n              ,'What was the original source of COVID-19?'\n              ,'What was the original host of COVID-19?'\n\n              ,'What is an effective treatment for COVID-19?'\n              ,'What is the motality rate of COVID-19?'\n              , 'How does COVID-19 respond to the presence of copper?'\n              , 'How are ACE2 receptors affected?'\n              , 'What patients are most susceptible to COVID-19?'\n              , 'How is COVID-19 outbreak similar to the 1918 pandemic?'\n              , 'How many people will die globally of coronavirus?'\n              , 'How is COVID-19 similar to MERS or SARS?'\n              , 'How does COVID-19 differ from MERS and SARS?'\n              , 'What is known about coronavirus transmission, incubation, and environmental stability?'\n              , 'What do we know about natural history, transmission, and diagnostics for COVID-19?'\n              , 'What have we learned about coronavirus infection prevention and control?'\n              ,'What is the range of incubation periods for COVID-19 in humans?'\n              ,'What is the prevalence of asymptomatic shedding and transmission of coronavirus?'\n              ,'How does Seasonality affect transmission rate of coronavirus?'\n              ,'How long does the virus persist on surfaces of different materials like copper, stainless steel, plastic?'\n              ,'Coronavirus shedding from infected persons?'\n              ,'What is the effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings?'\n              ,'What is the effectiveness of social distancing in reducing coronavirus transmission rate?'\n              ,'What is the best method to detect COVID-19 in asymptomatic patients?'\n              ,'What are COVID-19 risk factors?'\n              ,'What populations are suceptible to coronavirus?'\n              ,'What is the risk of fatality from COVID-19 among symptomatic hospitalized patients?'\n              ,'What co-infections present special risk with COVID-19?'\n              ,'What is the incubation period of COVID-19?'\n              ,'What is the serial interval of COVID-19?'\n             ,'What viral inhibitors are being examined for coronavirus and COVID-19?'\n              ,'Are there any non pharmaceutical interventions for COVID-19?'\n              ,'What is the compilance rate with bans on mass gatherings?'\n              ,'What will the economic impant of the COVID-19 pandemic be?'\n              ,'How is AI being used to monitor and evaluate real-time health care delivery?'\n              ,'Would chloroquine phosphate effective be effective against coronavirus?'\n              ,'How does ritonavir act as an anti-viral?'\n              ,'How does chloroquine act as an anti-viral?'\n              ,'What is the most effective anti-viral against coronavirus COVID-19?'\n              ,'When will the COVID-19 pandemic end?'\n              ,'How long will the COVID-19 pandemic last?'\n              ,'What is the survival rate for COVID-19 infections?'\n              ,'What is the survival rate for COVID-19 for those over 65 years of age?'\n            ,'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups'\n            ,'Does smoking or pre-existing pulmonary disease increase risk of COVID-19?'\n            ,'Are neonates and pregnant women at greater risk of COVID-19?'\n            ,'What is the severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups?'\n            ,'Does rise in pollution increase risk of COVID-19?'\n            ,'Are there public health mitigation measures that could be effective for control of COVID-19?'\n              ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_list=[ \n            'SARS-CoV-2 covid coronavirus infected cases disease | What are common COVID-19 symptoms?'\n\n            ,'disease pandemic increasing resistance | What presents a emerging threat to global health?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | When did COVID-19 appear?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | Where did COVID-19 first appear?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | What was the original source of COVID-19?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | What was the original source of SARS-COV2?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | What was the original host of COVID-19?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | What animal did SARS-COV2 come from?'\n            ,'What is the effective reproductive number of COVID-19?'\n            ,'What is the basic reproductive number (r0) of COVID-19?'\n            ,'What is the incubation period in days for COVID-19? '\n            ,'What is the mean or average COVID-19 incubation period in days?'\n            ,'What is the range of COVID-19 incubation periods in days?'\n            ,'SARS-CoV-2 covid coronavirus drug therapy treatment study trials efficacy | What is an effective treatment for COVID-19?'\n            ,'SARS-CoV-2 death fatality | What is the motality rate of COVID-19?'\n            ,'SARS-CoV-2 death motality | What is the fatality rate of COVID-19?'\n            ,'SARS-CoV-2 covid coronavirus cell bind receptor infect membrane express cause ACE-2| What is the role of ACE2 receptors in COVID-19?'\n            ,'SARS-CoV-2 covid coronavirus case severe infection treatment | What patients are most susceptible to COVID-19?'\n            ,'SARS-CoV-2 coronavirus epidemic | How is COVID-19 outbreak similar to the 1918 pandemic?'\n            ,'SARS-CoV-2 compared suggest | How is COVID-19 similar to MERS or SARS?'\n            ,'SARS-CoV-2 covid coronavirus infect infected range patients | What is the range of incubation periods for COVID-19 in humans?'\n            ,'SARS-CoV-2 covid-19 viral infection excreted | What is the rate or frequency of asymptomatic shedding and transmission of coronavirus?'\n            ,'SARS-CoV-2 covid-19 coronavirus seasonal season weather summer winter spring fall hot warm cold peak time| How does Seasonality affect transmission rate of coronavirus?'\n            ,'SARS-CoV-2 covid-19 coronavirus survive surface food contamination infectious | How long does the coronavirus persist on surfaces of different materials like copper, stainless steel, plastic?'\n            ,'SARS-CoV-2 covid-19 coronavirus survive surface food contamination infectious plastic metal steel copper paper cardboard aluminum polystyrene glass | How long does the coronavirus persist on surfaces?'\n            ,'SARS-CoV-2 covid-19 coronavirus survive deactivate inactivation disinfection decontaminate nutralize surfaces sterilize| What is an effective disinfectant for coronavirus?'\n            ,'SARS-CoV-2 covid-19 coronavirus drug therapy | What is the most effective anti-viral against coronavirus COVID-19?'\n            ,'SARS-CoV-2 coronavirus smoke lungs respritory COPD| Does smoking or pre-existing pulmonary disease increase risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus smoke | Does smoking increase risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus lungs respritory COPD| Does pre-existing pulmonary disease increase risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus  hypertension | How does high-blood pressure affect COVID-19?'\n            ,'SARS-CoV-2 coronavirus | How does diabetes affect COVID-19?'\n            ,'SARS-CoV-2 coronavirus factors | What increases the risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus factors | What are common risk factors for COVID-19 patients?'\n            ,'SARS-CoV-2 babies coronavirus maternal | Are neonates and pregnant women at greater risk of COVID-19?'\n\n]\t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The query engine**\n\nFinally we can ask some questions. This is primary done using is cdqa_pipeline.predict(...) or predict2(cdqa_pipeline,....) either of which will return a list of predictions.\n\nA prediction is of the format: tuple (answer, title, paragraph, score/logit)\n\nOne can modify retriever_score_weight vs the reader score used for the final ranking.\n\nretriever_score_weight: float (default: 0.35).\n\nThe weight of retriever score in the final score is used for prediction. Given the retriever score and reader average of start and end logits, the final score used for ranking is:\n\n> final_score = retriever_score_weight \\* retriever_score + (1 - retriever_score_weight) \\* (reader_avg_logit)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install summa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doIt = False\nif (doIt):\n    #The query processor and collector\n    ans_dict={} \n    key_dict={}\n    sum_dict={}   \n    ans_entry={}\n    ans_lists={}\n    num_predictions=5\n    showIt =True\n    rel_threshold = 0.7\n\n    for query in query_list:\n      ans_dict[query]=[]\n      ans_entry[query]=[]\n      #predictions = cdqa_pipeline.predict(query,n_predictions=num_predictions,retriever_score_weight=0.35)\n      readQuery = query\n      context=\"\"\n      if ('|' in query):\n        queryp = query.split('|')\n        readQuery = queryp[1] # the NL query\n        context = queryp[0] #extra context keywords\n\n      predictions= predict2(cdqa_pipeline,queryPre=query,query=readQuery,\n                            n_predictions=num_predictions,\n                            retriever_score_weight=0.45)\n      ptext=\"\"\n      if (showIt):\n          print(\"--------------------\")\n\n      max_score =predictions[0][3]\n\n      ans_lists[query]=[]\n      for predict in predictions:\n        rel_score = predict[3]/max_score\n        if (rel_score < rel_threshold): continue\n\n        if (showIt):\n            print('context: {}'.format(query))\n            print('query: {}'.format(readQuery))\n            print('answer: {}'.format(predict[0]))\n            print('score: {}'.format(predict[3]))\n            print('rscore: {}\\n'.format(rel_score))\n            print('title: {}'.format(predict[1]))\n            print('paragraph: {}\\n'.format(predict[2]))\n        ans_dict[query].append(predict[0])\n        ptext += \"{}\\n\".format(predict[2])\n\n        #generate a clickable link\n        searchTitle = predict[1]\n        searchTitle = re.sub(r'\\W+',' ', searchTitle)\n        if (len(searchTitle)>160):\n            p =searchTitle.find(' ',128)\n            if (p>0):\n                searchTitle = searchTitle[0:p]\n        qdict={'q': \"!ducky filetype:pdf \"+searchTitle}\n        if (len(predict[2])==0):\n            qdict={'q': \"!ducky \"+searchTitle}\n        link=\"https://duckduckgo.com/?\"+urllib.parse.urlencode(qdict)\n        linkkey = predict[1].lower()\n        if (linkkey in url_link) and (len(url_link[linkkey])<160):\n            link = url_link[linkkey]\n\n\n        relevant_paragraph = predict[2]\n        relevant_paragraph = relevant_paragraph.replace(predict[0],'<strong style=\"color: red;\"><em> {} </em></strong>'.format(predict[0]))\n        qual = \"r={1:8.2} a={0:8.3}\".format(predict[3],rel_score)\n        ans=('<b>'+predict[0]+' ('+ qual+')</b> -  <a href=\"'+link+'\" target=\"_blank\"><i>'+predict[1]+'</i></a>')\n        ans += '<br> paragraph: {}\\n'.format(relevant_paragraph)\n        ans_entry[query].append(ans)\n        ans_lists[query].append(predict[0])\n\n\n      numChar=1024\n      target_ratio = numChar / len(ptext)\n      if (target_ratio > 0.5):\n        target_ration = 0.5\n\n      sum = summarize(ptext, ratio=target_ratio)\n      keyterms = keywords(ptext,ratio=0.1).replace(\"\\n\",\" , \")\n      key_dict[query]=keyterms\n      sum_dict[query]=sum\n      if (showIt):\n        print('keywords: {}\\n'.format(keyterms))\n        print('summary: {}\\n'.format(sum))\n\n      print(\"DONE\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining it all into a callable query engine that applies a regex matching process to filter out a relevant subset to run the query over. It also produces a HTML report and draws a commonality graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\nfrom IPython.core.display import display, HTML\n\ndef subSetEngine(subsetRegex, query_list,base_df,\n                 rel_threshold=0.7,\n                 include_paragraphs=True,\n                 focus_on_sentence=True):\n    global ans_df\n    \n    if (use_distilled):\n        # Loading QAPipeline with CPU version of DistilBERT Reader pretrained on SQuAD 1.1\n        cdqa_pipeline_KB = QAPipeline(reader='models/distilbert_qa.joblib')\n    else:\n        # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n        cdqa_pipeline_KB = QAPipeline(reader='models/bert_qa.joblib')\n\n    # Fitting the retriever to the list of documents in the dataframe\n    print(\"filtering df ...\")\n    sub_kb = filterDF(base_df,subsetRegex)\n    kbfrac = len(sub_kb)/len(base_df)\n    \n\n    if (len(sub_kb)==0): return\n    print(\"building KB ...\")\n    try:\n        #cdqa_pipeline.fit_retriever(df=corona_df)\n        cdqa_pipeline_KB.fit_retriever(df=sub_kb)\n    except:\n        print(\"Unexpected error:\", sys.exc_info()[0])  \n        return\n    \n    display(HTML(\"<H1> SubDB {0} = {1} entries ( {2:8.3}%)</H1>\"\n                 .format(subsetRegex,len(sub_kb),100*kbfrac)))    \n    #The query processor and collector\n    ans_dict={} \n    key_dict={}\n    sum_dict={}   \n    ans_entry={}\n    ans_lists={}\n    src_dict={}\n    ptext_dict={}\n    stext_dict={}\n    num_predictions=5\n    showIt =False\n    \n\n    for query in query_list:\n      ans_dict[query]=[]\n      ans_entry[query]=[]\n      #predictions = cdqa_pipeline_KB.predict(query,n_predictions=num_predictions,retriever_score_weight=0.35)\n      \n      readQuery = query\n      context=\"\"\n      if ('|' in query):\n        queryp = query.split('|')\n        readQuery = queryp[1] # the NL query\n        context = queryp[0] #extra context keywords\n\n      predictions= predict2(cdqa_pipeline_KB,queryPre=subsetRegex+\" \"+query,query=readQuery,\n                            n_predictions=num_predictions,\n                            retriever_score_weight=0.45)\n      ptext=\"\"\n      stext=\"\"\n      if (showIt):\n          print(\"--------------------\")\n\n      max_score =predictions[0][3]\n\n      ans_lists[query]=[]\n      for predict in predictions:\n        rel_score = predict[3]/max_score\n        if (rel_score < rel_threshold): continue\n\n        if (showIt):\n            print('context: {}'.format(query))\n            print('query: {}'.format(readQuery))\n            print('answer: {}'.format(predict[0]))\n            print('score: {}'.format(predict[3]))\n            print('rscore: {}\\n'.format(rel_score))\n            print('title: {}'.format(predict[1]))\n            print('paragraph: {}\\n'.format(predict[2]))\n        ans_dict[query].append(predict[0])\n        ptext += \"{}\\n\".format(predict[2])\n\n        #generate a clickable link\n        searchTitle = predict[1]\n        searchTitle = re.sub(r'\\W+',' ', searchTitle)\n        if (len(searchTitle)>160):\n            p =searchTitle.find(' ',128)\n            if (p>0):\n                searchTitle = searchTitle[0:p]\n        qdict={'q': \"!ducky filetype:pdf \"+searchTitle}\n        if (len(predict[2])==0):\n            qdict={'q': \"!ducky \"+searchTitle}\n        link=\"https://duckduckgo.com/?\"+urllib.parse.urlencode(qdict)\n        linkkey = predict[1].lower()\n        if (linkkey in url_link) and (len(url_link[linkkey])<160):\n            link = url_link[linkkey]\n\n\n        relevant_paragraph = predict[2]\n        relevant_paragraph = relevant_paragraph.replace(predict[0],'<strong style=\"color: red;\"><em> {} </em></strong>'.format(predict[0]))\n        qual = \"r={1:8.2} a={0:8.3}\".format(predict[3],rel_score)\n        ans=('<b>'+predict[0]+' ('+ qual+')</b> -  <a href=\"'+link+'\" target=\"_blank\"><i>'+predict[1]+'</i></a>')\n        if (include_paragraphs):\n            ans += '<br> paragraph: {}\\n'.format(relevant_paragraph)\n        if (focus_on_sentence):\n            rpar = predict[2]\n            sents =  re.split(r'(?<=[^A-Z].[.?!]) +(?=[A-Z])', rpar)\n            for sent in sents:\n                if (predict[0] in sent):\n                    stext += \"{}\\n\".format(sent)\n                    sent = sent.replace(predict[0],'<strong style=\"color: red;\"><em> {} </em></strong>'.format(predict[0]))\n                    ans += '<br> sentence: {}\\n'.format(sent)    \n                \n        ans_entry[query].append(ans)\n        ans_lists[query].append(predict[0])\n\n\n      numChar=512\n      target_ratio = numChar / len(ptext)\n      if (focus_on_sentence):\n          target_ratio = numChar / len(stext)  \n      if (target_ratio > 0.5):\n        target_ration = 0.5\n\n      if (focus_on_sentence):\n        #print(\"Summarizing sentences:{}\".format(stext))\n        sum = summarize(stext, ratio=target_ratio)\n      else:\n        sum = summarize(ptext, ratio=target_ratio)\n        \n      keyterms = keywords.keywords(ptext,ratio=0.1).replace(\"\\n\",\" , \")\n      key_dict[query]=keyterms\n      sum_dict[query]=sum\n      if (showIt):\n        print('keywords: {}\\n'.format(keyterms))\n        print('summary: {}\\n'.format(sum))\n        \n      ptext_dict[query] = ptext\n      stext_dict[query] = stext\n        \n      if (focus_on_sentence):\n        src_dict[query]=stext\n     #   showNetwork(stext)\n      else:\n        src_dict[query]=ptext\n     #   showNetwork(ptext)\n    \n    # Append to answer frame\n      row = {\n                \"task\":None,\n                \"context\":None,\n                \"query\": None,\n                \"answers\": None,\n                \"keys\": None,\n                \"summary\": None \n          }\n      row[\"context\"]=context\n      row[\"task\"]=current_task\n      row[\"query\"]=readQuery\n      row[\"answers\"] = \" , \".join(ans_dict[query])\n      row[\"keys\"]=key_dict[query]\n      row[\"summary\"]=sum_dict[query]\n\n      ans_df= ans_df.append(row, ignore_index=True)\n\n      print(\"DONE :{}\".format(query))\n    \n\n    #display(HTML('<table>'))\n    #display(HTML('<tr><th></th></tr>'))\n    for query in query_list:\n        \n        #display(HTML('<tr>'))\n        #display(HTML('<td>'))\n        showNetwork(ptext_dict[query])\n        #display(HTML('</td>'))\n        #display(HTML('</tr>'))\n        \n        #display(HTML('<tr>'))\n        #display(HTML('<td>'))\n        showNetwork(stext_dict[query])\n        #display(HTML('</td>'))\n        #display(HTML('</tr>'))\n        \n        display(HTML('<table>'))\n        display(HTML('<tr>'))\n        display(HTML('<td>'))\n        readQuery = query\n        context=\"\"\n        if ('|' in query):\n            queryp = query.split('|')\n            context = queryp[0] #extra context keywords\n            readQuery = queryp[1] # the NL query\n\n        #print('query: {}\\n'.format(query))\n        #print('    keys: {}\\n'.format(key_dict[query]))\n        #print('    sum: {}\\n'.format(sum_dict[query]))\n        display(HTML('<h1>Query: '+readQuery+'</h1>'))\n        if (len(context)>0):\n            display(HTML('<h2>Context: '+context+'</h2>'))\n\n        display(HTML('<h2>Extracted Keywords: </h2>'+key_dict[query]))\n        display(HTML('<h2>Answers: </h2>'))\n        key_list = key_dict[query].split(\" , \")\n        key_list.sort(reverse=True,key=len)\n\n\n        for a in ans_entry[query]:\n            for k in key_list:\n                if (len(k)<3):continue\n                par = \"paragraph:\"\n                if (par in a):\n                    indx = a.index(par) + len(par)\n                    pre = a[0:indx]\n                    post = a[indx:]\n                    a =  pre+ post.replace(k+' ','<em style=\"color: DarkBlue;\">{}</em> '.format(k))\n\n            #print('    ans: {}\\n'.format(a))\n            display(HTML(a))\n\n        summ = sum_dict[query]\n        for a in ans_lists[query]:\n            summ =  summ.replace(a,'<strong style=\"color: red;\"><em>{}</em></strong>'.format(a))\n\n        for k in key_list:\n            if (len(k)<3):continue\n            summ =  summ.replace(k+' ','<em style=\"color: DarkBlue;\">{}</em> '.format(k))\n\n        #display(HTML('<h2>Keywords: </h2>'+ \" , \".join(key_list) ))\n        display(HTML('<h2>Summary: </h2> '+summ+'</h2>'))\n        display(HTML('<hr color=\"red\" size=\"8\" align=\"center\" noshade/>'))\n        display(HTML('</td>'))\n        display(HTML('</tr>'))\n        \n        display(HTML('</table>'))  \n        \n    #display(HTML('</table>'))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interactive Query Shell**\n\nFor a more interactive experience one can use the interactive shell below. You can define a context using a regex expression which will then filter out a subset of paragraphs to run the questions against.\nSince filtering can take some time, the usual pattern of operation is to defne the context pattern, define multiple queries then issue a \"run\" command to process those questions in that context. \n\n*Remember: Set doIt=True to run this cell.*\n\n\nCOVID-19 Corpus Answer Extraction engine.\n\nUse 'context:<regex> to focus on a subset of entries'\n    \nUse 'query:<question> to add a question to a batch'\n    \nUse 'run' to generate a report\n\nAnd of course 'help'\n\n\\>context:(death|fatal|mortal)\n\n  setting context pattern to:(death|fatal|mortal)\n  \n\\>query:What is the mortality rate for COVID-19?\n\n  adding 'what is the mortality rate for covid-19?' to query set.\n  \n\\>run\n\nRunning query set:\n\n  Context = '(death|fatal|mortal)'\n  \n  Threshold = 0.5\n  \n  ShowParagraphs = False\n  \n  showSentence = True\n  \n  Queries:\n  \n      'what is the mortality rate for covid-19?'\n      \n    HTML-REPORT FOLLOWS ...\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"doIt =False\ncp=\"(death|fatal|mortal).*(rate)\"#\"(.*)\"\nthreshold = 0.5\nquerySet=[]\nshowPara=False\nshowSentence=True\nif (doIt):\n    print(\"COVID-19 Corpus Answer Extraction engine.\")\n    print(\"Use 'context:<regex> to focus on a subset of entries'\")\n    print(\"Use 'query:<question> to add a question to a batch'\")\n    print(\"Use 'run' to generate a report\")\n    print(\"And of course 'help'\")\n    flag=True\n    while(flag==True):\n        user_response = input(\">\")\n        user_input = user_response\n        user_response=user_response.lower().strip()\n        if (\":\" in user_response):\n            args = user_response.split(':')\n            if (user_response.startswith('context:')):\n                cp = args[1]\n                print(\"  setting context pattern to:{}\".format(cp))\n            if (user_response.startswith('threshold:')):\n                threshold = float(args[1])\n                print(\"  setting threshold to:{}\".format(threshold))\n            if (user_response.startswith('query:')):\n                query=args[1]\n                querySet.append(query)\n                print(\"  adding '{}' to query set.\".format(query))\n            if (user_response.startswith('paragraphs:')):\n                val=args[1]\n                showPara = ((val==\"true\") or (val==\"on\"))\n                print(\"  setting paragraph display :{}\".format(showPara))\n            if (user_response.startswith('sentence:')):\n                val=args[1]\n                showSentence = ((val==\"true\") or (val==\"on\"))\n                print(\"  setting sentence display :{}\".format(showSentence))\n                \n        else:\n            if (user_response == \"run\"):\n                print(\"Running query set:\")\n                print(\"  Context = '{}'\".format(cp))\n                print(\"  Threshold = {}\".format(threshold))\n                print(\"  ShowParagraphs = {}\".format(showPara))\n                print(\"  showSentence = {}\".format(showSentence))\n                print(\"  Queries:\")\n                for q in querySet:\n                    print(\"      '{}'\".format(q))\n                subSetEngine(cp,querySet,corona_df,\n                             rel_threshold=threshold,\n                             include_paragraphs = showPara,\n                            focus_on_sentence= showSentence)\n            if (user_response == \"clear\"):\n                querySet=[]\n            if (user_response == \"exit\"):\n                flag=False\n            if (user_response == \"quit\"):\n                flag=False\n            if (user_response == \"list\"):\n                print(\"Context = '{}'\".format(cp))\n                print(\"Threshold = {}\".format(threshold))\n                print(\"ShowParagraphs = {}\".format(showPara))\n                print(\"ShowParagraphs = {}\".format(showSentence))\n                print(\"Queries:\")\n                for q in querySet:\n                    print(\"    '{}'\".format(q))\n            if (user_response ==\"help\"):\n                print (\" context:<regex-pattern>\")\n                print (\" threshold:<relative-threshold>\")\n                print (\" paragraphs:(on|off|true|false)\")\n                print (\" sentence:(on|off|true|false)\")\n                print (\" query:<query>\")\n                print (\" clear\")\n                print (\" run\")\n                print (\" list\")\n                print (\" 'exit' or 'quit'\")\n                print()\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doIt =False\n\nif (doIt):\n    print(\"============================\")\n    for query in query_list:\n        print('query: {}\\n'.format(query))\n        for a in ans_dict[query]:\n          print('    ans: {}\\n'.format(a))\n        print('    keys: {}\\n'.format(key_dict[query]))\n        print('    sum: {}\\n'.format(sum_dict[query]))\n        print() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doIt=False\nif (doIt):\n    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n    #get a sentiment Analyzer\n    sentimentAnalyzer = SentimentIntensityAnalyzer()\n    # provide answers as a dataframe\n    ans_features = {\"query\": [], \"answers\": [], \"keys\": [],\"summary\": [],\"sentiment\": []}\n    ans_df = pd.DataFrame.from_dict(ans_features)\n    for query in query_list:\n        vs = sentimentAnalyzer.polarity_scores(sum_dict[query])\n        readQuery = query\n        context=\"\"\n        if ('|' in query):\n            queryp = query.split('|')\n            context = queryp[0] #extra context keywords\n            readQuery = queryp[1] # the NL query\n\n        row = {\"context\":None, \"query\": None, \"answers\": None, \"keys\": None,\"summary\": None ,\"sentiment\":None}\n        row[\"context\"]=context\n        row[\"query\"]=readQuery\n        row[\"answers\"] = \" , \".join(ans_dict[query])\n        row[\"keys\"]=key_dict[query]\n        row[\"summary\"]=sum_dict[query]\n        row[\"sentiment\"]= vs['compound']\n        ans_df= ans_df.append(row, ignore_index=True)\n\n    ans_df.head()\n    ans_out = ans_df.to_csv('covid-19_answer_output.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Results\n\nThis is where we finally get the \"official\" results. Note that each call to subSetEngine is of a regex pattern and a list of questions along with a set of display settings. Each regex pattern produces a subset dataframe against which the list of questions are asked. Of course \\\"\\(\\.\\*\\)\\\" would include the entire corpus.  \n\nIt is visualized that other processes could generate both the filtering regular expressions or the questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_features = {\n                \"context\": [],\n                \"task\":[],\n                \"query\": [], \n                \"answers\": [], \n                \"keys\": [],\n                \"summary\": []\n                }\nans_df = pd.DataFrame.from_dict(ans_features)\n\n\n\nquery_list=[ \n            'SARS-CoV-2 covid coronavirus infected cases disease | What are common COVID-19 symptoms?'\n\n            ,'disease pandemic increasing resistance | What presents a emerging threat to global health?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | When did COVID-19 appear?'\n            ,'SARS-CoV-2 covid coronavirus infected cases disease | What animal did SARS-COV2 come from?'\n            ,'What is the effective reproductive number of COVID-19?'\n            ,'What is the basic reproductive number (r0) of COVID-19?'\n            ,'What is the incubation period in days for COVID-19? '\n            ,'What is the mean or average COVID-19 incubation period in days?'\n            ,'What is the range of COVID-19 incubation periods in days?'\n            ,'SARS-CoV-2 covid coronavirus drug therapy treatment study trials efficacy | What is an effective treatment for COVID-19?'\n            ,'SARS-CoV-2 death fatality | What is the motality rate of COVID-19?'\n            ,'SARS-CoV-2 death motality | What is the fatality rate of COVID-19?'\n            ,'SARS-CoV-2 covid-19 coronavirus survive deactivate inactivation disinfection decontaminate nutralize surfaces sterilize| What is an effective disinfectant for coronavirus?'\n            ,'SARS-CoV-2 covid-19 coronavirus drug therapy | What is the most effective anti-viral against coronavirus COVID-19?'\n            ,'SARS-CoV-2 coronavirus lungs respritory COPD| Does pre-existing pulmonary disease increase risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus  hypertension | How does high-blood pressure affect COVID-19?'\n            ,'SARS-CoV-2 coronavirus | How does diabetes affect COVID-19?'\n            ,'SARS-CoV-2 coronavirus factors | What increases the risk of COVID-19?'\n            ,'SARS-CoV-2 coronavirus factors | What are common risk factors for COVID-19 patients?'\n            ,'SARS-CoV-2 babies coronavirus maternal | Are neonates and pregnant women at greater risk of COVID-19?'\n\n]\n# first try a universal context\ncp=\"(.*)\"\nthreshold = 0.8\nquerySet=[]\nshowPara=False\nshowSentence=True\nprint(\"Running query set:\")\nprint(\"  Context = '{}'\".format(cp))\nprint(\"  Threshold = {}\".format(threshold))\nprint(\"  ShowParagraphs = {}\".format(showPara))\nprint(\"  Queries:\")\n\ncurrent_task=\"risk_factors\"\nfor q in query_list:\n    print(\"      '{}'\".format(q))\n\nsubSetEngine(cp,query_list,corona_df,\n        rel_threshold=threshold,\n        include_paragraphs = showPara,\n        focus_on_sentence= showSentence)\n\n#next try operation with a focused subset\ncurrent_task=\"death_rate\"\ncontextPatterns=[\n    \" (death|died|fatal|mortal).*(rate)\"\n]\nquerySet=[\n  'SARS-CoV-2 death fatality | What is the motality rate of COVID-19?'\n ,'SARS-CoV-2 death motality | What is the fatality rate of COVID-19?'\n]\nfor cp in contextPatterns:\n    subSetEngine(cp,querySet,corona_df,\n        rel_threshold=threshold,\n        include_paragraphs = showPara,\n        focus_on_sentence=  showSentence)\n\n#something more generic\ncurrent_task=\"risk_factors\"\ntasks = [\n            'comorbidities'\n            ,'risk factors'\n            ,'lung cancer'\n            ,'hypertension'\n            ,'heart disease'\n            ,'chronic bronchitis'\n            ,'cerebral infarction'\n            ,'diabetes'\n            ,'copd'\n            ,'chronic obstructive pulmonary disease'\n            ,'cardiovascular diseases'\n            ,'chronic kidney disease'\n            ,'bacterial pneumonia'\n            ,'blood type'\n            ,'smoking'\n            ,'pregnancy'\n        ]\nfor t in tasks:\n    query_list=[] \n    query = \"How does {} affect COVID-19?\".format(t)\n    query_list.append(query)\n    cp =\"({})\".format(t)\n    subSetEngine(cp,query_list,corona_df,\n        rel_threshold=threshold,\n        include_paragraphs = showPara,\n        focus_on_sentence= showSentence)\n    \n# how about the weather    \ncurrent_task=\"weather\"\nclimate_synonyms = [\n    'climate',\n    'weather',\n    'humidity',\n    'sunlight',\n    'air temperature',\n    'meteorology', # picks up meteorology, meteorological, meteorologist\n    'climatology', # as above\n    'a dry environment',\n    'a damp environment',\n    'a moist environment',\n    'a wet environment',\n    'a hot environment',\n    'a cold environment',\n    'a cool environment'\n]\n\nfor t in climate_synonyms:\n    query_list=[] \n    query = \"How does {} affect the transmission of COVID-19?\".format(t)\n    query_list.append(query)\n    cp =\"({})\".format(t).replace(\" \",\".*\")\n    subSetEngine(cp,query_list,corona_df,\n        rel_threshold=threshold,\n        include_paragraphs = showPara,\n        focus_on_sentence= showSentence)\n    \n#transmission\ncurrent_task=\"transmission\"\nhypothesis_list = [\"aerisol\",\"droplets\",\"food\",\"fecal matter\",\n                   \"contact\",\"water\"]\nfor t in hypothesis_list:\n    query_list=[] \n    query = \"Is the virus transmitted by {} ?\".format(t)\n    query_list.append(query)\n    cp =\"({})\".format(t).replace(\" \",\".*\")\n    subSetEngine(cp,query_list,corona_df,\n        rel_threshold=threshold,\n        include_paragraphs = showPara,\n        focus_on_sentence= showSentence)\n    \n#final output\nans_df.head()\nans_out = ans_df.to_csv('covid-19_answer_output.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\n* Depending on the query the system can have high percision.\n* TextRank can accidently focus on common boiler plate material in summary generation.\n\n* Need to explore the various weighting and Retriever options.\n* Need to import \"reports\" from other mining processes.\n* Need to adapt the system to domain by fine-tuning the Reader\n* Need to have a seperate query process for the Retriever than the Reader. Then you can do query expansion or key term focusing. (Done, queries have a context section and implemeted Predict2)\n* Need to filter based on score relative to the highest score (Implemented)\n* Need to explore clustering documents for each area of interest first, then build seperate KB's for each cluster and then run queries against each special interest KB.\n* Need to explore the keywords discovered for each query in a relevancy feedback scheme\n* Need to provide the option to focus on just the sentence that the system selected answer appears in instead of the whole paragraph (Implemented)\n* Need to display the TextRank graph (Implemented)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"TensorFlow-GPU","language":"python","name":"tf-gpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}