{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/bda-696x394.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BIG DATA ANALYSIS AND MACHINE LEARNING TUTORIAL**\n\n<p style=\"color:DodgerBlue;\">**Grasping the Fundamentals Of Big Data**-Chapter 1</p><br/>\n\n\n<p style=\"background-color:Tomato;\"> **Big Data Fundamentals** </p>\n\n\n\nAnd before starting my tutorial you can have a look at this Youtube video to get a brief idea,<br>\n<a href=\"https://www.youtube.com/watch?v=j-0cUmUyb-Y\">Big Data Analytics Video For Beginners-Click On Me</a>\n\nHere's some of the books that I can suggest for the beginners :)\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Book</th>\n    <th>Writers</th> \n  </tr>\n  <tr>\n    <td>Data Science For Dummies</td>\n    <td>Lillian Pierson</td> \n  </tr>\n  <tr>\n    <td>Big Data For Dummies</td>\n    <td>A Wiley Brand</td> \n  </tr>\n  <tr>\n    <td>Python Data Science Handbook</td>\n    <td>Jake VanderPlas</td> \n  </tr>\n  <tr>\n    <td> R for Data Science</td>\n    <td>Garret Grolemund and Hadley Wickham</td> \n  </tr>\n  <tr>\n    <td>Introduction to Statistical Learning</td>\n    <td> Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani</td> \n  </tr>\n  <tr>\n    <td> The Elements of Statistical Learning</td>\n    <td> Trevor Hastie, Robert Tibshirani, Jerome Friedman</td> \n  </tr>\n  <tr>\n    <td> Mining of Massive Datasets</td>\n    <td> Jure Leskovec, Anand Rajaraman, Jeff Ullman</td> \n  </tr>\n</table>\n\n\n\nAs we go on to this tutorial I'll also suggest the books in the field of machine learning and deep learning and I'll share the projects that I've created, with you.Don't forget to write comments.If you have any suggestions as a book,you can share with me.I'd like to hear from you :)\n\nLet's start with the heeadings we'll discuss at the beginning.\n\n**1.**History of data Management<br/>\n**2.**Understanding why big data matters to business<br/>\n**3.**Applying big data to business effectiveness<br/>\n**4.**Defining the fundamental elements of big data<br/>\n**5.**Examining the big data's role in future\n\nAround 2005, people began to realize just how much data users generated through **Facebook**, **YouTube**, and other online services. **Hadoop** (an open-source framework created specifically to store and analyze big data sets) was developed that same year. **NoSQL** also began to gain popularity during this time.\n\nThe development of open-source frameworks, such as Hadoop (and more recently, Spark) was essential for the growth of big data because they make big data easier to work with and cheaper to store. In the years since then, the volume of big data has skyrocketed. Users are still generating huge amounts of data—but it’s not just humans who are doing it.\n\nWith the advent of the **Internet of Things (IoT)**, more objects and devices are connected to the internet, gathering data on customer usage patterns and product performance. The emergence of **machine learning** has produced still more data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/history-bigdata.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Managing and analyzing big data have always offered the greatest benefits and the greatest challenges for organizations of all sizes and across all industries.Businesses have long struggled with finding a pragmatic approach to capturing information about customers ,products and services.Over time companies and markets they participate in have grown more complicated.\n\nTo **survive or gain a competitive advantage with customers **, these companies added more product lines and diversified how they deliver their product.Data struggles are not limited to business.Research and development ( R&D ) organizations,for example,have struggled to get enough computing power to run sophisticated models or to process images and other sources or scientific data.\n\nIndeed,we are dealing with a lot of complexity when it comes to data.Some data is **structured **and stored in a traditional relational database,while other data ,including documents ,customer service records,and even pictures and videos,is **unstructured**.Companies also have to consider new sources of data generated by machines such as sensors.Other new sources are human generated from website interactions .\n\nSo,We can classify the data into three groups **structured,unstructrured ans semi-structured.**To explain differences I've created a table.\n\nWhen you are dealing with so much information in so many different forms ,it is impossible to think about data management in traditional ways. A precise specification of ‘big’ is elusive. What is considered big for one organization may be small for another. What is large-scale today will likely seem small-scale in the near future; petabyte is the new terabyte. Thus, size alone cannot specify big data. The complexity of the data is an important factor that must also be considered.\n\n\n<table style=\"width:100%\">\n  <tr>\n    <th>Properties</th>\n    <th>Structured</th> \n    <th>Semi-structured</th> \n    <th>Unstructured</th> \n  </tr>\n  <tr>\n    <td>Technology</td>\n    <td>It is based on Relational database table</td>\n    <td>It is based on XML/RDF</td>\n    <td>It is based on character and binary data</td> \n  </tr>\n  <tr>\n    <td>Transaction management</td>\n    <td>Matured transaction and various concurrency technique</td>\n    <td>Transaction is adapted from DBMS not matured</td>\n    <td>No transaction management and no concurrency</td> \n  </tr>\n  <tr>\n    <td>Version management</td>\n    <td>Versioning over tuples,row,tables</td>\n    <td>Versioning over tuples or graph is possible</td>\n    <td>Versioned as whole</td> \n  </tr>\n  <tr>\n    <td>Flexibility</td>\n    <td>It is sehema dependent and less flexible</td>\n    <td>It is more flexible than structuded data but less than flexible than unstructured data</td>\n    <td>it very flexible and there is abbsence of schema</td> \n  </tr>\n  <tr>\n    <td>Scalability</td>\n    <td>It is very difficult to scale DB schema</td>\n    <td>It’s scaling is simpler than sstructured data</td>\n    <td>It is very scalable</td> \n  </tr>\n  <tr>\n    <td>Robustness</td>\n    <td>Very robust</td>\n    <td>New technology, not very spread</td>\n    <td>---</td> \n  </tr>\n  <tr>\n    <td>Query performance</td>\n    <td>Structured query allow complex joining</td>\n    <td>Queries over anonymous nodes are possible</td>\n    <td>Only textual query are possible</td>\n  </tr>\n</table>\n\n\n\n\nNow I'll explain the characteristics of Big Data\n\n\n\n<p style=\"background-color:Tomato;\">**The Characteristics Of Big Data**</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/threev.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue;\">Volume</h1>\n\nRefers to the vast amounts of data generated every second. Just think of all the emails, twitter messages, photos, video clips, sensor data etc. we produce and share every second. We are not talking Terabytes but Zettabytes or Brontobytes. On Facebook alone we send 10 billion messages per day, click the “like’ button 4.5 billion times and upload 350 million new pictures each and every day. If we take all the data generated in the world between the beginning of time and 2008, the same amount of data will soon be generated every minute! This increasingly makes data sets too large to store and analyse using traditional database technology. With big data technology we can now store and use these data sets with the help of distributed systems, where parts of the data is stored in different locations and brought together by software.\n\n<h1 style=\"background-color:DodgerBlue;\">Velocity</h1> Refers to the speed at which new data is generated and the speed at which data moves around. Just think of social media messages going viral in seconds, the speed at which credit card transactions are checked for fraudulent activities, or the milliseconds it takes trading systems to analyse social media networks to pick up signals that trigger decisions to buy or sell shares. Big data technology allows us now to analyse the data while it is being generated, without ever putting it into databases.\n\n<h1 style=\"background-color:DodgerBlue;\">Variety</h1>Refers to the different types of data we can now use. In the past we focused on structured data that neatly fits into tables or relational databases, such as financial data (e.g. sales by product or region). In fact, 80% of the world’s data is now unstructured, and therefore can’t easily be put into tables (think of photos, video sequences or social media updates). With big data technology we can now harness differed types of data (structured and unstructured) including messages, social media conversations, photos, sensor data, video or voice recordings and bring them together with more traditional, structured data.\n\n<h1 style=\"background-color:DodgerBlue;\">Veracity</h1>Refers to the messiness or trustworthiness of the data. With many forms of big data, quality and accuracy are less controllable (just think of Twitter posts with hash tags, abbreviations, typos and colloquial speech as well as the reliability and accuracy of content) but big data and analytics technology now allows us to work with these type of data. The volumes often make up for the lack of quality or accuracy.\n\n<h1 style=\"background-color:DodgerBlue;\">Value</h1> Then there is another V to take into account when looking at Big Data: Value! It is all well and good having access to big data but unless we can turn it into value it is useless. So you can safely argue that ‘value’ is the most important V of Big Data. It is important that businesses make a business case for any attempt to collect and leverage big data. It is so easy to fall into the buzz trap and embark on big data initiatives without a clear understanding of costs and benefits.\n\nSo,briefly:\n\n**Volume:** This refers to the vast amounts of data that is generated every second/minute/hour/day in our digitized world.<br/>\n\n**Velocity:** This refers to the speed at which data is being generated and the pace at which data moves from one point to the next.<br/>\n\n**Variety:** This refers to the ever-increasing different forms that data can come in, e.g., text, images, voice, geospatial.<br/>\n\n**Veracity:** This refers to the quality of the data, which can vary greatly.<br/>\n\n**Value:** Processing big data must bring about value from insights gained.<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/Management.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building a Successful Big Data Management Architecture**\n\nThe above figure illustrates that data must first be captured,and then organized and integrated.After this pase is successfully implemented,data can be analyzed based on the problem being addressed.Finally,management takes action based on the outcome of that analysis.For exmple, Amazon.com might recommend a book based on a past purchase or a customer might receive a coupon for a discount for a future purchase of a related product to one that was just purchased.Although this sounds straightforward,certain nuances of these functions are complicated.Validation is a particularly important issue.If your organization is combining data sources,it is critical that you have the ability to validate that these sources make sense when combined."},{"metadata":{},"cell_type":"markdown","source":"**Exploring sources of big structured data**\n\n**Machine Generated Data:**<br/>\n->Computer or machine generated<br/>\n->Human Generated<br/>\n->Sensor Data<br/>\n->Web Log Data<br/>\n->Point Of Sale Data<br/>\n->Financial Data<br/>\n\n**Human Generated:**<br/>\n->Input Data<br/>\n->Click-stream data<br/>\n->Gaming related data<br/>\n\n**Exploring sources of big unstructured data**<br/>\n\n**Machine Generated Data**<br/>\n->Satellite Images<br/>\n->Scientific Data<br/>\n->Photograph and Video<br/>\n->Radar and Sonar data<br/>\n**Human-Generated Data**<br/>\n->Text internal to your company<br/>\n->Social Media Data<br/>\n->Mobile Data<br/>\n->Website Content"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/company.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Big data** is **big** business. Eleanor O'Neill takes a look at ten of the companies using data and analytics to gain a competitive edge.\nThe term 'big data' refers to extremely large sets of digital data that may be analysed to reveal patterns, trends and associations relating to human behaviour and interactions.\n\nCompanies can use this information to their advantage; automating processes, gaining insight into their target market and improving overall performance using the feedback readily available.\n\nHere we look at some of the businesses integrating big data and how they are using it to boost their brand success.\n\n<p style=\"color:DodgerBlue;\">1. Amazon </p><br/>\nThe online retail giant has access to a massive amount of data on its customers; names, addresses, payments and search histories are all filed away in its data bank.\n\nWhile this information is obviously put to use in advertising algorithms, Amazon also uses the information to improve customer relations, an area that many big data users overlook.\n\nThe next time you contact the Amazon help desk with a query, don't be surprised when the employee on the other end already has most of the pertinent information about you on hand. This allows for a faster, more efficient customer service experience that doesn't include having to spell out your name three times.\n\n<p style=\"color:DodgerBlue;\">2. American Express </p><br/>\nThe American Express Company is using big data to analyse and predict consumer behaviour.\n\nBy looking at historical transactions and incorporating more than 100 variables, the company employs sophisticated predictive models in place of traditional business intelligence-based hindsight reporting.\n\nThis allows a more accurate forecast of potential churn and customer loyalty. In fact, American Express has claimed that, in their Australian market, they are able to predict 24% of accounts that will close within four months.\n\n<p style=\"color:DodgerBlue;\">3. BDO</p><br/>\nNational accounting and audit firm BDO puts big data analytics to use in identifying risk and fraud during audits.\n\nWhere, in the past, finding the source of a discrepancy would involve numerous interviews and hours of manpower, consulting internal data first allows for a significantly narrowed field and streamlined process.\n\nIn one case, BDO Consulting Director Kirstie Tiernan noted, they were able to cut a list of thousands of vendors down to a dozen and, from there, review data individually for inconsistencies. A specific source was identified relatively quickly.\n\n<p style=\"color:DodgerBlue;\">4. Capital One</p><br/>\nMarketing is one of the most common uses for big data and Capital One are at the top of the game, utilising big data management to help them ensure the success of all customer offerings.\n\nThrough analysis of the demographics and spending habits of customers, Capital One determines the optimal times to present various offers to clients, thus increasing the conversion rates from their communications.\n\nNot only does this result in better uptake but marketing strategies become far more targeted and relevant, therefore improving budget allocation.\n\n<p style=\"color:DodgerBlue;\">5. General Electric (GE)</p><br/>\nGE is using the data from sensors on machinery like gas turbines and jet engines to identify ways to improve working processes and reliability.\n\nThe resultant reports are then passed to GE's analytics team to develop tools and improvements for increased efficiency.\n\nThe company has estimated that data could boost productivity in the US by 1.5%, which, over a 20-year period, could save enough cash to raise average national incomes by as much as 30%.\n\n<p style=\"color:DodgerBlue;\">6. Miniclip</p><br/>\nMiniclip, who develop, publish and distribute digital games globally, use big data to monitor and improve user experience.\n\nDue to the nature of the company and sector, customer retention is a priority for Miniclip in order to make games more profitable and, therefore, to support business growth.\n\nBig data reporting, analysis, experimentation and machine learning data products allow the company to measure the successful elements of their products and implement them in future ventures, while also eliminating or improving the problematic components.\n\n<p style=\"color:DodgerBlue;\">7. Netflix</p><br/>\nThe entertainment streaming service has a wealth of data and analytics providing insight into the viewing habits of millions of international consumers.\n\nNetflix uses this data to commission original programming content that appeals globally as well as purchasing the rights to films and series boxsets that they know will perform well with certain audiences.\n\nFor example, Adam Sandler has proven unpopular in the US and UK markets in recent years but Netflix green-lighted four new films with the actor in 2015, armed with the knowledge that his previous work had been successful in Latin America.\n\n<p style=\"color:DodgerBlue;\">8. Next Big Sound</p><br/>\nNext Big Sound (NBS) has figured out how to use the data from Spotify streams, iTunes sales, SoundCloud plays, Facebook likes, Wikipedia page views, YouTube hits and Twitter mentions to predict the next big thing in music.\n\nThe company's analytics provide insight into social media popularity, the impact of TV appearances and many other nuggets of information that are invaluable to the music industry. Artists can also use the data for their own promotion, thanks to a partnership between NBS and Spotify.\n\nBillboard now publishes two charts based exclusively on NBS’s data and they have worked with companies such as Pepsi and American Express to help steer billions being spent brands on music-related marketing and sponsorships.\n\n<p style=\"color:DodgerBlue;\">9. Starbucks</p><br/>\nHave you ever wondered how Starbucks can open three branches on the same street and not have their business suffer?\n\nThe coffeehouse behemoth uses big data to determine the potential success of each new location, taking information on location, traffic, area demographic and customer behaviour into account.\n\nMaking this kind of assessment before opening a store means Starbucks can make a fairly accurate estimation of what the success rate will be and choose locations based on the propensity toward revenue growth.\n\n<p style=\"color:DodgerBlue;\">10. T-Mobile</p><br/>\nThe mobile network, like American Express, is combining customer transaction and interactions data to predict customer fluctuations.\n\nBy utilising internal information on billing and customer relations management along with data on social media usage, T-Moblie USA claims they halved customer defections within a single quarter.\n\nThe company has integrated the data gathering tools across its' IT systems."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\n\nImage(\"../input/images/Trump_New_York_Times_tweet_.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Yao Yue** has worked on Twitter’s Cache team since 2010. She recently gave a really great talk: Scaling Redis at Twitter. It’s about Redis of course, but it's not just about Redis.\n\nYao has worked at Twitter for a few years. She's seen some things. She’s watched the growth of the cache service at Twitter explode from it being used by just one project to nearly a hundred projects using it. That's many thousands of machines, many clusters, and many terabytes of RAM.\n\nIt's clear from her talk that's she's coming from a place of real personal experience and that shines through in the practical way she explores issues. It's a talk well worth watching.\n\nAs you might expect, Twitter has a lot of cache.<br/>\n**Why Redis?**<br/>\nRedis drives Timeline, Twitter’s most important service. Timeline is an index of tweets indexed by an id. Chaining tweets together in a list produces the Home Timeline. The User Timeline, which consists of tweets the user has tweeted, is just another list.\n\nWhy consider Redis instead of Memcache? The Network Bandwidth Problem and The Long Common Prefix Problem."},{"metadata":{},"cell_type":"markdown","source":"<p style=\"background-color:Tomato;\"><b>The Trade-offs</b></p>\n\nHere I want to give some info about trade-offs but not in detail.If you wanna get more you can just click on the link that I've provided at the end of this part of our discussion.\n\n<body style=\"background-color:powderblue;\">\n<p style=\"color:Tomato;\">**Performance vs Scalability** </p><br/>\n<p>How Do I Know If I Have a** Performance **problem ? <br> If your system is **slow** for a **single user**.</p>\n\n<p>How Do I Know If I Have a** Scalability **problem ? <br> If your system is **fast** for a **single user** but **slow** under **heavy load.**</p>\n<body style=\"background-color:powderblue;\">\n<p style=\"color:Tomato;\">**Latency vs Throughput** </p>\n<p>You should strive for **maximal** throughput with **acceptable latency**</p>  \n<body style=\"background-color:powderblue;\">\n<p style=\"color:Tomato;\">**Availability vs Consistency** </p>\n<p style=\"color:DodgerBlue;\">**Brewer's CAP Theorem** </p>    \n<p>You can only pick two :</p>    \n<p style=\"color:Tomato;\">Consistency</p>\n<p style=\"color:Tomato;\">Availability</p>\n<p style=\"color:Tomato;\">Partition Tolarance</p>\nAt a point given in time.<br>\n    \nIn a **Centralized System:(RDBMS)**-We don't have network partitions( P ) in CAP.So you get both Availability and Consistency.Atomic,Consistent,Isolated,Durable.<br>\n\nIn a** Distributed System:**We have network partitions( P ) in CAP.So you get to only pick one of Availability and Consistency.<br>\nSo, there are two types of systems :\n<p style=\"color:Tomato;\">CP and AP</p>There is only one choice to make.In case of a network partition,what do you sacrifice ?Consistency or Availability.( Basically Available,Soft State,Eventually Consistent )\n\n<p style=\"color:Tomato;\">Availability Patterns</p>\n**1.**Fail Over( Normal Operation----Failure Occurs------>Fail Over-----Service Restored--->Normal Operation )<br>\n**2.**Replication( Active Replication(Pull),Passive Replication(Pull-Data not available read from peer store it locally,Works well with timeout based caches )<br>\n  **2.1** Master-Slave<br>\n  **2.2** Tree-Replication<br>\n  **2.3** Master-Master<br>\n  **2.4** Buddy-Replication<br>\n</body>\n\nHow to scale out** RDBMS** ? **Sharding:Partitioning** and **Sharding:Replication** <br>\n\nI'm not going into detail.If you want to get more information about these topic here's the link,\n\n<a href=\"https://www.slideshare.net/jboner/scalability-availability-stability-patterns/64-Think_about_your_data_When\">Trade-Offs Slide Share</a>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"color:DodgerBlue;\"> **The best languages for big data** - Chapter 2 </p><br/>\n\n<p style=\"background-color:Yellow;\">1.Why use **Scala** for big data? :)</p>\n->Fast and robust<br/>\n->Suitable for working with Big Data tools like Apache Spark for distributed Big Data processing<br/>\n->JVM compliant, can be used in a Java-based ecosystem<br/>\n\n<p style=\"background-color:Yellow;\">2.Why use **Python** for big data?:)</p>\nGeneral-purpose<br/>\nRich libraries for data analysis and machine learning<br/>\nEasy to use<br/>\nSupports iterative development<br/>\nRich integration with Big Data tools<br/>\nInteractive computing through Jupyter notebooks<br/>\n\n<p style=\"background-color:Yellow;\">4.Why use **R** for big data? :)</p>\nBuilt for data science<br/>\nSupport for Hadoop and Spark<br/>\nStrong statistical modeling and visualization capabilities<br/>\nSupport for Jupyter notebooks<br/>\n\n<p style=\"background-color:Yellow;\">5.Why use **Java** for big data?:)</p>\nTraditional Big Data tools and frameworks are written in Java<br/>\nStable and production-ready<br/>\nLarge ecosystem of tried and tested tools and libraries<br/>\n\n<p style=\"background-color:Yellow;\">6.Why use **Go** for big data?</p>\nFast, easy to use<br/>\nMany tools used in the Big Data infrastructure are Go-based<br/>\nEfficient distributed computing<br/>\n\n**So…which language should you choose?**<br/>\n\nTo answer the question in short – it all depends on the use-case you want to develop. If your focus is hardcore data analysis which involves a lot of statistical computing, **R** would be your go-to language. On the other hand, if you want to develop streaming applications for your Big Data, **Scala** can be a preferable choice. If you wish to use **Machine Learning** to leverage your Big Data and build predictive models, Python will come to your rescue. Lastly, if you plan to build Big Data solutions using just the traditionally-available tools, **Java** is the language for you.I'm going to use python for this tutorial so if you don't know python well you can learn at this link below:\n\n<a href=\"https://www.w3schools.com/python/\">You can learn Python easily by cliking on this link</a>\n\nAnd I'll use **numpy,pandas,matplotlib,seaborn and plotly** for the first kernel that we are going to create.So let's get an idea about them together.\n\n<p style=\"color:DodgerBlue;\">**NUMPY**</p><br/>\nNumPy, which stands for Numerical Python, is a library consisting of multidimensional array objects and a collection of routines for processing those arrays. Using NumPy, mathematical and logical operations on arrays can be performed. This tutorial explains the basics of NumPy such as its architecture and environment. It also discusses the various array functions, types of indexing, etc. An introduction to Matplotlib is also provided. All this is explained with the help of examples for better understanding.<br>\n\n**Installations**\n\nYou can use **Anaconda's Spyder**(recommended) to try the things we'll learn or you can use** kaggle **too**.Standard Python distribution doesn't come bundled with NumPy module. A lightweight alternative is to install NumPy using popular Python package installer, pip.<br>\n\npip install numpy<br>\n\nThe best way to enable NumPy is to use an installable binary package specific to your operating system. These binaries contain full SciPy stack (inclusive of NumPy, SciPy, matplotlib, IPython, SymPy and nose packages along with core Python).<br>\n\n**Windows**<br>\nAnaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.<br>\n\n**Canopy**<br> (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.<br>\n\n**Python (x,y):**<br> It is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from https://www.python-xy.github.io/)<br>\n\n**Linux**<br>\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.<br>\n\n**For Ubuntu**<br>\nsudo apt-get install python-numpy<br> \npython-scipy python-matplotlibipythonipythonnotebook python-pandas<br> \npython-sympy python-nose<br>\n\nDon't worry :) Here's the Youtube video link that explains well how to install Anaconda and use Spyder below,\n\n<a href=\"https://www.youtube.com/watch?v=5mDYijMfSzs\">How to Install Anaconda and Use Spyder</a>\n\n<p style=\"background-color:Tomato;\">Let's start to learn Numpy :)</p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np #importing the Numpy library\n\narray = np.array([1,2,3]) #creating the one dimension numpy array -1 row and 3 colums\nprint( array ) #let's see what we've created by using print command\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numpy arrays with more than one dimensions \nimport numpy as np \narray = np.array([[1, 2], [3, 4]]) #creating the numpy array which has 2 rows and 2 colums \nprint( array )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtype parameter \nimport numpy as np \narray = np.array([1, 2, 3], dtype = complex) #dtype can take different parameters according to your array members\nprint( array )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \narray = np.array([[1,2,3],[4,5,6]]) #we can see how many rows and colums are there by using shape method\nprint(array.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we want to resize the array,here ew want to make it with 3 rows and 2 colums\nimport numpy as np \n\na = np.array([[1,2,3],[4,5,6]])\nprint( \"Array before resizing:\\n \", a )\na.shape = (3,2) \nprint(\"Array after resizing:\\n \", a ) \n\n#Also we can use the reshape methode for resizing\n\n#import numpy as np \n#a = np.array([[1,2,3],[4,5,6]]) \n#b = a.reshape(3,2) \n#print( b )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is one dimensional array-members starting from 1 and ends at 23 so don't forget 24 is exclusive \nimport numpy as np \narray= np.arange(24) \nprint( array ) # to see result we can just use the name of the array\n\n#print( type( array )) we can see the types by using type() method\n\n# dtype of array is now float32 (4 bytes)  \n#x = np.array([1,2,3,4,5], dtype = np.float32) \n#print( x.itemsize ) #if we want to learn the size of item \np1=[] # we have created an empty list\np1.append(4) #append int 4 at the end of the array\np1.append(5) #append int 5 at the end of the array \nprint( p1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now reshape it \nb = array.reshape(2,4,3) \nprint( b ) \n# b is having three dimensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can create empty arrays in numpy\n#numpy.empty(shape, dtype = float, order = 'C')\nimport numpy as np \nx = np.empty([3,2], dtype = int) \nprint( x )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Also we can create array of just zeros or ones\n#numpy.zeros(shape, dtype = float, order = 'C')\n#numpy.ones(shape, dtype = None, order = 'C')\n\n# array of five zeros. Default dtype is float \nimport numpy as np \nx = np.zeros(5) \nprint( x )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numpy.linspace(start, stop, num, endpoint, retstep, dtype)\nimport numpy as np \nx = np.linspace(10,20,5) \nprint( x )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# endpoint set to false \n#endpoint\n#True by default, hence the stop value is included in the sequence. If false, it is not included\nimport numpy as np \nx = np.linspace(10,20, 5, endpoint = False) \nprint( x )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#slicing arrays start point-end point-difference\nimport numpy as np \na = np.arange(10) \n\nprint( a[0:3]) #starting from 0 and ends at 2 ,here 3 is exclusive\n\nprint( a[:3] ) #starts once again from the zero if we had not written anything before : \n\nprint( a[:-1]) #starts from 0 and ends at last member from the end point ,8.\n\nreverse_array=a[::-1] # we can reverse array in this way\nprint( reverse_array )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \n\narray=np.array([[1,2,3,4,5],[6,7,8,9,10]]) # we have created an array with two rows and two columns\n\nprint( array[1,1]) # we can acces members in this way\nprint(\"------------\")\nprint( array[:,[1,2]]) #we take all of the rows  and first and second of the columns in this way\nprint(\"------------\")\nprint( array[-1,:] ) #we take last row and all of the columns\nprint(\"------------\")\nprint( array[:,-1] )#we take all of the rows and the last column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \n#flatten format and transposing of arrays\narray=np.array([[1,2,3],[4,5,6],[7,8,9]]) # we have created an array with three rows and three columns\n\nflatten=array.ravel() #if we want to get rid of all of these dimension structure use ravel() method\nprint( flatten )\nprint(\"-------------\")\n\ntransposed=array.T #we can alsso get the transpose of the given array in this way\nprint( transposed )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stacking arrays\n\narray1=np.array( [[1,2],[3,4]])\narray2=np.array( [[-1,-2],[-3,-4]])\n\n#vertical stacking\n\nVstacked=np.vstack((array1,array2))\nprint( Vstacked )\nprint(\"------------\")\n#horizontal stacking\n\nHstacked=np.hstack((array1,array2))\nprint( Hstacked )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copy and convert arrays\n\nlist1=[1,2,3,4]\n\narray=np.array(list1) # we can also create a numpy array by using the already created list\n\n#what happens if we run the following \n\nreferenced1=array\nreferenced2=referenced1\n\nreferenced2[0]=7\n\nprint( referenced1 )\nprint( array ) # we see that they are referenced of each other if we assigned them to each other directly\n#how to get rid of from that situation\n\ncopy1=array.copy()\ncopy1[0]=9\nprint( array )# this time array has not been changed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numpy operations\n\na=np.array([1,2,3])\nb=np.array([4,5,6])\n\n#we can make mathematical operations on arrays in the following ways\nprint(a+b) #summation\nprint(a-b) #difference\nprint(a*b) #mutiplication\nprint(a**2+1) #exponential operations\nprint(np.sin(a)) #trigonometric operations\nprint(a<2) #boolean operations individually\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n#element wise product with arrays\na=np.array([[1,2,3],[4,5,6]])\nb=np.array([[7,8,9],[5,7,8]])\n\nprint( a*b )\n#--------------------------------\n#matrix multiplication\nprint(\"-----------------\")\n#a.dot(b) gets value error,cause we cannot multiply two arrays with dimensions (2x3) and (2x3)\na.dot(b.T) #works well case we are multipliying two arrays with dimensions (2x3) and (3x2 ) which is transposed\nprint(a)\nprint(\"-----------------\")\nprint(np.exp(a))\n#we can use other methods like max,min and sum\nprint(a.sum(),'\\n')\nprint(a.max(),'\\n')\nprint(a.min(),'\\n')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n#element wise product with arrays\na=np.array([[1,2,3],[4,5,6]])\n\nprint( a.sum( axis=0 ))\nprint( a.sum( axis=1 ))\nprint( np.sqrt(a))\n#or instead of a+a we can use\n\nnp.add(a,a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the end of our discussion of Numpy guys.If you want to get Numpy in more detail I can suggest you that link below\n<a href=\"https://www.tutorialspoint.com/numpy/numpy_sort_search_counting_functions.htm\" target=\"_blank\">\nClick here for more detailed Numpy Tutorial</a> Let's go on with Pandas.\n\n<p style=\"color:DodgerBlue;\">**PANDAS**</p><br/>\nPandas is an open-source, BSD-licensed Python library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Python with Pandas is used in a wide range of fields including academic and commercial domains including finance, economics, Statistics, analytics, etc. In this tutorial, we will learn the various features of Python Pandas and how to use them in practice.<br>\n**Installations** <br>\nStandard Python distribution doesn't come bundled with Pandas module. A lightweight alternative is to install NumPy using popular Python package installer, pip..But You will also use the **Anaconda's spyder** and **kaggle** too.<br>\n\npip install pandas<br>\n\nIf you install Anaconda Python package, Pandas will be installed by default with the following −<br>\n\n**Windows** <br>\n**Anaconda **<br>(from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.<br>\n\n**Canopy** <br>(https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac. <br>\n\n**Python (x,y)** is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/) <br>\n\n**Linux** <br>\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.<br>\n\n**For Ubuntu Users** <br>\n\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook<br>\npython-pandas python-sympy python-nose<br>\n<p style=\"background-color:Tomato;\">Let's start to learn Pandas,</p>\n\nPandas deals with the following **three data structures** −\n\n1.Series<br>\n2.DataFrame<br>\n3.Panel<br>\n<table style=\"width:100%\">\n  <tr>\n    <th>Data Structures</th>\n    <th>Dimensions</th> \n    <th>Description</th>\n  </tr>\n  <tr>\n    <td>Series</td>\n    <td>1</td> \n    <td>1D labeled homogeneous array, sizeimmutable.</td>\n  </tr>\n  <tr>\n    <td>Data Frames</td>\n    <td>2</td> \n    <td>General 2D labeled, size-mutable tabular structure with potentially heterogeneously typed columns.</td>\n  </tr>\n  <tr>\n    <td>Panel</td>\n    <td>3</td> \n    <td>General 3D labeled, size-mutable array.</td>\n  </tr>\n</table>\n\n<p style=\"color:DodgerBlue;\">**Series**</p>\n\nSeries is a one-dimensional array like structure with homogeneous data. For example, the following series is a collection of integers 10, 23, 56, …<br>\n\n<table style=\"width:100%\">\n  <tr>\n    <th>10  |</th>\n    <th>23  |</th>\n    <th>56  |</th>\n    <th>...</th>\n  </tr>\n</table> <br>\n**Key Points**<br>\n1.Homogeneous data<br>\n2.Size Immutable<br>\n3.Values of Data Mutable<br>\n\n<p style=\"color:DodgerBlue;\">**DataFrame**</p>\nDataFrame is a two-dimensional array with heterogeneous data. For example,<br>\n<table style=\"width:100%\">\n  <tr>\n    <th>Name</th>\n    <th>Surname</th> \n    <th>Age</th>\n  </tr>\n  <tr>\n    <td>Abel</td>\n    <td>Markovski</td> \n    <td>35</td>\n  </tr>\n  <tr>\n    <td>Trianna</td>\n    <td>Ludwig</td> \n    <td>25</td>\n  </tr>\n  <tr>\n    <td>Eddie</td>\n    <td>Eagle</td> \n    <td>20</td>\n  </tr>\n</table>\n**Key Points**<br>\n1.Heterogeneous data<br>\n2.Size Mutable<br>\n3.Data Mutable\n\n<p style=\"color:DodgerBlue;\">**Panel**</p>\nPanel is a three-dimensional data structure with heterogeneous data. It is hard to represent the panel in graphical representation. But a panel can be illustrated as a container of DataFrame.<br>\n\n**Key Points**<br>\n1.Heterogeneous data<br>\n2.Size Mutable<br>\n3.Data Mutable<br>\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandas.Series( data, index, dtype, copy)\n#A series can be created using various inputs like −\n   #Array\n   #Dict\n   #Scalar value or constant\n#import the pandas library and aliasing as pd\nimport pandas as pd\nimport numpy as np\ndata = np.array(['a','b','c','d'])\ns = pd.Series(data)\nprint( s )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the pandas library and aliasing as pd\nimport pandas as pd\nimport numpy as np\ndata = np.array(['a','b','c','d'])\ns = pd.Series(data,index=[100,101,102,103]) # we can also use set_index() method later on\nprint(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create series from dictionaries\n#import the pandas library and aliasing as pd\nimport pandas as pd\nimport numpy as np\ndata = {'a' : 0., 'b' : 1., 'c' : 2.} #this is a normal python dictionary\ns = pd.Series(data) #we've used dictionary to create pandas series\nprint( s )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a Series from Scalar\n\n#import the pandas library and aliasing as pd\nimport pandas as pd\nimport numpy as np\ns = pd.Series(5, index=[0, 1, 2, 3])\nprint( s )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandas.DataFrame( data, index, columns, dtype, copy)\n#A pandas DataFrame can be created using various inputs like −\n  #Lists\n  #dict\n  #Series\n  #Numpy ndarrays\n  #Another DataFrame\nimport pandas as pd\ndata = [['Alex',10],['Bob',12],['Clarke',13]]\nprint( type( data ))#we've used list to create data frame for example\ndf = pd.DataFrame(data,columns=['Name','Age'])\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a DataFrame from Dict of ndarrays / Lists\n\nimport pandas as pd\ndata = {'Name':['Tomy', 'John', 'Stevens', 'Rownie'],'Age':[28,34,29,42]}\ndf = pd.DataFrame(data)\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a DataFrame from List of Dicts\n\nimport pandas as pd\ndata = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}]\ndf = pd.DataFrame(data)\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column Selection\nimport pandas as pd\n\nd = {'first' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n   'second' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n\ndf = pd.DataFrame(d)\nprint( df['first'] )\n#or we do\ndf #we can see it in the table format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column Addition\nimport pandas as pd\n\nd = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n\ndf = pd.DataFrame(d)\n\n# Adding a new column to an existing DataFrame object with column label by passing new series\n\nprint (\"Adding a new column by passing as Series:\")\ndf['three']=pd.Series([10,20,30],index=['a','b','c'])\nprint( df )\n\nprint (\"Adding a new column using the existing columns in DataFrame:\")\ndf['four']=df['one']+df['three']\n\nprint( df )\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# column Deletion\n# Using the previous DataFrame, we will delete a column\n# using del function\nimport pandas as pd\n\nd = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), \n   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']), \n   'three' : pd.Series([10,20,30], index=['a','b','c'])}\n\ndf = pd.DataFrame(d)\nprint (\"Our dataframe is:\")\nprint( df )\n\n# using del function\nprint (\"Deleting the first column using DEL function:\")\ndel df['one']\nprint( df )\n\n# using pop function\nprint (\"Deleting another column using POP function:\")\ndf.pop('two')\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Row Selection, Addition, and Deletion\nimport pandas as pd\n\nd = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), \n     'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n\ndf = pd.DataFrame(d)\nprint( df.loc['b'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Slice Rows\n\nimport pandas as pd\n\nd = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), \n   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n\ndf = pd.DataFrame(d)\nprint( df )\nprint( df.loc[:,[\"one\"]] ) #all rows and column one\n\nprint( df.loc[::-1,:])#reversed rows and all columns\n\nprint( df.iloc[:,1])#prints indexes instead of names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Addition of Rows\n#Add new rows to a DataFrame using the append function. This function will append the rows at the end.\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])\n\ndf = df.append(df2)\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deletion of Rows\n\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])\n\ndf = df.append(df2)\n\n# Drop rows with label 0\ndf = df.drop(0)\n\nprint( df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filters\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2,3,4], [3, 4,5,6], [6, 7,8,9]], columns = ['a','b','c','d'])\nprint(df)\n\nfilter1=df.a>2\n\nprint( df[ filter1 ]) #usage of filters\n\nfilter2=df.b<=5\n\nprint( df[ filter1 & filter2 ]) #combining two filters\nprint( df[ df.a > 5]) #we can use it briefly like\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list comprehension\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2,3,4], [3, 4,5,6], [6, 7,8,9]], columns = ['a','b','c','d'])\nprint(df)\n\navg_a=df.a.mean() #we've found the mean of the column a\nprint( avg_a)\n\ndf[\"avg\"]=[\"less\" if avg_a > each else \"much\" for each in df.a] #we are adding a new column called avg\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame([[1, 2,3,4], [3, 4,5,6], [6, 7,8,9]], columns = ['stock value','stock index','stock place no','zip code'])\nprint(df)\n#changing the column names which cannot be used easily\n\ndf.columns=[ each.split()[0]+\"_\"+each.split()[1] if len( each.split()) > 1 else each for each in df.columns]\nprint(df)#we have successfully changed the column names\ndf.drop([\"zip_code\"],axis=1,inplace=True)#this is a permanent change in the table ,use inplace=True\nprint(df) #zip code has gone then","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame([[1, 2,3,4], [3, 4,5,6], [6, 7,8,9],[64, 75,87,98],[9,8,7,5],[63, 72,81,91]], columns = ['stock value','stock index','stock place no','zip code'])\nprint(df)\n\nfirst5=df.head() #prints the first five values\nlast5=df.tail() #prints the last five values\ndata_concat1=pd.concat([first5,last5],axis=0) #combine rows together if axis=0\ndata_concat2=pd.concat([first5,last5],axis=1) #combie columns together if axis=1\n\nprint( first5 )\nprint( last5  )\nprint(data_concat1)\nprint( data_concat2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming data\n\ndf['stock value']=[each*2 for each in df['stock value']] #multiply all elements with 2\ndf\n#or transform with apply method\n\ndef multiplyby2( item ):\n    return item*2\ndf[\"stock value\"]=df[\"stock value\"].apply( multiplyby2 )\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we go on I'll give more details about pandas.Now over the dataset World happiness report I'll share what we can do with the matplotlib and seaborn and plotly.Here you can go on with the project's that I've created.<br>\n\nProjects for beginners,<br>\n\n<a href=\"https://www.kaggle.com/rabiayapicioglu/kernelf83a444f4f\">Matplotlib with the dataset Google Play Store Apps</a>\n\n<a href=\"https://www.kaggle.com/rabiayapicioglu/kernelf83a444f4f\">Matplotlib 2 with the dataset Diamonds</a>\n\nYou'll find seaborn explanations in the following kernels.<br>\n\n<a href=\"https://www.kaggle.com/rabiayapicioglu/kernelaeaee35323\">Matplotlib and Seaborn with the dataset World Happiness Report</a>\n\n<a href=\"https://www.kaggle.com/rabiayapicioglu/kernelb41891ca73\">Matplotlib and Seaborn 2 with the dataset Fatal Police Shootings in US</a>\n\n"},{"metadata":{},"cell_type":"markdown","source":"If you find useful this kernel don't forget to upvote :)Thank you for reading."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}