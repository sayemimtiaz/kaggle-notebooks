{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# <center>COVID-19 Semantic query and Question Answering with Bert and Albert </center>"},{"metadata":{},"cell_type":"markdown","source":"> * Traditional __Literature review__ requires researchers to spend a large amount of time in finding the most desired corpus and dive in to get the wanted idea or result. With the fast evolution of NLP, especially in neural embedding and its application on question answering, we are now able to simplify this problem with a new thought. In this notebook, we are going to present how to use question answering way to accelerate literature review process and achieve satisfying results. Besides, in order to get a better understanding of the whole dataset, we also developed a new pipeline in IR(information retrieval) and proved to be effective in corpus clustering and topic modeling.\n> * To quickly see our results in an interactive table, please [click here](#vis_res)"},{"metadata":{},"cell_type":"markdown","source":"## Content\n- 1. [Introduction](#introduction)\n* 2. [Methodology](#methodology)\n* 3. [Model detail](#model_detail)\n    * 3.1 [Data preprocess](#data_preprocess)\n    * 3.2 [Text data parsing](#data_parsing)\n    * 3.3 [Corpus query and generate candidate answers](#qa)\n        *  3.3.1[Model comparison: Bert vs Albert](#model_comparison)\n        *  3.3.2[Batchly compute the answers leverage GPU](#batch_gpu)\n        *  3.3.3[Generate score for answer sorting and show interactive result](#interactive_res)\n        *  3.3.4[Noun phrase/ Subject term extraction from candidate answers](#noun_subject_extract)\n    * 3.4 [Corpus clustering and topic modeling](#corpus_clustering)\n        *  3.4.1 [Feature generation](#feature_generation)\n        *  3.4.2 [Network drawing](#network_drawing)\n        *  3.4.3 [Clustering](#clustering)\n        *  3.4.4 [Layout adjustment](#layout_adjustment)\n        *  3.4.5 [Topic modeling](# Topic modeling)\n* 4. [Result visualization](#result_vis)\n* 5. [Discussion](#discussion)\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 1. Introduction<a id='introduction'></a>"},{"metadata":{},"cell_type":"markdown","source":"* With the increasing size of information and literature database, literature review is becoming increasingly time consuming and demanding, expecially under the case of global spread of Covid-19. In order to simplify and accelerate the speed of researcher finding the most desired ideas, we propose to use Albert(A lite version Bert proposed at the end of 2019) model fine-tuned with SQuAD database to crack this task. Our goal is to not only query the whole literature database and give back candidate paper list, but leverage fine-tuned Albert's ability in answer extracting and give back user a quick answer for each candidate paper. *__Through multiple task comparsion, we find the fine-tuned Albert achieved better result in answering our task based on given corpus,__* the result will be present shortly. \n* Besides, inspired by Neural embedding's widely use in language modeling and feature generation with superior computational power, we proposed a feature generation (both lexical and semantic feature), network drawing, clustering and topic modeling pipeline to visually explore the literature understanding and all the related information retrieval application. "},{"metadata":{},"cell_type":"markdown","source":"## 2. Methodology <a id='methodology'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Borrow the idea of Question Answering in literature review task"},{"metadata":{},"cell_type":"markdown","source":"<img src='http://www.mccormickml.com/assets/BERT/SQuAD/start_token_classification.png' width=450 height=450 style='display:inline'>\n<img src='http://www.mccormickml.com/assets/BERT/SQuAD/end_token_classification.png' width=350 height=350 style='display:inline'>"},{"metadata":{},"cell_type":"markdown","source":"* The way __Bert__ and __Albert__ works in question answering is to actually find two points, start and end, inside the given context based on the assumption that the satisfying result could directly appears in the given text and there is no need to answering question though combining different information got from different places together.\n* And the result is softmax format, which gives each token a float to represent their probability to serve as a start or end point. Here, by choosing the largest possibility of start and end points, we are able to get a valid answer for the specific task given specific context.  \n* In order to make this confidence level comparable, we further normalized this score with the length of its corresponding corpus and enables us to rank the answers which has the best score."},{"metadata":{},"cell_type":"markdown","source":"## 3. Model Detail<a id='model_detail'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Data preprocessing<a id='data_preprocess'></a>\n"},{"metadata":{},"cell_type":"markdown","source":"* Read in meta data and view its structure"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"### Data preprocess\nimport os\nimport pandas as pd\nimport warnings\nwarnings.simplefilter('ignore')\n##############################################\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\n### Readin meta data\nmetadata_path = f'{root_path}/metadata.csv'\nmetadata = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'doi': str\n})\nmetadata.set_index('sha', inplace=True);\nmetadata.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here, we define a file_scan function to extract all the json corpus file from the root directory, the details and sample usage can be found inside the below cell."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"### Helper function in data preprocess\n\ndef file_scan(root_path, subfolder = None):\n    '''\n    Function used to scan all data files inside the the directory \n    @Param: \n        root_path: str\n        subfolder: str\n            Determine whether to scan all files under different folder or one specific folder\n    return:\n        res: list of all json files in file_name (no '.json' fix)\n    '''\n    if subfolder:\n        root_path = root_path + os.sep + subfolder\n    res = []\n    for sub_path in os.listdir(root_path):\n        tmp_path = root_path+os.sep + sub_path\n        if os.path.isdir(tmp_path):\n            res += file_scan(tmp_path)\n        elif os.path.isfile(tmp_path) and tmp_path.endswith(\".json\"):\n            res.append(tmp_path)\n        else: \n            continue\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"### function file_scan use example:\nnon_comm_files = file_scan(root_path, 'noncomm_use_subset')\nbiorxiv_medrxiv = file_scan(root_path, 'biorxiv_medrxiv')\ncomm_use_subset = file_scan(root_path, 'comm_use_subset')\nprint(f'The number of files in non-comm folder: {len(comm_use_subset)}')\nprint(f'format example: \\n {comm_use_subset[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Paper data parsing <a name='data_parsing'></a>"},{"metadata":{},"cell_type":"markdown","source":"* Here, in data parsing. we first define several helper function in removing NonAscii char, remove punctuation, extract stem and lemmatizer.\n* Thes function are useful in extracting lexical and semantic feature"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"### Helper function and class object for data parsing\n\nimport numpy as np\nimport string\nimport json\nimport nltk\nnltk.download('punkt')\n############################################\n\ndef remove_nonascii(string):\n    '''\n    @Param: \n        string: str\n            original string, might contains Non-Ascii char\n        \n    return:\n        un-named: str\n            cleaned string withour non-ascii\n    '''\n    return \"\".join(i for i in string if ord(i) < 128)\n\n'''\nMove the definition of punctuation, stopwords, stemmer, lemmatizer outside of function \nto avoid redundent IO\n'''\npunctuation = string.punctuation\nstopwords = nltk.corpus.stopwords.words('english')\nstemmer = nltk.stem.porter.PorterStemmer()\nlemmatizer = nltk.WordNetLemmatizer()\n\ndef text_processor(text):\n    '''\n    Processing original text, remove punctuation, extract stem and furfill lemmatizer\n    \n    @Param: \n        text: str\n            \n    return:\n        un-named: str\n    '''\n    text = remove_nonascii(text)\n    if len(text) == 0:\n        return text\n    tok = nltk.word_tokenize(text)\n    refined_words = []\n    translator = str.maketrans('', '', punctuation)\n    for word in tok:\n        word = str(word).translate(translator)\n        word = word.lower()\n        if (2 <= len(word) <= 40) and (word not in stopwords) and (not word.isdigit()):\n            refined_words.append(word)\n    stemmed_words = []\n    for word in refined_words:\n        if word == \"aeds\" or word == \"aed\":\n            continue\n        word = stemmer.stem(word)\n        word = lemmatizer.lemmatize(word)\n        if (2 <= len(word) <= 40) and (word not in stopwords):\n            stemmed_words.append(word)\n    refined_text = \"\"\n    for word in stemmed_words:\n        refined_text += word\n        refined_text += \" \"\n    refined_text = refined_text.strip()\n    refined_text += \". \"\n    return refined_text\n\n\n######################\nclass File(object):\n    '''\n    The constuctor of File requires pre_readin of metadata\n    '''\n    def __init__(self, file_path):\n        with open(file_path, 'r') as f:\n            file_data = json.load(f)\n        self.paper_id = file_data.get('paper_id') or \"UK_id\"\n        self.title = file_data.get('metadata').get('title') if file_data.get('metadata') and file_data.get('metadata').get('title') else \"UK_title\"\n        self.abstract = [i['text'] for i in file_data['abstract']] if file_data.get('abstract') else []\n        self.body_text = [i['text'] for i in file_data['body_text']] if file_data.get('body_text') else []\n        \n        self.authors = [f'{i[\"first\"]} {i[\"last\"]}' for i in file_data['metadata']['authors']] if file_data.get('metadata') and file_data.get('metadata').get('authors') else []\n        try: \n            meta = metadata.loc[self.paper_id]\n            self.pubmed_id = meta['pubmed_id'] if not pd.isnull(meta['pubmed_id']) else \"\"\n            self.publish_time = meta['publish_time'] if not pd.isnull(meta['publish_time']) else \"\"\n            self.journal = meta['journal'] if not pd.isnull(meta['journal']) else \"\"\n        except:\n            self.pubmed_id = \"\"\n            self.publish_time = \"\"\n            self.journal = \"\"\n            \n    def __call__(self):\n        return {\n            'paper_id': self.paper_id,\n            'title': self.title,\n            'abstract': self.abstract,\n            'body_text': self.body_text,\n            'authors': self.authors,\n            'pubmed_id':self.pubmed_id,\n            'publish_time':self.publish_time,\n            'journal': self.journal\n            \n        }\n\n\ndef file_parse(root_path, subfolder = None, save_to_file=True, show_progress=True):\n    '''\n    Generate cleaned papar abstract, title, body text dataframe for later use\n    \n    Params:\n        root_path: string \n            Indicate the main folder of file\n        subfolder: string\n            Indicate whether to parse one folder or whole data\n        save_to_file: bool\n            Indicate whether to save it to local environment excel file\n    \n    Return:\n        df_covid: pandas.DataFrame\n            Return the parsed dataframe for quick use\n    '''\n    target_files = file_scan(root_path, subfolder)\n    \n    dict = {\n        'paper_id': [],\n        'abstract_raw': [], \n        'body_text': [], \n        'authors': [], \n        'title_raw': [], \n        'title_all':[],\n        'journal': [], \n        'abstract_all': [],\n        'pubmed_id': [],\n        'publish_time': [],\n    }\n    for idx, path in enumerate(target_files):\n        try:\n            if idx % (int(len(target_files) // 10)) == 0 and show_progress:\n                print(f'{round(idx / (len(target_files)) * 100)}% have finished');\n            file = File(path)\n            dict['paper_id'].append(file()['paper_id'])\n            dict['journal'].append(file()['journal'])\n            dict['pubmed_id'].append(file()['pubmed_id'])\n            dict['publish_time'].append(file()['publish_time'])\n            dict['authors'].append(','.join(file()['authors']))\n\n            dict['title_raw'].append(remove_nonascii(file()['title']).replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace(\"\\\\\", \"-\").replace(\"\\n\", \" \").strip())\n            ### Original title, de-NonAscii, de-some bothering char\n\n            dict['title_all'].append(text_processor(file()['title'].strip())) \n            ### Extract the stem and lemmetize the original title\n\n            dict['body_text'].append('\\n'.join(file()['body_text'])) \n            dict['abstract_raw'].append(remove_nonascii(\".\".join(file()['abstract'])).replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace(\"\\\\\", \"-\").replace(\"\\n\", \" \").strip())\n            dict['abstract_all'].append(text_processor(\".\".join(file()['abstract']).strip()))\n        except:\n            print(f\"id: {idx}, path: {path} readin fails, please check\")\n            continue;\n    print('All parsing finished!');\n    df_covid = pd.DataFrame(dict, columns = ['paper_id',\n        'abstract_raw', \n        'body_text', \n        'authors', \n        'title_raw', \n        'title_all',\n        'journal', \n        'abstract_all',\n        'pubmed_id',\n        'publish_time',])\n    if save_to_file:\n        filename = f\"df_covid_{subfolder}.xlsx\"\n        df_covid.to_excel(filename)\n    return df_covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sample usage of parsing the data file under the biorxiv_medrxiv folder"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"### Sample usage of file parser with biorxiv_medrxiv sub directory\n# Around one minute\nbiorxiv_medrxiv = file_parse(root_path, subfolder='biorxiv_medrxiv', show_progress=False)\nbiorxiv_medrxiv.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Corpus query and generate candidate answers <a name='qa'></a> "},{"metadata":{},"cell_type":"markdown","source":"#### 3.3.1 Model comparison: Bert vs Albert"},{"metadata":{},"cell_type":"markdown","source":"* Albert and Bert are both bidirectional tranformer structure and achieving satisfying results in question answering task, here, before we finally move into Albert, we compared their results in some sample task.\n* Here, given task __\"which movement strategy can efficiently prevent secondary transmission in community settings?\"__  and one same paper which talking about quarantine and the necessity of wearing facemask, we can see that both of these two models give back exactly same answer, which showing their ability in Give Back Quick Answer In Literature Reviewing  "},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"### Import Bert, Albert and configured for GPU accelerator if possible\nfrom transformers import (BertForQuestionAnswering,BertTokenizer)\nfrom transformers import (AlbertForQuestionAnswering, AlbertTokenizer)\nimport torch\nimport scipy\n#################################################\n### Load pretrained Bert large finetuned with SQuAD Dataset for Q&A\npretrained_bert_version = 'bert-large-uncased-whole-word-masking-finetuned-squad'\nmodel_bert = BertForQuestionAnswering.from_pretrained(pretrained_bert_version)\nmodel_bert_tokenizer = BertTokenizer.from_pretrained(pretrained_bert_version)\n\n### Load pretrained AlBert xlarge finetuned with SQuAD Dataset for Q&A\nmodel_name_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\nmodel_albert = AlbertForQuestionAnswering.from_pretrained(model_name_path)\nmodel_albert_tokenizer = AlbertTokenizer.from_pretrained(model_name_path)\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    model_bert.cuda()\n    model_albert.cuda()\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"# Test result of Bert and Albert with one task and paper abstract:\nquestion = \"which movement strategy can efficiently prevent secondary transmission in community settings?\"\ncorpus_text = \"Time variations in transmission potential have rarely been examined with regard to pandemic influenza. This paper reanalyzes the temporal distribution of pandemic influenza in Prussia, Germany, from 1918-19 using the daily numbers of deaths, which totaled 8911 from 29 September 1918 to 1 February 1919, and the distribution of the time delay from onset to death in order to estimate the effective reproduction number, Rt, defined as the actual average number of secondary cases per primary case at a given time..A discrete-time branching process was applied to back-calculated incidence data, assuming three different serial intervals (i.e. 1, 3 and 5 days). The estimated reproduction numbers exhibited a clear association between the estimates and choice of serial interval; i.e. the longer the assumed serial interval, the higher the reproduction number. Moreover, the estimated reproduction numbers did not decline monotonically with time, indicating that the patterns of secondary transmission varied with time. These tendencies are consistent with the differences in estimates of the reproduction number of pandemic influenza in recent studies; high estimates probably originate from a long serial interval and a model assumption about transmission rate that takes no account of time variation and is applied to the entire epidemic curve..The present findings suggest that in order to offer robust assessments it is critically important to clarify in detail the natural history of a disease (e.g. including the serial interval) as well as heterogeneous patterns of transmission. In addition, given that human contact behavior probably influences transmissibility, individual countermeasures (e.g. household quarantine and maskwearing) need to be explored to construct effective non-pharmaceutical interventions.\"\n\n\ndef quick_answer_test(question, corpus, model, tokenizer, device, sep=\"\", show_tokens=False):\n    if str(model).startswith('Albert'):\n        model_name='Albert'\n        sep = '▁'\n    elif str(model).startswith('Bert'):\n        model_name='Bert'\n        sep = ' ##'\n    else:\n        model_name='Unknown'\n        \n    answer_text = corpus\n    input_ids = tokenizer.encode(question, answer_text)\n    print(f'The {model_name} tokenizer find the input has a total of {len(input_ids)} tokens.')\n\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    if show_tokens: \n        for token, id in zip(tokens, input_ids):\n\n            # If this is the [SEP] token, add some space around it to make it stand out.\n            if id == tokenizer.sep_token_id:\n                print('')\n\n            # Print the token string and its ID in two columns.\n            print('{:<12} {:>6,}'.format(token, id))\n\n            if id == tokenizer.sep_token_id:\n                print('')\n    \n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(model_bert_tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n    \n    # device = torch.device(\"cpu\")\n    input_ids = torch.tensor(input_ids).to(device)\n    segment_ids = torch.tensor(segment_ids).to(device)\n    \n    start_scores, end_scores = model(input_ids.reshape(1,-1), # The tokens representing our input text.\n                                 token_type_ids=segment_ids.reshape(1,-1)) # The segment IDs to differentiate question from answer_text\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Combine the tokens in the answer and print it out.\n    answer = ' '.join(tokens[answer_start:answer_end+1]).replace(sep, ' ').replace(' ,',',').replace(' .','.')\n    \n    return answer\n\nprint('Answer generated from Albert:', quick_answer_test(question, corpus_text, model_albert, model_albert_tokenizer, device=device))\nprint('-'*20)\nprint('Answer generated from Bert:', quick_answer_test(question, corpus_text, model_bert, model_bert_tokenizer, device=device))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* However, when we actually use them on Biorxiv sub dataset, their difference starts to appear. \n* For the same task and same dataset, we find that the Albert give back us 47 valid answers, and though a quick view, we could easily tell that they are all very related with the give task, __Actually, the result Albert given us is really good__;\n* Howevere, when we look at the answers generated by Bert,  we find the model actually find over 200 valid answers,but if quick view could tell us that there are a lot of bad ansers such as \"Covid\",\"RNN\".etc\n* Throught some more testing we find that the answer given by Albert is more strict and more relevent to the given task. So we move to Albert"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"### Function used to quickly compare the result of Bert vs. Albert \n\ndef question_answering_method(task, corpus, abstract_or_not = True, model = model_albert, tokenizer=model_albert_tokenizer):\n    '''\n    Check whether one specific task could be handled with by specific corpus list, and give back the candidate answers for the specific task\n    \n    @Params:\n        task: str\n            String type variable used to describe the task or question\n        \n        corpus: pandas.DataFrame\n            DataFrame contains each paper's informaton about Title, Abstract, Journal, Author, ID and so on\n            \n        abstract_or_not: bool\n            Indicate whether to use abstract or just use title to find answer\n            \n        model: transformer model, by default is albert\n        \n        tokenizer: transformer tokenizer, by default is alberttokenizer\n        \n    return:\n        corpus: pandas.DataFrame\n            Updated corpus  dataframe has answer, score, can_handle_flag added for each paper\n            \n    '''\n    corpus['start_score_max_index'] = np.zeros(corpus.shape[0])\n    corpus['start_score_max'] = np.zeros(corpus.shape[0])\n    corpus['can_handle_flag'] = np.zeros(corpus.shape[0])\n    corpus['end_score_max_index'] = np.zeros(corpus.shape[0])\n    corpus['end_score_max'] = np.zeros(corpus.shape[0])\n    corpus['answer'] = np.array(['']*corpus.shape[0])\n    corpus['start_score_prob'] = np.array(corpus.shape[0])\n    \n    if str(model).startswith('Albert'):\n        sep = '▁'\n    elif str(model).startswith('Bert'):\n        sep = ' ##'\n    else: \n        print('Model given is not supported right now')\n        return \n    for row_nu in range(corpus.shape[0]):\n        tmp = corpus.iloc[row_nu]\n        answer_text = \"\"\n        answer_text += tmp['title_raw'] if not pd.isnull(tmp['title_raw']) else \"\"\n        answer_text += tmp['abstract_raw'] if abstract_or_not and not pd.isnull(tmp['abstract_raw']) else \"\"\n\n        # Apply the tokenizer to the input text, treating them as a text-pair.\n        input_ids = tokenizer.encode(task, answer_text, max_length=512)\n\n        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n        # Search the input_ids for the first instance of the `[SEP]` token.\n        sep_index = input_ids.index(tokenizer.sep_token_id)\n\n        # The number of segment A tokens includes the [SEP] token istelf.\n        num_seg_a = sep_index + 1\n\n        # The remainder are segment B.\n        num_seg_b = len(input_ids) - num_seg_a\n\n        # Construct the list of 0s and 1s.\n        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n        # There should be a segment_id for every input token.\n        assert len(segment_ids) == len(input_ids)\n\n        # Move the target data to GPU\n        input_ids = torch.tensor(input_ids).to(device)\n        segment_ids = torch.tensor(segment_ids).to(device)\n\n        # Run our example through the model.\n        start_scores, end_scores = model(input_ids.reshape(1,-1), # The tokens representing our input text.\n                                 token_type_ids=segment_ids.reshape(1,-1)) # The segment IDs to differentiate question from answer_text\n        \n\n        start_scores = start_scores.detach().to('cpu')\n        end_scores = end_scores.detach().to('cpu')\n\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)\n\n        if answer_start.item() > sep_index+1 and answer_end.item() >= answer_start.item() and answer_end.item() < len(tokens) - 1:\n            corpus.loc[row_nu, 'can_handle_flag'] = 1\n            corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n            corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n            corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n            corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n            if str(model).startswith('Albert'):\n                corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n            else:\n                corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n            corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n        else:\n            corpus.loc[row_nu, 'can_handle_flag'] = -1\n            corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n            corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n            corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n            corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n            if str(model).startswith('Albert'):\n                corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n            else:\n                corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n            corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n    return corpus.copy()\n\ndef answer_check(ans, return_answer=False, show = True):\n    ans = ans[ans['can_handle_flag'] == 1].sort_values('start_score_prob', ascending=False)\n    answers = []\n    \n    for i in ans['answer']:\n        if show:\n            print(i.replace('<unk>',''))\n        answers.append(i.replace('<unk>',''))\n    if return_answer:\n        return answers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"albert_bio_answers = question_answering_method(task='which are movement control strategies can efficiently prevent secondary transmission in community settings?', corpus=biorxiv_medrxiv, abstract_or_not = True, model = model_albert, tokenizer=model_albert_tokenizer)\nbert_bio_answers = question_answering_method(task='which are movement control strategies can efficiently prevent secondary transmission in community settings?', corpus=biorxiv_medrxiv, abstract_or_not = True, model = model_bert, tokenizer=model_bert_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"print('Answer of Albert on Bioxriv dataset')\nprint('='*20)\nanswer_check(albert_bio_answers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"print('Answer of Bert on Bioxriv dataset:')\nprint('='*20)\nanswer_check(bert_bio_answers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2 Batchly compute the answers leverage GPU<a id='batch_gpu'></a>\n\n<hr>\n* When we use Albert, we actually generate only a few things for each specific paper.\n    * _start_score_max_index_: indicates where the model think the correct answer should appear, and if model think there is no valid answer, [CLS] will return\n    * _answer_: the answer Albert generated between the most likely start node and end node\n    * _start_score_max_: serve as confidence level to let us know how possible the model think it can answer the give question with one specific start or end node"},{"metadata":{},"cell_type":"markdown","source":"* For quick usage, calculating results in batches for a task list, please check the modulized code below\n* We modulized the revised version of batchly compute the candidate answer and score, and modulized it into the EmbeddingSearch Class"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_kg_hide-output":true},"cell_type":"code","source":"class EmbeddingSearch(object):\n    \n    def __init__(self, config={}):\n        '''\n        @Param: \n            config: dict that may contains following keys \n                model: default is AlBertForQuestionAnswering\n                tokenizer: default is AlBertTokenizer \n                pretrained_version: default is 'bert-large-uncased-whole-word-masking-finetuned-squad'\n                dataset: pandas.DataFrame\n                gpu: bool,use or not, by default is false\n        return:\n            Embedding object which could be used to batchly calculate candidate answers for task lisk\n        '''\n        self.model = config.get('model') or AlbertForQuestionAnswering # from transformer\n        self.tokenizer = config.get('tokenizer') or AlbertTokenizer # from transformer\n        self.pretrained_version = config.get('pretrained_version') or 'ktrapeznikov/albert-xlarge-v2-squad-v2'\n        try:\n            self.model = self.model.from_pretrained(self.pretrained_version)\n            self.tokenizer = self.tokenizer.from_pretrained(self.pretrained_version)\n        except:\n            return f'Pretrained-weights importing fail'\n        \n#         assert config.get('dataset')\n        ## Tell whether dataset is qualified\n        \n        self.dataset = config.get('dataset') or None\n        \n        self.gpu_flag = config.get('gpu') or True\n        \n        if self.gpu_flag:\n            if torch.cuda.is_available():    \n\n                # Tell PyTorch to use the GPU.    \n                self.device = torch.device(\"cuda\")\n\n                print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n                print('We will use the GPU:', torch.cuda.get_device_name(0))\n                self.model.cuda()\n            # If not...\n            else:\n                print('No GPU available, using the CPU instead.')\n                self.device = torch.device(\"cpu\")\n                \n        self.trained_saved_inside = {}      \n        if str(self.model).startswith('Albert'):\n            self.model_name='albert'\n        elif str(self.model).startswith('Bert'):\n            self.modle_name='bert'\n        else:\n            print('Model unknown')\n            return\n    def qa_test(self, question, answer_text, show_tokens=False):\n        '''\n        \n        '''\n        # Apply the tokenizer to the input text, treating them as a text-pair.\n        input_ids = self.tokenizer.encode(question, answer_text, max_length=512)\n        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n        \n        print('The input has a total of {:} tokens.'.format(len(input_ids)))\n        \n        if show_tokens:\n            for token, id in zip(tokens, input_ids):\n                # If this is the [SEP] token, add some space around it to make it stand out.\n                if id == self.tokenizer.sep_token_id:\n                    print('')\n\n                # Print the token string and its ID in two columns.\n                print('{:<12} {:>6,}'.format(token, id))\n\n                if id == self.tokenizer.sep_token_id:\n                    print('')\n        sep_index = input_ids.index(tokenizer.sep_token_id)\n\n        # The number of segment A tokens includes the [SEP] token istelf.\n        num_seg_a = sep_index + 1\n\n        # The remainder are segment B.\n        num_seg_b = len(input_ids) - num_seg_a\n\n        # Construct the list of 0s and 1s.\n        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n        # There should be a segment_id for every input token.\n        assert len(segment_ids) == len(input_ids)\n        \n        # device = torch.device(\"cpu\")\n        input_ids = torch.tensor(input_ids).to(self.device)\n        segment_ids = torch.tensor(segment_ids).to(self.device)\n        \n        start_scores, end_scores = self.model(input_ids.reshape(1,-1), # The tokens representing our input text.\n                                 token_type_ids=segment_ids.reshape(1,-1))\n        start_scores = start_scores.to('cpu')\n        end_scores = end_scores.to('cpu')\n        \n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)\n\n        # Combine the tokens in the answer and print it out.\n        answer = ' '.join(tokens[answer_start:answer_end+1])\n\n        return 'Answer: \"' + answer + '\"'\n        \n    def find_task_answer_from_corpus(self, task, corpus, abstract_or_not = True, save_inside = True, save_to_file=True, dataset_name='undefined', task_index='undefined'):\n        '''\n        Check whether one specific task could be handled with one specific corpus\n        '''\n        corpus = corpus if corpus is not None else self.dataset\n        corpus['start_score_max_index'] = np.zeros(corpus.shape[0])\n        corpus['start_score_max'] = np.zeros(corpus.shape[0])\n        corpus['can_handle_flag'] = np.zeros(corpus.shape[0])\n        corpus['end_score_max_index'] = np.zeros(corpus.shape[0])\n        corpus['end_score_max'] = np.zeros(corpus.shape[0])\n        corpus['answer'] = np.array(['']*corpus.shape[0])\n        corpus['start_score_prob'] = np.array(corpus.shape[0])\n        \n        if str(self.model).startswith('Albert'):\n            sep = '▁'\n        elif str(self.model).startswith('Bert'):\n            sep = ' ##'\n        else: \n            print('Model given is not supported right now')\n            return \n        \n        for row_nu in range(corpus.shape[0]):\n            tmp = corpus.iloc[row_nu]\n            answer_text = \"\"\n            answer_text += tmp['title_raw'] if not pd.isnull(tmp['title_raw']) else \"\"\n            answer_text += tmp['abstract_raw'] if abstract_or_not and not pd.isnull(tmp['abstract_raw']) else \"\"\n            \n            # Apply the tokenizer to the input text, treating them as a text-pair.\n            input_ids = self.tokenizer.encode(task, answer_text, max_length=512)\n\n            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n\n            # Search the input_ids for the first instance of the `[SEP]` token.\n            sep_index = input_ids.index(self.tokenizer.sep_token_id)\n\n            # The number of segment A tokens includes the [SEP] token istelf.\n            num_seg_a = sep_index + 1\n\n            # The remainder are segment B.\n            num_seg_b = len(input_ids) - num_seg_a\n\n            # Construct the list of 0s and 1s.\n            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n            # There should be a segment_id for every input token.\n            assert len(segment_ids) == len(input_ids)\n\n            # Move the target data to GPU\n            input_ids = torch.tensor(input_ids).to(self.device)\n            segment_ids = torch.tensor(segment_ids).to(self.device)\n\n            # Run our example through the model.\n            start_scores, end_scores = self.model(input_ids.reshape(1,-1), # The tokens representing our input text.\n                                     token_type_ids=segment_ids.reshape(1,-1)) # The segment IDs to differentiate question from answer_text\n\n\n            start_scores = start_scores.detach().to('cpu')\n            end_scores = end_scores.detach().to('cpu')\n\n            answer_start = torch.argmax(start_scores)\n            answer_end = torch.argmax(end_scores)\n\n            if answer_start.item() > sep_index+1 and answer_end.item() >= answer_start.item() and answer_end.item() < len(tokens) - 1:\n                corpus.loc[row_nu, 'can_handle_flag'] = 1\n                corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n                corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n                corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n                corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n                if str(self.model).startswith('Albert'):\n                    corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n                else:\n                    corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n                corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n            else:\n                corpus.loc[row_nu, 'can_handle_flag'] = -1\n                corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n                corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n                corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n                corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n                if str(self.model).startswith('Albert'):\n                    corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n                else:\n                    corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n                corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n                \n        self.trained_saved_inside[task_index] = corpus.copy() if save_inside else None\n        if save_to_file:\n            corpus.to_excel(f'{task_index}_{dataset_name}_{self.model_name}.xlsx')\n        return corpus\n    \n    def batch_find_task_answer_from_corpus(self,corpus, task_list, abstract_or_not = True, save_inside = True, show_output=True,answer_check_algo=answer_check,dataset_name='undefined'):\n        '''\n        Calculate the answer of task, based on task_list with the given model\n\n        Param: \n            model:\n                transformer-Bert/Albert\n            tokenizer:\n\n            corpus: pandas.DataFrame\n\n            task_list: Dict \n                {'task_index': 'task_detail'}\n\n            show_corpus: bool\n                True for default\n\n            output: bool\n                True for default\n\n            answer_check_algo: function\n                function given to check the result after calulation\n\n        Return:\n            res: Dict\n                {'task_index': pandas.DataFrame}\n        '''\n        start_function = time.time()\n        res = {task: None for task in task_list.keys()}\n\n\n        for task_index, task in task_list.items():\n            tmp_res = self.find_task_answer_from_corpus(task, corpus, abstract_or_not = abstract_or_not, save_inside = save_inside, save_to_file=True, dataset_name=dataset_name, task_index=task_index)\n            if not save_inside:\n                res[task_index] = tmp_res.copy() #.copy is an absolute must\n            print(f'===================={task} has finished====================')\n            if show_output:\n                answer_check(tmp_res)\n        end_function = time.time()\n        print(f'Total Time Consumed is {end_function-start_function}')\n        if not save_inside:\n            return res\n        else: \n            return self.trained_saved_inside\n    \n    def trained_saved_inside_clear():\n        self.trained_saved_inside = {}\n        \n    @staticmethod\n    def question_answering_method(task, corpus, abstract_or_not = True, model = model_albert, tokenizer=model_albert_tokenizer):\n        '''\n        Check whether one specific task could be handled with one specific corpus\n        '''\n        corpus['start_score_max_index'] = np.zeros(corpus.shape[0])\n        corpus['start_score_max'] = np.zeros(corpus.shape[0])\n        corpus['can_handle_flag'] = np.zeros(corpus.shape[0])\n        corpus['end_score_max_index'] = np.zeros(corpus.shape[0])\n        corpus['end_score_max'] = np.zeros(corpus.shape[0])\n        corpus['answer'] = np.array(['']*corpus.shape[0])\n        corpus['start_score_prob'] = np.array(corpus.shape[0])\n\n        if str(model).startswith('Albert'):\n            sep = '▁'\n        elif str(model).startswith('Bert'):\n            sep = ' ##'\n        else: \n            print('Model given is not supported right now')\n            return \n        for row_nu in range(corpus.shape[0]):\n            tmp = corpus.iloc[row_nu]\n            answer_text = \"\"\n            answer_text += tmp['title_raw'] if not pd.isnull(tmp['title_raw']) else \"\"\n            answer_text += tmp['abstract_raw'] if abstract_or_not and not pd.isnull(tmp['abstract_raw']) else \"\"\n\n            # Apply the tokenizer to the input text, treating them as a text-pair.\n            input_ids = tokenizer.encode(task, answer_text, max_length=512)\n\n            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n            # Search the input_ids for the first instance of the `[SEP]` token.\n            sep_index = input_ids.index(tokenizer.sep_token_id)\n\n            # The number of segment A tokens includes the [SEP] token istelf.\n            num_seg_a = sep_index + 1\n\n            # The remainder are segment B.\n            num_seg_b = len(input_ids) - num_seg_a\n\n            # Construct the list of 0s and 1s.\n            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n            # There should be a segment_id for every input token.\n            assert len(segment_ids) == len(input_ids)\n\n            # Move the target data to GPU\n            input_ids = torch.tensor(input_ids).to(device)\n            segment_ids = torch.tensor(segment_ids).to(device)\n\n            # Run our example through the model.\n            start_scores, end_scores = model(input_ids.reshape(1,-1), # The tokens representing our input text.\n                                     token_type_ids=segment_ids.reshape(1,-1)) # The segment IDs to differentiate question from answer_text\n\n\n            start_scores = start_scores.detach().to('cpu')\n            end_scores = end_scores.detach().to('cpu')\n\n            answer_start = torch.argmax(start_scores)\n            answer_end = torch.argmax(end_scores)\n\n            if answer_start.item() > sep_index+1 and answer_end.item() >= answer_start.item() and answer_end.item() < len(tokens) - 1:\n                corpus.loc[row_nu, 'can_handle_flag'] = 1\n                corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n                corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n                corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n                corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n                if str(model).startswith('Albert'):\n                    corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n                else:\n                    corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n                corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n            else:\n                corpus.loc[row_nu, 'can_handle_flag'] = -1\n                corpus.loc[row_nu, 'start_score_max_index'] = answer_start.item()\n                corpus.loc[row_nu, 'end_score_max_index'] = answer_end.item()\n                corpus.loc[row_nu, 'start_score_max'] = torch.max(start_scores).item()\n                corpus.loc[row_nu, 'end_score_max'] = torch.max(end_scores).item()\n                if str(model).startswith('Albert'):\n                    corpus.loc[row_nu, 'answer'] =  ''.join(tokens[answer_start:answer_end+1]).replace('▁', ' ').replace(' ,',',').replace(' .','.')\n                else:\n                    corpus.loc[row_nu, 'answer'] =  ' '.join(tokens[answer_start:answer_end+1]).replace(sep, '').replace(' ,',',').replace(' .','.')\n                corpus.loc[row_nu, 'start_score_prob'] = scipy.special.softmax(start_scores.reshape(-1).detach().numpy()).max()\n        return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"###  EmbeddingSearch usage example \ntask_list = {\n    'task_1': \"What is known about transmission, incubation, and environmental stability of corona virus?\",\n    'task_2': \"Range of incubation periods for the disease in humans\",\n    'task_3': \"Persistence of corona virus on surfaces of different materials\",\n    'task_4': \"What are natural history of the virus and shedding of it from an infected person?\",\n    'task_5': \"What is corona virus' seasonality of transmission?\",\n    'task_6': \"What are the implementation of diagnostics and products to improve clinical processes?\",\n    'task_7': \"What is corona virus' immune response and immunity?\", \n    'task_8': \"Which are movement control strategies can efficiently prevent secondary transmission in health care?\",\n    'task_9': \"Which are movement control strategies can efficiently prevent secondary transmission in community settings\",\n    'task_10': \"What is the role of the environment in transmission\"\n}\nbio_embedding_search = EmbeddingSearch()\n### Not run considering the time consuming(5~10 minutes)\n# bio_embedding_search.batch_find_task_answer_from_corpus(corpus=biorxiv_medrxiv, task_list=task_list, abstract_or_not = True, save_inside = True, show_output=True,answer_check_algo=answer_check,dataset_name='biorxv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.3 Generate score for answer sorting and show interactive result <a id='interactive_res'></a>"},{"metadata":{},"cell_type":"markdown","source":"* Directly loading the generated answers generated with the task list and the whole COVID-19 Corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Load the file contains all the pretrained result\nboosted_all = pd.read_excel('../input/keywords-addon-boosted/v2valid_all.xlsx')\n### Fillin possible empty cell with 'UK'\nboosted_all.fillna('UK', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Generate and turn confidence into comparable score\n* Here, what we find is that the confidence is actually not comparable directly, since some good answer good appears at a very large corpus, which makes its confidence score really low.\n* And thee high score could only because their appearance in a very short corpus.\n\n* Thus we propose to Normalize the score with the length of the corpus.Here, L is the length of  the corpus in token number.\n<br>\n$$ X_{norm} = X_{original} * \\sqrt{L}$$\n\n* However, it could also possibly bonus the long corpus too much. so, directly multiply with the length should be limited with a cap, and we set it to be the average length og all corpus.\n$$ X_{norm+adjust} = X_{original} * \\sqrt{min(L, \\bar{L})}$$"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in range(1,11):\n    cur_task = 'task_'+str(i)\n    boosted_all[cur_task+'_answer'] = boosted_all[cur_task+'_answer'].apply(lambda item:item.strip().capitalize())\n    tmp_abs_length = boosted_all['abstract_raw'].apply(lambda x:len(x))\n    tmp_title_length = boosted_all['title_raw'].apply(lambda x:len(x))\n    tmp_length = tmp_abs_length + tmp_title_length\n    \n    tmp = tmp_length.apply(lambda x:max(np.sqrt(x), 1))\n    tmp_mean = tmp.mean()\n    tmp_mean_adjusted = tmp.apply(lambda x:min(x, tmp_mean))\n    \n    boosted_all[cur_task+'_score_normalized'] = boosted_all[cur_task+'_start_score_prob'] * (tmp_abs_length+tmp_title_length).apply(lambda x:max(np.sqrt(x), 1))\n\n    \nfor i in range(1,11):\n    cur_task = 'task_'+str(i)   \n    boosted_all[cur_task+'_score_normalized'] = boosted_all[cur_task+'_start_score_prob'] * tmp\n    boosted_all[cur_task+'_score_normalized_adjusted'] = boosted_all[cur_task+'_start_score_prob'] * tmp_mean_adjusted\n\n### Round the score result for display\nfor i in range(1,11):\n    cur_task = 'task_'+str(i)\n    boosted_all[cur_task+'_start_score_prob'] = boosted_all[cur_task+'_start_score_prob'].apply(lambda item:round(item,2))\n    boosted_all[cur_task+'_score_normalized'] = boosted_all[cur_task+'_score_normalized'].apply(lambda item:round(item,2))\n    boosted_all[cur_task+'_score_normalized_adjusted'] = boosted_all[cur_task+'_score_normalized_adjusted'].apply(lambda item:round(item,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interactive result with Bokeh <a id='vis_res'></a>\n* You may find the original confidence level as the standard, the ${X_{norm}}$ as normalized and $X_{norm+adjust}$ as \"norm + adjust\" in score type\n* Click the answer on  the left should show the paper source and where does the answer appear. Click paper's title should link you to the web page to find the full version of paper\n* Additionally, you could visit http://3.91.149.208/ to futhure test it (Apologize for one minute loading )"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"###\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, TableColumn, StringFormatter, Circle, Div, Paragraph, Select,DataTable\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.models.widgets import Slider,Dropdown\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column, row, gridplot\n##############\nboosted_all_dict = boosted_all.to_dict()\ntask_to_index = {value: key for key, value in task_list.items()}\noutput_notebook()\ndata = {\n    'answer': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_answer']),\n    'score':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_start_score_prob']),\n    'index_from_original': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1].index)\n}\n\ndetail_data = ColumnDataSource({\n    'title':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['title_raw']),\n    'answer': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_answer']),\n    'abstract':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['abstract_raw']),\n    'id': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['paper_id'])\n})\n\n\nsource = ColumnDataSource(data)\n\ncolumns = [\n    TableColumn(field='answer', title='Possible Answer', formatter=StringFormatter(font_style=\"bold\")),\n    TableColumn(field='score', title='Score', width=5)\n]\ndata_table =  DataTable(source=source, columns=columns, selectable=True, index_header=\"\", width=500,height=600, fit_columns=True, scroll_to_selection=True, height_policy='auto', editable=True)\n\nselect = Select(title='Task list', value=list(task_list.values())[0], options=list(task_list.values()), height=50, width=420)\n\nscoretype_select = Select(title='Score type', value='standard', options=['standard', 'normalized', 'norm+adjust'], width=80, height=50)\n\n\nscoretype_callback = CustomJS(args = dict(source=source, select = select, task_to_index=task_to_index, boosted_all_dict=boosted_all_dict), code=\"\"\"\n    \n    Object.filter = function( obj, predicate) {\n        var result = {};\n        for (let key in obj) {\n            if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n                result[key] = obj[key];\n            }\n        }\n        return result;\n    };\n    \n    Object.batch_select = function(obj, list){\n        var res = [];\n        for(let key of list){\n            res.push(obj[key]);\n        }\n        return res;\n    };\n    \n    var selected_type = cb_obj.value;\n    var selected_task = select.value;\n    var selected_index = task_to_index[selected_task];\n    var target_indexs = Object.keys(Object.filter(boosted_all_dict[selected_index+'_can_handle_flag'], item => item == 1));\n    var answer = Object.batch_select(boosted_all_dict[selected_index+'_answer'], target_indexs);\n    var score = [];\n    if (selected_type == 'standard'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_start_score_prob'], target_indexs);\n    }else if(selected_type == 'normalized'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized'], target_indexs);\n    }else if(selected_type == 'norm+adjust'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized_adjusted'], target_indexs);\n    }\n    var title = Object.batch_select(boosted_all_dict['title_raw'], target_indexs);\n    var abstract = Object.batch_select(boosted_all_dict['abstract_raw'], target_indexs);\n    var id = Object.batch_select(boosted_all_dict['paper_id'], target_indexs);\n    source.data = {\n        'answer': answer,\n        'score': score,\n        'index_from_original': target_indexs\n    };\n    detail_data.data = {\n       'title':title,\n       'answer':answer,\n       'abstract':abstract,\n       'id': id\n    };\n    source.change.emit();\n    detail_data.change.emit();\n\"\"\")\n\n\ntask_select_callback = CustomJS(args=dict(source=source,detail_data=detail_data, task_to_index=task_to_index, scoretype_select=scoretype_select, boosted_all_dict=boosted_all_dict), code=\"\"\"\n    Object.filter = function( obj, predicate) {\n        var result = {};\n        for (let key in obj) {\n            if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n                result[key] = obj[key];\n            }\n        }\n        return result;\n    };\n    \n    Object.batch_select = function(obj, list){\n        var res = [];\n        for(let key of list){\n            res.push(obj[key]);\n        }\n        return res;\n    };\n    var selected_type = scoretype_select.value;\n    var selected_task = cb_obj.value;\n    var selected_index = task_to_index[selected_task];\n    var target_indexs = Object.keys(Object.filter(boosted_all_dict[selected_index+'_can_handle_flag'], item => item == 1));\n    var answer = Object.batch_select(boosted_all_dict[selected_index+'_answer'], target_indexs);\n    var score = [];\n    if (selected_type == 'standard'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_start_score_prob'], target_indexs);\n    }else if(selected_type == 'normalized'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized'], target_indexs);\n    }else if(selected_type == 'norm+adjust'){\n        score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized_adjusted'], target_indexs);\n    }\n    var title = Object.batch_select(boosted_all_dict['title_raw'], target_indexs);\n    var abstract = Object.batch_select(boosted_all_dict['abstract_raw'], target_indexs);\n    var id = Object.batch_select(boosted_all_dict['paper_id'], target_indexs);\n    source.data = {\n        'answer': answer,\n        'score': score,\n        'index_from_original': target_indexs\n    };\n    detail_data.data = {\n       'title':title,\n       'answer':answer,\n       'abstract':abstract,\n       'id':id\n    };\n    \"\"\") \n    \npaper_detail = Div(text=\"Paper info shows here\", margin=(50,0,10,0), style={'border':'1px solid black', 'width':'400px', 'height':'600px', 'padding':'20px', 'border-top-left-radius':'5px','border-top-right-radius':'5px','text-align':'center','overflow-y':'auto'})\nselect_callback=CustomJS(args=dict(source=source, div=paper_detail, detail_data=detail_data), code=\"\"\"\n    var selection_index=source.selected.indices[0];\n    var answer = detail_data.data['answer'][selection_index];\n    var title = detail_data.data['title'][selection_index];\n    var abstract = detail_data.data['abstract'][selection_index] || \"\";\n    var idlink = \"\";\n    var id = detail_data.data['id'][selection_index];\n    if(id.startsWith(\"PMC\")){\n        idlink = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + id;\n    }else{\n        idlink = 'https://www.semanticscholar.org/paper/' + id;\n    }\n    var index = 0;\n    if(title.toLowerCase().indexOf(answer.toLowerCase()) !== -1){\n        index = title.toLowerCase().indexOf(answer.toLowerCase())\n        title = title.slice(0,index) + `<a href=${idlink} target=\"_blank\" style='text-decoration:none; background: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-image: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-position-x: initial; background-position-y: initial;background-size: initial; background-repeat-x: initial; background-repeat-y: initial; background-attachment: initial;background-origin: initial; background-clip: initial; background-color: initial; margin: 0 0.25em; line-height: 1.5; padding: 0px 3px !important; border-radius: 5rem !important;'><strong>${title.slice(index, index + answer.length)}</strong></a>` + title.slice(index + answer.length);\n    }\n    if(abstract.toLowerCase().indexOf(answer.toLowerCase()) !== -1){\n        index = abstract.toLowerCase().indexOf(answer.toLowerCase());\n        abstract = abstract.slice(0, index) + `<a style='text-decoration:none; background: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-image: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-position-x: initial; background-position-y: initial;background-size: initial; background-repeat-x: initial; background-repeat-y: initial; background-attachment: initial;background-origin: initial; background-clip: initial; background-color: initial; margin: 0 0.25em; line-height: 1.5; padding: 0px 3px !important; border-radius: 5rem !important;'><strong>${abstract.slice(index, index + answer.length)}</strong></a>` + abstract.slice(index + answer.length);\n    }\n    div.text = `<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">`;\n    div.text +=`<h2><a href=${idlink} target=\"_blank\" style='text-decoration:none'><center>${title}&nbsp<i class='fa fa-external-link' style='font-size:15px'></i></center></a></h2>`;\n    div.text += `<p><strong><center>Answer: <a style='color:blue; font-style:italic'>${detail_data.data['answer'][selection_index]}</a></center></strong></p>`;\n    div.text += `<br>`;\n    div.text += `<p><strong><center>Abstract</center></strong></p>`;\n    div.text += `<p>${abstract}</p>`;\n\n\"\"\")\n#    border:1px solid black; border-top-left-radius: 10px; border-top-right-radius: 10px;\n\nsource.selected.js_on_change('indices', select_callback)\nselect.js_on_change('value', task_select_callback)\n\nscoretype_select.js_on_change('value', scoretype_callback)\noutput_file('res.html')\n\ntitle = Div(text = \"<h1><center>Covid-19 Tasks' Candidate Answers<center></h1>\")\nshow(column(title, row(column(row(select,scoretype_select), data_table),paper_detail)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.4 Noun phrase/ Subject term extraction from candidate answers  <a id='noun_subject_extract'></a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### Noun phrase generation function\nfrom wordcloud import WordCloud\n##########################\ndef noun_phrase_generation(boosted_all):\n    '''\n    \n    '''\n    res = {i:[] for i in task_list}\n    for task_index in task_list:\n        task_answers = boosted_all[boosted_all[task_index+'_can_handle_flag']==1][task_index+'_answer']\n    ##\n        grammar = \"NP: {<DT|VB>?<JJ>*<NN|NNS|NNP.*>}\"\n        cp = nltk.RegexpParser(grammar)\n        for answer in task_answers:\n            answer_token = nltk.word_tokenize(answer)\n            answer_tag = nltk.pos_tag(answer_token)\n            answer_chunked = cp.parse(answer_tag)\n            for chunk in answer_chunked:\n                if str(chunk)[1:-1].startswith('NP'):\n                    try:\n                        str_chunk_list = str(chunk).split(' ')[1:]\n                        remove_sep = []\n                        for i in str_chunk_list:\n                            if '/' in i:\n                                remove_sep.append(i[0:i.index('/')])\n                            else:\n                                continue\n                        res[task_index].append(' '.join(remove_sep))\n                    except:\n                        return chunk\n    return res\n\n### noun_phrase_generation function usage example and wordcloud generation\nres_noun = noun_phrase_generation(boosted_all)\nwordcloud_noun_phrase_all = {i:None for i in task_list.keys()}\nfor key in wordcloud_noun_phrase_all.keys():\n    wordcloud_noun_phrase_all[key] = WordCloud(background_color=\"white\",width=1000, height=800, margin=2).generate(\" \".join(res_noun[key]))\n    wordcloud_noun_phrase_all[key].to_file(f'./{key}_all_wordcloud.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from bokeh.layouts import gridplot\noutput_notebook()\nwordcloud_vis = {key: None for key in task_list.keys()}\nfor task_index in task_list.keys():\n    worldcloud_source = ColumnDataSource({\n        'url': [f'./{task_index}_all_wordcloud.png']\n    })\n    p = figure(x_range=(0,1), y_range=(0,1), width=400, height=400)\n    p.image_url(url='url', x=0, y=1, w=1, h=0.8, source=worldcloud_source)\n    p.xaxis.visible = False\n    p.yaxis.visible = False\n    wordcloud_vis[task_index] = p\n\ngrid = gridplot(list(wordcloud_vis.values()), ncols=2, plot_width=300, plot_height=300)\nshow(grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Corpus clustering and topic modeling <a id='corpus_clustering'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 3.4.1 Feature generation <a id='feature_generation'></a>\n* 1. Lexical feature\n* 2. Semantic feature"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"### Generate Lexical feature\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise\nfrom scipy.sparse import hstack\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom random import shuffle\nfrom gensim import models\nimport seaborn as sns\n### title vectorizer\ndf_covid = biorxiv_medrxiv\ndf_covid.fillna('UK', inplace=True)\ntitles_vectorizer = CountVectorizer(max_df = 0.5, min_df = 2, stop_words = 'english', ngram_range=(1,1)) ### Unigram used here, could be fine-tunning\ntitles_vec = titles_vectorizer.fit_transform(df_covid['abstract_all'])\ntitles_simi = pairwise.cosine_similarity(titles_vec)\nprint(f'title_vec generation finished!')\n### abstract vectorizer\nabstracts_vectorizer = CountVectorizer(max_df = 0.5, min_df = 2, stop_words='english', ngram_range=(1, 1)) ### Unigram used here, could be fine-tunning\nabstracts_vec = abstracts_vectorizer.fit_transform(df_covid['abstract_all'])\nabstracts_simi = pairwise.cosine_similarity(abstracts_vec)\nprint(f'abstract_vec generation finished!')\n### author vectorizer\nauthors_vectorizer = CountVectorizer(max_df = 0.9, min_df = 3, stop_words='english') ### Unigram used here, could be fine-tunning\nauthors_vec = authors_vectorizer.fit_transform(df_covid['authors'])\nauthors_simi = pairwise.cosine_similarity(authors_vec)\nprint(f'authors_vec generation finished!')\n### body_text feature\nbody_text_vectorizer = CountVectorizer(max_df = 0.5, min_df = 2, stop_words='english', ngram_range=(1,3)) ### Uni to tri-grams used here\nbody_text_vec = body_text_vectorizer.fit_transform(df_covid['body_text'])\nbody_text_simi = pairwise.cosine_similarity(body_text_vec)\nprint(f'body_text_vec generation finished!')\n\n### journal feature\n# journal_vectorizer = CountVectorizer(max_df = 0.9, min_df = 1, stop_words='english')\n# journal_vec = journal_vectorizer.fit_transform(df_covid['journal'])\n# journal_simi = pairwise.cosine_similarity(journal_vec)\n# print(f'journal_vec generation finished!')\n\n### Combine tile, absract and author feature together\ncombine_vec = hstack((titles_vec, abstracts_vec, authors_vec, body_text_vec))\ncombine_simi = titles_simi*0.6 + abstracts_simi + authors_simi*0.5 + body_text_simi # the weights are adjustable\nprint(f'combine_vec and combine_simi generation finished!') ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"# Doc2Vec for title\ntitles_sentences = []\nlabel_index = 0\nfor title in df_covid['title_all']:\n    sentence = models.doc2vec.TaggedDocument(words = title.replace('.', '').split(), tags = ['Title_%s' % label_index])\n    titles_sentences.append(sentence)\n    label_index += 1\nmodel_doc2vec = models.Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\nmodel_doc2vec.build_vocab(titles_sentences)\n### Start to train own Doc2Vec\nfor epoch in range(20): \n    model_doc2vec.train(titles_sentences, total_examples=len(titles_sentences), epochs=1)\n    model_doc2vec.alpha -= 0.001\n    model_doc2vec.min_alpha = model_doc2vec.alpha\n    shuffle(titles_sentences)\n    print(f'-----------------------epoch {epoch} finish: title_doc2vec----------------------')\n\ntitles_doc2vec = [model_doc2vec.docvecs[i[1][0]] for i in titles_sentences]\n\n### Dimensionality reduction for titles feature with T-SNE\ntsne = TSNE(n_components = 2, init='pca', perplexity=100, random_state = 0)\nnp.set_printoptions(suppress = True)\ntitle_2d = tsne.fit_transform(titles_doc2vec)\n\n# Doc2Vec for abstract\nabstracts_sentences = []\nlabel_index = 0\nfor abstract in df_covid['abstract_all']:\n    sentence = models.doc2vec.TaggedDocument(words = abstract.replace('.', '').split(), tags = ['Abstract_%s' % label_index])\n    abstracts_sentences.append(sentence)\n    label_index += 1\nmodel_doc2vec = models.Doc2Vec(alpha=0.025, min_alpha=0.025)\nmodel_doc2vec.build_vocab(abstracts_sentences)\n\nfor epoch in range(20): # run for 20 passes for better performance\n    model_doc2vec.train(abstracts_sentences, total_examples=len(abstracts_sentences), epochs=1)\n    model_doc2vec.alpha -= 0.001\n    model_doc2vec.min_alpha = model_doc2vec.alpha\n    shuffle(abstracts_sentences)\n    print(f'-----------------------epoch {epoch} finish: abstract_doc2vec----------------------')\n    \nabstracts_doc2vec = [model_doc2vec.docvecs[i[1][0]] for i in abstracts_sentences]\n\n### Dimensionality reduction for abstracts feature with T-SNE\nabstract_2d = tsne.fit_transform(abstracts_doc2vec)\n\n# Doc2Vec for combination of Title and Abstract\ntexts_sentences = []\nlabel_index = 0\nfor (title, abstract) in zip(df_covid['title_all'], df_covid['abstract_all']):\n    sentence = models.doc2vec.TaggedDocument(words = title.replace('.', '').split() + abstract.replace('.', '').split(), tags = ['Text_%s' % label_index])\n    texts_sentences.append(sentence)\n    label_index += 1\nmodel_doc2vec = models.Doc2Vec(alpha=0.025, min_alpha=0.025) \nmodel_doc2vec.build_vocab(texts_sentences)\nfor epoch in range(10): # run for 20 passes for better performance\n    model_doc2vec.train(texts_sentences, total_examples=len(texts_sentences), epochs=1)\n    model_doc2vec.alpha -= 0.002\n    model_doc2vec.min_alpha = model_doc2vec.alpha\n    shuffle(texts_sentences)\n    print(f'-----------------------epoch {epoch} finish: texts_doc2vec----------------------')\n\ntexts_doc2vec = [model_doc2vec.docvecs[i[1][0]] for i in texts_sentences]\ntext_2d = tsne.fit_transform(texts_doc2vec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4.2 Network drawing <a  id='network_drawing'></a>\n\n#### Step 1:  Generation and sparsification\n* Generate a document network (graph) such that document are nodes and their relationships (similarities) are weighted edges. Theoretically, a document network is almost a complete network due to the existence of non-zero similarities between most document pairs. Thus sparsification is necessary to reduce hairballs, highlight important network properties, and bring a more human readable network for visualization. Thus we sparsify a document network by preserving edges with stronger weights (e.g., top 10%, can be made as an adjustable parameter in a future version) while removing edges with lighter weights. <br>\n* __Inputs:__ <br>\nCalculated document similarities (can be organized in a dictionary). Here we mainly use document lexical similarities resulted from a combination of multiple document fields. Specifically, combine_simi. <br>\n* __Returns:__<br>\nSparsified network (also called graph) with nodes as documents, and weighted edges as preserved document similarities. To support different graph layouts more conveniently later, we have the resulting graph in two formats: (1) in the igraph format, g, and (2) in the networkx format, g_nx. In addition, we also maintain and organize preserved edges, combine_simi_edge."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from igraph import Graph, plot\nimport networkx as nx\n\ng = Graph()\ng_nx = nx.Graph()\ng.add_vertices(label_index)\ncombine_simi_edge = np.zeros((label_index, label_index))\nfor i in range(0, label_index):\n    g_nx.add_node(i)\ncombine_simi_copy = combine_simi\nfor index in range(0, label_index):\n    combine_simi_copy[index][index] = 0\nflag_10percent_value = np.zeros(label_index)\n\nfor index1 in range(0, label_index):\n        flag_10percent_index = int(0.1 * (label_index-1))\n        flag_10percent_value[index1] = np.partition(combine_simi_copy[index1], int(-flag_10percent_index))[int(-flag_10percent_index)]\n        for index2 in range(0, label_index):\n            if combine_simi_copy[index1][index2] < flag_10percent_value[index1]:\n                combine_simi_copy[index1][index2] = 0\n                \ndegree = np.zeros(label_index)\nfor index in range(0, label_index):\n    degree[index] = np.count_nonzero(combine_simi_copy[index])\ncount_edge = np.zeros(label_index)\ncombine_simi_final = np.zeros((label_index, label_index))\n\nfor index1 in range(0, label_index):\n    count_edge[index1] = int(pow(degree[index1], 0.5)) + 1\n    #if count_edge[index1] == 0:\n    #\tcount_edge[index1] = 1\n    threshold_value = np.partition(combine_simi_copy[index1], int(-count_edge[index1]))[int(-count_edge[index1])]\n    for index2 in range(0, label_index):\n        #if combine_simi_copy[index1][index2] >= threshold_value and index2 > index1:\n        if combine_simi_copy[index1][index2] >= threshold_value:\n            g.add_edge(index1, index2, weight = combine_simi_copy[index1][index2])\n            g_nx.add_edge(index1, index2, weight = combine_simi_copy[index1][index2])\n            combine_simi_final[index1][index2] = combine_simi_copy[index1][index2]\n            \nfor index1 in range(0, label_index):\n    for index2 in range(0, label_index):\n        if index1 < index2 and combine_simi_final[index1][index2] > 0:\n            combine_simi_edge[index1][index2] = combine_simi_final[index1][index2]\n        if index1 > index2 and combine_simi_final[index1][index2] > 0 and combine_simi_final[index2][index1] == 0:\n            #print(str(index1) + \"-\" + str(index2))\n            combine_simi_edge[index2][index1] = combine_simi_final[index1][index2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Step 2: Layout\n\n* Draw a document network (graph) in a 2D space, in other words, place documents (nodes) and their connections (edges) in a 2D space, such that similar documents are placed closer together and clusters of similar documents can be spatially reflected. For doing so, we use force-directed algorithms, mainly including fruchterman_reingold (supported by igraph) and force_atlas (supported by fa2, requiring networkx graph)\n* __Inputs:__\nA network (graph) which is ready for drawing, and this graph is represented in two formats, including igraph format (g) and networkx format (g_nx).\n* __Returns:__\n2D layouts resulted from fruchterman_reingold (layout_fr) and force_atlas (layout_fa). Optionally, we also have layout_kk and layout_lgl."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"###\n! pip install fa2\nfrom fa2 import ForceAtlas2\nimport gensim\n################################\n\nweight = g.es['weight']\nlayout_fr = g.layout_fruchterman_reingold(weights=weight)\nlayout_kk = g.layout(\"kk\")\nlayout_lgl = g.layout(\"lgl\")\nforceatlas2 = ForceAtlas2(\n    # Behavior alternatives\n    outboundAttractionDistribution=False,  # Dissuade hubs\n    linLogMode=False,  # NOT IMPLEMENTED\n    adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n    edgeWeightInfluence=1.0,\n\n    # Performance\n    jitterTolerance=1.0,  # Tolerance\n    barnesHutOptimize=True,\n    barnesHutTheta=1.2,\n    multiThreaded=False,  # NOT IMPLEMENTED\n\n    # Tuning\n    scalingRatio=2.0,\n    strongGravityMode=False,\n    gravity=1.0,\n\n    # Log\n    verbose=False\n)\npositions = forceatlas2.forceatlas2_networkx_layout(g_nx, pos=None, iterations=2000)\nlayout_fa = np.zeros((label_index, 2))\nfor i in range(label_index):\n    layout_fa[i][0] = positions[i][0]\n    layout_fa[i][1] = positions[i][1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4.3 Clustering  <a  id='clustering'></a>\n\n#### Network Clustering:\n* Utilize Louvain method for network clustering, which is also known as community detection (community == cluster in network). Louvain Method is a network modularity optimization based method, also as a hierarchical clustering algorithm, it can result in multi-level clustering result and suggests the optimal clustering level that is corresponding to the optimal modularity. It is applied to a sparsified network, i.e., preserved document similarities.\n* __Inputs:__\nGenerated and sparsified network, g in igraph format.\n* __Returns:__\nClustering results from 3 levels, such that level2 is for a coarser clustering (as a basic or top level), and level1 and level0 are for finer-grained clusterings. Specifically, number_level for the number of levels (e.g., 3), level_size for the number of clusters on each level, community_map for a mapping FROM cluster IDs of level0 (the bottom level) TO cluster IDs of other levels (i.e., level0, 1, 2), community_maps is for a mapping FROM cluster IDs of each level (i.e., level0, 1, 2) TO cluster IDs of level2 (the top level), memberships is for a mapping FROM document indices TO cluster IDs of each level."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"memberships = []\nnumber_level = 0\nweight = g.es['weight']\nlevel_size = np.zeros(number_level)\n\ncommunity = g.community_multilevel(weights=weight) ### Louvain method (Blondel 2008)\nmodularity = community.modularity\nmembership = community.membership\n\n#Multilevel Community Detection with Louvain method (network clustering) for sparsified article networks\ncommunities_raw = g.community_multilevel(weights=weight, return_levels=True) # Louvain method (Blondel 2008)\ncommunities = []\n\nif(len(communities_raw) == 3):\n    communities = communities_raw\nelif(len(communities_raw) > 3):\n    for i in range(len(communities_raw) - 2, len(communities_raw)):\n        communities.append(communities[i])\nelif(len(communities_raw) == 2):\n    communities.append(communities_raw[0])\n    communities.append(communities_raw[0])\n    communities.append(communities_raw[1])\nelif(len(communities_raw) == 1):\n    communities.append(communities_raw[0])\n    communities.append(communities_raw[0])\n    communities.append(communities_raw[0])\nelse:\n    print(\"error in community detection\")\n\nmodularities = []\n\noptimal_level_index = 0\noptimal_modularity = 0\nfor level in communities: # iterate through different levels of clustering\n    number_level += 1\n    if level.modularity > optimal_modularity:\n        optimal_modularity = level.modularity\n        optimal_level_index = number_level - 1\n    modularities.append(level.modularity)\n    memberships.append(level.membership)\nlevel_size = np.zeros(number_level)\nfor i in range(0, number_level):\n    level_size[i] = len(set(memberships[i]))\n\ncommunity_map= np.zeros((int(max(level_size)), number_level)) # global map: level0, level1, level2\ncommunity_map = community_map - 1\nfor index in range(0, label_index):\n    community = int(memberships[0][index])\n    if community not in community_map[:,0]:\n        community_map[community][0] = community\n        for level in range(1, number_level):\n            community_map[community][level] = memberships[level][index]\ncommunity_maps = [] # the map between every level to the top level, the format is: current_level_cluster_id, top_level_cluster_id, sub_id_within_the_top_cluster\nfor level in range(0, number_level):\n    community_map_current = np.zeros((int(level_size[level]), 3))\n    community_map_current = community_map_current - 1\n    cluster_count = np.zeros(int(level_size[number_level - 1]))\n    for index in range(0, label_index):\n        community = int(memberships[level][index])\n        if community not in community_map_current[:,0]:\n            community_map_current[community][0] = community\n            top_community = memberships[number_level-1][index]\n            community_map_current[community][1] = top_community\n            community_map_current[community][2] = cluster_count[top_community]\n            cluster_count[top_community] += 1\n    community_maps.append(community_map_current)\n    \n    \n# Useful results: memberships/level_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### General clustering\n\n* Utilize general agglomerative clustering on document features. As a hierarchical clustering algorithm, it results in multi-level clustering result, and we refer to the Louvain method for the best clustering level (e.g., the optimal number of clusters). We apply this approach to lexical feature vectors and semantic feature vectors respectively.\n* __Inputs:__\nClustering level (i.e., number of clusters) as suggested by Louvain method, level_size; Generated lexical feature vectors, combine_vec; Generated semantic feature vectors, texts_doc2vec.\n* __Returns:__\nSimilar to clustering.network_clustering(g), we have community_map2, community_maps2, and memberships2 for lexical feature vectors, and community_map3, community_maps3, and memberships3 for semantic feature vectors."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# for memberships3, community_map3 and community_maps3\nmemberships3 = []\nfor level in range(0, number_level):\n    clustering = AgglomerativeClustering(linkage = 'ward', n_clusters = int(level_size[level]))\n    #clustering.fit(text_2d)\n    clustering.fit(texts_doc2vec)\n    #plot_clustering(combine_2d, combine_vec.toarray(), clustering.labels_, \"%s linkage\" % 'ward')\n    memberships3.append(clustering.labels_)\ncommunity_map3 = np.zeros((int(max(level_size)), number_level)) # global map: level0, level1, level2\ncommunity_map3 = community_map3 - 1\nfor index in range(0, label_index):\n    community = int(memberships3[0][index])\n    if community not in community_map3[:,0]:\n        #print (\"index-\" + str(index) + \", community0-\" + str(community) + \", community1-\" + str(memberships2[1][index]) + \", community2-\" + str(memberships2[2][index]));\n        community_map3[community][0] = community\n        for level in range(1, number_level):\n            community_map3[community][level] = memberships3[level][index]\ncommunity_maps3 = [] # the map between every level to the top level, the format is: current_level_cluster_id, top_level_cluster_id, sub_id_within_the_top_cluster\nfor level in range(0, number_level):\n    community_map_current = np.zeros((int(level_size[level]), 3))\n    community_map_current = community_map_current - 1\n    cluster_count = np.zeros(int(level_size[number_level - 1]))\n    for index in range(0, label_index):\n        community = int(memberships3[level][index])\n        if community not in community_map_current[:,0]:\n            community_map_current[community][0] = community\n            top_community = memberships3[number_level-1][index]\n            community_map_current[community][1] = top_community\n            community_map_current[community][2] = cluster_count[top_community]\n            cluster_count[top_community] += 1\n    community_maps3.append(community_map_current)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.4.4 Layout adjustment <a id='layout_adjustment'></a>\n\n* To highlight the clustering patterns, we adjust document layouts (positions in the 2D space) by aggregating documents towards theri cluster centers. Document layouts can be resulted from t-SNE (document map), or network drawing with a force-directed algorithm (document network).\n* __Inputs:__\nAll different types of layouts which can be arranged in a dict. Specifically, we consider layout_fa (force atlas considering lexical similarities), layout_fr (fruchterman reingold considering lexical similarities), layout_tsne_text (text_2d based on semantic doc2vec + tSNE). Also, the number of clustering levels (number_level, which should be 3).\n* __Returns:__\nFor each layout, we have an adjusted layout (_adjusts) with an adjustment magnitude of 0.5, and another adjusted layout (adjusts2) with an adjustment magnitude of 1. All results can be arranged in a dict. In a future version, we can apply more flexible or customizable magnitude/parameter."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"current_index = label_index\nlayout_tsne_text = text_2d\nlayout_tsne_text_adjusts = []\nlayout_tsne_text_adjusts2 = []\n # get the graph center\ngraph_center = [0,0]\nfor index in range(0, current_index):\n    graph_center += layout_tsne_text[index]\ngraph_center = graph_center/current_index\n# get the cluster centers (consider different clustering levels)\nfor level in range(0, number_level):\n    cluster_number = int(level_size[level])\n    cluster_center = np.zeros((cluster_number, 2))\n    cluster_diff = np.zeros((cluster_number, 2))\n    cluster_member_count = np.zeros(cluster_number)\n    for index in range(0, current_index):\n        cluster_id = memberships3[level][index]\n        cluster_center[cluster_id] += layout_tsne_text[index]\n        cluster_member_count[cluster_id] += 1\n    for index in range(0, cluster_number):\n        cluster_center[index] = cluster_center[index]/cluster_member_count[index]\n        cluster_diff[index] = cluster_center[index] - graph_center\n\n    layout_adjust = np.zeros((current_index, 2))\n    for index in range(0, current_index):\n        cluster_id = memberships3[level][index]\n        layout_adjust[index] = layout_tsne_text[index] + 0.5*cluster_diff[cluster_id]\n    layout_tsne_text_adjusts.append(layout_adjust)\n\n    layout_adjust2 = np.zeros((current_index, 2))\n    for index in range(0, current_index):\n        cluster_id = memberships3[level][index]\n        layout_adjust2[index] = layout_tsne_text[index] + 1*cluster_diff[cluster_id]\n    layout_tsne_text_adjusts2.append(layout_adjust2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.4.5 Topic modeling  <a id='topic_modeling'></a>\n\n* RAKE_based method, which generate document keywords using our RAKE_based method, and then synthesize cluster topics based on document keywords - with respect to documents belonging to a given cluster. Thus the resulting topics are related to our clustering results, and they could capture n-gram phrases in a more informative way."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### Helper function and Parameters for rake-based topic modeling\nimport operator\nmy_stopwords = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"about\", \"above\", \"addition\", \"after\", \"again\", \"against\", \"ain\", \"all\",\n\"also\", \"although\", \"am\", \"among\", \"an\", \"and\", \"any\", \"approach\", \"approached\", \"approaches\", \"approaching\", \"are\", \"aren\", \"as\", \"at\", \"b\", \"based\", \"be\",\n\"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"c\", \"called\", \"can\",\n\"consider\", \"considers\", \"consideres\", \"considering\", \"corresponding\", \"could\", \"couldn\", \"d\",\n\"develop\", \"developed\", \"developing\", \"develops\", \"did\", \"didn\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"down\", \"during\", \"e\",\n\"each\", \"f\", \"few\", \"first\", \"for\", \"from\", \"further\", \"g\", \"go\", \"goes\", \"h\", \"had\", \"hadn\", \"has\", \"hasn\", \"have\", \"haven\",\n\"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"if\", \"in\", \"include\", \"included\",\n\"includes\", \"including\", \"into\", \"is\", \"isn\", \"it\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"l\", \"ll\", \"m\", \"m\", \"ma\", \"many\", \"may\", \"me\",\n\"mg\", \"might\", \"mightn\", \"more\", \"most\", \"much\", \"must\", \"mustn\", \"my\", \"myself\", \"n\", \"needn\", \"never\", \"new\", \"no\", \"none\", \"nor\",\n\"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\",\n\"out\", \"over\", \"over\", \"own\", \"p\", \"particular\", \"present\", \"presented\", \"presenting\", \"presents\", \"propose\", \"proposed\", \"proposes\",\n\"proposing\", \"provide\", \"provided\", \"provides\", \"providing\", \"q\", \"r\", \"re\", \"result\", \"resulted\", \"resulting\", \"results\", \"s\", \"same\",\n\"shall\", \"shalln\", \"shan\", \"she\", \"should\", \"shouldn\", \"show\", \"showed\", \"showing\", \"shows\", \"since\", \"so\", \"some\", \"studied\", \"studies\",\n\"study\", \"studying\", \"sub\", \"such\", \"sup\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\",\n\"they\", \"this\", \"those\", \"though\", \"through\", \"throughout\", \"to\", \"too\", \"two\", \"u\", \"under\", \"until\", \"up\", \"use\", \"used\", \"uses\", \"using\",\n\"v\", \"ve\", \"very\", \"via\", \"w\", \"was\", \"wasn\", \"we\", \"well\", \"were\", \"weren\", \"what\", \"when\", \"where\", \"whether\", \"which\", \"while\", \"who\",\n\"whom\", \"why\", \"will\", \"with\", \"without\", \"won\", \"would\", \"wouldn\", \"x\", \"y\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\"]\n#########################################\n\ndef removeNonAscii(s):\n    return \"\".join(i for i in s if ord(i) < 128)\n\n\n# define the function to extract Noun Phrases (NP) from text, e.g., title and abstract\n# use more grammars, no stemming is applied, noun phrases are in the format of adj + noun\ndef NPextractor2(text):\n    text = removeNonAscii(text)\n    if len(text) == 0:\n        return text\n\n    tok = nltk.word_tokenize(text)\n    pos = nltk.pos_tag(tok)\n\n    # the original grammar, to get shorter NPs\n    grammar1 = r\"\"\"\n      NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n          {<NNP>+}                # chunk sequences of proper nouns\n    \"\"\"\n\n\n    # the self defined grammar based on the previous version above, to get longer NPs (as supplements)\n    grammar2 = r\"\"\"\n      NP: {<DT|PP\\$>?<JJ>*<NN|NNS|NNP|NNPS>+}   # chunk determiner/possessive, adjectives and noun(s)\n          {<NNP>+}                # chunk sequences of proper nouns\n    \"\"\"\n\n    chunker1 = nltk.RegexpParser(grammar1)\n    tree1 = chunker1.parse(pos)\n    chunker2 = nltk.RegexpParser(grammar2)\n    tree2 = chunker2.parse(pos)\n\n    nps = [] # word and pos_tag\n    nps_words = [] # only word\n\n    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'NP'):\n        nps.append(subtree.leaves())\n        current_np = []\n        for item in subtree.leaves():\n            current_np.append(item[0])\n        nps_words.append(current_np)\n\n    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP'):\n        if subtree.leaves() in nps:\n            continue\n        nps.append(subtree.leaves())\n        current_np = []\n        for item in subtree.leaves():\n            current_np.append(item[0])\n        nps_words.append(current_np)\n\n    refined_words = []\n    #stopwords = nltk.corpus.stopwords.words('english')\n    stopwords = my_stopwords\n    for np in nps_words:\n        if len(np) < 1:\n            continue\n        current_np = []\n        for word in np:\n            if (2 <= len(word) <= 40) and (word.lower() not in stopwords):\n                current_np.append(word.lower())\n        if len(current_np) >= 1:\n            refined_words.append(current_np)\n    return refined_words\n\n#############################################\n# Tell whether a word is punctuation or not.\ndef isPunct(word):\n    return len(word) == 1 and word in string.punctuation\n#############################################\n# Tell whether a word is numeric or not.\ndef isNumeric(word):\n    try:\n        float(word) if '.' in word else int(word)\n        return True\n    except ValueError:\n        return False\n############################################\n\n# Tell whether the a contains numerical part.\ndef containNumeric(word):\n    return any(char.isdigit() for char in word)\n\n# Define the RAKE method for keyword extraction\n# Reference: Reference: Automatic keyword extraction from individual documents\nclass RakeKeywordExtractor:\n    def __init__(self):\n        #self.stopwords = set(nltk.corpus.stopwords.words())\n        self.stopwords = set(my_stopwords)\n        self.top_fraction = 1 # consider top third candidate keywords by score\n    #########################################\n    '''\n    Chunk each sentence into phrases using punctuations and stopwords.\n    Upper_length restricts phrase length. If model == all, we also include additional noun phrases based on NLP grammars;\n    if mode == np, we only consider noun phrases, and RAKE-chunked phrases will be ignored.\n    '''\n    def _generate_candidate_keywords(self, sentences, upper_length, mode):\n        phrase_list = []\n        for sentence in sentences:\n            # Additional Noun phrases if they won't be detected by the Rake method below\n            nps = NPextractor2(sentence)\n            if mode == \"np\":\n                if len(nps) > 0:\n                    for item in nps:\n                        if len(item) > 0 and len(item) <= upper_length:  # restrict the length of phrase to be 1~5\n                            phrase_list.append(item)\n                continue\n\n            words = map(lambda x: \"|\" if x in my_stopwords else x, nltk.word_tokenize(sentence.lower()))\n            phrase = []\n            for word in words:\n                #if word == \"|\" or isPunct(word):\n                if word == \"|\" or isPunct(word) or isNumeric(word) or containNumeric(word):\n                    #if len(phrase) > 0:\n                    if len(phrase) > 0 and len(phrase) <= upper_length: # restrict the length of phrase to be 1~5\n                        if phrase not in nps:\n                            phrase_list.append(phrase)\n                        phrase = []\n                else:\n                    phrase.append(word)\n            if len(nps) > 0:\n                #phrase_list += nps\n                for item in nps:\n                    if len(item) > 0 and len(item) <= upper_length:  # restrict the length of phrase to be 1~5\n                        phrase_list.append(item)\n        return phrase_list\n    ###########################################\n    '''\n    For each phrase consisting of multiple words, calculate the score of each word, reflecting the word’s content meaningfulness.\n    '''\n    def _calculate_word_scores(self, phrase_list):\n        word_freq = nltk.FreqDist()\n        word_degree = nltk.FreqDist()\n        for phrase in phrase_list:\n            filterlist = list(filter(lambda x: not isNumeric(x) and not containNumeric(x), phrase))\n            degree = len(filterlist) - 1\n            for word in phrase:\n                #word_freq.inc(word)\n                word_freq[word] += 1\n                #word_degree.inc(word, degree) # other words\n                word_degree[word] += degree\n        for word in word_freq.keys():\n            word_degree[word] = word_degree[word] + word_freq[word] # itself\n    # word score = deg(w) / freq(w) (favor long phrases), or word score = deg(w) (not that favor long phrases)\n        word_scores = {}\n        for word in word_freq.keys():\n            #word_scores[word] = word_degree[word] / word_freq[word] # (favor long phrases)\n            word_scores[word] = word_degree[word]\n        return word_scores\n    #################################################################\n    '''\n    For each phrase consisting of multiple words, combine word scores into the phrase score, which represents the phrase’s content meaningfulness.\n    '''\n    def _calculate_phrase_scores(self, phrase_list, word_scores):\n        phrase_scores = {}\n        for phrase in phrase_list:\n            phrase_score = 0\n            for word in phrase:\n                phrase_score += word_scores[word]\n            phrase_scores[\" \".join(phrase)] = phrase_score\n            #phrase_scores[\" \".join(phrase)] = phrase_score/len(phrase)\n        return phrase_scores\n    #################################################################\n    '''\n    Extract keywords (key phrases) from the input text, which can consist of multiple sentences.\n    '''\n    def extract(self, text, incl_scores=False, number=30, upper_length=5, mode=\"all\"):\n        sentences = nltk.sent_tokenize(text) # break a text (paragraph) into an array of single sentences ending with a period\n        phrase_list = self._generate_candidate_keywords(sentences, upper_length, mode)\n        word_scores = self._calculate_word_scores(phrase_list)\n        phrase_scores = self._calculate_phrase_scores(phrase_list, word_scores)\n        sorted_phrase_scores = sorted(phrase_scores.items(), key=operator.itemgetter(1), reverse=True)\n        n_phrases = len(sorted_phrase_scores)\n        if incl_scores:\n            #return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n            return sorted_phrase_scores[0:number]\n        else:\n            #return map(lambda x: x[0], sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])\n            return map(lambda x: x[0], sorted_phrase_scores[0:number])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rake cluster based topics \n* Apply our RAKE_based method (RakeKeywordExtractor) to multiple documents (titles or abstracts) with respect to their clustering results, and generate cluster topics (synthesized from document keywords) for each cluster. Here we consider document titles and abstracts respectively; we consider clustering results from network clustering as well as general clusterings.\n* Inputs:\ndocument titles (titles_raw), document abstracts (abstracts_raw), number of clustering levels (number_level), network clustering results (community_maps and memberships), general clustering results (community_maps2, memberships2, community_maps3, and memberships3).\n* Returns:\nGenerated topics (salient keywords) for each cluster on each level, with respect to different clustering approaches. Specifically, network clustering based on lexical similarities (cluster_title_keywords and cluster_abstract_keywords), general clustering based on lexical features (cluster_title_keywords2 and “cluster_abstract_keywords2”), and general clustering based on semantic features (cluster_title_keywords3 and cluster_abstract_keywords3)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cluster_title_keywords3 = []\ncluster_abstract_keywords3 = []\ncluster_titles3 = [[\"\" for j in range(len(community_maps3[i]))] for i in range(number_level)]\ncluster_abstracts3 = [[\"\" for j in range(len(community_maps3[i]))] for i in range(number_level)]\ncluster_title_keywords3 = [[[] for j in range(len(community_maps3[i]))] for i in range(number_level)]\ncluster_abstract_keywords3 = [[[] for j in range(len(community_maps3[i]))] for i in range(number_level)]\nfor level in range(0, number_level):\n    for index in range(0, current_index):\n        cluster_id = int(memberships3[level][index])\n        cluster_titles3[level][cluster_id] += biorxiv_medrxiv['title_raw'][index]\n        cluster_titles3[level][cluster_id] += \" \"\n        cluster_abstracts3[level][cluster_id] += biorxiv_medrxiv['abstract_raw'][index]\n        cluster_abstracts3[level][cluster_id] += \" \"\nrake = RakeKeywordExtractor()\n\nfor level in range(0, number_level):\n    for cluster in range(len(community_maps3[level])):\n        cluster_title_keywords3[level][cluster] = rake.extract(cluster_titles3[level][cluster], incl_scores=False)\n        cluster_abstract_keywords3[level][cluster] = rake.extract(cluster_abstracts3[level][cluster], incl_scores=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cluster_title_keywords = []\ncluster_abstract_keywords = []\nfor level in range(number_level):\n    tmp_title = []\n    tmp_abstract = []\n    for cluster_ in memberships3[level]:\n        tmp_title.append(', '.join(list(cluster_title_keywords3[level][cluster_])[0:5]))\n        tmp_abstract.append(', '.join(list(cluster_abstract_keywords3[level][cluster_])[0:5]))\n    cluster_title_keywords.append(tmp_title)\n    cluster_abstract_keywords.append(tmp_abstract)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Result Visualization <a id='result_vis'></a>"},{"metadata":{},"cell_type":"markdown","source":"* Here, we can see that for the sample bioxirv dataset, since we further move different cluster apart with the result of clustering algorithm,  each cluster's  contour is really clear "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"output_notebook()\nselected_num = 2\ny_labels = memberships3[selected_num]\n### Data source\nsource = ColumnDataSource(data = {\n    'x' : layout_tsne_text_adjusts[selected_num][:,0],\n    'y' : layout_tsne_text_adjusts[selected_num][:,1],\n    'title' : df_covid['title_raw'],\n    'desc' : y_labels,\n    'author' : df_covid['authors'],\n    'journal' : df_covid['journal'],\n    'labels' : ['Cluster '+str(x) for x in y_labels],\n#     'title_keywords': cluster_title_keywords[selected_num],\n#     'abstract_keywords': cluster_abstract_keywords[selected_num]\n})\n\n### Hover information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@title\"),\n    (\"Author(s)\", \"@author\"),\n    (\"Journal\", \"@journal\"),\n#     (\"Keywords of Cluster: title\", '@title_keywords'),\n#     (\"Keywords of Cluster: abstract\", '@abstract_keywords')\n],point_policy=\"follow_mouse\")\n\n### Map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'lasso_select'], \n           title=\"COVID-19 Semantic Cluster - Fine\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n\n# source.selected.js_on_change('indices', CustomJS(args={'x':layout_tsne_text_adjusts[selected_num][:,0], 'y': layout_tsne_text_adjusts[selected_num][:,1]}, code=\"\"\"\n#         var inds = cb_obj.indices;\n        \n#     \"\"\")\n# )\n\ncallback = CustomJS(args={'source':source, 'db':layout_tsne_text_adjusts, 'memberships3':memberships3, 'cluster_title_keywords':cluster_title_keywords, 'cluster_abstract_keywords':cluster_abstract_keywords}, code='''\n    var data = source.data;\n    var x = data['x']\n    var y = data['y']\n    var f = cb_obj.value;\n    var desc = data['desc']\n    var labels = data['labels']\n    var selected_num = 0;\n    switch(f){\n        case 0:\n            selected_num=0\n            break;\n        \n        case 1:\n            selected_num=1\n            break;\n            \n        case 2:\n            selected_num=2\n            break;\n        \n    };\n    var db = db;\n    for (var i = 0; i < x.length; i++) {\n        x[i] = db[selected_num][i][0];\n        y[i] = db[selected_num][i][1];\n        labels[i] = 'Cluster '+memberships3[selected_num][i];\n        desc[i] = memberships3[selected_num][i];\n        \n    }\n    //source.data['title_keywords'] = cluster_title_keywords[selected_num];\n    //source.data['abstract_keywords'] = cluster_abstract_keywords[selected_num];\n    source.change.emit();\n''')\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Semantic Cluster</h1>\"\"\")\n\nslider = Slider(start=0, end=2, value=1, step=1, title=\"Num of clusters: large to small\")\nslider.js_on_change('value', callback)\n\nlayout = column(header, row(column(slider)), p)\n#show\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# def get_desc_col_name(cluster_level):\n#     return 'all_memberships_size'+str(cluster_level)\n\n# def get_x_col_name(cluster_level, sample=''):\n#     return f'all_layout_tsne_text_adjusts_{str(cluster_level)}_x'\n\n# def get_y_col_name(cluster_level, sample=''):\n#     return f'all_layout_tsne_text_adjusts_{str(cluster_level)}_y'\n\n# def get_keyword_col_name(cluster_level):\n#     return f'all_cluster_keywords_for_mem{str(cluster_level)}'\n\n# wordcloud_library = {key: [ f'./{key}_all_wordcloud.png']* boosted_all[boosted_all[key+'_can_handle_flag']==1].shape[0] for key in task_list}\n\n# from bokeh.palettes import Spectral6\n# %config InlineBackend.figure_format = 'retina'\n# output_notebook()\n\n# cluster_level = 10\n# ### Data source\n# source_tsne = ColumnDataSource(data = {\n#     'x' : boosted_all[get_x_col_name(cluster_level)],\n#     'y' : boosted_all[get_y_col_name(cluster_level)],\n#     'title' : boosted_all['title_raw'],\n#     'desc' : boosted_all[get_desc_col_name(cluster_level)],\n#     'author' : boosted_all['authors'],\n#     'journal' : boosted_all['journal'],\n#     'keyword': boosted_all[get_keyword_col_name(cluster_level)],\n#     'labels' : boosted_all[get_desc_col_name(cluster_level)]\n# })\n\n# ### Hover information\n# hover = HoverTool(tooltips=[\n#     (\"Title\", \"@title\"),\n#     (\"Author(s)\", \"@author\"),\n#     (\"Journal\", \"@journal\"),\n#     (\"Keyword of cluster\", \"@keyword\")\n# ],point_policy=\"follow_mouse\")\n\n# ### Map colors\n# mapper = linear_cmap(field_name='desc', \n#                      palette=Category20[20],\n#                      low=min(boosted_all[get_desc_col_name(cluster_level)]) ,high=max(boosted_all[get_desc_col_name(cluster_level)]))\n\n\n# p = figure(plot_width=600, plot_height=600, \n#            tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'lasso_select'], \n#            title=\"COVID-19 Semantic Cluster - Fine\", \n#            toolbar_location=\"right\")\n\n# # plot\n# render = p.circle('x', 'y', size=5,\n#           source=source_tsne,\n#           fill_color=mapper,\n#           line_alpha=0.3,\n#           line_color=\"black\",\n#           legend = 'labels',color=Spectral6)\n\n\n# subclass_select = Select(title=\"Subclass:\", value=\"All\", options=['All'] + [str(i) for i in range(cluster_level)], width=100)\n\n# callback = CustomJS(args={'source':source_tsne, 'db':boosted_all_dict, 'subclass_select': subclass_select}, code='''\n#     var sliderIndex = cb_obj.value;\n#     var x_col_name = \"all_layout_tsne_text_adjusts_\" + sliderIndex.toString() + '_x';\n#     var y_col_name = \"all_layout_tsne_text_adjusts_\" + sliderIndex.toString() + '_y';\n#     var desc_col_name = \"all_memberships_size\" + sliderIndex.toString();\n#     source.data['x'] = Object.values(db[x_col_name]);\n#     source.data['y'] = Object.values(db[y_col_name]);\n#     source.data['desc'] = Object.values(db[desc_col_name]);\n#     source.data['labels'] = Object.values(db[desc_col_name]);\n#     var tmp = ['All'];\n#     let i = 0;\n#     while(i < sliderIndex){\n#         tmp.push(i.toString());\n#         i = i + 1;\n#     }\n#     subclass_select.options = tmp; \n#     source.change.emit();\n# ''')\n\n\n\n# subclass_callback = CustomJS(args = dict(source=source_tsne, boosted_all_dict=boosted_all_dict), code=\"\"\"\n#     Object.filter = function( obj, predicate) {\n#         var result = {};\n#         for (let key in obj) {\n#             if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n#                 result[key] = obj[key];\n#             }\n#         }\n#         return result;\n#     };\n    \n#     Object.batch_select = function(obj, list){\n#         var res = [];\n#         for(let key of list){\n#             res.push(obj[key]);\n#         }\n#         return res;\n#     };\n    \n#     var selected_index = cb_obj.value;\n#     var cluster_level = parseInt(cb_obj.options[cb_obj.options.length-1]) + 1;\n#     var target = [];\n#     if (selected_index === \"All\"){\n#         source.selected.indices = [];\n#     } else {    \n#         selected_index = parseInt(selected_index);\n#         target = Object.keys(Object.filter(boosted_all_dict['all_memberships_size'+ cluster_level.toString()], item=>item == selected_index));\n#         source.selected.indices = target;\n#     }\n    \n# \"\"\")\n\n\n# #header\n# header = Div(text=f\"<h3>COVID-19 Candidate Corpus Semantic Cluster - {boosted_all.shape[0]} total points</h3>\")\n\n# slider = Slider(start=5, end=20, value=10, step=1, title=\"Num of clusters\",width=500 )\n# slider.js_on_change('value', callback)\n# subclass_select.js_on_change('value', subclass_callback)\n# layout = column(header, row(slider, subclass_select), p)\n# show(layout)\n# # layout = column(header, row(subclass_select), p)\n# # ######\n# # wordcloud_source = ColumnDataSource(data={\n# #     'url': wordcloud_library['task_1']\n# # })\n\n# # data = {\n# #     'answer': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_answer']),\n# #     'score':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_start_score_prob']),\n# #     'index_from_original': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1].index)\n# # }\n\n# # detail_data = ColumnDataSource({\n# #     'title':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['title_raw']),\n# #     'answer': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['task_1_answer']),\n# #     'abstract':list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['abstract_raw']),\n# #     'id': list(boosted_all[boosted_all['task_1_can_handle_flag'] == 1]['paper_id'])\n# # })\n\n# # # worldcloud_source = ColumnDataSource({\n# # #     'url': res['task_1'][-1]\n# # # })\n\n\n# # source = ColumnDataSource(data)\n\n# # columns = [\n# #     TableColumn(field='answer', title='Possible Answer', formatter=StringFormatter(font_style=\"bold\")),\n# #     TableColumn(field='score', title='Score', width=5)\n# # ]\n# # data_table =  DataTable(source=source, columns=columns, selectable=True, index_header=\"\", width=500,height=600, fit_columns=True, scroll_to_selection=True, height_policy='auto', editable=True)\n\n# # select = Select(title='', value=list(task_list.values())[0], options=list(task_list.values()), height=50, width=420)\n\n# # scoretype_select = Select(title='Score type', value='standard', options=['standard', 'normalized', 'norm+adjust'], width=80)\n\n\n# # scoretype_callback = CustomJS(args = dict(source=source, select = select, task_to_index=task_to_index, boosted_all_dict=boosted_all_dict), code=\"\"\"\n    \n# #     Object.filter = function( obj, predicate) {\n# #         var result = {};\n# #         for (let key in obj) {\n# #             if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n# #                 result[key] = obj[key];\n# #             }\n# #         }\n# #         return result;\n# #     };\n    \n# #     Object.batch_select = function(obj, list){\n# #         var res = [];\n# #         for(let key of list){\n# #             res.push(obj[key]);\n# #         }\n# #         return res;\n# #     };\n    \n# #     var selected_type = cb_obj.value;\n# #     var selected_task = select.value;\n# #     var selected_index = task_to_index[selected_task];\n# #     var target_indexs = Object.keys(Object.filter(boosted_all_dict[selected_index+'_can_handle_flag'], item => item == 1));\n# #     var answer = Object.batch_select(boosted_all_dict[selected_index+'_answer'], target_indexs);\n# #     var score = [];\n# #     if (selected_type == 'standard'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_start_score_prob'], target_indexs);\n# #     }else if(selected_type == 'normalized'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized'], target_indexs);\n# #     }else if(selected_type == 'norm+adjust'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized_adjusted'], target_indexs);\n# #     }\n# #     var title = Object.batch_select(boosted_all_dict['title_raw'], target_indexs);\n# #     var abstract = Object.batch_select(boosted_all_dict['abstract_raw'], target_indexs);\n# #     var id = Object.batch_select(boosted_all_dict['paper_id'], target_indexs);\n# #     source.data = {\n# #         'answer': answer,\n# #         'score': score,\n# #         'index_from_original': target_indexs\n# #     };\n# #     detail_data.data = {\n# #        'title':title,\n# #        'answer':answer,\n# #        'abstract':abstract,\n# #        'id':id\n# #     };\n# #     source.change.emit();\n# #     detail_data.change.emit();\n# # \"\"\")\n\n\n# # subclass_callback = CustomJS(args = dict(source=source_tsne, boosted_all_dict=boosted_all_dict), code=\"\"\"\n# #     Object.filter = function( obj, predicate) {\n# #         var result = {};\n# #         for (let key in obj) {\n# #             if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n# #                 result[key] = obj[key];\n# #             }\n# #         }\n# #         return result;\n# #     };\n    \n# #     Object.batch_select = function(obj, list){\n# #         var res = [];\n# #         for(let key of list){\n# #             res.push(obj[key]);\n# #         }\n# #         return res;\n# #     };\n    \n# #     var selected_index = cb_obj.value;\n# #     var cluster_level = parseInt(cb_obj.options[cb_obj.options.length-1]) + 1;\n# #     var target = [];\n# #     if (selected_index === \"All\"){\n# #         source.selected.indices = [];\n# #     } else {    \n# #         selected_index = parseInt(selected_index);\n# #         target = Object.keys(Object.filter(boosted_all_dict['all_memberships_size'+ cluster_level.toString()], item=>item == selected_index));\n# #         source.selected.indices = target;\n# #     }\n    \n# # \"\"\")\n\n\n\n\n# # task_select_callback = CustomJS(args=dict(source=source,detail_data=detail_data, source_tsne=source_tsne, task_to_index=task_to_index, scoretype_select=scoretype_select, boosted_all_dict=boosted_all_dict, wordcloud_source=wordcloud_source, wordcloud_library=wordcloud_library), code=\"\"\"\n# #     Object.filter = function( obj, predicate) {\n# #         var result = {};\n# #         for (let key in obj) {\n# #             if (obj.hasOwnProperty(key) && predicate(obj[key])) {\n# #                 result[key] = obj[key];\n# #             }\n# #         }\n# #         return result;\n# #     };\n    \n# #     Object.batch_select = function(obj, list){\n# #         var res = [];\n# #         for(let key of list){\n# #             res.push(obj[key]);\n# #         }\n# #         return res;\n# #     };\n# #     var selected_type = scoretype_select.value;\n# #     var selected_task = cb_obj.value;\n# #     var selected_index = task_to_index[selected_task];\n# #     var target_indexs = Object.keys(Object.filter(boosted_all_dict[selected_index+'_can_handle_flag'], item => item == 1));\n# #     var answer = Object.batch_select(boosted_all_dict[selected_index+'_answer'], target_indexs);\n# #     var score = [];\n# #     if (selected_type == 'standard'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_start_score_prob'], target_indexs);\n# #     }else if(selected_type == 'normalized'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized'], target_indexs);\n# #     }else if(selected_type == 'norm+adjust'){\n# #         score = Object.batch_select(boosted_all_dict[selected_index+'_score_normalized_adjusted'], target_indexs);\n# #     }\n# #     var title = Object.batch_select(boosted_all_dict['title_raw'], target_indexs);\n# #     var abstract = Object.batch_select(boosted_all_dict['abstract_raw'], target_indexs);\n# #     var id = Object.batch_select(boosted_all_dict['paper_id'], target_indexs);\n# #     source.data = {\n# #         'answer': answer,\n# #         'score': score,\n# #         'index_from_original': target_indexs\n# #     };\n# #     detail_data.data = {\n# #        'title':title,\n# #        'answer':answer,\n# #        'abstract':abstract,\n# #        'id':id\n# #     };\n# #     wordcloud_source.data['url'] = wordcloud_library[selected_index];\n# #     source_tsne.selected.indices = target_indexs;\n# #     wordcloud_source.change.emit();\n# #     \"\"\") \n    \n# # paper_detail = Div(text=\"Paper info shows here\", margin=(50,10,10,30), style={'border':'1px solid black', 'width':'400px', 'height':'600px', 'padding':'20px', 'border-top-left-radius':'5px','border-top-right-radius':'5px','text-align':'center','overflow-y':'auto'})\n# # select_callback=CustomJS(args=dict(source=source, div=paper_detail, source_tsne=source_tsne, detail_data=detail_data), code=\"\"\"\n# #     var selection_index=source.selected.indices[0];\n# #     var answer = detail_data.data['answer'][selection_index];\n# #     var title = detail_data.data['title'][selection_index];\n# #     var abstract = detail_data.data['abstract'][selection_index] || \"\";\n# #     var idlink = \"\";\n# #     var id = detail_data.data['id'][selection_index];\n# #     if(id.startsWith(\"PMC\")){\n# #         idlink = \"https://www.ncbi.nlm.nih.gov/pmc/articles/\" + id;\n# #     }else{\n# #         idlink = 'https://www.semanticscholar.org/paper/' + id;\n# #     }\n# #     var index = 0;\n# #     if(title.toLowerCase().indexOf(answer.toLowerCase()) !== -1){\n# #         index = title.toLowerCase().indexOf(answer.toLowerCase())\n# #         title = title.slice(0,index) + `<a href=${idlink} target=\"_blank\" style='text-decoration:none; background: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-image: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-position-x: initial; background-position-y: initial;background-size: initial; background-repeat-x: initial; background-repeat-y: initial; background-attachment: initial;background-origin: initial; background-clip: initial; background-color: initial; margin: 0 0.25em; line-height: 1.5; padding: 0px 3px !important; border-radius: 5rem !important;'><strong>${title.slice(index, index + answer.length)}</strong></a>` + title.slice(index + answer.length);\n# #     }\n# #     if(abstract.toLowerCase().indexOf(answer.toLowerCase()) !== -1){\n# #         index = abstract.toLowerCase().indexOf(answer.toLowerCase());\n# #         abstract = abstract.slice(0, index) + `<a style='text-decoration:none; background: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-image: linear-gradient(90deg, rgb(147, 222, 241), rgb(147, 222, 23)); background-position-x: initial; background-position-y: initial;background-size: initial; background-repeat-x: initial; background-repeat-y: initial; background-attachment: initial;background-origin: initial; background-clip: initial; background-color: initial; margin: 0 0.25em; line-height: 1.5; padding: 0px 3px !important; border-radius: 5rem !important;'><strong>${abstract.slice(index, index + answer.length)}</strong></a>` + abstract.slice(index + answer.length);\n# #     }\n# #     div.text = `<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">`;\n# #     div.text +=`<h2><a href=${idlink} target=\"_blank\" style='text-decoration:none'><center>${title}&nbsp<i class='fa fa-external-link' style='font-size:15px'></i></center></a></h2>`;\n# #     div.text += `<p><strong><center>Answer: <a style='color:blue; font-style:italic'>${detail_data.data['answer'][selection_index]}</a></center></strong></p>`;\n# #     div.text += `<br>`;\n# #     div.text += `<p><strong><center>Abstract</center></strong></p>`;\n# #     div.text += `<p>${abstract}</p>`;\n# #     source_tsne.selected.indices = [source.data['index_from_original'][selection_index]];\n# # \"\"\")\n\n\n# # header_wc = Div(text=f\"<h3>Noun/Subject phrase wordcloud from candidate answers</h3>\")    \n# # wordcloudpng = figure(x_range=(0,1), y_range=(0,1), width=300, height=300)\n# # wordcloudpng.image_url(url='url', x=0, y=1, w=1, h=0.8, source=wordcloud_source)\n# # wc_layout = column(header_wc, Div(text=\"\"), wordcloudpng)\n# # source.selected.js_on_change('indices', select_callback)\n# # select.js_on_change('value', task_select_callback)\n# # subclass_select.js_on_change('value', subclass_callback)\n# # scoretype_select.js_on_change('value', scoretype_callback)\n# # output_file('all_valid.html')\n\n# # # show(column(row(layout, wc_layout),row(column(row(select,scoretype_select), data_table),column(paper_detail))))\n# # # grid = gridplot([layout, wc_layout,column(row(select,scoretype_select), data_table),paper_detail], ncols=2, plot_width=250, plot_height=250)\n# # # show(row(column(layout), column(row(select,scoretype_select), data_table, row(paper_detail), wc_layout)))\n# # title = Div(text = \"<h1><center>Covid-19 Tasks' Candidate Answers<center></h1>\")\n# # show(column(title, row(layout, wc_layout),row(column(row(select,scoretype_select), data_table),paper_detail)))\n# # # show(row(layout, column(select, data_table, paper_detail)))\n# # # show(column(row(column(select, data_table), paper_detail)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Discussion <a name='discussion'></a>\n\n*  During this notebook, we showed in details about how to use the idea of Question Answering in Literature reivew and interactively showed our results for ten interesting task. The results are really promising, because it not only directly give back us a paper list which can possibly answer the given task, but also give a quick answer for the specific paper, and if user are interested in the answer, they coud dive deeper into the paper.\n\n* However, we find that the answer we got in some way not very related with the given topic,  for example \"role of environment in transmission\" task, the model might focus more on transmission than the environment during answer finding. So in the near future, we are thing about combining Albert and Doc2Vec similarity together in fine tune the answer and paperlist\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}