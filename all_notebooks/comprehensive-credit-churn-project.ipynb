{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This is an exploration notebook for the credit card customer churn problem.\n\n## Table of Contents:\n\n* [Data Dictionary](#section-one)\n* [Pakages](#section-two)\n* [Reading and Splitting Data](#section-three)\n* [Initial Analysis](#section-four)\n    - [Target](#section-four-one)\n    - [Feature Type](#section-four-two)\n    - [Correlation](#section-four-three)\n    - [Feature EDA](#section-four-four)\n* [Feature Engineering](#section-five)\n    - [Outlier treatment](#section-five-one)\n* [Pre Processing](#section-six)\n    - [One Hot Encoder](#section-six-one)\n    - [Data Scaling](#section-six-two)\n    - [Class Balance](#section-six-three)\n* [Comparing Algorithms](#section-seven)\n    - [Creating Models](#section-seven-one)\n    - [Is XGBoost Overfitting?](#section-seven-two)\n    - [Measuring XGBoost Performance on Test Set](#section-seven-three)\n    - [Feature Selection](#section-seven-four)\n* [Model Evaluation](#section-eight)\n    - [Final Model](#section-eight-one)\n    - [Model Interpretability - SHAP](#section-eight-two)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Data Dictionary:"},{"metadata":{},"cell_type":"markdown","source":"- **CLIENTNUM**: Client number. Unique identifier for the customer holding the account.\n- **Attrition_Flag**: Internal event (customer activity) variable - if the account is closed then 1 else 0.\n- **Customer_Age**: Demographic variable - Customer's Age in Years\n- **Gender**: Demographic variable - M=Male, F=Female\n- **Dependent_count**: Demographic variable - Number of dependents\n- **Education_Level**: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n- **Marital_Status**: Demographic variable - Married, Single, Divorced, Unknown\n- **Income_Category**: Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n- **Card_Category**: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n- **Months_on_book**: Period of relationship with bank\n- **Total_Relationship_Count**: Total no. of products held by the customer\n- **Months_Inactive_12_mon**: No. of months inactive in the last 12 months\n- **Contacts_Count_12_mon**: No. of Contacts in the last 12 months\n- **Credit_Limit**: Credit Limit on the Credit Card\n- **Total_Revolving_Bal**: Total Revolving Balance on the Credit Card\n- **Avg_Open_To_Buy**: Open to Buy Credit Line (Average of last 12 months)\n- **Total_Amt_Chng_Q4_Q1**: Change in Transaction Amount (Q4 over Q1)\n- **Total_Trans_Amt**: Total Transaction Amount (Last 12 months)\n- **Total_Trans_Ct**: Total Transaction Count (Last 12 months)\n- **Total_Ct_Chng_Q4_Q1**: Change in Transaction Count (Q4 over Q1)\n- **Avg_Utilization_Ratio**: Average Card Utilization Ratio"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic packages\nimport pandas as pd\nimport numpy as np\n\n# Graphs\nimport matplotlib.pyplot as plt\n\n# Feature Engineering\nfrom scipy.stats import boxcox\n\n# Class balance\nfrom imblearn.over_sampling import SMOTENC\n\n# Sklearn\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n\n# ML Algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n# Model evaluation\nimport shap\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n\n# Reading and Splitting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")\n\n# Remove last two columns\ndf = df.iloc[:,:21]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndf.isna().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make target binary\ndf[\"Attrition_Flag\"] = df[\"Attrition_Flag\"].apply(lambda x: 0 if x == 'Existing Customer' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:,df.columns != \"Attrition_Flag\"]\n\ny = df[[\"Attrition_Flag\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove client id column from train set\nX_train = X_train.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Initial Analysis"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-one\"></a>\n## Target"},{"metadata":{},"cell_type":"markdown","source":"There is a class imbalance in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.iloc[:,0].value_counts().plot(kind = \"bar\", rot = 0, title = \"Class distribution - Target\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-two\"></a>\n## Feature type"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n            'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1',\n            'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n\ncat_vars = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-three\"></a>\n## Correlation"},{"metadata":{},"cell_type":"markdown","source":"There seems to be a high correlation between \"Avg_Open_To_Buy\" and \"Credit_Limit\" features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = X_train.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove one of them, decided to remove Credit_Limit at first\nX_train = X_train.drop(columns = \"Credit_Limit\")\n\nnum_vars.remove(\"Credit_Limit\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-four\"></a>\n## Feature EDA"},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features"},{"metadata":{},"cell_type":"markdown","source":"We can see a high churn ratio amongst Platinum card users"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in cat_vars:\n    # Join training objects into a single dataframe\n    feat = pd.DataFrame({\"Feature\": X_train[var].values, \"target\": y_train[\"Attrition_Flag\"].values})\n    # Aggregate count and sum to calculate ratio\n    feat = feat.groupby(by = \"Feature\").agg(count = pd.NamedAgg(column = \"target\", aggfunc = \"count\"),\n                                            sum = pd.NamedAgg(column = \"target\", aggfunc = \"sum\"))\n    feat['churn_ratio'] = feat['sum']/feat['count']\n    \n    # Create subplot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = [10, 4.5])\n    fig.suptitle(var)\n    X_train[var].value_counts().plot(kind = 'bar', ax = ax1, title = \"Bar plot\")\n    feat['churn_ratio'].plot(kind = 'bar', ax = ax2, title = \"Churn Ratio\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical Features"},{"metadata":{},"cell_type":"markdown","source":"For numerical features, we can see some interesting things.\n- As expected, clients who holds more products, tend to be more loyal than those with less products. (Total_Relationship_Count)\n- Clients who less than two months, also tend to churn less. However, if the clients stays inactive for 3 or 4 months, chances are higher that they will cancel their products. (Months_Inactive_12_mon)\n- Also, the more the client has to get in touch with the bank, the more insatified they are. (Contacts_Count_12_mon)\n... and so on\n\nAs for outliers, Avg_Open_To_Buy, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt and Total_Ct_Chng_Q4_Q1 features must be treated more closel"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in num_vars:\n    # Join training objects into a single dataframe\n    feat = pd.DataFrame({\"Feature\": X_train[var].values, \"target\": y_train[\"Attrition_Flag\"].values})\n    \n    # Create subplot\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = [14, 4.5])\n    fig.suptitle(var)\n    \n    X_train[var].plot(kind = 'hist', ax = ax1, title = \"Bar plot\")\n    \n    X_train[var].plot(kind = 'box', ax = ax2, title = \"Box plot\")\n    \n    feat[\"Feature\"].plot(kind = 'kde', ax = ax3, title = \"Churn ratio\", label = \"Total Distribuition\")\n    feat.loc[feat[\"target\"] == 1, \"Feature\"].plot(kind = 'kde', ax = ax3, label = \"Churned\")\n    ax3.legend(loc = 2)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bi-variate analysis"},{"metadata":{},"cell_type":"markdown","source":"We could plot all features against each other and see how they behave. For simplicity, all plot an example of categorical vs categorical, categorical vs numerical and numerical vs numerical"},{"metadata":{},"cell_type":"markdown","source":"#### Education_Level vs Income_Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"bivariate_cat = X_train[['Education_Level', 'Income_Category', 'Gender']].groupby(by = ['Education_Level', 'Income_Category']).agg(count = pd.NamedAgg(column = 'Gender', aggfunc = 'count'))\n\nbivariate_cat = bivariate_cat.reset_index().pivot(index = 'Education_Level', columns = 'Income_Category', values = 'count')\n\n# Sum row-wise\nbivariate_cat['Total'] = bivariate_cat.sum(axis = 1)\n\n# Calculate % by class, except Total\nfor column in bivariate_cat.columns:\n    if column != 'Total':\n        bivariate_cat[column] = 100*(bivariate_cat[column]/bivariate_cat['Total'])\n\n# Plot without Total column\nbivariate_cat.drop(columns = 'Total').plot.bar(stacked = True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gender vs Avg_Open_To_Buy"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = X_train[['Gender', 'Avg_Open_To_Buy']].pivot(columns = 'Gender').boxplot()\n\n# Change x-axis labels\nax.set_xticklabels(['F', 'M'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Total_Trans_Ct vs Avg_Utilization_Ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nX_train[['Total_Trans_Amt', 'Total_Trans_Ct']].join(y_train).plot.scatter(x = 'Total_Trans_Ct', \n                                                                          y = 'Total_Trans_Amt', \n                                                                          c = 'Attrition_Flag',\n                                                                          colormap = 'viridis',\n                                                                          ax = ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-one\"></a>\n## Outlier treatment"},{"metadata":{},"cell_type":"markdown","source":"The graphs beneath show two different kinds of treatments that could be applied to features in order to adjust outlier values. \n\nFor a better understanding on how boxcox works: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n\nFor these transformations, if the minimun value of a feature is 0, then we have to add 1 to it (otherwise, log(0) = inf)"},{"metadata":{},"cell_type":"markdown","source":"### Avg_Open_To_Buy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The minimun value of the feature is:\", X_train['Avg_Open_To_Buy'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize = [14, 9])\n\nX_train['Avg_Open_To_Buy'].plot(kind = 'box', ax = axs[0, 0], title = 'As is boxplot')\n\nX_train['Avg_Open_To_Buy'].apply(np.log).plot(kind = 'box', ax = axs[0, 1], title = 'Log transformation boxplot')\n\nvalues, lmbda = boxcox(X_train['Avg_Open_To_Buy'])\n\npd.DataFrame({'Avg_Open_To_Buy': values}).plot(kind = 'box', ax = axs[0, 2], title = 'Boxcox transformation boxplot')\n\nX_train['Avg_Open_To_Buy'].plot(kind = 'hist', ax = axs[1, 0], title = 'As is histogram')\n\nX_train['Avg_Open_To_Buy'].apply(np.log).plot(kind = 'hist', ax = axs[1, 1], title = 'Log transformation histogram')\n\npd.DataFrame({'Avg_Open_To_Buy': values}).plot(kind = 'hist', ax = axs[1, 2], title = 'Boxcox transformation histogram')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Trans_Amt"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The minimun value of the feature is:\", X_train['Total_Trans_Amt'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize = [14, 9])\n\nX_train['Total_Trans_Amt'].plot(kind = 'box', ax = axs[0, 0], title = 'As is boxplot')\n\nX_train['Total_Trans_Amt'].apply(np.log).plot(kind = 'box', ax = axs[0, 1], title = 'Log transformation boxplot')\n\nvalues, lmbda = boxcox(X_train['Total_Trans_Amt'])\n\npd.DataFrame({'Total_Trans_Amt': values}).plot(kind = 'box', ax = axs[0, 2], title = 'Boxcox transformation boxplot')\n\nX_train['Total_Trans_Amt'].plot(kind = 'hist', ax = axs[1, 0], title = 'As is histogram')\n\nX_train['Total_Trans_Amt'].apply(np.log).plot(kind = 'hist', ax = axs[1, 1], title = 'Log transformation histogram')\n\npd.DataFrame({'Total_Trans_Amt': values}).plot(kind = 'hist', ax = axs[1, 2], title = 'Boxcox transformation histogram')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Amt_Chng_Q4_Q1"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The minimun value of the feature is:\", X_train['Total_Amt_Chng_Q4_Q1'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize = [14, 9])\n\nX_train['Total_Amt_Chng_Q4_Q1'].plot(kind = 'box', ax = axs[0, 0], title = 'As is boxplot')\n\nX_train['Total_Amt_Chng_Q4_Q1'].apply(lambda x: np.log(x + 1)).plot(kind = 'box', ax = axs[0, 1], title = 'Log transformation boxplot')\n\nvalues, lmbda = boxcox(X_train['Total_Amt_Chng_Q4_Q1'].apply(lambda x: x + 1))\n\npd.DataFrame({'Total_Amt_Chng_Q4_Q1': values}).plot(kind = 'box', ax = axs[0, 2], title = 'Boxcox transformation boxplot')\n\nX_train['Total_Amt_Chng_Q4_Q1'].plot(kind = 'hist', ax = axs[1, 0], title = 'As is histogram')\n\nX_train['Total_Amt_Chng_Q4_Q1'].apply(lambda x: np.log(x + 1)).plot(kind = 'hist', ax = axs[1, 1], title = 'Log transformation histogram')\n\npd.DataFrame({'Total_Amt_Chng_Q4_Q1': values}).plot(kind = 'hist', ax = axs[1, 2], title = 'Boxcox transformation histogram')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Ct_Chng_Q4_Q1"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The minimun value of the feature is:\", X_train['Total_Ct_Chng_Q4_Q1'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize = [14, 9])\n\nX_train['Total_Ct_Chng_Q4_Q1'].plot(kind = 'box', ax = axs[0, 0], title = 'As is boxplot')\n\nX_train['Total_Ct_Chng_Q4_Q1'].apply(lambda x: np.log(x + 1)).plot(kind = 'box', ax = axs[0, 1], title = 'Log transformation boxplot')\n\nvalues, lmbda = boxcox(X_train['Total_Ct_Chng_Q4_Q1'].apply(lambda x: x + 1))\n\npd.DataFrame({'Total_Ct_Chng_Q4_Q1': values}).plot(kind = 'box', ax = axs[0, 2], title = 'Boxcox transformation boxplot')\n\nX_train['Total_Ct_Chng_Q4_Q1'].plot(kind = 'hist', ax = axs[1, 0], title = 'As is histogram')\n\nX_train['Total_Ct_Chng_Q4_Q1'].apply(lambda x: np.log(x + 1)).plot(kind = 'hist', ax = axs[1, 1], title = 'Log transformation histogram')\n\npd.DataFrame({'Total_Ct_Chng_Q4_Q1': values}).plot(kind = 'hist', ax = axs[1, 2], title = 'Boxcox transformation histogram')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that even boxcox could not handle all the outliers. However, in most of them the distribution became much closer to a normal distribution. Therefore, let's use it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_box = X_train.copy()\n\n# Generate lambda values for features\noutlier_vars = ['Avg_Open_To_Buy', 'Total_Trans_Amt', 'Total_Amt_Chng_Q4_Q1', 'Total_Ct_Chng_Q4_Q1']\n\nlmbdas_treatment = {}\n\nfor var in outlier_vars:\n    if (X_train_box[var].min() > 0):\n        values, lmbda = boxcox(X_train_box[var])\n        X_train_box[var] = values\n        lmbdas_treatment[var] = lmbda\n    else:\n        values, lmbda = boxcox(X_train_box[var].apply(lambda x: x + 1))\n        X_train_box[var] = values\n        lmbdas_treatment[var] = lmbda","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# Pre Processing"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-one\"></a>\n## One Hot Encoding Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create encoder and fit encoder\nencoder = OneHotEncoder(drop = 'first', sparse = False)\n\nencoder = encoder.fit(X_train[cat_vars])\n\n# Create new training set\nX_train_new = pd.DataFrame(encoder.transform(X_train[cat_vars]), columns = encoder.get_feature_names(cat_vars))\n\nX_train_new = X_train_new.join(X_train[num_vars].reset_index(drop = True))\n\n# Create new training set with boxcox\nX_train_box_new = pd.DataFrame(encoder.transform(X_train_box[cat_vars]), columns = encoder.get_feature_names(cat_vars))\n\nX_train_box_new = X_train_box_new.join(X_train_box[num_vars].reset_index(drop = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-two\"></a>\n## Data scaling"},{"metadata":{},"cell_type":"markdown","source":"We have numerical features in all sorts of scales. For distance based algorithms, we need them to be in a compatible scale, so we do not assign a higher weight to a feature just because of its values.\n\nWe have two most used option for it: Normalization and Standardization\n\nNormalization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n\nStandardization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"},{"metadata":{},"cell_type":"markdown","source":"### Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"normal_scaler = MinMaxScaler()\n\nnormal_scaler = normal_scaler.fit(X_train_new[num_vars])\n\nX_train_normal_new = X_train_new.copy()\n\nX_train_normal_new[num_vars] = normal_scaler.transform(X_train_new[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal_box_scaler = MinMaxScaler()\n\nnormal_box_scaler = normal_box_scaler.fit(X_train_box_new[num_vars])\n\nX_train_box_normal_new = X_train_box_new.copy()\n\nX_train_box_normal_new[num_vars] = normal_scaler.transform(X_train_box_new[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"standard_scaler = StandardScaler()\n\nstandard_scaler = standard_scaler.fit(X_train_new[num_vars])\n\nX_train_standard_new = X_train_new.copy()\n\nX_train_standard_new[num_vars] = normal_scaler.transform(X_train_new[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standard_box_scaler = StandardScaler()\n\nstandard_scaler = standard_scaler.fit(X_train_box_new[num_vars])\n\nX_train_box_standard_new = X_train_box_new.copy()\n\nX_train_box_standard_new[num_vars] = normal_scaler.transform(X_train_box_new[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-three\"></a>\n## Balance classes"},{"metadata":{},"cell_type":"markdown","source":"There are quite a few ways of handling imbalanced datasets\n- We can just leave as it is, in such case we should choose a scoring metric accordingly (accuracy would just tell you to choose majority class)\n- We can use stratified cross validation, in order to adjust variance during training step\n- Undersample majority class\n- Oversample minority class\n- Generate synthetic data, etc.\n\nEach method has its ups and downs. I'll apply here synthetic data generation to balance class and try to keep it as representative to reality as possible. However, I AM chaging the way the algorithm calculates probabilities, therefore choose carefully whatever approach and plan accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 100\n\n# Categorical features\nnew_cat_vars = encoder.get_feature_names(cat_vars)\n\ncat_vars_index = []\n\nfor var in encoder.get_feature_names(cat_vars):\n    cat_vars_index.append(X_train_new.columns.get_loc(var))\n\n# Create SMOTE object to balance minority class\noversample = SMOTENC(categorical_features = cat_vars_index, random_state = seed)\n\n# Apply SMOTE to training set without any FE\nX_train_balanced, y_train_balanced = oversample.fit_resample(X_train_new, y_train)\n\n# Apply SMOTE to training set with normalization\nX_train_normal_balanced, y_train_normal_balanced = oversample.fit_resample(X_train_normal_new, y_train)\n\n# Apply SMOTE to training set with standardization\nX_train_std_balanced, y_train_std_balanced = oversample.fit_resample(X_train_standard_new, y_train)\n\n# Apply SMOTE to training set with boxcox\nX_train_box_balanced, y_train_box_balanced = oversample.fit_resample(X_train_box_new, y_train)\n\n# Apply SMOTE to training set with boxcox and normalization\nX_train_box_normal_balanced, y_train_box_normal_balanced = oversample.fit_resample(X_train_box_normal_new, y_train)\n\n# Apply SMOTE to training set with boxcox and standardization\nX_train_box_std_balanced, y_train_box_std_balanced = oversample.fit_resample(X_train_box_standard_new, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_balanced.iloc[:,0].value_counts().plot(kind = 'bar', rot = 0, title = \"Balanced target\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n# Comparing performances"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-one\"></a>\n## Create models"},{"metadata":{},"cell_type":"markdown","source":"I'll set max_depth to 4, due to training dataset size. By doing so, we at least try to avoid overfitting the model. Changing learning rate, gamma (for XGBoost), among other parameters could also help avoiding it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo os valores para o n√∫mero de folds\nnum_folds = 10\nscoring = 'roc_auc'\nseed = 7\n\n# Preparando a lista de modelos\nmodels = []\nmodels.append(('LR w/o FE', LogisticRegression()))\nmodels.append(('RF w/o FE', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB w/o FE', XGBClassifier(max_depth = 4, verbosity = 0)))\nmodels.append(('LR normal', LogisticRegression()))\nmodels.append(('RF normal', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB normal', XGBClassifier(max_depth = 4, verbosity = 0)))\nmodels.append(('LR std', LogisticRegression()))\nmodels.append(('RF std', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB std', XGBClassifier(max_depth = 4, verbosity = 0)))\nmodels.append(('LR boxcox', LogisticRegression()))\nmodels.append(('RF boxcox', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB boxcox', XGBClassifier(max_depth = 4, verbosity = 0)))\nmodels.append(('LR boxcox normal', LogisticRegression()))\nmodels.append(('RF boxcox normal', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB boxcox normal', XGBClassifier(max_depth = 4, verbosity = 0)))\nmodels.append(('LR boxcox standard', LogisticRegression()))\nmodels.append(('RF boxcox standard', RandomForestClassifier(max_depth = 4, criterion = 'entropy')))\nmodels.append(('XGB boxcox standard', XGBClassifier(max_depth = 4, verbosity = 0)))\n\n# Avaliando cada modelo em um loop\nresults = []\nnames = []\n\nfor name, model in models:\n    # Create KFold validation\n    kfold = KFold(n_splits = num_folds, random_state = seed, shuffle = True)\n    # Conditions for each treatment\n    if (\"w/o FE\" in name):\n        cv_results = cross_val_score(model, \n                                     X_train_balanced, \n                                     y_train_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    elif ((\"boxcox\" not in name) & (\"normal\" in name)):\n        cv_results = cross_val_score(model, \n                                     X_train_normal_balanced, \n                                     y_train_normal_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    elif ((\"boxcox\" not in name) & (\"std\" in name)):\n        cv_results = cross_val_score(model, \n                                     X_train_std_balanced, \n                                     y_train_std_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    elif ((\"boxcox\" in name) & (\"normal\" in name)):\n        cv_results = cross_val_score(model, \n                                     X_train_box_normal_balanced, \n                                     y_train_box_normal_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    elif ((\"boxcox\" in name) & (\"standard\" in name)):\n        cv_results = cross_val_score(model, \n                                     X_train_box_std_balanced, \n                                     y_train_box_std_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    else: \n        cv_results = cross_val_score(model, \n                                     X_train_box_balanced, \n                                     y_train_box_balanced, \n                                     cv = kfold, \n                                     scoring = scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize = [14, 9])\nfig.suptitle('Comparing Classification Algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nplt.xticks(rotation = 45)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-two\"></a>\n## Is XGB classifier overfitting?"},{"metadata":{},"cell_type":"markdown","source":"As the XGB performance were all pretty close, for simplicity of treatment (and deployment later on) I'll use the model with no FE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform train set\nX_test_encoded = encoder.transform(X_test[cat_vars])\n\n# Concatenate one hot encoded categorical features with numerical\nX_test_new = pd.DataFrame(X_test_encoded, columns = encoder.get_feature_names(cat_vars))\n\nX_test_new = X_test_new.join(X_test[num_vars].reset_index(drop = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create classifier\nmodel = XGBClassifier(max_depth = 4)\n\n# Create eval set with both training and test set\neval_set = [(X_train_balanced, y_train_balanced), (X_test_new, y_test)]\n\n# Fit model \nmodel.fit(X_train_balanced, y_train_balanced, eval_metric=[\"auc\", \"logloss\"], eval_set=eval_set, verbose=False)\n\n# Make predictions for test data\ny_pred = model.predict(X_test_new)\n\n# Evaluate predictions\nauc = roc_auc_score(y_test, y_pred)\nprint(\"AUC: %.2f%%\" % (auc * 100.0))\n\n# Retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n\n# Plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n\n# Plot AUC\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Test')\nax.legend()\nplt.ylabel('AUC')\nplt.title('XGBoost AUC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, log loss and auc gain on both training and test set stay pretty close over training rounds, so there is no evidence of overfitting. On the other hand, as stated previously, this is a small dataset and predictions may get a little off over time."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-three\"></a>\n## Measuring XGBoost Performance on Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create classifier\nxgb = XGBClassifier(max_depth = 4, verbosity = 0)\n\n# Fit model\nxgb = xgb.fit(X_train_balanced, y_train_balanced)\n\n# Make Predictions\ny_pred = xgb.predict_proba(X_test_new)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ROC curve variables\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred)\n\n# Print model AUC\nprint(\"AUC: %.2f%%\" % (roc_auc_score(y_test, y_pred) * 100.0))\n\n# Print best threshold\noptimal_idx = np.argmax(true_positive_rate - false_positive_rate)\noptimal_threshold = threshold[optimal_idx]\nprint(\"Best threshold value is:\", optimal_threshold)\n\n# Ploting ROC curves\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform predictions to round number\ny_pred_round = [1 if pred > optimal_threshold else 0 for pred in y_pred]\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred_round)\n\ncmd = ConfusionMatrixDisplay(cm, display_labels=['not churn','churn'])\ncmd.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-four\"></a>\n## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"We could lower model complexity by using only the most suitable features. This can be achieved with lots of techniques.\n\nI'll use mutual information gain as score function. Mutual information gain is a non parametric test, which means it does not make any assumptions on the distribution  of data. Chi-square is another option here, however it does assume the features are normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mutual info score\ninfo_score = mutual_info_classif(X_train_balanced, y_train_balanced, n_neighbors = 3, random_state = seed)\n\nmutual_info = pd.DataFrame({'Feature': X_train_balanced.columns.values, 'Mutual Info Gain': info_score})\n\nfig, ax = plt.subplots(figsize = [10,6])\nmutual_info.sort_values(by = 'Mutual Info Gain').plot.barh(x = 'Feature', y = 'Mutual Info Gain', ax = ax, title = \"Mutual Information Gain by Feature\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KBest processor\nbest_processor = SelectKBest(score_func = mutual_info_classif, k = 12).fit(X_train_balanced, y_train_balanced)\n\n# Transform training set\ncolumns = X_train_balanced.columns.values[best_processor.get_support(indices=True)]\n\nX_train_balanced_fs = pd.DataFrame(best_processor.transform(X_train_balanced), \n                                   columns = columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform test set\nX_test_new_fs = X_test_new[columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train new model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create classifier\nxgb = XGBClassifier(max_depth = 4, verbosity = 0)\n\n# Tuple of sets\n#eval_set = [(X_train_balanced, y_train_balanced), (X_test_new, y_test)]\n\n# Fit model\nxgb = xgb.fit(X_train_balanced_fs, y_train_balanced)\n\n# Make Predictions\ny_pred = xgb.predict_proba(X_test_new_fs)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ROC curve variables\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred)\n\n# Print model AUC\nprint(\"AUC: %.2f%%\" % (roc_auc_score(y_test, y_pred) * 100.0))\n\n# Print best threshold\noptimal_idx = np.argmax(true_positive_rate - false_positive_rate)\noptimal_threshold = threshold[optimal_idx]\nprint(\"Best threshold value is:\", optimal_threshold)\n\n# Ploting ROC curves\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform predictions to round number\ny_pred_round = [1 if pred > optimal_threshold else 0 for pred in y_pred]\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred_round)\n\ncmd = ConfusionMatrixDisplay(cm, display_labels=['not churn','churn'])\ncmd.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n# Model evaluation"},{"metadata":{},"cell_type":"markdown","source":"There should be a step of hyperparameters tuning before evaluating the final model, but the performance is so high alreay that I don't think it is necessary in this case.\n\nSo, let's jump to understanding why the classifier gave the results it did. To do so, SHAP package is one of the best.\n\nTo better understand SHAP: https://github.com/slundberg/shap and https://shap.readthedocs.io/en/latest/index.html"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight-one\"></a>\n## Final model"},{"metadata":{},"cell_type":"markdown","source":"Using model with all features to see how changes in each of them impact the outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create classifier\nxgb_final = XGBClassifier(max_depth = 4, verbosity = 0)\n\n# Fit model\nxgb_final = xgb_final.fit(X_train_balanced, y_train_balanced)\n\n# Make Predictions\ny_pred = xgb_final.predict_proba(X_test_new)[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight-two\"></a>\n## How feature values impact classification?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create explainer object\nexplainer = shap.TreeExplainer(xgb_final)\nshap_values = explainer.shap_values(X_train_balanced)\n\nprint(\"Business person: Why is this person being rated as such? \\nData scientist: Historically, people who buy and spend approximately 1.35 times more on Q4 compared to Q1 are less prone to churning\")\n\n# Visualize prediction\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train_balanced.iloc[0,:], matplotlib=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Business person: Cool! Ok, so... what else affects the output in general? \\nData scientist: Total transaction count greatly impacts the outcome. In general, people who make many purchases tend to churn less. On the other hand, people who spend a lot are proner to churning. An explanation to that could be that people with high average ticket sales may cancel their credit card to cut some expenses!\")\n\n# Plot \nshap.summary_plot(shap_values, X_train_balanced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at how the combination of values of two features could affect the logs odd of the outcome can be really insightful as well!"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in X_train_balanced.columns:\n    shap.dependence_plot(var, shap_values, X_train_balanced)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}