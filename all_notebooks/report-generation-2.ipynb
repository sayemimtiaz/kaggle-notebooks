{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download necessary packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\n\n#import swifter\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  =pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings'].iloc[0:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['impression'].unique().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MeSH'].unique().tolist()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('/kaggle/input/chest-xrays-indiana-university/images/images_normalized/1_IM-0001-3001.dcm.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.projection.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build vocab","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text cleaner**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['report'] = df[df.columns[1:]].apply(\n    lambda x: ','.join(x.astype(str)),\n    axis=1\n)\ndf['report'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['report'] = df['report'].apply(normalize_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['report'] = '<start> '+df['report']+' <end>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = []\nfor row in df['report'].tolist():\n    num_words.append(len(word_tokenize(row)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words= np.array(num_words)\nprint(\"min length             : \", num_words.min())\nprint(\"max length             : \", num_words.max())\nprint(\"50th percentile length : \", np.percentile(num_words,50))\nprint(\"75th percentile length : \", np.percentile(num_words,75))\nprint(\"90th percentile length : \", np.percentile(num_words,90))\nprint(\"95th percentile length : \", np.percentile(num_words,95))\nprint(\"98th percentile length : \", np.percentile(num_words,98))\nprint(\"98th percentile length : \", np.percentile(num_words,99))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nmax_len = 260\n\ntok = Tokenizer(num_words=vocab_size,  oov_token='UNK', )\ntok.fit_on_texts(df['report'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge Images Path","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2[df2['projection']=='Frontal']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  =pd.merge(df,df2,  on=['uid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass det_gen(tensorflow.keras.utils.Sequence):\n    'Generates data from a Dataframe'\n    def __init__(self,df, tok, max_len,images_path, dim=(256,256), batch_size=8):\n        self.df=df\n        self.dim = dim\n        self.images_path = images_path\n        self.tok= tok\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.nb_iteration = math.ceil((self.df.shape[0])/self.batch_size)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.nb_iteration\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.df=self.df.sample(frac=1)\n    \n    def load_img(self, img_path):\n        \n        img = cv2.imread(img_path)\n        img =cv2.resize(img,(self.dim))\n        \n        \n        return img\n        \n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        \n        indicies = list(range(index*self.batch_size, min((index*self.batch_size)+self.batch_size ,(self.df.shape[0]))))\n        \n        images = []\n        for img_path in self.df['filename'].iloc[indicies].tolist():\n            img = self.load_img(os.path.join(self.images_path,img_path))\n            images.append(img)\n            \n            \n        \n        \n        x_batch = self.df['report'].iloc[indicies].tolist()\n        \n        x_batch_input = [sample[:-len(\" <end>\")] for sample in x_batch]\n        \n        x_batch_gt = [sample[len(\" <start>\"): ] for sample in x_batch]\n        \n        \n        x_batch_input = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_input),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        x_batch_gt = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_gt),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        \n        \n        \n        \n        \n        return [np.array(images), np.array(x_batch_input)] , np.array(x_batch_gt)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_split= 0.2\ndf = df.sample(frac=1)\ndf_train = df.iloc[:-int(df.shape[0]*validation_split)]\ndf_val   = df.iloc[-int(df.shape[0]*validation_split):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_path = \"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"\ntrain_dataloader =  det_gen(df_train, tok, max_len,images_path)\nval_dataloader =  det_gen(df_val, tok, max_len,images_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[X_img,X_report] ,Y = next(enumerate(train_dataloader))[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_img[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_report[0])\nprint(\"====================\")\nprint(Y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=None,\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\nencoder_output = layers.Dropout(0.2)(encoder_output)\nencoder_output = layers.Dense(512,activation='relu')(encoder_output)\n\n##decoder ########################\n\n#layers\ngru_layer =  layers.GRU(512, return_sequences=True)\ndense_layer= layers.Dense(vocab_size,activation='softmax')\nembedding_layer = layers.Embedding(vocab_size, 300, mask_zero=True)\ndropout = layers.Dropout(0.2)\n\n# decoder model\nembedding_output = embedding_layer(report_input)\ngru_output = gru_layer(embedding_output, initial_state=encoder_output )\ngru_output = dropout(gru_output)\noutput  = dense_layer(gru_output)\nmodel = Model([img_input,report_input ],output)\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,encoder_output)\n\n#decoder_inference model\nprev_hidden_state= layers.Input(shape= (512))\nreport_input2 = layers.Input(shape= (1,))\nembedding_output2= embedding_layer(report_input2)\ngru_output2 = gru_layer(embedding_output2, initial_state=prev_hidden_state )\ngru_output2 = dropout(gru_output2)\noutput2 = dense_layer(gru_output2)\ndecoder_model = Model([report_input2,prev_hidden_state],[output2,gru_output2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs =5\nlr=1e-3\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom nltk.translate.bleu_score import corpus_bleu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokens_to_text(tokens,tok,end_token='end'):\n    sentence=\"\"\n    for token in tokens:\n        if token ==0:\n            break\n        \n        word = tok.index_word[token]\n        \n        if word==end_token:\n            break\n            \n        sentence+= word+\" \"\n        \n    sentence = sentence.strip()\n    \n    return sentence\n\n\ndef greedy_inference(input_img, tok,encoder_model, decoder_model,max_len,start_token=\"start\",end_token='end',decoder_type=\"GRU\"):\n    if decoder_type=='LSTM':\n        a0,c0  =encoder_model(np.expand_dims(input_img,axis=0))\n    elif decoder_type=='GRU': \n        hidden_layer  =encoder_model(np.expand_dims(input_img,axis=0))\n        \n    word = tok.word_index[start_token]\n    \n    words = []\n    \n    for index in range(max_len):\n        if decoder_type=='LSTM':\n            word_probs , a0,c0 = decoder_model.predict([[np.array([word]),a0,c0]])\n        elif decoder_type=='GRU': \n            word_probs , hidden_layer = decoder_model.predict([[np.array([word]),hidden_layer]])\n            hidden_layer=hidden_layer[0]\n        \n        word = np.argmax(word_probs)\n        \n        try:\n            if tok.index_word[word]==end_token:\n                break\n        except:\n            pass\n        \n        words.append(word)\n        \n    words = tokens_to_text(words,tok)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"start\"\n                                     ,end_token='end', inference_type='greedy',decoder_type='GRU'):\n    \n    data_loader_iterator = data_loader.__iter__()\n    \n    pred_sentences = []\n    Gt_sentences = []\n    for index, (X,Y) in enumerate(data_loader_iterator):\n        for img,_,sample_y in zip(X[0],X[1],Y):\n            \n            if inference_type=='greedy':\n                pred_sentence = greedy_inference(img, tok,encoder_model, decoder_model,max_len,\n                                                 start_token=\"start\",end_token='end',decoder_type='GRU')\n            \n            GT_sentence   = tokens_to_text(sample_y,tok)\n            \n            pred_sentences.append(pred_sentence)\n            Gt_sentences.append(GT_sentence)\n        \n        if index == data_loader.nb_iteration -1:\n            break\n        print(\"Done with batch number: {} \", index)\n        \n    return Gt_sentences, pred_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n    BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=(1.0, 0, 0, 0))\n    BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.5, 0.5, 0, 0))\n    BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.3, 0.3, 0.3, 0))\n    BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_from_dataloader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"start\",end_token='end', inference_type='greedy',decoder_type=\"GRU\"):\n    Gt_sentences, pred_sentences = get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=start_token,end_token=end_token, inference_type=inference_type,decoder_type=decoder_type)\n    BLEU_1,BLEU_2,BLEU_3,BLEU_4 = calculate_bleu_evaluation(Gt_sentences, pred_sentences)\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEU_1,BLEU_2,BLEU_3,BLEU_4 =  evaluate_from_dataloader(val_dataloader,tok,encoder_model, decoder_model,max_len)\nprint(BLEU_1)\nprint('-------')\nprint(BLEU_2)\nprint('-------')\nprint(BLEU_3)\nprint('-------')\nprint(BLEU_4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}