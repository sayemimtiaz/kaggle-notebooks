{"cells":[{"metadata":{"_cell_guid":"7d54d376-cf14-41cd-a33f-462c996bac77","_uuid":"f7d623bfa6b9aa6ff8f7e3cae2bb91eff7fc2634"},"cell_type":"markdown","source":"# Energy Efficiency of Residential Buildings"},{"metadata":{"_cell_guid":"43e3f27b-cfbe-4e92-9b18-da0caafa9fdc","_uuid":"c83c9e67af9f793f93fdd112a6feb7ff8f737d59"},"cell_type":"markdown","source":"# A. Problem Understanding"},{"metadata":{"_cell_guid":"c1774855-5db6-48a8-94eb-de51360cec1e","_uuid":"d6f76345505474a648d605783ee5bd7ac458616b"},"cell_type":"markdown","source":"When it comes to efficient building design, the computation of the heating load (HL) and the cooling load (CL) is\nrequired to determine the specifications of the heating and cooling equipment needed to maintain comfortable indoor\nair conditions. In order to estimate the required cooling and heating capacities, architects and building desioners\nneed information about the characteristics of the building and of the conditioned space (for example occupancy and\nactivity level). For this reason, we will investigate the effect of eight input variables: (RC), surface area, wall area, roof area, overall height, orientation, glazing area, and glazing area distribution, to determine the output variables HL and CL of residential buildings."},{"metadata":{"_cell_guid":"794ce63b-d5f8-49fc-9f0b-5f29e36c9c98","_uuid":"1c0358bdeb1c9a64495afa6871c846f771120f40"},"cell_type":"markdown","source":"To evaluate our model performance we will use R squared (R2 score). R-squared is a statistical measure of how close the data are to the fitted regression line. This is very important to create predictions that are close to the real values. In this case we want to achieve a high R squared, the higher the R squared, the better the model fits the data."},{"metadata":{"_cell_guid":"ef2041b7-fbdb-4766-a394-0aa4e8ffa6d8","_uuid":"cf73ce3084b8f6dc4b08cef5834ee4785b41ed27"},"cell_type":"markdown","source":"# B. Data Understanding"},{"metadata":{"_cell_guid":"189834cb-666a-41f0-97db-178d3d35a51a","_uuid":"f199d00b109fe96ebd3ca63975ed49c9f50cba00"},"cell_type":"markdown","source":"We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer."},{"metadata":{"_cell_guid":"5f35f074-d816-4558-b473-69aa4b80c732","_uuid":"b3371bdb117f09e38d975067a43ddcdfdf7b4325"},"cell_type":"markdown","source":"#### 1. Data Description"},{"metadata":{"_cell_guid":"def49a95-40dc-4fad-ac2b-d358989e5450","_uuid":"8e77779de13079685f7c2734850217d150d6562a"},"cell_type":"markdown","source":"**ENB2012_data.xlsx**"},{"metadata":{"_cell_guid":"69c8f977-947b-4c89-a59c-1cdcafcaf6a4","_uuid":"d47ff808fd957400aebe3bd78d4c491f92f9d47a"},"cell_type":"markdown","source":"The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses. \n\nSpecifically: \n* X1\tRelative Compactness \n* X2\tSurface Area \n* X3\tWall Area \n* X4\tRoof Area \n* X5\tOverall Height \n* X6\tOrientation \n* X7\tGlazing Area \n* X8\tGlazing Area Distribution \n* y1\tHeating Load \n* y2\tCooling Load"},{"metadata":{"_cell_guid":"ec7a47f7-8ac3-4be2-aecb-15d6902fd5f5","_uuid":"21c70c0ea6d4fe79cc1e3fcd6d34b4dc51eb59b0"},"cell_type":"markdown","source":"#### 2. Load The Data"},{"metadata":{"collapsed":true,"_cell_guid":"39061da1-f0eb-44c2-a25b-1d1494082cf2","_uuid":"d77708f26ae3ec9f84c95ca31e5717768a87aa26","trusted":false},"cell_type":"code","source":"#import library\nimport pandas as pd\nimport pandas_profiling\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\n#import data\ndata = pd.read_csv('../input/ENB2012_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d18134ee-f5a2-482d-8ca5-22955428569b","_uuid":"adab4fa8101ce5b21e6a4f9a61de2de638134135","trusted":false},"cell_type":"code","source":"# Read the data to g an overview\ndata","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"109060b6-cfa4-4c21-b97d-d3e16d82864a","_uuid":"c8ec27f44099daac0f865c211d9c84683f82e89a","trusted":false},"cell_type":"code","source":"#Rename columns\ndata.columns = ['relative_compactness', 'surface_area', 'wall_area', 'roof_area', 'overall_height',\n                'orientation', 'glazing_area', 'glazing_area_distribution', 'heating_load', 'cooling_load']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a3e7e28-e1f4-48e7-bd74-1e0e834b0ce8","_uuid":"a7adb079b749ffa57c4ad486127cb822ead182e6"},"cell_type":"markdown","source":"#### 3. Data Types "},{"metadata":{"collapsed":true,"_cell_guid":"2571d54c-4de6-4147-a3c7-33da34bb576e","_uuid":"03e54b7b29c56a0916b917daa8a51e003bff698a","trusted":false},"cell_type":"code","source":"# Memory usage and data types\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d075f197-bba7-4da5-9d42-84059ca819b6","_uuid":"e2bc28fbe6d38286ab7fc0b5497a02bed091f657"},"cell_type":"markdown","source":"# C. Data Exploration"},{"metadata":{"_cell_guid":"c2934846-ee36-4b3a-92b8-8ae3ebb5284c","_uuid":"598a33516126d8b911a1d95be656c06c9f772fd1"},"cell_type":"markdown","source":"On the data exploration, we will see the distribution of each variable using a histogram. In the histogram, the horizontal axis is the data of the feature while the vertical axis is the frequency of occurrence. The correlation test is used to evaluate the relationship between two numerical variables. If two variables have a correlation coefficient, then the two variables are numerical variables, while the remainder are categorical variables."},{"metadata":{"_cell_guid":"f677d95d-1168-415c-8616-b99fbb54e931","_uuid":"6ad003a2058cf2950de71b4361b413edae5d349b"},"cell_type":"markdown","source":"Let's get an overview of variables and its distribution."},{"metadata":{"collapsed":true,"scrolled":false,"_cell_guid":"806f136a-1d11-4433-9a73-2dee7cace897","_uuid":"5a9f0f7cba63a5730cc54277178d26daf577f3ed","trusted":false},"cell_type":"code","source":"# Variables & Distribution\npandas_profiling.ProfileReport(data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"02007dd7-3611-472f-8158-15a9baa7c33c","_uuid":"f01bc18ffe0740112a0b5f04e717771b64fe3d77"},"cell_type":"markdown","source":"From the above distributions, an interesting fact is that the unique value of the data are not so many\n* X1\tRelative Compactness has 12 possible values\n* X2\tSurface Area has 12 possible values\n* X3\tWall Area has 7 possible values\n* X4\tRoof Area has 4 possible values\n* X5\tOverall Height has 2 possible values\n* X6\tOrientation has 4 possible values\n* X7\tGlazing Area has 4 possible values\n* X8\tGlazing Area Distribution has 6 possible values\n* y1\tHeating Load has 586 possible values\n* y2\tCooling Load has 636 possible values"},{"metadata":{"_cell_guid":"9537b7bd-eeae-4189-bb44-fb12a63ff31f","_uuid":"6335c26fcd00c8ff8fbb9237452e41c288905a55"},"cell_type":"markdown","source":"Now, we want to know the correlation between variables in numbers"},{"metadata":{"collapsed":true,"_cell_guid":"18362c24-7d90-4a9f-b5ba-a4f876cf6098","_uuid":"cb9a60837681797fcca1885178899983cc4e2dd0","trusted":false},"cell_type":"code","source":"# Preview correlation\nplt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d58bde27-36f1-4f54-b03e-d64f4a59afaf","_uuid":"c92830fa729cbd17ce735865ac0bd9aa706f36d0"},"cell_type":"markdown","source":"Because it's still difficult to read, we want to format it and check the correlation again."},{"metadata":{"collapsed":true,"_cell_guid":"0fa228fd-8150-43d3-bfac-283752e89e34","_uuid":"67bada509b57288970b67788255e85d46ac63e98","trusted":false},"cell_type":"code","source":"# Change number format in correlations\npd.set_option('display.float_format',lambda x: '{:,.2f}'.format(x) if abs(x) < 10000 else '{:,.0f}'.format(x))\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f079208-3376-4237-b5d8-110b6605c2be","_uuid":"9ef388d230cae67aef9e186858b5a5620ee9c242"},"cell_type":"markdown","source":"The tables shows that there is a strong correlation between targets. We cannot exclude one of those, because heating load and cooling are equally important outputs to be predicted."},{"metadata":{"collapsed":true,"_cell_guid":"71e357cb-e8bf-4ac6-8132-d5e0c186a2a9","_uuid":"9a165937d86ff86a1ae67ca8ec085c5a9f512786","trusted":false},"cell_type":"code","source":"# Correlation between inputs and outputs\nplt.figure(figsize=(5,5))\nsns.pairplot(data=data, y_vars=['cooling_load','heating_load'],\n             x_vars=['relative_compactness', 'surface_area', 'wall_area', 'roof_area', 'overall_height',\n                     'orientation', 'glazing_area', 'glazing_area_distribution',])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"05f596a9-7e31-4173-bbe9-46df32aa9135","_uuid":"528ce4d08fb880ccaab38330ef871320a4a17bb9"},"cell_type":"markdown","source":"From the table, we can see some information about correlations between all variables. For example, the overall_height (an input) has a strong correlation (0.90) with the output - cooling_load. Besides, the pairplot depicts the the relationship between them. For the overall_height and cooling_load plot, there is only 2 values of overall height due to the distribution and made us difficult to see the linear correlations of those variables. We will use preprocessing method to refine the distributions."},{"metadata":{"_cell_guid":"d170fceb-4d65-463c-9a69-e53494430bd8","_uuid":"01f9c9e968e417efc260c5532d5ab6a2ea893c88"},"cell_type":"markdown","source":"# D. Data Preprocessing"},{"metadata":{"_cell_guid":"0b9500da-ff10-409d-8ad9-38407e91c85b","_uuid":"349578c29f192fe91152ed95ba2b718ef869f387"},"cell_type":"markdown","source":"#### 1. Data Selection"},{"metadata":{"_cell_guid":"fa78eb73-07f6-4fe9-bb23-c70d02a1e30d","_uuid":"0544495ffd2646d40b5dab01f279343ffdc1b8a0"},"cell_type":"markdown","source":"Considering the distribution and correlation on our exploration of the data, we well use all vales as we want to create the best fit for prediction lines by evaluating R squared. R squared will always increase as we add more independent variables. To get a high R, we includes all variables."},{"metadata":{"_cell_guid":"5906755b-bd6f-413d-998a-59b8650ca0f3","_uuid":"39dbf4e79e599d21f93394187b10f9002c1bd8e3"},"cell_type":"markdown","source":"#### 2. Preprocessing & Data Transformation"},{"metadata":{"collapsed":true,"_cell_guid":"6da02460-5dad-4620-881a-201ca624d967","_uuid":"e5b834e73863d1a0dc453a51b912a1b8fdd1fcbd","trusted":false},"cell_type":"code","source":"# Check missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1e29dae6-5119-45dd-97ae-6d3aab3e332f","_uuid":"daac50f11f2cebc9436fe205fbbbdad165dc4054","trusted":false},"cell_type":"code","source":"#Summary statistics\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"636d868c-ab96-4f61-98ec-9eb8a026a9f3","_uuid":"bd975de98da25776a9696292bb9868e836ec378d"},"cell_type":"markdown","source":"Each feature has different scale, as we can see the minimum and maximum values for each of variables. To obtain a better scale, it is good to normalize the data because it makes distributions better."},{"metadata":{"collapsed":true,"_cell_guid":"20d0609a-d83d-4e26-8e2b-fe656ceba237","_uuid":"681878b417fa114dcc22048092e4a84ba977a2a4","trusted":false},"cell_type":"code","source":"#Normalize the inputs and set the output\nfrom sklearn.preprocessing import Normalizer\nnr = Normalizer(copy=False)\n\nX = data.drop(['heating_load','cooling_load'], axis=1)\nX = nr.fit_transform(X)\ny = data[['heating_load','cooling_load']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c468a155-1839-414e-9856-15b4ac382f76","_uuid":"62510a1825029082ade69c1447abbbebac8aefd2"},"cell_type":"markdown","source":"# E. Data Modelling"},{"metadata":{"_cell_guid":"bf388622-8abc-40c5-9825-59098eea7cec","_uuid":"e075b9e3d4a1cc76e256de544588b58b4c206eda"},"cell_type":"markdown","source":"Let's prapare our input and output using tran test split before we create models."},{"metadata":{"collapsed":true,"_cell_guid":"69e61d40-271a-488f-ae36-0a5ec4b0697a","_uuid":"c52faf99d93d0b73de1f79252937311f8ecfb58e","trusted":false},"cell_type":"code","source":"# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f0cf22d-81e4-46ae-8334-ebdfc0f07409","_uuid":"632b5d6df3f8cfb1e00275509803037b5b80fa15"},"cell_type":"markdown","source":"Then create a function to evaluate our model using R squared (R2 score)."},{"metadata":{"collapsed":true,"_cell_guid":"0c77f91d-77de-45db-a674-d69d262aa9f8","_uuid":"6e523895a43702997adcb2ef4b780330d248a700","trusted":false},"cell_type":"code","source":"#Create model evaluation function\ndef evaluate(model, test_features, test_labels):\n    from sklearn.metrics import r2_score\n    predictions = model.predict(test_features)\n    R2 = np.mean(r2_score(test_labels, predictions))\n    print('R2 score = %.3f' % R2)\n    return r2_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"552b5f60-9243-438d-851a-586abb00571d","_uuid":"13c2ea6223ce8dc75a32bcff7755dc3e422378ca"},"cell_type":"markdown","source":"The histograms have already told us that our data seems to be descret , like categorical data but in numbers. We should therefore use tree based algorithms to expect the best model using those type of data. We create 3 basic models and then optimze each models using Hyperparameter Search technique. The model we used are:\n1. Decission Tree Regression\n2. Random Forest Regression\n3. Extra Trees Regression"},{"metadata":{"_cell_guid":"c35f3027-f5e6-47b2-8a78-e721739218cf","_uuid":"e50765c404bc0b796005b29bc6b809a4cb693780"},"cell_type":"markdown","source":"#### 1. Decission Tree Regressor"},{"metadata":{"collapsed":true,"_cell_guid":"8229290d-afdd-457c-a559-1f42eb3967bc","_uuid":"0a2fd78b4ea6d45a08ad80ec77424cc906d6ae8f","trusted":false},"cell_type":"code","source":"#Import decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor\n# Create decision tree model \ndt_model = DecisionTreeRegressor(random_state=123)\n# Apply the model\ndt_model.fit(X_train, y_train)\n# Predicted value\ny_pred1 = dt_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"86fa2ea8-0508-4048-bbd8-3c92e235d02d","_uuid":"eb40f0622e0a529d48a69b8f71c6bc4a22c3611d","trusted":false},"cell_type":"code","source":"#R2 score before optimization\nR2_before_dt= evaluate(dt_model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"48be199d-47ee-4310-ad49-7446a910c184","_uuid":"de697401dfec86f682f24f7504c6a080177a06de","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output before optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred1[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load Before Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output before optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred1[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load Before Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c4082470-4a97-45cd-afed-dd273e9d3afc","_uuid":"e2de42938975b0aa2320b68fe2bf035b1b1b5df5","trusted":false},"cell_type":"code","source":"# Finding the best decision tree optimization parameters\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n# Max Depth\ndt_acc = []\ndt_depth = range(1,11)\nfor i in dt_depth:\n    dt = DecisionTreeRegressor(random_state=123, max_depth=i)\n    dt.fit(X_train, y_train)\n    dt_acc.append(dt.score(X_test, y_test))\nax1.plot(dt_depth,dt_acc)\nax1.set_title('Max Depth')\n\n#Min Samples Split\ndt_acc = []\ndt_samples_split = range(10,21)\nfor i in dt_samples_split:\n    dt = DecisionTreeRegressor(random_state=123, min_samples_split=i)\n    dt.fit(X_train, y_train)\n    dt_acc.append(dt.score(X_test, y_test))\nax2.plot(dt_samples_split,dt_acc)\nax2.set_title('Min Samples Split')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"723d7c2d-37e7-407d-b082-fa5d0aef7782","_uuid":"88e44137a2502d0cdab2ff3e30f1bb28a1c2871b","trusted":false},"cell_type":"code","source":"#Min Sample Leaf\nplt.figure(figsize = (5,5))\ndt_acc = []\ndt_samples_leaf = range(1,10)\nfor i in dt_samples_leaf:\n    dt = DecisionTreeRegressor(random_state=123, min_samples_leaf=i)\n    dt.fit(X_train, y_train)\n    dt_acc.append(dt.score(X_test, y_test))\n\nplt.plot(dt_samples_leaf,dt_acc)\nplt.title('Min Sample Leaf')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4843eab-ea4e-4b60-9374-19993ce8db77","_uuid":"1969622d1d78393556dccc9a2c0e12c6720761f3"},"cell_type":"markdown","source":"From the y axis, we can see the accuracy of the models is pretty high. We then choose some indexes that produce the maximum values and put them in our parameters."},{"metadata":{"collapsed":true,"_cell_guid":"27881284-6908-4c31-a3c0-c77e2ff248c0","_uuid":"708cc9e1a236953e7ceabf940d5587136afa72b1","trusted":false},"cell_type":"code","source":"# Decision tree optimization parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'max_depth' : [7,8,9],\n              'min_samples_split': [16,17,18],\n              'min_samples_leaf' : [6,7,8]}\n\n\n#Create new model using the GridSearch\ndt_random = GridSearchCV(dt_model, parameters, cv=10)\n\n#Apply the model\ndt_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f58fc5f1-efdc-4ba9-97e6-a4ba4065af9c","_uuid":"15caac748bc6c505b5837ccb05d45c6333ba2dc9","trusted":false},"cell_type":"code","source":"#View the best parameters\ndt_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"248969ff-42b5-4965-a2f9-eda075cf9837","_uuid":"eac7f6773a5968d859650949df86af7367bc9dfb","trusted":false},"cell_type":"code","source":"# Predicted value\ny_pred1_ = dt_random.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"185aca9d-19f5-46d4-8507-a32428493ae2","_uuid":"e20e1b66d545f803aed477d4df36624bba32268f","trusted":false},"cell_type":"code","source":"#R2 score after optimization\ndt_best_random = dt_random.best_estimator_\nR2_after_dt= evaluate(dt_best_random, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e0094dfc-7c27-4778-babc-3b2eb8dabfc7","_uuid":"c5eb3c20f11f7828c9ac97e61d6e629c7b721e82","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output after optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred1_[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load After Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output after optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred1_[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load After Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d04f745d-dcd1-4ee6-a7af-6850b571b64e","_uuid":"127b69a6a7451ad9c402e913303e63599c5fb3ef"},"cell_type":"markdown","source":"#### 2. Random Forest Regressor"},{"metadata":{"collapsed":true,"_cell_guid":"3f70a860-9821-493f-b289-36ff0390cb07","_uuid":"93177a5f30536dc4087392081957a5a2d5756cc5","trusted":false},"cell_type":"code","source":"#Import random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n# Create random forest model \nrf_model = RandomForestRegressor(random_state=123)\n# Apply the model\nrf_model.fit(X_train, y_train)\n# Predicted value\ny_pred2 = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"44f67238-9ee2-4d4b-a80e-5adb92333701","_uuid":"ff2fbc148c04563ab774266610ec0fce008107f0","trusted":false},"cell_type":"code","source":"#R2 score before optimization\nR2_before_rf= evaluate(rf_model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bf61d9d1-6247-47fa-bd69-82b5c73b4cb6","_uuid":"a77a6cf02ac51cd0223a2c4efa6747b4ad5f3757","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output before optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred2[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load Before Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output before optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred2[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load Before Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":false,"_cell_guid":"4ec8e578-256b-4850-bbc4-aebbe48123d9","_uuid":"76b706d25c743bccf7fe172e2800d584970580bb","trusted":false},"cell_type":"code","source":"# Finding the best random forest optimization parameters\n\nf, axarr = plt.subplots(2, 2)\n\n# Max Depth\nrf_acc = []\nrf_depth = range(1,11)\nfor i in rf_depth:\n    rf = RandomForestRegressor(random_state=123, max_depth=i)\n    rf.fit(X_train, y_train)\n    rf_acc.append(rf.score(X_test, y_test))\naxarr[0, 0].plot(rf_depth,rf_acc)\naxarr[0, 0].set_title('Max Depth')\n\n#Min Samples Split\nrf_acc = []\nrf_samples_split = range(10,21)\nfor i in rf_samples_split:\n    rf = RandomForestRegressor(random_state=123, min_samples_split=i)\n    rf.fit(X_train, y_train)\n    rf_acc.append(rf.score(X_test, y_test))\naxarr[0, 1].plot(rf_samples_split,rf_acc)\naxarr[0, 1].set_title('Min Samples Split')\n\n#Min Sample Leaf\nrf_acc = []\nrf_samples_leaf = range(1,10)\nfor i in rf_samples_leaf:\n    rf = RandomForestRegressor(random_state=123, min_samples_leaf=i)\n    rf.fit(X_train, y_train)\n    rf_acc.append(rf.score(X_test, y_test))\n\naxarr[1, 0].plot(rf_samples_leaf,rf_acc)\naxarr[1, 0].set_title('Min Sample Leaf')\n\n#N Estimator\nrf_acc = []\nrf_estimators = range(50,59)\nfor i in rf_estimators:\n    rf = RandomForestRegressor(random_state=123, n_estimators=i)\n    rf.fit(X_train, y_train)\n    rf_acc.append(rf.score(X_test, y_test))\n\naxarr[1, 1].plot(rf_estimators,rf_acc)\naxarr[1, 1].set_title('N Estimator')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"13e14eb4-e2e0-4a92-8eb8-5c6867ac1690","_uuid":"c474f0270f50a80f5e87f2f97863556db7f72027","trusted":false},"cell_type":"code","source":"# Random forest optimization parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'max_depth' : [6,7,8],\n              'min_samples_split': [11,12,13],\n              'min_samples_leaf' : [4,5,6],\n              'n_estimators': [49,50,51]}\n\n\n#Create new model using the GridSearch\nrf_random = GridSearchCV(rf_model, parameters, cv=10)\n\n#Apply the model\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"239adf75-cf71-4ec5-836d-436bb3a2faf1","_uuid":"a4eea4c2e1da132323fb17fb2b50f1b6218ce6f5","trusted":false},"cell_type":"code","source":"#View the best parameters\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ad51833a-6d7c-482e-9f8b-3edfbcef01bb","_uuid":"bb326ed5585dded8b10bb7f512c107a935d0d502","trusted":false},"cell_type":"code","source":"# Predicted value\ny_pred2_ = rf_random.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bf357685-7744-4af2-a65b-4e06603d75ea","_uuid":"955a46922563034f2e1448a46518f137c05fa70c","trusted":false},"cell_type":"code","source":"#R2 score after optimization\nbest_random_rf = rf_random.best_estimator_\nR2_after_rf= evaluate(best_random_rf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0573731c-6827-49f4-8b27-9f8a87f19369","_uuid":"bb8b93d5bcb4eacdec9e6415bac719e0dd9cada6","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output after optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred2_[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load After Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output after optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred2_[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load After Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6267f5b-711c-48c2-8a8e-c080c8d5a048","_uuid":"68618109f9ed787c328367aebb02a7194ab41b89"},"cell_type":"markdown","source":"#### 3. Extra Trees Regressor"},{"metadata":{"collapsed":true,"_cell_guid":"431fa395-984b-48d0-86b6-4923a2e900b7","_uuid":"d6c999b3c7c4b267ab1db845d669f0ecb4d7bf84","trusted":false},"cell_type":"code","source":"#Import extra trees regressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n# Create extra trees model \netr_model = ExtraTreesRegressor(random_state=123)\n# Apply the model\netr_model.fit(X_train, y_train)\n# Predicted value\ny_pred3 = etr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c80e02a3-7668-4ab7-a9cc-55b86ce1dbe2","_uuid":"3ddfcfc3c5b7effcf81a6e26822be42efb843dbc","trusted":false},"cell_type":"code","source":"#R2 score before optimization\nR2_before_etr= evaluate(etr_model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ab82127c-ec93-4b80-9c1d-cca897d1199a","_uuid":"17802a86ae29b87aa74c77c70828d9222638c99e","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output before optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred3[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load Before Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output before optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred3[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load Before Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ca1e73c6-0765-49c3-9e5b-4c5f2617a8ac","_uuid":"468325e795dba5c0f013a546208faf7f4b3c3e85","trusted":false},"cell_type":"code","source":"# Finding the best extra trees regressor optimization parameters\n\nf, axarr = plt.subplots(2, 2)\n\n# Max Depth\netr_acc = []\netr_depth = range(1,11)\nfor i in etr_depth:\n    etr = ExtraTreesRegressor(random_state=123, max_depth=i)\n    etr.fit(X_train, y_train)\n    etr_acc.append(etr.score(X_test, y_test))\naxarr[0, 0].plot(etr_depth,etr_acc)\naxarr[0, 0].set_title('Max Depth')\n\n#Min Samples Split\netr_acc = []\netr_samples_split = range(16,26)\nfor i in etr_samples_split:\n    etr = ExtraTreesRegressor(random_state=123, min_samples_split=i)\n    etr.fit(X_train, y_train)\n    etr_acc.append(etr.score(X_test, y_test))\naxarr[0, 1].plot(etr_samples_split,etr_acc)\naxarr[0, 1].set_title('Min Samples Split')\n\n#Min Sample Leaf\netr_acc = []\netr_samples_leaf = range(3,8)\nfor i in etr_samples_leaf:\n    etr = ExtraTreesRegressor(random_state=123, min_samples_leaf=i)\n    etr.fit(X_train, y_train)\n    etr_acc.append(etr.score(X_test, y_test))\n\naxarr[1, 0].plot(etr_samples_leaf,etr_acc)\naxarr[1, 0].set_title('Min Sample Leaf')\n\n#N Estimator\netr_acc = []\netr_estimators = range(40,46)\nfor i in etr_estimators:\n    etr = ExtraTreesRegressor(random_state=123, n_estimators=i)\n    etr.fit(X_train, y_train)\n    etr_acc.append(etr.score(X_test, y_test))\n\naxarr[1, 1].plot(etr_estimators,etr_acc)\naxarr[1, 1].set_title('N Estimator')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4ac30591-4eec-41b1-8804-3c603a39c980","_uuid":"2f353b76f455c7616f5af797c0e25194761cb1a7","trusted":false},"cell_type":"code","source":"# Extra trees regressor optimization parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'max_depth' : [6,7,8],\n              'min_samples_split': [19,20,21],\n              'min_samples_leaf' : [4,5,6],\n              'n_estimators': [43,44,45]}\n\n\n#Create new model using the GridSearch\netr_random = GridSearchCV(etr_model, parameters, cv=10)\n\n#Apply the model\netr_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0491f498-461e-406e-aa74-c711d183f07c","_uuid":"30af732a8b4c05ed7fc4182dc81571fc4bfcedb1","trusted":false},"cell_type":"code","source":"#View the best parameters\netr_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f45c7936-d8d2-42c6-9286-c0e4164fcc7b","_uuid":"06cf3eccd03eace6a841c2817f8182358a014fc7","trusted":false},"cell_type":"code","source":"# Predicted value\ny_pred3_ = etr_random.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a966c470-1aae-464b-8ce7-ce8980e18071","_uuid":"8f4136f5a888549bc1cf00bda5289f08e68bb50b","trusted":false},"cell_type":"code","source":"#R2 score after optimization\nbest_random_etr = etr_random.best_estimator_\nR2_after_etr= evaluate(best_random_etr, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b87b8211-1e2a-4013-89e7-97a0ac5af600","_uuid":"5e0a1036c80b9308461a9130deaa6c66dee6cd5b","trusted":false},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n#Visualize the heating load output after optimization\nplt.figure(figsize = (5,5))\nax1.plot(range(0,len(X_test)),y_test.iloc[:,0],'o',color='red',label = 'Actual Values')\nax1.plot(range(0,len(X_test)),y_pred3_[:,0],'X',color='yellow',label = 'Predicted Values')\nax1.set_xlabel('Test Cases')\nax1.set_ylabel('Heating Load')\nax1.set_title('Heating  Load After Optimization')\nax1.legend(loc = 'upper right')\n\n#Visualize the cooling load output after optimization\nplt.figure(figsize = (5,5))\nax2.plot(range(0,len(X_test)),y_test.iloc[:,1],'o',color='green',label = 'Actual Values')\nax2.plot(range(0,len(X_test)),y_pred3_[:,1],'X',color='blue',label = 'Predicted Values')\nax2.set_xlabel('Test Cases')\nax2.set_ylabel('Cooling Load')\nax2.set_title('Cooling Load After Optimization')\nax2.legend(loc = 'upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f9c1434-fabd-463b-83fd-8099151c449f","_uuid":"5a0d58cfbc9566f53c3b3d3764792c195fd50951"},"cell_type":"markdown","source":"# F. Evaluation"},{"metadata":{"_cell_guid":"33943c01-4e4f-48ab-8691-921feb99b6da","_uuid":"016fd304d665f7ed52da8e1ac9368ff093f05b1e"},"cell_type":"markdown","source":"#### 1. Conclusion"},{"metadata":{"collapsed":true,"_cell_guid":"a4fa4c4e-ce65-4780-b255-5495db377ca2","_uuid":"0d8ac014a84711b8f01437a8f24d277348dbc9f7"},"cell_type":"markdown","source":"Overall, the model perform well to predict the the Heating Load and Cooling Load with R2 score >= 97% even not using hyperparameter optimization\n\n* R2 score of **Decision Tree Regressor** model = 97.0%\n* R2 score of **Random Forest Regressor** model = 97.8%\n* R2 score of **Extra Trees Regressor** model = 97.7%\n\nTo increase the F1 score, we have applied hyperparameter tuning using GridSearch and obtain\n* R2 score of **Decision Tree Regressor** model = 97.9%\n* R2 score of **Random Forest Regressor** model = 98.0%\n* R2 score of **Extra Trees Regressor** model = 98.3%\n\nWe can conclude that Extra Tree Regressor is the best model to predict the Heating Load and Cooling Load values with the optimum R2 score of 98.3%"},{"metadata":{"collapsed":true,"_cell_guid":"d1689fb5-8c6f-4613-9578-56726d8a7dc4","_uuid":"de5fbe7000cb2c4e6df21ea7fb9f7ad3b5da8f4b"},"cell_type":"markdown","source":"#### 2. Recommendation"},{"metadata":{"_cell_guid":"02c66506-8c96-4313-bc6d-fb4345a7dde9","_uuid":"d0aad695fd574ede7e4d97b0c8f022173613c013"},"cell_type":"markdown","source":"R squared sometimes is not a good indicator of fit. R squared (R2 score) will always increase as we add more independent variables â€“ but adjusted R2 will decrease if we add an independent variable that does not help the model. This is good way to determine if an additional variable should even be included. However, adjusted R2 which penalizes model complexity to control for overfitting, generally under penalizes complexity. "},{"metadata":{"collapsed":true,"_cell_guid":"cbd1046c-287e-4ea6-a944-070dba8253cd","_uuid":"76cfe4ff1d353282aa0701b7ca87f5eeac7e95a8","trusted":false},"cell_type":"code","source":"# create a fitted model with all features\nimport statsmodels.formula.api as smf\ndata2=data.copy()\nlm1 = smf.ols(formula='heating_load ~ relative_compactness + surface_area + wall_area + roof_area + overall_height + orientation + glazing_area + glazing_area_distribution', data=data2).fit()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d8ff5bcb-0ef8-4ade-bcef-7a19ba9838d5","_uuid":"cf1ca0d0a384b71362c0528b2c8209c9096957c9","trusted":false},"cell_type":"code","source":"# Summarize fitted mode\nlm1.summary()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8ab6de50-dc8a-419a-ba7d-7bec1228a35e","_uuid":"b73b2c6ae6f95984a57befa3fe12fa237df621e9","trusted":false},"cell_type":"code","source":"# create a fitted model with all features excluding \"orientation\"\nlm2 = smf.ols(formula='heating_load ~ relative_compactness + surface_area + wall_area + roof_area + overall_height + glazing_area + glazing_area_distribution', data=data2).fit()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"23e122f3-a1b6-4de0-b7c3-2e2770852d38","_uuid":"ec2ccac398f2712a71dade6d21ce2bf9f60a493c","trusted":false},"cell_type":"code","source":"# Summarize fitted mode\nlm2.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46f5c744-6d5e-46bd-b2a2-e869fb0ddc25","_uuid":"fb35d7f73799b6a633a9a39b958ff27fda0a4f2b"},"cell_type":"markdown","source":"Adjusted R squared is recommended than R squared in terms of goodness of fit. The above steps is the example of creating two models, one model uses all features and the other one loses one feature. In this case, it shows that adding variables does not always make our model better (see the R squared and Adj. R squared from the summary tables). So, we do not need to include all variable for multiple regression if not helping to create better model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","version":"3.6.4","mimetype":"text/x-python","name":"python"}},"nbformat":4,"nbformat_minor":1}