{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment = df.sentiment.map({\"positive\":1,\"negative\":0})\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass IMDBDataset:\n    def __init__(self,reviews,sentiments):\n        self.reviews = reviews\n        self.sentiments = sentiments\n        \n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self,item):\n        review = self.reviews[item,:]\n        target = self.sentiments[item]\n        return {\n            \"review\": torch.tensor(review,dtype = torch.long),\n            \"target\": torch.tensor(target,dtype = torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn \nimport torch.nn.functional as F\n\nclass LSTM(nn.Module):\n    def __init__(self,embedding_matrix):\n        super(LSTM,self).__init__()\n        self.embedding_matrix = embedding_matrix\n        # num of words = no of rows of the embedding matrix\n        num_words = self.embedding_matrix.shape[0]\n        # dimension of embedding matrix is num of columns in the embedding matrix\n        embed_dim = self.embedding_matrix.shape[1]\n        # we define an input embedding layer as \n        self.embedding = nn.Embedding(num_embeddings=num_words,embedding_dim=embed_dim)\n        # Embedding layer is used as the as weights of the embedding layer\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        \n        # we do not want to train the pretrained embeddings\n        self.embedding.weight.requires_grad = False\n        \n        # a simple bidirectional lstm with an hidden_dim of 128\n        self.lstm = nn.LSTM(embed_dim,128,bidirectional=True,batch_first=True)\n        # output layer is a layer which has only one output \n        # input(512) = 128+128 for mean and same for max pooling\n        self.out = nn.Linear(512,1)\n        \n    def forward(self,x):\n        # pass the data through embedding layer the input is just the tokens\n        x = self.embedding(x)\n        \n        # move the embedding output to lstm\n        x,_ = self.lstm(x)\n        # apply mean and max pooling on lstm output\n        avg_pool = torch.mean(x,1)\n        max_pool,_ = torch.max(x,1)\n        # concatenate mean and max pooling this is why 512\n        # 128 for each direction = 256\n        # avg_pool = 256, max_pool = 256\n        out = torch.cat((avg_pool,max_pool),1)\n        # pass through the output layer and return the output\n        out = self.out(out)\n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef train(data_loader,model,optimizer,device):\n    \"\"\"\n    This is the main training function that trains model\n    for one epoch\n    :param data_loader: this is the torchdataloader\n    :param model: model(lstm model)\n    :param optimizer: optimizer Adam, SGD etc\n    :param device: this can be \"cuda\" or \"cpu\"\n    \"\"\"\n    # set the model to training mode\n    model.train()\n    \n    # go through the batches of data in data_loader:\n    for data in data_loader:\n        reviews = data[\"review\"]\n        targets = data[\"target\"]\n        # move the data to the device that we want to use\n        reviews = reviews.to(device,dtype=torch.long)\n        targets = targets.to(device,dtype=torch.float)\n        # clear the gradients\n        optimizer.zero_grad()\n        # make predictions from the models\n        predictions = model(reviews)\n        # loss\n        loss = nn.BCEWithLogitsLoss()(predictions,targets.view(-1,1))\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader,model,device):\n    final_predictions = []\n    final_targets = []\n    \n    model.eval()\n    for data in data_loader:\n        reviews = data[\"review\"]\n        targets = data[\"target\"]\n        # move the data to the device that we want to use\n        reviews = reviews.to(device,dtype=torch.long)\n        targets = targets.to(device,dtype=torch.float)\n        predictions = model(reviews)\n        predictions = predictions.detach().cpu().numpy().tolist()\n        targets = data[\"target\"].detach().cpu().numpy().tolist()\n        final_predictions.extend(predictions)\n        final_targets.extend(targets)\n        \n        \n    return  final_predictions,final_targets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nimport torch\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_vectors(fname):\n    fin = open(fname)\n    data = {}\n    for line in fin:\n        tokens = line.split()\n        data[tokens[0]] = np.array([float(value) for value in tokens[1:]])\n        \n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_embedding_matrix(word_index,embedding_dict):\n    \"\"\"\n    This function creates the embedding matrix\n    :param word_index: a dictionary of word: index_value\n    :param embedding_dict:\n    :return a numpy array with embedding vectors for all known words\n    \"\"\"\n    # intialize the embedding matrix \n    embedding_matrix = np.zeros((len(word_index)+1,300))\n    for word, i in word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[i] = embedding_dict[word]\n    return embedding_matrix\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(df):\n    y = df.sentiment.values\n    train_df,valid_df = train_test_split(df,test_size = 0.2, stratify = y)\n    \n    print('Fitting tokenizer')\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(df.review.values.tolist())\n    \n    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,maxlen = MAX_LEN)\n    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,maxlen = MAX_LEN)\n    train_dataset = IMDBDataset(reviews=xtrain,sentiments=train_df.sentiment.values)\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,num_workers=2)\n    valid_dataset = IMDBDataset(reviews=xtest,sentiments=valid_df.sentiment.values)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=VALID_BATCH_SIZE,num_workers=2)\n    \n    print(\"Load embeddings\")\n    embedding_dict = load_vectors('../input/glove-embeddings/glove.6B.300d.txt')\n    embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict)\n    # create a torch device since we are using cuda\n    device = torch.device(\"cuda\")\n    model = LSTM(embedding_matrix)\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n    print(\"Traning model\")\n    best_accuracy = 0\n    early_stopping_counter = 0\n    for epoch in range(1,EPOCHS+1):\n        train(train_loader,model,optimizer,device)\n        outputs,targets = evaluate(valid_loader,model,device)\n        outputs = np.array(outputs)>0.5\n        accuracy = metrics.accuracy_score(targets,outputs)\n        print(f\"EPOCH:{epoch}, Accuracy Score: {accuracy}\")\n        if accuracy>best_accuracy:\n            best_accuracy = accuracy\n        else:\n            early_stopping_counter +=1\n        if early_stopping_counter>2:\n            break\n            \n            \n            \n            \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}