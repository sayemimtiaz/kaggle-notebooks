{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\nfrom keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\nfrom keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\nfrom keras.layers import Reshape, merge, Concatenate, Lambda, Average\nfrom keras.models import Sequential, Model, load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.layers.merge import add\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import initializers, regularizers\nfrom keras import optimizers\nfrom keras.engine.topology import Layer\nfrom keras import constraints\n\n############################################## \n\"\"\"\n# ATTENTION LAYER\nCite these works \n1. Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n\"Hierarchical Attention Networks for Document Classification\"\naccepted in NAACL 2016\n2. Winata, et al. https://arxiv.org/abs/1805.12307\n\"Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision.\" \naccepted in ICASSP 2018\nUsing a context vector to assist the attention\n* How to use:\nPut return_sequences=True on the top of an RNN Layer (GRU/LSTM/SimpleRNN).\nThe dimensions are inferred based on the output shape of the RNN.\nExample:\n\tmodel.add(LSTM(64, return_sequences=True))\n\tmodel.add(AttentionWithContext())\n\tmodel.add(Addition())\n\t# next add a Dense layer (for classification/regression) or whatever...\n\"\"\"\n##############################################\n\ndef dot_product(x, kernel):\n\t\"\"\"\n\tWrapper for dot product operation, in order to be compatible with both\n\tTheano and Tensorflow\n\tArgs:\n\t\tx (): input\n\t\tkernel (): weights\n\tReturns:\n\t\"\"\"\n\tif K.backend() == 'tensorflow':\n\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n\telse:\n\t\treturn K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n\t\"\"\"\n\tAttention operation, with a context/query vector, for temporal data.\n\tSupports Masking.\n\tfollows these equations:\n\t\n\t(1) u_t = tanh(W h_t + b)\n\t(2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n\t(3) v_t = \\alpha_t * h_t, v in time t\n\t# Input shape\n\t\t3D tensor with shape: `(samples, steps, features)`.\n\t# Output shape\n\t\t3D tensor with shape: `(samples, steps, features)`.\n\t\"\"\"\n\n\tdef __init__(self,\n\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n\t\t\t\t bias=True, **kwargs):\n\n\t\tself.supports_masking = True\n\t\tself.init = initializers.get('glorot_uniform')\n\n\t\tself.W_regularizer = regularizers.get(W_regularizer)\n\t\tself.u_regularizer = regularizers.get(u_regularizer)\n\t\tself.b_regularizer = regularizers.get(b_regularizer)\n\n\t\tself.W_constraint = constraints.get(W_constraint)\n\t\tself.u_constraint = constraints.get(u_constraint)\n\t\tself.b_constraint = constraints.get(b_constraint)\n\n\t\tself.bias = bias\n\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n\n\tdef build(self, input_shape):\n\t\tassert len(input_shape) == 3\n\n\t\tself.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n\t\t\t\t\t\t\t\t initializer=self.init,\n\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n\t\tif self.bias:\n\t\t\tself.b = self.add_weight(shape=(input_shape[-1],),\n\t\t\t\t\t\t\t\t\t initializer='zero',\n\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n\n\t\tself.u = self.add_weight(shape=(input_shape[-1],),\n\t\t\t\t\t\t\t\t initializer=self.init,\n\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n\n\t\tsuper(AttentionWithContext, self).build(input_shape)\n\n\tdef compute_mask(self, input, input_mask=None):\n\t\t# do not pass the mask to the next layers\n\t\treturn None\n\n\tdef call(self, x, mask=None):\n\t\tuit = dot_product(x, self.W)\n\n\t\tif self.bias:\n\t\t\tuit += self.b\n\n\t\tuit = K.tanh(uit)\n\t\tait = dot_product(uit, self.u)\n\n\t\ta = K.exp(ait)\n\n\t\t# apply mask after the exp. will be re-normalized next\n\t\tif mask is not None:\n\t\t\t# Cast the mask to floatX to avoid float64 upcasting in theano\n\t\t\ta *= K.cast(mask, K.floatx())\n\n\t\t# in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n\t\t# Should add a small epsilon as the workaround\n\t\t# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n\t\ta = K.expand_dims(a)\n\t\tweighted_input = x * a\n\t\t\n\t\treturn weighted_input\n\n\tdef compute_output_shape(self, input_shape):\n\t\treturn input_shape[0], input_shape[1], input_shape[2]\n\t\nclass Addition(Layer):\n\t\"\"\"\n\tThis layer is supposed to add of all activation weight.\n\tWe split this from AttentionWithContext to help us getting the activation weights\n\tfollows this equation:\n\t(1) v = \\sum_t(\\alpha_t * h_t)\n\t\n\t# Input shape\n\t\t3D tensor with shape: `(samples, steps, features)`.\n\t# Output shape\n\t\t2D tensor with shape: `(samples, features)`.\n\t\"\"\"\n\n\tdef __init__(self, **kwargs):\n\t\tsuper(Addition, self).__init__(**kwargs)\n\n\tdef build(self, input_shape):\n\t\tself.output_dim = input_shape[-1]\n\t\tsuper(Addition, self).build(input_shape)\n\n\tdef call(self, x):\n\t\treturn K.sum(x, axis=1)\n\n\tdef compute_output_shape(self, input_shape):\n\t\treturn (input_shape[0], self.output_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading data...')\n#thay đổi dữ liệu đầu vào tùy thuộc vào tập dữ liệu cần chạy\npath ='/kaggle/input/url-malicious-lstm/2k5.csv'\n# load data\ndf =  pd.read_csv(path)\n\nprint(f'Data size: {df.shape}')\n\nprint(df['label'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trộn dữ liệu để phục vụ tạo bộ dữ liệu train, test và validation\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples = df.url\nlabels = df.label\nmax_chars = 20000\nmaxlen = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=max_chars, char_level=True)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=maxlen)\nlabels = [1 if i=='bad' else 0 for i in labels]\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#phân chia tập dữ liệu thành train và validation\ntraining_samples = int(len(samples) * 0.95)\nvalidation_samples = int(len(labels) * 0.05)\nprint(training_samples, validation_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tách bộ dữ liệu train thành train và test\nx = data[:training_samples]\ny = labels[:training_samples]\nx_test = data[training_samples: training_samples + validation_samples]\ny_test = labels[training_samples: training_samples + validation_samples]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_chars = len(tokenizer.word_index)+1\nembedding_vector_length = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom tensorflow.keras.optimizers import Adam, SGD\ndef make_model(n_batch,num_chars, embedding_vector_length, maxlen):\n    model = Sequential()\n    model.add(Embedding(num_chars, embedding_vector_length, input_length=maxlen))\n    model.add(SpatialDropout1D(0.2))\n    model.add(Bidirectional(LSTM(64,  \n                                 dropout=0.25, \n                                 recurrent_dropout=0.25,return_sequences=True)))\n    model.add(BatchNormalization())\n    model.add(Bidirectional(LSTM(64,  \n                                 dropout=0.25, \n                                 recurrent_dropout=0.25, \n                                 return_sequences=True)))\n    model.add(AttentionWithContext())\n    model.add(Addition())\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.summary()\n    start = time.time()\n    model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', f1_m, recall_m, precision_m])\n    print(\"Compilation Time : \", time.time() - start)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model_no_attention(n_batch,num_chars, embedding_vector_length, maxlen):\n    model = Sequential()\n    model.add(Embedding(num_chars, embedding_vector_length, input_length=maxlen))\n    model.add(SpatialDropout1D(0.2))\n    model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.summary()\n    start = time.time()\n    model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', f1_m, recall_m, precision_m])\n    print(\"Compilation Time : \", time.time() - start)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\ndef evaluate_model_with_n_layers(n_batch, x, y, x_test, y_test, num_chars, embedding_vector_length, maxlen,callbacks_list):\n    model = make_model(n_batch,num_chars, embedding_vector_length, maxlen)\n    hist=model.fit(x, y,\n                epochs=100,\n                batch_size=n_batch,\n                callbacks=callbacks_list,\n                validation_split=0.20,\n                shuffle=True,\n                verbose=1\n                )\n    test_acc = model.evaluate(x_test,y_test, verbose=0)\n    return hist,test_acc\n\n\nall_history = list ()\nn_layers = 2\nnum_batch_size = [32]\nfor n_batch in num_batch_size:\n    file_path = \"BestModel_data_2k5_\"+str(n_batch)+\".hdf5\"\n\n    callbacks_list = [\n        ModelCheckpoint(\n            file_path,\n            monitor='val_loss',\n            verbose=1,\n            save_best_only=True,\n            mode='min'\n        ),\n        EarlyStopping(\n            monitor='val_loss', \n            min_delta=0,\n            patience=5, \n            verbose=1,\n            mode='auto',\n        )\n    ]\n    print('Starting test....')\n    history, result = evaluate_model_with_n_layers(n_batch, x, y, x_test, y_test, num_chars, embedding_vector_length, maxlen,callbacks_list)\n    print('Batch_Size =%d:' % n_batch)\n    print(result)\n    all_history.append(history)\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.plot(history.history['val_accuracy'],label='val_accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}