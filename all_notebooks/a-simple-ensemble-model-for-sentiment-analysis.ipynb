{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook uses the well-known huggingface transformers library and tokeninzers with tensorflow & keras! We're beginners to NLP so any feedback is appreciated! :) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Statements","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nfrom tqdm.notebook import tqdm\nimport emoji\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\n\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initialize TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read training data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/student-shopee-code-league-sentiment-analysis/train.csv', index_col='review_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read supplementary data\n\nCredits to Tony Ng for the additional web scrape data! https://www.kaggle.com/shymammoth/shopee-reviews \nThank you :)\n\nWe chose to use only 10% of the additional data (it has 1.5 million-ish rows of reviews) due to computational limitations. But that additional 10% contributed to big improvement of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_df = pd.read_csv('/kaggle/input/shopee-reviews/shopee_reviews.csv')\nextra_df.columns = ['rating','review']\nextra_df = extra_df.drop(extra_df.loc[extra_df['rating']=='label'].index.to_list())\nextra_df.rating = extra_df.rating.astype('int64')\nextra_df.review = extra_df.review.astype('str')\nextra_df = extra_df[['review', 'rating']]\nextra_df = extra_df.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concat additional data with training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.concat([data_df, extra_df])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions for cleaning data\n\nOn our initial tries, we did some really basic data cleaning and that yielded really poor test accuracies. After a lot of EDA, we realised that the data is really noisy and a thorough cleaning is essential. \nWe meticulously cleaned the data for:\n\n1. Emojis\n    * There are so many rows with emojis. It would be great if we could leverage their meaning to provide insight to the sentiment of the review.\n\n1. URLs\n    * There were also a considerable number of rows that contained URLs and links. Since they would only contribute to additional noise, we decided to remove them.\n\n1. Emoticons (eg. ':)', ':-)', ':-(' etc.)\n    * Same as emojis, if we could leverage their meaning in reviews, it would provide additional insight!\n\n1. Indonesian shorthand words\n\n1. Repeated characters in words (eg. baaaguusssssss)\n\n1. All random punctuation\n\nCredits to 'https://www.kaggle.com/indralin/text-processing-augmentation-tpu-baseline-0-4544' by Indra Lin for helping with point 1, 4 & 5! Thank you :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_emoji(review):\n    \n    emojis = set([char for word in review.split() for char in list(word) if char in emoji.UNICODE_EMOJI])\n    emojis = emoji.demojize(' '.join([emoji for emoji in emojis]))\n    emojis = emojis.replace(':', '')\n    emojis = emojis.replace('_', ' ')\n    \n    review = review.encode('ascii', 'ignore').decode('ascii')\n    review = review + ' ' + emojis\n    \n    return review.lower()\n\ndef clean_urls(review):\n    review = review.split()\n    review = ' '.join([word for word in review if not re.match('^http', word)])\n    return review\n\ndef clean_smileys(review):\n    \n    review = re.sub(r'(:\\)|: \\)|\\(\\:|:-\\)|: -\\)|: - \\)|:D|: D)', ' smile ', review)\n    review = re.sub(r'(:\\(|: \\(|\\)\\:|:-\\(|: -\\(|: - \\(|:\\'\\()', ' dislike ', review)\n    review = re.sub(r'(<3)', ' heart ', review)\n    review = re.sub(r'(:/)', ' dislike ', review)\n    review = re.sub(r'(;\\)|; \\))', ' wink ', review)\n    return ' '.join([word for word in review.split()])\n\ndef recover_shortened_words(text):\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    text = re.sub(r'\\borg\\b', 'orang', text)\n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text\n\ndef delete_repeated_char(text):\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    return text\n\ndef remove_punc(review):\n    review =  review.translate(str.maketrans('', '', string.punctuation))\n    review = ' '.join([word for word in review.split()])\n    review = review.lower()\n    return review","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean training data\n\nWe realised after cleaning that there are a lot of duplicated rows of reviews. Dropping them is vital as they can lead to an artificial increase in validation accuracy during training and yield bad test accuracies. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['review'] = data_df['review'].apply(clean_emoji).apply(clean_urls).apply(clean_smileys).apply(recover_shortened_words).apply(delete_repeated_char).apply(remove_punc)\n\ndata_df['count'] = data_df['review'].str.split().map(len)\ndrop_indexes = data_df.loc[data_df['count']==0].index.tolist()\ndata_df = data_df.drop(drop_indexes)\n\ndata_df = data_df.drop_duplicates(subset=['review'])\n\ndata_df = data_df.sample(frac=1, random_state=42)\ndata_df.rating-=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read and clean test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/student-shopee-code-league-sentiment-analysis/test.csv', index_col='review_id')\ntest_df['review'] = test_df['review'].apply(clean_emoji).apply(clean_urls).apply(clean_smileys).apply(recover_shortened_words).apply(delete_repeated_char).apply(remove_punc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate class weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\n\nclass_weights = dict(zip(np.unique(data_df.rating), sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(data_df.rating), data_df.rating)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions for tokenisation & model building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])\n\ndef build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy',\n    metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set training parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train models\n\nWe decided to go with Bert multilingual uncased model and xlm roberta as they're SOTA. Also, we decided to use K-fold cross validation (4-fold), which also greatly helped increase our test accuracies. We wrote a loop to preprocess data into tensorflow datasets and run on TPU uninterrupted. Overall, this whole notebook takes abt 2h45mins to run, which is within the 3hr limit Kaggle imposes on TPUs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAMES = ['bert-base-multilingual-uncased','jplu/tf-xlm-roberta-base']\nmodels = [0 for i in range(len(MODEL_NAMES))]\ntest_probs = [0 for i in range(len(MODEL_NAMES))]\ntest_results = [0 for i in range(len(MODEL_NAMES))]\n\nwith strategy.scope():\n    \n    for i in range(len(MODEL_NAMES)):\n        \n        models[i] = transformers.TFAutoModel.from_pretrained(MODEL_NAMES[i])\n        \n        tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAMES[i])\n        reviews = regular_encode(data_df.review.astype(str), tokenizer, maxlen=MAX_LEN)\n    \n        sss = StratifiedShuffleSplit(n_splits=4, random_state=42, test_size=0.2)\n        X, y = reviews, tf.keras.utils.to_categorical(data_df.rating.astype(int).values, num_classes=5)\n\n        models[i] = build_model(models[i], max_len=MAX_LEN)\n        models[i].summary()\n\n        for train_index, valid_index in sss.split(X, y):\n\n            X_train, X_valid = X[train_index], X[valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n\n            train_dataset = (\n                tf.data.Dataset\n                .from_tensor_slices((X_train, y_train))\n                .repeat()\n                .shuffle(2048)\n                .batch(BATCH_SIZE)\n                .prefetch(AUTO)\n            )\n\n            valid_dataset = (\n                tf.data.Dataset\n                .from_tensor_slices((X_valid, y_valid))\n                .batch(BATCH_SIZE)\n                .cache()\n                .prefetch(AUTO)\n            )\n\n            n_steps = X_train.shape[0] // BATCH_SIZE\n            models[i].fit(\n                train_dataset,\n                steps_per_epoch=n_steps,\n                validation_data=valid_dataset,\n                epochs=EPOCHS,\n                class_weight=class_weights\n            )\n            \n        test_reviews = regular_encode(test_df.review.astype(str), tokenizer, maxlen=MAX_LEN)\n\n        test_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((test_reviews))\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n        )\n\n        test_probs[i] = models[i].predict(test_dataset, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble results\n\nWe averaged of probabilities from the 2 models, which is the simplest method of ensembling. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['rating'] = np.argmax(((test_probs[0] + test_probs[1])/2), axis = 1)\ntest_df.rating +=1\ntest_df.rating.unique() #sanity check LOL","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.drop(['review'], axis=1)\ntest_df.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}