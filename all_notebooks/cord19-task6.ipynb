{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Preprocess data\n* Queries\n* Rank document\n    * BM25 and RM3\n    * Sentence-Bert\n* Summerization  \n* Relevant sentences\n    * Sentence-Bert \n* Information Extraction\n    * Name entity recognition\n    * Relation extraction  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Preprocess data\n * dataset: comm_use_subset\n * extract meaningful contents \n          * pdf_json: extract title, abstract and body\n          * pmc_json: extract title and body\n * clean data\n          * delete URLs, stopwords and punctuations","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# # Extract meaningful fields and clean the data.\n# import json\n\n# import os\n\n# import re\n# import string\n# from nltk.tokenize import word_tokenize\n# from nltk.corpus import stopwords\n\n# import pandas as pd\n# import math\n\n# stop_words = stopwords.words('english')\n\n# path = \"/kaggle/input/CORD-19-research-challenge\"\n\n\n# # files = os.listdir(path)\n\n\n# def clean(content):\n#     # delete URL\n#     results = re.compile(r'http://[a-zA-Z0-9.?/&=:]*', re.S)\n#     content = results.sub(\"\", content)\n#     content = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%|-)*\\b', '', content, flags=re.MULTILINE)\n\n#     content = content.lower()\n#     # delete stopwords\n#     tokens = word_tokenize(content)\n#     content = [i for i in tokens if not i in stop_words]\n#     s = ' '\n#     content = s.join(content)\n\n#     # delete punctuations, but keep '-'\n#     del_estr = string.punctuation\n#     del_estr = list(del_estr)\n#     del_estr.remove('-')\n#     del_estr = ''.join(del_estr)\n#     replace = \" \" * len(del_estr)\n#     tran_tab = str.maketrans(del_estr, replace)\n#     content = content.translate(tran_tab)\n\n#     return content\n\n\n# data = pd.read_csv(path + '/metadata.csv')\n# print(data.head(5))\n# print(len(data))\n# print(data['pdf_json_files'][15])\n\n# i = 0\n\n# for file in range(len(data)):\n#     s = []\n#     if not isinstance(data['pdf_json_files'][file], float):\n#         print(data['pdf_json_files'][file])\n#         pdf_path = data['pdf_json_files'][file].split('; ')\n#         filepath = path + \"/\" + pdf_path[0]\n#     elif not isinstance(data['pmc_json_files'][file], float):\n#         print(data['pmc_json_files'][file])\n#         pmc_path = data['pmc_json_files'][file].split('; ')\n#         filepath = path + \"/\" + pmc_path[0]\n#     else:\n#         continue\n\n#     with open(filepath, 'r', encoding='utf-8') as f:\n#         temp = json.loads(f.read())\n#         # print(temp.keys())\n        \n#         contents = []\n#         file_path = data['cord_uid'][file]\n#         if file_path in ppath:\n#             continue\n#         else:\n#             contents.append(file_path)\n#             contents.append('\\n')\n#             ppath.append(file_path)\n\n#         # print(file_path)\n\n#         metadata_dict = temp['metadata']\n#         metadata = []\n#         if 'title' in metadata_dict.keys():\n#             metadata.append(metadata_dict['title'])\n\n#         abstract = []\n#         if 'abstract' in temp.keys():\n#             abstract_list = temp['abstract']\n#             for content in abstract_list:\n#                 # print(content.keys())\n#                 if 'text' in content.keys():\n#                     abstract.append(content['text'])\n\n#         body_text_list = temp['body_text']\n#         body_text = []\n#         for content in body_text_list:\n#             # print(content.keys())\n#             if 'text' in content.keys():\n#                 body_text.append(content['text'])\n#         #\n#         # print(metadata)\n#         # print(\"___________________\")\n#         # print(body_text)\n#         # print(\"+++++++++++++++++++\")\n\n# #         contents = []\n\n# #         file_path = data['cord_uid'][file]\n# #         contents.append(file_path)\n# #         contents.append('\\n')\n\n#         contents.append(filepath)\n#         contents.append('\\n')\n\n#         metadata = str(metadata)\n#         metadata = clean(metadata)\n#         contents.append(metadata)\n\n#         abstract = str(abstract)\n#         abstract = clean(abstract)\n#         contents.append(abstract)\n\n#         body_text = str(body_text)\n#         body_text = clean(body_text)\n#         contents.append(body_text)\n\n#         # print(contents)\n#         #\n#         # f1 = open(path+\"/extract_3/%d.txt\" % (i + 1), 'w', encoding='utf-8')\n#         # contents = \"\".join(contents)\n#         # f1.write(contents)\n#         print(contents)\n#         print(\"!!!!!!!!!!!!!!!!!\")\n#         i += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Queries \n1 What do we know about non-pharmaceutical interventions?  \n2 What do we know about the effectiveness of non-pharmaceutical interventions?   \n3 What is known about equity and barriers to compliance for non-pharmaceutical interventions?  \n4 Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.  \n5 Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.  \n6 Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.  \n7 Methods to control the spread in communities, barriers to compliance and how these vary among different populations..  \n8 Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.  \n9 Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.  \n10 Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).  \n11 Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen or mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Rank documents\n* Use BM25 ,RM3 and sentence-bert to rank documents, and extract the top-20 relevant documents.(1/2)  \n\n    1. Here I used java Lucene to implement BM25 ,RM3 to rank. Since kaggle does not support java, I attached github link here: \n    https://github.com/YiboWANG214/COVID19/tree/master/lucenerank\n\n    Part of the results are shown below.  \n    Format is: query_id + 'Q0' + doc_path + rank + score + agent.  \n    ![WechatIMG55.png](https://i.loli.net/2020/05/05/9tOdRJmi7N2EwXl.png)\n    ![WechatIMG56.png](https://i.loli.net/2020/05/05/viGJpot9dODCmnS.png)\n    \n    Then I built some csv files according to doc_path to show rankings and document contents. The complete results with document contents are shown in Google docs:  \n    BM25: https://drive.google.com/file/d/1n40OmWaOmuyZDXn_EH7TVaf2xhYC8Yz1/view?usp=sharing   \n    RM3: https://drive.google.com/file/d/1tRMl0UqSQjTSjoURm-6zyr1hfZpteTGu/view?usp=sharing  \n    The intersection of BM25 and RM3: https://drive.google.com/file/d/1XuBQo5iKMF7ZBl5KQe-yDS_iIZoF6NvO/view?usp=sharing\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Create csv files.\n# import json\n# import pandas as pd\n# import csv\n\n# BM25 = []\n# paper_num_BM25 = []\n# with open('lucene_result_BM25.txt', 'r', encoding='utf-8') as f1:\n#     for line in f1:\n#         # print(line)\n#         line = line.split(' ')\n#         BM25.append(line[5])\n#         paper_num_BM25.append(line[4])\n\n# RM3 = []\n# paper_num_RM3 = []\n# with open('lucene_result_RM3.txt', 'r', encoding='utf-8') as f2:\n#     for line in f2:\n#         # print(line)\n#         line = line.split(' ')\n#         RM3.append(line[6])\n#         paper_num_RM3.append(line[4])\n\n# query = []\n# with open('/kaggle/input/covid19task6queries/queries.txt', 'r', encoding='utf-8') as f3:\n#     for line in f3:\n#         # line = line.split(' ')\n#         line = line.strip('\\n')\n#         query.append(line[2:])\n# # print(query)\n\n# with open(\"/BM25.csv\", \"a\") as csvfile:\n#     writer_BM25 = csv.writer(csvfile)\n#     writer_BM25.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\"])\n\n#     for i in range(11):\n#         k = 0\n\n#         query_curr = query[i]\n#         rank_curr = k\n#         for file in BM25[20*i: 20*i+20]:\n#             # print(file)\n#             k += 1\n#             insert = []\n#             insert.append(query_curr)\n#             insert.append(k)\n#             with open(file, 'r', encoding='utf-8') as f:\n#                 temp = json.loads(f.read())\n\n# #                 paper_id_curr = temp['paper_id']\n#                 paper_id_curr = paper_num_BM25[20*i+k-1]\n#                 insert.append(paper_id_curr)\n\n#                 metadata_dict = temp['metadata']\n\n#                 if 'title' in metadata_dict.keys():\n#                     title = metadata_dict['title']\n#                 insert.append(title)\n\n#                 abstract = []\n#                 if 'abstract' in temp.keys():\n#                     abstract_list = temp['abstract']\n#                     for content in abstract_list:\n#                         if 'text' in content.keys():\n#                             abstract.append(content['text'])\n#                 insert.append(' '.join(abstract))\n\n#                 if 'body_text' in temp.keys():\n#                     contents_list = temp['body_text']\n#                     contents = []\n#                     for content in contents_list:\n#                         if 'text' in content.keys():\n#                             contents.append(content['text'])\n#                 insert.append(' '.join(contents))\n\n#             # print([insert])\n#             writer_BM25.writerows([insert])\n\n\n# with open(\"/RM3.csv\", \"a\") as csvfile:\n#     writer_RM3 = csv.writer(csvfile)\n#     writer_RM3.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstrct\", \"contents\"])\n#     for i in range(11):\n#         k = 0\n#         # writer.writerows([[0,1,3],[1,2,3],[2,3,4]])\n#         query_curr = query[i]\n#         rank_curr = k\n#         for file in RM3[20*i: 20*i+20]:\n#             # print(file)\n#             k += 1\n#             insert = []\n#             insert.append(query_curr)\n#             insert.append(k)\n#             with open(file, 'r', encoding='utf-8') as f:\n#                 temp = json.loads(f.read())\n\n# #                 paper_id_curr = temp['paper_id']\n#                 paper_id_curr = paper_num_RM3[20*i+k-1]\n#                 insert.append(paper_id_curr)\n\n#                 metadata_dict = temp['metadata']\n\n#                 if 'title' in metadata_dict.keys():\n#                     title = metadata_dict['title']\n#                 insert.append(title)\n\n#                 if 'abstract' in temp.keys():\n#                     abstract_list = temp['abstract']\n#                     abstract = []\n#                     for content in abstract_list:\n#                         if 'text' in content.keys():\n#                             abstract.append(content['text'])\n#                 insert.append(' '.join(abstract))\n\n#                 if 'body_text' in temp.keys():\n#                     contents_list = temp['body_text']\n#                     contents = []\n#                     for content in contents_list:\n#                         if 'text' in content.keys():\n#                             contents.append(content['text'])\n#                 insert.append(' '.join(contents))\n\n#             # print([insert])\n#             writer_RM3.writerows([insert])\n\n\n# with open(\"/intersection.csv\", \"a\") as csvfile:\n#     writer_inter = csv.writer(csvfile)\n#     writer_inter.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\"])\n#     for i in range(11):\n#         k = 0\n#         # writer.writerows([[0,1,3],[1,2,3],[2,3,4]])\n#         query_curr = query[i]\n#         rank_curr = k\n#         a = BM25[20*i: 20*i+20]\n#         b = RM3[20*i: 20*i+20]\n#         intersection = list(set(a).intersection(set(b)))\n#         c = paper_num_BM25[20*i: 20*i+20]\n#         d = paper_num_RM3[20*i: 20*i+20]\n#         num_intersection = list(set(c).intersection(set(d)))\n#         print(len(intersection))\n#         for file in intersection[0:10]:\n#             k += 1\n#             # insert = [query_curr, k]\n#             insert = []\n#             insert.append(query_curr)\n#             insert.append(k)\n#             with open(file, 'r', encoding='utf-8') as f:\n#                 temp = json.loads(f.read())\n\n# #                 paper_id_curr = temp['paper_id']\n#                 paper_id_curr = num_intersection[20*i+k-1]\n#                 insert.append(paper_id_curr)\n\n#                 metadata_dict = temp['metadata']\n\n#                 if 'title' in metadata_dict.keys():\n#                     title = metadata_dict['title']\n#                 insert.append(title)\n\n#                 if 'abstract' in temp.keys():\n#                     abstract_list = temp['abstract']\n#                     abstract = []\n#                     for content in abstract_list:\n#                         if 'text' in content.keys():\n#                             abstract.append(content['text'])\n#                 insert.append(' '.join(abstract))\n\n#                 if 'body_text' in temp.keys():\n#                     contents_list = temp['body_text']\n#                     contents = []\n#                     for content in contents_list:\n#                         if 'text' in content.keys():\n#                             contents.append(content['text'])\n#                 insert.append(' '.join(contents))\n\n#             # print([insert])\n#             writer_inter.writerows([insert])\n\n\n\n# # for i in range(11):\n# #     f3 = open(\n# #         \"/query/query%d.txt\" % (\n# #                     i+1), 'a', encoding='utf-8')\n# #     a = BM25[20*i: 20*i+20]\n# #     b = RM3[20*i: 20*i+20]\n# #     intersection = list(set(a).intersection(set(b)))\n# #     for file in intersection[0:3]:\n# #         with open(file, 'r', encoding='utf-8') as f:\n# #             temp = json.loads(f.read())\n# #\n# #             paper_id = temp['paper_id']\n# #             # print(paper_id)\n# #             # f3.write(paper_id+'\\n')\n# #\n# #             metadata_dict = temp['metadata']\n# #             if 'title' in metadata_dict.keys():\n# #                 title = metadata_dict['title']\n# #             # print(type(title))\n# #             # print(title)\n# #             # f3.write(title+'\\n')\n# #\n# #             if 'abstract' in temp.keys():\n# #                 abstract_list = temp['abstract']\n# #                 abstract = []\n# #                 for content in abstract_list:\n# #                     if 'text' in content.keys():\n# #                         abstract.append(content['text'])\n# #                 # f3.write(str(abstract)+'\\n')\n# #                 # print(str(abstract))\n# #             else:\n# #                 # f3.write('None\\n')\n# #                 # print('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * Use BM25 ,RM3 and sentence-bert to rank documents, and extract the top-20 relevant documents.(2/2)  \n\n     2. Second, I used java Lucene BM25 to retrieve the top-1000 documents, and then used sententce-bert to rerank according to titles of documents and get the top 20 documents.  \n        \n        The complete results with document contents are shown in Google docs:  \n        Bert: https://drive.google.com/file/d/1-5C_7hDe6Y5rw7Zp4rwZsjmVM2chNCp1/view?usp=sharing  \n        \n     3. I also tried to rerank according to titles and abstracts to involve more information. But it is super slow, so I just reranked on the top-200 documents.   \n           ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Collect useful information of the top-1000 documents to build a csv file.\n\n# BM25 = []\n# paper_num_BM25 = []\n# with open('lucene_BM25_1000.txt', 'r', encoding='utf-8') as f1:\n#     for line in f1:\n#         # print(line)\n#         line = line.split(' ')\n#         BM25.append(line[5])\n#         paper_num_BM25.append(line[4])\n        \n# with open(\"/BM25_1000.csv\", \"a\") as csvfile:\n#     writer_BM25 = csv.writer(csvfile)\n#     writer_BM25.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\"])\n\n#     for i in range(11):\n#         k = 0\n\n#         query_curr = query[i]\n#         rank_curr = k\n#         for file in BM25[1000*i: 1000*i+1000]:\n#             # print(file)\n#             k += 1\n#             insert = []\n#             insert.append(query_curr)\n#             insert.append(k)\n#             with open(file, 'r', encoding='utf-8') as f:\n#                 temp = json.loads(f.read())\n\n# #                 paper_id_curr = temp['paper_id']\n                \n#                 paper_id_curr = paper_num_BM25[1000*i+k-1]\n#                 insert.append(paper_id_curr)\n\n#                 metadata_dict = temp['metadata']\n\n#                 if 'title' in metadata_dict.keys():\n#                     title = metadata_dict['title']\n#                 insert.append(title)\n\n#                 if 'abstract' in temp.keys():\n#                     abstract_list = temp['abstract']\n#                     abstract = []\n#                     for content in abstract_list:\n#                         if 'text' in content.keys():\n#                             abstract.append(content['text'])\n#                 insert.append(' '.join(abstract))\n\n#                 if 'body_text' in temp.keys():\n#                     contents_list = temp['body_text']\n#                     contents = []\n#                     for content in contents_list:\n#                         if 'text' in content.keys():\n#                             contents.append(content['text'])\n#                 insert.append(' '.join(contents))\n\n#             # print([insert])\n#             writer_BM25.writerows([insert])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test GPU.\n\nimport tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\n\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')\n\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Use bert to rerank.\n\n# !pip install -U sentence-transformers\n\n# from sentence_transformers import SentenceTransformer\n# model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n\n# import pandas as pd\n# import numpy as np\n# np.set_printoptions(threshold=sys.maxsize)\n# pd.set_option('display.width',None)\n\n# df = pd.read_csv(\"/kaggle/input/preparationforbert/BM25_1000.csv\")\n# # print(df.head(5))\n# df['title'] = df['title'].astype(str)\n# df['encoding'] =\"\"\n# for rows,index in df.iterrows():\n#     title = index['title']\n#     print(title)\n#     search_phrase_vector = model.encode([title])[0]\n#     # print(search_phrase_vector)\n#     df.at[rows,'encoding'] = search_phrase_vector\n#     # print(df.loc[rows])\n# # df.to_csv('bert_encodings.csv')\n\n# query = []\n# with open('/kaggle/input/covid19task6queries/queries.txt', 'r', encoding='utf-8') as f1:\n#     for line in f1:\n#         # line = line.split(' ')\n#         line = line.strip('\\n')\n#         query.append(line[2:]) \n        \n# from sklearn.metrics.pairwise import cosine_similarity\n# import csv\n\n# with open(\"result.csv\", \"a\") as csvfile:\n    \n#     writer = csv.writer(csvfile)\n#     writer.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\", \"value\"])\n#     for i in range(11):\n#         query_en = model.encode([query[i]])[0]\n#         query_en = query_en.reshape(-1,1024)\n#         print(query[i])\n\n#         result_cur = []\n#         for j in range(i*1000, i*1000+1000):\n#             row = df.loc[j]\n#             doc = row['encoding']\n#             doc = doc.reshape(-1,1024)\n#             value = cosine_similarity(doc, query_en)\n#             query_cur = query[i]\n#             paper_id = row['paper_id']\n#             result_cur.append((paper_id, value))\n#         print(result_cur[0])\n#         result_cur = sorted(result_cur, key=lambda x:x[1], reverse=True)\n#         print(result_cur[0:6])\n\n#         t = 0\n#         for k in range(20):\n#             for j in range(i*1000, i*1000+1000):\n#                 if df.loc[j]['paper_id'] == result_cur[k][0]:\n#                     title_cur = df.loc[j]['title']\n#                     abstract_cur = df.loc[j]['abstract']\n#                     contents_cur = df.loc[j]['contents']\n#             t += 1\n#             result = []\n#             result.append(query_cur)\n#             result.append(str(t))\n#             result.append(result_cur[k][0])\n#             result.append(title_cur)\n#             result.append(abstract_cur)\n#             result.append(contents_cur)\n#             result.append(str(result_cur[k][1][0][0]))\n#             print(result)\n#             writer.writerows([result])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Summerization  \n* First, I cleaned title, abstract and contents of the document; \n* Then I calculated word frequencies of terms, which is not stop words, in the document, where the most frequent term is considered as the most important term;\n* Generalized word frequencies;\n* Calculated sentence scores by adding generalized word frequencies together;\n* Selected the top 5 sentences with highest score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/bertresultstitleonly/result_queries_10.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\n\nstop_words = stopwords.words('english')\n\ndef clean(content):\n    if pd.isnull(content):\n        return ''\n    # delete URL\n    results = re.compile(r'http://[a-zA-Z0-9.?/&=:]*', re.S)\n    content = results.sub(\"\", content)\n    content = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%|-)*\\b', '', content, flags=re.MULTILINE)\n\n    content = content.lower()\n    # delete stopwords\n    tokens = word_tokenize(content)\n    content = [i for i in tokens if not i in stop_words]\n    s = ' '\n    content = s.join(content)\n\n    # delete punctuations, but keep '-'\n    del_estr = string.punctuation\n    del_estr = list(del_estr)\n    del_estr.remove('-')\n    del_estr = ''.join(del_estr)\n    replace = \" \" * len(del_estr)\n    tran_tab = str.maketrans(del_estr, replace)\n    content = content.translate(tran_tab)\n\n    return content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq\nimport nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nsummerize = []\nfor i in range(len(df)):\n    word_frequencies = {}\n    text_original = ''\n    if not pd.isnull(df['title'][i]):\n        text_original = text_original + df['title'][i] + ' '\n    if not pd.isnull(df['abstract'][i]):\n        text_original = text_original + df['abstract'][i] + ' '\n    if not pd.isnull(df['contents'][i]):\n        text_original = text_original + df['contents'][i]\n    text = clean(text_original)\n\n    # print(i, text[:10])\n\n    for word in nltk.word_tokenize(text):\n        if word not in stopwords:\n            if word not in word_frequencies.keys():\n                word_frequencies[word] = 1\n            else:\n                word_frequencies[word] += 1\n#     print(len(word_frequencies))\n\n    maximum_frequncy = max(word_frequencies.values())\n#     print(maximum_frequncy)\n#     print(max(word_frequencies,key=word_frequencies.get))\n\n    for word in word_frequencies.keys():\n        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n\n    sentence_list = nltk.sent_tokenize(text_original.replace('e.g.', 'eg'))\n#     print(sentence_list)\n\n    sentence_scores = {}\n    for sent in sentence_list:\n        for word in nltk.word_tokenize(sent.lower()):\n            if word in word_frequencies.keys():\n                if len(sent.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word]\n\n    summary_sentences = heapq.nlargest(5, sentence_scores, key=sentence_scores.get)\n\n    summary = ' '.join(summary_sentences)\n    summerize.append(summary)\n#     print(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head(5))\ndf.to_csv(\"summerize.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 5. Retrieve relevant sentences\n* Use sentence-bert to retrieve the top 10 relevant sentences of each queries from retrieved documents. \n\n    1. I used sentence-bert to calculated the cosine similarity score between each sentence and the chosen query;  \n    2. Since the retrieved relevant sentences should be relevant to each other, I used kmeans to classify the top 50 sentences, where the top 10 sentences of the class with the highest mean score are retrieved.  \n    \n    The complete results are shown in Google docs: https://drive.google.com/file/d/1j9P8mnZfbCF_r24hzRWGgoMWMRYXybno/view?usp=sharing  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('/kaggle/input/bertresultstitleonly/result_queries_10.csv')\nprint(data.head(5))\n\ncontents = data['contents']\n# print(contents[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n\ncontent_curr = nltk.sent_tokenize(contents[0].replace('e.g.', 'eg'))\n# print(type(content_curr))\n# content_curr = contents[0].replace('e.g.', 'eg').split('. ')\n# print(content_curr)\nprint(len(content_curr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade gap-stat\n# !pip install gapkmean\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef optimalK(data, nrefs=3, maxClusters=15):\n    gaps = np.zeros((len(range(1, maxClusters)),))\n    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n    for gap_index, k in enumerate(range(1, maxClusters)):\n\n        refDisps = np.zeros(nrefs)\n\n        for i in range(nrefs):\n            randomReference = np.random.random_sample(size=data.shape)\n            \n            km = KMeans(k)\n            km.fit(randomReference)\n            \n            refDisp = km.inertia_\n            refDisps[i] = refDisp\n\n        km = KMeans(k)\n        km.fit(data)\n        \n        origDisp = km.inertia_\n\n        gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n\n        gaps[gap_index] = gap\n        \n        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n\n    return (gaps.argmax() + 1, resultsdf) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport csv\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n\n\nquery = []\nwith open('/kaggle/input/covid19task6queries/queries.txt', 'r', encoding='utf-8') as f1:\n    for line in f1:\n        # line = line.split(' ')\n        line = line.strip('\\n')\n        query.append(line[2:]) \n\n\nwith open(\"result_sentence_knn_10.csv\", \"a\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"query\", \"rank\", \"sentence\", \"value\"])\n    for i in range(11):\n        query_en = model.encode([query[i]])[0]\n        query_en = query_en.reshape(-1,1024)\n        print(query[i])\n\n        result_feature = []\n        result_sentence = []\n        result_value = []\n        for j in range(10*i, 10*i+10):\n            content_curr = nltk.sent_tokenize(contents[j].replace('e.g.', 'eg'))\n            length = len(content_curr)\n            for k in range(length):\n                sentence = model.encode([content_curr[k]])[0]\n                sentence = sentence.reshape(-1,1024)\n                value = cosine_similarity(sentence, query_en)\n                result_feature.append(sentence)\n                result_sentence.append(content_curr[k])\n                result_value.append(value)\n\n        my_df = pd.DataFrame(data=result_sentence, columns=['sentence'])\n        my_df['feature'] = result_feature\n        my_df['value'] = result_value\n        my_df = my_df.sort_values(by='value', ascending=False)\n#         print(len(my_df))\n        my_df = my_df.iloc[:50]\n#         print(len(my_df))\n#         print(my_df.head(5))\n\n        # result_feature = np.asarray(my_df.feature.to_numpy()).reshape(-1,1024)\n        # print(np.array(my_df.feature.to_list()).shape)\n        bestKValue, gapdf = optimalK(np.array(my_df.feature.to_list()).reshape(-1,1024), nrefs=5, maxClusters=5)\n#         print(bestKValue)\n        \n\n        km = KMeans(bestKValue)\n        result = km.fit_predict(np.array(my_df.feature.to_list()).reshape(-1,1024))\n        clusters = km.labels_.tolist()\n\n        my_df['label'] = clusters\n\n        mean_values = []\n        for ii in range(bestKValue):\n            a = np.mean(my_df[my_df.label==ii].value)\n            mean_values.append(a)\n        label = np.argmax(mean_values)\n\n        sentences = my_df[my_df.label==label].sentence.tolist()\n        values = my_df[my_df.label==label].value.tolist()\n\n        query_sentence = query[i]\n        for m in range(min(10, len(sentences))):\n            input_contents = [query_sentence, str(m+1), sentences[m], values[m]]\n            # print(input_contents)\n            writer.writerows([input_contents])\n\n\n        # query_sentence = query[i]\n        # for m in range(10):\n        #     input_contents = [query_sentence, str(m+1), result_sentence[m][0], str(result_sentence[m][1][0][0])]\n        #     print(input_contents)\n            # writer.writerows([input_contents])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Information Extraction\n* Use stanford corenlp to obtain the triple relation of the top 10 relevant sentences. \n\nThe complete results are shown in Google docs: https://drive.google.com/file/d/12fOkprGYMQndJg51pI8Iwa1gknfqf4jU/view?usp=sharing  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ndf = pd.read_csv('result_sentence_knn_10.csv')\nprint(df.head(5))\n\nsentences = df['sentence']\nprint(sentences[0])\n\nprint(df.columns)\n# print(type(sentences))\n# print(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\n!unzip stanford-corenlp-full-2018-02-27.zip\n!cd stanford-corenlp-full-2018-02-27\n\n!echo \"Downloading CoreNLP...\"\n!wget \"http://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip\" -O corenlp.zip\n!unzip corenlp.zip\n!mv ./stanford-corenlp-4.0.0 ./corenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install stanza\n\nimport stanza\n\n\nimport os\nos.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n\nfrom stanza.server import CoreNLPClient\n\nclient = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='4G', endpoint='http://localhost:9001')\nprint(client)\n\nclient.start()\nimport time; time.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycorenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom pycorenlp import *\nimport collections\n\nrelation = []\n\nfor sentence in sentences:\n#     print(sentence)\n    doc = client.annotate(sentence, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n                                                \"outputFormat\": \"json\",\n                                                \"triple.strict\":\"true\"\n                                                #  \"openie.triple.strict\":\"true\",\n                                                # \"openie.max_entailments_per_clause\":\"2\"\n                                                })\n    result = [doc[\"sentences\"][0][\"openie\"] for item in doc]\n\n    extraction = []\n    subjects = []\n    for i in result:\n        # print(i)\n        for rel in i:\n            relationSent=rel['subject'],rel['relation'],rel['object']\n            if relationSent[0] not in subjects:\n                subjects.append(relationSent[0])\n                extraction.append(relationSent)\n                # print(relationSent)\n            # print(relationSent)\n        # print(\"_____\")\n        # rel = i[-1]\n        # relationSent=rel['subject'],rel['relation'],rel['object']\n        # print(relationSent)\n    # print(extraction)\n    # print(\"____________\")\n    relation.append(extraction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(relation[:5])\nrelation = pd.Series(relation)\nprint(relation)\ndf[\"relation\"] = relation\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"result_relation_knn_10.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}