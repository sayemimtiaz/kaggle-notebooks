{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, we will answer the tasks through the use of transformer neural networks.\n\nNowadays, state-of-the-art neural networks for Natural Language Processing are the transformer networks, making use of attention mechanisms to get the information from context words. Huggingface provides a great library for fast prototyping with transformers.\n\nSee [Huggingface transformers](https://github.com/huggingface/transformers)\n\nand the most important papers: \n[transformer](https://arxiv.org/pdf/1706.03762.pdf)\n[Bert](https://arxiv.org/pdf/1810.04805.pdf)\n\n\nAs the transformers are huge architectures with hundred of millions of parameters, they are very slow and cannot be used directly for Information Retrieval tasks.\n\nWe will therefore follow this plan:\n\n* Filter the papers mentioning the coronavirus and equivalent terms, with a query search in the paper text using BM25\n* Filter the papers mentioning the query terms, again using BM25 because it is a cheap algorithm which proved its usefulness in the Information retrieval field for decades\n\nOnce we have only 50 papers to work with, we can start using neural networks:\n\n* Refine the query search by using the sentence embedding similarity between the query and abstracts using Biobert\n* Use the improved Sentence-Bert algorithm for computing the sentence embedding similarity between the query and abstracts\n* Use Sentence-Bert on the text paragraphs instead of the abstracts to rank the papers. This is quite slow.\n* Use Question-Answering with Bert-QA to answer targetted questions.\n* Summarize the top ranked papers with Bart. This step is also slow.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Copy the Git repository for the helper functions.\nSee: [github.com/JeremyKraft67/covid-kaggle](https://github.com/JeremyKraft67/covid-kaggle.git)\n\nAlso install the other external libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/github-files/covid-kaggle-master/* ./\n!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n!pip install rank_bm25\n!pip install -U sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom time import time\nfrom scipy.stats import spearmanr, kendalltau\nfrom sentence_transformers import SentenceTransformer\nimport scipy\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom rank_bm25 import BM25Okapi, BM25Plus # don't use BM25L, there is a mistake\nimport os\nimport torch\nimport numpy\nfrom tqdm import tqdm, trange\nfrom transformers import *\nfrom tqdm import tqdm\nimport warnings\n\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport nltk\nimport re\nfrom time import time\nfrom scipy.stats import spearmanr, kendalltau\nfrom sentence_transformers import SentenceTransformer\nimport scipy\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport math\nimport string\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the helper functions","execution_count":null},{"metadata":{"_uuid":"1f30b8e5-fb41-4516-9a30-0668ea641271","_cell_guid":"017bce1c-05dd-43a4-a07e-b5d765d7450f","trusted":true},"cell_type":"code","source":"from covid import BM25_helper_functions as BM25_hf\nfrom covid import transformer_helper_functions as transf_hf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the preprocessed data.\n\nThe data are first extracted from the JSON files into dataframes using the code from:\n[xhlulu](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv)\n\nThen, the texts are prepared for indexing by BM25, with the following steps:\n* Lower case\n* remove punctuation\n* numeric strings\n* stopwords\n* stemming\n\nSee [BM25 benchmark paper](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\nThen we prefilter the papers containing the covid19, sars-cov2, etc, keywords, to reduce the amount of papers.\nSee the code on github for more details about the preprocessing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/papers-12gb-prepared/* ./\n# load the 50.000 papers, and filter them to contain covid 19 keywords, about ~ 1900 papers remaining\ndata_df_red = pd.read_csv(\"./all_doc_df_12gb_filtered.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df_red","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Choose what algorithm to use and download the corresponding models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BIOBERT = True\nS_BERT = True\nCOMPUTE_PARAGRAPH_SCORE = True\nQA = True\nBART = True\ntop_res = 50\n\n\n# prepare models\n\nif BIOBERT:\n    # load model\n    tokenizer_biobert = AutoTokenizer.from_pretrained('monologg/biobert_v1.1_pubmed', do_lower_case=False)\n    config = AutoConfig.from_pretrained('monologg/biobert_v1.1_pubmed', output_hidden_states=True)\n    model_biobert = AutoModel.from_pretrained('monologg/biobert_v1.1_pubmed', config=config)\n    assert model_biobert.config.output_hidden_states == True\nif S_BERT:\n    embedder = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\nif QA:\n    # QA\n    # tokenizer = AutoTokenizer.from_pretrained(\"ahotrod/albert_xxlargev1_squad2_512\")\n    # model = AutoModelForQuestionAnswering.from_pretrained(\"ahotrod/albert_xxlargev1_squad2_512\")\n    tokenizer_qa = AutoTokenizer.from_pretrained(\"clagator/biobert_squad2_cased\")\n    model_qa = AutoModelForQuestionAnswering.from_pretrained(\"clagator/biobert_squad2_cased\")\n    # tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n    # model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n    # tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n    # model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n\nif BART:\n    \n    if torch.cuda.is_available():\n      device = 'cuda'\n    else:\n      device = 'cpu'\n\n    model_name= \"bart-large-xsum\"\n    model_name = \"bart-large-cnn\" # Use the cnn model. Otherwise the model hallucinates too many details.\n    \n    model_bart = BartForConditionalGeneration.from_pretrained(model_name)\n    tokenizer_bart = BartTokenizer.from_pretrained(model_name)\n    model_bart.to(device)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Query","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_query = 'Are the pregnant women more at risk?'\nflat_query = 'Is there a seasonality in transmission?'\nflat_query = 'How does temperature and humidity affect the transmission of 2019-nCoV?'\nflat_query = 'What is the longest duration of viral shedding?'\n# flat_query = 'What is the fatality rate?'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Query search\n\n**Second paper filtering**\n\nSearch for the query terms in the paper texts using a classical string search based algorithm: BM25 to reduce the amount of papers to run the transformers on to 50.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quest_text = [flat_query]\n# remove \\n, but has no impact, this is already done somewhere else\nmodel = BM25Okapi\nindices, scores, quest = BM25_hf.search_corpus_for_question(\n    quest_text, data_df_red, model, len(data_df_red), 'cleaned_text')\n# remove again docs without keywords if searched with Okapi BM25\nif model == BM25Okapi:\n    contain_keyword = np.array(indices)[scores>0]\n    answers = data_df_red.iloc[contain_keyword, :].copy()\n    answers['scores_BM25'] = scores[scores>0]\nelse:\n    answers = data_df_red.iloc[indices, :].copy()\n    answers['scores_BM25'] = scores\nanswers = answers.sort_values(['scores_BM25'], ascending=False)\nanswers = answers.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enrich the paper abstracts by concatenating the title and the abstract. This is necessary as some titles / abstracts are missing.\n\nThis field will be used by the transformers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def concat_title_abstract(ans):\n    \"\"\"\n    Compute the title with the abstract\n    to enrich the abstracts.\n    Parameters\n    ----------\n    ans: pandas dataframe\n        Dataframe with the results of BM25\n    Returns\n    -------\n    ans: pandas dataframe\n        Dataframe with the added column\n    \"\"\"\n\n    # concatenate title and abstract\n    title_list = list(ans['title'])\n    abstract_list = list(ans['abstract'])\n    ind_list = list(range(len(title_list)))\n    title_abstr_list = list(map(\n        lambda x: clean_hf.add_title_to_abstr(x, title_list, abstract_list),\n        ind_list))\n    ans['title_abstr'] = title_abstr_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = answers[['scores_BM25', 'title', 'abstract', 'text']].copy()\nans = transf_hf.concat_title_abstract(ans)\n# clean-up\n# remove rows without titles and abstracts\nans = ans[~(ans['title_abstr'].isna() | (ans['title_abstr'] == ' '))].iloc[:top_res, :].copy()\nans = ans.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that BM25 provides a very cheap yet effective search.\nIt can be run over the whole paper text, thus leveraging the full information. Importantly, it can be used as a pre-filter or a first ranker.\n\nWe will refine this query search by using the semantic similarity provided by neural networks instead of the exact match string searches.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Similarity search\n\n**Refine the search by computing the similarity between the query and the abstracts with the transformer neural networks.**\n\nWe compute the embeddings of the query and the abstracts with pre-trained models from Huggingface. Then we compute the cosine similarity between both to rank the papers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper function to compare the similarity embeddings against the BM25 results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_scores(ans):\n    \"\"\"\n    Compare the scores to the BM25 scores.\n    Parameters\n    ----------\n    ans : pandas dataframe\n        Dataframe containing the BM25 score and other algorithm's score\n    \"\"\"\n\n    col_list = list(ans.columns)\n    # find out which column have scores\n    col_has_score = list(map(lambda x: ('score' in x) and not('BM25' in x), col_list))\n    col_w_score = np.array(col_list)[col_has_score]\n\n    # compute comparisons between methods\n    for col in col_w_score:\n        print('Similarity between ' + col + ' and BM25:')\n        print(spearmanr(ans[col], ans['scores_BM25']))\n        print(kendalltau(ans[col], ans['scores_BM25']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first try with a Bert model pre-trained on biomedical papers.\n\nCompute the Biobert similarity.\n\nSee the following papers for more information:\n\n[Biobert](https://arxiv.org/pdf/1901.08746.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_scibert(text, tokenizer, model):\n    \"\"\"\n    Compute the transformer embeddings.\n    If the text is too long, chunk it.\n    Parameters\n    ----------\n    text : string\n        Input text\n    tokenizer : Huggingface tokenizer\n        Tokenizer of the model\n    model : Huggingface model\n        Model\n    Returns\n    -------\n    text_ids : tensor\n        Tensor of token ids of dimension 1, quantity of tokens + 2 ( for CLS, SEP)\n    text_words : list of string\n        List of tokens\n    state : tensor\n        Tensor of output embeddings of dimension 1, quantity of tokens, hidden size\n    class_state : tensor\n        Tensor of the CLS embedding of dimension hidden size\n    layer_concat : tensor\n        Tensor of the 4 last layers concatenated of dimension 1, quantity of tokens, hidden size * 4\n        ordered (-4, ..., -1)\n    \"\"\"\n\n    # check that the model has been configured to output the embeddings from all layers\n    assert model.config.output_hidden_states == True\n\n    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n\n    n_chunks = int(numpy.ceil(float(text_ids.size(1)) / 510))\n    states = []\n    class_states = []\n    layer_concats = []\n\n    # chunk the text into passages of maximal length\n    for ci in range(n_chunks):\n        text_ids_ = text_ids[0, 1 + ci * 510:1 + (ci + 1) * 510]\n        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n        if text_ids[0, -1] != text_ids[0, -1]:\n            text_ids_ = torch.cat([text_ids_, text_ids[0, -1].unsqueeze(0)])\n\n        with torch.no_grad():\n            res = model(text_ids_.unsqueeze(0))\n            # stock all embeddings and the CLS embeddings\n            state = res[0][:, 1:-1, :]\n            class_state = res[0][:, 0, :]\n            # compute the concatenation of the last 4 layers\n            # initialize the embeddings\n            all_embed = res[2]\n            layer_concat = all_embed[-4][:, 1:-1, :]\n            for i in range(3):\n                # take only regular tokens\n                layer_concat = torch.cat((layer_concat, all_embed[3 - i][:, 1:-1, :]), dim=2)\n\n        states.append(state)\n        class_states.append(class_state)\n        layer_concats.append(layer_concat)\n\n    # give back the results as tensors of dim (# tokens, # fetaures)\n    state = torch.cat(states, axis=1)[0]\n    class_state = torch.cat(class_states, axis=1)[0]\n    layer_concat = torch.cat(layer_concats, axis=1)[0]\n\n    return text_ids, text_words, state, class_state, layer_concat\n\n\ndef cross_match(state1, state2, use_CLS=False):\n    if not use_CLS:\n        sim = torch.cosine_similarity(torch.mean(state1, 0), torch.mean(state2, 0), dim=0)\n    else:\n        sim = torch.cosine_similarity(state1, state2, dim=0)\n    sim = sim.numpy()\n    return sim\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get a quantitative idea how well the similarity search is performing, we evaluate the scores against the BM25 algorithm ranks using correlation metrics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if BIOBERT:\n\n    # BERT embeddings similarity\n\n    # consider only top results from BM25\n    use_CLS = False # TO DO: solve bug when abstract is longer than 512 tokens\n    use_last_four = True\n    search_field = 'title_abstr'\n\n    # process query\n    query_ids, query_words, query_state, query_class_state, query_layer_concat =\\\n        extract_scibert(flat_query, tokenizer_biobert, model_biobert)\n\n    # compute similarity scores\n    sim_scores = []\n    for text in tqdm(ans[search_field]):\n        text_ids, text_words, state, class_state, layer_concat = extract_scibert(text, tokenizer_biobert, model_biobert)\n        if use_CLS:\n            sim_score = cross_match(query_class_state, class_state, True) # try cosine on CLS tokens\n        elif use_last_four:\n            sim_score = cross_match(query_layer_concat, layer_concat, False)\n        else:\n            sim_score = cross_match(query_state, state, False)\n        sim_scores.append(sim_score)\n\n    # Store results in the dataframe\n    end_ans = ans.copy()\n    orig_col = list(ans.columns)\n    end_ans['score_biobert'] = np.array(sim_scores)\n    # reorder columns\n    end_ans = end_ans[['score_biobert'] + orig_col]\n    end_ans = end_ans.sort_values(['score_biobert'], ascending=False)\\\n                    .reset_index(drop=True)\n    ans = end_ans.copy()\n    # print scores\n    compare_scores(ans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try Sentence Bert to compute the similarities.\n\nSentence Bert is a novel architecture to improve Bert for textual similarity tasks. It processes the 2 sentences to compare with a Bert + siamese network.  This has  2 main benefits:\n* It is much more efficient than processing each possible query / paragraph pair by Bert, as in the original Bert paper, because it avoids a combinatorial explosion if there are many pairs to search.\n* It provides a much better sentence representation than averaging the Bert embeddings or using the CLS token embedding from Bert.\n\nSee the sentence Bert paper:\n[sentence-Bert](https://arxiv.org/pdf/1908.10084.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef search_w_stentence_transformer(embedder, flat_query,\n                                corpus_list,\n                                show_progress_bar=True, batch_size=8):\n    \"\"\"\n    Compute the similarity scores of Sentence transformer.\n    Parameters\n    ----------\n    embedder : Sentence Transformer model\n        Model to use\n    flat_query : string\n        Query text\n    corpus_list: list of string\n        Texts to search in\n    show_progress_bar : boolean\n        True to show the progress bar when computing the corpus embeddings\n    batch_size : int\n        batch size for Sentence Transformer inference\n    Returns\n    -------\n    s_bert_res : list\n        Results\n    \"\"\"\n\n    # compute embeddings\n    query_embedding = embedder.encode([flat_query])\n    corpus_embeddings = embedder.encode(corpus_list, batch_size= batch_size,  show_progress_bar=show_progress_bar)\n\n    # compute similarity\n    sim_scores = cosine_similarity(query_embedding[0].reshape(1, -1),\n                              np.array(corpus_embeddings))[0]\n    s_bert_res = sim_scores\n\n    return s_bert_res\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if S_BERT:\n\n    # Try sentence transformers\n\n    search_field = 'title_abstr'\n\n    res_col_name='score_S_Bert'\n    corpus_list = list(ans[search_field])\n    res = search_w_stentence_transformer(embedder, flat_query,\n                                        corpus_list=corpus_list,\n                                       show_progress_bar=True, batch_size=8)\n    orig_col = list(ans.columns)\n    ans[res_col_name] = res\n    # reorder columns\n    ans = ans[[res_col_name] + orig_col].copy()\n    ans = ans.sort_values([res_col_name], ascending=False)\\\n                    .reset_index(drop=True)\n    # print scores\n    compare_scores(ans)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentence-Bert seems indeed to work much better than Biobert. We refine the search by running the queries over the text paragraphs.\n\nWe will rank the papers by the maximum of their paragraph scores.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_paragraph(text, min_words=2):\n    \"\"\"\n    Split the paper text into paragraphs.\n    Parameters\n    ----------\n    text: string\n        paper text\n    min_words: int\n        remove paragraphs with quantity of words <= min words\n    Returns\n    -------\n    split_text_clean: list of string\n        list of paragraphs\n    \"\"\"\n\n    split_text = text.split('\\n\\n')\n    # remove last ''\n    split_text = split_text[:-1]\n    # remove trash\n    split_text_clean = [t for t in split_text if len(t.split()) > min_words]\n\n    return split_text_clean\n\ndef compute_parag_scores(index, parag_list, embedder, flat_query):\n    \"\"\"\n    Compute the similarity scores of Sentence transformer.\n    Parameters\n    ----------\n    index : int\n        row index\n    parag_list : list of list of string\n        list of paragraphs / row\n    embedder : Sentence Transformer model\n        Model to use\n    flat_query : string\n        Query text\n    Returns\n    -------\n    res : matrix of float\n        Similarity scores / paragraphs / row\n    \"\"\"\n\n    parag_paper = parag_list[index]\n    res = search_w_stentence_transformer(embedder, flat_query,\n                                        corpus_list=parag_paper,\n                                       show_progress_bar=False, batch_size=8)\n\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif COMPUTE_PARAGRAPH_SCORE:\n    # compute paragraph scores\n\n    orig_col = list(ans.columns)\n    # compute paragraphs\n    paragraph_mat = list(map(lambda x: split_paragraph(x), list(ans['text'])))\n    # coompute scores\n    res = list(map(lambda x:compute_parag_scores(x, paragraph_mat, embedder, flat_query),\n             trange(len(ans))))\n    max_parag_score = list(map(lambda x: np.max(x), res))\n\n    # store results in dataframe\n    res_col_name = 'score_max_parag'\n    ans[res_col_name] = max_parag_score\n    # reorder columns\n    ans = ans[[res_col_name] + orig_col].copy()\n    ans = ans.sort_values([res_col_name], ascending=False)\\\n                    .reset_index(drop=True)\n\n    compare_scores(ans)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We assess the quality of the embeddings by comparing the embedding similarity results against the BM25 results.\n\nThis makes sense, because BM25 runs over the whole text and we can assume that it is closer to the 'ground truth' than comparing the semantic similarity between abstracts and queries.\n\n**Results:**\n* We can see that giving more information to the network might result in a better quality. But sometimes, the abstract contains more condensed information than separated paragraphs and can be better to work on.\n* Most of the time Sentence Bert also works better. It is more important to use a better sentence representation and a network fine-tuned on a similar task than a network pre-trained on similar data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question Answering\n\nInstead of returning a list of papers, we can directly answer specific questions with a Question Answering system.\n\nWe will now use Bert for question answering, with a version fine-tuned on SQUAD2.\nThe question answering is giving very good results. We choose Squad2 to include the possibility of the absence of answer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper functions for the QA model.\n\nWe process the queries in batch and we also take care that the end of the answer does not precede the start.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\ndef answer_question_batch(question, answer_text, tokenizer, model, squad2=True, tau=0.5, batch_size=4):\n    \"\"\"\n    Search the answer of a question and do the computations in batch.\n    Parameters\n    ----------\n    question : string\n        The natural language question\n    answer_text : list of string\n        The corpus to search in\n    tokenizer : Huggingface tokenizer\n        The tokenizer to use\n    model : Huggingface model\n        The model to use\n    squad2 : boolean\n        True if the model is trained on Squad2, False for Squad 1.\n        In Squad 1, the model presupposes that the answer exists, and might return garbage\n        if there is no answer in the text. For Squad 2, the model also predicts if there is an answer in the text\n    tau : float\n        For squad 2. Buffer used to decide if there is an answer.\n        is_answer_found = score > cls_score + tau, see Bert paper\n    batch_size : int\n        the batch size\n    Returns\n    -------\n    ans_qa_batch : Pandas dataframe\n        Dataframe with the columns\n    \"\"\"\n\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n\n    answer_l = []\n    score_l = []\n\n    all_embeddings = []\n    length_sorted_idx = np.argsort([len(sen) for sen in answer_text])\n\n    # chunks\n    iterator = range(0, len(answer_text), batch_size)\n    iterator = tqdm(iterator, desc=\"Batches\")\n\n    # compute batches\n    for batch_idx in iterator:\n        # process per batch\n        start_scores_l = []\n        end_scores_l = []\n\n        batch_start = batch_idx\n        batch_end = min(batch_start + batch_size, len(answer_text))\n        batch = length_sorted_idx[batch_start: batch_end]\n\n        # compute the longest length in the batch\n        # assume that it is for the last element\n\n        # solve bug in indices for last element\n        if batch_end != len(answer_text):\n            longest_text = answer_text[length_sorted_idx[batch_end]]\n        else:\n            longest_text = answer_text[length_sorted_idx[batch_end - 1]]\n        longest_seq = tokenizer.encode_plus(question,\n                              answer_text[length_sorted_idx[batch_end-1]],\n                              max_length=model.config.max_position_embeddings,\n                              pad_to_max_length=False,\n                              return_attention_masks=True)['input_ids']\n        max_len = len(longest_seq)\n\n\n        token_dict_batch = [tokenizer.encode_plus(question,\n                                                  answer_text[text_id],\n                                                  max_length = min(max_len, model.config.max_position_embeddings),\n                                                  pad_to_max_length=True,\n                                                  return_attention_masks=True)\n                            for text_id in batch]\n\n        input_ids = torch.tensor([token_dict_batch[i]['input_ids'] for i in range(len(token_dict_batch))])\n        token_ids = torch.tensor([token_dict_batch[i]['token_type_ids'] for i in range(len(token_dict_batch))])\n        attention_mask = torch.tensor([token_dict_batch[i]['attention_mask'] for i in range(len(token_dict_batch))])\n\n        # compute scores (as before the softmax)\n        start_scores, end_scores = model(input_ids,\n                                         token_type_ids=token_ids,\n                                         attention_mask=attention_mask)\n\n        # save scores\n        start_scores = start_scores.data.numpy()\n        end_scores = end_scores.data.numpy()\n\n        start_scores_l.append(start_scores)\n        end_scores_l.append(end_scores)\n\n        # reconstruct the answers\n        for i in range(len(start_scores_l[0])):\n            start_scores = start_scores_l[0][i]\n            end_scores = end_scores_l[0][i]\n\n            # get the best start and end score, with start <= end\n\n            # compute the upper triangular matrix of Mij = start_i + end_j\n\n            mat_score = np.tile(start_scores, (len(start_scores), 1)).transpose()\n            mat_score = mat_score + np.tile(end_scores, (len(end_scores), 1))\n            # take the upper triangular matrix to make sure that the end index >= start index\n            mat_score = np.triu(mat_score)\n\n            # find the indices of the maximum\n            arg_max_ind = np.argmax(mat_score.flatten())\n            answer_start = arg_max_ind // len(mat_score)\n            answer_end = arg_max_ind % len(mat_score)\n            assert np.max(mat_score) == mat_score[answer_start, answer_end]\n            score = mat_score.flatten()[arg_max_ind]\n\n            # check if answer is found (score > CLS_score + tau, see paper)\n            # otherwise return no answer\n            if squad2:\n                # check if answer exists\n                cls_score = start_scores[0] + end_scores[0]\n                is_answer_found = score > cls_score + tau\n                # redefine answer\n                score = score if is_answer_found else cls_score\n                answer_start = answer_start if is_answer_found else 0\n                answer_end = answer_end if is_answer_found else 0\n            # stock the answer\n            answer = tokenizer.decode(\n                token_dict_batch[i]['input_ids'][int(answer_start): int(answer_end)+1],\n                skip_special_tokens = False,\n                clean_up_tokenization_spaces = True)\n            answer = answer if answer != '[CLS]' else 'No answer found.'\n            answer_l.append(answer)\n            score_l.append(score)\n\n\n    # create dataframe results\n    ans_qa_batch = pd.DataFrame(zip(score_l, answer_l, length_sorted_idx), columns=['score_qa', 'answer', 'original_idx'])\n    ans_qa_batch['original_idx'] = ans_qa_batch['original_idx'].astype(int)\n    ans_qa_batch = ans_qa_batch.sort_values(['score_qa'], ascending=False) \\\n        .reset_index(drop=True)\n\n    return ans_qa_batch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the QA model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRY QA\n\nif QA:\n\n    # search articles\n\n    # compute answer scores\n\n    answer_text = list(ans['title_abstr'])\n    ans_qa_batch = answer_question_batch(flat_query, answer_text, tokenizer_qa, model_qa, squad2=True, tau=5, batch_size=4)\n\n    # merge qa results\n    # drop qa answer columns to do another search\n    if 'score_qa' in ans.columns:\n        ans = ans.drop(['score_qa', 'answer', 'original_idx'], axis=1)\n    orig_col = list(ans.columns)\n\n    ans['original_idx'] = list(range(len(ans)))\n    ans = ans.merge(ans_qa_batch, on='original_idx')\n    ans = ans[['score_qa', 'answer', 'original_idx'] + orig_col]\n    ans = ans.sort_values(['score_qa'], ascending=False) \\\n        .reset_index(drop=True)\n\n    # remove rows with no answers\n    no_answer = (ans['answer'] == 'No answer found.') | (ans['answer'] == '')\n    ans_clean = ans[~no_answer].copy()\n    ans_clean = ans_clean.reset_index(drop=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_clean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the results of the Question Answering system are very good. They are even more impressive for quantitative questions.\n\nUsing Squad 2 instead of Squad 1 generally results in a poorer performance for simple questions with a clear answer.\nBut allowing the network to decide if there is an answer prevents it from outputting rubbish for difficult questions or when there is no answer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Summarization\n\nLet's now summarize the paper abstracts where an answer was found.\nWe will use the Bart model for text generation.\n\nBart has an encoder-decoder architecture and provides both comparable results as Bert on general NLU tasks and state-of-the-art results on text generation tasks.\n\nSee: [Bart paper](https://arxiv.org/pdf/1910.13461.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Helper function to process the summaries in batches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\nif BART:\n\n    DEFAULT_DEVICE = \"cpu\"\n\n\n    def chunks(lst, n):\n        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n        for i in range(0, len(lst), n):\n            yield lst[i : i + n]\n\n    def generate_summaries(model, tokenizer,\n        df, col_name: str = 'title_abstr', batch_size: int = 8, device: str = DEFAULT_DEVICE,\n        max_length_input: int = 1024):\n\n        \"\"\"\n        Summarize with batch processing.\n\n        Parameters\n        ----------\n        model : Huggingface model\n            The model to use\n        tokenizer : Huggingface tokenizer\n            The tokenizer to use\n        df : pandas dataframe\n            The dataframe containing the paragraph to summarize\n        col_name : string\n            column to summarize\n        batch_size : int\n            the batch size\n        device : str\n            the device to use for running the network\n        max_length_input : int\n            Maximum length of input. Longer input will be truncated.\n\n        Returns\n        -------\n        df : Pandas dataframe\n            Input dataframe with the added columns ['summary']\n        \"\"\"\n\n        examples = df[col_name]\n        summ_l = []\n\n        max_length = 50\n        min_length = 10\n\n        \n        examples = df[col_name]\n        summ_l = []\n\n        max_length = 50\n        min_length = 10\n\n        # choose the batches\n\n        all_embeddings = []\n        length_sorted_idx = np.argsort([len(sen) for sen in list(examples)])\n\n        # chunks\n        iterator = range(0, len(examples), batch_size)\n        iterator = tqdm(iterator, desc=\"Batches\")\n\n        # compute batches\n        for batch_idx in iterator:\n            # process per batch\n\n            batch_start = batch_idx\n            batch_end = min(batch_start + batch_size, len(examples))\n            batch = length_sorted_idx[batch_start: batch_end]\n\n            # compute the longest length in the batch\n            # assume that it is for the last element\n\n            # solve bug in indices for last element\n            if batch_end != len(examples):\n                longest_text = examples[length_sorted_idx[batch_end]]\n            else:\n                longest_text = examples[length_sorted_idx[batch_end - 1]]\n\n            longest_seq_dct = tokenizer.batch_encode_plus([longest_text], return_tensors=\"pt\")\n            max_len = len(longest_seq_dct['input_ids'].squeeze())\n\n            # encode th whole batch\n            dct = tokenizer.batch_encode_plus(examples[batch],\n                                              max_length=min(max_len, max_length_input),\n                                              return_tensors=\"pt\", pad_to_max_length=True)\n            # generate batch summaries\n            summaries = model.generate(\n                input_ids=dct[\"input_ids\"].to(device),\n                attention_mask=dct[\"attention_mask\"].to(device),\n                num_beams=5,\n                temperature=1,\n                length_penalty=5.0,\n                max_length=max_length + 2,  # +2 from original because we start at step=1 and stop before max_length\n                min_length=min_length + 1,  # +1 from original because we start at step=1\n                no_repeat_ngram_size=3,\n                early_stopping=True,\n                decoder_start_token_id=model.config.eos_token_id,\n            )\n            summ = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n\n            # store the results\n            summ_l.append(summ)\n\n        # restore order\n\n        flat_summ_l = [item for sublist in summ_l for item in sublist]\n\n        # create dataframe results\n        summary_batch = pd.DataFrame(zip(flat_summ_l, length_sorted_idx),\n                                    columns=['summary', 'original_idx'])\n        summary_batch['original_idx'] = summary_batch['original_idx'].astype(int)\n        summary_batch = summary_batch.sort_values(['original_idx'], ascending=True) \\\n            .reset_index(drop=True)\n\n        # copy results in input dataframe\n        orig_col = list(df.columns)\n        df['summary'] = summary_batch['summary']\n        # reorder columns\n        df = df[['summary'] + orig_col]\n\n        return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Launch Bart. Use the model trained on extractive summarization tasks (extract the best sentence from the text), because scientific papers contain too many details for an abstractive model. If we tried an abstractive model, it would mix up generated sentences with some textual content from the papers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res = generate_summaries(model_bart, tokenizer_bart, df=ans_clean, batch_size = 4, device= \"cuda\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sum in res['summary']:\n    print('\\n' + sum)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can again see that the results from the summarization network are impressive.\nSummarization together with Question Answering are so powerful that they give more useful results to the user than ranking the papers with embedding similarity.\n\nHowever summarization is very expensive to compute due to the search of the most suitable sentence (beam search).\nIt is thus necessary to use an auxiliary cheaper technique to preselect only a handful of papers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n**BM 25**\n* The standard string search algorithm BM25 is very effective for information retrieval\n* Its speed makes the use of Neural Network only relevant as re-ranker\n\n**Embedding similarity**\n* We don't have any label, so we can only rank the query / papers with neural networks based on their embeddings similarity\n* It is too slow to be run on the whole text. However running it on the abstracts might leave out useful information.\n* Large models like Albert xx large yield good results, yet they are way too slow to be useful\n* A simple Bert model pre-trained on our corpus is better than a larger model trained on regular wikipedia / book texts\n* Adding a more complex similarity matching method as in Sentence Bert is better than computing a simple cosine similarity\n\n**Question Answering**\n* Out-of-the-box models are very powerful\n* This give us much more useful results than returning a list of ranked papers\n* Works better for numerical / closed questions.\n* Allowing the possibility of not finding an answer like the models trained on Squad 2 gives more relevant results\n\n**Summarization**\n* Is very slow and cannot be perfomed on many documents\n* Also gives impressive results and is a lot of fun to try out\n* Extractive summarization models like the ones trained on cnn articles work better than abstractive summarization for our task. A more abstractive model will invent some terms and this is not appropriate to summarize scientific papers.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}