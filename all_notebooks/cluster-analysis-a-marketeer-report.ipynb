{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Clustering Algorithms\n\nThere are many types of clustering algorithms.\n\nMany algorithms use similarity or distance measures between examples in the feature space in an effort to discover dense regions of observations. As such, it is often good practice to scale data prior to using clustering algorithms.\n\nCentral to all of the goals of cluster analysis is the notion of the degree of similarity (or dissimilarity) between the individual objects being clustered. A clustering method attempts to group the objects based on the definition of similarity supplied to it.\n\n\nThe scikit-learn library provides a suite of different clustering algorithms to choose from.\n\nA list of 10 of the more popular algorithms is as follows:\n\nAffinity Propagation\nAgglomerative Clustering\nBIRCH\nDBSCAN\nK-Means\nMini-Batch K-Means\nMean Shift\nOPTICS\nSpectral Clustering\nMixture of Gaussians\nEach algorithm offers a different approach to the challenge of discovering natural groups in data.\n\nThere is no best clustering algorithm, and no easy way to find the best algorithm for your data without using controlled experiments.\n\nSource: https://machinelearningmastery.com/clustering-algorithms-with-python/","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy import unique\nfrom numpy import where\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"CustomerID\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Encode the Categorical Feature ie. Gender**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nGender_ec = encoder.fit_transform(df.iloc[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Gender\"] = Gender_ec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scaling**\n\nWhen you’re working with a learning model, it is important to scale the features to a range which is centered around zero. This is done so that the variance of the features are in the same range. If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset, which is not something we want happening in our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaled = scaler.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(data = scaled, columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA for Dimensionality Reduction**\n\nGiven a collection of points in two, three, or higher dimensional space, a \"best fitting\" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 2)\ndf2 = pca.fit_transform(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df2[:, 0], df2[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KMeans Clustering and Marketeer Report**\n\nThe KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of  samples  into  disjoint clusters , each described by the mean \n of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from , although they live in the same space.\n\nThe K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion:\n\n \n \nInertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n\nInertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n\nInertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters = 5)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of KMeans cluster\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.labels_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model label tells which cluster is assigned to the object","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nice Pythonic way to get the indices of the points for each corresponding cluster\nmydict = {i: np.where(model.labels_ == i)[0] for i in range(model.n_clusters)}\n\n# Transform this dictionary into list (if you need a list as result)\ndictlist = []\nfor key, value in mydict.items():\n    temp = [key, value]\n    dictlist.append(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This list contains indices of objects in the cluster\ndictlist[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To get the array of our original encoded dataset \ndf3 = df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To get items from the original dataset\naccessed_mapping = map(df3.__getitem__, dictlist[0])\ncl1 = list(accessed_mapping)\ncluster_1 = pd.DataFrame(cl1[1], columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])\n\naccessed_mapping = map(df3.__getitem__, dictlist[1])\ncl2 = list(accessed_mapping)\ncluster_2 = pd.DataFrame(cl2[1], columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])\n\naccessed_mapping = map(df3.__getitem__, dictlist[2])\ncl3 = list(accessed_mapping)\ncluster_3 = pd.DataFrame(cl3[1], columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])\n\naccessed_mapping = map(df3.__getitem__, dictlist[3])\ncl4 = list(accessed_mapping)\ncluster_4 = pd.DataFrame(cl4[1], columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])\n\naccessed_mapping = map(df3.__getitem__, dictlist[4])\ncl5 = list(accessed_mapping)\ncluster_5 = pd.DataFrame(cl5[1], columns = [\"Gender\", \"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The objects in clusters\ncluster_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Final Report of Cluster 1\n\n\nprint(\"*\" * 75)\nprint(\"The Average age of Customers in cluster 1 is:\")\nprint(cluster_1.Age.mean())\nprint(\"*\" * 75)\nprint(\"The Number of Male(1) and female(0) customers in cluster 1 are:\")\nprint(cluster_1[\"Gender\"].value_counts())\nprint(\"*\" * 75)\nprint(\"The Average annual income (in dollars) of Customers in category 1 is:\")\nprint(cluster_1[\"Annual Income (k$)\"].mean())\nprint(\"*\" * 75)\nprint(\"The Mean,Median and Mode of spending Score of people in category 1 is:\")\nprint(cluster_1[\"Spending Score (1-100)\"].mode())\nprint(\"*\" * 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Final Report of Cluster 2\n\nprint(\"*\" * 75)\nprint(\"The Average age of Customers in cluster 2 is:\")\nprint(cluster_2.Age.mean())\nprint(\"*\" * 75)\nprint(\"The Number of Male(1) and female(0) customers in cluster 2 are:\")\nprint(cluster_2[\"Gender\"].value_counts())\nprint(\"*\" * 75)\nprint(\"The Average annual income (in dollars) of Customers in category 2 is:\")\nprint(cluster_2[\"Annual Income (k$)\"].mean())\nprint(\"*\" * 75)\nprint(\"The Mean,Median and Mode of spending Score of people in category 2 is:\")\nprint(cluster_2[\"Spending Score (1-100)\"].mode())\nprint(\"*\" * 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Final Report of Cluster 3\n\nprint(\"*\" * 75)\nprint(\"The Average age of Customers in cluster 3 is:\")\nprint(cluster_3.Age.mean())\nprint(\"*\" * 75)\nprint(\"The Number of Male(1) and female(0) customers in cluster 3 are:\")\nprint(cluster_3[\"Gender\"].value_counts())\nprint(\"*\" * 75)\nprint(\"The Average annual income (in dollars) of Customers in category 3 is:\")\nprint(cluster_3[\"Annual Income (k$)\"].mean())\nprint(\"*\" * 75)\nprint(\"The Mean,Median and Mode of spending Score of people in category 3 is:\")\nprint(cluster_3[\"Spending Score (1-100)\"].mode())\nprint(\"*\" * 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Final Report of Cluster 4\n\nprint(\"*\" * 75)\nprint(\"The Average age of Customers in cluster 4 is:\")\nprint(cluster_4.Age.mean())\nprint(\"*\" * 75)\nprint(\"The Number of Male(1) and female(0) customers in cluster 4 are:\")\nprint(cluster_4[\"Gender\"].value_counts())\nprint(\"*\" * 75)\nprint(\"The Average annual income (in dollars) of Customers in category 4 is:\")\nprint(cluster_4[\"Annual Income (k$)\"].mean())\nprint(\"*\" * 75)\nprint(\"The Mean,Median and Mode of spending Score of people in category 4 is:\")\nprint(cluster_4[\"Spending Score (1-100)\"].mode())\nprint(\"*\" * 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Final Report of Cluster 5\n\nprint(\"*\" * 75)\nprint(\"The Average age of Customers in cluster 5 is:\")\nprint(cluster_5.Age.mean())\nprint(\"*\" * 75)\nprint(\"The Number of Male(1) and female(0) customers in cluster 5 are:\")\nprint(cluster_5[\"Gender\"].value_counts())\nprint(\"*\" * 75)\nprint(\"The Average annual income (in dollars) of Customers in category 5 is:\")\nprint(cluster_5[\"Annual Income (k$)\"].mean())\nprint(\"*\" * 75)\nprint(\"The Mean,Median and Mode of spending Score of people in category 5 is:\")\nprint(cluster_5[\"Spending Score (1-100)\"].mode())\nprint(\"*\" * 75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8 other Clustering Algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Affinity Propagation**\n\nAffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.\n\n\nAffinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.\n\nThe main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \n, where  is the number of samples and  is the number of iterations until convergence. Further, the memory complexity is of the order \n if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AffinityPropagation\nmodel = AffinityPropagation(damping=0.9)\nmodel.fit(df2)\nyhat = model.predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    #plt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], marker = '+', label='Clusters', c = \"red\")\n    plt.title(\"Sklearn version of Affinity Propagation\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method isn't really good for our dataset because the clusters are not distinct. also if you run labels code for this model the negative value will indicate outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Agglomerative Hierarchical Clustering**\n\nThe agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster. Next, pairs of clusters are successively merged until all clusters have been merged into one big cluster containing all objects. The result is a tree-based representation of the objects, named dendrogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nmodel = AgglomerativeClustering(n_clusters = 5)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of Agglomerative Clustering\")\n    plt.style.use('fivethirtyeight')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BIRCH**\n\nThe Birch builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\n\nThe CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:\n\nNumber of samples in a subcluster.\n\nLinear Sum - A n-dimensional vector holding the sum of all samples\n\nSquared Sum - Sum of the squared L2 norm of all samples.\n\nCentroids - To avoid recalculation linear sum / n_samples.\n\nSquared norm of the centroids.\n\nThe Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.\n\nThis algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import Birch\nmodel = Birch(threshold=0.01, n_clusters=5)\nmodel.fit(df2)\nyhat = model.predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n##plt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], marker = '+', label='Clusters', c = \"red\")\n    plt.title(\"Sklearn version of BIRCH\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DBSCAN**\n\nThe DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nmodel = DBSCAN(eps=0.30)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of DBSCAN\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This algorithm is not suitable for our dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**MiniBatch Clustering**\n\nMini Batch K-means algorithm‘s main idea is to use small random batches of data of a fixed size, so they can be stored in memory. Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence. Each mini batch updates the clusters using a convex combination of the values of the prototypes and the data, applying a learning rate that decreases with the number of iterations. This learning rate is the inverse of the number of data assigned to a cluster during the process. As the number of iterations increases, the effect of new data is reduced, so convergence can be detected when no changes in the clusters occur in several consecutive iterations.\nThe empirical results suggest that it can obtain a substantial saving of computational time at the expense of some loss of cluster quality, but not extensive study of the algorithm has been done to measure how the characteristics of the datasets, such as the number of clusters or its size, affect the partition quality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nmodel = MiniBatchKMeans(n_clusters=5)\nmodel.fit(df2)\nyhat = model.predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n##plt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], marker = '+', label='Clusters', c = \"red\")\n    plt.title(\"Sklearn version of Mini Batch Means\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OPTICS**\n\nThe OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a reachability_ distance, and a spot within the cluster ordering_ attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for max_eps, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given eps value using the cluster_optics_dbscan method. Setting max_eps to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import OPTICS\n\nmodel = OPTICS(eps=0.8)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of optics clustering\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is by far worst suiting algorithm on our dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Spectral Clustering**\n\nFor two clusters, SpectralClustering solves a convex relaxation of the normalised cuts problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\n\nmodel = SpectralClustering(n_clusters = 5)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of Spectral clustering\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GMM**\n\nGaussian mixture models (GMMs) are often used for data clustering. You can use GMMs to perform either hard clustering or soft clustering on query data.\n\nTo perform hard clustering, the GMM assigns query data points to the multivariate normal components that maximize the component posterior probability, given the data. That is, given a fitted GMM, cluster assigns query data to the component yielding the highest posterior probability. Hard clustering assigns a data point to exactly one cluster. For an example showing how to fit a GMM to data, cluster using the fitted model, and estimate component posterior probabilities, see Cluster Gaussian Mixture Data Using Hard Clustering.\n\nAdditionally, you can use a GMM to perform a more flexible clustering on data, referred to as soft (or fuzzy) clustering. Soft clustering methods assign a score to a data point for each cluster. The value of the score indicates the association strength of the data point to the cluster. As opposed to hard clustering methods, soft clustering methods are flexible because they can assign a data point to more than one cluster. When you perform GMM clustering, the score is the posterior probability. For an example of soft clustering with a GMM, see Cluster Gaussian Mixture Data Using Soft Clustering.\n\nGMM clustering can accommodate clusters that have different sizes and correlation structures within them. Therefore, in certain applications,, GMM clustering can be more appropriate than methods such as k-means clustering. Like many clustering methods, GMM clustering requires you to specify the number of clusters before fitting the model. The number of clusters specifies the number of components in the GMM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nmodel = GaussianMixture(n_components = 5)\nyhat = model.fit_predict(df2)\nclusters = unique(yhat)\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(df2[row_ix, 0], df2[row_ix, 1])\n    plt.title(\"Sklearn version of Gaussian Mixture\")\n    plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These algorithms can be used as per what fits your data best. You can create the report of cluster as shown after the KMeans Cluster. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}