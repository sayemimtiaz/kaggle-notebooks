{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MODEL_NAME = 'sentiment'\n\nACTIVATION = 'sigmoid'\nBATCH_SIZE = 32\n\nCLASSES = ['negative', 'positive']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stockmarket-sentiment-dataset/stock_data.csv')\nprint(df.shape)\nprint(df.columns)\n# df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Preprocess Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tanh_to_sigmoid(x):\n    return 0 if x == -1 else 1\n\n\ndf['Y'] = df['Sentiment'].apply(lambda x: tanh_to_sigmoid(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.02, random_state=1)\n\nX_train, X_test = df_train['Text'].values, df_test['Text'].values\nY_train, Y_test = np.asarray(df_train['Y'].tolist()), np.asarray(df_test['Y'].tolist())\n\nprint('X:', X_train.shape, X_test.shape)\nprint(X_train[0])\nprint('Y:', Y_train.shape, Y_test.shape)\nprint(Y_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install tensorflow_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nimport tensorflow_text\n\n# Enable to modify cache location with PATH variable: TFHUB_CACHE_DIR=/Users/Cache\nuse_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\", input_shape=[], dtype=tf.string, trainable=False)\n\n\nclass PreprocessText(layers.Layer):\n    def __init__(self, name=\"preprocess_text\", **kwargs):\n        super(PreprocessText, self).__init__(name=name, **kwargs)\n\n    def call(self, input):\n        data_preprocessor.process_text(input)\n        return self.preprocess(input)\n\n\n\ndef create_model(out_shape, activation='softmax'):\n    # Functional\n    inputs = layers.Input(shape=(1,), dtype=tf.string)\n    \n    X = use_layer(tf.squeeze(tf.cast(inputs, tf.string)))\n    X = layers.Dense(512, activation='relu')(X)\n    X = layers.Dropout(0.3)(X)\n    X = layers.Dense(512, activation='relu')(X)\n    X = layers.Dropout(0.3)(X)\n\n    outputs = layers.Dense(out_shape, activation=activation)(X)\n\n    model = Model(inputs=inputs, outputs=outputs) \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(Y_train.shape[1] if ACTIVATION == 'softmax' else 1, activation=ACTIVATION)\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = Adam(lr=3e-4, decay=1e-6, beta_1=0.9, beta_2=0.999), \n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BEST_WEIGHTS = (f'{MODEL_NAME}_use_best_weights.hdf5')\nMODEL_FILE = (f'{MODEL_NAME}_use_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\ncheckpoint = ModelCheckpoint(filepath=BEST_WEIGHTS, verbose=1, save_best_only=True)   # Save the best model\ntensorboard = TensorBoard(log_dir='logs/{} - {}'.format(MODEL_NAME, datetime.now().strftime('%Y%m%d%H%M%S')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(\n    X_train,\n    Y_train,\n    batch_size = BATCH_SIZE, \n    epochs = 30, \n    validation_data=(X_test, Y_test),\n    validation_split=0.02,\n    callbacks = [monitor, checkpoint, tensorboard],\n    shuffle=True,\n    verbose=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(BEST_WEIGHTS)\nmodel.save(MODEL_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_train_history(history):\n    # plot the cost and accuracy \n    loss_list = history['loss']\n    val_loss_list = history['val_loss']\n    accuracy_list = history['accuracy']\n    val_accuracy_list = history['val_accuracy']\n    # epochs = range(len(loss_list))\n\n    # plot the cost\n    plt.plot(loss_list, 'b', label='Training cost')\n    plt.plot(val_loss_list, 'r', label='Validation cost')\n    plt.ylabel('cost')\n    plt.xlabel('iterations')\n    plt.title('Training and validation cost')\n    plt.legend()\n    \n    plt.figure()\n    \n    # plot the accuracy\n    plt.plot(accuracy_list, 'b', label='Training accuracy')\n    plt.plot(val_accuracy_list, 'r', label='Validation accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('iterations')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n\nplot_train_history(hist.history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test)\n\nprint (\"Test Loss = \" + str(score[0]))\nprint (\"Test Accuracy = \" + str(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test_pred = model.predict(X_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get threshold is binary classifation\n\nif ACTIVATION == 'sigmoid':\n    from sklearn.metrics import roc_curve, auc\n\n    def calculate_optimal_threshold(Y, Y_pred):\n        # ROC Curve\n        fpr, tpr, thresholds = roc_curve(Y, Y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC Curve')\n        plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n        plt.xlim([-0.025, 1.025])\n        plt.ylim([-0.025, 1.025])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('RoC Curve')\n        print(\"AUC: \", roc_auc)\n\n        # Calculate the optimal threshold\n        i = np.arange(len(tpr)) # index for df\n        roc_df = pd.DataFrame({'threshold' : pd.Series(thresholds, index = i), \n                               'fpr': pd.Series(fpr, index=i), \n                               '1-fpr' : pd.Series(1-fpr, index = i), \n                               'tpr': pd.Series(tpr, index = i), \n                               'diff': pd.Series(tpr - (1-fpr), index = i) })\n        opt_threshold = roc_df.iloc[roc_df['diff'].abs().argsort()[:1]]\n        print(opt_threshold)\n\n        return opt_threshold['threshold'].values[0]\n\n\n    threshold = calculate_optimal_threshold(Y_test, Y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, precision_score, recall_score, classification_report\n\ndef analyze(Y, Y_pred, classes, activation=\"softmax\"):\n    if activation == \"sigmoid\":\n        Y_cls = Y\n        Y_pred_cls = (Y_pred > threshold).astype(float)\n    elif activation == \"softmax\":\n        Y_cls = np.argmax(Y, axis=1)\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    \n    accuracy = accuracy_score(Y_cls, Y_pred_cls)\n    print(\"Accuracy score: {}\\n\".format(accuracy))\n    \n    \n    rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n    print(\"RMSE score: {}\\n\".format(rmse))\n\n    \n    # plot Confusion Matrix\n    print(\"Confusion Matrix:\")\n    cm = confusion_matrix(Y_cls, Y_pred_cls)\n    print(cm)\n    # Plot the confusion matrix as an image.\n    plt.matshow(cm)\n    # Make various adjustments to the plot.\n    num_classes = len(classes)\n    plt.colorbar()\n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, range(num_classes))\n    plt.yticks(tick_marks, range(num_classes))\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    \n    \n    # plot Classification Report\n    print(\"Classification Report:\")\n    print(classification_report(Y_cls, Y_pred_cls, target_names=classes))\n\n\n\nanalyze(Y_test, Y_test_pred, CLASSES, ACTIVATION)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show Mislabeled","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_mislabeled(X, Y, Y_pred, classes, activation=\"softmax\", num_show = None):\n    num_col = 5\n    \n    if activation == \"sigmoid\":\n        Y_cls = Y\n        Y_pred_cls = np.squeeze((Y_pred > threshold).astype(float))\n    elif activation == \"softmax\":\n        Y_cls = np.argmax(Y, axis=1)\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    mislabeled_indices = np.where(Y_cls != Y_pred_cls)[0]\n    print(f'{len(mislabeled_indices)} mislabeled\\n')\n    \n    if num_show is None or num_show > len(mislabeled_indices):\n        num_show = len(mislabeled_indices)\n        \n    for i, index in enumerate(mislabeled_indices[:num_show]):   \n        print(\"{}\\nPrediction: {}\\nLabel: {}\\n\\n\".format(X[index], \n                                                         classes[int(Y_pred_cls[index])], \n                                                         classes[int(Y_cls[index])]))\n\n\n\nshow_mislabeled(X_test, Y_test, Y_test_pred, CLASSES, ACTIVATION, 10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}