{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Classification using TensorFlow v2.\n","metadata":{"id":"fpRvMsHUNGEq"}},{"cell_type":"markdown","source":"\nThis tutorial will explain the complete pipeline from loading data from several sources to predicting results. This is a tutorial to build an image classification model from scratch using TensorFlow v2. \n\nThis tutorial will explain how to use GPU efficiently, load image data, build and train a convolution neural network... Data augmentation is  included in the model.\n\nMake sure to change the Accelerator on the right to GPU.","metadata":{"id":"olAK3tYzZ0QB"}},{"cell_type":"markdown","source":"The **objectives** of this tutorial will be:\n* The tutorial is intended to be a first contact with the RoCoLe dataset.\n* Create an input pipeline from different input sources, images and Excel file, using TensorFlow tools.\n* Use a predefined model as feature extractor(ResNet101V2 & MobileNetV2) and own binary classifier.\n* Diagnose deep learning model performance using learning curves.","metadata":{"id":"BflCT_ZI8ZzC"}},{"cell_type":"markdown","source":"# 1.Installs","metadata":{"id":"xK9lt3m67xYX"}},{"cell_type":"code","source":"!pip install xlrd>=1.3.0\n!pip install openpyxl","metadata":{"id":"a2BXEQ3qZ0QC","outputId":"1c9ae930-4974-42a8-f60f-fa8d4953f7cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.Imports","metadata":{"id":"eyx7MWteZ0QD"}},{"cell_type":"code","source":"try:\n    %tensorflow_version 2.x\n\nexcept Exception:\n    pass\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Flatten\nimport tensorflow_datasets as tfds\nimport tensorflow_hub as hub\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib\n\nimport itertools\n\nprint('Tensorflow version : {}'.format(tf.__version__))\n\ntf.keras.backend.clear_session()\ntf.executing_eagerly()==True","metadata":{"id":"c7Xa8csHjR1f","outputId":"2a628a7d-a635-4380-c3d9-40f97f207031","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.Selecting Between Strategies","metadata":{"id":"XgLU04x0-cjG"}},{"cell_type":"markdown","source":"## 3.1.CPU or GPU detection\n\nDepending on the hardware available, we will use different distribution strategies. For this version we will use GPU or CPU. \n- If more than one GPU is available, then you'll use the Mirrored Strategy\n- If one GPU is available or if just the CPU is available, you'll use the default strategy.","metadata":{"id":"kf809Imm-kQS"}},{"cell_type":"code","source":"# Detect hardware\ntry:\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\nexcept ValueError:\n    gpu = 0\n    \n# Select appropriate distribution strategy\nif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"id":"6D_hKiRC-oDl","outputId":"eb25b326-5243-4547-abe6-6e5247f0ba0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.Global Parameters\n\nThe global BATCH_SIZE variable is the batch size per replica times the number of replicas in the distribution strategy.","metadata":{"id":"g6Zy7vBI-sev"}},{"cell_type":"code","source":"BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.","metadata":{"id":"KdPhHTdY-qk4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nIMAGE_SIZE=224\nCLASS_NAMES=[]\nIMG_COUNT=0\nnum_classes=0\n\nMAIN_IMG_DIR = '../input/rocoleoriginal/Photos'\nMAIN_ANN_DIR = '../input/rocoleoriginal/Annotations'\n","metadata":{"id":"X6INNXdmZ0QG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.Loading and Preprocessing the Dataset","metadata":{"id":"CbOVnEF09nJ3"}},{"cell_type":"markdown","source":"## 5.1 RoCoLe: A robusta coffee leaf images dataset","metadata":{"id":"HQJQaQGokVta"}},{"cell_type":"markdown","source":" **RoCoLe**\n\nhttps://data.mendeley.com/datasets/c5yvn32dzg/2\n\nThere are two main directories:\n\n\n*   Photos: contains 1560 coffee leaf images (.jpg)\n*   Annotations:  Segmentation, Classification etc.\n\n\n","metadata":{"id":"eVvJvDTf8WFu"}},{"cell_type":"code","source":"data_dir = pathlib.Path(MAIN_IMG_DIR)\nprint(\"Directory:\", data_dir)\nIMG_COUNT = len(list(data_dir.glob('*.jpg')))\nprint(\"Number of images:\",IMG_COUNT)","metadata":{"id":"9OKK5i0rDT_r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see it is a small dataset composed by 1560 pictures of coffee leaf images. Beause of that, we are going  to perform a binary classification.\nAlso, the data is divided in several inputs : \n* Images files  \n* xlsx files.","metadata":{"id":"97P5QLX0Z0QI"}},{"cell_type":"code","source":"#We need to use the file where classes are described:\n!ls ../input/rocoleoriginal/Annotations","metadata":{"id":"GlIiRqPeEL9k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We cannot created a tf.data.Dataset from image files in a directory since the information is separated in two different directories and files. \n\nTherefore, we will create a dataset containing location and tagging information using Pandas. Then, we will convert it into a shuffle dataset (from tensor slices). Finally we will create the dataset with the processed images.","metadata":{"id":"dVd5mYO7BtsW"}},{"cell_type":"markdown","source":"## 5.1. Explore data","metadata":{"id":"ZFqH6N1RaETt"}},{"cell_type":"markdown","source":"We want to perform a binary classification, so we are going to drop unnecessary information. As we can see, Multiclass label shall be removed.","metadata":{"id":"lpYMGDUvAn8u"}},{"cell_type":"code","source":"df = pd.read_excel(MAIN_ANN_DIR+'/RoCoLe-classes.xlsx',engine='openpyxl')\ndf.head()","metadata":{"id":"eMxLtQ1kAl0q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Multiclass.Label'], axis=1, inplace=True)\ndf.rename(columns={'Binary.Label':'Label'},inplace=True)","metadata":{"id":"5i_ad96wQY5K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the following cell to see how many healthy/unhealthy pictures we have .","metadata":{"id":"e4G8dfyiZ0QJ"}},{"cell_type":"code","source":"#How many Labels has Multiclass.Label? R: 6\n#How many Labels has Binary.Label? R: 2\nnum_classes=df.drop_duplicates(subset = [\"Label\"]).count()[\"Label\"]\nprint(\"Number of Labels(in Binary.Label  column): \"+ str(num_classes))\ndf.drop_duplicates(subset = [\"Label\"])\n\ncheck_imbalance = df.pivot_table(index=['Label'], aggfunc='size')\nprint (check_imbalance)\n","metadata":{"id":"a68zSNJ_Z0QJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that the there are a little more images that are classified as healthy than unhealthy. However, this not shows that we have big imbalance in our data, because the diference between healthy and unhealthy is only 22 images.","metadata":{"id":"vf06kynmZ0QJ"}},{"cell_type":"markdown","source":"Let's change the Label column from string to integer. Also, we are going to create a dictionary of Words, we will use this latter.","metadata":{"id":"6DEXKzkCZ0QK"}},{"cell_type":"code","source":"CLASS_NAMES = (pd.Series.to_string(df.drop_duplicates(subset = [\"Label\"])[\"Label\"],index=False).strip()).split()\nCLASS_NAMES.sort()\nprint(CLASS_NAMES)","metadata":{"id":"sR-s2fVHpgkc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictOfWords = { CLASS_NAMES[i]:i for i in range(0, len(CLASS_NAMES))}\ndictOfWords","metadata":{"id":"YhB15cSuPqWJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Label\"] = df[\"Label\"].map(dictOfWords)\ndf","metadata":{"id":"IQWs8zzkRHmr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2. Process Data","metadata":{"id":"D-V0IzZaCayE"}},{"cell_type":"markdown","source":"### 5.2.1.Load Pandas Dataframe as ShuffleDataset\n\nDefine 2 dataset:\n\n* train_ds = the training set, 80%.\n* val_ds    = the validation set, 20%.\n\nTest dataset is not available because we are going to use the 20% percent to create the validation dataset.\n","metadata":{"id":"K2dZ_XNsZ0QL"}},{"cell_type":"code","source":"target = df.pop('Label')\ndir_img=MAIN_IMG_DIR+'/{}'\ndf=df.applymap(dir_img.format)\n\n\ndataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\ndataset = dataset.shuffle(IMG_COUNT, reshuffle_each_iteration=False)\n\nval_size = int(IMG_COUNT * 0.20)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(int(val_size))\n\n\ntrain_size=tf.data.experimental.cardinality(train_ds).numpy()\nval_size=tf.data.experimental.cardinality(val_ds).numpy()\n\nprint(\"Training size: {}\".format(train_size))\nprint(\"Validation size: {}\".format(val_size))\nprint(\"Total images: \",train_size+val_size)\nprint(\"\\nExample: \")\nfor feat, targ in train_ds.take(5):\n    print ('Image: {}, Label: {}'.format(feat, targ))","metadata":{"id":"3lXFk8F9Z0QL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.2.Configure dataset and Performance\nDefine some helper functions that will pre-process our data:\n\n* parse_image: load,decode,convert...an image.\n* get_training_dataset: loads data and splits it to get the training set.\n* get_validation_dataset: loads and splits the data to get the validation set.","metadata":{"id":"Ieu2SRHxZ0QL"}},{"cell_type":"code","source":"'''\nTransforms each image in dataset \n'''\n\ndef parse_image(feat, targ):\n\n    image = tf.io.read_file(feat[0])\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    # image pretreatment \n    image = tf.image.resize(image, [1512, 1512])\n    image =  tf.image.central_crop(image, central_fraction=0.65)\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE]) \n  \n    label= tf.cast(targ,tf.int32)\n\n    return image, label\n'''\nLoads and maps the training split of the dataset using the map function. \n'''\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef get_training_dataset():\n\n      with  strategy.scope():\n        dataset = train_ds.map(parse_image, num_parallel_calls=16)\n        dataset = dataset.shuffle(buffer_size=train_size,\n                                  reshuffle_each_iteration=True)\n        dataset = dataset.repeat() \n        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n        dataset = dataset.prefetch(AUTOTUNE) \n        return dataset\n\n'''\nLoads and maps the validation split of the dataset using the map function. \n'''  \ndef get_validation_dataset():\n\n    dataset = val_ds.map(parse_image, num_parallel_calls=16)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n    dataset = dataset.shuffle(buffer_size=train_size, \n                              reshuffle_each_iteration=True)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(AUTOTUNE) \n    return dataset\n\n# instantiate the datasets\nwith strategy.scope():\n    training_dataset = get_training_dataset()\n    validation_dataset = get_validation_dataset()","metadata":{"id":"wgF-OEMrZ0QL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.3.Visualize","metadata":{"id":"b4iNxQEaZ0QM"}},{"cell_type":"code","source":"image_batch, label_batch = next(iter(training_dataset))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i])\n    label = label_batch[i]\n    #index = tf.argmax(label.numpy(), axis=0)\n    plt.title(CLASS_NAMES[label])\n","metadata":{"id":"KOD1Zsst9x0p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.Models","metadata":{"id":"iwnwoy9DZ0QM"}},{"cell_type":"markdown","source":"We will use two predefined models, which we will use the weights provided by imagenet. ","metadata":{"id":"mWccxZh0ATFy"}},{"cell_type":"markdown","source":"## 6.1.Utilities and model variables","metadata":{"id":"ZuEBc09HqF_C"}},{"cell_type":"code","source":"#Training variables\nEPOCHS = 100\nsteps_per_epoch = (train_size)//BATCH_SIZE \nvalidation_steps = (val_size)//BATCH_SIZE \nIMG_DIM=(IMAGE_SIZE,IMAGE_SIZE)","metadata":{"id":"VqrxACPyZ0QM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Callback functions\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=20,\n                                                  restore_best_weights=True)\n\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 20)\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)","metadata":{"id":"igUM8JBBZ0QN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Training and Validation Accuracy\ndef plot_history(hist,Text):\n  history=hist\n  acc = history.history[\"accuracy\"]\n  val_acc = history.history[\"val_accuracy\"]\n\n  loss = history.history[\"loss\"]\n  val_loss = history.history[\"val_loss\"]\n\n  epochs_range = range(len(history.epoch))\n\n  plt.figure(figsize=(10,10))\n  plt.subplot(1,2,1)\n  plt.plot(epochs_range,acc,label=\"Train Accuracy\")\n  plt.plot(epochs_range,val_acc,label=\"Validation Accuracy\")\n  plt.legend(loc = 'lower right')\n  plt.title(\"Accuracy\")\n  plt.subplot(1,2,2)\n  plt.plot(epochs_range,loss,label=\"Train Loss\")\n  plt.plot(epochs_range,val_loss,label=\"Validation Loss\")\n  plt.legend(loc = 'upper right')\n  plt.title(\"Loss\")\n  plt.suptitle(Text)","metadata":{"id":"DXnp2_4PAw0r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2.ResNet101V2","metadata":{"id":"YWHSTbXOZ0QN"}},{"cell_type":"markdown","source":"Our model has the following sequential structure:\n\nData augmentation -> Feature extractor (resnet) -> Binary clasification","metadata":{"id":"iQCt0DmGXOkK"}},{"cell_type":"markdown","source":"### 6.2.1.Define model","metadata":{"id":"Q18OCbu3alOg"}},{"cell_type":"code","source":"IMG_DIM=(IMAGE_SIZE,IMAGE_SIZE)\ndef make_model():\n    data_augmentation = tf.keras.Sequential([\n    tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),  # 128x128x3\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(IMAGE_SIZE, \n                                                              IMAGE_SIZE,\n                                                              3)),\n     tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\", \n                                                 input_shape=(IMAGE_SIZE, \n                                                              IMAGE_SIZE,\n                                                              3)),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.5),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(0.80)\n    ])\n    \n    base_model = tf.keras.applications.ResNet101V2(input_shape=(*IMG_DIM, 3),\n                                             include_top=False,\n                                             weights= 'imagenet')\n    base_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        data_augmentation, \n        base_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1024, activation=\"relu\"),\n         tf.keras.layers.Dropout(0.4),\n         tf.keras.layers.Dense(512, activation=\"relu\" ),\n         tf.keras.layers.Dropout(0.4),\n         tf.keras.layers.BatchNormalization(),\n         tf.keras.layers.Dense(num_classes-1,activation=\"sigmoid\")\n])\n        \n    return model","metadata":{"id":"hRwyGyJeZ0QN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2.2.Compile model","metadata":{"id":"Wah2riOea44B"}},{"cell_type":"code","source":"with strategy.scope():\n    model = make_model()\n    model.compile(loss=\"binary_crossentropy\",\n    optimizer='adam',\n    metrics=[\"accuracy\"])","metadata":{"id":"5MFZw_TmZ0QN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"xI8w5IoFCXRM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2.3.Train model","metadata":{"id":"6HM5ln_yao7_"}},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"RN101v2.h5\",\n                                                save_best_only=True)\nhistory1 = model.fit(training_dataset,steps_per_epoch=steps_per_epoch, epochs=EPOCHS, \n                    validation_data = validation_dataset, validation_steps=validation_steps, \n                    batch_size=BATCH_SIZE, \n                     callbacks=[checkpoint, early_stopping,lr_scheduler])","metadata":{"id":"w8PuPtdJZ0QN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2.4.Plot results","metadata":{"id":"grkEz0JVbH85"}},{"cell_type":"code","source":"# Plotting Training and Validation Accuracy\nplot_history(history1,\"ResNet101V2\")\n","metadata":{"id":"ShXPLUBwZ0QO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3.MobileNetV2","metadata":{"id":"JY5paCYc_0D0"}},{"cell_type":"markdown","source":"Our model has the following sequential structure:\n\nData augmentation -> Feature extractor (MobileNetV2) -> Binary clasification","metadata":{"id":"_9kF2pUf_0D7"}},{"cell_type":"markdown","source":"### 6.3.1.Define model","metadata":{"id":"e4_WTRWJ_0D8"}},{"cell_type":"code","source":"\ndef make_model():\n    data_augmentation = tf.keras.Sequential([\n    tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),  # 128x128x3\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(IMAGE_SIZE, \n                                                              IMAGE_SIZE,\n                                                              3)),\n     tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\", \n                                                 input_shape=(IMAGE_SIZE, \n                                                              IMAGE_SIZE,\n                                                              3)),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.5),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(0.80)\n    ])\n    \n    base_model = tf.keras.applications.MobileNetV2(input_shape=(*IMG_DIM, 3),\n                                             include_top=False,\n                                             weights= 'imagenet')\n    base_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        data_augmentation, \n        base_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1024, activation=\"relu\"),\n         tf.keras.layers.Dropout(0.4),\n         tf.keras.layers.Dense(512, activation=\"relu\" ),\n         tf.keras.layers.Dropout(0.4),\n         tf.keras.layers.BatchNormalization(),\n         tf.keras.layers.Dense(num_classes-1,activation=\"sigmoid\")\n])\n        \n    return model","metadata":{"id":"BTGHNcz-_0D8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3.2.Compile model","metadata":{"id":"A6FKMEd2_0D8"}},{"cell_type":"code","source":"with strategy.scope():\n    model2 = make_model()\n    model2.compile(loss=\"binary_crossentropy\",\n    optimizer='adam',\n    metrics=[\"accuracy\"])","metadata":{"id":"kITJfzKu_0D8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.summary()","metadata":{"id":"JF-ZDQuwEn_v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3.3.Train model","metadata":{"id":"yakMawpj_0D9"}},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"MNetV2.h5\",\n                                                save_best_only=True)\nhistory2 = model2.fit(training_dataset,steps_per_epoch=steps_per_epoch, epochs=EPOCHS, \n                    validation_data = validation_dataset, validation_steps=validation_steps, \n                    batch_size=BATCH_SIZE, \n                     callbacks=[checkpoint, early_stopping,lr_scheduler])","metadata":{"id":"rrJiu9cg_0D9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3.4.Plot results","metadata":{"id":"8JduDHat_0D9"}},{"cell_type":"code","source":"# Plotting Training and Validation Accuracy\nplot_history(history2,\"MobileNetV2\")","metadata":{"id":"viQCu6Dx_0D-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.Evaluation and conclusions","metadata":{"id":"-r8vFK6E-AuY"}},{"cell_type":"markdown","source":"\n\"*When the learning curve for training loss that shows improvement and the learning curve for validation loss also shows improvement, but a large gap remains between both curves, we can conclude that we have an unrepresentative training dataset.* \n\n...\n\n*Also,when the learning curve for training looks that achieves a good fit and the learning curve for validation loss shows noisy movements around the training loss or the validation loss is inferior than training loss, we can conclude that we have an unrepresentative validation dataset*\". \n\n...\n\n*This may occur if the training dataset has too few examples as compared to the validation dataset* (or viceversa).\n\n...\n\n*Unrepresentative validation dataset implies that the ability of the model to generalize is very poor because the validation dataset does not provide enough information. While unrepresentative training dataset means that the training dataset does not provide sufficient information to learn the problem, relative to the validation dataset used to evaluate it.*\"\n\nSource [Jason Brownlee ](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)\n\nAlthough RoCoLe has information to be able to classify in binary it is not enough for these two trained networks. In the article they explain that they have used other types of ML algorithms such as SVM. However, they donâ€™t give more information nor results.\n\nAnyway, the tutorial has been satisfactory to show all the essential steps to start analyzing a dataset using TensorFlow tools. It has been demonstrated, using visual tools, that this dataset is not optimal for a binary classification following the above steps.\n\nHope you liked it!","metadata":{"id":"D_dK21tb-yxP"}}]}