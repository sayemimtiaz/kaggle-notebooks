{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np #for numerical computations\nimport pandas as pd #for dataframe operations\n\nfrom matplotlib import pyplot as plt #for viewing images and plots\n%matplotlib inline \n#So that Matplotlib plots don't open in separate windows outside the notebook\n\nimport urllib #For fetching data from Web URLs\n\nimport cv2   #For image processing\n\nfrom sklearn.preprocessing import LabelEncoder    #For encoding categorical variables\nfrom sklearn.model_selection import train_test_split #For splitting of dataset\n\n#All tensorflow utilities for creating, training and working with a CNN\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I always add the following cell in every jupyter notebook of mine. It enable every cell to show multiple views and results.","metadata":{}},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Importing Dataset","metadata":{}},{"cell_type":"code","source":"#Import the dataset\n\ndf = pd.read_csv(r'/kaggle/input/fashion-dataset-with-over-15000-labelled-images/dress.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following function plots any valid image web URL and plots it. We'll use it to plot an image from the dataset at random.","metadata":{}},{"cell_type":"markdown","source":"### Read Image from URL and view it using Pyplot","metadata":{}},{"cell_type":"code","source":"def show_image_from_url(image_url):\n\n  \"\"\"\n  Fetches image online from the image_url and plots it as it is using matplotlib's pyplot's image show\n  \"\"\"\n\n  response = urllib.request.urlopen(image_url)\n  image = np.asarray(bytearray(response.read()), dtype=\"uint8\")\n  image_bgr = cv2.imdecode(image, cv2.IMREAD_COLOR)\n  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n  plt.imshow(image_rgb), plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nshow_image_from_url(df['image_url'].loc[9564])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Every image has a lot of background noise. We are only interested in the parts of the images that contain the dresses. Thankfully, those parts have been pre-tagged in the images with red rectangles. We will later use these rectangles to crop the images.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('All categories : \\n ', df['category'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will remove the category 'OTHER' for adding simplicity.","metadata":{}},{"cell_type":"code","source":"n_classes = df['category'].nunique()\nprint('Total number of unique categories:', n_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove the category 'OTHER' from the dataset\n\ndf = df.loc[(df['category'] != 'OTHER')].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Processing the Images","metadata":{}},{"cell_type":"markdown","source":"Our CNN model can only work with numbers. What we have are Image URLs and their categorical labels. We will deal with the labels later. Now we need to convert the images into something the CNN can work with.\n\nAs we have seen earlier, the images are colored. An image is just a matrix of pixels. Every pixel in a colored image consists of three-dimensional information (amount of red, green, and blue respectively). The pixels of grayscale (black and white) images however only need one-dimensional data. It varies from 0 to 255. The lower corresponds to absolute white and the upper end corrsponds to absolute black. Everything in between can represent any shade of grayscale possible. \n\nWe will urlib to download the images from the URLs, use numpy to convert the images into nice clean ndarrays. We will use OpenCV to read those ndarrays in color form (The order in Open CV is Blue-Green-Red). We will convert it to another format 'HSV' where we can create a mask that will detect those red rectangles.\n\nUsing OpenCV, we can either read the ndarray directly in grayscale format or convert the BGR format to grayscale. \n\nThen we will fetch the pixel coordinates of the corners of the detected rectangles, and using them crop the grayscale versions.We also need uniformity in our data. The original images have different shapes. So, we will reshape every cropped grayscale image into 100x100 pixels.\n\nEvery OpenCV image is stored as array data. So all these ndarrays can be flattened into a single dimensional array of length 10000x1 (we can reshape arrays into any desirable dimensions as long as the total number of elements in the original array and the transformed array remain the same.\n\nNote that reshaping images on OpenCV though is different since there loss of information is not an issue. We can compromise with the image resoution there.\n\nAlso note that we are only using OpenCV to read and edit images. We will use Pyplot to view the images, and that requies RGB(Red-Green-Blue) format. We can use OpenCV to convert from BGR to RGB.","metadata":{}},{"cell_type":"code","source":"def image_processing(image_url):\n\n  \"\"\"\n  Converts the URL of any image to an array of size 100x1 \n  The array represents an OpenCV grayscale version of the original image\n  The image will get cropped along the biggest red contour (4 line polygon) tagged on the original image (if any)\n  \"\"\"\n\n  #Download from image url and import it as a numpy array\n  response = urllib.request.urlopen(image_url)\n  image = np.asarray(bytearray(response.read()), dtype=\"uint8\")                         \n\n  #Read the numpy arrays as color images in OpenCV\n  image_bgr = cv2.imdecode(image, cv2.IMREAD_COLOR)\n\n  #Convert to HSV for creating a mask\n  image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n\n  #Convert to grayscale that will actually be used for training, instead of color image \n  image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n\n  #Create a mask that detects the red rectangular tags present in each image\n  mask = cv2.inRange(image_hsv, (0,255,255), (0,255,255))\n\n  #Get the coordinates of the red rectangle in the image, \n  #But take entire image if mask fails to detect the red rectangle\n  if len(np.where(mask != 0)[0]) != 0:\n    y1 = min(np.where(mask != 0)[0])\n    y2 = max(np.where(mask != 0)[0])\n  else:\n    y1 = 0                                     \n    y2 = len(mask)\n  \n  if len(np.where(mask != 0)[1]) != 0:\n    x1 = min(np.where(mask != 0)[1])\n    x2 = max(np.where(mask != 0)[1])\n  else:\n    x1 = 0\n    x2 = len(mask[0])\n\n  #Crop the grayscle image along those coordinates\n  image_cropped = image_gray[y1:y2, x1:x2]\n\n  #Resize the image to 100x100 pixels size\n  image_100x100 = cv2.resize(image_cropped, (100, 100))\n\n  #Save image as in form of array of 10000x1\n  image_arr = image_100x100.flatten()\n  return image_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Iterating over image URL and generating the required array is a time-consuming process. You can remove the triple quotes in the below cell and run it if you wish.\n\nThe resulting matrix can be saved as a binary file using numpy in the Kaggle's working directory of this notebook. I alread had the binary file on my Google drive. So, I used the \"gdown\" utility to import it directly from there. It's an efficient way since the file size is huge (over 1 GB). Afterall, there are over 15000 rows in the matrix, with each row representing each image, and the length of each row array being 10000.\n\nNote that I have also normalized the data between 0 and 1 by dividing every element of it by 255.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimage_list = []\n\nfor url in df['image_url'] :\n  image_list.append(image_processing(url))\n\nX = np.array(image_list)\n\nX = X/255\n\nX = np.save('/kaggle/working/X.npy', X)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading large data files directly from Google Drive ","metadata":{}},{"cell_type":"markdown","source":"The library that we want to use for directly importing files from any google drive is not pre-installed on Kaggle. So, turn on the internet in the notebook settings first. ","metadata":{}},{"cell_type":"code","source":"pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown\n\nurl = 'https://drive.google.com/uc?id=1B6_rtcmGRy49hqpwoJT-_Ujnt6cYj5Ba'\noutput = 'X.npy'\ngdown.download(url, output, quiet=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You should comment out above cell after 'X' is downloaded. ","metadata":{}},{"cell_type":"code","source":"X = np.load('/kaggle/working/X.npy')\nX[0:3]\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see how well the image processing function did its job , we will fetch 5 samples from 'X' at random and plot them.","metadata":{}},{"cell_type":"code","source":"#Select 5 processed images at random from 'X' and plot \n\nnp.random.seed(17)\nfor i in np.random.randint(0, len(X), 5):\n  plt.figure()\n  plt.imshow(X[i].reshape(100, 100)), plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so the image processing part is done. We have our 'X' that we need as input for training the CNN.","metadata":{}},{"cell_type":"markdown","source":"# 4. Splitting the data","metadata":{}},{"cell_type":"markdown","source":"We need to split the data into training, validation, and test sets. ","metadata":{}},{"cell_type":"markdown","source":"### Creating the Target Array","metadata":{}},{"cell_type":"markdown","source":"We already have our 'X'. \nNow, we need our 'Y'.\n\nWe will ceate it from the 'Category' column of our dataframe. It consists of the categorical labels. We will map each label with a number using the LabelEncoder.","metadata":{}},{"cell_type":"code","source":"#Fetch the categories column from the dataframe, and tranform into to numerical labels\n\nencoder = LabelEncoder()\nTargets = encoder.fit_transform(df['category'])\nTargets\nTargets.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now we have a single column array. We can use this directly as our 'Y'. Then out CNN model will have a single node in the output layer. The activation fuction in that case after the output node will be 'ReLU'\n\nBut, a better approach is to create a ndarray where the values lie between 0 and 1, and each column corresponds to particular label. This is called one-hot encoding. Now, the ouput layer will have as many nodes as the number of categories. The activation layer will have a 'Softmax' function that will spit out a number between 0 and 1 for every output node. We preferably want it to produce 1 for the correct node and zeros for the other output nodes. But its output is still a nice probaility distribution fuction over the output nodes. We can extract more meaning from that. For example, if an image is vague enough that it can be labelled under multiplte categories, the values at the output nodes will give a clear indication. ","metadata":{}},{"cell_type":"code","source":"#One-hot encoding of the Target vector\n\nY = to_categorical(Targets, num_classes = n_classes)\nY[0:3]\nY.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separation of a validation set, and a test set","metadata":{}},{"cell_type":"code","source":"#Segregation of a test set for testing on the trained model\n\nX_test = X[14000:,]\nY_test = Y[14000:,]\n\n#Seperation of a validation set from the remaing training set (required for validation while training)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X[:14000,], Y[:14000,], test_size=0.15, random_state=13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reshaping the input data","metadata":{}},{"cell_type":"markdown","source":"I know we reshaped out images earlier into 10000x1 row arrays, since it was easier to store them that way. But CNN requires input data in form of nice multi-dimensional grids. Reshaping into three-dimensions is suitable for videos. For our image data, the third dimesion will simply be 1 ","metadata":{}},{"cell_type":"code","source":"#Reshape the input matrices such that each sample is three-dimensional\n\nimg_rows, img_cols = 100, 100\ninput_shape = (img_rows, img_cols, 1)\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"### Compiling the model","metadata":{}},{"cell_type":"code","source":"#Define a Convolutional Neural Network Model\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n                 input_shape = input_shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(n_classes, activation='softmax'))\n\nlearning_rate = 0.001\n\nmodel.compile(loss = categorical_crossentropy,\n              optimizer = Adam(learning_rate),\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the best weights","metadata":{}},{"cell_type":"markdown","source":"By using 'Callback' and 'ModelCheckpoint'utilities of Keras, we can save the model with the best weights. By 'best', I mean the ones that gave the best performance over the validation set while training. It checks if the performance of model with updated weights after every epoch is better than the performance of the saved model.","metadata":{}},{"cell_type":"code","source":"#Save the model during training \n\nsave_at = \"/kaggle/working/model.hdf5\"\nsave_best = ModelCheckpoint (save_at, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='max')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the CNN","metadata":{}},{"cell_type":"code","source":"#Train the CNN\n\nhistory = model.fit( X_train, Y_train, \n                    epochs = 15, batch_size = 100, \n                    callbacks=[save_best], verbose=1, \n                    validation_data = (X_val, Y_val))\n\n# Plot the training history (Training accuracy & Validation accuracy)\n\nplt.figure(figsize=(6, 5))\nplt.plot(history.history['accuracy'], color='r')\nplt.plot(history.history['val_accuracy'], color='b')\nplt.title('Model Accuracy', weight='bold', fontsize=16)\nplt.ylabel('accuracy', weight='bold', fontsize=14)\nplt.xlabel('epoch', weight='bold', fontsize=14)\nplt.ylim(0.4, 0.9)\nplt.xticks(weight='bold', fontsize=12)\nplt.yticks(weight='bold', fontsize=12)\nplt.legend(['train', 'val'], loc='upper left', prop={'size': 14})\nplt.grid(color = 'y', linewidth='0.5')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Evaluating Performace over test-set","metadata":{}},{"cell_type":"code","source":"#Run model on the held-out test set\n\nmodel = load_model('/kaggle/working/model.hdf5')\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now plot 10 images at random from test set, but with titles as classified by the model, with every correct classification titled in 'green' color, and every incorrect classification titles in 'red' color.","metadata":{}},{"cell_type":"code","source":"Y_pred = np.round(model.predict(X_test))\n\nnp.random.seed(87)\nfor rand_num in np.random.randint(0, len(Y_test), 5):\n  plt.figure()\n  plt.imshow(X_test[rand_num].reshape(100, 100)), plt.axis('off')\n  if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(Y_test[rand_num] == 1)[0].sum():\n    plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n  else :\n    plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Finding similar Images","metadata":{}},{"cell_type":"markdown","source":"What if I have an image, and expect to see a few similar images from the existing dataset? Below function does just that.","metadata":{}},{"cell_type":"code","source":"def find_similar_images(image_url, no_of_images):\n\n  \"\"\"\n  Takes in an image_url, and number of similar images needed.\n  Model makes prediction on the dress category in the image\n  The original image and desired number of similar images from the dataset are plotted\n  \"\"\"\n\n  X_query = image_processing(image_url)\n  X_query = X_query/255\n  X_query = X_query.reshape(1, 100, 100, 1)\n  Y_query = np.round(model.predict(X_query))\n  i = np.where(Y_query == 1)[0].sum()\n  print('Type detected by model:', encoder.classes_[i].upper())\n  df_req = df.loc[ df['category'] == encoder.classes_[i]]\n  df_req = df_req.reset_index(drop=True)\n\n  if no_of_images > len(df_req):\n    return(print('number of images needed are more than similar images in the dataset'))\n\n  plt.figure()\n  show_image_from_url(image_url)\n  plt.title('Query Image')\n\n  c = 1\n  np.random.seed(13)\n  for j in np.random.randint(0, len(df_req), no_of_images):\n    plt.figure()\n    url = df_req['image_url'].iloc[j]\n    show_image_from_url(url)\n    plt.title('Similar Image {}'.format(c))\n    c += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is an random image that I picked online.","metadata":{}},{"cell_type":"code","source":"find_similar_images('https://i.dailymail.co.uk/1s/2018/11/06/23/5855600-6360713-Ashley_James_stuns_in_emerald_green_animal_print_dress_at_glitzy-a-123_1541546195058.jpg', 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}