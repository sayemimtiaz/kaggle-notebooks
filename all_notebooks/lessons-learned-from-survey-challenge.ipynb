{"cells":[{"metadata":{"_uuid":"e74d46413805a0a5d840cb85734545a02ea12245"},"cell_type":"markdown","source":"# Lessons Learned from Survey Challenge\n\nThe winners of the 2018 Kaggle ML & DS Survey Challenge will be announced on Monday (December 10th), and I'm hoping to update this kernel with a summary of lessons learned from those. Below is an appendix with some meta analysis regarding medals awarded to kernels and the timing of those kernels being published (WIP). \n"},{"metadata":{"_uuid":"25abb9f36d38129ee119e68dcf3d98a901b63fdd"},"cell_type":"markdown","source":"## Takeaways from Winning Submissions - How to tell a data story\n\n227 kernels were made public before the survey challenge deadline. A large subset of these have been submitted to Kaggle as the challenge entry (a simple online form had to be filled out with a link to your kernel to be eligeable for prizes). I'm sure it has been a difficult task for the Kaggle team to identify the top 6 kernels, as overall I found that about half the kernels told an indepth story. The criteria that Kaggle used to judge the entries are as follows:\n> _Composition_ - Is there a clear narrative thread to the story thatâ€™s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.<br>\n<br>_Originality_ - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.<br>\n<br>_Documentation_ - Are your code, and kernel, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible\n\nWhile these categories are all subjective, I think it is instructive to review the winning entries and pull out key themes that made them successful. I breakdown these winning themes below, with links to original kernels which I encourage everyone to review.\n\n### 1. Focus on a single topic...\nWhether it's survey respondents from [Africa](https://www.kaggle.com/mhajabri/africai) or highlighting [gender divides](https://www.kaggle.com/martinlbarron/the-gender-divide-in-data-science), or how Kagglers [feel about AI accountability](https://www.kaggle.com/strangemane/measuring-accountability-in-ds-and-ml-with-waffles), it is important to stick to one topic rather than try to cover everything and anything. This was not an EDA challenge, it was a storytelling one. A good story needs to draw the reader by introducing an intriguing idea or one that the reader doesn't think about very often. Then run with that idea to educate, persuade, or call to action. I've noticed a number of kernels did not provide a conclusion, but rather just ended with an explanation of their last plot. The opening and closing words are so important to tie everything together, to solidify your point, and to make a compelling story. \n\n### 2. ...and make it a relevant one\nChosing a good subject to focus on is half the battle. Before settling on the [MOOC analysis](https://www.kaggle.com/ogakulov/the-mooc-wars-kaggle-s-perspective), I've tried a couple of other ideas, but found them to not have much interesting insight. It was frustrastrating, but I knew that without a good single thread, my submission would be doomed (see _Originality_ criteria). Topics that placed in the top 6 spots are intriguing and appealing to a wide spectrum of Kagglers. [What should I do to earn more?](https://www.kaggle.com/andresionek/what-makes-a-kaggler-valuable) [Which IDE should I use?](https://www.kaggle.com/robikscube/a-tale-of-4-kaggler-types-by-ide-use-2018-survey) Or more general topics like gender divide and the hot topic of model bias which came into the spotlight recently. All of these are relevant to the individuals in the Data Science community. Compare these to something like \"what is data science?\" or \"who is a data scientist?\" filled with descriptive statistics of the dataset. These topics have been covered time and time again and easily blend into the noice, rather than stand out.\n\n### 3. Polish your visuals\nNo good data story was ever told without data visualization. I've been somewhat of an evangelist of best practice use of bar graphs by commenting on various kernels, but there is much more to it than that. Just pick up a copy of \"Storytelling with Data\" by Cole Knaflic from your local library and you will be well on your way to improving your viz game. But it's not enough to just have plots and describe, for example, the highest value is X and the lowest is Y, and the standard deviation is Z. That is ok in a report, but it doesn't tell a story. All of the winning submissions have organically woven data visualizations into their kernels to support their narrative. You will also notice a visual consistency throughout each of the winning kernels, meaning that the same type of graph, with the same pallet, font sizes, etc. has been used to make comparisons throughout the story. \n\nDon't be afraid to step outside the bounds of pies and bars. Bri Taylor showed great use of waffle charts throuout [her kernel](https://www.kaggle.com/strangemane/measuring-accountability-in-ds-and-ml-with-waffles). Similarly Martin L Barron consistently used cleveland dot plot to show the [difference between genders](https://www.kaggle.com/martinlbarron/the-gender-divide-in-data-science). These may seem like advanced techniques, but there are wonderful packages that make them fairly simple to use. \n\n### 4. Explain your code\nOne of the criteria in the challenge was making your code well commented so others can learn from it. Here, I must say, not all of the top kernels did a great job. I will give props to [Andre](https://www.kaggle.com/andresionek/what-makes-a-kaggler-valuable) and [Martin](https://www.kaggle.com/martinlbarron/the-gender-divide-in-data-science/notebook) for having a good balance of comments throughout the code.  But the real star in this category is [Bri](https://www.kaggle.com/strangemane/measuring-accountability-in-ds-and-ml-with-waffles/code), who really spent the time to explain what her code was doing for almost every line. \n\n### 5. Have fun!\nMemes, gifs, and waffle makers all made an appearance among the winning kernels. Remember that an engaging story doesn't have to be all serious and dry. Don't be afraid to inject some personality into your data exploration. Give your clusters [funny names](https://www.kaggle.com/robikscube/a-tale-of-4-kaggler-types-by-ide-use-2018-survey) or include [personal annecdotes](https://www.kaggle.com/mhajabri/africai) to make your story come to life.\n "},{"metadata":{"_uuid":"4a1c03e97782cd20aca2c05d317c3505870d5660"},"cell_type":"markdown","source":"\n\n\n## Appendix: Want a medal? Gotta post early.\nAs of December 7th, there were 245 public kernels using the 2018 Kaggle Survey dataset. 45% of these have been awarded at least a bronze medal. I think this is on the high end, thogh I haven't looked too deep into this stat (e.g. Center for Policing Equity dataset has 74 kernels, and 39% of them have medals), What's interesting is the timing of when a kernel has been made public with respect to the timeline of this challenge."},{"metadata":{"trusted":true,"_uuid":"67a92ba895f6c00b504ffc42d2c46c925f07f132","_kg_hide-input":true},"cell_type":"code","source":"# Import standard libraries\nimport pandas as pd \nimport datetime\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\n# READ IN THE DATA\n# Survey data\nsurv = pd.read_csv('../input/kaggle-survey-2018/multipleChoiceResponses.csv', low_memory=False)\n\n# get the dataset id for the 2018 Kaggle Survey Kernel\ndatasets_v = pd.read_csv('../input/meta-kaggle/DatasetVersions.csv')\ndata_v = datasets_v.loc[datasets_v['Title'] == '2018 Kaggle ML & DS Survey Challenge', 'Id']\n\n# get the kernels that used any version of the survey data\nkernel_data = pd.read_csv('../input/meta-kaggle/KernelVersionDatasetSources.csv')\nkernel_v = kernel_data.loc[kernel_data['SourceDatasetVersionId'].isin(data_v), 'KernelVersionId']\n\n# get the kernel ids from kernel versions\nkernels = pd.read_csv('../input/meta-kaggle/Kernels.csv', parse_dates=['MadePublicDate', 'MedalAwardDate', 'CreationDate'])\nchall_kernels = kernels[kernels['CurrentKernelVersionId'].isin(kernel_v)]\n\n# Set additional medal level, when no medal is awarded\npd.options.mode.chained_assignment = None\nchall_kernels.loc[:, 'Medal_or_0'] = chall_kernels['Medal'].fillna(0)\n\n# Count kernels by each date and medal level\nt = pd.pivot_table(chall_kernels, values=['Id'], index='MadePublicDate', columns=['Medal_or_0'], aggfunc='count')\nt = t.fillna(0)\n\n# PLOT\n# figure setting\nplt.figure(figsize=(15,5))\n\n# Draw bars\nplt.bar(x=t.index, height=t['Id', 3], color='goldenrod', width=0.7, label='Bronze')\nplt.bar(x=t.index, bottom=t['Id', 3], height=t['Id', 2], color='silver', width=0.7, label='Silver')\nplt.bar(x=t.index, bottom=t['Id', 3] + t['Id', 2], height=t['Id', 1], color='gold', width=0.7, label='Gold')\nplt.bar(x=t.index, bottom=t['Id', 3] + t['Id', 2] + t['Id', 1], height=t['Id', 0], color='beige', width=0.7, label='No Medal')\n\n# Format x-axis\nplt.xlabel('Date Kernel was Made Public')\nax = plt.gca()\nplt.xticks(pd.date_range(start=min(t.index), periods=5, freq='W'))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))\n\n# Format y-axis\nplt.ylabel('Count of Kernels')\n\n# Other formatting\nplt.tight_layout()\nplt.legend(loc='best', fontsize=12);\n\n# Annotations:\nplt.annotate('Challenge Launched', (mdates.date2num(t.index[7]), 10), (mdates.date2num(t.index[5]), 25), arrowprops=dict(arrowstyle='->'))\nchall_end = mdates.date2num(t.index[31] + datetime.timedelta(hours=11.5)) # end of challenge \nplt.plot([chall_end, chall_end], [0, 26], color='black', linewidth=1, linestyle=':')\nplt.text(chall_end - 3.85, 25, 'Challenge Deadline');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eadef1752d3a941519fa0c36d5c7517e43e4e495"},"cell_type":"markdown","source":"\nThe chances of a kernal being awarded a medal steadily decreased as the challenge went on. This makes intuitive sense. For one, to get a medal your kernel has to be upvoted, and for that it needs to stand out from other available kernels. It is markedly easier to stand out when there are fewer kernels overall. Also, it takes time to earn a medal - it may take a few days for your kernel to gain a critical mass of upvotes. So, having your kernel available longer than others makes it more likely to be awarded a medal. Notice that hardly any kernels published after November 29th (five days before challenge deadline) have gotten any medals."},{"metadata":{"trusted":true,"_uuid":"814681b5045d0738902a083f0fec9c842ca81246","_kg_hide-input":true},"cell_type":"code","source":"# Proportion of kernels that are awarded a medal\nt['Id', 'Medal'] = t['Id', 1] + t['Id', 2] + t['Id', 3]\nmed_prop = t['Id', 'Medal'].cumsum() / (t['Id', 0].cumsum() + t['Id', 'Medal'].cumsum())\n\n# PLOT\n# Figure\nplt.figure(figsize=(12,6))\n\n# Draw lineplot\nplt.plot(med_prop.index, med_prop)\n\n# Format x-axis\nplt.xlabel('Date Kernel was Made Public')\nax = plt.gca()\nplt.xticks(pd.date_range(start=min(t.index), periods=5, freq='W'))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))\n\n# Format y-axis\nplt.ylabel('Proportion of Kernels with Medals')\n\n# Annotations:\nplt.annotate('Challenge Launched', (mdates.date2num(t.index[7]), 0.75), (mdates.date2num(t.index[4]), 0.95), arrowprops=dict(arrowstyle='->'));\n#chall_end = mdates.date2num(t.index[31] + datetime.timedelta(hours=11.5)) # end of challenge \n#plt.plot([chall_end, chall_end], [0, 0.95], color='black', linewidth=1, linestyle=':')\n#plt.plot([mdates.date2num(t.index[0]), mdates.date2num(t.index[35])], [0.5, 0.5], color='black', linewidth=1, linestyle=':')\n#plt.text(chall_end - 3.9, 25, 'Challenge Deadline');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f298779065d7f80975625a9377ce1f9475b99c8"},"cell_type":"markdown","source":"Over 75% of kernels that have been made public _before_ the Challenge Launch have attained a medal. As I'm sure Kaggle will be running a similar ML & DS survey next year, a strategy (if you want a medal) should be to publish your kernel as soon as the dataset becomes available, even before the announcement email hits your inbox."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}