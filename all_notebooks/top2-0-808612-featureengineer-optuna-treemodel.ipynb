{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Titanic Survivor Prediction"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport IPython\n# warning\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n# statistics\nfrom scipy.stats import skew, kurtosis,probplot,norm\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import fcluster\n# plot libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Preprocessing and encoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,OrdinalEncoder, LabelEncoder\n# model evaluation and selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,ShuffleSplit,StratifiedKFold\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport optuna\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\n# Ensemble Classifiers\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier,VotingClassifier,StackingClassifier\nfrom xgboost.sklearn import XGBClassifier\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nimport kerastuner as kt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# load dataset\ntitanic_raw_train = pd.read_csv('..//input/titanic/train.csv')\ntitanic_raw_test = pd.read_csv('..//input/titanic/test.csv')\ntitanic_raw_train.info()\ntitanic_raw_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"titanic_raw_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"titanic_raw_train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***\nThere are 11 columns in total except for \"PassengerId\" as index. Some are numerical, others are ordinal or categorical. \"Survived\" is the column we want to predic. Let's dive into them one by one. "},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# make a copy for feature engineering\ntrain = titanic_raw_train.copy()\ntest = titanic_raw_test.copy()\ntrain_test = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Attention:***  \nIt's not always safe to combine train and test set and do feature engineering together because this will cause data leakage. But if you want a higher score on test data, it's a good option. In real life, we learn everything only from features in training set and apply the same preprocessing or feature engineering in test set. Otherwise, your model might be overfiting on test set and have a poor performance on unseen data."},{"metadata":{},"cell_type":"markdown","source":"### Convert feature types\n"},{"metadata":{},"cell_type":"markdown","source":"Overall correlation between features.\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"corr = train.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.heatmap(data=corr,vmax=1,vmin=-1,center=0,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pclass\nOrdinal feature without missing value. Let's see it's distribution and relation to other features."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Pclass.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Pclass Distribution by Survived\nsns.catplot(data=train,x='Pclass',hue='Survived', kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.catplot(data=train,x='Pclass',y='Survived', kind=\"point\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fare distribution by Pclass with Survived as label\nsns.catplot(data=titanic_raw_train,x='Pclass',y='Fare',hue='Survived',kind='swarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Survived','Pclass']].groupby('Pclass').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***\nMost people brought 3rd class(over 75%).Most people in 3rd class died. More than half people in 1st class survived!"},{"metadata":{},"cell_type":"markdown","source":"### Name\nName contains tons of informations like family, gender, title, etc. Here we use name to extract Title and Family_name features."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#looks for strings which lie between A-Z or a-z and followed by a .\ntrain_test['Title']=train_test.Name.str.extract('([A-Za-z]+)\\.') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.crosstab(train_test.Title, train_test.Survived)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group rare titles, and replace french words('Mlle','Mme')."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Title.replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                         ['Miss','Mrs','Miss','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare','Rare'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.crosstab(train_test.Title, train_test.Survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is_married feature based on title\ntrain_test.loc[train_test['Title'] == 'Mrs','Is_Married'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Family Name"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract family name\ntrain_test['Family_name'] = train_test.Name.str.extract('(\\w+),', expand=False)\ntrain_test.Family_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Family Survived  \nWe use family name to find family members in both training and test set. Family tends to escape together, if we know some family members in training set survived, there is a good chance members in test set will survive as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use mean survived rate as Familiy_survived feature \nm = train_test[['Family_name', 'Survived']].groupby('Family_name').mean()\nc = train_test[['Family_name', 'PassengerId']].groupby('Family_name').count()\nm = m.rename(columns={'Survived': 'Family_survived'})\nc = c.rename(columns={'PassengerId': 'FamilyMemberCount'})\n# if family name is unique in all data, set Family_survived as -1\nm = m.where(m.join(c).FamilyMemberCount > 1, other=-1).fillna(-1).join(c)\nm.Family_survived = m.Family_survived.astype('int64')\ntrain_test = train_test.join(m, on='Family_name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age"},{"metadata":{},"cell_type":"markdown","source":"Handle missing values in 'Age' using 'Title' mean.(just for now)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Age.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test[['Age','Title']].groupby('Title').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set missing age as the mean of age for its corresponding title."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# fill in missing ages\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Mr'),'Age']=32\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Mrs'),'Age']=37\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Master'),'Age']=5\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Miss'),'Age']=22\ntrain_test.loc[(train_test.Age.isnull())&(train_test.Title=='Rare'),'Age']=45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cast 'Age' into 'int'."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test['Age'] = train_test['Age'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.distplot(a=train_test.Age, kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_skew(feature):\n    \"\"\"\n    Function to plot distribution and probability(w.r.t quantiles of normal distribution)\n    \"\"\"\n    fig, axs = plt.subplots(figsize=(20,10),ncols=2)\n    sns.distplot(feature,kde=True,fit=norm,ax=axs[0])\n    # Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).\n    f=probplot(feature, plot=plt)\n    print('Skewness: {:f}'.format(feature.skew()))\n    print('Kurtosis: {:f}'.format(feature.kurtosis()))\nplot_skew(train.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy?scriptVersionId=2051374\nplt.hist(x = [train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.catplot(x='Sex',data=train,hue = 'Survived', kind='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.catplot(x=\"Sex\", y=\"Survived\", kind=\"point\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# https://seaborn.pydata.org/tutorial/categorical.html#distributions-of-observations-within-categories\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"point\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Survived','Sex']].groupby('Sex').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***\nFemale has much higher surviving rate than male."},{"metadata":{},"cell_type":"markdown","source":"### SibSp and Parch"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.SibSp.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.distplot(a=train.SibSp, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=[10,6])\nplt.subplot(121)\nplt.hist(x = [train[train['Survived']==1]['SibSp'], train[train['Survived']==0]['SibSp']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('SibSp Histogram by Survival')\nplt.xlabel('# SibSp')\nplt.ylabel('# of Passengers')\nplt.subplot(122)\nplt.hist(x = [train[train['Survived']==1]['Parch'], train[train['Survived']==0]['Parch']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Parch Histogram by Survival')\nplt.xlabel('# Parch')\nplt.ylabel('# of Passengers')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.catplot(x='SibSp',kind='count',hue='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.barplot(x = 'SibSp', y = 'Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig, saxis = plt.subplots(1, 2,figsize=(16,12))\nsns.barplot(x = 'SibSp', y = 'Survived', data=train,ax=saxis[0])\nsns.barplot(x = 'Parch', y = 'Survived', order=[1,2,3], data=train, ax=saxis[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***\nSibSp and Parch are related to each other and has a similiar distribution on Survived."},{"metadata":{},"cell_type":"markdown","source":"### Fare"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Fare.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Fare.fillna(test.Fare.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Fare distribution\nplt.figure(figsize=(10,5))\nsns.distplot(a=titanic_raw_train.Fare, kde=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_skew(train_test.Fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test['Fare_log'] = np.log1p(train_test.Fare)\nplot_skew(train_test.Fare_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ticket"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract ticket numbers \nticket = train_test.Ticket.str.extract('(\\d+$)', expand=False).fillna(0).astype(int).ravel()\n# cluster data from https://www.kaggle.com/shaochuanwang/titanic-ml-tutorial-on-small-dataset-0-82296/notebook\nZ = linkage(ticket.reshape(train_test.shape[0], 1), 'single')\nclusters = fcluster(Z, 20, criterion='distance')\ntrain_test['Ticket_Code'] = clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\ncount = train_test[['PassengerId', 'Ticket_Code']].groupby('Ticket_Code').count().rename(columns={'PassengerId': 'Number'})\ntrain_test['Ticket_Code_Remap'] = train_test.Ticket_Code.replace(dict(zip(count.index[count.Number <= 10], itertools.cycle([0]))))\nfig, axs = plt.subplots(figsize=(20,20),nrows=2)\nsns.barplot(train_test.Ticket_Code, train_test.Survived,ax=axs[0])\nsns.barplot(train_test.Ticket_Code_Remap, train_test.Survived,ax=axs[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ticket frequency\ntrain_test['Ticket_Frequency'] = train_test.groupby('Ticket')['Ticket'].transform('count')\ntrain_test.Ticket_Frequency.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cabin\n"},{"metadata":{},"cell_type":"markdown","source":"To many cabin information are missing but this feature might be important since it will directly influence the escaping routine."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Cabin.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group cabin using area code\ntrain_test['Cabin_code'] = train_test.Cabin.str.get(0).fillna('Z')\ntrain_test[['Survived','Cabin_code']].groupby('Cabin_code').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embarked"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Embarked.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill missing value with most frequent value."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.Embarked.fillna(train_test.Embarked.mode()[0], inplace=True)\npd.crosstab(train_test.Embarked, train_test.Survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.catplot(data=train,x='Embarked',y='Survived', kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Embarked','Survived']].groupby('Embarked').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***\nPassengers embarked from 'Cherbourg' have higher survival rate."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineer and Encoding\n"},{"metadata":{},"cell_type":"markdown","source":"### Family Size and IsAlone"},{"metadata":{},"cell_type":"markdown","source":"Remember that we find out the relation between SibSp and Parch, we now create two new features: 'Family_size' and 'Is_alone' based on those.https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test['Family_size']=train_test.SibSp+train_test.Parch\ntrain_test['Is_alone']=1\ntrain_test.loc[train_test['Family_size'] > 1,'Is_alone'] = 0\ntrain_test['Family_size'].astype('int64')\ntrain_test['Is_alone'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x='Family_size', y='Survived',kind='point',data = train_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation*:**  \nIt's pretty clear that 'Family_size' has four group: Alone(0), small family(1-3), middle family(4-6) and large family(>=7). And each group has different Survival Rate. This will help us to encode this feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Is_alone', y='Survived',kind='point',data = train_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Another way to predit missing 'Age'  \nUse features highly related to 'Age' to predict it. Here we use random forest Regressor. Tree model doesn't need to scale features, so might be a good option for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# #choose related data to predict age\n# age_df =train_test[['Age','Fare', 'Family_size', 'Title', 'Pclass','Is_alone','Sex']]\n# age_df_notnull = age_df.loc[train_test['Age'].notnull()]\n# age_df_isnull = age_df.loc[(train_test['Age'].isnull())]\n# Xtr_age = pd.get_dummies(age_df_notnull.drop(columns=['Age']))\n# Xte_age = pd.get_dummies(age_df_isnull.drop(columns=['Age']))\n# Y_age = age_df_notnull.Age\n# # use RandomForestRegression to train data\n# RFR = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n# RFR.fit(Xtr_age,Y_age)\n# predictAges = RFR.predict(t)\n# train_test.loc[train_test['Age'].isnull(), ['Age']]= predictAges\n# RFR.score(Xtr_age,Y_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binning continuous features"},{"metadata":{},"cell_type":"markdown","source":"Bining continuous features to ordinal features."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(10,8),ncols=2)\nsns.distplot(a=train.Age, ax=axs[0])\nsns.distplot(a=train.Fare, ax=axs[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test['Age_band'] = pd.cut(train_test.Age, bins=10, precision=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test['Fare_band'] = pd.qcut(train_test.Fare_log, q=13, precision=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(28,8),ncols=2)\nsns.pointplot(x='Age_band', y='Survived', data=train_test,ax=axs[0])\nsns.pointplot(x='Fare_band', y= 'Survived', data=train_test,ax=axs[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scale continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"MM_scaler = MinMaxScaler(feature_range=(0,1))\ntrain_test[['Age_scaled']]=MM_scaler.fit_transform(train_test[['Age']])\nsns.distplot(a=train_test.Age_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Fare' feature has strong outlier, so use Standardization instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"St_scaler = StandardScaler()\ntrain_test[['Fare_scaled']]=St_scaler.fit_transform(train_test[['Fare_log']])\nsns.distplot(a=train_test.Fare_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding"},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal Features:"},{"metadata":{},"cell_type":"markdown","source":"Ordinal features include: Parch, Age, Fare, Family_size. Age and Fare should be encoded by their band."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"le = OrdinalEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test[['Age_code']] = le.fit_transform(train_test[['Age_band']])\ntrain_test[['Fare_code']] = le.fit_transform(train_test[['Fare_band']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for row in train_test:\n    train_test.loc[train_test['Family_size']==0, 'Family_type']=1\n    train_test.loc[(1<=train_test['Family_size'])&(train_test['Family_size']<=3), 'Family_type']=2\n    train_test.loc[(3<=train_test['Family_size'])&(train_test['Family_size']<=6), 'Family_type']=3\n    train_test.loc[7<=train_test['Family_size'], 'Family_type']=4\ntrain_test.Family_type = train_test.Family_type.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use label encoder to encode Cabin, because it doesn't show great importance in our model. Since we need to decrease the number of feature to avoid overfitting, one hot encoder will not be considered for this feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lb_cabin = LabelEncoder()\n# lb_title = LabelEncoder()\n# lb_embarked = LabelEncoder()\n# train_test.Cabin_code = lb_cabin.fit_transform(train_test.Cabin_code)\n# train_test.Title = lb_title.fit_transform(train_test.Title)\n# train_test.Embarked = lb_embarked.fit_transform(train_test.Embarked)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One hot encoding"},{"metadata":{},"cell_type":"markdown","source":"Use one hot encoding to encode Sex and Title features."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# encode Sex\ntrain_test.Sex.replace(['female', 'male'], [1,0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build data"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X = pd.get_dummies(train_test[['Pclass','Sex','Title','Family_type',\n                               'Age_code','Fare_code','Embarked',\n                               'Cabin_code','Ticket_Code_Remap','Family_survived']])[:891]\nX_test = pd.get_dummies(train_test[['Pclass','Sex','Title','Family_type',\n                                    'Age_code','Fare_code','Embarked',\n                                    'Cabin_code','Ticket_Code_Remap','Family_survived']])[891:]\nY = train.Survived\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.heatmap(X.join(Y).corr(),annot=True,cmap='RdYlGn',annot_kws={'size':10})\nfig=plt.gcf()\nfig.set_size_inches(22,22)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model data"},{"metadata":{},"cell_type":"markdown","source":"Create 5 stratified folds on training set to find the best hyperparameters for each model. The folds are made by preserving the percentage of samples for each class. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\nn_trials = 500# no. of trials during tuning","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base Model"},{"metadata":{},"cell_type":"markdown","source":"I use GridSearchCV and optuna framework to select best parameters for each model. And the evaluation matrix is roc_auc instead of accuracy becauset roc_auc is more complicated than accuracy and will avoid overfitting a little bit. The whole process takes ages to run, so I preserved running results and build model directly upon those. Feel free to test by yourself."},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # hyperparameter tuning using GridSearchCV\n# penalty  = ['l1', 'l2','elasticnet', 'none'] # specify the norm used in the penalization\n# C = np.logspace(-2, 2, 10) # 50 nums start form 0.1 to 10 Inverse of regularization strength\n# hyper={'penalty':penalty,'C':C}\n# gd=GridSearchCV(estimator=LogisticRegression(),param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring='accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# LR_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     penalty = trial.suggest_categorical('penalty',['l1', 'l2','elasticnet', 'none'])\n#     if penalty !='none':\n#         C =trial.suggest_loguniform('C', 1e-4, 1e4)\n#         # define classifier\n#         LR_clf = LogisticRegression(penalty=penalty, C=C)\n#     else:\n#         # define classifier\n#         LR_clf = LogisticRegression(penalty=penalty)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(LR_clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# # run study to find best objective\n# study.optimize(objective, n_trials=200)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**500 trials running result:**  \nBest model parameters:{'penalty': 'l2', 'C': 109.16587461586346}   \nBest score: 0.883486"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR_param = study.best_params\nLR_param = {'penalty': 'l2', 'C': 109.16587461586346} \nLR_best= LogisticRegression(**LR_param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # hyperparameter tuning using GridSearchCV\n# C=[0.4,0.5,0.6,0.8,1,5] # Regularization parameter.\n# gamma=['scale','auto',0.01,0.1,0.2,0.3,0.5,1]\n# kernel=['rbf']\n# degree=[3,5,7] # Degree of the polynomial kernel \n# hyper={'kernel':kernel,'C':C,'gamma':gamma,'degree':degree}\n# gd=GridSearchCV(estimator=SVC(),param_grid=hyper,verbose=5,n_jobs=-1,cv=CV,refit=True,scoring='accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# SVM_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n\n#     gamma = trial.suggest_loguniform('gamma',1e-4,1e2) #Kernel coefficient \n#     C =trial.suggest_loguniform('C', 1e-2, 1e3) # Regularization parameter.\n#     kernel = trial.suggest_categorical('kernel',['rbf'])\n#     #degree = trial.suggest_int('degree',1,3)\n#     clf = SVC(gamma=gamma, C=C,kernel=kernel)\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# s=study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trials running result:  \nBest model parameters:{'gamma': 0.04190895340786603, 'C': 4.615992928303742, 'kernel': 'rbf'}   \nBest score: 0.875871  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM_params = study.best_params\nSVM_params = {'gamma': 0.04190895340786603, 'C': 4.615992928303742, 'kernel': 'rbf'}\nSVM_best= SVC(**SVM_params,probability=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# criterion=['gini', 'entropy'] #The function to measure the quality of a split.\n# max_depth=[3,5,10,15,20,None]  #The maximum depth of the tree. \n# splitter=['best', 'random']#The strategy used to choose the split at each node.\n# min_samples_split = [1,3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [1,3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# hyper={'criterion':criterion, 'max_depth':max_depth,'splitter':splitter,\n#        'min_samples_split':min_samples_split,'max_features':max_features,\n#       'min_samples_leaf':min_samples_leaf}\n# gd=GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),param_grid=hyper,\n#                 verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# DT_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n\n#     max_depth = trial.suggest_int(\"max_depth\", 2, 32,5) #The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split', 20,200,20) ##The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 20, 200,20) # minimum num of samples required to be a leaf node\n#     clf = DecisionTreeClassifier(min_samples_split=min_samples_split, max_depth=max_depth, \n#                                    min_samples_leaf=min_samples_leaf, random_state=42)\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='maximize')\n# s=study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trials running result:  \nBest model parameters:{'max_depth': 12, 'min_samples_split': 60, 'min_samples_leaf': 20}   \nBest score: 0.888831"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DT_param =study.best_params\nDT_param ={'max_depth': 12, 'min_samples_split': 60, 'min_samples_leaf': 20}\nDT_best= DecisionTreeClassifier(**DT_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize Decision Tree\nDT_best.fit(X,Y)\nfig = plt.figure(figsize=(30,30))\n_ = tree.plot_tree(DT_best, \n                   feature_names=X.columns,\n                   filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search hyperparameter tunning\n# n_neighbors=[5,10,15,20,100,200] #Number of neighbors \n# weights=['uniform','distance'] # weight function used in prediction\n# p=[1,2] #Power parameter for the Minkowski metric.\n# hyper={'n_neighbors':n_neighbors,'weights':weights,'p':p}\n# gd=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=hyper,cv=CV,refit=True,verbose=2,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# KNN_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_neighbors=trial.suggest_int('n_neighbors',5,305,50) #Number of neighbors \n#     weights=trial.suggest_categorical('weights',['uniform','distance']) # weight function used in prediction\n#     p=trial.suggest_int('p',1,2) #Power parameter for the Minkowski metric.\n#     # define classifier\n#     clf = KNeighborsClassifier(n_neighbors=n_neighbors,weights=weights,p=p,n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective, n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trials running result:  \nBest model parameters:{'n_neighbors': 5, 'weights': 'uniform', 'p': 1}   \nBest score: 0.846715"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN_param = study.best_params\nKNN_param ={'n_neighbors': 5, 'weights': 'uniform', 'p': 1}\nKNN_best= KNeighborsClassifier(**KNN_param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # network builder\n# def model_builder(hp):\n#     model = keras.Sequential()\n#     # Dense layer\n#     for i in range(hp.Int('layers',1,2)):\n#         model.add(keras.layers.Dense(units=hp.Int('units_{:}'.format(i),min_value=32, max_value=512,step=32), \n#                                      activation = hp.Choice('actv_{:}'.format(i),['relu','tanh'])))\n#     # output layer\n#     model.add(keras.layers.Dense(1,activation='sigmoid'))\n#     # model config\n#     model.compile('adam', 'binary_crossentropy',metrics=[keras.metrics.AUC(name='auc')])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # build tuner\n# tuner = kt.Hyperband(model_builder, \n#                     objective = kt.Objective('val_auc','max'),\n#                     max_epochs=20,\n#                     factor = 3,\n#                     directory = 'keras_logs',\n#                     project_name = 'NN')\n# class ClearTrainingOutput(tf.keras.callbacks.Callback):\n#     def on_train_batch_end(*args,**kwargs):\n#         IPython.display.clear_output(wait=True)\n# # start tuning\n# tuner.search(X,Y,epochs = 20, validation_split=0.2,callbacks=[ClearTrainingOutput()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN_best_params = tuner.get_best_hyperparameters(num_trials=1)[0]\n# NN_best_model = tuner.hypermodel.build(NN_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search to tune parameters\n# hidden_layer_sizes=[(40,40),(80,80),(100,),(40),(80),(120)] # the number of neurons in the ith hidden layer\n# activation=['identity', 'logistic', 'tanh', 'relu'] # activation function\n# solver=['lbfgs','sgd','adam'] #The solver for weight optimization.\n# alpha=[0.0001,0.001,0.01,.1] #L2 penalty (regularization term) parameter.\n# learning_rate=['constant','invscaling','adaptive']# Learning rate schedule for weight updates.\n# hyper={'hidden_layer_sizes':hidden_layer_sizes,'activation':activation,\n#        'solver':solver,'alpha':alpha,'learning_rate':learning_rate}\n# gd=GridSearchCV(estimator=MLPClassifier(random_state=42,early_stopping=True),param_grid=hyper,\n#                 cv=CV,refit=True,verbose=2,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# MLP_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_layers = trial.suggest_int('n_layers', 1, 2) # no. of hidden layers \n#     layers = []\n#     for i in range(n_layers):\n#         layers.append(trial.suggest_int(f'n_units_{i+1}', 10, 210,50)) # no. of hidden unit\n#     activation=trial.suggest_categorical('activation',['logistic', 'tanh', 'relu']) # activation function \n#     alpha=trial.suggest_loguniform('alpha',0.0001,50) #L2 penalty (regularization term) parameter.\n#     # define classifier\n#     clf = MLPClassifier(random_state=42,\n#                         solver='adam',\n#                         early_stopping=True,\n#                         activation=activation,\n#                         alpha=alpha,\n#                         learning_rate='adaptive',\n#                         learning_rate_init=0.01,\n#                         batch_size=32,\n#                         hidden_layer_sizes=(layers))\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV)\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials =500,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trial running result:  \nBest model parameters:{'n_layers': 1, 'n_units_1': 160, 'activation': 'logistic', 'alpha': 0.00010276620717757648}   \nBest score: 0.832779"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLP_param =study.best_params\nMLP_best= MLPClassifier(random_state=42,\n                        solver='adam',\n                        early_stopping=True,\n                        activation='relu',\n                        alpha= 0.0002746340910250398,\n                        learning_rate='adaptive',\n                        learning_rate_init=0.01,\n                        batch_size=32,\n                        hidden_layer_sizes=160)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensembling"},{"metadata":{},"cell_type":"markdown","source":"#### **Bagging**"},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search\n# n_estimators=[40,50,60,500] #The number of trees in the forest.\n# # criterion=['gini', 'entropy']#The function to measure the quality of a split. \n# max_depth=[3,4,5,7]#The maximum depth of the tree.\n# min_samples_split = [3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# # oob_score=['True','False']\n# hyper={'n_estimators':n_estimators, 'max_depth':max_depth,\n#       'min_samples_split':min_samples_split,'max_features':max_features,\n#        'min_samples_leaf':min_samples_leaf}\n# gd=GridSearchCV(estimator=RandomForestClassifier(random_state=42,oob_score=True,criterion='gini'),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# RF_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of trees in the forest.\n#     max_depth=trial.suggest_int('max_depth',1,5,1)#The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split',20,200,20) #The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf',20,200,20) # minimum num of samples required to be a leaf node\n#     # define classifier\n#     clf = RandomForestClassifier(random_state=42,\n#                                 criterion='gini',\n#                                 oob_score=True,\n#                                 max_depth=max_depth,\n#                                 min_samples_split=min_samples_split,\n#                                 min_samples_leaf=min_samples_leaf,\n#                                 n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV, scoring='roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 Trials running results:  \nBest model parameters:{'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 40, 'min_samples_leaf': 20}   \nBest score: 0.881499"},{"metadata":{"trusted":true},"cell_type":"code","source":"# RF_param =study.best_params\nRF_param={'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 40, 'min_samples_leaf': 20}\nRF_best= RandomForestClassifier(random_state=42,\n                                criterion='gini',\n                                oob_score=True,\n                                **RF_param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ExtraTreesClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search\n# n_estimators=[40,50,60,500] #The number of trees in the forest.\n# # criterion=['gini', 'entropy']#The function to measure the quality of a split. \n# max_depth=[3,4,5,7,10]#The maximum depth of the tree.\n# min_samples_split = [3,0.01,0.05,0.1] #The minimum num of samples required to split an internal node.\n# min_samples_leaf = [3,5,7,0.1] # minimum num of samples required to be a leaf node\n# max_features=['auto'] #The number of features to consider when looking for the best split\n# hyper={'n_estimators':n_estimators, 'max_depth':max_depth,\n#       'min_samples_split':min_samples_split,'max_features':max_features}\n# gd=GridSearchCV(estimator=ExtraTreesClassifier(random_state=42,criterion='gini'),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# RF_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of trees in the forest.\n#     max_depth=trial.suggest_int('max_depth',1,5,1)#The maximum depth of the tree.\n#     min_samples_split = trial.suggest_int('min_samples_split',20,200,20) #The minimum num of samples required to split an internal node.\n#     min_samples_leaf = trial.suggest_int('min_samples_leaf',20,200,20) # minimum num of samples required to be a leaf node\n#     # define classifier\n#     clf = ExtraTreesClassifier( random_state=42,\n#                                 criterion='gini',\n#                                 max_depth=max_depth,\n#                                 min_samples_split=min_samples_split,\n#                                 min_samples_leaf=min_samples_leaf,\n#                                 n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 Trials running result:  \nBest model parameters:{'n_estimators': 250, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 20}   \nBest score: 0.875066"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ET_params = study.best_params\nET_params = {'n_estimators': 250, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 20} \nET_best = ExtraTreesClassifier( random_state=42,\n                                criterion='gini',\n                                n_jobs=-1,\n                               **ET_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search parameter tuning\n# n_estimators=[5,10,20,50,100]\n# base_estimator__C=[0.4,0.8,1,2,5] # Regularization parameter.\n# base_estimator__gamma=['scale','auto',0.01,0.1,0.5,1]\n# base_estimator__kernel=['rbf']\n# # base_estimator__degree=[3,5,7] # Degree of the polynomial kernel \n# hyper={'n_estimators':n_estimators,\n#        'base_estimator__C':base_estimator__C,\n#        'base_estimator__gamma':base_estimator__gamma,\n#        'base_estimator__kernel':base_estimator__kernel}\n# gd=GridSearchCV(estimator=BaggingClassifier(base_estimator=SVC(),random_state=42),\n#                 param_grid=hyper,verbose=2,n_jobs=-1,cv=CV,refit=True,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# SVMB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',50,500,50) #The number of base estimators\n#     base_estimator__C=trial.suggest_loguniform('base_estimator__C',1e-5,1e3)# Regularization parameter.\n#     base_estimator__gamma = trial.suggest_loguniform('base_estimator__gamma',1e-5,1e3) # kenerl coefficient\n#     # define classifier\n#     clf = BaggingClassifier(base_estimator=SVC(C=base_estimator__C,\n#                                                gamma=base_estimator__gamma,\n#                                                kernel = 'rbf'),\n#                             random_state=42,\n#                             n_jobs=-1)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trials running result:  \nBest model parameters:{'n_estimators': 300, 'base_estimator__C': 3.232901108594473, 'base_estimator__gamma': 0.07183110256410177}   \nBest score: 0.870113"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVMB_params = study.best_params\nSVMB_params = {'n_estimators': 300, 'base_estimator__C': 3.232901108594473, 'base_estimator__gamma': 0.07183110256410177}  \nSVMB_best = BaggingClassifier(base_estimator=SVC(kernel = 'rbf',probability=True,C=3.232901108594473,gamma=0.07183110256410177),\n                              random_state=42,n_jobs=-1,n_estimators=350)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boosting"},{"metadata":{},"cell_type":"markdown","source":"#### Adaboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_estimators=list(range(30,300,10))\n# learn_rate=[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n# base_estimator__max_depth=[1,2,3]\n# hyper={'n_estimators':n_estimators,'learning_rate':learn_rate,\n#       'base_estimator__max_depth':base_estimator__max_depth,}\n# gd=GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini'),\n#                                              random_state=42),param_grid=hyper,verbose=2,\n#                                             cv=CV,refit=True,n_jobs=-1,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# ADB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',40,500,20) #The number of base estimators\n#     learn_rate=trial.suggest_loguniform('learn_rate',1e-5,0.1)\n#     base_estimator__max_depth=trial.suggest_int('base_estimator__max_depth',1,5,1)\n#     # define classifier\n#     clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=base_estimator__max_depth,\n#                                                                   criterion='gini'),\n#                             learning_rate=learn_rate,\n#                             n_estimators=n_estimators,\n#                             random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trials running results:  \nBest model parameters:{'n_estimators': 160, 'learn_rate': 0.03990900089241141, 'base_estimator__max_depth': 2}   \nBest score: 0.913918"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADB_param =study.best_params\nADB_best= AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',\n                                                                  max_depth=2),\n                            learning_rate=0.03990900089241141 ,\n                            n_estimators=160,\n                            random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search parameter tunning\n# n_estimators=list(range(20,150,10))\n# learn_rate=[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n# max_depth=[3,5,8,10]\n# min_samples_split=np.linspace(0.1, 0.3, 4)\n# criterion=['friedman_mse','mae']\n# max_features=['log2','sqrt']\n# hyper={'n_estimators':n_estimators,\n#        'learning_rate':learn_rate,\n#        'max_depth':max_depth,\n#        'criterion':criterion,\n#        'min_samples_split':min_samples_split,\n#        'max_features':max_features}\n# gd=GridSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n#                 param_grid=hyper,verbose=2,refit=True,n_jobs=-1,\n#                cv=CV,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# GDB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators=trial.suggest_int('n_estimators',40,500,20) #The number of base estimators\n#     learning_rate=trial.suggest_loguniform('learning_rate',1e-5,0.1)\n#     max_depth=trial.suggest_int('max_depth',1,5,1)\n#     min_samples_split=trial.suggest_int('min_samples_split',20,200,20)\n#     min_samples_leaf=trial.suggest_int('min_samples_leaf',20,200,20)\n#     # define classifier\n#     clf = GradientBoostingClassifier(max_depth=max_depth,\n#                                      min_samples_split=min_samples_split,\n#                                      min_samples_leaf=min_samples_leaf,\n#                                      learning_rate=learning_rate,\n#                                      n_estimators=n_estimators,\n#                                      subsample=0.8,\n#                                      n_iter_no_change=10,\n#                                      random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # create study\n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 Trials running results:\nBest model parameters:{'n_estimators': 420, 'learning_rate': 0.08199591231901683, 'max_depth': 3, 'min_samples_split': 140, 'min_samples_leaf': 20}   \nBest score: 0.910832"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GDB_param = study.best_params\nGDB_param = {'n_estimators': 420, 'learning_rate': 0.08199591231901683, \n             'max_depth': 3, 'min_samples_split': 140, 'min_samples_leaf': 20} \nGDB_best= GradientBoostingClassifier(**GDB_param,\n                                     subsample=0.8,\n                                     n_iter_no_change=10,\n                                     random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost"},{"metadata":{},"cell_type":"markdown","source":"1. Grid Seach hyperparameters with 5 fold cross validataion.\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Grid Search hyperparameter tuning\n# #tree features\n# max_depth=[5,8]\n# # subsamples=[0.8,0.5]# the fraction of observations to be randomly samples for each tree.\n# colsample_bytree=[0.5,0.8]# the fraction of columns to be randomly samples for each tree.\n# gamma = [0.1,0.3,0.5] #Minimum loss reduction required to make a further partition on a leaf node of the tree.\n# min_child_weight =[1,3,5] # Minimum sum of instance weight(hessian) needed in a child.\n# # boosting features\n# n_estimators=[25,50,90,120] #Number of gradient boosted trees(rounds).\n# learning_rate =[0.01, 0.25, 0.1]\n# # learning_rate =[0.01, 0.025, 0.05, 0.1, 0.15, 0.2]# Boosting learning rate (xgbs eta), typically in [0.01,0.2].\n# booster=['dart'] # tree booster always better than linear.\n# # regularization\n# #reg_alpha=[1e-5, 1e-2, 0.1] # L1 regularization term on weights\n# reg_lambda =[1e-5, 1e-2] # L2 regularization term on weights\n# hyper={'n_estimators':n_estimators,\n#        'learning_rate':learn_rate,\n#        'booster':booster,\n#        'max_depth':max_depth,\n#        'colsample_bytree':colsample_bytree,\n#        'gamma':gamma,\n#        'min_child_weight':min_child_weight,\n#        'reg_lambda':reg_lambda} \n# gd=GridSearchCV(estimator=XGBClassifier(verbosity=1,n_jobs =-1,random_state=42),\n#                 param_grid=hyper,verbose=2,refit=True,n_jobs=-1,\n#                cv=CV,scoring = 'accuracy')\n# gd.fit(X,Y)\n# print('Best evaluation score:{:.6f}'.format(gd.best_score_))\n# XGB_best=gd.best_estimator_\n# print('Best parameters:{}'.format(gd.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # hyperparameter tuning using optuna framework\n# def objective(trial):\n#     # define sample space and distibution of parameters\n#     max_depth = trial.suggest_int(\"max_depth\", 1,5,1)\n#     n_estimators = trial.suggest_int(\"n_estimators\", 40, 500, 20)\n#     booster = trial.suggest_categorical('booster',['gbtree','gblinear','dart'])\n#     min_child_weight = trial.suggest_int('min_child_weight',5,105,10)\n#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-6,1e-3)\n#     gamma = trial.suggest_loguniform('gamma', 0.00001, 100)\n#     reg_alpha = trial.suggest_loguniform('reg_alpha',1e-3,1e2) # L1 regularization term on weights.\n#     reg_lambda = trial.suggest_loguniform('reg_lambda',1e-3,1e2) # L2 regularization term on weights.\n#     colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree',0.4,0.8,0.2) # sub-features to use \n#     subsample = trial.suggest_discrete_uniform('subsample',0.8,1.0,0.1) # subsamples to use\n#     # define classifier\n#     clf = XGBClassifier(objective='binary:logistic',\n#                         booster = booster,\n#                         subsample=subsample,\n#                         colsample_bytree=colsample_bytree,\n#                         learning_rate=learning_rate,\n#                         n_estimators=n_estimators,\n#                         max_depth=max_depth,\n#                         gamma=gamma,\n#                         reg_alpha=reg_alpha,\n#                         reg_lambda=reg_lambda,\n#                         n_jobs=-1,\n#                         random_state=42)\n#     # defin evaluation matrix\n#     score = cross_val_score(clf, X, Y, n_jobs=-1, cv=CV,scoring = 'roc_auc')\n#     accuracy = score.mean()\n#     return accuracy\n# # define optimizor's direction and sample algorithms \n# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n# # run optimizor with n_trials to find best parameters\n# s=study.optimize(objective, n_trials=500, n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"500 trails running result:  \nBest model parameters:{'max_depth': 5, 'n_estimators': 500, 'booster': 'dart', 'min_child_weight': 45, 'learning_rate': 0.00028818174062883895, 'gamma': 0.0701754028803822, 'reg_alpha': 0.09673762960851098, 'reg_lambda': 0.020973617864068886, 'colsample_bytree': 0.6000000000000001, 'subsample': 1.0}   \nBest score: 0.908897"},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB_param = study.best_params\nXGB_param = {'max_depth': 5, 'n_estimators': 500, 'booster': 'dart', \n             'min_child_weight': 45, 'learning_rate': 0.00028818174062883895, \n             'gamma': 0.0701754028803822, 'reg_alpha': 0.09673762960851098, 'reg_lambda': 0.020973617864068886, \n             'colsample_bytree': 0.6000000000000001, 'subsample': 1.0} \nXGB_best= XGBClassifier(**XGB_param,\n                        objective='binary:logistic',\n                        n_jobs=-1,\n                        random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Evaluation before Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"@ignore_warnings(category=ConvergenceWarning)\ndef model_eval():\n    acc_mean = []\n    std = []\n    acc = []\n    classifiers=['Svm','LR','KNN','DT','MLP','RF','ADB','ET','XGB','GDB','SVMB']\n    models=[SVM_best,LR_best,KNN_best,DT_best,MLP_best,RF_best,ADB_best,ET_best,XGB_best,GDB_best,SVMB_best]\n    for model in models:\n        cv_result = cross_val_score(model,X,Y, cv = 5,scoring = \"roc_auc\")\n        acc_mean.append(cv_result.mean())\n        std.append(cv_result.std())\n        acc.append(cv_result)\n    performance_df=pd.DataFrame({'CV_Mean':acc_mean,'Std':std},index=classifiers)\n    return performance_df\n\nperformance_df = model_eval()\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Voting and Stacking"},{"metadata":{},"cell_type":"markdown","source":"#### Voting classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('XGB', XGB_best),\n                                       ('RF',RF_best),\n                                       ('ADB',ADB_best),\n                                       ('ET', ET_best),\n                                       ('GDB',GDB_best),\n                                       ('DT',DT_best)],\n                           voting='hard', n_jobs=-1,verbose=True)\nvotingC.fit(X,Y)\n\ncv_result = cross_val_score(votingC,X,Y, cv = 5,scoring = \"accuracy\")\nvot_acc = cv_result.mean()\nvot_std = cv_result.std()\nperformance_df.loc['Voting'] = {'CV_Mean':vot_acc, 'Std':vot_std}\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"stacking = StackingClassifier(estimators=[('XGB', XGB_best),\n                                       ('RF',RF_best),\n                                       ('ADB',ADB_best),\n                                       ('ET', ET_best),\n                                       ('GDB',GDB_best),\n                                       ('DT',DT_best)],\n                             final_estimator=LogisticRegression(),\n                             cv=5,\n                             n_jobs=-1)\ncv_result = cross_val_score(stacking,X,Y, cv = 5,scoring = \"accuracy\")\nstk_acc = cv_result.mean()\nstk_std = cv_result.std()\nperformance_df.loc['Stacking'] = {'CV_Mean':stk_acc, 'Std':stk_std}\nperformance_df.sort_values(by=['CV_Mean'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation "},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = XGB_best\nrf.fit(X,Y)\nfeatures = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = rf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=False, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Observation:***  \nSurprisingly, Family_survived feature has the second importance in our model. "},{"metadata":{},"cell_type":"markdown","source":"#### Learn from misclassified data"},{"metadata":{},"cell_type":"markdown","source":"Fit classifier on training set and select misclassified data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data\nX_tr, X_te, Y_tr, Y_te = train_test_split(X,Y)\n# fit model\nclf = RF_best.fit(X_tr,Y_tr)\n# find misclassified data in test set\nmis_df = X_te[np.logical_xor(Y_te,clf.predict(X_te))] \n# show orginal data\nmis_df=train.loc[mis_df.index]\nmis_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_raw_train.loc[mis_df.index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_df.CV_Mean.idxmax()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based my submission score, Random Forest and Voting Classifier has the best score on public leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_model = RF_best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = sub_model.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=False, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RF_best.fit(X,Y)\nsub = clf.predict(X_test)\nsub_pd = pd.DataFrame({'PassengerId':titanic_raw_test.PassengerId,'Survived':sub})\nsub_pd.to_csv('submit.csv' ,index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Change Log  \n* 2020/9/6  \n1. Add MLP classifier and SVM bagging.  \n* 2020/9/7  \n 1. Analyze feature importance. Find that after binning continuous features('Age' and 'Fare'), feature importance dropped a lot.Other than binning, add scaled continuous features.  \n 2.  Increase number of bins.\n 3. Instead of using 'Title' mean, use random forest to predict missing value.(forget to deploy on test data!)  \n* 2020/9/8  \n 1.  Add skewness analysis.\n 2.  Try Optuna framework.  \n* 2020/9/9  \n 1.  Try StratifiedKFold cross validator.\n 2.  Visualize Decision Tree\n* 2020/9/10  \n 1.  Hyperparameter tuning\n 2.  Try ExtraTreeClassifier \n 3.  Add 'Learn from misclassified data\n* 2020/9/11  \n 1.  More feature engineering.(Family Type)\n 2.  Hyperparameter tuning\n 3.  Add stack model.\n* 2020/9/28 \n 1. Add Ticket_code and family_survived\n 2. Try label encoder to reduce feature \n 3. Try ROC-AUC as evaluation matrix in parameter tuning to reduce overfitting.\n"},{"metadata":{},"cell_type":"markdown","source":"# Reference: \n * https://www.kaggle.com/shaochuanwang/titanic-ml-tutorial-on-small-dataset-0-82296/notebook\n * https://www.kaggle.com/startupsci/titanic-data-science-solutions\n * https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n * https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}