{"cells":[{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:red\">Introduction</span>\n#### A glance over the data shows that it allows for various studies into different aspects of the popularity of some YouTube videos (or lack-there-of). In particular perhaps, the types of videos which happen to be more contentious. So let's look at the ESPN data for example:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nespn = pd.read_csv(\"../input/youtube-video-statistics/ESPN.csv\")\nespn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:red\">Assembling a table for analysis</span>\n#### One way of managing the complexity of so many videos, is clustering them into several topics. To do so, first we need to vectorize the \"title\" column:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\nclass LemmaTokenizer: # Keeping terms composed of only alphabets\n  def __init__(self):\n   self.wnl = WordNetLemmatizer()\n  def __call__(self, doc):\n   return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if re.match(r'(?u)\\b[A-Za-z]+\\b',t)] \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef warn(*args, **kwargs): # Turning off the warnings\n    pass\nimport warnings\nwarnings.warn = warn\n\nvectorizer = TfidfVectorizer(lowercase = False, tokenizer=LemmaTokenizer(), stop_words='english', min_df=0.001, max_df=0.99)\ncorpus = vectorizer.fit_transform(espn['title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now that we have the \"corpus\" (in the form of a sparse matrix), we can dig out topics using a suitable matrix factorization.    \n\n#### The classic method here would be SVD (Singular Value Decomposition). SVD in particular, generates a byproduct called sigma matrix which is useful in some applications . SVD is also an exact factorization method. There is also NMF (Nonnegative Matrix Factorization), which may not deliver the mentioned perks, yet is otherwise fast and straightforward. In particular, NMF unlike SVD, does not produce negative values, which turn out to be tricky when it comes to interpretation (for more information on how NMF compares to SVD you can check out [here](https://medium.com/@nixalo/comp-linalg-l2-topic-modeling-with-nmf-svd-78c94330d45f) and [here](https://discuss.analyticsvidhya.com/t/how-is-svd-different-from-other-matrix-factorization-techniques-like-non-negative-matrix-factorization/67519/4)). So long story short, let's choose NMF over SVD to produce 20 topics (which sounds enough for sports category):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF \nmodel = NMF(n_components=20, init='random', random_state=0)\ncorpus_by_topics = model.fit_transform(corpus) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Next I'll assign the most relevant topic to each document in the corpus (corresponding to each title in the main table).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_index_in_row = np.argmax(corpus_by_topics, 1)\nmax_value_in_row = np.amax(corpus_by_topics, 1)\n\nprint(max_index_in_row)\nprint(max_value_in_row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can statistically look at how accurately the assigned topic describes each document: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nstats.describe(max_value_in_row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### An average of almost 0.2 (with a rather small variance), doesn't seem to be terrible for 20 topics. Next I'll assemble a new table to work on: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topic = pd.Series(max_index_in_row)\nrelevance = pd.Series(max_value_in_row)\nframe = {'topic':topic, 'relevance':relevance}\ndf = pd.DataFrame(frame)\n\nespn_derived = pd.concat([df, espn[['likes', 'dislikes', 'comments']]], axis=1, sort=False)\nespn_derived.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning up the invalid / poor values which happen to exist in the dataframe:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting rid of negative values\nstats.describe(espn_derived['comments'])\nespn_derived['comments'] = np.where(espn_derived['comments'] < 0, 0, espn_derived['comments'] )\n\n#removing outliers\nz_scores = stats.zscore(espn_derived)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nespn_derived = espn_derived[filtered_entries]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:red\">Topic analysis</span>\n#### Visualizing five dimensions of the dataframe: likes (x-axis), dislikes (y-axis), topic (color), number of comments (bubble size), the relevance of the topic (transparency of the bubble):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"white\")\nimport matplotlib.pyplot as plt\n\n#used https://mokole.com/palette.html to generate 20 visually distinct colors\ncolors=[ \"#696969\", \"#ffe4c4\", \"#2e8b57\", \"#8b0000\", \"#808000\", \"#000080\", \"#ff0000\", \"#ff8c00\", \"#ffd700\", \"#ba55d3\", \"#00ff7f\", \"#00bfff\", \"#0000ff\", \"#adff2f\", \"#ff00ff\", \"#f0e68c\", \"#fa8072\", \"#dda0dd\", \"#ff1493\", \"#7fffd4\"]\n\nplt.figure(figsize=(15,8))\n\n# it is not possible to parametrize alpha in Seaborn; so I define three levels of alphas and draw them seperately.\nespn_derived[\"alpha\"] = np.where(espn_derived['relevance'] < 0.22, 0.1, np.where(espn_derived['relevance'] < 0.4, 0.4, 0.5))\n\nax = sns.scatterplot(x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\", sizes=(1,500), size_norm=(100,4000), alpha=0.1, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.1])\n\nsns.scatterplot(legend=False, ax=ax, x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\",sizes=(1,500), size_norm=(100,4000), alpha=0.4, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.4])\n\nsns.scatterplot(legend=False, ax=ax, x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\", sizes=(1,500), size_norm=(100,4000),alpha=0.5, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.5])\n\nplt.plot([0,3000], [0,3000], color='r')\n\nplt.plot([0,14000], [0,1800], color='b')\n\nplt.legend(bbox_to_anchor=(1, 1), loc=2)\n\nax.legend(ncol=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The red line delineates where the number of likes equals the number of dislikes. So in a way, one can call the points on this line, controversial. But the situation seems to be more nuanced. For example we usually call the points close to the vertical axis also controversial. Or considering the fact that videos normally seem to get more likes than dislikes, something like the blue delineator might have some metirts to it as well.  At any rate, the plot right-off-the-bat, reveals curious materials for inference:    \n* #### Viewers do not seem to comment on videos when they haven't hit like or unlike (as the number of comments are considerably lower around the origin).\n* #### Some of the most liked and unliked videos (bubbles close to the tip of the margins), are not discussed as much.\n* #### The less nuanced topics (videos with lower relevance score), have lesser numbers of likes, dislikes, and comments.\n* #### Some of the most overwhelmingly liked or disliked videos are nuanced topics.   \n\n#### Now regarding the topics, topic #4 seems to be amongst the most contentious. A quick way to get a sense of the topic would be the wordcloud visualization: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorpus_by_topics_model = model.fit(corpus) # the wieght of each term in every topic       \nweight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[4])) # associating the actual terms with their weight\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rather than just particular sports or technical aspects of them, topic #4 centers around some strong words which foment controversy. We may also take a look at topic #17 (associated with some of the most controversial videos):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[17]))\n \nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Topic #17 mostly centers around the basketball personalities with James Leborn on the top (also including Michael Jordan, Dwyane Wade, Tyronn Lue, etc). On the other side of the spectrum, topic #3 seems to be quite favored by most of the viewers:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[3]))\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Topic #3 is about Stephan A Smith (an ESON host) with a significant NFL undertone. Topic #0 is also among the favorites, albeit less than topic #0: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[0]))\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Basically the same ESPN host, but here with a heavy NBA undertone.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}