{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Notebook on Predicting Amsterdam Airbnb listing price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Opinions are my own.\n\nThis notebook has served as a playing ground for me to explore the Airbnb Amsterdam data and try to predict listing prices through several different methods, including:\n\n* Linear Regression\n* Random Forrest Regression\n* OLS\n* H2O Auto ML\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Getting up and running\nImport all the libaries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import all the necessary libraries \n\n# commonly used libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# visualization library\nimport seaborn as sns\n\n# data manipulation utility libraries\nimport distutils\nimport datetime\nimport re\n\n# sklearn libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# stats library\nfrom scipy import stats\nfrom scipy.stats import boxcox\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data\ndf = pd.read_csv('../input/airbnb-amsterdam/listings_details.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at top rows of dataframe\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Quick summary of dataframe\ndf.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of rows and columns\ndf.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sum of NaN values in price column\nnp.sum(df.price.notnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# price column\ndf.price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out all columns with numeric values\nnum_vars = df.select_dtypes(include=['float', 'int']).columns\nnum_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out all columns with categorical values\nnum_cat = df.select_dtypes(include=['object']).columns\n\nnum_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check values in different neighbourhood columns\ndf[['neighbourhood','neighborhood_overview','neighbourhood_cleansed']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see ratio of categorical values \ndf.neighbourhood_cleansed.value_counts() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preparation**\n\n* Dropping many columns\n* Cleaning some data quality issues\n* Cutting outliers based on mod-z\n* Create dummy variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns that are irelevant \ndf_clean = df.drop(['id', 'scrape_id', 'thumbnail_url', 'medium_url', 'xl_picture_url',\n              'host_id', 'host_total_listings_count', 'neighbourhood_group_cleansed',\n              'latitude','longitude', 'calculated_host_listings_count', \n              'listing_url', 'last_scraped', 'name', 'summary', 'space',\n              'description', 'experiences_offered', 'neighborhood_overview', 'notes',\n              'transit', 'access', 'interaction', 'house_rules', 'picture_url',\n              'host_url', 'host_name', 'host_location', 'host_about',\n              'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood','host_verifications',\n              'street', 'neighbourhood', 'city', 'state',\n              'zipcode', 'market', 'smart_location', 'country_code', 'country',\n              'weekly_price', 'monthly_price','security_deposit', 'cleaning_fee',\n              'extra_people', 'calendar_last_scraped', 'requires_license', 'license',\n              'jurisdiction_names','guests_included','host_response_time','host_response_rate',\n              'host_acceptance_rate','square_feet'\n             ], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# #Drop all NaN values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all NaN values\ndf_clean = df_clean.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get rid of string items in Price column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use string.replace to get rid of string items in price column\ndf_clean = df_clean.assign(price=df_clean['price'].str.replace(r'$', ''))\ndf_clean = df_clean.assign(price=df_clean['price'].str.replace(r',', ''))\n\n#Set price as float type\ndf_clean['price'] = df_clean['price'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get boolean expression from 'f'& 't' string","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use lambda and distutils to go from string to boolean expression\ndf_clean = df_clean.assign(host_is_superhost=df_clean['host_is_superhost'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(host_has_profile_pic=df_clean['host_has_profile_pic'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(host_identity_verified=df_clean['host_identity_verified'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(is_location_exact=df_clean['is_location_exact'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(instant_bookable=df_clean['instant_bookable'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(is_business_travel_ready=df_clean['is_business_travel_ready'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(require_guest_profile_picture=df_clean['require_guest_profile_picture'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(require_guest_phone_verification=df_clean['require_guest_phone_verification'].apply(lambda x: bool(distutils.util.strtobool(x))))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify usefull amenities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# find amenity availability by amenity\ndf_clean = df_clean.assign(has_tv=df_clean['amenities'].apply(lambda x: x.find('Wifi') != -1))\ndf_clean = df_clean.assign(has_fireplace=df_clean['amenities'].apply(lambda x: x.find('Indoor fireplace') != -1))\ndf_clean = df_clean.assign(has_kitchen=df_clean['amenities'].apply(lambda x: x.find('Kitchen') != -1))\ndf_clean = df_clean.assign(has_family_friendly=df_clean['amenities'].apply(lambda x: x.find('Family/kid friendly') != -1))\ndf_clean = df_clean.assign(has_host_greeting=df_clean['amenities'].apply(lambda x: x.find('Host greets you') != -1))\ndf_clean = df_clean.assign(has_24hrs_checkin=df_clean['amenities'].apply(lambda x: x.find('24-hour check-in') != -1))\ndf_clean = df_clean.assign(has_breakfast=df_clean['amenities'].apply(lambda x: x.find('Breakfast') != -1))\ndf_clean = df_clean.assign(has_pets=df_clean['amenities'].apply(lambda x: x.find('Pets live on this property') != -1))\ndf_clean = df_clean.assign(has_dishwasher=df_clean['amenities'].apply(lambda x: x.find('Dishwasher') != -1))\ndf_clean = df_clean.assign(has_private_entrance=df_clean['amenities'].apply(lambda x: x.find('Private entrance') != -1))\ndf_clean = df_clean.assign(has_patio_balcony=df_clean['amenities'].apply(lambda x: x.find('Patio or balcony') != -1))\ndf_clean = df_clean.assign(has_self_checkin=df_clean['amenities'].apply(lambda x: x.find('Self check-in') != -1))\ndf_clean = df_clean.assign(has_workspace=df_clean['amenities'].apply(lambda x: x.find('Laptop friendly workspace') != -1))\ndf_clean = df_clean.assign(has_bathtub=df_clean['amenities'].apply(lambda x: x.find('Bathtub') != -1))\ndf_clean = df_clean.assign(has_longterm=df_clean['amenities'].apply(lambda x: x.find('Long term stays allowed') != -1))\ndf_clean = df_clean.assign(has_parking=df_clean['amenities'].apply(lambda x: x.find('Free parking on premises') != -1))\ndf_clean = df_clean.assign(has_garden=df_clean['amenities'].apply(lambda x: x.find('Garden or backyard') != -1))\n\n# drop amenities column\ndf_clean = df_clean.drop(['amenities'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use datetime calculation to get days metric for Host Since","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create days delta calculation function\nday_calc = lambda x: (datetime.date.today() - datetime.datetime.strptime(x, \"%Y-%m-%d\").date()).days\n\n# apply on host_since column\ndf_clean = df_clean.assign(host_since=df_clean['host_since'].apply(day_calc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop some more columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns that will not add more value than host since column\ndf_clean = df_clean.drop(['first_review','last_review'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# max price\ndf_clean.price.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio of occurrences of value by certain column\ndf_clean.number_of_reviews.value_counts() / df_clean.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio of occurrences of value by certain column\ndf_clean.minimum_nights.value_counts() / df_clean.shape[0]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check boxplot for price\nsns.boxplot(x=df_clean['price'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cut outliers for Price using Mod-z ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create mod_z function (copied from: https://stackoverflow.com/questions/58127935/how-to-calculate-modified-z-score-and-iqr-on-each-column-of-the-dataframe)\ndef mod_z(col: pd.Series, thresh: float=3.5) -> pd.Series:\n    med_col = col.median()\n    med_abs_dev = (np.abs(col - med_col)).median()\n    mod_z = 0.7413 * ((col - med_col) / med_abs_dev)\n    mod_z = mod_z[np.abs(mod_z) < thresh]\n    return np.abs(mod_z)\n\n# run mod_z function on dataframe\ndf_mod_z = df_clean.select_dtypes(include=[np.number]).apply(mod_z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply above function to price \ndf_clean_filtered = df_clean[df_mod_z['price'] >= 0]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['price'] > 0]\n\n#check shape\ndf_clean_filtered.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check summary\ndf_clean_filtered.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cut outliers with hardcoded parameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cut outliers\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['host_listings_count'] < 60]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['bathrooms'] < 20]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['beds'] < 20]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['minimum_nights'] < 365]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['maximum_nights'] < 2000]\n\n#Check shape\ndf_clean_filtered.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop a few final columns\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop some  because they're too tricky \ndf_clean_filtered_drop = df_clean_filtered.drop(['calendar_updated','has_availability'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create dummy variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dummy in dataframe function\ndef create_dummy_df(df, cat_cols, dummy_na):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as categorical\n            2. removes all the original columns in cat_cols\n            3. dummy columns for each of the categorical columns in cat_cols\n            4. if dummy_na is True - it also contains dummy columns for the NaN values\n            5. Use a prefix of the column name with an underscore (_) for separating \n    '''\n    for col in  cat_cols:\n        try:\n            # for each cat add dummy var, drop original column\n            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n        except:\n            continue\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set categorical columns to be dummied\ncat_cols_lst = df_clean_filtered_drop.select_dtypes(include=['object']).columns\n\n#Apply create dummy function\ndf_model = create_dummy_df(df_clean_filtered_drop, cat_cols_lst, dummy_na=False) #Use your newly created function\n\n#check df\ndf_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check normal distribution of Response variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot distribution of Price\nplt.figure(figsize=(15,10))\nplt.tight_layout()\nsns.distplot(df_model['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Normalize some columns to combine them\n* Transform some data to create normal distribution of input variables\n* Standardization using StandardScaler","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Normalize and combine ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Normalizer from sklearn\nfrom sklearn.preprocessing import Normalizer \n\n#Set variables to normalize\nnorm_vars = ['beds','bedrooms','accommodates']\n\n# initiate normalizer and apply to scaled data array\nnormalize = Normalizer().fit(df_model[norm_vars])\nnorm_array = normalize.transform(df_model[norm_vars])\n\n# create a DataFrame from the array\ndf_model_norm_vars = pd.DataFrame(norm_array, columns = norm_vars, index = df_model.index)\n\n# merge new DataFrame with full dataframe model \ndf_model_merged = pd.merge(df_model_norm_vars,df_model.drop(norm_vars,axis=1), right_index=True, left_index=True)\n\n# create combined column for beds, bedrooms and acommodates\ndf_model_merged['combine_beds_bedrooms_acommodates'] = df_model_merged['beds'] + df_model_merged['bedrooms'] + df['accommodates']\n\n# drop already combined variables\ndf_model_merged = df_model_merged.drop(['beds','bedrooms','accommodates'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Check normal distribution of all numerical input variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms of all vars with power transformation BoxCox\n\n# set vars to check\ncheck_vars = ['host_since', 'host_listings_count', 'bathrooms',\n       'price', 'minimum_nights', 'maximum_nights',\n       'number_of_reviews', 'review_scores_rating',\n       'review_scores_communication', 'review_scores_location',\n       'review_scores_value','combine_beds_bedrooms_acommodates',\n           'reviews_per_month','availability_30', 'availability_60','availability_90', 'availability_365',\n            'review_scores_accuracy','review_scores_cleanliness', 'review_scores_checkin']\n\n# For loop on showing separate histograms per item\ni = 0\nfor x in check_vars:\n    # set data\n    data = df_model_merged[x]\n    \n    # plot\n    plt.figure(i)\n    plt.title(x)\n    plt.hist(data)\n    print(plt.figure(i))\n    \n    # iterate \n    i = i + 1\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of variables that need to be transformed to fit a normal distribution\n\nto_check_vars = ['price','host_listings_count','bathrooms','minimum_nights','maximum_nights','number_of_reviews','review_scores_rating',\n            'review_scores_communication','review_scores_location','review_scores_value','review_scores_accuracy',\n             'review_scores_cleanliness','review_scores_checkin',\n             'combine_beds_bedrooms_acommodates',\n            'reviews_per_month'\n           ]\n\n\npoly_vars = ['availability_30','availability_60','availability_90','availability_365']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boxcox transform and check outputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# For loop on showing separate histograms per item\ni = 0\nfor x in to_check_vars:\n    plt.figure(i)\n    plt.title(x)\n    \n    # power transform\n    data = df_model_merged[x] + 1\n    data = boxcox(data)\n    \n    #print the boxcox lambda value\n    print(data[1])\n    \n    #plot the graph\n    plt.hist(data)\n    print(plt.figure(i))\n    \n    #increment the counter\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mark variables that can fit a normal distribution and get rid a few others","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# variables to keep and boxcox transform \nboxcox_vars = ['price','number_of_reviews','review_scores_rating',\n            'review_scores_location','review_scores_value'\n              ,'combine_beds_bedrooms_acommodates']\n\n\n# variables to drop\nto_drop = ['host_listings_count','bathrooms','minimum_nights','maximum_nights',\n           'review_scores_communication','review_scores_accuracy','review_scores_cleanliness',\n           'review_scores_checkin','reviews_per_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop variables\ndf_model_merged = df_model_merged.drop(to_drop,axis=1)\n\n#drop poly variables\ndf_model_merged = df_model_merged.drop(poly_vars,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boxcox transform needed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create boxcox in dataframe function\ndef create_boxcox_df(df, boxcox_vars):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    boxcox_vars - list of strings that are associated with selected boxcox columns\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as boxcox columns\n            2. removes all the original columns in boxcox_vars\n            3. boxcox transforms for each of the boxcox columns in boxcox_vars\n            4. returns df\n            5. returns list of lambda values for maxlog()\n    '''\n    #iniate empty list\n    lambda_list = list()\n    \n    #start for loop\n    for col in boxcox_vars:\n        try:\n            # for each var boxcox transform\n            data = df[col] +1\n            data = boxcox(data)\n            \n            #lambda list append\n            lambda_list.append(data[1])\n            \n            #create dataframe from array\n            df_insert = pd.DataFrame(data[0],columns = [col],index = df_model_merged.index)\n            \n            #concat dataframes\n            df = pd.merge(df_insert,df.drop(col, axis=1), right_index=True, left_index=True)\n\n            \n        except:\n            continue\n    return df, lambda_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply to dataframe\ndf_model_merged, lambda_list = create_boxcox_df(df_model_merged,boxcox_vars=boxcox_vars)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nprint(plt.show())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization with StandardScaler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing preprocessing part \nfrom sklearn.preprocessing import StandardScaler\n\n# take all the variables that we want to standardize \nscaler_vars = ['number_of_reviews','review_scores_rating',\n            'review_scores_location','review_scores_value'\n              ,'combine_beds_bedrooms_acommodates']\n\n\n# initiate standardscaler and apply to data\nsc = StandardScaler()\nscaled_array = sc.fit_transform(df_model_merged[scaler_vars])\n\n# create new dataframe with scaled variables\ndf_model_scaled = pd.DataFrame(scaled_array, columns = scaler_vars, index = df_model.index)\n\n# merge them all back together\ndf_model_merged = pd.merge(df_model_scaled,df_model_merged.drop(scaler_vars,axis=1), right_index=True, left_index=True)\n\n#print(df_model_merged.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nprint(plt.show())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Look at all adapted distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nplt.show\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for Multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['host_since','price','number_of_reviews', 'review_scores_rating', 'review_scores_location','review_scores_value']\n\n#df_plot = df_model_merged.select_dtypes(include=[np.number])\ndf_plot = df_model_merged[num_vars]\n\nmatrix = np.triu(df_plot.corr())\n\nplt.figure(figsize=(12, 9))\nsns.heatmap(df_plot.corr(), annot=False, mask=matrix, linewidths=.5, fmt='.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Take out variables where needed to avoid multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# based on above correlation matrix, take out a few columns\ndf_model_merged = df_model_merged.drop(['review_scores_value'],axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n* Implement Sklearn Linear Regression\n* Refine with some PolyNomial Features\n* Implement Sklearn RandomForestRegressor\n* Implement Statsmodels OLS\n* Implement H2O AutoML\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_linear_mod(df, response_col, test_size=.3, rand_state=42):\n    '''\n    INPUT:\n    df - a dataframe holding all the variables of interest\n    response_col - a string holding the name of the column\n    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n    rand_state - an int that is provided as the random state for splitting the data into training and test \n    \n    OUTPUT:\n    test_score - float - r2 score on the test data\n    train_score - float - r2 score on the test data\n    lm_model - model object from sklearn\n    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n    \n    This function should:\n    1. Split your data into an X matrix and a response vector y\n    2. Create training and test sets of data\n    3. Instantiate a LinearRegression model with normalized data\n    4. Fit your model to the training data\n    5. Predict the response for the training data and the test data\n    6. Obtain an rsquared value for both the training and test data\n    '''\n\n    #Split into explanatory and response variables\n    X = df.drop(response_col, axis=1)\n    y = df[response_col]\n\n    #Split into train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n\n    lm_model = LinearRegression(normalize=True) # Instantiate\n    lm_model.fit(X_train, y_train) #Fit\n\n    #Predict using your model\n    y_test_preds = lm_model.predict(X_test)\n    y_train_preds = lm_model.predict(X_train)\n\n    #Score using your model\n    test_score = r2_score(y_test, y_test_preds)\n    train_score = r2_score(y_train, y_train_preds)\n\n    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use dataframe before preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Test your function with the above dataset\ntest_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds = fit_linear_mod(df_model, 'price')\n\n#Print training and testing score\nprint(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))\n\n# The coefficients\n#print('Coefficients: \\n', lm_model.coef_)\n\n# print RMSE\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use dataframe after preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# after preprocessing\n\n#Test your function with the above dataset\ntest_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds = fit_linear_mod(df_model_merged, 'price')\n\n#Print training and testing score\nprint(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))\n\n# The coefficients\n#print('Coefficients: \\n', lm_model.coef_)\n\n# print RMSE\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA analysis\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying PCA function on training \nfrom sklearn.decomposition import PCA \n  \npca = PCA()\n  \nX_train = pca.fit_transform(X_train) \n  \nexplained_variance = pca.explained_variance_ratio_\nprint(explained_variance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implement Statsmodels OLS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import statsmodel library\nimport statsmodels.api as sm\n\n#set response column and df to use\nresponse_col = 'price'\ndf_to_use_ols = df_model_merged\n\n# set X matrix and y \nX = df_to_use_ols.drop(response_col, axis=1)\nX = sm.add_constant(X)\ny = df_to_use_ols[response_col]\n\n# fit and predict\nest = sm.OLS(y.astype(float), X.astype(float)).fit()\nypred = est.predict(X)\n\n# evaluate\nrmse = np.sqrt(mean_squared_error(y, ypred))\nprint(rmse)\n\n# show stats summary\nest.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implement RandomForestRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=2, random_state=0)\n\nresponse_col = 'price'\ndf_rfr_to_use = df_model_merged\n\nX = df_rfr_to_use.drop(response_col, axis=1)\ny = df_rfr_to_use[response_col]\nregr.fit(X, y)\nmodelPred = regr.predict(X)\n\nprint(\"The R2 score: \",regr.score(X,y))\n\nprint(\"Number of predictions:\",len(modelPred))\n\nmeanSquaredError=mean_squared_error(y, modelPred)\nprint(\"MSE:\", meanSquaredError)\nrootMeanSquaredError = np.sqrt(meanSquaredError)\nprint(\"RMSE:\", rootMeanSquaredError)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implement H2O AutoML solution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n# tutorial: https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/Python/automl_binary_classification_product_backorders.ipynb\n\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\n\ndf_h2o_to_use = df_model_merged\n\n# Identify predictors and response\nx = df.columns.tolist()\ny = \"price\"\nx = x.remove(y)\n\ndf_model_h2o = h2o.H2OFrame(df_h2o_to_use)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1)\naml.train(x=x, y=y, training_frame=df_model_h2o)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}