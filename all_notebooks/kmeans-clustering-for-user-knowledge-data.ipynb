{"cells":[{"metadata":{},"cell_type":"markdown","source":"# User Knowledge Clustering using K-Means\n## Overview\n### Background\nUser knowledge is where a person who uses knowledge generated through research to make informed decisions about policies, programs and/or best practices. This dataset was part of the Ph.D. Thesis of Dr. Hamdi Tolga Kahraman from Turkey back in 2009. It is an unlabelled dataset containing 5 features as explained in the column descriptions. The data contains real information about the student's knowledge status about the subject of Electrical DC Machines. The insipiration is to find clusters within data, explore their properties and make meaningful conclusions and actions / recommendations that might add value to the process from where the data was obtained.\n\n### K-Means Clustering\nClustering is process of grouping of objects into groups that have same or similar properties. K-means is an algorithm in unsupervised learning that tries to partition or group data into K groups, where 1 k point is only owned by 1 group and the characteristics of the data in one group have a high similarity while the characteristics of data between gorups are different. In k means we will know several terms, i.e.:\n- Cluster Centroids: The cluster centroid is the most representative point of a specific cluster. So, if we decide to find three clusters, we will have three cluster centroid.\n- Euclidean Distance: Is the distance between two data points and this term is essential when gathering the distance between the cluster centroids and the data points.\n- Elbow Method: The elbow method is a technique used to choose the most optimal number of clusters. Remember, in Kmeans clustering we add the number of clusters in a manual way, so the elbow method is useful when using Kmeans. \n\n**Criteria to produce good cluster**\n- The similarity between members in a cluster (intracluster) is high\n- Similarities between members of different clusters (inter-cluster) is low\n\n**The way kmeans algorithm works**\n- Specify number of clusters K.\n- Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n    - Compute the sum of the squared distance between data points and all centroids.\n    - Assign each data point to the closest cluster (centroid).\n    - Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n- Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isnâ€™t changing.\n\n## Packages\nThe main module in python used for clustering is `scikit-learn`. Whereas for visualization here I use `plotly`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly as py\nimport plotly.graph_objs as go\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation \nFirst of all, we read the user knowledge dataset to see how far we can dig from the dataset."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input\")\nusers = pd.read_csv('data_student.csv', delimiter=',')\nusers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"users.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The user knowledge dataset has 6 columns and 258 observations. The values in the criteria column for measuring user knowledge are numeric. Variable `UNS` has 4 unique values that represented knowledge level of users. The description of each variable is as follows:\n- STG : The degree of study time for goal object materails\n- SCG : The degree of repetition number of user for goal object materails\n- STR : The degree of study time of user for related objects with goal object\n- LPR : The exam performance of user for related objects with goal object\n- PEG : The exam performance of user for goal objects\n- UNS : The knowledge level of user"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"users.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results above we know that the dataset we have doesn't have missing values and duplicates data. \nThe statistical summary below shows that each variable has a data distribution that is not very different."},{"metadata":{"trusted":true},"cell_type":"code","source":"users.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data"},{"metadata":{},"cell_type":"markdown","source":"In the previous stage we have done data tidying. After our data is tidy, the next step before entering clustering is we can explore the data to see the distribution and allow to see hidden patterns in the data. Below I show the distribution of data from each original class. From this bar chart we know that the highest proportion is in the Middle class, but the proportion in the Low class is also not much different from the Middle class. From this we can represent that the users involved in research have a medium-to-low level of knowledge."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 , 4))\npal1 = [\"#FA5858\", \"#58D3F7\", \"#704041\", \"#f5c3c4\"]\nsns.countplot(y = ' UNS' , data = users, palette=pal1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\")\npal = [\"#FA5858\", \"#58D3F7\", \"#adf2f1\", \"#704041\", \"#197a64\"]\n\nsns.pairplot(users, hue=\" UNS\", palette=pal)\nplt.title(\" UNS\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice:**\nThe plot above shows the distribution in each variable / column. \nWe can focus our attention on the PEG box. As we know that PEG is exam performance of user for goal objects. We can see that the higher result exam of a user, this user tends to be classified into the class of \"high knowledge\" and vice versa. "},{"metadata":{},"cell_type":"markdown","source":"## K-Means Clustering\n### Find Optimum *k*\nThe elbow method is mostly used in unsupervised learning algorithms to determine the optimal number of clusters that should be used to find specific unknown groups within our population. The elbow method finds the average sum of squares distance between the cluster centroid and the data observations. Below I use 2 variables, STG and PEG, which are my reference in grouping data. This selection is based on the achievement of the exam results with the length of study time to achieve the goal."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = users[['STG' , 'PEG']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 10):\n    models = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=100, \n                        tol=0.0001,  random_state= 100  , algorithm='elkan') )\n    models.fit(X)\n    inertia.append(models.inertia_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph below tells us that the *k* value which is considered quite optimum to be used for clustering is *k* = 3."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 10) , inertia , 'o')\nplt.plot(np.arange(1 , 10) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering and Visualization"},{"metadata":{},"cell_type":"markdown","source":"After choosing the number *k* = 3, it means we will clustering users into 3 levels of user knowledge. \nTo see how the characteristics of each cluster, let's look at the clustering graph below."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = (KMeans(n_clusters = 3 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nmodels.fit(X)\nlabels = models.labels_\ncentroids = models.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(models.cluster_centers_)\nprint(models.inertia_)\nprint(models.n_iter_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has an inertia value of 9.13. Inertia is sum of squared distances of samples to their closest cluster center. From information above, to achieve convergence this model requires 6 iterations."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nplt.scatter(X[:,0], X[:,1], c=models.labels_, cmap=\"Set1_r\", s=25)\nplt.scatter(models.cluster_centers_[:,0] ,models.cluster_centers_[:,1], color='blue', marker=\"*\", s=250)\nplt.title(\"Kmeans Clustering \\n Finding Unknown Groups in the Population\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster Characteristics:\n- Cluster 1: centered at the point (0.71452381, 0.61690476). Users in this cluster have the ability to achieve goals with a high enough time and exam results show a high enough value. If referring to a research paper, this user group is classified as a user who has a `Middle` level of knowledge.\n- Cluster 2: centered at the point (0.30986325, 0.21284615). Users in this cluster have the ability to achieve goals with relatively low time achievements and the exam results show quite low values as well. If referring to a research paper, this user group is classified as a user who has a `Low` level of knowledge.\n- Cluster 3: center on the point (0.29789899, 0.68171717). Users in this cluster have the ability to achieve goals with relatively low time performance but the exam results show a high enough value. If referring to a research paper, this user group is classified as a user who has a `High` level of knowledge.\n\nTo see a more specific group, we will try to do clustering again by changing the value of k to k = 4."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"models2 = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nmodels2.fit(X)\nlabels2 = models2.labels_\ncentroids2 = models2.cluster_centers_\nprint(models2.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(models2.inertia_)\nprint(models2.n_iter_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second model with 4 clusters has an inertia value of 6.719. From information above, to achieve convergence this model requires 10 iterations. When compared with the previous model, this second model has a smaller value of inertia. That means the distance between samples and cluster points in the second model is shorter or it can be said that the clusters in the second model are denser than the first model. However, this second model requires more iterations to achieve convergence."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nplt.scatter(X[:,0], X[:,1], c=models2.labels_, cmap=\"Set1_r\", s=25)\nplt.scatter(models2.cluster_centers_[:,0] ,models2.cluster_centers_[:,1], color='blue', marker=\"*\", s=250)\nplt.title(\"Kmeans Clustering \\n Finding Unknown Groups in the Population\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster Characteristics:\n- Cluster 1: centered at the point (0.71810811, 0.65594595). Users in this cluster have the ability to achieve goals with quite high time achievements and exam results to achieve goals show high scores as well. Users in this cluster are classified as users who have a Middle level of understanding.\n- Cluster 2: center on the point (0.51728889, 0.24488889). Users in this cluster have the ability to achieve goals with a high enough time but the results of the exam to reach the goal show a high value. Users in this cluster are classified as users who have a Very Low level of understanding.\n- Cluster 3: center on the point (0.21289744, 0.20478205). Users in this cluster have the ability to achieve goals with relatively low time achievements and exam results to achieve goals show low scores as well. Users in this cluster are classified as users who have a low level of understanding.\n- Cluster 4: centered at the point (0.29900000, 0.68408163). Users in this cluster have the ability to achieve goals with relatively low time performance, but the results of the exam to achieve goals show high scores. Users in this cluster are classified as users who have a high level of understanding.\n\n**Conclusion:**\nUsers in cluster 2 need special attention and handling because when viewed in a long study period and it was sufficient but the exam scores obtained by these users tend to be small, this can happen because of other factors beyond the factors used for research."},{"metadata":{},"cell_type":"markdown","source":"## 3D Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = users[['STG' , 'LPR', 'PEG']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 10):\n    models3 = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=100, \n                        tol=0.0001,  random_state= 100  , algorithm='elkan') )\n    models3.fit(X)\n    inertia.append(models3.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 10) , inertia , 'o')\nplt.plot(np.arange(1 , 10) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models3 = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nmodels3.fit(X)\nlabels3 = models3.labels_\ncentroids3 = models3.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users['labels3'] =  labels3\ntrace3 = go.Scatter3d(\n    x= users['STG'],\n    y= users['LPR'],\n    z= users['PEG'],\n    mode='markers',\n     marker=dict(\n        color = users['labels3'], \n        size= 15,\n        line=dict(\n            color= users['labels3'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace3]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'STG'),\n            yaxis = dict(title  = 'LPR'),\n            zaxis = dict(title  = 'PEG')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical clustering\nHierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as shc\nX = users[['STG', 'SCG', 'STR', 'LPR', 'PEG']].iloc[: , :].values\nplt.figure(figsize=(10, 7))\nplt.title(\"User Knowledge Dendograms\")\nplt.xlabel('Users')\ndend = shc.dendrogram(shc.linkage(X, method='complete'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the bottom there are 258 points that make up each group. From each of these points will be grouped with other points that have the closest distance so that later it will only be one large group above. Each height in the dendrogram represents the distance between points in the cluster. We will get 4 cluster if we set distance of cluster 1.50 and 1.20."},{"metadata":{},"cell_type":"markdown","source":"### If you liked my work please UPVOTE , Thank you."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}