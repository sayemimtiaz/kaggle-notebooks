{"cells":[{"metadata":{},"cell_type":"markdown","source":"This Kernel extracts building with unet based resnet34.The building dataset is from [alibaba](https://tianchi.aliyun.com/competition/entrance/231767/introduction)\n.This kernel is just a basic architecture to help the green hand learn how to apply deep learning in building extraction. You can try the following steps to improve result when you master it.\n- You can upload your dataset in kaggle\n- try some data augmentation\n- change the encoder, such as resnet50, senet, etc","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pdb\nimport time\nimport warnings\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom albumentations.pytorch.functional import img_to_tensor\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torchvision \nfrom torchvision import models\nfrom torchvision import transforms\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom random import sample\nimport tqdm\nimport glob\nimport cv2\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **show some images and labels**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_img_folder = \"../input/ali-data/train_images\"\nprint(len(os.listdir(input_img_folder)))\nimageList = glob.glob(f\"{input_img_folder}/*png\")\nImage1_dir = imageList[random.randint(1, 150)]\nImage2_dir = imageList[random.randint(1, 150)]\nimage1 = Image.open(Image1_dir)\nlabel1 = Image.open(Image1_dir.replace(\"train_images\", \"train_labels\"))\nimage2 = Image.open(Image2_dir)\nlabel2 = Image.open(Image2_dir.replace(\"train_images\", \"train_labels\"))\nplt.subplot(221)\nplt.title(\"image\")\nplt.imshow(image1)\nplt.subplot(222)\nplt.title(\"label\")\nplt.imshow(label1)\nplt.subplot(223)\nplt.imshow(image2)\nplt.subplot(224)\nplt.imshow(label2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data augmentation function ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_monochrome(x):\n    # x_ = x.convert('L')\n    x_ = np.array(x).astype(np.float32)  # convert image to monochrome\n    return x_\n\n\ndef to_tensor(x):\n    x_ = np.expand_dims(x, axis=0)\n    x_ = torch.from_numpy(x_)\n    return x_\n    \n    \nclass DualCompose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, x, mask=None):\n        for t in self.transforms:\n            x, mask = t(x, mask)\n        return x, mask\n\n\nclass VerticalFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 0)\n            if mask is not None:\n                mask = cv2.flip(mask, 0)\n        return img, mask\n\n\nclass HorizontalFlip:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 1)\n            if mask is not None:\n                mask = cv2.flip(mask, 1)\n        return img, mask\n\n\nclass RandomFlip:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            d = random.randint(-1, 1)\n            img = cv2.flip(img, d)\n            if mask is not None:\n                mask = cv2.flip(mask, d)\n\n        return  img, mask\n\n\nclass RandomRotate90:\n    def __init__(self, prob=0.6):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            factor = random.randint(0, 4)\n            img = np.rot90(img, factor)\n            if mask is not None:\n                mask = np.rot90(mask, factor)\n        return img.copy(), mask.copy()\n\n\nclass Rotate:\n    def __init__(self, limit=90, prob=0.5):\n        self.prob = prob\n        self.limit = limit\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            angle = random.uniform(-self.limit, self.limit)\n            height, width = img.shape[0:2]\n            mat = cv2.getRotationMatrix2D((width/2, height/2), angle, 1.0)\n            img = cv2.warpAffine(img, mat, (height, width),\n                                 flags=cv2.INTER_LINEAR,\n                                 borderMode=cv2.BORDER_REFLECT_101)\n            if mask is not None:\n                mask = cv2.warpAffine(mask, mat, (height, width),\n                                      flags=cv2.INTER_LINEAR,\n                                      borderMode=cv2.BORDER_REFLECT_101)\n\n        return img, mask\n\n\nclass Shift:\n    def __init__(self, limit=50, prob=0.5):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            limit = self.limit\n            dx = round(random.uniform(-limit, limit))\n            dy = round(random.uniform(-limit, limit))\n\n            height, width, channel = img.shape\n            y1 = limit + 1 + dy\n            y2 = y1 + height\n            x1 = limit + 1 + dx\n            x2 = x1 + width\n\n            img1 = cv2.copyMakeBorder(img, limit+1, limit + 1, limit + 1, limit +1,\n                                      borderType=cv2.BORDER_REFLECT_101)\n            img = img1[y1:y2, x1:x2, :]\n            if mask is not None:\n                mask1 = cv2.copyMakeBorder(mask, limit+1, limit + 1, limit + 1, limit +1,\n                                      borderType=cv2.BORDER_REFLECT_101)\n                mask = mask1[y1:y2, x1:x2, :]\n\n        return img, mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class RSDataset(Dataset):\n    def __init__(self, input_root, mode=\"train\", debug = False):\n        super().__init__()\n        self.input_root = input_root\n        self.mode = mode\n        if debug == False:\n            self.input_ids = sorted(img for img in os.listdir(self.input_root))\n        else:\n            self.input_ids = sorted(img for img in os.listdir(self.input_root))[:500]\n        \n        self.mask_transform = transforms.Compose([\n            transforms.Lambda(to_monochrome),\n            transforms.Lambda(to_tensor),\n        ])\n            \n        self.image_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.transform = DualCompose([\n                RandomFlip(),\n                RandomRotate90(),\n                Rotate(),\n                Shift(),\n            ])\n        \n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        # at this point all transformations are applied and we expect to work with raw tensors\n        imageName = os.path.join(self.input_root,self.input_ids[idx])\n        image = np.array(cv2.imread(imageName), dtype=np.float32)\n        mask = np.array(cv2.imread(imageName.replace(\"train_images\", \"train_labels\")))/255\n        h, w, c = image.shape\n        mask1 = np.zeros((h, w), dtype=int)\n\n        if self.mode == \"train\":\n            image, mask  =  self.transform(image, mask)\n            mask1 = mask[:,:,0]\n            return self.image_transform(image), self.mask_transform(mask1)\n        else:\n            mask1 = mask[:,:,0]\n            return self.image_transform(image), self.mask_transform(mask1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataloader(input_img_folder = \"../input/ali-data/train_images\",\n                 batch_size = 16,\n                 num_workers = 4):\n    # Get valid dataset and train dataset from raw dataset\n    num_train = len(sorted(img for img in os.listdir(input_img_folder)))\n    indices = list(range(num_train))\n    indices = sample(indices, len(indices))\n    split = int(np.floor(0.2 * num_train))\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    #set up datasets\n    train_dataset = RSDataset(\n        input_img_folder,\n        mode = \"train\",\n    )\n\n    val_dataset = RSDataset(\n        input_img_folder,\n        mode=\"valid\",\n    )\n\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, sampler=valid_sampler,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, valid_loader\n\ndef seed_everything(seed=168):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Losses\n\nThis kernel uses BCE Loss (binary cross entropy loss)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n    \"\"\"\n    Args:\n        pr (torch.Tensor): A list of predicted elements\n        gt (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: IoU (Jaccard) score\n    \"\"\"\n\n    if activation is None or activation == \"none\":\n        activation_fn = lambda x: x\n    elif activation == \"sigmoid\":\n        activation_fn = torch.nn.Sigmoid()\n    elif activation == \"softmax2d\":\n        activation_fn = torch.nn.Softmax2d()\n    else:\n        raise NotImplementedError(\n            \"Activation implemented for sigmoid and softmax2d\"\n        )\n\n    pr = activation_fn(pr)\n\n    if threshold is not None:\n        pr = (pr > threshold).float()\n\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n    fn = torch.sum(gt) - tp\n\n    score = ((1 + beta ** 2) * tp + eps) \\\n            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n\n    return score\n\n\nclass DiceLoss(nn.Module):\n    __name__ = 'dice_loss'\n\n    def __init__(self, eps=1e-7, activation='sigmoid'):\n        super().__init__()\n        self.activation = activation\n        self.eps = eps\n\n    def forward(self, y_pr, y_gt):\n        return 1 - f_score(y_pr, y_gt, beta=1., \n                           eps=self.eps, threshold=None, \n                           activation=self.activation)\n\n\nclass BCEDiceLoss(DiceLoss):\n    __name__ = 'bce_dice_loss'\n\n    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n        super().__init__(eps, activation)\n        if activation == None:\n            self.bce = nn.BCELoss(reduction='mean')\n        else:\n            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n        self.lambda_dice=lambda_dice\n        self.lambda_bce=lambda_bce\n\n    def forward(self, y_pr, y_gt):\n        dice = super().forward(y_pr, y_gt)\n        bce = self.bce(y_pr, y_gt)\n        return (self.lambda_dice*dice) + (self.lambda_bce* bce)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some more utility functions\n\nHere are some utility functions for calculating IoU and Dice scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Metrics:\n    \"\"\"Tracking mean metrics\n    \"\"\"\n\n    def __init__(self, labels):\n        \"\"\"Creates an new `Metrics` instance.\n\n        Args:\n          labels: the labels for all classes.\n        \"\"\"\n\n        self.labels = labels\n\n        self.tn = 0\n        self.fn = 0\n        self.fp = 0\n        self.tp = 0\n\n    def add(self, actual, predicted, threshold=0.5):\n        \"\"\"Adds an observation to the tracker.\n\n        Args:\n          actual: the ground truth labels.\n          predicted: the predicted labels.\n        \"\"\"\n        # print(f\"actual:{actual.shape}\")\n        # print(f\"predicted:{predicted.shape}\")\n#         predicted = torch.sigmoid(predicted)\n        actual = actual.view(-1).float()\n        predicted = predicted.view(-1).float()\n        assert (actual.shape == predicted.shape)\n        probability = predicted\n        p = (probability > threshold).float()\n        t = (actual > threshold).float()\n        # confusion = masks.view(-1).float() / actual.view(-1).float()\n        confusion = p / t\n\n        self.tn += torch.sum(torch.isnan(confusion)).item()\n        self.fn += torch.sum(confusion == float(\"inf\")).item()\n        self.fp += torch.sum(confusion == 0).item()\n        self.tp += torch.sum(confusion == 1).item()\n\n    def get_miou(self):\n        \"\"\"Retrieves the mean Intersection over Union score.\n\n        Returns:\n          The mean Intersection over Union score for all observations seen so far.\n        \"\"\"\n        return np.nanmean([self.tn / (self.tn + self.fn + self.fp), self.tp / (self.tp + self.fn + self.fp)])\n\n    def get_fg_iou(self):\n        \"\"\"Retrieves the foreground Intersection over Union score.\n\n        Returns:\n          The foreground Intersection over Union score for all observations seen so far.\n        \"\"\"\n\n        try:\n            iou = self.tp / (self.tp + self.fn + self.fp)\n        except ZeroDivisionError:\n            iou = float(\"Inf\")\n\n        return iou\n\n    def get_mcc(self):\n        \"\"\"Retrieves the Matthew's Coefficient Correlation score.\n\n        Returns:\n          The Matthew's Coefficient Correlation score for all observations seen so far.\n        \"\"\"\n\n        try:\n            mcc = (self.tp * self.tn - self.fp * self.fn) / math.sqrt(\n                (self.tp + self.fp) * (self.tp + self.fn) * (self.tn + self.fp) * (self.tn + self.fn)\n            )\n        except ZeroDivisionError:\n            mcc = float(\"Inf\")\n\n        return mcc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(loader, num_classes, device, net, optimizer, criterion):\n    global global_step\n\n    num_samples = 0\n    running_loss = 0\n\n    metrics = Metrics(range(num_classes))\n\n    net.train()\n    for images, masks in loader:\n        images = images.to(device)\n        masks = masks.to(device)\n        # print(\"images'size:{},masks'size:{}\".format(images.size(),masks.size()))\n\n        num_samples += int(images.size(0))\n\n        optimizer.zero_grad()\n        outputs= net(images)\n\n        loss = criterion(outputs, masks)\n        loss.backward()\n        batch_loss = loss.item()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        running_loss += batch_loss\n\n        for mask, output in zip(masks, outputs):\n            prediction = output.detach()\n            metrics.add(mask, prediction)\n\n    assert num_samples > 0, \"dataset contains training images and labels\"\n\n    return {\n        \"loss\": running_loss / num_samples,\n        \"miou\": metrics.get_miou(),\n        \"fg_iou\": metrics.get_fg_iou(),\n        \"mcc\": metrics.get_mcc(),\n    }\n\ndef validate(loader, num_classes, device, net, scheduler, criterion):\n    num_samples = 0\n    running_loss = 0\n\n    metrics = Metrics(range(num_classes))\n\n    net.eval()\n    with torch.no_grad():\n        for images, masks in loader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            num_samples += int(images.size(0))\n\n            outputs = net(images)\n\n            loss = criterion(outputs, masks)\n\n            running_loss += loss.item()\n\n            for mask, output in zip(masks, outputs):\n                metrics.add(mask, output)\n\n    assert num_samples > 0, \"dataset contains validation images and labels\"\n\n    scheduler.step(metrics.get_miou())  # update learning rate\n    torch.cuda.empty_cache()\n\n    return {\n        \"loss\": running_loss / num_samples,\n        \"miou\": metrics.get_miou(),\n        \"fg_iou\": metrics.get_fg_iou(),\n        \"mcc\": metrics.get_mcc(),\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels,\n                 use_batchnorm=True):\n        super().__init__()\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.block(x)\n        return x\n\n\nclass UNet(nn.Module):\n    \"\"\"\n        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n\n        \"\"\"\n    def __init__(self, num_classes=1,  pretrained=True, use_batchnorm=True, freeze_encoder=False):\n        \"\"\"\n        :param num_classes:\n        :param pretrained:\n            False - no pre-trained network is used\n            True  - encoder is pre-trained with resnet34\n        :is_deconv:\n            False: bilinear interpolation is used in decoder\n            True: deconvolution is used in decoder\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.pool = nn.MaxPool2d(2, 2)\n\n        net = torchvision.models.resnet34(pretrained=pretrained)\n        net.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # with torch.no_grad():\n        #     pretrained_conv1 = net.conv1.weight.clone()\n        #     # Assign new conv layer with 4 input channels\n        #     net.conv1 = torch.nn.Conv2d(4, 64, kernel_size=3, padding=1, bias=False)\n        #     net.conv1.weight[:, :3] = pretrained_conv1\n        #     net.conv1.weight[:, 3] = pretrained_conv1[:, 0]\n        self.encoder = net\n\n        decoder_channels = (256, 128, 64, 32, 16)\n        encoder_channels = (512, 256, 128, 64, 64)\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        for layer in self.encoder.parameters():\n            layer.requires_grad = not freeze_encoder\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv0 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu,\n                                   self.pool)\n        self.conv1 = self.encoder.layer1\n        self.conv2 = self.encoder.layer2\n        self.conv3 = self.encoder.layer3\n        self.conv4 = self.encoder.layer4\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0], use_batchnorm=use_batchnorm)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1], use_batchnorm=use_batchnorm)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2], use_batchnorm=use_batchnorm)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3], use_batchnorm=use_batchnorm)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4], use_batchnorm=use_batchnorm)\n        self.final = nn.Conv2d(out_channels[4], num_classes, kernel_size=1)\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        conv0 = self.encoder.conv1(x)\n        conv0 = self.encoder.bn1(conv0)\n        conv0 = self.encoder.relu(conv0)\n\n        conv1 = self.pool(conv0)\n        conv1 = self.conv1(conv1)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n\n        x = self.layer1([conv4, conv3])\n        x = self.layer2([x, conv2])\n        x = self.layer3([x, conv1])\n        x = self.layer4([x, conv0])\n        x = self.layer5([x, None])\n        x = self.final(x)\n\n        return torch.sigmoid(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### set GPU mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using {}\".format(device))\n\n##### res-Unet\nnum_classes = 1\nnet = UNet()\n\nbest_miou = -100\nnum_epochs = 10      ########set the number of epoch\noptimizer = Adam(params=net.parameters(), lr=0.005, weight_decay=0.0001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.2, patience=5, verbose=True)\ncriterion = BCEDiceLoss(eps=1.0, activation=None)\nnet = net.to(device)\n\n###prepare dataset#############\ntrain_loader, val_loader = build_dataloader(batch_size = 16, num_workers = 4)\n\nfor epoch in range(num_epochs):\n        ####################train####################################\n        start = time.strftime(\"%H:%M:%S\")\n        print(f\"Starting epoch: {epoch+1} | phase: train | ⏰: {start}\")\n        train_hist = train(train_loader, num_classes, device, net, optimizer, criterion)\n        print( 'loss',train_hist[\"loss\"],\n                'miou',train_hist[\"miou\"],\n                'fg_iou',train_hist[\"fg_iou\"],\n                'mcc',train_hist[\"mcc\"])\n\n        ######################valid##################################\n        start = time.strftime(\"%H:%M:%S\")\n        print(f\"Starting epoch: {epoch+1} | phase: valid | ⏰: {start}\")\n        val_hist = validate(val_loader, num_classes, device, net, scheduler, criterion)\n        print('loss',val_hist[\"loss\"],\n                'miou',val_hist[\"miou\"],\n                'fg_iou',val_hist[\"fg_iou\"],\n                'mcc',val_hist[\"mcc\"])\n        \n        torch.cuda.empty_cache()\n        # saving model when the validation metric is best\n        if val_hist[\"miou\"] > best_miou:\n            state = {\n                \"epoch\": epoch + 1,\n                \"model\": net,\n                \"best_miou\": val_hist[\"miou\"]\n            }\n            torch.save(state, 'model.pth')  # save model\n            print(\"The model has saved successfully!\")\n            best_miou = val_hist[\"miou\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import ndimage as ndi\nfrom skimage.morphology import opening, closing, square\nimport glob\nimport tqdm\n\nn_workers = 4\ncomponent_size = 81\n# predict on more model\nnet = torch.load('./model.pth', map_location=lambda storage, loc: storage)[\"model\"]\nnet = net.to(device)\n\nimglist = os.listdir(input_img_folder)\nimg = cv2.imread(os.path.join(input_img_folder, imglist[400]))\ntensor = img_to_tensor(img)\ntensor = Variable(torch.unsqueeze(tensor, dim=0).float(), requires_grad=False)\npredict = net(tensor.to(device))[0,0,:,:]\npredict = predict.detach().cpu().numpy()\npredict[predict <= 0.5] = 0\npredict[predict > 0.5] = 1\n\nplt.subplot(121)\nplt.imshow(img)\nplt.subplot(122)\nplt.imshow(predict)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}