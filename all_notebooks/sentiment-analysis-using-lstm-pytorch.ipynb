{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this kernel we will go through a sentiment analysis on imdb dataset using LSTM.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport string\nimport re\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_csv = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndf = pd.read_csv(base_csv)\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting to train and test data","metadata":{}},{"cell_type":"markdown","source":"We will split data to train and test initially.Doing this on earlier stage allows to avoid data lekage.\n","metadata":{}},{"cell_type":"code","source":"X,y = df['review'].values,df['sentiment'].values\nx_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\nprint(f'shape of train data is {x_train.shape}')\nprint(f'shape of test data is {x_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysing sentiment","metadata":{}},{"cell_type":"code","source":"dd = pd.Series(y_train).value_counts()\nsns.barplot(x=np.array(['negative','positive']),y=dd.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tockenization","metadata":{}},{"cell_type":"code","source":"def preprocess_string(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n\n    return s\n\ndef tockenize(x_train,y_train,x_val,y_val):\n    word_list = []\n\n    stop_words = set(stopwords.words('english')) \n    for sent in x_train:\n        for word in sent.lower().split():\n            word = preprocess_string(word)\n            if word not in stop_words and word != '':\n                word_list.append(word)\n  \n    corpus = Counter(word_list)\n    # sorting on the basis of most common words\n    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n    # creating a dict\n    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n    # tockenize\n    final_list_train,final_list_test = [],[]\n    for sent in x_train:\n            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                     if preprocess_string(word) in onehot_dict.keys()])\n    for sent in x_val:\n            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                    if preprocess_string(word) in onehot_dict.keys()])\n            \n    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Length of vocabulary is {len(vocab)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysing review length","metadata":{}},{"cell_type":"code","source":"rev_len = [len(i) for i in x_train]\npd.Series(rev_len).hist()\nplt.show()\npd.Series(rev_len).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations : <br>a) Mean review length = around 69.<br> b) minimum length of reviews is 2.<br>c)There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis.","metadata":{}},{"cell_type":"markdown","source":"### Padding","metadata":{}},{"cell_type":"markdown","source":"Now we will pad each of the sequence to max length ","metadata":{}},{"cell_type":"code","source":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have very less number of reviews with length > 500.\n#So we will consideronly those below it.\nx_train_pad = padding_(x_train,500)\nx_test_pad = padding_(x_test,500)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batching and loading as tensor","metadata":{}},{"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n\n# dataloaders\nbatch_size = 50\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to add an embedding layer because there are less words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.","metadata":{}},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n        super(SentimentRNN,self).__init__()\n \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n \n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n    \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        #lstm\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True)\n        \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n    \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        # embeddings and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n        \n        \n        \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden\n\n              ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_layers = 2\nvocab_size = len(vocab) + 1 #extra 1 for padding\nembedding_dim = 64\noutput_dim = 1\nhidden_dim = 256\n\n\nmodel = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n\n#moving to gpu\nmodel.to(device)\n\nprint(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip = 5\nepochs = 5 \nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n            val_h = tuple([each.data for each in val_h])\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            output, val_h = model(inputs, val_h)\n            val_loss = criterion(output.squeeze(), labels.float())\n\n            val_losses.append(val_loss.item())\n            \n            accuracy = acc(output,labels)\n            val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n    \nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inferance","metadata":{}},{"cell_type":"code","source":"def predict_text(text):\n        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n                         if preprocess_string(word) in vocab.keys()])\n        word_seq = np.expand_dims(word_seq,axis=0)\n        pad =  torch.from_numpy(padding_(word_seq,500))\n        inputs = pad.to(device)\n        batch_size = 1\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(inputs, h)\n        return(output.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nindex = 30\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'Predicted sentiment is {status} with a probability of {pro}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nindex = 32\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'predicted sentiment is {status} with a probability of {pro}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some improvement suggestions are as follow:\n\n* Running a hyperparameter search to optimize your configurations.\n* Using pretraned word embeddings like Glove word embeddings\n* Increasing the model complexity like adding more layers/ using bidirectional LSTMs\n","metadata":{}}]}