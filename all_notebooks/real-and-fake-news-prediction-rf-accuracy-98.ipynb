{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake and Real News Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true=pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake=pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(fake),len(true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true[\"category\"]=1\nfake[\"category\"]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.concat([true,fake])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(15,10))\nsb.countplot(data['subject'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['fulltext'] = data.title + ' ' + data.text\ndata.drop(['title','text'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = data[['fulltext', 'category']]\nfinal = data.reset_index()\nfinal.drop(['index'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Data Preprocessing\n\n## Data Cleaning\n\n.Removing the repeated data.\n\n.Removing Stop-words\n\n.Remove any punctuations or limited set of special characters like , or . or # etc.\n\n.Snowball Stemming the word\n\n.Convert the word to lowercase.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ni=0;\nfor sent in final['fulltext'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nstop = set(stopwords.words('english')) \nsno = nltk.stem.SnowballStemmer('english')\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#.|,|)|(|\\|/]',r'',sentence)\n    \n    return  cleaned\nprint(stop)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n# this code takes a while to run as it needs to run on 500k sentences.\nimport re\ni=0\nstr1=' '\nfinal_string=[]\nall_true_words=[] # store words from +ve reviews here\nall_fake_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['fulltext'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['category'].values)[i] == '1': \n                        all_true_words.append(s) #list of all words used to describe positive reviews\n                    if(final['category'].values)[i] == '0':\n                        all_fake_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['CleanedText']=final_string\nfinal.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label=final[\"category\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample=final['CleanedText']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(sample, label, test_size=0.30, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n##from sklearn.cross_validation import cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sb\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF Vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nX_train = tf_idf_vect.fit_transform(X_train)\nX_test= tf_idf_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n# Creating alpha values in the range from 10^-4 to 10^4\nneighbors = []\ni = 0.0001\nwhile(i<=10000):\n    neighbors.append(np.round(i,3))\n    i *= 3\n\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in neighbors:\n    bn = MultinomialNB(alpha = k)\n    scores = cross_val_score(bn, X_train, Y_train, cv=10, scoring='f1_macro', n_jobs=-1)\n    cv_scores.append(scores.mean())  \n    \n# determining best value of alpha\noptimal_alpha = neighbors[cv_scores.index(max(cv_scores))]\nprint('\\nThe optimal value of alpha is %.3f.' % optimal_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot f1_score vs alpha \nplt.plot(neighbors, cv_scores)\nplt.xlabel('Value of alpha',size=10)\nplt.ylabel('f1_score',size=10)\nplt.title('f1_score VS Alpha_Value Plot',size=16)\nplt.grid()\nplt.show()\n\nprint(\"\\n\\nAlpha values :\\n\",neighbors)\nprint(\"\\nf1_score for each alpha value is :\\n \", np.round(cv_scores,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ============================== Multinomial Naive Bayes with alpha = optimal_alpha ============================================\n# instantiate learning model alpha = optimal_alpha\nbn_optimal = MultinomialNB(alpha = optimal_alpha)\n\n# fitting the model\nbn_optimal.fit(X_train, Y_train)\n\n# predict the response\npredictions = bn_optimal.predict(X_test)\n\n# evaluate accuracy\nacc = accuracy_score(Y_test, predictions) * 100\nprint('\\nThe Test Accuracy of the Multinomial naive Bayes classifier for alpha = %.3f is %f%%' % (optimal_alpha, acc))\n\n# Variables that will be used for  making table in Conclusion part of this assignment\ntfidf_multinomial_alpha = optimal_alpha\ntfidf_multinomial_train_acc = max(cv_scores)*100\ntfidf_multinomial_test_acc = acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bn_optimal.classes_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Now we can find log probabilities of different features for both the classes\nclass_features = bn_optimal.feature_log_prob_\n\n#  row_0 is for 'Fake' class and row_1 is for 'True' class\nFake_features = class_features[0]\nTrue_features = class_features[1]\n\n# Getting all feature names\nfeature_names = tf_idf_vect.get_feature_names()\n\n# Sorting 'Fake_features' and 'True_features' in descending order using argsort() function\nsorted_Fake_features = np.argsort(Fake_features)[::-1]\nsorted_True_features = np.argsort(True_features)[::-1]\n\nprint(\"Top 20 Important Features and their log probabilities For Fake News :\\n\\n\")\nfor i in list(sorted_Fake_features[0:20]):\n    print(\"%s\\t -->\\t%f  \"%(feature_names[i],Fake_features[i]))\n    \nprint(\"\\n\\nTop 20 Important Features and their log probabilities For true news :\\n\\n\")\nfor i in list(sorted_True_features[0:20]):\n    print(\"%s\\t -->\\t%f  \"%(feature_names[i],True_features[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nMNB_f1 = round(f1_score(Y_test, predictions, average='weighted'), 3)\nMNB_accuracy = round((accuracy_score(Y_test, predictions)*100),2)\n\nprint(\"Accuracy : \" , MNB_accuracy , \" %\")\nprint(\"f1_score : \" , MNB_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for drawing seaborn heatmaps\nclass_names = ['Fake','True']\ndf_heatmap = pd.DataFrame(confusion_matrix(Y_test, predictions), index=class_names, columns=class_names )\nfig = plt.figure(figsize=(10,7))\nheatmap = sb.heatmap(df_heatmap, annot=True, fmt=\"d\")\n\n# Setting tick labels for heatmap\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nplt.ylabel('Predicted label',size=18)\nplt.xlabel('True label',size=18)\nplt.title(\"Confusion Matrix\\n\",size=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble models\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint as sp_randint\n\ndepths=[1,5,50,100]\nestimators=[1,5,50,100]\nclf = RandomForestClassifier()\n\nparams = {'max_depth' : depths,\n          'n_estimators':estimators  \n          }\n\ngrid = GridSearchCV(estimator = clf,param_grid=params ,cv = 2,n_jobs = 3,scoring='roc_auc')\ngrid.fit(X_train, Y_train)\nprint(\"best depth = \", grid.best_params_)\nprint(\"AUC value on train data = \", grid.best_score_*100)\na1 = grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noptimal_depth1 = a1.get('max_depth')\noptimal_bases1 = a1.get('n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=optimal_depth1,n_estimators=optimal_bases1) \n\nclf.fit(X_train,Y_train)\n\npred = clf.predict(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfpr, tpr, threshold = metrics.roc_curve(Y_test, pred)\nroc_auc = metrics.auc(fpr, tpr)\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint(\"Best AUC value\")\nprint(roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Code for drawing seaborn heatmaps\nclass_names = ['Fake','True']\ndf_heatmap = pd.DataFrame(confusion_matrix(Y_test, pred), index=class_names, columns=class_names )\nfig = plt.figure(figsize=(10,7))\nheatmap = sb.heatmap(df_heatmap, annot=True, fmt=\"d\")\n\n# Setting tick labels for heatmap\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nplt.ylabel('Predicted label',size=18)\nplt.xlabel('True label',size=18)\nplt.title(\"Confusion Matrix\\n\",size=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nacc1 = accuracy_score(Y_test, pred) * 100\npre1 = precision_score(Y_test, pred) * 100\nrec1 = recall_score(Y_test, pred) * 100\nf11 = f1_score(Y_test, pred) * 100\nprint('\\nAccuracy=%f%%' % (acc1))\nprint('\\nprecision=%f%%' % (pre1))\nprint('\\nrecall=%f%%' % (rec1))\nprint('\\nF1-Score=%f%%' % (f11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate feature importances from decision trees\nimportances = clf.feature_importances_\n\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1][:25]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = tf_idf_vect.get_feature_names()\n\nsb.set(rc={'figure.figsize':(11.7,8.27)})\n\n# Create plot\nplt.figure()\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(25), importances[indices])\n\n# Add feature names as x-axis labels\nnames = np.array(names)\nplt.xticks(range(25), names[indices], rotation=90)\n\n# Show plot\nplt.show()\n# uni_gram.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=names[indices]\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(width = 800, height = 600,background_color ='white').generate(str(df))\nplt.imshow(wordcloud)\nplt.title(\" Frequent words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank You ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}