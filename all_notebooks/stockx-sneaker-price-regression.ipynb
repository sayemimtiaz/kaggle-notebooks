{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# StockX Sneaker Price Regression\n\nIn this notebook, I will perform regression to predict the price of a given sneaker based on several factors such as order date, brand, sneaker name, sale price, and several others. This dataset is directly from StockX and was used for their 2019 data competition. \n\n### Description from StockX:\n\nThe data we’re giving you consists of a random sample of all Off-White x Nike and Yeezy 350 sales from between 9/1/2017 (the month that Off-White first debuted “The Ten” collection) and the present. There are 99,956 total sales in the data set; 27,794 Off-White sales, and 72,162 Yeezy sales. The sample consists of U.S. sales only.\n\nTo create this sample, we took a random, fixed percentage of StockX sales (X%) for each colorway, on each day, since September 2017. So, for each day the Off-White Jordan 1 was on the market, we randomly selected X% of its sale from each day. (It’s not important to know what X is; all that matters is that it’s a random sample, and that the same fixed X% of sales was selected from every day, for every sneaker).\n\nTo create this sample, we took a random, fixed percentage of StockX sales (X%) for each colorway, on each day, since September 2017. So, for each day the Off-White Jordan 1 was on the market, we randomly selected X% of its sale from each day. (It’s not important to know what X is; all that matters is that it’s a random sample, and that the same fixed X% of sales was selected from every day, for every sneaker).\n\nWe’ve included 8 variables for you to work with: Order Date, Brand, Sneaker Name, Sale Price ($), Retail Price ($), Release Date, Shoe Size, and Buyer State (the U.S. state the buyer shipped to). You can use whatever variables you want in the analysis; you can use 1 variable, or you can use all 8. And remember, every row in the spreadsheet represents an individual StockX sale. There are no averages or order counts; this is just a random sample of daily sales data.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as plticker\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in the data\noriginal_data = pd.read_csv('../input/stockx/StockX-Data-Contest-2019-3.csv', header = 2)\ndf = original_data.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values\nnulls = pd.concat([df.isnull().sum()], axis=1)\nnulls[nulls.sum(axis=1) > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change 'order date' dtype\ndf['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change 'release date' dtype\ndf['Release Date'] = pd.to_datetime(df['Release Date'], format='%m/%d/%Y')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove - from sneaker name\ndf['Sneaker Name'] = df['Sneaker Name'].apply(lambda x: x.replace('-', ' '))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove $ and comma from sale price\ndf['Sale Price'] = df['Sale Price'].apply(lambda x: x.replace('$', ''))\ndf['Sale Price'] = df['Sale Price'].apply(lambda x: x.replace(',', ''))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove $ from retail price\ndf['Retail Price'] = df['Retail Price'].apply(lambda x: x.replace('$', ''))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting some object columns into numerical columns\nobj_cols = ['Sale Price','Retail Price']\nfor col in obj_cols:\n    df[str(col)] = pd.to_numeric(df[str(col)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Bought For Less Than Retail column\ndf['Bought for Less Than Retail'] = df['Sale Price'] < df['Retail Price']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Bought For Retail column\ndf['Bought for Retail'] = df['Sale Price'] == df['Retail Price']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Bought For More Than Retail column\ndf['Bought for More Than Retail'] = df['Sale Price'] > df['Retail Price']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Genereal numeric correlations\n# Analyze trend between shoe size and sale price\n# Analyze trend between sale price and retail price\ncorrelations = df.corr()\nsns.heatmap(correlations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Release date, buyer region, and sneaker name, retail price, shoe size, and brand distribution analysis\ndf_cat = ['Release Date', 'Buyer Region', 'Sneaker Name', 'Retail Price', 'Shoe Size', 'Brand', 'Bought for Retail', 'Bought for Less Than Retail', 'Bought for More Than Retail' ]\nfor cat in df_cat:\n    cat_num = df[str(cat)].value_counts()\n    plt.figure(figsize=(15,6))\n    chart = sns.barplot(x = cat_num.index, y= cat_num)\n    chart.set_title(\"Sneakers Sales by %s\" % (cat))\n    plt.ylabel(\"Sneaker Sales\")\n    chart.set_xticklabels(chart.get_xticklabels(), rotation = 90)\n    plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n    plt.show(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze trend between Sneaker Name & Sale price\n# Create average retail price df\nbruh = df[['Sneaker Name', 'Sale Price']]\n\n# Clean up this list\nsneakernames = ['adidas Yeezy Boost 350 V2 Butter',\n       'Adidas Yeezy Boost 350 V2 Beluga 2pt0',\n       'Adidas Yeezy Boost 350 V2 Zebra',\n       'Adidas Yeezy Boost 350 V2 Blue Tint',\n       'Adidas Yeezy Boost 350 V2 Cream White',\n       'Adidas Yeezy Boost 350 V2 Sesame', 'adidas Yeezy Boost 350 V2 Static',\n       'Adidas Yeezy Boost 350 V2 Semi Frozen Yellow',\n       'Air Jordan 1 Retro High Off White University Blue',\n       'adidas Yeezy Boost 350 V2 Static Reflective',\n       'Nike Air Presto Off White Black 2018',\n       'Nike Air Presto Off White White 2018',\n       'Nike Air VaporMax Off White 2018',\n       'Nike Blazer Mid Off White All Hallows Eve',\n       'Nike Blazer Mid Off White Grim Reaper', 'Nike Zoom Fly Off White Pink',\n       'Nike Air VaporMax Off White Black',\n       'Nike Zoom Fly Off White Black Silver',\n       'Nike Air Force 1 Low Off White Volt',\n       'Adidas Yeezy Boost 350 V2 Core Black Red 2017',\n       'Nike Air Force 1 Low Off White Black White',\n       'Air Jordan 1 Retro High Off White Chicago',\n       'Nike Air Max 90 Off White Black',\n       'Nike Zoom Fly Mercurial Off White Total Orange',\n       'Nike Air Max 90 Off White Desert Ore',\n       'Nike Zoom Fly Mercurial Off White Black', 'Nike Air Max 90 Off White',\n       'Adidas Yeezy Boost 350 V2 Core Black White',\n       'Nike Air Presto Off White', 'Nike Air Max 97 Off White',\n       'Nike Air VaporMax Off White', 'Nike Blazer Mid Off White',\n       'Adidas Yeezy Boost 350 Low V2 Beluga',\n       'Nike React Hyperdunk 2017 Flyknit Off White',\n       'Nike Air Force 1 Low Off White', 'Nike Zoom Fly Off White',\n       'Nike Air Max 97 Off White Menta',\n       'Air Jordan 1 Retro High Off White White',\n       'Adidas Yeezy Boost 350 V2 Core Black Red',\n       'Nike Air Max 97 Off White Black',\n       'Nike Blazer Mid Off White Wolf Grey',\n       'Adidas Yeezy Boost 350 V2 Core Black Copper',\n       'Nike Air Max 97 Off White Elemental Rose Queen',\n       'Adidas Yeezy Boost 350 V2 Core Black Green',\n       'Adidas Yeezy Boost 350 Low Pirate Black 2016',\n       'Adidas Yeezy Boost 350 Low Moonrock',\n       'Adidas Yeezy Boost 350 Low Pirate Black 2015',\n       'Adidas Yeezy Boost 350 Low Oxford Tan',\n       'Adidas Yeezy Boost 350 Low Turtledove',\n       'Nike Air Force 1 Low Virgil Abloh Off White AF100'\n       ]\navgs = []\nfor name in sneakernames:\n    shoerow = bruh.loc[bruh['Sneaker Name'] == name]\n    avgs.append(shoerow.mean()[0])\nAvgPrice = pd.Series(avgs)\nSneakerName = pd.Series(sneakernames)\navgprice_df = pd.DataFrame(columns = ['Sneaker_Name', 'Average_Price'])\navgprice_df['Sneaker_Name'] = SneakerName\navgprice_df['Average_Price'] = AvgPrice\n\n# Crerating visual of average shoe price\nfig_dims = (15, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nchart = sns.barplot(x = avgprice_df['Sneaker_Name'] , y= avgprice_df['Average_Price'])\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90)\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find average sale price by brand\navgs_2 = []\nbds = df[['Brand', 'Sale Price']]\nbrands = [' Yeezy', 'Off-White']\nfor brand in brands:\n    brandrow = bds.loc[bds['Brand'] == str(brand)]\n    avgs_2.append(brandrow['Sale Price'].mean())\nprint('Yeezy average price: $' + str(avgs_2[0]))\nprint('Off-White average price: $' + str(avgs_2[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create boxplot distribution of sales price by brand\nfor brand in brands:\n    brandrow = bds.loc[bds['Brand'] == str(brand)]\n    chart = sns.boxplot(y=brandrow[\"Sale Price\"], showfliers = False)\n    chart.set_title(\"Sale Price Distribution of %s sneakers\" % (brand))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze trend between Sales Price and Order Date\n# Find average sale price per day\ndts = df[['Order Date', 'Sale Price']]\nuniq_ord_dates = df['Order Date'].value_counts().index.tolist()\navg_3 = []\n\nfor date in uniq_ord_dates:\n    daterow = dts.loc[dts['Order Date'] == str(date)]\n    avg_3.append(daterow['Sale Price'].mean())\n\nunq_dates = pd.Series(uniq_ord_dates)\ndate_avgs = pd.Series(avg_3)\ndateprice_df = pd.DataFrame(columns = ['Order_date', 'Average_Price'])\ndateprice_df['Order_date'] = unq_dates.sort_values(ascending = True)\ndateprice_df['Average_Price'] = date_avgs\ndateprice_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create visualization of Average Sale Price Over time\nfig_dims = (20, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nchart = sns.scatterplot(x=\"Order_date\", y=\"Average_Price\", data=dateprice_df)\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(40))\nchart.set_title(\"Average Daily Sale Price Over time\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Average Sale Price on Release Dates Over Time\ndts = df[['Release Date', 'Sale Price']]\nuniq_rel_dates = df['Release Date'].value_counts().index.tolist()\navg_4 = []\n\nfor date in uniq_rel_dates:\n    daterow = dts.loc[dts['Release Date'] == str(date)]\n    avg_4.append(daterow['Sale Price'].mean())\n\nunq_dates = pd.Series(uniq_rel_dates)\ndate_avgs = pd.Series(avg_4)\ndateprice_df_2 = pd.DataFrame(columns = ['Release_date', 'Average_Price'])\ndateprice_df_2['Release_date'] = unq_dates.sort_values(ascending = True)\ndateprice_df_2['Average_Price'] = date_avgs\ndateprice_df_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze trend between buyer region and sale price\n\nbrg = df[['Buyer Region', 'Sale Price']]\nunq_brgs = df['Buyer Region'].value_counts().index.tolist()\navg_5 = []\n\nfor region in unq_brgs:\n    regionrow = brg.loc[brg['Buyer Region'] == str(region)]\n    avg_5.append(regionrow['Sale Price'].mean())\n\nunq_regions = pd.Series(unq_brgs)\nregion_avgs = pd.Series(avg_5)\nregionprice_df = pd.DataFrame(columns = ['Buyer Region', 'Average Price'])\nregionprice_df['Buyer Region'] = unq_regions.sort_values(ascending = True)\nregionprice_df['Average Price'] = region_avgs\n\nfig_dims = (11, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nchart = sns.barplot(x=\"Average Price\", y=\"Buyer Region\", data=regionprice_df, color=\"b\")\nplt.gca().xaxis.set_major_locator(plt.MultipleLocator(20))\nchart.set_title(\"Average Sale Price by Buyer Region\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Quick Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming columns to get rid of spaces \ndf = df.rename(columns={\n    \"Order Date\": \"Order_date\",\n    \"Sneaker Name\": \"Sneaker_Name\",\n    \"Sale Price\": \"Sale_Price\",\n    \"Retail Price\": \"Retail_Price\",\n    \"Release Date\": \"Release_Date\",\n    \"Shoe Size\": \"Shoe_Size\",\n    \"Buyer Region\": \"Buyer_Region\"\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting dates into numericals\nimport datetime as dt\n\ndf['Order_date'] = pd.to_datetime(df['Order_date'])\ndf['Order_date']=df['Order_date'].map(dt.datetime.toordinal)\n\ndf['Release_Date'] = pd.to_datetime(df['Release_Date'])\ndf['Release_Date']=df['Release_Date'].map(dt.datetime.toordinal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting spltis\nfrom sklearn import preprocessing, metrics\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Sale_Price', 'Bought for More Than Retail', 'Bought for Less Than Retail', 'Bought for Retail'], axis=1)\ny = df.Sale_Price\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting categorical data to numerical\nfrom sklearn.preprocessing import OneHotEncoder\n\nobject_cols = ['Sneaker_Name', 'Buyer_Region', 'Brand']\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Adding the column names after one hot encoding\nOH_cols_train.columns = OH_encoder.get_feature_names(object_cols)\nOH_cols_valid.columns = OH_encoder.get_feature_names(object_cols)\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Starting linear regression\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(OH_X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at y-int\nprint(lm.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at coefficient scores of each variable\ncoeff_df = pd.DataFrame(lm.coef_, OH_X_train.columns,columns=['Coefficient'])\nranked_coeff = coeff_df.sort_values(\"Coefficient\", ascending = False)\nranked_coeff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing predictions and running evaluation metrics\npredictions = lm.predict(OH_X_valid)\nfrom sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(y_valid, predictions))\nprint('MSE:', metrics.mean_squared_error(y_valid, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(OH_X_train,y_train)\nrfe = RFE(lm, 10)\nrfe = rfe.fit(OH_X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(OH_X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = OH_X_train[OH_X_train.columns[rfe.support_]]\nX_train_rfe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(X,y):\n    X = sm.add_constant(X) #Adding the constant\n    model = sm.OLS(y, X)\n    results = model.fit() # fitting the model\n    print(results.summary()) # model summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = build_model(OH_X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkVIF(X_train_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['Sneaker_Name_Adidas Yeezy Boost 350 V2 Core Black Green',\n       'Sneaker_Name_Nike Air Force 1 Low Off White',\n       'Sneaker_Name_Nike Air Max 90 Off White',\n       'Sneaker_Name_Nike Air VaporMax Off White Black',\n       'Buyer_Region_Alabama', 'Buyer_Region_Alaska', 'Buyer_Region_Arkansas',\n       'Buyer_Region_Colorado', 'Buyer_Region_Connecticut',\n       'Buyer_Region_Delaware', 'Buyer_Region_District of Columbia',\n       'Buyer_Region_Georgia', 'Buyer_Region_Hawaii', 'Buyer_Region_Idaho',\n       'Buyer_Region_Illinois', 'Buyer_Region_Indiana', 'Buyer_Region_Iowa',\n       'Buyer_Region_Kansas', 'Buyer_Region_Louisiana', 'Buyer_Region_Maine',\n       'Buyer_Region_Massachusetts', 'Buyer_Region_Michigan',\n       'Buyer_Region_Minnesota', 'Buyer_Region_Mississippi',\n       'Buyer_Region_Missouri', 'Buyer_Region_Montana',\n       'Buyer_Region_Nebraska', 'Buyer_Region_Nevada',\n       'Buyer_Region_New Hampshire', 'Buyer_Region_New Jersey',\n       'Buyer_Region_New Mexico', 'Buyer_Region_New York',\n       'Buyer_Region_North Carolina', 'Buyer_Region_North Dakota',\n       'Buyer_Region_Ohio', 'Buyer_Region_Oklahoma',\n       'Buyer_Region_Pennsylvania', 'Buyer_Region_Rhode Island',\n       'Buyer_Region_South Carolina', 'Buyer_Region_South Dakota',\n       'Buyer_Region_Tennessee', 'Buyer_Region_Texas', 'Buyer_Region_Utah',\n       'Buyer_Region_Vermont', 'Buyer_Region_Virginia',\n       'Buyer_Region_Washington', 'Buyer_Region_West Virginia',\n       'Buyer_Region_Wyoming'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = build_model(X_train_new,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping brand names because of collinearity\nX_train_new = X_train_new.drop(['Brand_Off-White', 'Brand_ Yeezy'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_X_train = build_model(X_train_new,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bruv = checkVIF(best_X_train)\nbruv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think that this is as accurate as I can get it. Now let's try it on the test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train, best_X_train).fit()\ny_train_price = lm.predict(best_X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 50)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing into X and y\ny_test = y_valid\nX_test = OH_X_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's use our model to make predictions.\nX_train_new = best_X_train.drop('const',axis=1)\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_pred = lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scoring the model\nfrom sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}