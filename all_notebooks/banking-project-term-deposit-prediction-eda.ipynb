{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\n\n### Business Use Case\n\nThere has been a revenue decline for a Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing efforts on such clients.\n\n### Data Science Problem Statement\n\nPredict if the client will subscribe to a term deposit based on the analysis of the marketing campaigns the bank performed.\n\n### Evaluation Metric\nWe will be using Precision/Recall for evaluation. \n\n### Objective of this template notebook\n\nThe main objective of this template is to take you through the entire working pipeline that you may follow while appraoching a Machine Learning problem.\n\nWe will be defining a task to be performed and write the code to solve the task.\n\n__The tasks performed below should serve as a good guide regarding the steps that you should go about a Machine Learning Problem. But kindly do not restrict yourself to only the tasks that have been performed in this notebook and feel free to bring your ideas,skills and strategies and implement them as well.__\n\n\n### Word of caution\n\nThis template is just an example of a data-science pipeline, every data science problem is unique and there are multiple ways to tackle them. Go through this template and try to leverage the information in this while solving your hackathon problems but you may not be able to use all the functions created here."},{"metadata":{},"cell_type":"markdown","source":"# Understanding the dataset\n\n**Data Set Information**\n\nThe data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be subscribed ('yes') or not ('no') subscribed.\n\nThere are two datasets:\n`train.csv` with all examples (32950) and 21 inputs including the target feature, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n\n`test.csv` which is the test data that consists  of 8238 observations and 20 features without the target feature\n\nGoal:- The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n\n**Features**\n\n|Feature|Feature_Type|Description|\n|-----|-----|-----|\n|age|numeric|age of a person|  \n|job |Categorical,nominal|type of job ('admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')|  \n|marital|categorical,nominal|marital status ('divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)|  \n|education|categorical,nominal| ('basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown') | \n|default|categorical,nominal| has credit in default? ('no','yes','unknown')|  \n|housing|categorical,nominal| has housing loan? ('no','yes','unknown')|  \n|loan|categorical,nominal| has personal loan? ('no','yes','unknown')|  \n|contact|categorical,nominal| contact communication type ('cellular','telephone')|  \n|month|categorical,ordinal| last contact month of year ('jan', 'feb', 'mar', ..., 'nov', 'dec')| \n|day_of_week|categorical,ordinal| last contact day of the week ('mon','tue','wed','thu','fri')|  \n|duration|numeric| last contact duration, in seconds . Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no')|\n|campaign|numeric|number of contacts performed during this campaign and for this client (includes last contact)|  \n|pdays|numeric| number of days that passed by after the client was last contacted from a previous campaign (999 means client was not previously contacted)|  \n|previous|numeric| number of contacts performed before this campaign and for this client|  \n|poutcome|categorical,nominal| outcome of the previous marketing campaign ('failure','nonexistent','success')|  \n\n**Target variable (desired output):**  \n\n|Feature|Feature_Type|Description|\n|-----|-----|-----|\n|y | binary| has the client subscribed a term deposit? ('yes','no')|"},{"metadata":{},"cell_type":"markdown","source":"###  Importing necessary libraries\n\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Importing necessary libraries\n\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier ,GradientBoostingClassifier\nfrom xgboost import XGBClassifier \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,roc_curve,confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns',None)\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading and Cleaning\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n### Load and Prepare dataset\n\n- In this task, we'll load the dataframe in pandas, drop the unnecessary columns and display the top five rows of the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# accessing to the folder where the file is stored\npath = '../input/banking-project-term-deposit/new_train.csv'\n\n# Load the dataframe\ndataframe = pd.read_csv(path)\n\nprint('Shape of the data is: ',dataframe.shape)\n\ndataframe.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Numeric and Categorical Features\n\nIf you are familiar with machine learning, you will know that a dataset consists of numerical and categorical columns.\n\nLooking at the dataset, we think we can identify the categorical and continuous columns in it. Right? But it might also be possible that the numerical values are represented as strings in some feature. Or the categorical values in some features might be represented as some other datatypes instead of strings. Hence it's good to check for the datatypes of all the features.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# IDENTIFYING NUMERICAL FEATURES\n\nnumeric_data = dataframe.select_dtypes(include=np.number) # select_dtypes selects data with numeric features\nnumeric_col = numeric_data.columns                                                                # we will store the numeric features in a variable\n\nprint(\"Numeric Features:\")\nprint(numeric_data.head())\nprint(\"====\"*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# IDENTIFYING CATEGORICAL FEATURES\ncategorical_data = dataframe.select_dtypes(exclude=np.number) # we will exclude data with numeric features\ncategorical_col = categorical_data.columns                                                                              # we will store the categorical features in a variable\n\n\nprint(\"Categorical Features:\")\nprint(categorical_data.head())\nprint(\"====\"*20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECK THE DATATYPES OF ALL COLUMNS:\n    \nprint(dataframe.dtypes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Missing Data \n\nOne of the main steps in data preprocessing is handling missing data. Missing data means absence of observations in columns that can be caused while procuring the data, lack of information, incomplete results etc. Feeding missing data to your machine learning model could lead to wrong prediction or classification. Hence it is necessary to identify missing values and treat them.\n\n- In the code below, we calculate the total missing values and the percentage of missing values in every feature of the dataset.\n- The code ideally returns a dataframe consisting of the feature names as index and two columns having the count and percentage of missing values in that feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To identify the number of missing values in every feature\n\n# Finding the total missing values and arranging them in ascending order\ntotal = dataframe.isnull().sum()\n\n# Converting the missing values in percentage\npercent = (dataframe.isnull().sum()/dataframe.isnull().count())\nprint(percent)\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping missing values\n\nThe given dataset is a pretty clean dataset. But this might not be the case always as you can often encounter missing values represented as `NaN` values  in the data. \n\nThere are two methods of dealing with missing data \n- Dropping them\n- Imputing them.\n\nDepending on the case we can allow a specific proportion of missing values, beyond which we might want to drop the variable from analysis.\n\nThis varies from case to case on the amount of information you think the variable has. For example, if you are working on some dataset which contains a column for date of marriage. It may be blank for 50% (or even more) of the population, but might have very high information about the lifestyle of the person. In such cases, you would still use the variable.\n\nIf the information contained in the variable is not that high, you can drop the variable if it has more than 50% missing values. There are projects / models where imputation of even 20 - 30% missing values provided better results - the famous Titanic dataset on Kaggle being one such case. Age is missing in ~20% of cases, but you benefit by imputing them rather than ignoring the variable.\n\n- Now you have the number and percentage of missing values in every feature, from the previous function. \n- Using this information, you can decide as to what proportion of missing values you should remove from every feature.\n- The code below takes a threshold value of your choice and removes the features having missing value percentage greater than this threshold. The function can take three parameters - the dataframe, missing data dataframe and threshold value."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# dropping features having missing values more than 60%\ndataframe = dataframe.drop((percent[percent > 0.6]).index,axis= 1)\n\n# checking null values\nprint(dataframe.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill null values in continuous features\n\nThere are no null values in any of the continuous columns in this dataset. But when null values exist in a continuous column, a good approach would be to impute them.\n\nThere exists many approach to missing-data imputation and they usually depend on your problem and how your data algorithm behaves. If the features are numeric you can use simple approaches, such as average values and sampling from the feature distribution.\n\n- Missing values in continuous data are mostly imputed using mean or median. What to choose depends on a lot of factors and is to be decided by you\n- Let's write a code snippet that will take the dataframe and the impute missing data with either mean or mode, depending on the user's choice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing missing values with mean\n\nfor column in numeric_col:\n    mean = dataframe[column].mean()\n    dataframe[column].fillna(mean,inplace = True)\n    \n#   imputing with median\n# for column in numeric_col:\n#     mean = dataframe[column].median()\n#     dataframe[column].fillna(mean,inpalce = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for Class Imbalance\n\nClass imbalance occurs when the observations belonging to one class in the target are significantly higher than the other class or classes. A class distribution of **80:20 or greater** is typically considered as an imbalance for a binary classification. \n\nSince most machine learning algorithms assume that data is equally distributed, applying them on imbalanced data often results in bias towards majority classes and poor classification of minority classes. Hence we need to identify & deal with class imbalance. \n\nThe code below that takes the target variable and outputs the distribution of classes in the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are finding the percentage of each class in the feature 'y'\nclass_values = (dataframe['y'].value_counts()/dataframe['y'].value_counts().sum())*100\nprint(class_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Observations : \n- The class distribution in the target is ~89:11. This is a clear indication of imbalance.\n- By now you should be well familiar with the methods on how to deal with the imbalance in data."},{"metadata":{},"cell_type":"markdown","source":"### Detect outliers in the continuous columns\n\nOutliers are observations that lie far away from majority of observations in the dataset and can be represented mathematically in different ways.\n\nOne method of defining outliers are: outliers are data points lying beyond **(third quartile + 1.5xIQR)** and below **(first quartile - 1.5xIQR)**. \n\nThe code below takes a dataframe and outputs the number of outliers in every numeric feature based on the above rule of *IQR* \n\nYou can even modify the code below to capture the outliers as per their other definitions. "},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n- As per the IQR methodology, there are outliers in majority of the columns.\n- In the further steps below, we will see how to deal with the outliers."},{"metadata":{},"cell_type":"markdown","source":"## EDA & Data Visualizations\n\nExploratory data analysis is an approach to analyzing data sets by summarizing their main characteristics with visualizations. The EDA process is a crucial step prior to building a model in order to unravel various insights that later become important in developing a robust algorithmic model. "},{"metadata":{},"cell_type":"markdown","source":"###  Univariate analysis of Categorical columns\n\nUnivariate analysis means analysis of a single variable. Itâ€™s mainly describes the characteristics of the variable.\n\nIf the variable is categorical we can use either a bar chart or a pie chart to find the distribution of the classes in the variable.\n\n- The code plots the frequency of all the values in the categorical variables. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the categorical columns\ncategorical_col = dataframe.select_dtypes(include=['object']).columns\nplt.style.use('ggplot')\n# Plotting a bar chart for each of the cateorical variable\nfor column in categorical_col:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    dataframe[column].value_counts().plot(kind='bar')\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n\nFrom the above visuals, we can make the following observations: \n- The top three professions that our customers belong to are - administration, blue-collar jobs and technicians.\n- A huge number of the customers are married.\n- Majority of the customers do not have a credit in default\n- Many of our past customers have applied for a housing loan but very few have applied for personal loans.\n- Cell-phones seem to be the most favoured method of reaching out to customers.\n- Many customers have been contacted in the month of **May**.\n- The plot for the target variable shows heavy imbalance in the target variable. \n- The missing values in some columns have been represented as `unknown`. `unknown` represents missing data. In the next task, we will treat these values.  "},{"metadata":{},"cell_type":"markdown","source":"### Imputing `unknown` values of categorical columns \n\nIn the previous task we have seen some categorical variables have a value called `unknown`. `unknown` values are a kind of missing data.\nDepending on the use case, we can decide how to deal with these values. One method is to directly impute them with the mode value of respective columns.\n\n- The code below imputes the value `unknown` in the categorical columns with the mode value of that column. You can modify this function to replace any unwanted value(for e.g `NaN` value) in a column with a value of your choice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute mising values of categorical data with mode\nfor column in categorical_col:\n    mode = dataframe[column].mode()[0]\n    dataframe[column] = dataframe[column].replace('unknown',mode)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis of Continuous columns\nJust like for categorical columns, by performing a univariate analysis on the continuous columns, we can get a sense of the distrbution of values in every column and of the outliers in the data. Histograms are great for plotting the distribution of the data and boxplots are the best choice for visualizing outliers. \n\n- The code below plots a histogram of all the continuous features and other that plots a boxplot of the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numeric_col:\n    plt.figure(figsize=(20,5))\n    plt.subplot(121)\n    sns.distplot(dataframe[column])\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numeric_col:\n    plt.figure(figsize=(20,5))\n    plt.subplot(122)\n    sns.boxplot(dataframe[column])\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation :\n\n- As we can see from the histogram, the features `age`, `duration` and `campaign` are heavily skewed and this is due to the presence of outliers as seen in the boxplot for these features. We will deal with these outliers in the steps below.\n- Looking at the plot for `pdays`, we can infer that majority of the customers were being contacted for the first time because as per the feature description for `pdays` the value 999 indicates that the customer had not been contacted previously. \n- Since the features `pdays` and `previous` consist majorly only of a single value, their variance is quite less and hence we can drop them since technically will be of no help in prediction."},{"metadata":{},"cell_type":"markdown","source":"### Dropping the columns `pdays` & `previous`"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.drop(['pdays','previous'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis - Categorical Columns\n\nBivariate analysis involves checking the relationship between two variables simultaneously. In the code below, we plot every categorical feature against the target by plotting a barchart. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfor column in categorical_col:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.countplot(x=dataframe[column],hue=dataframe['y'],data=dataframe)\n    plt.title(column)    \n    plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- The common traits seen for customers who have subscribed for the term deposit are :\n    - Customers having administrative jobs form the majority amongst those who have subscirbed to the term deposit with technicians being the second majority.\n    - They are married \n    - They hold a university degree\n    - They do not hold a credit in default\n    - Housing loan doesn't seem a priority to check for since an equal number of customers who have and have not subscribed to it seem to have subscribed to the term deposit.\n    - Cell-phones should be the preferred mode of contact for contacting customers."},{"metadata":{},"cell_type":"markdown","source":"### Treating outliers in the continuous columns\n\n- Outliers can be treated in a variety of ways. It depends on the skewness of the feature.\n- To reduce right skewness, we use roots or logarithms or reciprocals (roots are weakest). This is the most common problem in practice.\n- To reduce left skewness, we take squares or cubes or higher powers.\n- But in our data, some of the features have negative values and also the value 0. In such cases, square root transform or logarithmic transformation cannot be used since we cannot take square root of negative values and logarithm of zero is not defined.\n- Hence for this data we use a method called **Winsorization**. In this method we define a confidence interval of let's say 90% and then replace all the outliers below the 5th percentile with the value at 5th percentile and all the values above 95th percentile with the value at the 95th percentile. It is pretty useful when there are negative values and zeros in the features which cannot be treated with log transforms or square roots. Do read up on it more [here](https://www.statisticshowto.datasciencecentral.com/winsorize/)\n\nLets' write a code below that treats all the outliers in the numeric features using winsorization."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_col = dataframe.select_dtypes(include=np.number).columns\n\nfor col in numeric_col:    \n    dataframe[col] = winsorize(dataframe[col], limits=[0.05, 0.1],inclusive=(True, True))\n\n# Now run the code snippet to check outliers again","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation :\n\nUsing winsorization has resulted in removal of all the outliers from the numerical columns.  You can even use normalization or standardization for dealing with outliers. "},{"metadata":{},"cell_type":"markdown","source":"### Function to Label Encode Categorical variables\n\nBefore applying our machine learning algorithm, we need to recollect that any algorithm can only read numerical values. It is therefore essential to encode categorical features into numerical values. Encoding of categorical variables can be performed in two ways:\n- Label Encoding\n- One-Hot Encoding.\n\nFor the given dataset, we are going to label encode the categorical columns. \n\n- In the code below we will perform label encoding on all the categorical features and also the target (since it is categorical) in the  dataset. You can modify the below function in order to perform One-Hot Encoding as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing lable encoder\nle = LabelEncoder()\n\n# Initializing Label Encoder\nle = LabelEncoder()\n\n# Iterating through each of the categorical columns and label encoding them\nfor feature in categorical_col:\n    try:\n        dataframe[feature] = le.fit_transform(dataframe[feature])\n    except:\n        print('Error encoding '+feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.to_csv(r'preprocessed_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nprof = ProfileReport(dataframe)\nprof","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}